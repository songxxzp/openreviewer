[
    {
        "id": "ZXR8ZA6VrL3",
        "original": null,
        "number": 1,
        "cdate": 1665756712360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665756712360,
        "tmdate": 1665756712360,
        "tddate": null,
        "forum": "ObtGcyKmwna",
        "replyto": "ObtGcyKmwna",
        "invitation": "ICLR.cc/2023/Conference/Paper1133/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a sequential monte carlo (SMC) algorithm for planning. The main contribution is to define the particle weights using the Q(s, a)-function, which allows using a large number of actions for the same state without having to evaluate the transition model for each.",
            "strength_and_weaknesses": "**strengths**\n\n* The paper is proposing a simple modification of regular SMC (in the context of planning) which brings two benefits: a computational one (that we can use a larger number of particles for the same number of transition evaluations) and a conceptual one (that the resulting algorithm can be used in a model-free fashion).\n* The paper contains results on refining a prior policy, which is an interesting scenario that is somewhat underrepresented in current research.\n\n**weaknesses**\n\n* Given the method's proximity to [1], it is unfortunate that the paper doesn't contain a comparison to [1] in the same experimental suite or an overlapping one. That makes it harder to compare the two methods.\n* The baselines considered in the experiments are: ITRA, SAC, SMC and rejection sampling. The only baseline which has previous published results on these problems is ITRA, which the proposed method does outperform, but that is also to be expected since the authors are building on top of ITRA. In other words, the paper is missing a baseline which has previous published results on this experiment suite. In my opinion this is the paper's biggest issue at the moment.\n\n---\n\n[1] Probabilistic Planning with Sequential Monte Carlo Methods, https://openreview.net/forum?id=ByetGn0cYX",
            "clarity,_quality,_novelty_and_reproducibility": "The algorithm described in the paper (see: Algorithm 1) suffers from the well-known optimism bias of the control-as-inference framework [2]. This is when conditioning on optimality causes the planner to assume that any stochasticity in the system to turn out in the agent's favor. The authors are aware of this problem, and yet the only discussion of this happens in the final paragraph of the paper. Here, the authors point out that this issue does not affect the experiments in the paper because the systems used are deterministic. I have two issues here:\n\n* The way the optimism bias is discussed is not ideal, because a reader that is familiar with the control-as-inference framework might notice the bias issue as soon as reading the algorithm box for Algorithm 1 and will be confused about why this problem is not addressed. To be clear, it is fine to assume deterministic systems, but if an algorithm has an obvious pitfall, it should be discussed more prominently.\n* I'm not sure that the systems in the paper are actually deterministic. The toy experiment features additional agents which \"[...] move adversarially towards the ego agent with some added Gaussian noise.\" (Section 4.1). This sounds like a stochastic system to me, unless the movement of the adversaries is pre-computed and then kept fixed throughout.\n\n---\n\n[2] Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review, https://arxiv.org/abs/1805.00909",
            "summary_of_the_review": "The paper proposes a simple method which has tangible benefits. The presentation could be improved to make limitations clearer (i.e. the optimism bias) and the experimental evaluation could be strengthened (e.g. by comparing to [1] using parts of their experiments section) but the paper still makes contributions that are worthy of acceptance.\n\n---\n\n[1] Probabilistic Planning with Sequential Monte Carlo Methods, https://openreview.net/forum?id=ByetGn0cYX",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_9uTg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_9uTg"
        ]
    },
    {
        "id": "-3KnnMJjc9",
        "original": null,
        "number": 2,
        "cdate": 1666669760525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669760525,
        "tmdate": 1668735403508,
        "tddate": null,
        "forum": "ObtGcyKmwna",
        "replyto": "ObtGcyKmwna",
        "invitation": "ICLR.cc/2023/Conference/Paper1133/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an approach for planning-as-inference in reinforcement learning problems with sparse rewards. A method based on sequential Monte Carlo is derived, similar to Pich\u00e9 et al. (2019), to sample from the optimal state-action trajectories distribution by applying filtering and smoothing techniques. The algorithm uses estimates of a soft Q-function as a critic to provide heuristic factors when resampling actions according to their optimality within the SMC loop. In contrast to prior work on SMC for planning in RL, CriticSMC uses a fixed prior policy, instead of learned one, and uses the learned Q-function to guide action sampling, which simplifies the training process.",
            "strength_and_weaknesses": "# Strengths\n* The idea of using Q-functions to guide sampling in SMC when observations are sparse is interesting and might be useful beyond the RL and planning contexts.\n* The proposed algorithm shows significant performance improvements in a toy collision-avoidance experiment and a more realistic one using a benchmarking dataset when compared to a baseline human-like driving policies and soft-actor critic.\n\n# Weaknesses\n* **Background.** The background on SMC for planning-as-inference is quite confusing for someone familiar with SMC and control, but unfamiliar with this line of work applying SMC to planning. Questions I had at a first pass: *Is it only simulating states in parallel and \"observing\" the optimality indicator*? *How do you use the sampler with a (physical) agent*? There is no clear distinction between **what's done in simulation and what's applied in the actual environment** by the agent. I had to go and read Pich\u00e9 et al.'s (2019) work on SMC for planning to understand that SMC is playing the role of a sampler from the optimal state-action trajectories distribution. The sampled optimal action trajectories can then either be executed blindly by an agent in the target environment after sampling or be used in a model predictive control (MPC) fashion where only the first action is applied to the environment, and from the resulting next state a new SMC planning procedure unrolls.\n\n* **Sec. 3.1: Deterministic vs. stochastic state transitions.** The soft Q-function approximation in Equation 3 relies on the assumption of deterministic state transitions, but the algorithm descriptions in both Figure 2 and Algorithm 1 use stochastic state transitions. It also gives the impression to me that deterministic transitions could lead to some degenaracy in the SMC scheme. However, this mixed presentation in the methodology is confusing and does not explain how deterministic transitions may affect sampling performance in practice.\n\n* **Sec. 3.2: Data for training.** It is not clear to me what is the used to learn the parametric Q function estimators in Eq. 4. No reference for what *state-action occupancy* (SAO) means is provided. I am not sure if it is based on the experience data (from the target environment) observed up to a given point in time, when executing online, or purely on simulation data using the world model that is applied within SMC.\n\n* **Related work.** It might be worth contrasting the paper with some earlier work on SMC for RL:\n\n    Lazaric, A., Restelli, M., & Bonarini, A. (2007). Reinforcement Learning in Continuous Action Spaces through Sequential Monte Carlo Methods. In J. Platt, D. Koller, Y. Singer, & S. Roweis (Eds.), Advances in Neural Information Processing Systems (Vol. 20).\n\n* **Experiments.** The method is proposed as a general framework for planning in environments with sparse rewards, but it is only tested on two autonomous driving environments. It would be beneficial to compare its improvements over state-of-the-art on RL methods for sparse rewards in other classic environments used in this line of work, such as Atari games. Otherwise, the claims of the paper should at least be adjusted to reflect the focus on autonomous driving.",
            "clarity,_quality,_novelty_and_reproducibility": "* The paper is mostly well written and the ideas seem novel.",
            "summary_of_the_review": "The paper's contribution seems novel, but aspects of the methodology are confusing and the experimental comparisons should be expanded to better support the claims.\n\n#### Rebuttal update\nI have raised my score (5 to 6) after reading the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_qbn3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_qbn3"
        ]
    },
    {
        "id": "FK92egiBKqu",
        "original": null,
        "number": 3,
        "cdate": 1667137086445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667137086445,
        "tmdate": 1668765717196,
        "tddate": null,
        "forum": "ObtGcyKmwna",
        "replyto": "ObtGcyKmwna",
        "invitation": "ICLR.cc/2023/Conference/Paper1133/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the Critic Sequential Monte-Carlo approach, a simple extension of the Sequential Monte-Carlo method, in the framework of \"planning as inference\".\nThe approach combines the sequential Monte-Carlo technique with heuristic factors computed using a soft q-function.\nThe authors then test the framework in two safety-related tasks, a multi-agent point-mass environment, and a driving simulator.\n",
            "strength_and_weaknesses": "- The paper is sufficiently well written\n- The paper is too incremental\n- The paper lacks a proper experimental analysis on standard and complex benchmarks: the toy example is too simple, and the simulator seems too complicated and convoluted to understand the algorithm's behavior.\n- it is missing a strong theoretical contribution to the method, right now it seems to be mostly a heuristics\n- The paper builds on pretty strong assumptions: known model and deterministic dynamics\n- The paper tries to tackle a typical problem of safe reinforcement learning, without citing any of the approaches and papers on the areas.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably well written.\nThe novelty of the work seems extremely limited. Every modification presented in this approach seems to be already present in the previous literature.\nI cannot evaluate the reproducibility of this approach at this stage.",
            "summary_of_the_review": "While this paper seems to be overall a decent contribution, it is too incremental. To accept incremental work, I would expect clear performance improvements, a strong experimental campaign, vast ablation studies, or interesting insights from the theoretical standpoint.\nThis paper misses all the points above.\n\nFurthermore, the authors are using safety scenarios as a test bed. There is an entire literature on safety in Markov decision processes:\n1) The Constrained MDP framework (CMDP)\n2) The Control barrier function approach\n3) The shielding approach\n4) Reachability analysis\n\nMany of these methods are also model-based and are able to ensure safety while optimizing a policy. It is easy to perform slight modifications to these methods and could fit perfectly the target domains (I suspect these methods will outperform the proposed approach in the given tasks)\n\nI suggest the reviewer either expand the set of tasks to show more generality of their approach (they claim it, but is not shown) or add safe RL baselines to the comparison. For sure, if the focus of the paper is safety, at least a discussion on safe RL methods should be fundamental, explaining why a \"planning as inference\" approach should be better than methods explicitly designed to enforce safety.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_toeN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_toeN"
        ]
    },
    {
        "id": "EjzifjZOw7",
        "original": null,
        "number": 4,
        "cdate": 1667167483078,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667167483078,
        "tmdate": 1667167483078,
        "tddate": null,
        "forum": "ObtGcyKmwna",
        "replyto": "ObtGcyKmwna",
        "invitation": "ICLR.cc/2023/Conference/Paper1133/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a solution to the planning as inference problem in domains where it is easy to generate samples of actions in the current state but it is expensive to simulate samples of next states. The proposed solution involves learning a parametric approximation of the soft value of each state-action pair by using gradient descent. The paper also shows some examples on a self-driving car datasets and demonstrate better results. There are also some ablation studies.\n\nThe main contribution of the work is the observation that in some domains it is hard to simulate the next state, but very cheap to sample an action in the current state. This implies that the existing methods in the literature of learning a soft value of a state is not as helpful as learning the soft-value of a state-action pair (the so called critic function). The authors demonstrate this point by devising a particle filter-based algorithm that uses hypothetical particles in the training episodes to learn the critic function quickly.",
            "strength_and_weaknesses": "The paper is written quite clearly and the main point is very easy to grasp. This is a very logical extension of the previous work by Pich\u00e9 et al 2019 which learns the soft-state-value function. The idea of using putative particles to learn the soft-state-action-value function aka the critic is quite well motivated.\n\nWhat I have would have liked to see is some tradeoffs between using a large number of putative particles versus using a few putative states. In the work of Pich\u00e9 only one putative state is used, but one could consider a simple extension of that approach where multiple putative states are used. I understand the paper's contention that putative actions are cheaper than putative states, but putative actions are not entirely free either.\n\nNevertheless, there are clear benefits of doing model-free learning unlike the previous approach of Pich\u00e9.\n",
            "clarity,_quality,_novelty_and_reproducibility": "As stated, the main point of the paper comes out very clearly but some of the auxiliary points are not as clear. For example, I didn't follow the extension to human-like driving behavior. The paper discusses using a baseline model ITRA, but it doesn't go into enough detail of how this model was merged/improved with the Critic SMC algorithm in the current paper.\n\nThe main idea of critic SMC, on the other hand, appears easy to reproduce.\n\nAs far as novelty this paper is a logical extension of prior work, as explained earlier. However it is still distinct from previous work.\n\nThe quality of the writing and experiments is quite good. In particular the state, action visualization in Figure 3 very clearly demonstrates the main contribution of this work.\n",
            "summary_of_the_review": "Slightly novel contribution in the area of planning as inference using deep learning with some parts unclear, but in general a reasonable contribution.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_ViDn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1133/Reviewer_ViDn"
        ]
    }
]