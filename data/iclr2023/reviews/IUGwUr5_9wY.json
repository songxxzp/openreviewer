[
    {
        "id": "UlgMxE4GLea",
        "original": null,
        "number": 1,
        "cdate": 1666303566903,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666303566903,
        "tmdate": 1666331275017,
        "tddate": null,
        "forum": "IUGwUr5_9wY",
        "replyto": "IUGwUr5_9wY",
        "invitation": "ICLR.cc/2023/Conference/Paper2527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper at hand proposes a new inductive bias (in the form of a proxy reward) for MI-based unsupervised skill discovery. The reward is designed so that exploration is improved, in particular in environments with bottleneck states or non-trivial dynamics. This is turn motivated by the observation that MI-based methods generally exhibit poor exploration capabilities as it is not strictly required to satisfy the main training criterion (the ability to discriminate skills by they states they visit).",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written with a clear presentation and motivation\n- The exploration problem is quite pressing in unsupervised skill discovery (USD), and failure modes of previous works are nicely summarized in Figure 1.\n- Although the approach is primarily motivated in navigation environments, it also achieves comparably good scores on DMC-100k.\n\nWeaknesses:\n- UPSIDE would be a very relevant baseline here since its main motivation (improving exploration by extending skills) is very related. I'm not 100% sure whether the environments are matching, but what about adding the results from Table 2 in Kamienny et al. (2021) to your Table 1? UPSIDE achieves a coverage of 85.67 in the bottleneck maze, which (assuming the environments are the same), almost matches your result.\n- Another potentially USD baseline would be the training of a goal-based policy, e.g., with hindsight experience replay. This should be particularly effective in navigation environments, assuming the extents of the maze are known.\n- The goal skill selection seems to be mainly based on heuristics, assuming that (1) skill termination states can be reliably reached and (2) random walks. It would good to (briefly) discuss possible failure modes here, e.g., in large state spaces (I assume that in the Ant mazes, you consider X/Y only for the random walk end states?), for stochastic environments, or if dynamics differ more significantly across the state region as in some Atari games.\n- In a similar vein, the proposed algorithm introduces several hyper-parameters. Apart from an ablation of major components (which is useful), the sensitivity to, e.g., the accuracy threshold for which skills should be guided is not discussed.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is generally easy to follow and clearly written, but I have a few questions still:\n- A minor clarification concerns the purpose of underscoring in results tables\n- Finally, it would be good to provide videos of the skill policies in the continuous-control tasks considered\n\nQuality: Besides further baselines as mentioned above, and additional discussion of limitations of specific design choices and hyper-parameters, there are few minor points wrt experiments:\n- The time horizon for the Ant maze environments is pretty short (200 steps). Would your method produce skills that cover the whole maze (Figure 14 b and c) with a sufficiently long time horizon? \n- Likewise on Ant Maze, for downstream learning you provide a dense reward that takes walls into account. What's the performance of the random-init policy here?\n\nNovelty: In terms of overall motivation, there seems to be a strong relation to UPSIDE (identifying skills that provide good final states for further exploring the environment) which is not discussed in detail. Why would your approach be preferred?\n\nReproducibility: The authors use publicly available benchmarks, and it seems like Algorithm 1 can be tucked on top of existing DIAYN implementations. A few sentences on `find_guide_skill` would be appreciated; in 3.1., you mention random walks from skill termination states but there are a few possible ways this could be done concretely. In 4.2.2, the authors note that they follow the fine-tuning procedure in Liu & Abbeel (2021), which could be explained in a few words in the Appendix (at least, I couldn't find the details I was looking for with a cursory glance at that reference).",
            "summary_of_the_review": "Overall, my main concern with the paper is the supposedly minor amount of novelty with respect to UPSIDE, which it is not considered as a baseline and the pros/cons of both approaches are not discussed. I would be happy to see the authors addressing this concern.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_RjRU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_RjRU"
        ]
    },
    {
        "id": "bcPZc1Fy1VS",
        "original": null,
        "number": 2,
        "cdate": 1666328740253,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666328740253,
        "tmdate": 1668832720565,
        "tddate": null,
        "forum": "IUGwUr5_9wY",
        "replyto": "IUGwUr5_9wY",
        "invitation": "ICLR.cc/2023/Conference/Paper2527/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggests an unsupervised skill discovery method named DISCO-DANCE. The authors focus on the issue that the commonly used MI reward ($I(S; Z)$) does not encourage exploration, and propose a novel approach to resolve this problem. Their method (DISCO-DANCE) first finds a \"guide\" skill that likely leads to unexplored states and then learns several \"apprentice\" skills that explore near the terminal state of the guide skill. They evaluate DISCO-DANCE on 2D maze and continuous control environments, showing that their proposed method outperforms previous skill discovery methods, such as DIAYN, SMM, APS, and DISDAIN.",
            "strength_and_weaknesses": "### Strengths\n- The authors tackle an important problem of skill discovery with a sensible approach.\n- The paper is clearly written and the figures are helpful for understanding the concept of the method.\n\n### Weaknesses\n- There is a missing baseline, UPSIDE. Since UPSIDE uses a very similar strategy (they also have a \"directed\" part and \"random-walk\" part as in this work) and shows significantly improved coverage in the same 2D maze domain, I believe the authors should include comparisons with UPSIDE and report the results.\n- The proposed method seems to leverage a fairly strong assumption that the environment is resettable to any arbitrary state (for performing multiple random walks from the terminal state of each skill). Is there any efficient way to avoid using this assumption, and how well does the method perform without it? For example, just running each skill $R$ times and then doing a single random walk after each of the rollouts could be one naive way to resolve this, but this strategy may cause significant inefficiency in terms of the number of environment steps. Also, how do the authors take this (the number of samples required for random walks) into account when computing the number of total training steps (e.g., 5M steps for hard mazes)?\n- It seems that the method is tailored toward synthetic 2D maze environments. What kind of skills does DISCO-DANCE learn in the DMC HalfCheetah and Quadruped environments? Videos and figures of learned skills in such environments would be very helpful to further understand the method.\n- The result in Table 2 is only computed from three random seeds. Given its marginal performance improvement and the overlapping confidence intervals, it would be difficult to say DISCO-DANCE shows superior performance to the baselines in this environment.\n\n### Additional questions\n- Why is the performance of DISDAIN in Table 4 significantly worse than DIAYN in HalfCheetah? According to the DISDAIN paper, DISDAIN with $N = 1$ and $\\lambda = 0$ (in their notations) exactly recovers the standard DIAYN, so I believe its performance should be at least equal to DIAYN's.\n- How do the authors decide to add new skills versus reuse existing ones?\n- How do the authors fine-tune their agents (and other baselines) on the DMC benchmark?\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is easy to understand and clearly written.\n\n### Quality\n- Appendix B.1: taks -> tasks\n\n### Novelty\nThe proposed method is novel to my knowledge.\n\n### Reproducibility\nThe authors have not released the code yet (but claimed to release it when the discussion period begins).",
            "summary_of_the_review": "While the method tackles a relevant problem of skill discovery and shows improved performance, due to insufficient evaluations and comparisons, I am not able to recommend acceptance.\n\n(11/18 Update): While I appreciate the response as well as the effort to make comparisons with UPSIDE, I believe a fair comparison between DISCO-DANCE and its closest baseline (UPSIDE) is necessary for assessing this work. I acknowledge that it could be difficult to reproduce the baseline's result without its code, but I feel it is a bit premature to conclude that UPSIDE is not reproducible within this short period of the discussion phase. Therefore, I cannot recommend acceptance in its current form.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_DqoP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_DqoP"
        ]
    },
    {
        "id": "3wyC7ULmSt",
        "original": null,
        "number": 3,
        "cdate": 1666683838909,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683838909,
        "tmdate": 1666683838909,
        "tddate": null,
        "forum": "IUGwUr5_9wY",
        "replyto": "IUGwUr5_9wY",
        "invitation": "ICLR.cc/2023/Conference/Paper2527/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Disco-Dance is a method for improving skill learning by expanding the diversity of the states that skills can reach.  The method first selects guide skills by finding skills most likely to lead to new states, then trains new skills (or old skills that have low discriminability) to reach novel states using mutual-information maximization.  ",
            "strength_and_weaknesses": "The paper addresses an important problem in skill learning, which is the diversity of states reached by current unsupervised rewards.  The Disco-Dance method is very well described, and experiments are clear as well.\n\nMy major concerns:\n\n- Similarity to previous methods is difficult to assess.  Disco-Dance seems very similar to both Go-Explore and Direct-then-Diffuse (UPSIDE) both of which are cited in the current paper.  However, despite the citations, there is little discussion of the differences between Disco-Dance and these prior approaches, and disappointingly there is no experimental comparison to those previous approaches.  This makes the impact of the current paper hard to assess.\n\n- In high-dimensional state spaces, like in the ant-maze experiments, how is the distance measured?  Does the approach require knowledge of which subspace represents the x-y plane?  If not, I'd imagine the number of skills required to get the coverage numbers reported in table 2 would be very large.  If so, does that not defeat the purpose of having high-dimensional state spaces, at least for the concerns of the current approach?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, the visualizations are informative and helpful.  The novelty of the Disco-dance is hard to assess given similarities to prior work mentioned above and little direct comparison with those approaches.  ",
            "summary_of_the_review": "An interesting method for dealing with a difficult problem.  My concerns mostly involve similarity to previous work without comparison to those approaches.  If the differences can be clarified, both through explanation and experiment, I would be willing to raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_xuUS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2527/Reviewer_xuUS"
        ]
    }
]