[
    {
        "id": "lQmlrqgX3oO",
        "original": null,
        "number": 1,
        "cdate": 1666659980689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659980689,
        "tmdate": 1666659980689,
        "tddate": null,
        "forum": "OK6LV2q50l",
        "replyto": "OK6LV2q50l",
        "invitation": "ICLR.cc/2023/Conference/Paper1075/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a personalized sparsification strategy for federated learning that compresses the updates in both directions, i.e., from clients to server and from server to clients. The proposed method, FedPSE, achieves that by (1) sparsification of model updates using Top-k algorithm before communication from clients to server, (2) element-wise aggregation at the server, and (3) sparsifying the global model updates in a personalized way before communication from the server to the clients. The experimental results show that FedPSE outperforms FedAVG consistently, and FedSTC and FedSCR occasionally on MNIST and FMNIST datasets. ",
            "strength_and_weaknesses": "Strengths:\n- The paper cares about bidirectional communication cost, which is nice as server-to-client communication cost is mostly neglected in the literature. \n- The authors provide theoretical convergence guarantees. \n\nWeaknesses:\n- I am confused about the Sign(.) operation in Algorithms 2 and 3. What is the point of taking the sign of a nonnegative matrix $|\\Delta \\hat{W}^r_i|$?\n\n- The upstream sparsification and element-wise aggregation are part of many existing frameworks. The only novel block seems to be the downstream personalized sparsification. \n\n- In Section 2.3, it says FedSTC and FedSCR share the uniform model. What does that mean? What I understand from a uniform model is a model with uniformly distributed parameters. But I think the authors meant that each client has the same model. I think changing the terminology here would avoid some potential confusion about the prior work. \n\n- Since each client have a different dataset distribution and a task, wouldn't it make more sense to choose different sparsity ratios for each client? By forcing the same sparsity ratio on all clients, FedPSE performs worse for clients whose datasets require denser models. So I think this approach brings a fairness problem. Do the authors have a solution for this?\n\n- In Section 5.1, the authors said they used the same learning rate for all compression methods. The learning rate and most other hyperparameters should be tuned for each baseline for the best performance. Otherwise, the comparisons are not valid and they would favor the method the authors tuned the learning rate for. \n\n- It would make reading easier if the references for FedAVG, FedSTC, and FedSCR were included in the Table captions, or at least somewhere in the experiments section. \n\n- What is the strategy to communicate the sparse models between the server and the clients? Once the model update is sparsified, it should be encoded in a way that the sparsity helps reduce the bitrate. What is the proposed strategy there?  \n\n- It seems from Table 1 that FedPSE is losing the performance gain for $\\lambda < 1$? While it outperforms the baselines consistently for $\\lambda=1$, it either underperforms or performs almost the same as the baselines for $\\lambda < 1$. Does this imply that FedPSE does not provide much advantage for not purely heterogeneous datasets? A comment on this in the experiments section would be nice.   ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, with a few confusing parts that I mentioned above. The hyperparameters are not provided anywhere (including in the appendix), and the authors didn't share the code for reproducibility. ",
            "summary_of_the_review": "Please see my comments above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I don't think the paper needs an ethics review, but I expressed my fairness concern above. I am happy to revisit it if the authors address my concern. ",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_7ra3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_7ra3"
        ]
    },
    {
        "id": "FVYhIz-IMcK",
        "original": null,
        "number": 2,
        "cdate": 1667098368031,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667098368031,
        "tmdate": 1667098368031,
        "tddate": null,
        "forum": "OK6LV2q50l",
        "replyto": "OK6LV2q50l",
        "invitation": "ICLR.cc/2023/Conference/Paper1075/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on enhance the performance of non-iid FL on the basis of reducing the communication cost.\nThe author proposes a method that uses the personalized Top-K sparsification and element-wise average techniques.\nThe main contribution of the paper is that it proposes a downstream selection mechanism to personalize the clients' models, which adapts to various distributions and can increase the performance in the non-iid setting.",
            "strength_and_weaknesses": "1. This paper is not the first one to utilize element-wise aggregation in FL to enhance the performance in the non-iid setting. For example, pFedLA (Layer-wised Model Aggregation for Personalized Federated Learning), which was published in CVPR 2022, also utilizes element-wise aggregation in FL. So the comparison of the proposed method and pFedLA is required to be included in the paper's analysis as well as experiments.\n\n2. The details of the experiments are missed, such as learning rate, random seed, batch size.\n\n3. The experiments have not show the advantage of the communication efficiency. It only compare with the algorithms that also have improve the communication efficiency. However, it's necessary to compare with the SOTA personalized federated learning approach that didn't consider the communication efficiency, to show the necessity of communication efficiency. For example, if your approach performs much worse than those general personalized FL, then the FL operators may insist to use general SOTA personalized FL. Besides, it's necessary to do a experiment to compare with a method that skipping the communication efficiency enhanced strategy of your proposed algorithm to show the contribution to reducing the communication cost; on the other hand, it's necessary to compare with that skipping the \nelement-wise aggregation to show it's useful to set up such a component.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is lack of a full screen that contain details of the whole method. It would be better to include pseudocode.\n\nQuality: The motivation of the paper is good. \n\nReproducibility: It seems not straightforward to reproduce the proposed algorithm based on the materials provided so far.",
            "summary_of_the_review": "In short, the motivation of the paper is good to me. The author(s) may consider doing more experiments as mentioned above to show the strengths of the proposed algorithm. Besides, it would be more convincing if pseudocode/code is provided.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_DP2J"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_DP2J"
        ]
    },
    {
        "id": "QDGXWWj-Qb",
        "original": null,
        "number": 3,
        "cdate": 1667108875910,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667108875910,
        "tmdate": 1667108875910,
        "tddate": null,
        "forum": "OK6LV2q50l",
        "replyto": "OK6LV2q50l",
        "invitation": "ICLR.cc/2023/Conference/Paper1075/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new communication compression method for federated learning with heterogeneous data. In particular, the compression scheme is personalized to each client. The authors show that several new techniques, such as downstream compression personalization, and element-wise aggregation, are critical to achieve a good performance in the presence of non-IID data.",
            "strength_and_weaknesses": "Strength\n1. The proposed element-wise aggregation method seems to be very promising. It can significantly increase the performance of compression algorithms in FL.\n2. Combining all new techniques, the proposed algorithm can achieve significant improves over existing algorithms, especially on CIFAR-10 dataset.\n\n\nWeakness\n1. Weak motivation: The first key problem I found is that the problem this paper aims to address may not even exist. In the introduction, the authors claimed that \"the current compression techniques, ignoring the personalization of clients, face a significant performance degradation on Non-IID datasets\" However, if we look at table 1, previous FedSTC and FedSCR algorithms seems to be even more robust to the data heterogeneity than FedAvg. In many cases, the performance of FedSCR and FedSTC even increases along with the increase of data heterogeneity. These experimental observations make the motivation of this paper invalid. The authors may need significant efforts to change the wording in the whole paper.\n2. Misleading name: The name of \"element-wise aggregation\" is confusing. Because in FedAvg and many other basic FL algorithms, we also conduct element-wise averaging over clients. What the authors really does here is element-wise normalization. I suggest the authors to change the method name.\n3. Compared with previous communication compression works, I think one of the key differences in this paper is downstream personalization. Clients will download different parts of the global model to begin the next round. So a key question here is that how effective is this technique? The authors did not provide ablation study on this. It is possible that this technique is unnecessarily complex. For example, how about transferring the same compressed global model to all clients?\n4. The theoretical analysis is a bit weak. As there is no local updates at clients, which is the basic component in FL algorithms. Also, it is unclear what the exact algorithm the authors analyzed. They remove the local updates part. So are there any components removed from the algorithm in order to conduct the analysis? It would be nice if the authors can write down the exact update rule of the proposed algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written. But I do think the motivation of this paper is not clear and needs to be revised, as mentioned in the weakness section.",
            "summary_of_the_review": "Currently I am leaning towards a reject. The main reasons are (1) the motivation of this paper is in conflict with their own experimental observations. So it requires some rewriting; (2) The effectiveness of DPS is unclear. More ablation studies are needed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_E2iq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1075/Reviewer_E2iq"
        ]
    }
]