[
    {
        "id": "koFFJU5nwv7",
        "original": null,
        "number": 1,
        "cdate": 1666365382146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365382146,
        "tmdate": 1666365382146,
        "tddate": null,
        "forum": "Vf6WcUDnY7c",
        "replyto": "Vf6WcUDnY7c",
        "invitation": "ICLR.cc/2023/Conference/Paper4141/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel CL method based on Supervised Principal Component Analysis (SPCA). The main contributions can be fo summarized as:\n\n(1) Proposing a simple continual learning algorithm, which is based on supervised principal component analysis.\n\n(2) Providing a a theoretical analysis (exact classification error rather than bounds) using high dimensional statistics.\n\n(3) Developing a develop a label optimization scheme.\n\n(4) Several applications are presented",
            "strength_and_weaknesses": "\nStrengths:\n\n(a) This paper provides extensive theoretical analysis.\n\n(b) The proposed method seems to be effective in the provided experiments.\n\nWeaknesses:\nI am not very good at supervised principal component analysis. Therefore, I talk about my feelings of this work.\n\n(a) A more detailed description of supervised principal component analysis (SPCA) is necessary. What is SPCA? How is SPCA connected with CL? \n\n(b) The description of proposed method is confusing. In the main file, no description of proposed method can be found, while they are placed in appendix. Clear description of proposed method is necessary.\n\n(c) What does assumption 2 mean? What can we infer from it? ",
            "clarity,_quality,_novelty_and_reproducibility": "Poor clarity.\nGood quality.\nNice originality of the work.",
            "summary_of_the_review": "I am not very good at supervised principal component analysis. I recommend marginally above the acceptance threshold. I will change my score according to other reviewer's comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_CZ28"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_CZ28"
        ]
    },
    {
        "id": "CQZQWH0wCd",
        "original": null,
        "number": 2,
        "cdate": 1666497260420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666497260420,
        "tmdate": 1668909340095,
        "tddate": null,
        "forum": "Vf6WcUDnY7c",
        "replyto": "Vf6WcUDnY7c",
        "invitation": "ICLR.cc/2023/Conference/Paper4141/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper propose a theoretical analysis of a SPCA-based continual learning algorithm using high dimensional statistics. Based on the theoretical analysis, they propose a label optimization process. Empirical evaluations highlight the effectiveness of the method.",
            "strength_and_weaknesses": "The theoretical result is sound and significant. While there exist some concerns as follows:\n- Some mathematical notations seems inconsistent across the paper, making it hard to understand. E.g., the subscript order of c = [n11/n, \u2026, n2t/n] in the appendix does not seem to be consistent with the previous description. Ds and Dc is not explained for readers.\n- The formula of Q(z) is wrong. In the cumulative distribution function of standard normal distribution, The exponent of e should be -t^2/2. Whether this error will have an impact on subsequent theoretical proofs and experimental results.\n- For the conclusion of Theorem 1, when the number of tasks t increases indefinitely, can the comparison of the information that SPCA-based methods and other continuous learning methods need to preserve be illustrated?\n- Regarding the weight setting seems to be arbitrary, it is recommended to explain the rules for weight setting.\n- The analysis of the experimental results is weak, e.g, in Table 1, why in DIL setting, OSCL substantially underperforms compared methods. In addition, since it is theoretically proven that the OSCL method avoids catastrophic forgetting, to illustrate this, it is recommended to include the average forgetting measure when presenting the results.\n- Whether this method can be combined with a neural network?\n- How to update the statistics in the online task environment\uff1fIn my understanding, the dimension is increase when the number of tasks increase. How to use previous statistics to obtain current statistics?",
            "clarity,_quality,_novelty_and_reproducibility": "The  theoretical result is promising, while many details need to clarify, and the writting needs to improve. ",
            "summary_of_the_review": "Overall, this paper is interesting, since it provide a novel theoretical analysis of catastrophic forgetting. However, some important points (see above) need to clarify. The current version can not meet the requriement for publishing in ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_w222"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_w222"
        ]
    },
    {
        "id": "r-5xtIXGcfN",
        "original": null,
        "number": 3,
        "cdate": 1666580259474,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666580259474,
        "tmdate": 1668734596087,
        "tddate": null,
        "forum": "Vf6WcUDnY7c",
        "replyto": "Vf6WcUDnY7c",
        "invitation": "ICLR.cc/2023/Conference/Paper4141/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose to use supervised PCA to tackle catastrophic forgetting in continual learning. They provide a theoretical analysis of the proposed method. They develop a label optimization scheme. They validate the method on four datasets and three scenarios (task incremental learning, domain incremental learning and class incremental learning).",
            "strength_and_weaknesses": "Strength: \n1) using PCA and SPCA (or other dimensionality reduction methods) in continual learning can be beneficial, though it is not well motivated in this paper.  \n2) experimental results suggest performance improvement. \n\nWeakness:\n1) It is unclear why SPCA can prevent catastrophic forgetting in both the motivation and theoretical analysis.  \n2) There is a view (e.g. see https://arxiv.org/pdf/2011.05309.pdf) that SPCA can be simply replaced with a task loss plus the traditional PCA loss producing even superior result than Barshan\u2019s method. Task loss is readily given in continual learning. Thus this may suggest a traditional PCA loss added to continual learning may do a better job that the proposed method. \n3) Gradients Orthogonal Projection method is relevant to the proposed method, but is not analysis/compared sufficiently nor empirically.  \n4) I don't understand why labels need to be optimised. Labels are given by the tasks/datasets. The distribution of the labels are, in general, inaccessible, and the dataset is a sample of the distribution. \n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper lacks clarity, which cast a shadow on quality and novelty. \n\nIt would be better if the authors can explain how and why the theorem 1 and 2 can support the proposed method can prevent catastrophic forgetting. ",
            "summary_of_the_review": "I don't think the paper is ready to publish yet. \n\nPost-rebuttal:\nThe original form of the paper was unclear and confusing. The authors answered all my questions well and these issues can be addressed in the camera-ready version. Hence I have increased my score.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_6kJ9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_6kJ9"
        ]
    },
    {
        "id": "uJLrGmsHagB",
        "original": null,
        "number": 4,
        "cdate": 1667572499894,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667572499894,
        "tmdate": 1667572499894,
        "tddate": null,
        "forum": "Vf6WcUDnY7c",
        "replyto": "Vf6WcUDnY7c",
        "invitation": "ICLR.cc/2023/Conference/Paper4141/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a theoretical analysis using SPCA, which predicts the performance of algorithm in advance. In addition, it propose a label optimization scheme to prevent the catastrophic forgetting with theoretical support (under the strong assumption of X are independent Gaussian random vectors with identity covariance).",
            "strength_and_weaknesses": "**Strengths**\n- S1: (Even with strong assumptions) The theoretical analysis and a novel label optimization formulation seems novel.\n\n**Weaknesses**\n- W1: **Empirical validation is weak** -- Used benchmarks are all MNIST variants, thus in a small scale and relatively simple. Since the method is based on the strong assumption (See my summary), the empirical validation is quite weak.\n- W2: **No discussion on the empirical comparison** -- Why the proposed method does not outperform the GEM in RMNIST.\n- W3: **Presentation is somewhat unclear\" -- Missing citation for the compared methods. Though the method of EWC, GEM, iCaRL and etc are quite well known, they should be properly referred either in the text or in the table.\n- W4: **Recent replay-based methods (published in 2021-2022 -- see the list below) are largely ignored in review** - Some of them could be compared with but not all as they are evaluated in different set-ups.\n  - Wang et al., \"Continual Learning with Lifelong Vision Transformer\", CVPR 2022\n  - Hersche et al., \"Constrained Few-shot Class-incremental Learning\", CVPR 2022\n  - Wan et al., \"Continual Learning for Visual Search with Backward Consistent Feature Embedding\", CVPR 2022\n  - Tang et al., \"Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data\", CVPR 2022\n  - Bang et al., \"Rainbow memory: Continual learning with a memory of diverse samples\", CVPR 2021",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is clearly written and the proposal is in high quality (though there is some concern in novelty). There are some suggestion to improve the clarity of the presentation (see my comments in weaknesses).",
            "summary_of_the_review": "Though the analysis and the proposed method is simple and limited in applicability, it presents an exact analysis on catastrophic forgetting and a novel label optimization formulation (though it's simple). The reviewer believe that this contribution is not trivial and meaningful to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_62oS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4141/Reviewer_62oS"
        ]
    }
]