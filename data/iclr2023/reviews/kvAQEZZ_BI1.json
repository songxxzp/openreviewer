[
    {
        "id": "hnI-CtXEs5",
        "original": null,
        "number": 1,
        "cdate": 1666516733669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666516733669,
        "tmdate": 1669135048094,
        "tddate": null,
        "forum": "kvAQEZZ_BI1",
        "replyto": "kvAQEZZ_BI1",
        "invitation": "ICLR.cc/2023/Conference/Paper3891/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for learning from data following a mixture of distributions whose input-output relationships are different. The proposed method maintains multiple hypothesis functions accounting for those different distributions and minimizes the loss between the label and the closest output among those given by the hypothesis functions. The authors explain a connection between the proposed method and the EM algorithm. They also show an identifiability result and a generalization error bound between the proposed training objective and an oracle risk. The experiments show the superiority of the proposed method against other methods that are not designed for this setup.",
            "strength_and_weaknesses": "# Strengths\n- The paper seems to tackle an important and interesting problem.\n- The method performs excellently according to the experiments.\n\n# Weaknesses\n- The problem setup is not very clear. In particular, the evaluation metric, or the test error, is not clearly defined, and the paper does not explain what information is given to the model in prediction.\n- It does not seem easy to choose $K$. The authors mention some connection to $R(D)$, but how do we estimate this number?\n- In Eq. (3), the method allows $\\hat{g}$ to look at $y^{ij}$, which feels strange to me because we don't have access to it in the test phase.\n- There is no guarantee that the proposed training minimizes the \n- Knowing each set of $m$ samples comes from one distribution seems to be a strong assumption. The theory requires $m$ tends to infinity to ensure the convergence of $\\hat{\\operatorname{err}}(H)$ to $\\operatorname{err}(H)$. The paper briefly mentions this point, but the argument is not very convincing to me.",
            "clarity,_quality,_novelty_and_reproducibility": "I think the writing should be improved. Even the problem setup is not totally clear to me. According to the PAC learnability analysis in the appendix, the authors consider something similar to the top-k error, but there is no discussion about related work on the top-k error.",
            "summary_of_the_review": "I like the direction of the work, but I think the quality of the paper should be improved before being published. There are many major issues as I mentioned in the weakness section. I suggest rejecting the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_4gxX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_4gxX"
        ]
    },
    {
        "id": "vTCsCFZFNM8",
        "original": null,
        "number": 2,
        "cdate": 1666635610138,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635610138,
        "tmdate": 1666635610138,
        "tddate": null,
        "forum": "kvAQEZZ_BI1",
        "replyto": "kvAQEZZ_BI1",
        "invitation": "ICLR.cc/2023/Conference/Paper3891/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Authros proposed the method that can deal with conflicting contexts scenario. Authors proposed a LEAF architecture for solving the problem. Authors also showed the effectiveness of the proposed method using toy and real-world datasets. Also, they offer the theoretical bounds for their method.",
            "strength_and_weaknesses": "- I think the problem of conflicing contexts seems novel and important for solving the real-world problems.\n- The examples for context-conflicting cases are well exampled.\n- The LEAF method looks sound and novel enough.\n- Experimental results are impressive, showing superior performance to several competitive baselines (ie. meta-learning, MoE, etc)\n- Theoretical bounds also look sound.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clearly described in the draft.",
            "summary_of_the_review": "I think the draft contains the solid method for solving unique aspect of the supervised learning task. The problem looks important and the proposed method looks also sound. Experimental and theoretical analyses are also thorough. I recommend accept for this draft.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_eW7y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_eW7y"
        ]
    },
    {
        "id": "NL0D6ZGc9n",
        "original": null,
        "number": 3,
        "cdate": 1666677895130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677895130,
        "tmdate": 1666677895130,
        "tddate": null,
        "forum": "kvAQEZZ_BI1",
        "replyto": "kvAQEZZ_BI1",
        "invitation": "ICLR.cc/2023/Conference/Paper3891/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper first formally defines a new and practically important problem setting where data are generated from multiple domains and the generation process is completely hidden. It then propose a concrete EM-based algorithm with reasonable theoretical guarantees. Empirical evaluations are conducted on various tasks and high quality python code is provided.\n",
            "strength_and_weaknesses": "Strength\n- This paper considers the new problem setting of learning from hiddenly conflicting data, and proposes a reasonable rigorous definition based on domain generalization.\n- It then proposes a pratical EM-based algorithm for the new problem, and analyzed its learnability and generalization ability compared with existing general supervised learning case.\n- Empirical evaluation is carefully conducted and experiment python files are provided with detailed README.\n\nWeakness\n- As the proposed algorithm is based EM, it is nice to have dicussions on its convergence conditions.\n- Hidden conflicts defined in this paper resembles graphical models with a latent variable, although graphical models cast stronger assumptions on the data generation process. Some dicussion on this topic is desired.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is of high quality, overall rigorously written and easy to follow.\n- The problem setting has reasonably high novelty and pratical importance.\n- Reproducibility is guaranteed by the detailed description and provided sample python code.\n",
            "summary_of_the_review": "I recommend to accept the paper considerting the overall contribution of problem setting, algorithm, theoretical analysis and empirical evaluation.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_yevj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_yevj"
        ]
    },
    {
        "id": "3_46Me3Iikb",
        "original": null,
        "number": 4,
        "cdate": 1666823119231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666823119231,
        "tmdate": 1669662902117,
        "tddate": null,
        "forum": "kvAQEZZ_BI1",
        "replyto": "kvAQEZZ_BI1",
        "invitation": "ICLR.cc/2023/Conference/Paper3891/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers supervised learning in the setting where the data is drawn in a 2 stage process: first, a domain is selected, second, a batch of data is drawn from this domain. The observations include the batch index, but not the underlying domain the data was collected from. The learning problem considered here is to find 1. a predictor that is optional for each domain, and 2. a rule for assigning a batch to a domain. The paper proposes a straightforward method for doing this.",
            "strength_and_weaknesses": "For context, it may help to consider two closely related cases:\n1. (latent variable modeling) We know that each data point is drawn according a hierarchical scheme that first draws a distribution, then draws the data point from this distribution. In this case, the obvious thing to do would be to set up learning via a latent variable model. This corresponds to the case where batch_size=1 in the present paper.\n2. (unsupervised adaptation) We observe batches drawn from a number of domains, but we don't know a priori the relationship between domains. In particular, we don't know that there are only K distinct kinds of possible relationship. Then, the natural approach is to build a model that, given a new set of inputs, self-modifies to improve predictions on these inputs; see https://arxiv.org/abs/2102.12206 for an example of this kind of approach, and https://arxiv.org/abs/2112.05090 for a benchmark of such methods. In the present paper, this is done by guessing which of the K classifiers should be used for the test batch.\n\nWith this context, the main strength of the paper is that the particular setting it considers---data is observed as batches, and we know each batch is from a fixed domain---seems to add in extra structural assumptions in a manner that is realistic for some applications, and which may allow for better methods or analysis. The particular approach they take here (use EM to alternate assigning batches to classifiers and updating the classifiers according to batch assignment) is straightforward and reasonable.\n\nHowever, in this view, the main weakness of the paper is that doesn't really focus on what's interesting about their data setup. Specifically, the natural baselines for comparison are 1 (latent variable modeling) and 2 (unsupervised domain adaptation). But it seems to me that the discussion, theory, and experiments all comparatively neglect these comparitor cases. That makes it both hard to assess the importance of the method, and hard to know what the important conceptual takeaways of the paper are. Generally, I think the paper needs to be substantially reworked to make all this clear.\n\nAs particular instances,\n1. How does knowing that some pairs share the same domain help with identification (relative to straight latent variable modelling)? \n2. How does the assumption that that there are only K domains help relative to general unsupervised domain adaptation? E.g., in terms of the PAC rate and in terms of empirical performance\n\nSome further issues I see:\n1. Theorems 1 and 2 seem trivial. \n\n2. It's actually not obvious that ERM should fail in this data setup. The optimal classifier is P(Y| x) = \\sum_k p(k | x) P(Y | k, x), where k denotes the underlying domain. If p(k|x) is learnable from the data, then a suitable ERM procedure should succeed. The claim here is presumably something like p(k|x) is not learnable given only single sample batches, but is learnable using multi sample batches. But, if that is the claim, I'm not able to find it spelled out clearly anywhere in the paper.\n\n3. In the experiments, it's not clear to me what \"ERM\" means. In particular, is the capacity of the ERM model the same as the LEAF model? A fair comparison would be to run the EM algorithm, but eliminate the information about which datapoints come from the same batch---is that what you're doing? If so, it wasn't clear to me\n ",
            "clarity,_quality,_novelty_and_reproducibility": "See above.\n\nA plus of the paper is that the writing is unusually clear. Though, I do think the focus of comparisons is not quite right to really clearly communicate the significance of the results. ",
            "summary_of_the_review": "Although I think the data setup is interesting and potentially fruitful, I don't think the results of the paper in its current form really communicate this. Minimally, the paper would require making it precisely clear how the batch size > 1 case helps vs simple latent variable modeling.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_zrsD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3891/Reviewer_zrsD"
        ]
    }
]