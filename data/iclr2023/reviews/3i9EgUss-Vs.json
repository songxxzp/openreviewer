[
    {
        "id": "Dd7aftFDts2",
        "original": null,
        "number": 1,
        "cdate": 1666076138522,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666076138522,
        "tmdate": 1666076138522,
        "tddate": null,
        "forum": "3i9EgUss-Vs",
        "replyto": "3i9EgUss-Vs",
        "invitation": "ICLR.cc/2023/Conference/Paper3322/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents GC-Flow, a generative framework to generate the representation of graphs. The framework is effective for node classification and unravels the inherent structure of data for clustering. More specifically, GC-Flow takes both the advantages of normalizing flows and graph convolutions. For prediction, it computes the class conditional likelihood p(x|y) and applies the Bayes rule together with the class prior p(y). Experimental results have shown the effectiveness of various benchmarks compared with the related baselines.",
            "strength_and_weaknesses": "Strengths:\n1. This work is quite interesting. The paper provides a new angle to understand the graph node representation from clustering by the nature of normalizing flows, which is rarely considered in GNNs. \n2. The proposed GC-Flow is simple and practical on various datasets. In addition, two variants are studied which show the flexibility of the GC-Flow framework. \n3. Extensive experiments show the effectiveness of the proposed method. Some of the experimental results are inspiring. The hyperparameter tuning in the appendix is clear and reasonable.\n4. The appendix contains solid derivations for Lemma 1, which is well organized and clear to understand.\n\nWeaknesses:\n1. In Eq.(8) and Eq.(9), the authors define the class-conditional likelihood $p(x_i|y_i=k)$ and marginal likelihood $p(x_i)$. However, the total loss function is not illustrated clearly. It is better to briefly describe a concrete formulation of the loss function for GC-Flow. \n2. There is no analysis of complexity. Scalability is important for flow-based generative models. As the paper described, GC-Flow is based on normalizing flows, and there is a need to calculate the Jacobian determinant. Considering the #train/val/test listed in Table 3, I am wondering if only a part of the samples for each data set were used for the empirical study. I suggest the authors add complexity analysis to improve this paper.\n3. Some important related works are missing. It is better for the authors to discuss some differences and connections between GC-Flow and the method [1] which also aims to model distributions of graph-structured data using normalizing flows. Besides, related works on node clustering [2] are missing.\n\n[1] Deng, Z., Nawhal, M., Meng, L., & Mori, G. (2019). Continuous graph flow. arXiv preprint arXiv:1908.02436.\n\n[2] Fettal, C., Labiod, L., & Nadif, M. (2022). Efficient Graph Convolution for Joint Node Representation Learning and Clustering. WSDM.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:  The paper is well-organized and clearly written.\n\nQuality: The paper is technically sound; the important derivations and detailed variants are provided in the appendix. \n\nNovelty: This work provides some interesting insights into the node representation of graphs from the view of clustering. The approach is novel. \n\nReproducibility: The empirical results seem reasonable, and I don\u2019t doubt the reproducibility.\n",
            "summary_of_the_review": "The paper is strong, and I recommend it for acceptance. The proposed method is simple and practical. It also provides interesting insights into normalizing flows and GNNs. However, there is still room for improvement. I kindly hope the authors consider the points highlighted in my reviews.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_qGKA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_qGKA"
        ]
    },
    {
        "id": "FB5XXdj8-P",
        "original": null,
        "number": 2,
        "cdate": 1666321781424,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666321781424,
        "tmdate": 1666329706004,
        "tddate": null,
        "forum": "3i9EgUss-Vs",
        "replyto": "3i9EgUss-Vs",
        "invitation": "ICLR.cc/2023/Conference/Paper3322/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focused on learning a structured node representation on top of a GNN model. It follows the idea of FlowGMM and applies normalizing flow to the GNN model. The proposed GC-Flow model leads to significantly better clustering results and improves node classification. ",
            "strength_and_weaknesses": "Strengths:\n1. The proposed GC-Flow model is an innovative generative model on the graph data, which considers the interdependency among instances (nodes) and incorporates it into the normalizing flow model. \n2. The improvement in the clustering results is very significant, and the visualization demonstrates that GC-Flow has a more interpretable latent representation.\n\nWeakness:\n1. The main drawback of this paper lies in that it fails to identify a significant problem in the current GNN models. I admit the method proposed is interesting, but I cannot see the significance of the problem it studies. If the main purpose of adding normalizing flow is to increase the clustering performance, why do not consider adding the community partition losses/clustering losses to the supervised losses? Adding normalizing flow is not that straightforward to realize this goal.\n\n   As pointed out by FlowGMM, it does not actually increase the performance too much but treats interpretability and broad applicability as its main advantage. I think it is unnecessary to introduce normalizing flow on GNN to improve the clustering performance only. \n\n2. The baselines are too weak and graph contrastive learning methods [1,2,3] are totally missing from the baselines, which are expected to have good performance in both clustering and node classification. Both [2] and [3] have the loss to explicitly encourage clustering. \n\n[1] Graph Contrastive Learning with Adaptive Augmentation. Zhu et al., WWW 2021.\n\n[2] Graph Communal Contrastive Learning. Li et al.,  WWW 2022.\n\n[3] X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning, Jing et al., CIKM 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and easy to follow, it has a clear step-by-step derivation of its normalizing flow model. The method it proposes is innovative in the graph domain. However, as I mentioned before, this paper lacks a clear definition of what kind of problem it wants to solve, if it only focuses on improving the clustering performance of GNN under semi-supervised learning, I do not think it is innovative enough.",
            "summary_of_the_review": "This paper has good writing and is well-organized. The model proposed is very interesting to the community and demonstrates strong power in improving the clustering performance of current GNN models. But generally, I feel the authors do not have a clear definition of what kind of significant problem they want to solve, or they do not explore the benefits of their model enough. The baselines in the experiment are also too weak. I vote for the rejection by this time unless the authors could clearly tackle the weakness I mentioned before.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_kEff"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_kEff"
        ]
    },
    {
        "id": "4l4BSEVTbW",
        "original": null,
        "number": 3,
        "cdate": 1666594322503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594322503,
        "tmdate": 1666594322503,
        "tddate": null,
        "forum": "3i9EgUss-Vs",
        "replyto": "3i9EgUss-Vs",
        "invitation": "ICLR.cc/2023/Conference/Paper3322/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed to use the combination graph convolution and normalizing flows to replace the traditional GNN layers. Comparing with the conventional normalizing flow that transform an original x into a latent z (usually Gaussian distributed), there are two main differences here. First, instead of only transforming the original data x, at each intermediate step of transformation, the model will first fuse the data from neighboring nodes and then transform the fused data using a standard NF transformation. Second, different from conventional NF that intend to eventually transform the raw data x into a Gaussian distributed representation, the proposed method transforms data into mixture Gaussian distribution, hoping to produce representations that are friendly for clustering. A set of experiments were conducted to show the proposed model could be simultaneously good at classification and clustering.",
            "strength_and_weaknesses": "Strength:\n1)\tThe paper is well organized.\n2)\tThe idea of combining graph convolution into normalizing flow and using Gaussian mixture distribution to encourage clustered representation learning is interesting, \n3)\tExperimental results demonstrate that the representations learned under the proposed method appear to be more clustered than the previous GNN models, in addition to maintaining classification performance.\n\n\nWeaknesses\n1)\tThe proposed method can be viewed as a direct combination of GCN and normalizing flow, with the ultimate transformed distribution, which is Gaussian in conventional NF, replaced by Gaussian mixture distribution, encouraging the latent representation to be more clustered. Technically, there is no enough new stuffs here.\n2)\tMore Seriously, to ensure the intractability of the normalizing flow after absorbing the graph neural network, the proposed model has to replace the basic operation \\sigma(AXW) with the operation \\sigma(AX) in the graph neural networks, abandon the feature affine transformation operation, i.e., XW, before passing the intermediate representations to neighboring nodes. Since the W is the main parameters to be learned in GNN, abandoning it means the representation ability of GNN is restricted significantly. The experimental results also show that the proposed model brings very little gains over the old models like GCN and GAT on classification tasks.\n3)\tWithout using the feature affine transformation AXW, then the dimension of intermediate hidden representations will always be kept the same as that of input feature since the NF have to maintain the dimension unchanged. Then, if the dimension of input feature is very high, in addition to the complexity issue, the learned feature will be also be very high, which may not be very useful as nowadays we often expect the learned features to be compact.\n4)\tFor the experiments, since the paper want to demonstrate the proposed model is able to learn clustering-friendly representations, we expect to directly see how the model performs on clustering task on the clustering performance metric, like accuracy ACC, normalized mutual information NMI etc, rather than the indirect Silhouette criteria, which is not meaningful at all.\n",
            "clarity,_quality,_novelty_and_reproducibility": "please see the strength and weakness.",
            "summary_of_the_review": "The proposed method looks like a direct combination of GNN and NF, making the technical contribution here limited. To guarantee the tractability of NF, the proposed model has to abandon the transformation of feature operation XW before passing to neighbors, making the modeling ability of GNN reduced significantly. The experiments cannot support the claims or objectives, neither.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_bfhQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_bfhQ"
        ]
    },
    {
        "id": "1Le2Yxql5H",
        "original": null,
        "number": 4,
        "cdate": 1666678766904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678766904,
        "tmdate": 1666678766904,
        "tddate": null,
        "forum": "3i9EgUss-Vs",
        "replyto": "3i9EgUss-Vs",
        "invitation": "ICLR.cc/2023/Conference/Paper3322/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method to replace a GNN layer by a combination of graph convolutions and normalizing flows under a Gaussian mixture representation space. The proposed method not only classifies well, but also yields node representations that capture the inherent structure of data, as a result forming high-quality clusters (i.e. cluster well).\n",
            "strength_and_weaknesses": "Strength: \nThe paper proposes a coherent story, etc\n\nWeakness:\n1 replace GNN layer with graph convolutions & normalizing flows will significantly add more computational burden? especially inverse of Jacobians? Will the normalizing flow harder to train due to  intermediate stage of graph functions' ill behaviour (due to graph convolution), leading to instability and divergence of the algorithm? \nHave the author consider other priors than gaussian mixture?\n\n\n2. The paper reports microF1 score in Table 1\n\nHow does the paper\u2019s performance compare with leaderboard here\nhttps://paperswithcode.com/sota/node-classification-on-pubmed\nfor example, pubmed acc is 91+ \n\nhow does performance compare with https://arxiv.org/pdf/2109.05641v1.pdf\nhttps://arxiv.org/pdf/2012.06113.pdf (Table 3)\nseems much better than Table 1 of the paper. \nCan the author clarify? \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The author provide a quite complete story",
            "summary_of_the_review": "I am not fully on board on the motivation of this and the results section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_cUhi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3322/Reviewer_cUhi"
        ]
    }
]