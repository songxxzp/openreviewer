[
    {
        "id": "G5PvTyhvEh",
        "original": null,
        "number": 1,
        "cdate": 1666427042270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666427042270,
        "tmdate": 1671081585684,
        "tddate": null,
        "forum": "Oz0npxjLAsI",
        "replyto": "Oz0npxjLAsI",
        "invitation": "ICLR.cc/2023/Conference/Paper3851/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a mega graph structure and introduces it to a GNN named MeGraph. MeGraph aims to obtain better node representations by enabling repeated information exchange across multi-scale graphs.  Experimental results are provided to demonstrate the effectiveness of MeGraph.",
            "strength_and_weaknesses": "Strengths:\n1. The idea of the paper is straightforward and easy to understand.\n2. Experimental evaluation is performed on various tasks and datasets. \n\nWeaknesses:\n1. The proposed framework is incremental, which limits the technical contribution.\nThe three main components, the graph pooling, the GN layer, and the encode-decode architecture, are all adopted from existing methods.\n\n2. The organization of the paper is poor and should be improved.\nGraph pooling is well-known and widely used in GNNs. There is no need to introduce it in great details. It is suggested to put more details of the proposed method in the main body.\n\n3. The expressiveness power of the proposed method needs further theoretical evaluations.\nThe paper aims to address the inability of a GNN node to observe nodes that are farther away than the number of layers n (so-called the under-reaching problem). This problem was first discussed in [1]. In this work, however, introducing large-scale graph representations to update node representations could aggravate over-squashing. Moreover, from the perspective of graph isomorphism, connecting nodes from different scales could harm the ability of GNN to distinguish non-isomorphic graphs. The authors should analyze how powerful the proposed framework is, in theory, to make sure it addresses the under-reaching problem.\n\n4. Experimental study should be enhanced.\n(1) The proposed method only compares with basic GNNs, such as GCN and GIN. More SOTA methods should be added as baselines. For instance, [2] and [3] also try to extend receptive fields by rewiring the input graph. \n(2) The selection of each component in MeGraph could significantly influence the performance. The discussion on the selection of each component, such as the pooling method and the GN block, is insufficient. \n(3) Since the GN block in the proposed method is realized as a GFuN layer, the authors should report the experimental results of GFuN to clarify the performance improvements that come from the proposed MeGraph.\n\n[1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021.\n[2] Topping J, Di Giovanni F, Chamberlain B P, et al. Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2022.\n[3] Br\u00fcel-Gabrielsson R, Yurochkin M, Solomon J. Rewiring with positional encodings for graph neural networks. arXiv preprint arXiv:2201.12674, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, but the organization of the paper could be improved. The idea of the paper is incremental. The proposed framework works better than basic GNNs but further comparisons are necessary. ",
            "summary_of_the_review": "Main concerns of the paper are: (1) The technical contribution of the paper is limited. (2) The expressiveness of the proposed model should be analysed theoretically. (3) Baseline models are weak. Comparisons with SOTA models are needed. (4) MeGraph intensely relies on the graph pooling and the GN layer, which are well-studied and have different options. The discussion on the selection of these components is insufficient.\n\nPost-rebuttal: I appreciate the authors' efforts in providing theoretic proof and additional experimental results, which have partially addressed my concerns. However, my concerns on the limited technical contribution and the lack of extensive experiments for the universality of the framework still remain. I have slightly increased the rating.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_M2DM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_M2DM"
        ]
    },
    {
        "id": "rKyviBG0h5k",
        "original": null,
        "number": 2,
        "cdate": 1666528085838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666528085838,
        "tmdate": 1666528085838,
        "tddate": null,
        "forum": "Oz0npxjLAsI",
        "replyto": "Oz0npxjLAsI",
        "invitation": "ICLR.cc/2023/Conference/Paper3851/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a novel architecture for graph neural networks called MeGraph.\n\nMeGraph builds on the Select-Reduce-Connect framework for graph pooling (Grattarola et al., 2022). \nIn particular, the Select function of a pooling method implicitly induces a bipartite \"inter-graph\" between the input and output graph. By joining all graphs in the pyramid with the corresponding inter-edges, we obtain a pyramidal \"mega-graph\".\n\nBy applying a specifically designed message-passing scheme (called Mee) to the mega-graphs, the authors allow for long-range communication between nodes and inter-layer communication across the pyramid.\nTo do so, the authors extend the SRC framework with an \"Expand\" function that enables backward communication from higher-level (coarse) layers to lower-level (fine) ones.  \n\nThe authors also propose a novel pooling technique that extends EdgePool by allowing for arbitrary coarsening ratios. \n\nFinally, the authors perform an in-depth experimental analysis on several relevant benchmarks, also introducing 5 new synthetic datasets to explicitly test long-range communication in GNNs. \n\nThe results show that the proposed method outperforms relevant baselines. ",
            "strength_and_weaknesses": "**Strengths**: \n- The paper presents numerous novel ideas related to graph neural networks and, more specifically, graph pooling. \n- The design of the proposed method is well-motivated and validated through an ablation study.\n- The performance improvements are significant and, in certain cases, MeGraph outperforms the baselines by a large margin. \n- The experimental section is very thorough with convincing results. \n\n**Weaknesses**: \n- What is the meaning of the following sentence?\n  > In the process stage, an elementary component is a layer shaped like a mirrored E\n- One of the motivations for the Mee layer is overcoming the need of having multiple GCN steps to enable communication between layers of the pyramid (the naive message-passing strategy). However, because each step in Mee diffuses information between adjacent layers of the pyramid, this issue is not really solved (it's just hidden \"inside\" the layer). Have I misunderstood something? Could a more efficient inter-layer communication be achieved via dense self-attention between all pyramid layers?\n- The Expand function is commonly referred to as \"lifting\" the graph. The authors should either justify why they renamed the function or change the name to \"Lift\" (or something along those lines). \n- While the experimental results are interesting, it is not clear if the baselines reported from other works are comparable to the proposed method (in terms of the number of parameters, number of layers, etc). Can the authors comment on this?\n- Since computational cost is an issue (as commented by the authors in Section 6), the authors should report a comparison of the actual computational times of their architecture compared to the baselines.\n- The color-coding of results in Tables 1, 2, and 3 is unclear since the differences between the results are small and the differences in color are barely perceptible. ",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: the paper is clear and well-written.\n- Quality: the work is of good quality, with minor issues that should be easily addressed by the authors. \n- Novelty: the work proposes several novel ideas and the references to previous work are complete. \n- Reproducibility: it should be possible to implement the main method and reproduce the results from what is described in the paper. The authors mentioned that the code would be provided separately but I could not find it (I suppose OpenReview does not allow posting comments until the reviews are released). I encourage the authors to provide the anonymized code before final recommendations are submitted. ",
            "summary_of_the_review": "The work presents several novel and interesting ideas to improve the performance of graph neural networks (especially concerning long-range communication). The proposed method makes smart use of graph pooling and achieves good results in many diverse experiments. \n\nI recommend acceptance, conditional on the authors addressing the minor concerns that I have raised above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_F7rn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_F7rn"
        ]
    },
    {
        "id": "iYZPuCvszz",
        "original": null,
        "number": 3,
        "cdate": 1666650230716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650230716,
        "tmdate": 1666650230716,
        "tddate": null,
        "forum": "Oz0npxjLAsI",
        "replyto": "Oz0npxjLAsI",
        "invitation": "ICLR.cc/2023/Conference/Paper3851/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a method to create multi-scale graphs using graph pooling. In the process, they construct a mega-graph. Their approach applies graph convolutions to intra-graph edges, while convolutions over inter-graph edges transfer information along the hierarchy. They extend graph convolutions and pooling. Their experimental results include 55 synthetic datasets and 26 real datasets. Their results outperform the chosen baseline algorithms. To share the code, they will share a link in an official comment on OpenReview. The article provides ample detail on the datasets, methods and architectures, implementation, and experimental results in the appendix.",
            "strength_and_weaknesses": "The paper proposes a novel approach to aggregate graph neural networks. State-of-the-art results on various synthetic and natural datasets, incorporating some novel benchmarks. The authors promise that the code is going to be available. Throughout the article, particularly in the appendix, the authors describe the different aspects of and around their proposal. \n\nThere are some minor editing details, which include:\n***\u201d However, if the architecture could inference from a larger scope, e.g., constructing multi-scale graphs in a hierarchy, the shortest path is easier to be\nestimated by aggregating and delivering information from multi-level scopes.\u201d Infer?\n*** convoluations\n*** SEELCT\n*** Is layer n in Figure 3 missing some symbols because they are white? I am talking about the nodes X_i^{n-???}. This problem also happens to the first column of nodes in Figure 4.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is reasonably well-written and provides considerable detail and a level of description for reproducibility. Despite that, the authors state that they will share their computer code.",
            "summary_of_the_review": "The paper presents a novel idea that is sustained on state-of-the-art results with respect to baseline alternatives on a range of synthetic and natural datasets. On top of a detailed description, the authors offer to share their computer code. \nI would invite the authors to edit the manuscript to improve their presentation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_nn8y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_nn8y"
        ]
    },
    {
        "id": "cI6Yddp-gT-",
        "original": null,
        "number": 4,
        "cdate": 1667081890420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667081890420,
        "tmdate": 1667081890420,
        "tddate": null,
        "forum": "Oz0npxjLAsI",
        "replyto": "Oz0npxjLAsI",
        "invitation": "ICLR.cc/2023/Conference/Paper3851/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies graph representation learning and propose MeGraph that could learn multi-scale graph representation. MeGraph enables information exchange across multi-scale graphs, which distinguishes MeGraph from many recent hierarchical graph neural networks.\nTo achieve this goal, MeGraph first uses graph pooling to create multi-scale graphs, then convolve the intra-graph edges and inter-graph edges separately, and finally adopts graph full network (GFuN) and stridden edge contraction pooling (S-EdgePool) for feature extraction. ",
            "strength_and_weaknesses": "Cons:\n\n1. The algorithm studied in this paper is quite complicated, it has many technical details and is very hard to digest, and requires massive ablation study. By looking at their ablation study results, I am not confident about which part of the proposed modules could really boost the performance. Although the authors provide massive experiment results in the appendix, I would suggest the authors summarize the main take-home messages in bullet points (e.g., by comparing results A with B we know using module C could roughly improve D percent performance.) Keep in mind to make the religious argument and avoid over-claiming the contributions when conducting an ablation study.\n\n2. The multi-scale graph learning issue in this paper could be very related to jumping knowledge [1] and its follow-up works. According to my understanding, the biggest difference is this paper allows information exchange across multi-scale graphs and a different type of graph convolution operator. I think authors might also need to compare with those types of works, instead of just comparing to vanilla GCN and GIN, to help readers better understand the advantages of this work.\n\n[1] Representation Learning on Graphs with Jumping Knowledge Networks  https://arxiv.org/abs/1806.03536\n\n3. The authors mention many related methods, e.g., Graph U-Net and various pooing methods. These methods are expected to compare with the experiment to understand how much MeGraph gains from these baselines. \n\n4. If possible, please also compare with OGB's node classification baseline methods on node classification datasets such as OGB-Arxiv, and OGB-Products. The algorithm developed for OGB node classification tasks is usually more advanced and more challenging.\n\n5. The impact and contributions of this work might not be enough for ICLR. \nAlthough some new techniques are introduced in this paper, these methods are not fundamental enough, are mostly at the ML engineering aspect, and could have very limited impact in practice.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation could be improved and the paper could be better organized. The current version of this manuscript is not very easy to follow. The proposed methods sound novel but it is a mostly incremental improvement over a combination of previous methods. Code is provided.",
            "summary_of_the_review": "This paper is an empirical paper working on graph representation learning. The algorithm studied in this paper is quite complicated, it has many technical details and is very hard to digest. By looking at their ablation study results, I am not convinced how MeGraph could improve the model performance. There are baselines and related methods that need to be compared with given this method is also quite complicated and related to many other existing strategies. The impact and contribution of this paper might not enough to reach the bar of ICLR.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_AA1h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3851/Reviewer_AA1h"
        ]
    }
]