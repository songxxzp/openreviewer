[
    {
        "id": "cEC7t9frAAf",
        "original": null,
        "number": 1,
        "cdate": 1666270666898,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666270666898,
        "tmdate": 1666591838286,
        "tddate": null,
        "forum": "_ruvo2KCL2x",
        "replyto": "_ruvo2KCL2x",
        "invitation": "ICLR.cc/2023/Conference/Paper942/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new approach for hyperparameter optimization (Deep Ranking Ensembles) in both the transfer and non-transfer learning setting. \n\nThe are 3 main components to this approach:\n1. Use neural networks that learn to rank configurations as a surrogate model in the Bayesian optimization setting. \n2. Create diverse ensembles of these networks in order to perform uncertainty quantification (Lakshminarayanan et al., 2017), which is required to use acquisition functions like expected improvement \n3. Enhance the surrogate models by using additional learned dataset meta-feature representations (Jomaa et al., 2021).\n\nAn illustrative example of the method is provided, followed by an extensive evaluation on the HPO-B benchmark.\n",
            "strength_and_weaknesses": "Strengths:\n- While not all components here are novel (e.g., the method re-uses deep ensembles for uncertainty quantification and the dataset meta-feature approach from existing works), the core idea of using learning to rank inside BO surrogates is novel to the best of my knowledge, and the combination of this with the existing ideas is innovative and worthy of publication. \n- The illustrative example provided really helps the reader to grasp the main concepts, which is much appreciated. \n- Extensive experiments are provided comparing with many baselines in both the transfer and non-transfer context, in addition to a variety of different ablation studies, and statistical hypothesis testing is provided in the appendix.\n\nWeaknesses:\n- In Section 4.3 it is stated that during the meta-test phase the scoring network is training for 1000 epochs. How sensitive is the algorithm to the number of epochs used? It seems rather a lot, given that in early phase of BO one might only have a handful of configurations that have been evaluated. Are mini-batches used during the online training phase? If so, what was the batch size? If not, how is diversity ensured when training the scoring networks (it is stated in Section 3.2 that diversity in ensured by sampling different mini-batches). \n- No mention is provided about the runtime of DRE compared to other baselines. I understand that in the limit when it is very expensive to evaluate a hyperparameter configuration, this may not matter very much. However. it would give a lot of insight into whether this scheme could be used to solve more general classes of black-box optimization problems in which evaluating a configuration may be less expensive than training an ensemble of neural networks for thousands of epochs. \n- It was not very clear to me what is the role of the dataset meta-features in the non-transfer setting. Based on last sentence at the end of Section 4.3, it seems that the meta-feature extractor is trained also for DRE-RI. Could the authors comment on how these features might be expected to help in this context?\n- I appreciate the authors including the somewhat negative result comparing to HEBO in non-transfer setting (even if it is clearly surpasses by DRE in the transfer setting). Can the authors provide any explanation or intuition why HEBO might be winning in the non-transfer setting when the number of trials gets large. \n\nMinor corrections:\nSection 4.3 - I believe the text says \"32 networks\" when it should say \"32 neurons\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, well-written and contains some novel ideas. Experiments are performed on publicly available benchmarks, and code is also provided for the method (although I have not tried to run the code myself). ",
            "summary_of_the_review": "This is a well-written paper with some interesting new ideas and impressive experimentation. Given the general applicability of the approach, I think the potential for impact is high. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper942/Reviewer_Eztd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper942/Reviewer_Eztd"
        ]
    },
    {
        "id": "ysb_86O8b-",
        "original": null,
        "number": 2,
        "cdate": 1666578514280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578514280,
        "tmdate": 1666578585481,
        "tddate": null,
        "forum": "_ruvo2KCL2x",
        "replyto": "_ruvo2KCL2x",
        "invitation": "ICLR.cc/2023/Conference/Paper942/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new idea on hyperparameter optimization (HPO) by formulating it as a weighted learning to rank problem. By using a novel loss function, coupled with existing work on neural ensembles and transfer learning, the proposed method is shown to achieve state-of-the-art results in HPO.",
            "strength_and_weaknesses": "Strength:\n- The idea presented is well motivated with clear mention to related work and background. \n- The method is proposed clearly.\n- The demonstration of the toy example is very intuitive. \n- Experiments are thorough and discussions on the 5 hypotheses are clear. \n\nWeaknesses: \nNot something significant but I have several questions for discussion:\n- The weight in loss function (1) seems to be from an older paper, and is not tuned (in experiment seems only compare with the non-weighted version)? Do we think there could be better weights available, especially for different tasks? \n- The rank of the configurations is a less well behaved surface to model compared to the original loss. Even if the underlying loss function is smooth, the rank could still be of large variability. Intuitively, how the method is able to perform better in this case? Consider a example where there are two local extremes, then modeling the original surface should be fine, but modeling the ranking can be harder due to the similarities of ranks near two local extremes? \n- The authors should discuss the efficiency of the algorithm compared to its competitors. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and with good quality. \n\nIn terms of novelty, although major components rely on different works (wight of loss function, uncertainty estimation, transfer learning), but it's a pretty novel idea to combine them all together and use it an a new method for HPO.",
            "summary_of_the_review": "Overall I believe this paper propose a new idea in the important area of HPO, which is proved through extensive benchmark datasets to yield state-of-the-art results. I think it's a good paper for ICLR.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper942/Reviewer_MGU4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper942/Reviewer_MGU4"
        ]
    },
    {
        "id": "OLQW9eMiDV",
        "original": null,
        "number": 3,
        "cdate": 1667100334658,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667100334658,
        "tmdate": 1668784017422,
        "tddate": null,
        "forum": "_ruvo2KCL2x",
        "replyto": "_ruvo2KCL2x",
        "invitation": "ICLR.cc/2023/Conference/Paper942/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes an HPO method that learns to select the best HP from the rankings of HPs. The main insight comes from the authors that the optimal strategy for training surrogates is to preserve the ranks of the performances of HPO as an L2R problem. More specifically, the method meta-learns neural network surrogates optimized for ranking the configurations' performance while modeling their uncertain via ensembling. The ranking surrogate will be used in the BO framework to help with HP suggestion.",
            "strength_and_weaknesses": "**Strengths**:\n\n1. To the best of my knowledge, the ranking-based HPO method is quite novel.\n2. A comprehensive evaluation is done to show the effectiveness of the proposed method.\n\n**Concerns and questions**:\n\n1. My main concern about this work is the impact of the ranking list size. I do not find the actual definition of $N^{(s)}$ in Alg.1, but it seems to be the size of the ranking list in the rank scorer. In my opinion, this ranking list size is a crucial factor in building the ranking surrogate. Presumably, a small-size list is not that informative (one extreme case is the size being 1), and a larger size shall be preferred. However, larger size also means that the HPO algorithm needs to evaluate a large batch of configurations at each iteration (according to my understanding, computing the true rank requires one to evaluate the actual performance of the configs in the list). Thus I believe it is important to investigate the impact of this factor in this method. Currently, there is no discussion and no evaluation of this factor.\n\n2. A minor comment on the main hypothesis, \"the optimal strategy for training surrogates is to preserve the ranks of the performances of HPO as an L2R problem.\": Although the idea of using L2R to learn the ranking of HPs to help with HPO is a reasonably good idea and can be effective, I do not see it necessarily being an \"optimal\" strategy. Since in HPO, the ultimate objective is to find one config which has the best performance, does the full ranking necessarily matter? This work verifies some hypotheses but does not discuss thoroughly on this main hypothesis.\n\n3. A question on the computation cost: I wonder does training the scorer, and the neural network surrogates bring significant computation overhead to the HPO process. The empirical evaluation in this work only evaluates the performance regarding the number of trials and does not evaluate the end-to-end computation cost. I hope the authors can provide more information on the computation overhead of the rank-based surrogate and also the anytime performance over computation resources used instead of the number of trials.\n\n4. On the meaning of BO iterations, trials, and the actual number of evaluated configurations: I have confusion on (1) Does # of BO iterations = # of trials? (2) Does # of trials in the Figure 5 = # of actual hyperparameter configurations evaluated?",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity: The paper is overall well-written. But there are several places where important details are missing. For example, the setting of rank list size.\n2. Quality: The overall quality of this work is good.\n3. Reproducibility: Code is submitted. The reproducibility is not a problem. ",
            "summary_of_the_review": "This work proposes an HPO method that learns to select the best HP from the rankings of HPs.  To the best of my knowledge, the ranking-based HPO method is quite novel. A comprehensive evaluation is done to show the effectiveness of the proposed method. However, I have concerns about how sensitive the method is on the ranking list size. I also have confusions about how the evaluation is done (see the Strength And Weaknesses). ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper942/Reviewer_7HGf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper942/Reviewer_7HGf"
        ]
    }
]