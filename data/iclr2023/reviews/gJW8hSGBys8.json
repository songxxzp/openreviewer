[
    {
        "id": "IdknAFkO6we",
        "original": null,
        "number": 1,
        "cdate": 1666388920606,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666388920606,
        "tmdate": 1669053326308,
        "tddate": null,
        "forum": "gJW8hSGBys8",
        "replyto": "gJW8hSGBys8",
        "invitation": "ICLR.cc/2023/Conference/Paper2875/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the least-to-most prompting technique to solve more realistic semantic parsing tasks. Specifically, the approach works as follows:\n\n1. Prompt a large language model with examples to perform syntactic parsing of the input sentence and decompose it into increasingly simpler parts\n2. Select relevant examples based on the decomposition obtained in 1\n3. Prompt the large language model with the relevant examples obtained in 2 in order to solve the task on all parts of the input (according to the decomposition found in 1), starting from simple parts and ending with the full input sentence.\n\nThe paper shows positive results on the CFQ and COGS datasets while only selecting examples from a small subset of the training data (e.g., 1%).",
            "strength_and_weaknesses": "Strength: this paper introduces a novel extension to least-to-most prompting with several improvements: tree decomposition; example selection to fit more relevant examples into the prompt (in realistic cases, the label space is too large to be able to fit examples for everything); subproblems are solved by giving context to the model. The results shown on CFQ and COGS are very good.\n\nWeakness (addressed in rebuttal): The approach requires dataset-specific prompts (for syntactic parsing) and heuristics (for exemplar selection). It is unclear how much the final performance depends on the details of the prompts/heuristics and how much the reported results overfit to these choices. Also, it looks like a lot of manual effort is needed if one wants to apply this method to a new dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is quite clear but many important details are in the appendix due to the complexity of prompts and heuristics.\n\nThis extension to least-to-most prompting is completely novel, to the best of my knowledge.",
            "summary_of_the_review": "I believe this paper is overall worthy of publication thanks to novelty and good results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_e27M"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_e27M"
        ]
    },
    {
        "id": "M5Pl203Ijl",
        "original": null,
        "number": 2,
        "cdate": 1666391837021,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666391837021,
        "tmdate": 1669046394629,
        "tddate": null,
        "forum": "gJW8hSGBys8",
        "replyto": "gJW8hSGBys8",
        "invitation": "ICLR.cc/2023/Conference/Paper2875/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Compositionality is an important ability of a model and in this paper, the authors propose to achieve such capacity by using a novel method inspired by least-to-most prompting. It addresses two practical challenges for applying least-to-most prompting:\n1. how to decompose the questions to generate least-to-most prompting\n2. In practice the uttererances can vary a lot, so how to choose prompt from a large pool\n\nBy addressing the above difficulties, the athors propose an s enhanced least-to-most prompt tuning method; empirically, the authors achieve SOTA/near SOTA performance for two popular compositionality bench marks COGS and CFQ.  ",
            "strength_and_weaknesses": "Strength:\n1. The paper has clear motivations to achieve compositionality for more real-life settings\n2. The proposed method addresses practical concerns\n3. The method shows great performance in two real-life like datasets (compared to SCAN)\n\nWeakness:\n1. The paper consists mainly an application of least-to-most prompt tuning method with relatively standard NLP techniques\n2. The ablations studies of the paper's contribution (sentence decomposition and dynamic prompt selection is missing from the main paper). The question that leave readers are for example: what is an effective sentence decomposition? what is the performance with no setntence decomposition but just some dynamic prompting? What about simpler prompt selection method?\n3. For CFQ dataset, the paper mentions that they use a list of manually chosen examplars (e.g. footnote 3), this limits a bit the applicability of the proposed method.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is nice to read. It argues well about the motivation and practical difficulties to apply least-to-most prompt tuning method to more complex semantic parsing tasks and well address the concerns. Both the motivation and the solution is clearly exposed.\n\nThe paper is an application of least-to-most prompt tuning applied to compositionality and the techniques proposed to address the concerns are quite standard NLP techniques. I don't rate highly on originality especially there misses a thorough ablation studies showing why such technical choices are important.",
            "summary_of_the_review": "The paper tackles how to achieve semantic parsing compositionality using least-to-most prompt tuning. It addresses two practical challenges that are 1) how to decompose a sentence in this case 2) how to dynamically choose the best examplars. The paper gives clear motivation and intuition about the proposed methods and achieve convincing results for two challenging complex semantic parsing tasks focusing on compositionality. \n\nOn the downside, the paper doesn't study closely for its technical choices, the decomposition alone is quite complicated (i.e. Appendix A.1 and B.1) and the paper doesn't mention how important and why such choices matter for the end performance; I don't see detailed ablation analysis for examplar choosing algorithms either and there are limitations of the methods shown by the implementation (e.g. manually choosing some examplars). \n\nPS: I have read the authors' comments, thanks a lot for authors to give detailed comments as well as new experimental results. My two main concerns (how to choose examples and ablation studies about the decomposition) have been answered in detail. The paper clearly has its own merits; however, I found the paper doesn't dive deep enough in its decomposition choices (e.g. Reviewer P921 mentioned a possiblity to apply syntactic parsing to do decomposition) and I am convinced that the manual step can be applied to other tasks but is also incurs cost so not sure if such appraoches will be easily widely adopted. I would not change my score although I sincerely appreciate the informative and constructive discussions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_WSSM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_WSSM"
        ]
    },
    {
        "id": "PViJwW-np3P",
        "original": null,
        "number": 3,
        "cdate": 1666578337650,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666578337650,
        "tmdate": 1666579080067,
        "tddate": null,
        "forum": "gJW8hSGBys8",
        "replyto": "gJW8hSGBys8",
        "invitation": "ICLR.cc/2023/Conference/Paper2875/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers compositional generalization and proposes a \"dynamic least-to-most prompting\" method to tackle the problem. \"dynamic least-to-most prompting\" consists of three steps: decomposition using syntactic parsing, dynamic exemplar selection, and sequential solution. The experiment results are impressive: on CFQ dataset, the proposed algorithm beats all the existing baselines while using much less training data. On COGS, although the performance is a little bit lower than one of the baseline, the number of training data used is much less.",
            "strength_and_weaknesses": "Strength:\n\n1. The idea used seems new and interesting.\n2. The experiment results are very good and impressive.\n\nWeakness:\n1. I suggest the authors also include the baseline results using the same number of training data as the proposed methods for a better comparison if possible.\n2. For the syntactic parsing part, although the experiment results of the proposed method is very good, it is not clear why we should use LM to parse instead of the more \"classical\" ways like training a PCFG like model. In fact, the paper from Qiu et al. uses the QCFG model to \"parse\". Is LM perform better? or it is much more sample-efficient? or it is much simpler?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written.\n\nQuality: good.\n\nNovelty: the idea is new, and the experiment results are impressive.\n\nReproducibility: good.",
            "summary_of_the_review": "I am not very familiar with this sub-field, and thus based on my evaluation, I think this paper is interesting and make some contribution to compositional generalization. The paper is well written and the comparison with the previous work seems enough to me. Besides, the numerical experiments show the effectiveness of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_P921"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_P921"
        ]
    },
    {
        "id": "_dfHr88iI6",
        "original": null,
        "number": 4,
        "cdate": 1666676862764,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666676862764,
        "tmdate": 1666676862764,
        "tddate": null,
        "forum": "gJW8hSGBys8",
        "replyto": "gJW8hSGBys8",
        "invitation": "ICLR.cc/2023/Conference/Paper2875/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors first perform decomposition by dividing the syntactic parsing task into multiple steps and provide LMs with exemplars that illustrate the task to be performed. They then choose an exemplar pool with 1000 examples on CFQ and 89 for COGS. And they select for each input between 4 and 35 exemplars for CFQ and between 1 and 3 exemplars for COGS. They highlight the effectiveness of such compositional ability and set new SOTA results for CFQ while requiring only 1% of the training data used by traditional finetuning approaches. ",
            "strength_and_weaknesses": "S:\n* The paper is well-written, well-motivated, and easy to follow.\n* The Dynamic least-to-most prompting is a good idea that it first sequentially predicts solutions to subproblems before generating the final output, where the subproblems are extracted through different prompts.  \n\nW:\n*There is no obvious weakness in this work, given the improvement and effectiveness of 1% data is very promising. \n* One minor thing could be there is no analysis between task complexity (for example, number of tokens, number of vocabulary, number of decomposed subquestions) and performance. Another minor thing is that there is no study on how sensitive the Codex model is to the data preprocessing used in the work as shown in the Appendix.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* CFQ in the abstract is used before introduced.",
            "summary_of_the_review": "This paper is working on a natural language semantic parsing benchmark, and the authors provide a dynamic version of least-to-most prompting to show promising generalization ability. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_gAic"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2875/Reviewer_gAic"
        ]
    }
]