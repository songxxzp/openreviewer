[
    {
        "id": "qCPCP-WRYfn",
        "original": null,
        "number": 1,
        "cdate": 1666795406647,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666795406647,
        "tmdate": 1666803895237,
        "tddate": null,
        "forum": "35PLkGkJOQ4",
        "replyto": "35PLkGkJOQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper4237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a tabular benchmark that includes various energy consumption metrics for a subset of networks in the NAS-Bench-101 search space. The authors provide simple multi-objective and single-objective baselines and run those on the benchmark.",
            "strength_and_weaknesses": "I find the proposed benchmark and the motivation for using such benchmark beneficial to the community as the awareness for resource efficient NAS increases. The paper is in general easy to follow and well-structured. The authors also release their codebase together with the API. However, I find the paper could improve with the following:\n\n- **A larger and novel search space**. The smaller versions of the NAS-Bench-101 search space do seem way far from being realistic in my opinion. With 91 and 2532 architectures, respectively, they do not provide a realistic testbed and even the simplest search algorithms would lead to optimal solutions pretty quickly. Moreover, these spaces do not provide support for one-shot NAS methods.\n\n- **More multi-objective algorithms to evaluate**. The authors only evaluate a single multi-objective algorithm in Section 4. It would be beneficial if they would add more methods to this section. Check [1] for some simple methods.\n\n- **More on the benchmark than the algorithms used**. The authors spend most of Section 3 by describing the multi-objective optimization algorithm they use to evaluate on their benchmark. While such a detailed description is appreciated, it does not serve the main purpose of the paper and therefore I do not find it necessary to be included in the main paper. Rather, more statistics and analysis of the benchmark would be more useful.\n\n-- References --\n\n[1] Bag of Baselines for Multi-objective Joint Neural Architecture Search and Hyperparameter Optimization. Guerrero-Viu et al. 2021",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general easy to follow. The authors provide the code with the supplementary material. The main issue with this submission is the novelty. When proposing a new benchmark, there should be some novel component in terms of the search space, benchmark construction (surrogate model used for instance in surrogate benchmarks), or empirical evaluations that provide major unforeseen insights into the field. Unfortunately, none of these criteria is fulfilled.",
            "summary_of_the_review": "The motivation for having a benchmark that provides energy consumption metrics is valid and would be beneficial for the NAS community. However, I think this submission is not ready for acceptance due to the reasons I mention above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_gjRN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_gjRN"
        ]
    },
    {
        "id": "4ed9_9wrrs",
        "original": null,
        "number": 2,
        "cdate": 1667030314244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667030314244,
        "tmdate": 1667030314244,
        "tddate": null,
        "forum": "35PLkGkJOQ4",
        "replyto": "35PLkGkJOQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper4237/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed an energy consumption-aware tabular benchmark for NAS based on NAS-101, called EC-NAS-Bench. EC-NAS-Bench contains the training energy consumption, power consumption and carbon footprint of each architecture in the benchmark. The author performs single-objective optimization and multi-object optimization (based on the algorithm proposed in the paper \"Multi-objective optimization with unbounded solution sets\") on EC-NAS-Bench, and noticed that multi-objective optimization is able to figure out architectures with about 70% energy reduction and <1% performance degradation\n",
            "strength_and_weaknesses": "Strength\n\n- Incorporating energy consumption data in NAS is an important problem. The author took the first step in this direction. Experiments show that it is able to find architecture that consumes much less energy but with comparable performance.\n\nWeakness\n\n- Different from the performance of the model, energy consumption of neural networks is a moving target. This is related to factors like global economy and the computational hardware. The author has not discussed how they will incorporate potential changes in the energy/power consumption and carbon footprint of the models.\n- The multi-objective optimization algorithm is largely based on the cited paper \"Multi-objective optimization with unbounded solution sets\". Are there novel components proposed in the paper?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is overall well-written. The author can improve the description of the MOO algorithm (e.g., give more details in the Appendix)\n\nQuality + Novelty: The reviewer is concerned that it may be difficult for other researchers to continue their experiments on EC-NAS-Bench. The reason is that new hardwares appear every year and EC-NAS-Bench needs to be updated accordingly. The reviewer agrees that incorporting energy consumption in NAS is an important topic but EC-NAS-Bench won't be very useful when new hardwares appear.\n\nReproducibility: The author attached the source code. The reviewer has not ran the source code and only reviewed the content. It looks reproducible.",
            "summary_of_the_review": "I vote for weak rejection because EC-NAS-Bench may soon outdated when new hardwares appear. Thus, this benchmark may not be useful to the research community.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_bAGs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_bAGs"
        ]
    },
    {
        "id": "zUS-BLbrVV",
        "original": null,
        "number": 3,
        "cdate": 1667099867345,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667099867345,
        "tmdate": 1667099867345,
        "tddate": null,
        "forum": "35PLkGkJOQ4",
        "replyto": "35PLkGkJOQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper4237/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes an energy consumption-aware tabular benchmark for NAS based on NAS-Bench-101. For each architecture, it adds the training energy consumption, power consumption, and carbon footprint. This work also demonstrates the usefulness of multi-objective architecture exploration for finding energy-efficient architectures without sacrificing much accuracy.\nThe MOO algorithm used is based on existing work from Krause et al. 2016.\n\n",
            "strength_and_weaknesses": "**Strengths**: Energy consumption is indeed an important factor to consider in NAS. This benchmark can help facilitate further research and development on energy-efficient NAS. This work presents an interesting showcase of using MOO to find energy-efficient NAS.\n\n**Questions and concerns**: This benchmark changed the search space of NAS-Bench-101. For example, instead of evaluating a 7V space, this work tests 5V and 4V spaces. Can the authors justify why this change is made? Clarity of the results: The results presented in Figure 2 are not very clear to me. Figure 2a shows the Pareto front obtained from one of the MOO runs. But what do the different markers and colors mean? It is my understanding that the blue, green, and yellow points correspond to the three types of architecture, including two extrema and a knee point. Why the red one, i.e., the optimal solution in SOO is, not shown in the figure? It would be great if the authors could establish benchmarked baseline performance on this new benchmark.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity: The paper is clear overall. But there are some places in the empirical evaluation that are not very clear.   \n2. Quality: The quality of the presentation can be improved. \n3. Reproducibility: The authors mentioned that the proposed benchmark is open-sourced. This makes me believe the reproducibility is not an issue. ",
            "summary_of_the_review": "The established energy-aware NAS benchmark is a very good contribution. However, the presentation of results could be improved. And benchmarked baselines should be added. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_Bb6X"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_Bb6X"
        ]
    },
    {
        "id": "GoPtyu7BGrc",
        "original": null,
        "number": 4,
        "cdate": 1667180193044,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667180193044,
        "tmdate": 1667180222889,
        "tddate": null,
        "forum": "35PLkGkJOQ4",
        "replyto": "35PLkGkJOQ4",
        "invitation": "ICLR.cc/2023/Conference/Paper4237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper present a new benchmark that extends NASBench-101 with the training energy footprints. The author also presents a MOO algorithm to perform the NAS.",
            "strength_and_weaknesses": "Strength:\n1) curate a new dataset and perform NAS on it. \n2) the authors also evaluate their algorithms on NAS benchmark.\n3) the paper is quite easy to follow, it is more like a tool paper that proposes a new benchmark for others to use.\n\nWeakness:\n1) I don't think it is necessary for such paper to introduce a new optimizer then spent a huge amount of content on the optimizer. Instead, I think it is more reasonable for authors to reuse the latest MOO algorithm such as [1] and [2] to perform NAS on their datasets, then release them as a baseline.\n\n[1] Zhao, Yiyang, et al. \"Multi-objective Optimization by Learning Space Partitions.\" arXiv preprint arXiv:2110.03173 (2021).\n\n[2] Daulton, Samuel, Maximilian Balandat, and Eytan Bakshy. \"Parallel bayesian optimization of multiple noisy objectives with expected hypervolume improvement.\" Advances in Neural Information Processing Systems 34 (2021): 2187-2200.\n\n2) The experiments spent too much on the optimizer but not on the dataset, but I'd like to see more analysis on the dataset since I assume the focus of this paper is on introducing a new dataset.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "3) I'd like author clarify the following question? Currently there are training based evaluation (most accurate but most expensive), one-shot evaluation (least accurate but least expensive), and few-shot evaluations (between the one-shot and training) [3]. In one-shot or few-shot, the users only train one or few supernets, therefore it should be the most energy efficient. How do we quantify the energy efficiency in few-shot scenarios? [3] Thank you.\n\n[3] Zhao, Yiyang, et al. \"Few-shot neural architecture search.\" International Conference on Machine Learning. PMLR, 2021.\n",
            "summary_of_the_review": "It is a nice paper, but the authors seem to strike the reviews with some novelty. Frankly, it is okay for such tool paper to be not that novel as long as it solves practical problems. So I'd hope authors focus more on analyzing the dataset itself and establish a set of canonical baselines for using the dataset. Thank you.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_FG4d"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4237/Reviewer_FG4d"
        ]
    }
]