[
    {
        "id": "-VnVtEz1XP",
        "original": null,
        "number": 1,
        "cdate": 1666619863284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619863284,
        "tmdate": 1672483687987,
        "tddate": null,
        "forum": "0pdSt3oyJa1",
        "replyto": "0pdSt3oyJa1",
        "invitation": "ICLR.cc/2023/Conference/Paper5968/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper introduces Specformer, a novel Transformer-based architecture for node- and graph-level prediction tasks. A Specformer layer embeds structure (eigenvalues) and node features separately, and then combines the obtained representations using graph convolutions. The model performs self-attention in the spectral domain. Experiments on synthetic and real-world datasets show the effectiveness of the proposed method.",
            "strength_and_weaknesses": "\nStrengths\n - Simplicity. The idea is easy-to-follow. \n - The paper introduces synthetic datasets for which it reports the approximation capability of Specformer.\n - Good performance. The proposed method yields good performance overall. For instance, it gets 0.066 MAE on the ZINC dataset.\n\nWeaknesses\n - Limited evaluation setup. Despite the promising results, the paper lacks relevant benchmarks for node-level tasks and only considers molecular datasets (small graphs) for graph classification. \n - It is unclear whether the computational overhead of the proposed method is significant or not, and how it compares to other GNNs. \n - The paper provides limited theoretical insights about the proposed architecture.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding **clarity and quality**, I have a few concerns and questions:\n\n- C1: There are some unclear statements in the paper. For instance: \n  - \"spectral filters are often approximated via fixed order\" , \"this truncated approximation is less expressive\". What do we want to approximate here? \n  - \"The reason is that the spectrum of Squirrel is more difficult to learn\" (Section 5.4). Isn't the spectrum only the multiset of eigenvalues?\n\n- C2: What is $\\hat{S}_i$ in Eq. (6)? Does the matrix $W_x$ change with $l$? If so, I would write explicitly for precision and clarity.\n\n- C3: How is the MLP in Eq. (5) applied to size-varying graphs in graph-level tasks? The dimensionality of $S_m$ would change with the graph size.\n\n- C4: The paper repeatedly mentions expressivity to motivate the proposal and the adoption of Transformers. How does the expressive power of the proposed method compare to other GNNs? What are the limitations of the proposed approach?\n\n - C5: Does the complexity analysis (paragraph on Page 5) account for the eigendecomposition? How does the training/test time of the proposed method compare to other GNNs? Is the Laplacian eigendecomposition pre-computed? Is it negligible?\n \n - C6: It is hard to observe some curves (e.g., ground truth) in Figure 2.  \n\n - C7: Experiments on graph-level tasks only consider molecular data. Is there any particular reason? The graphs in these datasets are rather small. Also, the paper does not consider very popular benchmarks for node classification from OGB. I am curious about the method's performance across graphs of very different sizes.\n\nRegarding **originality**, to the best of my knowledge, the proposed architecture is somewhat novel. \n\n**Reproducibility**: Code is unavailable during the reviewing process. Thus, we can not check further details and correctness. Also, some details about training (e.g., optimizer) and model selection are missing.",
            "summary_of_the_review": "Overall, the paper is relatively well-written and introduces a promising architecture for node- and graph-level prediction tasks. I have raised some concerns regarding the significance of the empirical evaluation, the computational cost/scalability, reproducibility, and clarity. Also, the paper provides limited theoretical insights about the proposed architecture, and the motivation behind some model choices is unclear/ not strong. Therefore, I am ranking this paper as marginally below the acceptance threshold.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_ur3z"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_ur3z"
        ]
    },
    {
        "id": "wREO0JC8Ztg",
        "original": null,
        "number": 2,
        "cdate": 1666656938772,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656938772,
        "tmdate": 1666656938772,
        "tddate": null,
        "forum": "0pdSt3oyJa1",
        "replyto": "0pdSt3oyJa1",
        "invitation": "ICLR.cc/2023/Conference/Paper5968/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In this paper, authors challenge a usual way of designing a spectral function with a dimensional-wise mapping on Graph diffusion. Based on this, authors pay more attention on the global pattern of the specturm rather than single eigenvalue and propose a learnable set2set spectral filter. Based on the new spectral filter, more powerful spectral graph neural network is built. In experiments, authors demonstrate the convincing result for the proposed method.",
            "strength_and_weaknesses": "Strength\n1. The idea is novel because all of spectral GNNs only focus on the spectral filter on the single eigenvalues. And it is very easy to find the rationality, even if the whitening is used in images, we still need nxnxk convolution filter rather than nxnx1 convolution filter. Thus I believe the motivation is reasonable, and we all ignore this.\n2. The proposed method also inspires me. Although it still has some drawbacks but still surprises me.\n3. Comprehensive experiments are convincing in my opinion.\n\nWeaknesses:\n1. I am curious if the set2set filter is able to find some interesting patterns from a spatial perspective. Because existing experiments are so quantitive, is there any possibility of digging up some quantitative experiments to demonstrate what kind of pattern the proposed method is able to find and the scalar-based spectral filter cannot?\n2. The computational complexity is quite high because of eigenvalues. So is it possible to explicitly use eigenvalues rather than implictly work on eigenvalues? Because I notice the experiments on small datasets. Although this method is promising, I still hope there is any possibility for a large-scale version.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is easy to follow and the clarity is good. Overall, the quality is good for an ICLR paper. The novelty is pretty good. No code is provided to evaluate reproducibility.",
            "summary_of_the_review": "This paper is very interesting, although there are some shortcomings in implementations. It opens an promising direction for GNN. I am optimistic about these issues. Thus I make the recommendation to accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_JCjn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_JCjn"
        ]
    },
    {
        "id": "RgZWCqTfI4",
        "original": null,
        "number": 3,
        "cdate": 1666753235415,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666753235415,
        "tmdate": 1666753235415,
        "tddate": null,
        "forum": "0pdSt3oyJa1",
        "replyto": "0pdSt3oyJa1",
        "invitation": "ICLR.cc/2023/Conference/Paper5968/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a spectral graph filter via introducing transformer structure, the idea seems novel, but the following points need to be concerned and explained.\n",
            "strength_and_weaknesses": "1. The author introduces some weaknesses for current spectral fitler, but the explaination is not convincing enough. \n1) At first, the authors propose -- \"mapping a single eigenvalue to a single filtered value, thus\nignoring the global pattern of the spectrum\", which means that the interaction among eigenvalues are lacked. However, how to understand the interaction among different eigenvalues (frequiences in Graph Signal Processing)? \nI think that this assumption should be supported via theoretical analysis. \n2) Secondly, the authors propose -- \"these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility\". \nIn fact, the polynomials paradigm was proposed to avoid the huge cost of eigenvalue decomposition for calculating U of Laplacian matrix. \nHowever, the proposed model needs to calculate U, which introduces huge cost. In other words, we can also learn the fiter parameters based on other networks (without transformer) if we ignore the cost of eigenvalue decomposition. \nThus I think the proposed model does not handle the trade-off between computation cost and expressiveness.\n2. I also have some questions for the architecture of proposed model, includes,\n1) How to understand the benifits of Eq.(2)-the EE form which encodes the eigenvalues? \n2) The proposed model needs to calculate all eigenvectors U and eigenvalues (O(n^3)), the complexity analysis is not true. ",
            "clarity,_quality,_novelty_and_reproducibility": "\n1. The author introduces some weaknesses for current spectral fitler, but the explaination is not convincing enough. \n1) At first, the authors propose -- \"mapping a single eigenvalue to a single filtered value, thus\nignoring the global pattern of the spectrum\", which means that the interaction among eigenvalues are lacked. However, how to understand the interaction among different eigenvalues (frequiences in Graph Signal Processing)? \nI think that this assumption should be supported via theoretical analysis. \n2) Secondly, the authors propose -- \"these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility\". \nIn fact, the polynomials paradigm was proposed to avoid the huge cost of eigenvalue decomposition for calculating U of Laplacian matrix. \nHowever, the proposed model needs to calculate U, which introduces huge cost. In other words, we can also learn the fiter parameters based on other networks (without transformer) if we ignore the cost of eigenvalue decomposition. \nThus I think the proposed model does not handle the trade-off between computation cost and expressiveness.\n2. I also have some questions for the architecture of proposed model, includes,\n1) How to understand the benifits of Eq.(2)-the EE form which encodes the eigenvalues? \n2) The proposed model needs to calculate all eigenvectors U and eigenvalues (O(n^3)), the complexity analysis is not true. ",
            "summary_of_the_review": "This paper proposes a spectral graph filter via introducing transformer structure, the idea seems novel, but the following points need to be concerned and explained. The author introduces some weaknesses for current spectral fitler, but the explaination is not convincing enough. How to understand the benifits of Eq.(2)-the EE form which encodes the eigenvalues? The proposed model needs to calculate all eigenvectors U and eigenvalues (O(n^3)), the complexity analysis is not true. More questions can been seen above.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_T8vi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_T8vi"
        ]
    },
    {
        "id": "nPygH3H9rYk",
        "original": null,
        "number": 4,
        "cdate": 1666783591646,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666783591646,
        "tmdate": 1666833204624,
        "tddate": null,
        "forum": "0pdSt3oyJa1",
        "replyto": "0pdSt3oyJa1",
        "invitation": "ICLR.cc/2023/Conference/Paper5968/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes Specformer, a Transformer-based graph spectral filter that captures the magnitudes and relative dependencies of all Laplacian eigenvalues. Specformer is permutation equivariant and can perform non-local graph convolutions. Extensive experiments on the node-level and graph-level datasets demonstrate Specformer's promising performance.",
            "strength_and_weaknesses": "Strengths\uff1a\n\n1)\tThe motivation for using self-attention in the spectral domain is clear and inspiring.\n\n2)\tThe authors' theoretical comparisons of Specformer to Polynomial GNNs, MPNNs, and Graph Transformers indicate Specformer's flexibility and universality, which looks sound to me.\n\n3)\tExperiments demonstrate that Specformer can learn a flexible and expressive filter and outperform baselines on node-level and graph-level tasks, particularly on heterophilic graph datasets.\n\n4)\tThis paper is well-written and easy to follow.\n\nWeaknesses:\n\n1)\tThe scalability of Specformer is seriously limited by its quadratic time complexity.\n\n2)\tIt would be helpful to empirically compare Specformer with baselines in terms of time and space overhead, even as the Specformer's time complexity is theoretically analyzed.\n\n3)\tSome baseline experimental results are missing, and no explanation. For example, the results of the Graphormer on MolHIV and MolPCBA are not displayed in Table 3 but are reported in the Graphormer's paper.\n\n4)\tThe experimental datasets are small and a bit old. Using large and latest datasets (such as ogbn-arxiv in OGB and Penn94 in LINKX [1]) may significantly enhance the paper's quality.\n\n[1] Lim, Derek, et al. \"Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.\" In NeurIPS 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is overall reader-friendly. \n\nQuality and Novelty: This paper has a clear motivation, and the proposed method is marginally novel.\n\nReproducibility: There is no available code.\n",
            "summary_of_the_review": "The motivation of this paper is clear, and the proposed method is promising. However, Specformer's quadratic time complexity is a serious limitation, and the experimental results are incomplete since some baseline results are missing. Due to the negative aspects of this work, I am slightly inclined to recommend rejection, but I would encourage the authors to address some of the issues raised in the comments in the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_Ge61"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5968/Reviewer_Ge61"
        ]
    }
]