[
    {
        "id": "_O-MQTFTN3B",
        "original": null,
        "number": 1,
        "cdate": 1666555168466,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666555168466,
        "tmdate": 1666555168466,
        "tddate": null,
        "forum": "e9CKiV6pgBD",
        "replyto": "e9CKiV6pgBD",
        "invitation": "ICLR.cc/2023/Conference/Paper3547/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper claims that humans do not always favor high-likelihood texts; decoding using high likelihood leads to repetition and boredom. So, the paper proposes a sampling method. It rescales the high-likelihood probabilities in a distribution (determined by quartiles and interquartile range aka IQR, as shown in Equation 2). The paper claims that the rescaled distribution has a \u201ccloser resemblance to the quality-likelihood curve\u201d shown in Figure 1. Experimenting on unconditional text generation (using a specific prompt) \u2013 generation is a-few-sentence long, novelty and diversity are better for this approach compared to the baselines. \n",
            "strength_and_weaknesses": "\n\nDecoding is an important problem, and I\u2019m glad that there\u2019s attention on this issue. \n\nIt\u2019s good to see that the authors have pointed out that lower perplexity does not equal better quality. It\u2019s good to see the human evaluation details. \n\n\nThe likelihood trap isn\u2019t a strong enough motivation for the authors\u2019 algorithm. \n- First, the authors point out the likelihood trap where the \u201crepetition tendency grows stronger when more loops occur\u201d (which the degeneration paper by Holtzmann et al. already discussed) \u2013 if cognitive scientists do the same experiments for humans, it\u2019s likely that humans will also assign higher probabilities to repetition loops if there are already repetitions in the prefix. \n- Second, to solve this problem, one simple strategy is n-gram blocking, so we just don\u2019t generate a sentence with two trigrams that are exactly the same, for example. See Paulus et al. (2018) https://openreview.net/pdf?id=HkAClQgA- (Section 2.5) and Chen et al. (2021) https://arxiv.org/pdf/2012.14919.pdf (search n-gram blocking). \n\nOn empirical results\u2026\n- The authors only experimented on language modeling (unconditional text generation). I\u2019m not convinced that this approach would work on the vast majority of text generation applications (translation, summarization, question answering, or even dialogue). \n- It\u2019s not to me whether the proposed approach will work for longer story generation. \n- It\u2019s not clear to me whether human evaluation considers the consistency of the generated article. \n- For baselines, it\u2019s not clear how the k, p, temperature are tuned. It\u2019s also not clear whether the same human evaluation trend will work for other perplexity levels. \n- What is the annotator agreement? Please let me know if I missed this information.\n- Relatedly, the authors claim (at the end of page 4) that V^{very high} will be singleton on a peaked distribution that contains unquestionably right words. But it\u2019s not clear to me what the boundary between \u201cunquestionably right\u201d and \u201cquestionably right\u201d is. \n\nIt\u2019s a bit unconvincing to me that the authors\u2019 claim that \u201clower likelihood words on a flat distribution are reasonable choices\u201d in Section 2.2 is only verified on one example only (Figure 4). \n\nRelatedly, the authors may have misrepresented previous work in Section 2.2. The authors claim that \u201cit is proven beneficial to increase generation diversity by emphasizing less probable words during training\u201d and have cited Welleck et al. (2020). But the unlikelihood objective in the Welleck et al. paper, in fact, is trying to make incorrect repeating tokens less likely and make frequent tokens less likely. This is not equivalent to simply making less probable words more likely. \n",
            "clarity,_quality,_novelty_and_reproducibility": "A few other comments\n- The design decision of Equation (1) is not totally clear to me. \n- The design decision of Equation (2) seems quite arbitrary to me. How is rho chosen? \n- What does \u201cregularized\u201d mean in \u201cregularized distribution on V\u201d under Equation (1)?\n",
            "summary_of_the_review": "Interesting observation that sometimes we don't want highest-scoring tokens, but algorithm design decisions are sometimes arbitrary. Experiments are limited on a specific task. Baselines are not entirely convincing. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_yKG8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_yKG8"
        ]
    },
    {
        "id": "zvBaDN2CadJ",
        "original": null,
        "number": 2,
        "cdate": 1666663467482,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663467482,
        "tmdate": 1666663467482,
        "tddate": null,
        "forum": "e9CKiV6pgBD",
        "replyto": "e9CKiV6pgBD",
        "invitation": "ICLR.cc/2023/Conference/Paper3547/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new probabilistic decoding method for generating a sequence of text from a trained autoregressive model $p(x_{t+1}|x_{1:t})$. The method, IQR-IP, is carried out by first doing top-p and top-k filtering on the possible generations, then computing the quantiles of the remaining tokens. For quartiles $Q_1, Q_2, Q_3, Q_4$, we take the set of generations with probability greater than $Q_3 + \\rho \\cdot\\text{IQR}$, where $\\text{IQR}$ is the interquartile range. Then we reweight this set of candidates. This reweighted set is combined with the post top-k, top-p set to get the set of candidates, which are sampled from.",
            "strength_and_weaknesses": "Strengths\n\n+ The paper is reasonably clear, although certain parts are hard to understand exactly. \n\nWeaknesses\n\n+ The method is not very well justified. The authors do not clearly describe why their method is appropriate. Their central idea is that of inverse-probability weighting the 'head' of the candidate distribution. This is justified with a vague reference to [Zhang 2021] and 'causal inference', but I think this central point needs to be justified better. For instance, if I have a 'head' with probability [1/2, 1/3], after the IQR-IP algorithm I have candidates with probability [1/3, 1/2]. After reading the paper closely, it still seems somewhat mysterious why this would make sense.\n+ The experiments are relatively weak. The most glaring flaw is that as far as I can tell, all the experiments are carried out with the same prompt ('She walks in beauty'). This doesn't give me much confidence that the results are going to generalize to other prompts. Similarly, the comparisons are lacking. Given the structure of IQR-IP, it would make sense to have a comparison with the intersection of top-p and top-k. Furthermore, the experiments only use a single, relatively small model (GPT2-XL). It is common practice to evaluate sampling methods over several models, particularly paying attention to the change in performance as the number of parameters increases. The authors could use the recently-released OPT series of models to study this.\n+ Nitpick: Why is there no V^VeryLow?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n\nI believe the paper gets the message across, but could have a much clearer motivation. \n\nQuality\n\nThe work is overall of medium quality. The motivation for the method is muddled. The experiments are quite systematic in terms of mechanical turk investment, etc, but seem to have a glaring flaw with only one single prompt. \n\nNovelty\n\nThe method is mostly just a mix of previous methods, with the exception of the IP part. As far as I am aware, that is novel in this context.\n\nReproducibility\n\nFull code is provided, so theoretically it is 100% reproducible.\n\n",
            "summary_of_the_review": "The paper proposes a method which appears to have some slight advantages over competing decoding methods. However, since it is only evaluated on one prompt and contains several ad-hoc choices of filters, parameters etc, it's difficult to say if the method would generalize to other prompts. Similarly, we only know the performance on a single model, which is relatively small in 2022. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_bnch"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_bnch"
        ]
    },
    {
        "id": "mwILCQSAbA",
        "original": null,
        "number": 3,
        "cdate": 1666664319673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664319673,
        "tmdate": 1666664319673,
        "tddate": null,
        "forum": "e9CKiV6pgBD",
        "replyto": "e9CKiV6pgBD",
        "invitation": "ICLR.cc/2023/Conference/Paper3547/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel sampling algorithm for distributions learned by neural language models. The authors start by highlighting a pitfall of \u201cnaively\u201d sampling from these models, where high likelihood tokens being picked leads to their future probability increasing and to repetitive loops in the samples.\n\nTo increase diversity, they then propose modifying the distribution we sample from by rescaling this high-likelihood tokens. After the removal of tail, very low-probabity tokens with previously proposed methods (such as top-k and nucleus), the authors propose rescaling the tokens belonging to the top quartile (25%) of probabilities to make their probabilities proportional to their inverse.\n\nTo evaluate their propose sampling algorithm, a pretrained GPT-2 XL model is used to generate samples from a pre-defined prompt:\n\n\u201cShe walks in beauty\u201d\n\ngenerating 5000 sample passages for their method and baseline sampling approaches. They then evaluate this passage according to (1) fluency based on the perplexity of the model (2) diversity based on Self-BLEU, N-gram entropy and Zipf coeficient and (3) 5-point Likert Scale for Fluency & Novelty. Finding that their method proposes comparably to other methods in human evaluation, arguably generating more diverse samples.",
            "strength_and_weaknesses": "- The analysis earlier on in the paper is interesting. Although the repetitive-ness problem has been identified before, the connections made with high-likelihood tokens and their increasing probability is relevant and should be highlighted.\n- I believe that modifying the sampling algorithm to penalize tokens in the \"head\" is a promising direction to improve the quality of samples decoded from large models.\n\nI think the paper has a couple of important flaws:\n\n- The proposed method lacks throughout analysis and isn\u2019t particularly novel or theoretically motivated. Half of the procedure is just applying two well known pruning techniques, nucleus and top-k. The choice of only re-scaling the top quartile seems kind of arbitrary and not well studies (why not pick, lets say, the top 10% percentile?) and there is very little discussion on why rescaling according to their inverse probability values is a good choice. Given that this algorithm is very heuristic, these choices need to be motivated by having a good ablation study.  Also, their proposed sampling algorithm has 3 hyperparameters, leading me to believe that it will be hard to tune in practice\n- The experimental setup is lackluster, and the results are disappointing. The authors only evaluate their algorithm according to generated completions for a **single** prompt and by a **single** model, compare to other simple baseline sampling algorithms for the same prompt and model. The results then show that their model achieves comparable perplexity to human references and a bit more novelty according human-references. However the differences in values are very small, and since no confidence bounds are statistical tests are done, it\u2019s hard to grasp if their method is clearly better\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "While not yet published in a venue (and therefor doesn\u2019t have an impact on my assessment) I believe in the future authors should compare their approach to prior work by (Meister et al 2022: ****Locally Typical Sampling****) since it introduces the similar idea of pruning the head of the distribution to increase diversity, but with much less hyperparameters and more theoretically motivated.",
            "summary_of_the_review": "This paper proposes a novel sampling algorithm that down-weights the probabilities of the most-likely tokens in the distribution. While the intuition behind it is interesting and has potential, some of the design choices are not well-explored and the experimental evaluation is very lack-luster.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_yD5T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_yD5T"
        ]
    },
    {
        "id": "V0iE8ixXmd",
        "original": null,
        "number": 4,
        "cdate": 1666967144755,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666967144755,
        "tmdate": 1666967144755,
        "tddate": null,
        "forum": "e9CKiV6pgBD",
        "replyto": "e9CKiV6pgBD",
        "invitation": "ICLR.cc/2023/Conference/Paper3547/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a scheme to improve open-ended generation from autoregressive models that is based on manipulating the token-level distribution of the model during ancestral sampling. Specifically, this scheme proposes pruning off the tail at each step like other popular decoding methods like Nucleus sampling and top-k sampling. But in addition to pruning, this scheme also involves rescaling the \"head\" the distribution so that items with very high likelihoods are deliberately suppressed compared to other items in the vocabulary in order to improve diversity and reduce repetition in the generated text. For this heuristic rescaling, the distribution over the vocabulary is divided into bins and the bin with the highest likelihood is reweighted according to the inverse probability of the items. This scheme is empirically compared to other popular approaches like Nucleus sampling, top-k sampling on various metrics like fluency, diversity, human judgement etc.",
            "strength_and_weaknesses": "Strengths:\n\n-- This paper identifies valid concerns with the quality of sampling from autoregressive models for open-ended generation and the scheme is motivated by empirical findings in recent works studying generation from large autoregressive models.\n\n-- The empirical comparison includes various sensible metrics and comparisons which give a holistic picture of the effect of the proposed scheme.\n\n-- The proposed scheme is straightforward to implement and is not any more computationally expensive than other common methods used for ancestral sampling-based schemes.\n\n-- The results suggest that this approach performs comparably to other common sampling approaches, but has higher diversity than approaches like Nucleus sampling which is encouraging but not surprising given the scheme's effect of flattening the distribution.\n\nWeaknesses:\n\n-- The scheme does not seem grounded in any statistical principles related to sampling from probabilistic generators and neither does it ground itself in a clear objective that needs to be optimized. It involves arbitrary heuristics and introduces many more hyperparameters which seem crucial to get right from the results.\n\n-- This paper fails to discuss and compare with other related work that also rescales the distribution such that the \"head\" is suppressed compared to other items in the vocabulary. An example of such work is [2], that uses entropy of the token-level distribution to rescale the distribution. Empirical comparison should be made against such works as well. Also, although cited, more discussion about comparison with other related work like [3] should be done in the main body.\n\n-- This paper refers to [1] at many places but doesn't sufficiently describe the findings and setup of that paper which makes it difficult to read as a standalone paper.\n\nMoreover Figure 1 of [1] refers to human judgements over full sentences whereas this work seemingly incorrectly applies that finding to *token-level distribution* in their argument (Figure 1 of this paper).\n\n-- The human judgement section in the main body is difficult to understand.\n\n[1] Trading off diversity and quality in natural language generation. Zhang et al., 2021\n[2] Locally Typical Sampling. Meister et al., Feb 2022\n[3] MIROSTAT: A neural text decoding algorithm that directly controls perplexity. Basu et al., 2021",
            "clarity,_quality,_novelty_and_reproducibility": "Please see my review above. The paper can be made clearer, especially with more context about [1].\n\nRegarding novelty, the issues with autoregressive models and open-ended generation are well documented in the prior work and some attempts like [2] have also attempted to rescale the distribution such that the high likelihood words are suppressed during generation. The proposed approach is novel in a precise sense because the exact algorithm has not been proposed before to the best of my knowledge. However, this work needs a more thorough comparison with other existing work in this area.\n\nThe approach is straightforward to implement and hence it should not be difficult to reproduce the results.",
            "summary_of_the_review": "The paper is generally well- written and the approach is reasonable and is supported by clear experiments. However, comparison to other existing work is inadequate and the approach seems seems dependent on arbitrary heuristics.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_XtRt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3547/Reviewer_XtRt"
        ]
    }
]