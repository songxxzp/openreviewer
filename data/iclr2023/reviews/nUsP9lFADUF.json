[
    {
        "id": "3zFXI2me3C",
        "original": null,
        "number": 1,
        "cdate": 1666597245503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597245503,
        "tmdate": 1666597245503,
        "tddate": null,
        "forum": "nUsP9lFADUF",
        "replyto": "nUsP9lFADUF",
        "invitation": "ICLR.cc/2023/Conference/Paper2212/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work presents the analysis that decision boundary is of critical importance to the performance of few-shot demonstrations and the traditional decision boundary leads to the fragility of prompting LMs. The authors propose prototypical calibration to adaptively learn a more robust decision boundary for few-shot learning of language models.",
            "strength_and_weaknesses": "Strength: The analysis that decision boundary is of critical importance to the performance and Instability of few-shot demonstrations is interesting and suggestive. The authors propose prototypical calibration to adaptively learn a decision boundary for\nfew-shot classification of language models. They present provide rich experiments to show the effectiveness of their proposed PROCA on a wide range of text classification tasks.\n\n\nWeaknesses: The paper is somewhat vague for me and I am still confused about some notions or symbols. For example, \u03bcn and \u03a3n should be placed with vector and matrix symbols. I am confused the relationship between \u03bcn and \u03bcn,l. Does the {\u03bcn,l} is the l-th element of \u03bcn or they are different variables? Since tuple k is a cluster-label assignment, does this mean that k represents a matrix of N * N?  The authors transforme the assignment score to a weighted bipartite matching problem between N clusters and N labels. It would be better if the authors can provide more ditails about the  bipartite matching problem. In my opinion, the key problem in this work is learn cluster-label assignment, which however omits useful details and also overall algorithm. Since we are dealing a N way few shot problem, whether the few labeled samples in N way few shot task are utilized for the learning of assignment score. Besides, the authors utilize a small-scale unlabeled in-domain dataset, named as estimate set (Desti), to estimate the parameters. Does this mean the estimate set is sampled from \"N way\" without labeling?  In that cases, is the PROCA still fair for a typical N way few shot setting? Maybe the authors can clarify the estimate set  from this view.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "For me, this submission omitts some useful information, making it difficult to understand the novelty of this work.",
            "summary_of_the_review": "Although calibrating prototypes for few-shot learning sounds reasonable, I have difficulty in understanding the specific approach of this submission, especially 3.2 and 3.3.\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_iRwh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_iRwh"
        ]
    },
    {
        "id": "lqprhzJ2Yn",
        "original": null,
        "number": 2,
        "cdate": 1666665225679,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665225679,
        "tmdate": 1666665225679,
        "tddate": null,
        "forum": "nUsP9lFADUF",
        "replyto": "nUsP9lFADUF",
        "invitation": "ICLR.cc/2023/Conference/Paper2212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a highly empirically effective approach for in-context learning using so-called _prototype calibration_. The simple, yet dramatically effective approach is well motivated by the authors and demonstrated to be empirically effective under a wide variety of settings, testing sensitivity to number of shots, prompt template, class imbalance and more. ",
            "strength_and_weaknesses": "Overall I believe this to be a very strong empirical investigation into in-context learning. The proposed approach is clearly motivated, simple and seems to be highly effective. \n\nStrengths:\n* Dramatic performance benefits from the proposed approach. 10+ pt improvement in most cases on average, 5+ in a few others (Table 1). ]\n* Detailed empirical analysis that includes: effectiveness across templates, demonstration perturbation, class imbalance, estimate set construction, estimate set size, \n* The results and analysis are clearly presented as are the methodological approach.\n* Provides basis for future work on using using prototypes/clusters in the midst of in-context learning\n\nWeaknesses: \n* It might be interesting to consider how the fit of the GMM distribution effects the downstream task performance. Also since the number of points in the estimate set is not so many, it could be interesting to consider a wider array of clustering inference methods including more exhaustive/expensive methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation is clear and quality of experiments are apparent to me.",
            "summary_of_the_review": "This is a strong paper which presents a simple and effective approach for in context learning. The approach uses a GMM cluster-based approach to make predictions. The effectiveness of the approach is demonstrated through extensive experiments that evaluate the performance along many dimensions of number of shots, prompts, estimate set, etc.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_f9YD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_f9YD"
        ]
    },
    {
        "id": "iXtjpUjNZL",
        "original": null,
        "number": 3,
        "cdate": 1666984054102,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666984054102,
        "tmdate": 1666984106910,
        "tddate": null,
        "forum": "nUsP9lFADUF",
        "replyto": "nUsP9lFADUF",
        "invitation": "ICLR.cc/2023/Conference/Paper2212/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes prototypical calibration for zero-shot and few-shot classification tasks when evaluating large language models. The approach first uses Gaussian mixture distribution to estimate the prototypical clusters for all categories of the classification task and then assigns each cluster to the corresponding label by solving a weighted bipartite matching problem. Each example\u2019s prediction is calibrated by the likelihood of the prototypical clusters. They demonstrate significant improvements over contextual calibration (Zhao et al., 2021) on a wide range of tasks.\n",
            "strength_and_weaknesses": "Strength\n- Very strong and consistent empirical results from calibration. The performance is exceptionally strong for relatively smaller-sized models with fewer in-context learning examples. \n\n\nWeaknesses\n- There exist related works, though they do not necessarily claim to develop calibration approaches, they share the same spirit as this method to modify the output distributions for zero-shot or few-shot approaches [1][2]. Discussing these previous works and comparing results with them would help better position this paper. \n- Related to the previous point, I think the related work section does not present the best relevant paper, especially the first section. I suggest the authors conduct a more thorough literature review, along with the direction of calibrating language models. \n- I would also love to see if the calibration method works for masked language models given that MLMs have much stronger zero-shot performance on these evaluation datasets.\u2028\n\n[1] Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right \n[2] Noisy Channel Language Model Prompting for Few-Shot Text Classification\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written very clearly and easy to follow. The idea is novel and I believe it's easy to reproduce the results. ",
            "summary_of_the_review": "The paper has a novel idea for calibrating language models and shows strong empirical improvements over baselines. One concern is that the paper does not discuss a few critical related works or show empirical comparisons to these works. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_9WSu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_9WSu"
        ]
    },
    {
        "id": "4DOw5EI5Wr",
        "original": null,
        "number": 4,
        "cdate": 1667560560442,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667560560442,
        "tmdate": 1667560560442,
        "tddate": null,
        "forum": "nUsP9lFADUF",
        "replyto": "nUsP9lFADUF",
        "invitation": "ICLR.cc/2023/Conference/Paper2212/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper explores the importance of the decision boundary in few-shot learning in language models with GPT. Then, the authors propose an improvement over the existing few shot methods by adaptively learning this decision boundary. The decision boundary is learned by modeling the output distributions of GPT few-shot model as GMM and fitting to the class labels. The results shown are better than the baselines up to 13%. The ablation study is done by measuring the effect of template choices, class imbalance and class permutations. The robustness is measured by running with several random initializations.",
            "strength_and_weaknesses": "Strengths:\n\n* The experimental results and applicability are well supported by using multiple datasets and their ablation study. Substantial improvements are obtained in terms of the accuracies. \n\n* The idea of focusing on the decision boundary is well motivated and demonstrated. \n\nWeaknesses:\n\n* The label assignment part and multiple initializations and runs in the proposed algorithm can be computationally heavy as the authors mentioned. However, this might not be concerning since it is performed at the output space after the training.\n\n*  Why the authors chose 4 and 8 shot in their experiments? It looks limiting and not reasoned well. Is it a standard procedure for these datasets?\n\n* Although the authors mentioned and used averaging and careful initializations, the stability of the method is still a bit concerning to me. Too much unstable/sensitive methods are combined and dataset and task itself also poses another sensitivity issue by nature and since they are scarce. However, the results suggest otherwise unless it is 0 or 1 shot.\n\n* It would be good to explore the boundaries of the method in terms of the original dataset/labels and the desired dataset/task, and a notion of closeness between these tasks. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly and with a good writing quality. The experimental section is also extensive and well written. \n\nIn terms of the originality, it is on the incremental side since 1-the decision boundary problem in few shot is known in the literature, 2-all the methods are borrowed from the literature. However, the authors did a good job with the presentation of the decision boundary problem and the way the methods are used and context can be considered as novel. \n\nThe authors provide the code, I skimmed through the lines but I did not run their code. I would say that it was reproducible from my impression, but I cannot comment in a certain way.",
            "summary_of_the_review": "The authors propose a method where they learn decision boundary for an in context GPT few shot learning method. The motivation, story and the experiments parts are the strongest side of the paper. The ideas and results are plenty and well supported, which surpass the disadvantage coming from the computational complexity. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_uD7y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2212/Reviewer_uD7y"
        ]
    }
]