[
    {
        "id": "OHOcSUaECJ",
        "original": null,
        "number": 1,
        "cdate": 1666570427027,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570427027,
        "tmdate": 1669235040636,
        "tddate": null,
        "forum": "2EFQ_QlcPs8",
        "replyto": "2EFQ_QlcPs8",
        "invitation": "ICLR.cc/2023/Conference/Paper3816/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper leads a discussion on multi-vector document retrieval by introducing a sparse alignment perspective that can include serval previous document retrieval scheme. On top of that, the paper proposes an entropy-regularized sparse relaxation of the novel alignment scheme by separating the query-document token-wise similarity scores to a similarity times unary encoding of the query and the document . The advantage of that is to improve efficiency, avoiding costly computation on n*m matrix of the similarity. It is worth mentioning that the sparsity controls can be different between training and test. During training, the paper uses top-1 and during test, the paper find optimal sparsity using cross validation. ",
            "strength_and_weaknesses": "Strengths: (1) novel perspective on unifying previous multi-vector document retrieval and build the sparsified end-to-end scheme on top of them. \n(2) Good efficiency trade off on the sparsity side\n(3) Good results on the  BEIR benchmark\n\n\nWeakness: (1) entropy-regularized linear programming? The paper does not explain why  entropy-regularization on the linear programming is a desired choice. For enabling end-to-end training, a quadratic term would also work? \n(2) The paper does not explain why end-to-end training in the  entropy-regularization is necessary. During training, isn't the  model uses top-1 alignment where the only one document token is activated for each query token.\n(3) As the sparsity of the training and test can be different, it is good to investigating optimal training sparsity.\n(4) When training with top-1 alignment, the model looks similar to ColBERT except for the unary masking. It is good to analyze how and why the  unary masking is beneficial",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and of good shape with reasonable reproducibility. It will be good if the author publish training and test code.",
            "summary_of_the_review": "The paper provides a novel perspective on unifying various multi-vector document retrieval scheme under a same framework and proposed a sparse alignment on top of the framework. The idea is interesting and novel however is still with some under-explained concerns of mine",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_5rDb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_5rDb"
        ]
    },
    {
        "id": "whKTBhKPc1",
        "original": null,
        "number": 2,
        "cdate": 1666704542692,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666704542692,
        "tmdate": 1666709429950,
        "tddate": null,
        "forum": "2EFQ_QlcPs8",
        "replyto": "2EFQ_QlcPs8",
        "invitation": "ICLR.cc/2023/Conference/Paper3816/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Multi-vector retrieval models can fetch improved results on certain retrieval tasks, while at the same time the approach leads to large search indexes and expensive computation. Thus, this paper proposes ALIGNER, a novel multi-vector retrieval model that learns sparse pairwise alignments between query and document tokens, as well as the unary saliences of each token reflecting their relative importance to the retrieval. The authors show experimentally that controlling the sparsity of pairwise token alignments usually leads to significant performance gains. On the other hand, unary saliences determine whether a token needs to be aligned with other tokens for retrieval. With sparsified unary saliences, it is possible to prune a large number of query and document token vectors and improve the efficiency of multi-vector retrieval. In a zero-sample setup, ALIGNER scores 51.1 points nDCG@10 implements the latest state of the new retrieval-only retriever on 13 tasks in the BEIR benchmark test.",
            "strength_and_weaknesses": "Generally speaking, this paper puts forward a universal retrieval scheme, and achieves good results, the specific advantages are as follows:\n\n(1) The sparse alignment of multi-vector retrieval is a good solution to solve the retrieval effect and efficiency.\n\n(2) The new proposed model in this paper has achieved better results than the previous work in many tasks on MSMARCO.\n\n(3) The proposed model in this paper uses sparse alignment matrix to aggregate token-level similarity, where each element represents the alignment of a pair of tokens, which can develop different retrieval models in a unified way and identify the shortcomings of existing models.\n\nHowever, there is a lack of unified experimental standards and ablation experiments in this paper. So I also have a few doubts about this article:\n\n(1) This paper uses 6B T5-V1.1, but the previous baseline work only GTRxxl has the same size, while ColBERTv2 using multi-vector retrieval model has only 110m model size. However, GTR is a single vector retrieval model, so there is no unified standard to show that the effect of the proposed model in this paper is better than the previous model.\n\n(2) In this paper, similarity and alignment structures are proposed, but no ablation experiments have been carried out to prove the effectiveness of the improved model.\n\n(3) This model still needs to calculate the similarity matrix for queries and documents, so the computational efficiency is not improved compared with other multi-vector retrieval models in the training stage.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is well-written and the motivation is very clear.\nQuality: The quality of the article is good.\nNovelty: In general, compared with the previous multi-vector retrieval model, this paper introduces sparse alignment, which is a good attempt.\nReproducibility: It is can be implemented following the description in the paper.\n",
            "summary_of_the_review": "This paper proposes a sparse aligned multi-vector retrieval model, a novel model structure, which achieves good retrieval results and alleviates the shortcomings of previous work to some extent. Despite there is lack of corresponding ablation experiments, I think it is an overall good paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_4BA2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_4BA2"
        ]
    },
    {
        "id": "XVPa2Cl8yW",
        "original": null,
        "number": 3,
        "cdate": 1666706505900,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706505900,
        "tmdate": 1669390082184,
        "tddate": null,
        "forum": "2EFQ_QlcPs8",
        "replyto": "2EFQ_QlcPs8",
        "invitation": "ICLR.cc/2023/Conference/Paper3816/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In IR, this paper generalizes some neural interaction models like e.g. ColBERT, ME-BERT or COIL, by \"learning\" the alignement, as well as pruning the multi-vector representations. The model is shown to have a few-sample adaptability on the BEIR dataset.\n",
            "strength_and_weaknesses": "\nStrengths:\n- Learning the alignement allows to improve results of already successful models such as ColBERT.\n- Few-shot learning based on selecting the right alignment improve the results\n- Interesting sparsity regularizer with a target ratio (or number? This is not really clear) of tokens kept for the index\n\nWeaknesses:\n- The alignement is not learned\n- The sparsity is not really learned since during indexing the ratio is different ($\\beta$ vs $\\alpha$) and more importantly it is fixed - why this discrepancy?\n- Novelty is low (pruning has been proposed in ColBERTer)\n- The experimental comparisons lack rigor (compared models have 60x times less parameters, ColBERTer is not included in the experiments, some models could have been tested with the alignement adaptation)\n- No complexity analysis (comparing it to ColBERTv2 or ColBERTer)\n",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is mostly clear and well organized. There is some novelty although sparsifying is largely studied in IR.\n\nSection 3.2 (sparsity regularizer) is not well motivated - why not using a L1 loss (like in ColBERTer)? The authors should discuss and motivate much more this aspect since it could interesting for other works (but like it is, the approach is not really convincing). Also, why using two pruning ratios (for training and inference)? How were those ratios determined?\n\nIn the experiments,\n\n1) ColBERTer and/or ColBERTv2 should be compared with the approach (or at least, your entropy-based regularization should be compared with a simpler L1 one). Some related works cited in the ColBERTer paper are purely ignored, but quite related to what you are doing (e.g. Multi-Vector Attention Models for Deep Re-ranking, )\n2) An even simpler pruning strategy, random pruning, has not been experimented with\n3) Some extra-care should be taken to compare comparable models (in terms of parameters at least, a 60x ratio is not acceptable)\n\nOther issues:\n- Foonote 4: $H$ is not a point-wise entropy if defined that way - looks like the standard entropy (but $\\lambda_i$ does not define a probability distribution)\n- \"We further check\" (p. 7): this paragraph is not self-contained and cannot be understood without reading the appendix",
            "summary_of_the_review": "This paper proposes to (1) use different alignment patterns for few-shot learning (2) sparsify the set of vectors used to compute a document. There are some unclear parts and a lack of motivation (in particular for (2)), and experimental results lack some rigor in the comparison. The alignment few-shot learning could have been more interesting and innovative by looking at various patterns (and motivating them better).\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_sjAr"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_sjAr"
        ]
    },
    {
        "id": "1ziDHVa6PY",
        "original": null,
        "number": 4,
        "cdate": 1666720026004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666720026004,
        "tmdate": 1666720026004,
        "tddate": null,
        "forum": "2EFQ_QlcPs8",
        "replyto": "2EFQ_QlcPs8",
        "invitation": "ICLR.cc/2023/Conference/Paper3816/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors cast the multi-vector retrieval problem as sparse alignment between query and document tokens. They propose ALIGNER, a novel multi-vector retrieval model that learns sparsified pairwise alignments between query and document tokens (e.g. \u2018dog\u2019 vs. \u2018puppy\u2019) and per-token unary saliences reflecting their relative importance for retrieval. They show that controlling the sparsity of pairwise token alignments often brings significant performance gains. In a zero-shot setting, ALIGNER scores 51.1 points nDCG@10, achieving a new retriever-only state-of-the-art on 13 tasks in the BEIR benchmark.",
            "strength_and_weaknesses": "Strength\n1. The authors proposed a new way to learn sparsified pairwise alignments between query and document tokens.\n2. The unary saliences can significantly reduce the document token representations minimal performance loss.\n3. The method can achieve good performance on BEIR benchmark under the zero-shot setting.\n\nWeakness:\n1. No analysis on retrieval speed and index space.\n2. The size of the datasets seem not large enough. Most of the datasets have been pre-processed by some existing IR tools. It would be great if the authors could build index from scratch and compare to the method of \"Dense Passage Retrieval for Open-Domain Question Answering\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "What's the retrieval speed and index space? How the hyper parameters influence the speed and space?",
            "summary_of_the_review": "I think this is an interesting paper by introducing a new method of sparsified pairwise alignments, and achieve solid performance on benchmark IR dataset. It would be better if the author can discuss more details about the cost on speed/space and further compare with the setting in dense passage retrieval work.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_waGp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3816/Reviewer_waGp"
        ]
    }
]