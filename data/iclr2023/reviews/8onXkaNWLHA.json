[
    {
        "id": "7UQJjiuh6LJ",
        "original": null,
        "number": 1,
        "cdate": 1665863122663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665863122663,
        "tmdate": 1665863122663,
        "tddate": null,
        "forum": "8onXkaNWLHA",
        "replyto": "8onXkaNWLHA",
        "invitation": "ICLR.cc/2023/Conference/Paper4920/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a series of hyper-parameter changes (i.e. scale of the crop, frame sampling) and data filtering for learning representation from videos. The changes are implemented on top MoCLR. On the dataset and tasks tested the paper results show that video pretraining can match imagenet pretraining. However, the technical novelty is limited and the experimental results insufficient.",
            "strength_and_weaknesses": "Strengths:\n\n- Overall, the paper is well written\n- The results confirm that with careful considerations (by filtering the data and tuning the augmentations) video based pretraining can match in certain conditions imagenet pretraining.\n\nWeaknesses and questions:\n\nLow technical novelty:\n\n- the method itself is a direct adaptation/implementation of MoCLR.\n\n- The study of adapting the augmentation to videos, while reasonable, is straight-forward and its more of a hyper-parameter tuning than an actual contribution (e.g. setting the augm scale and frame sampling).\n\n- The data filtering pipeline addresses the distribution mismatch between ImageNet and the video dataset. It's not surprising that matching the data distribution will improve the performance and such pipelines, are common, albeit perhaps not for this very specific use case.\n\nInsufficient evaluation, ablations and comparisons:\n\n- As multiple frames are present in a video, counting all the frames, the filtered dataset is still 1 order of magnitude higher than imagenet. What is the performance when the same number o frames as that of the imagenet images are used?\n\n- Expanding on the prior question, what is the performance of a model trained with a single frame (most representative according to the imagenet classifier used for filtering)? This should hopefully emphasize how much the additional dynamic present in video helps.\n\n- No results and ablation with other backbones: how does the performance vary when the backbone is larger (e.g: ResNet-50-2x, ViT-B/L) and respectively smaller?\n\n- Current state-of-the-art for video recognition is achieved by initializing from pre-training on static images. How does the performance vary?\n\n- Insufficient evaluations: only object detection and segmentation considered. What is the performance on classification for example? What is the few-shot performance of such models?\n\n- How does this compare with imagenet-22k pretraining? How would the filtering perform based on these categories?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written generally and the hyper parameters alongside the implementation details are listed in the appendix.",
            "summary_of_the_review": "While the result itself is interesting, the evaluation is very limited, with insufficient ablation and comparisons. Moreover, on the technical side, the novelty is very low as the approach is a direct implementation of an existing technique with appropriate hyper parameter tuning for video data.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_zSuF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_zSuF"
        ]
    },
    {
        "id": "OqJRpqsYdtQ",
        "original": null,
        "number": 2,
        "cdate": 1666513262428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513262428,
        "tmdate": 1666513262428,
        "tddate": null,
        "forum": "8onXkaNWLHA",
        "replyto": "8onXkaNWLHA",
        "invitation": "ICLR.cc/2023/Conference/Paper4920/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to learn image-based representations from video. The paper makes two contributions: (1) on the network side, the authors introduce simple modifications to the way augmentations are applied (e.g. change the scale and uniform temporal sampling ) and a simple attention head for attention pooling to replace average pooling before the features are passed to the MLP. (2) on the data side, a new video dataset is proposed using a data curation process which tries to address the domain gap between imagenet and other video datasets. They evaluate on semantic segmentation and object detection after fine-tuning.",
            "strength_and_weaknesses": "On the positive side:\n+ The paper reports some results that might be useful for the community. \nOn the negative side:\n- The methodological contributions are quite thin, mainly comprising minimal changes to existing strategies for augmentation and attention pooling (which is also very standard).\n- The method is shown to work on the proposed curated dataset. Although the point of using a video dataset close to imagenet for a fair comparison is valid, in my opinion it also weakens the paper's contribution in a sense that learning from uncurated video data is a much more challenging and impactful goal. \n- Experiments: from the experiments (table 1) it is not clear how VITO performs when trained on other video datasets (like Kinetics or Youtube 8M). Similarly, it is not obvious how other previously proposed methods would perform when trained on the newly proposed curated dataset. I suppose a strong baseline would be also to train DINO on the new dataset.",
            "clarity,_quality,_novelty_and_reproducibility": "The papers reads very well. I have my concerns about novelty and significance of experimental validation as explained above.",
            "summary_of_the_review": "The paper reports some results that might be useful for the community, but I believe more work is required to increase the impact of the proposed method.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethics concerns",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_kndf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_kndf"
        ]
    },
    {
        "id": "RK-ZnXyW6ie",
        "original": null,
        "number": 3,
        "cdate": 1666645837837,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645837837,
        "tmdate": 1666645837837,
        "tddate": null,
        "forum": "8onXkaNWLHA",
        "replyto": "8onXkaNWLHA",
        "invitation": "ICLR.cc/2023/Conference/Paper4920/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors adapt existing contrastive learning approaches for self-supervised image representation learning to learning from video frames instead of static images. In particular, they first adapt the data augmentation steps, which are key to contrastive learning, to the video domain. Two main adaptations are proposed: first, the crop size is increased to account for the fact that, unlike ImageNet, in-the-wild videos are not centered on a single object, hence very aggressive cropping can make the task unnecessary hard. Second, they utilize a multi-scale attention pooling module to account for the fact that the frame content can shift significantly over time. Finally, they collect a new dataset which is automatically curated (using an ImageNet classifier) to match the ImageNet category distribution. They then experimentally demonstrate that, by combining these model and data improvements, self-supervised pre-training on video frames can come close to pre-training on ImageNet in terms of downstream performance on the tasks of semantic segmentation and object detection. ",
            "strength_and_weaknesses": "Strengths:\n\nThe paper is well written and is easy to follow.\n\nThe proposed approach is sound and its effectiveness is convincingly confirmed by an ablation analysis.\n\nThe final model trained on curated video frames archives similar performance to models trained on ImageNet in terms of downstream semantic segmentation and object detection accuracy.\n\n\nWeaknesses:\n\nThis is a purely empirical study, there is no technical contribution (all that techniques that are used have been introduced in the past).\n\nRelated work overview is incomplete. In particular, the authors ignore the whole line of work on space-time walks for self-supervised image representation learning from videos  which is highly relevant (e.g. Jabri et al., NIPS'21 but also prior works and follow ups).\n\nIn the experiments the authors do not control for the amount of training data. In particular, they have collected 1,180,042 videos each of which is at least 10 seconds long. How many frames is that? There is indeed a lot of redundancy between frames from the same video, but the redundancy is by far not 100%, especially in relatively long videos. For a fair comparison, the authors need to report results when only one pair of frames is sampled from each vides in a pre-processing step, so that the overall amount of data is comparable to ImageNet.\n\nMost importantly, the value of using videos as a source of training data for image-level contrastive learning is not convincingly demonstrated. The authors have to go to great lengths to simply match the ImageNet performance. The downsides of this approach are clear (e.g. much higher storage requirements) but the benefits are not obvious from the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the paper is well written and easy to follow.\n\nOriginality: the paper is novel in an experimental sense (the first to match ImageNet pre-training using video frames), but there is no technical novelty. \n\nReproducibility: the authors are planning to release the code, but not the data (which is collected from the internet). Hence, reproducing their results will be very challenging, but, strictly speaking, not impossible.",
            "summary_of_the_review": "This is a (for the most part) solid empirical study on adapting image-level contrastive learning methods to video frames. That said, a crucial factor is missing from the ablation analysis (control for the amount of training data) and the overall value of using video frames in this framework is not well justified. Moreover, related work overview has significant omissions. I hope the authors can address these concerns in the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_ACJo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_ACJo"
        ]
    },
    {
        "id": "iaOf-a3Z0ez",
        "original": null,
        "number": 4,
        "cdate": 1666677216305,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677216305,
        "tmdate": 1666678043323,
        "tddate": null,
        "forum": "8onXkaNWLHA",
        "replyto": "8onXkaNWLHA",
        "invitation": "ICLR.cc/2023/Conference/Paper4920/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a video self-supervised pretraining pipeline called VITO. It made several modifications over existing contrastive learning frameworks including larger crop size, improved temporal sampling scheme, and multi-scale attention feature pooling for the projector. The authors also investigated the data domain mismatch and propose a video curation procedure to further improve the model pretraining performance. Experimental results on semantic segmentation and object detection show that the proposed framework can achieve better results among methods pretrained on video datasets, and be competitive with the models pretrained on ImageNet.",
            "strength_and_weaknesses": "Pros:\n\nThis paper is well-organized and easy to follow. The technical contribution and the way to do data curation make sense to me. The ablation is comprehensive and the final results compared with other video-based model looks good, which shows the effectiveness of the proposed framework.\n\nCons:\n\nThe motivation of this paper is not very clear, as the results using video pretraining do not show superior performance compared with image pretraining based on the benchmark the authors chose. If the authors believe video is beneficial maybe choose another benchmark (like video segmentation or tracking) or at least combine ImageNet with the proposed VideoNet data to showcase better performance compared with the ImageNet baseline.\nFor other misc comments see the section below.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. it's not clear what `we sample from a uniform distribution over the entire video segment of length T = 2.56s` means, and I don't quite get how the newly proposed temporal sampling works actually. Would be better if the authors could consider rephrasing it for a better understanding.\nIt's also not clear to me why it is good that the mode of the distribution is at 0 (according to Figure A.1), wouldn't it be a better choice to make the distribution of time delays uniform?\n\n2. Using an ImageNet-pretrained model to do data curation is less preferable from my perspective. Although I agree that it might be necessary to do this in order to compete with the already-curated ImageNet data, this inevitably obscures the boundary between semi-supervised and self-supervised learning.\n\n3. [1] should also be discussed in related work as it is close to both topics the authors covered.\n\n[1] Xiong, Yuwen, et al. \"Self-supervised representation learning from flow equivariance.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.",
            "summary_of_the_review": "Overall I believe this paper makes novel technical contributions (using learnable attention gates to align features from different frames, pointing out that the video data quality actually matters a lot for existing models on several benchmarks, etc.). However, the evaluation part is my major concern, would be great if the authors could better motivate why video is important (although this might be shown in other works) and show either video+image could perform better than image only, or video pretraining learn better representations on some tasks compared with image-based and other video-based baselines. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_iRvv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4920/Reviewer_iRvv"
        ]
    }
]