[
    {
        "id": "iBoHeQZQXZ",
        "original": null,
        "number": 1,
        "cdate": 1666290345942,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666290345942,
        "tmdate": 1669730071411,
        "tddate": null,
        "forum": "kTqYrmqnUm1",
        "replyto": "kTqYrmqnUm1",
        "invitation": "ICLR.cc/2023/Conference/Paper4158/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a benchmark suite for hyperparameter optimization (HPO) specifically tailored to the problem of federated learning. They argue that the problem features several distinct characteristics, which are not accounted for in standard benchmarks, but should be. FedHPO-Bench was designed to alleviate this situation with the corresponding differences in mind. In order to increase the size of available federated HPO problems, the authors also present a way to generate a federated HPO problem from a standard one. In an experimental study, the authors show how standard and federated HPO approaches perform on federated HPO problems from the proposed benchmark.",
            "strength_and_weaknesses": "### Strengths\n* As federated learning becomes more and more important in practice, considering HPO for federated learning is an important research direction. The idea of having a corresponding benchmark is in general an important contribution. \n* Language-wise the paper is well-written.\n\n### Weaknesses\n* The overall structure of the paper is not very compelling. The authors spend quite some space on explaining the differences between the standard HPO and the federated HPO setting without properly defining the latter. There is a vague description of the overall process on Page 7 (directly before the experiments), which should be a) much clearer and b) much earlier in the paper to convince the reader that the two problems are truly distinct. Overall, I am missing a clear path to follow through the paper, which makes it not very convincing.\n* The points made by the authors regarding the differences between the standard HPO and federated HPO problem are not convincing to me. Each difference detailed on Page 3 can very well be modeled within the realm of standard HPO: \n    * Function evaluation in FL: The argument for FL algorithms being iterative is not well supported. Many standard ML models such as neural networks are also iteratively trained making FL not very different from it. However, we still tune neural networks using standard HPO methods. Similarly, the heterogeneity between FL clients is not well elaborated on. Why does this change the HPO problem in particular?\n    * Hyperparameter dimensions: New hyperparameters are not a reason for a new HPO formulation and benchmark. If this was the case, we would need a new benchmark with each new ML algorithm or variant thereof. Showing the figure in Appendix H.3.1 is not very convincing as different HPO problem instantiations (e.g. with different algorithms) also can have different shapes. Moreover, the figure is rather destructive to the overall motivation for the federated HPO problem as it suggests that the optimal region is in a very large plateau, which can easily be found by random search. In fact this observation is even corroborated in the experiments later on where the authors argue that none of the standard HPO approaches shows a significant improvement. In principle, this is also not such a new insight as something similar can also be observed for HPO problems in general [7].\n    * Fidelity dimensions: We can have different fidelity dimensions in standard HPO as well. Moreover, the argument conducted simply shows that some of the hyperparameters are correlated - an issure that we also have on standard HPO. \n    * Privacy: While this is certainly an important property to be analyzed, it seems that this is, once again, simply controlled via a (set of) hyperparameters, which could also be modeled in the standard HPO setting. Privacy simply seems to be a second objective suggesting that federated HPO is actually a multi-objective HPO problem. Unfortunately, this is never mentioned by the authors. \n    * Fairness: First of all, fairness often has a very different notion in the area of machine learning nowadays, which might be very confusing for readers of this paper. The authors should make much clearer, what they mean by privacy as was done in the corresponding references given by the authors. Second, the description here suggests that federated HPO should be multi-objective. \n    * Concurrent exploration: Once again, this is also something that exists in standard HPO and is in fact quite beneficial to perform in the standard setting. For example, [2] suggests a UCB based acquisition function, which can be used in concurrent exploration as done in D-SMAC [3]. \nOverall this makes the description of differences not very convincing. To be clear: I am very sure that the problem features several important characteristics, which make it inefficient to treat it using standard HPO methods. But this is simply not well conveyed by the authors. Most importantly, the overall setup, i.e. how the process works (description directly before evaluation) already suggests that tuning should NOT be performed as standard in the sense that a single function evaluation consists of several rounds between the clients and the master until convergence, but that the iterative process itself should be abused. In fact, this suggests the usage of multi-fidelity approaches or, if the tuning should be performed duringthe training of the federated approaches as it is suggested in this work, using methods from dynamic algorithm configuration [4,5]. The authors should really consider completely rethinking the way they present (a) the federated learning problem and (b) its differences to the standard setting.\n* It is unclear why the authors do not build upon existing benchmarks for federated learning such as [1] and why existing federated HPO methods have not been well benchmarked as claimed by the authors. Once again, this does not make a compelling point for the need for this work.\n* The evaluation is not very compelling. \n    * Firstly, RQ3 is raised in the main paper, but never again mentioned there. It is answered in the appendix, but this makes the main paper not self-contained. The authors should either answer the question within the main paper or not raise it there in this form in the first place, but completely defer it to the appendix. \n    * Secondly, I do not see on the basis of which federated HPO problems RQ1 is answered. While Figure 1 looks interesting, the results are not well put into context. As mentioned earlier, Appendix H.3.1 suggests that the problem can be solved well by random search. Correspondingly, the result \"Comparing these advanced optimizers with their baselines, only BOGP, BORF, and DE win on more than half of the problems but have no significant improvement, which is inconsistent with the non-FL setting.\" is not very surprising, which is not mentioned. Moreover, the second result \"Meanwhile, no MF optimizers show any advantage in exploiting experience, which differs from non-FL cases.\" cannot be verified from my point of view. It seems that some optimizers do work better than others at earlier stages of the optimization from Fig. 4. Other than that, I am not sure how you come to that conclusion from the results shown in the paper. \n    * Thirdly, the experiment to answer RQ2 is limited to only a single federated HPO task, which seems strange considering that the authors suggest a complete benchmark here, which should feature many more problems. Moreover, it is unclear which loss function is displayed in Figure 5 (validation of what loss?) making the figure meaningless as it cannot be properly interpreted. \n    * Fourthly, the evaluation completely lacks error bands or standard deviation values to put the result into context.\n* Important details regarding the benchmark are missing in the description. For example, what are the benchmark problems contained? How many problems does it contain? How exactly does it support the user in evaluating federated HPO algorithms? Which algorithms are already implemented? Will the benchmark be further extended by the authors? \n* There exist several notational and descriptional problems in the work. For example, the HPO problem description in the introduction is not great as the in the end one is not interested in optimizing the validation performance, but rather the test performance. In fact, one is actually interested in optimizing the out of sample error, which is approximated by the test error. Similarly, several symbols of Eq. 1 are not defined: What is S_{down}, S_{up}, B_{down}, B_{up}?\n\n## Minor remarks\n* Figure 1: text \"$\\lambda, b$ as query\" is too long and is displayed above another part of the figure\n* p.6: \"As a result, the learning dynamics of FL change, and thus different privacy budgets often correspond to different HPO objective functions\" -> I disagree. The objective function does not change with a different privacy hyperparameter, as the objective is always to maximize performance, as far as I can tell. What might change is your additional objective, if you explicitly make privacy an objective, but I do not read anything regarding multi-dimensional HPO in the paper.\n* p.6: model execution time calibration -> The authors should consider citing [6], where something similar is done to adhere to varying hardware. \n* Eq. 1: Several symbols undefined -> not possible to understand the idea from the main paper here\n    * What is S_{down}, S_{up}, B_{down}, B_{up}\n* p. 7 last sentence: unclear - what is \"#round\"? \n\n### Typos: \n* p.4: \"much helpful\" -> very helpful\n* p.7: \"it needs to synchronous\" -> \"it needs to synchronize\"\n\n[1] He, C., Li, S., So, J., Zhang, M., Wang, H., Wang, X., Vepakomma, P., Singh, A., Qiu, H., Shen, L. and Zhao, P., 2020. Fedml: A research library and benchmark for federated machine learning. arXiv preprint arXiv:2007.13518\n[2] Donald R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21(4):345\u2013383, 2001.\n[3] Frank Hutter, Holger H. Hoos, & Kevin Leyton-Brown. Parallel algorithm configuration. In International Conference on Learning and Intelligent Optimization, pages 55\u201370. Springer, 2012.\n[4] Biedenkapp, Andr\u00e9, et al. \"Dynamic algorithm configuration: foundation of a new meta-algorithmic framework.\" ECAI 2020. IOS Press, 2020. 427-434.\n[5] Adriaensen, Steven, et al. \"Automated Dynamic Algorithm Configuration.\" arXiv preprint arXiv:2205.13881 (2022).\n[6] Mohr, Felix, et al. \"Predicting machine learning pipeline runtimes in the context of automated machine learning.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.9 (2021): 3055-3066.\n[7] PUSHAK, YASHA, and HOLGER H. HOOS. \"AutoML Loss Landscapes.\" ACM Transactions on Evolutionary Learning and Optimization (TELO) (2022).\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality\nAs the list of weaknesses shows, the quality of this work is not convincing. I believe in the general idea, but the overall quality of the paper itself is not at a stage where it should be published.\n### Clarity\nLanguage-wise, clarity is great. I had no problem understanding any point made. However, considering the overall story of the paper, clarity is not great. As written earlier, the arguments conducted by the authors are not compelling. I suggest to completely rewrite the paper following a clearer structure to make clearer a) what the federated HPO problem is, b) why and how it is different from the standard HPO one, c) why different HPO approaches are needed, d) why a special benchmark suite is needed and e) how the benchmark looks like exactly.\n### Novelty\nAs this seems to be the first HPO benchmark tailored to federated learning, novelty is great. \n### Reproducibility\nReproducibility is, in general, guaranteed as the code of the benchmark and evaluation is released. That being said, I would like to see some more details given in the actual evaluation section of the main paper.\n",
            "summary_of_the_review": "Although I very much like the idea of investigating the HPO problem for federated learning, I have to suggest a rejection of the paper for the following reasons: \n\n1. The overall story of the paper is not well written making it unclear a) what the federated HPO problem actually is and in how far it differs from the standard HPO problem, b) why existing HPO methods are not well suited, c) why this benchmark is needed and what it offers in detail.\n1. The evaluation presented in the paper has several flaws, which should be fixed.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_BCqK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_BCqK"
        ]
    },
    {
        "id": "UsfzBxKVP5b",
        "original": null,
        "number": 2,
        "cdate": 1666556034276,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666556034276,
        "tmdate": 1666556034276,
        "tddate": null,
        "forum": "kTqYrmqnUm1",
        "replyto": "kTqYrmqnUm1",
        "invitation": "ICLR.cc/2023/Conference/Paper4158/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new benchmark suite for federated hyperparameter optimization (FedHPO), called FedHPO-bench. FedHPO-bench focuses on three main desiderata, which are: (1) comprehensiveness -- which is the diversity of tasks; (2) flexibility -- which is the customizability of the federated environment; and (3) extensibility -- which is the ease of evolving the benchmark suite. The paper demonstrates the usefulness of the proposed benchmark suite on a substantial collections of experiments and through which reveals interesting insights about FedHPO.",
            "strength_and_weaknesses": "Strength: The paper is well motivated. The proposed software has great practical values and is potentially impactful in the growing field of Fed Learning. While I believe there is much more to be done, the current implementation might be sufficient as a prototype.\n\nWeakness: My main concern is that the empirical studies are not very well designed. The authors mentioned that there are three desiderata for a good benchmark suite. The experiments should focus on demonstrating these three desiderata. The insights described in Sec 4 are good to know, but not entirely relevant. My specific comments regarding this are as follows:\n\n-- Comprehensiveness: In my opinion, this is the most well demonstrated desiderata. The authors show in Fig. 2 a wide range of hyperparameters-performance distributions, obtained over different categories of model types. But these are confined to the vanilla FL setting. It would be much better if the authors can demonstrate on other interesting settings, such as increasing/decreasing the heterogeneity of the federated tasks. For example, I would expect that increasing the heterogeneity will make the ecdf curves of most datasets be more like that of the PubMed dataset (i.e., harder to find a good configuration that works well on all clients). Many existing papers on non-IID FL have done this study before, so I believe it would be useful if this feature is implemented as an environment parameter that users have control over.\n\n-- Flexibility: None of the experiments really demonstrate how easy it is to use the proposed software to generate interesting experiments. Varying the heterogeneity (as pointed out above) would be a great feature to have (and to conduct demonstration for). In addition, I also find it unintuitive to customize the objective function (to reflect different privacy budget/fairness level -- Sec 3.2). Personally, I think it is better to separately implement DP and fairness metrics and let the users decide which objective function to use. As the system is designed to be extensible, the dev team can incorporate implementations of privacy and fairness preserving FL objectives later. \n\n-- Extensibility: This is arguably harder to demonstrate, and I think the description in Sec 3.3 did a fairly good job to convince me that the bench can be extended in principle. My concern is that while three concurrent exploration methods were implemented (FedEx, FTS, personalized FedEx), it seems that only FedEx was demonstrated in the experiments. In addition, does the unification result in other meaningful concurrent exploration methods by varying the sync, update & aggr operations? I would recommend the authors trying several sensible configurations and show that they yield decent results (not necessarily better) to confirm that the unification is indeed useful. That will also demonstrate the extensibility of the bench. Otherwise, it seems pointless to unify these methods.\n\nOther concerns:\n\n-- The authors should make clear the distinction between HPO and FedHPO in the problem setting (Sec 2.1 or beginning of 2.2). Is the goal learning one HP setting for all clients or a personalized setting for each client?\n-- I do not get the discussion about the fidelity dimensions (Sec 2.2). I understand what they are and they seem like parameters of the federated environment, which FedHPO-bench should give users control over (i.e., flexibility). I also do get that it is interesting to demonstrate the effect of varying these parameters (which is done in Appendix G), but it is unclear from the description whether FedHPO-bench does have this customization feature? If so, the authors should explicitly state it.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Generally quite clear, except for a few details pointed out above.\nQuality: Straight forward and useful idea. Potentially will have great practical impact. However, the empirical study is not quite relevant.\nNovelty: As far as I know this is the first step in this direction. The unification is interesting, but needs to be better demonstrated.\nReproducibility: The authors released a github repo. (minor note -- I would recommend separating the appendix from the main submission to follow the ICLR guideline). I'm convinced that it can be reproduced.",
            "summary_of_the_review": "I recommend a marginally below acceptance for now, because I'm not convinced by the empirical demonstration. I hope that the authors can provide me with good reasons to upgrade my score, because the proposed bench seems like a fundamental contribution if executed well.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_eSy6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_eSy6"
        ]
    },
    {
        "id": "VumUnWodUh",
        "original": null,
        "number": 3,
        "cdate": 1666597856212,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666597856212,
        "tmdate": 1666597856212,
        "tddate": null,
        "forum": "kTqYrmqnUm1",
        "replyto": "kTqYrmqnUm1",
        "invitation": "ICLR.cc/2023/Conference/Paper4158/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduced a new benchmark suite for Federated HPO called FedHPO-Bench. The FedHPO-Bench is considered distinct from other HPO benchmark suites such as HPO-Bench because of the inherent requirements for FL algorithms.\n\nThe paper first identified the need for a FedHPO benchmark suite because of its distinct separation between client and server hyperparameters, and then provided some results to evaluate the performance of their benchmark suite. The analysis of the performance differences between separated client and server hyperparameters and standard HPO algorithms defined the majority of the reasoning behind the separation of HPO and FedHPO, and the corresponding need to create their benchmark suite. The authors clearly described this distinction, but did not provide convincing reasoning behind the need for their benchmark suite. \nAs stated in this paper, the benchmark suite is very closely related to HPO-Bench, and does not appear to provide a truly novel solution. Instead, it appears as a small addition to an already existing technology.",
            "strength_and_weaknesses": "trengths: This paper clearly described the the current FL landscape. \nThe explanation of its extensibility, and the experimental results give a clear idea of the given justification of FedHPO-Bench as a novel research path.\n\nWeaknesses: There were some instances throughout the paper where information was not put forth in an easily understandable format. For instance, Equation 1 provides could be summarized by saying that the total time is bounded by the summation of the slowest client\u2019s computation, the server\u2019s computation, and data transfer speeds. Figure 1 also introduces another issue like this, wherein the interface design is difficult to understand. Also, on page 19 (Appendix E), citing Wikipedia isn't a usable source for an academic conference due to the nature of the source of its information.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper clearly stated their contributions to the field, the setup of their experiments, and the background surrounding their research.\n\nThe paper maintained academic rigor in their experimental evaluation, setup, and the consideration of the factors specific to FL.\n\nThe originality of this work is clear, as they introduce a small, although distinct, contribution of a new HPO benchmark suite tailored for FL algorithms. That being said, this work bears an overwhelming similarity to HPO-Bench, and generally presents itself as HPO-Bench ported to FL rather than an entirely novel benchmark suite.",
            "summary_of_the_review": "I would not recommend this paper for acceptance due to the fact that it does not provide a novel enough contribution to the field. Although it may definitely improve benchmarking for FedHPO, it is specific to only a single field, and it provides few novel aspects of HPO benchmark suites in general.\n\nWhile I can see how there may be some need for a solution to easily benchmark FedHPO algorithms, I remain unconvinced that this is novel enough to warrant publication.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_QKdH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4158/Reviewer_QKdH"
        ]
    }
]