[
    {
        "id": "Y5Hlb8-r8EX",
        "original": null,
        "number": 1,
        "cdate": 1665742575843,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665742575843,
        "tmdate": 1669335900003,
        "tddate": null,
        "forum": "uzbd9jGCnN4",
        "replyto": "uzbd9jGCnN4",
        "invitation": "ICLR.cc/2023/Conference/Paper921/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper empirically investigates the phenomena of double descent and offline-online correspondence (the gap between test errors of offline and online learners, as far as I understood) for similarity learning. The authors introduce two types of dataset conversion procedures from data points to pairs and two types of noise-generating processes to see how these factors affect generalization in similarity learning. These data-generating processes are expected to imitate well actual procedures in contrastive learning with data augmentation and negative sampling. After all, the authors discover for double descent that a type of noise---called pair label noise (PLN)---affects generalization more significantly because it breaks the label \"transitivity,\" making the interpolation/overfitting harder. In addition, the authors reveal that offline learners are more prone to label noise than online learners in the empirical study of offline-online correspondence.",
            "strength_and_weaknesses": "### Strengths\n\nThis work extends the concept of double descent from well-studied classification and regression to similarity learning. Many recent deep learning training procedures partly rely on self-supervised pre-training models that leverage contrastive learning, hence it is essential to understand more what kind of regime goes on under the over-parametrized regime in similarity learning. The authors provide one perspective to this answer to reveal that pair label noise has a severe effect on the generalization aspect. This should be an interesting perspective for researchers studying generalization.\n\n### Weaknesses\n\nThe main drawback of this paper is the unclearness of the motivation and writing. I provide several writings that could be improved in the \"Clarity, Quality, Novelty and Reproducibility\" section and focus on the motivation aspect here.\n\nAlthough research on double descent/interpolation/over-parametrization regime has been gaining attention in the recent community, I could not understand well what is the motivation of this work. In the study on double descent, we are primarily interested in how model performances degrade and improve as the model size increases, and there should not be label noises. Specifically, we usually consider the data-generating process $Y_i = f(X_i) + \\\\epsilon_i$ for regression and treat $\\\\epsilon_i$ as \"white noise,\" but do not consider \"more drastic (label flipping) noise\" to purely focus on the clean generalization aspect. The current study seems to entangle labeling noise and the generalization aspect, so it is not sufficiently clear that the authors would like to focus on generalization or the effect of the noise.\n\nIn addition, the authors introduce two types of noise models (SLN and PLN) and two scenarios (sparse and dense connections). However, I did not understand well the reason why the authors introduce them. Are these introduced to imitate the real-world data-generating process in similarity learning? Or, are these introduced to make the data-generating process more scalable (in particular, the sparse connection)? This part makes me puzzled so I'm not sure the authors would like to study/reveal a certain phenomenon or propose a new data sampling procedure.\n\nAs for SLN and PLN, if we follow the initial motivation---to study the generalization aspect of contrastive learning with data augmentation and negative sampling---then these noise-generating processes could be overly simple. At least, in real-world scenarios, SLN and PLN could happen simultaneously. Why do we consider these two noises independently?",
            "clarity,_quality,_novelty_and_reproducibility": "Here, I make a list of unclear points in the paper. I hope this helps the authors to improve the manuscript.\n\n- (In the abstract, introduction, and several places) The authors say \"NNs generalize the concept of similarity\" occasionally, which is confusing because the verb \"generalize\" in the machine learning context should be an intransitive verb. I guess an alternative way to express what the authors initially thought could be \"NNs generalize in similarity learning,\" which causes less misunderstanding.\n- (In the abstract) The authors state \"SLN outperforms PLN in the overparametrized region,\" which sounds like the authors propose a new noise model to achieve better performance by injecting noises on purpose, causing a misunderstanding regarding the aim of this paper.\n- (In the introduction; 1st paragraph) The connection between the online-offline learning correspondence and double descent is not sufficiently understandable from the current sentences. In particular, the last part \"overparameterized models (trained on a finite number of samples) and underparameterized models (trained on very large datasets)\" would be ambiguous. In addition, I would appreciate if the authors state slightly more about why the topic drifts to the online/offline correspondence from the double descent in the middle of this paragraph.\n- (At beginning of page 2) The sentence \"online/offline training compares the network performances varying the quality (diversity) of the dataset\" puzzles me. What is the quality (diversity) of the dataset in this context and what is the related concept in the latter part of this paper? Does online/offline training itself vary the quality?\n- (At the bottom of page 3) I failed to understand what the phrase \"dataset quality and pairs topology\" indicates. Does the \"pairs topology\" refer to SLN vs. PLN?\n- (In Section 2) In the data-generating process introduced in the 2nd paragraph of Section 2, the authors choose to generate positive pairs within the same class in a \"chain\" manner, but why is it? It does not seem to represent a realistic data-generating scenario.\n- (In Section 2.1) What is the purpose to introduce two scenarios, sparse and dense connections? It looks like the authors intend to accelerate data sampling with these two new scenarios, but do they represent a realistic scenario? I am not sure whether they are just a proxy model to a real scenario or a proposed sampling method for some goals.\n- (In Section 3) The last paragraph of page 7 may be excessively long. It can be split into several pieces because it contains several important topics.\n- (In Section 3; the last paragraph of page 7) In the last part, the authors claim \"we need to take into account the number of possible configurations\" but I failed to understand what is \"possible configurations\" and for what purpose we *need* to think about it. In addition, I am not sure what the last equation shown in page 7 calculates (partly because of some undefined notations like $N\\_{\\\\mathrm{eq.pairs}}$).\n- (In page 8; the latter part of the 1st paragraph in \"Online vs. offline\") Could you explicate why an online/offline correspondence is supported by *showing that the bootstrap error is small*?\n\nI feel slightly sorry for pointing out structural and grammatical issues too much but could not avoid doing so because these issues hinder me from understanding the main scope of this work. I believe improving these aspects must make the paper more attractive.\n\nIn addition, while looking at the experimental results, I am not confident enough that \"the PLN peaks appear to be shifted to the right-hand side, hitting that PLN data are harder to interpolate\" (stated in the middle of page 7) from Figure 1 (bottom right). Both peaks of SLN and PLN range over similar regions and the difference does not look significant.",
            "summary_of_the_review": "I think this paper reveals an interesting contrast between the behavior of SLN and PLN in regard to the double descent phenomena. Nevertheless, several parts of the paper would not be sufficiently clear so the scope of the paper is not easy to grasp. There would be room for improvement in the structure and logical flow of the paper as suggested in \"Clarity, Quality, Novelty And Reproducibility\" section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Not applicable",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper921/Reviewer_JwMY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper921/Reviewer_JwMY"
        ]
    },
    {
        "id": "Nr2fVrfK34I",
        "original": null,
        "number": 2,
        "cdate": 1665978881502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665978881502,
        "tmdate": 1665991573659,
        "tddate": null,
        "forum": "uzbd9jGCnN4",
        "replyto": "uzbd9jGCnN4",
        "invitation": "ICLR.cc/2023/Conference/Paper921/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies how deep networks generalize the concept of similarity in the presence of noise. Specifically, two phenomena are studied (1) double descent behavior and (2) online/offline relations. The double descent behavior also exhibits in the cases of contrastive learning with noise. Besides, the equivalence between online optimization and offline generalization is probed. An extensive empirical study is provided to justify the paper's claims. ",
            "strength_and_weaknesses": "**Strengths**\n- The research problem is interesting and significant. \n- Experiments are sufficient to support claims. \n\n**Weaknesses**\n- The main contributions of this paper are unclear. \n- Technical novelty is somewhat limited. \n- Both writing and organization should be improved to reach the requirements of a top-tier conference. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity. At the present stage, there are some concerns that need to be addressed: \n- The paper claims that \"SLN outperforms PLN in the overparametrized region in dense datasets\". Both SLN and PLN are noise. Does the paper mean that the SLN noise is less harmful in this case?\n- For claims \"online and of\ufb02ine test soft-errors match each other for classi\ufb01cation tasks under certain conditions\", is there some evidence for this statement?\n- Perhaps, it is better to stress similarity learning but not contrastive learning, since the paper mainly exploits SNNs. \n- What are \"re\ufb02exive, symmetric, and transitive properties\"? Could the paper provide more formal definitions for these properties?\n- The paper shows that the interpolation threshold (training error = 0) cannot be achieved in some cases. Is this because the network capacity is not large enough, because prior works, e.g., [1] show that deep networks can fully fit any given noisy labels? \n- The double descent phenomenon in learning with label (label-pair) noise is not very exciting. In fact, prior works such as [2] have presented this phenomenon through training dynamics. Intuitively, since clean labels are dominant in noisy classes, the double descent phenomenon exists when we train deep networks on noisy data. \n- Could the paper provide more discussions about why an online/offline correspondence is supported by showing that the bootstrap error is small? It is confusing to me.\n\nQuality. The quality of this paper should be improved, although the presentations in the experimental parts are admirable. \n\nNovelty. The conceptual and technical novelty is not prominent.\n\nReproducibility. The reproducibility is good. The paper provides detailed descriptions of implementations.",
            "summary_of_the_review": "This paper studies an important problem. Extensive empirical evidence is provided. However, at the present stage, there are still some unclear explanations that need to be addressed. Therefore, the rating is \"5\" before the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper921/Reviewer_egc1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper921/Reviewer_egc1"
        ]
    },
    {
        "id": "NRz5ivcdAWJ",
        "original": null,
        "number": 3,
        "cdate": 1666627376221,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627376221,
        "tmdate": 1669739457918,
        "tddate": null,
        "forum": "uzbd9jGCnN4",
        "replyto": "uzbd9jGCnN4",
        "invitation": "ICLR.cc/2023/Conference/Paper921/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies generalization in similarity learning with noisy labeled data focusing on SNNs. Empirical studies were designed to show that dataset topology crucially affects generalization under SLN and PLN, and both the network architecture and the loss function choice can disturb an online/offline correspondence for similarity tasks.",
            "strength_and_weaknesses": "This work studies how NNs generalize the concept of similarity in the presence of noise by investigating two phenomena: Double Descent (DD) behavior and online/offline correspondence, which is interesting.\n\nBelow are some concerns:\n1. The authors stated that it is widely known that larger models (with more parameters) usually obtain better generalization. However, in my point of view, over-parameterized models usually have the overfitting problem, leading to bad generalization ability.\n2. What are the limits in `this framework connects under- and overparameterized limits.'\n3. I am confused about this sentence `the training dataset size dictates the two regimes instead of the number of parameters'. Do you mean ``the training dataset size instead of the number of parameters dictates the two regimes''?\n4. What do you mean by ``online and offline test soft-errors match each other ''?\n5. I don't see why DD and online/offline correspondence are two **complementary** approaches in terms of generalization. Can you provide some explanation on the **complementary**?\n6. ``DD and online/offline correspondence were mainly applied to classification and regression, but, if true, they should also hold for other tasks such as similarity learning.''. What do you mean by `if true'?\n7. What is DIBS? What do the gray shaded areas stand?\n8. So many special terms are mentioned without definition, e.g., effective noise, which make it hard for readers to understand this paper.\n9.  The density of this graph is defined as $\\left|N_{\\text {pairs }}\\right| /\\left(\\begin{array}{c}\nN \\\\\n2\n\\end{array}\\right)$ \", which tells us how much information we have about the input images. The density is proportional to $1/N$. How do you maximize the information by constructing all possible pairs (increasing N)?\n10. In scenario 1, how do you determine $N_{sample}$?\n11. Can you provide some theoretical analysis on the generalization in similarity learning with two kinds of label noise under two scenarios in terms of DD and online/offline correspondence?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Overall this paper is not well-written. The introduction section makes up half of the whole article, which is too long. The position of the figures is not well designed. I don't see why figures of totally different meanings are combined together. As pointed out above, many sentences are hard to read.\n\nQuality: This work is not theoretically evaluated. The empirical study is a little bit rough.\n\nNovelty: The main claim, i.e., the noise affects the generalization of similarity learning seems to be obvious.\n",
            "summary_of_the_review": "To sum up, the contributions of this paper are limited. It would be better if the authors can provide some theoretical analysis and refine the writing of this work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper921/Reviewer_UiBx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper921/Reviewer_UiBx"
        ]
    }
]