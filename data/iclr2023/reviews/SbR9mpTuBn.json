[
    {
        "id": "QUuKMHj7khj",
        "original": null,
        "number": 1,
        "cdate": 1666641606198,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641606198,
        "tmdate": 1666641606198,
        "tddate": null,
        "forum": "SbR9mpTuBn",
        "replyto": "SbR9mpTuBn",
        "invitation": "ICLR.cc/2023/Conference/Paper1638/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the important problem of learning text-to-speech synthesis from noisy unpaired data. To order to achieve this, the authors propose several practical tricks, including normalizing variable and noisy information in speech data, curriculum learning,\nlength augmentation and auxiliary supervised learning. Experimental results on public datasets confirm the effectiveness of the proposed tricks.",
            "strength_and_weaknesses": "Strength:\n\nThe main contributions are clearly stated and supported by the experiments.\n\nMajor works on the similar topic are widely covered and referenced.\n\nThe samples are quite convincing.\n\nEvaluation is thorough enough to support the arguments with in-depth ablation studies.\n\nWeaknesses:\n\nThe complicated training algorithm makes the proposed method hard to reproduce. It would be helpful if the authors can provide brief guidelines such as how to tune the hyper parameters. It seems that the performance of this system heavily depends on the performance of the self-supervised voice conversion. It is better to compare the proposed VC approach with SOTA VC models such as YourTTS, although YourTTS is trained with the need of text scripts.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The contributions are somewhat new. Aspects of the contributions exist in prior work.\n",
            "summary_of_the_review": "Overall I think this is a good paper, that is likely to prove quite useful for the development of zero-shot or few-shot text-to-speech synthesis solution.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_sG8y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_sG8y"
        ]
    },
    {
        "id": "Y8xgy2hSTj9",
        "original": null,
        "number": 2,
        "cdate": 1666642871454,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642871454,
        "tmdate": 1666642871454,
        "tddate": null,
        "forum": "SbR9mpTuBn",
        "replyto": "SbR9mpTuBn",
        "invitation": "ICLR.cc/2023/Conference/Paper1638/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper improves upon the unsupervised speech synthesis. The motivation of such work is to serve to low resource languages where the supervised approach might not be feasible due to lack of annotated data.\n\nThe paper proposes and implements several modifications (bag of tricks) to the existing methods to improve the performance. The tricks are: the variational normalization to tackle the noisy information; the curriculum learning; non-autoregressive TTS. \n\nThe main contribution of the paper is that they present a high-quality method for unsupervised TTS that allows training without the lexicon.  ",
            "strength_and_weaknesses": "# Strengths\n\n- Despite the complexity of the methods for TTS, the paper is able to clearly explain prior and the proposed methods.\n- The motivation of the work is clear and introduced well\n- Judging by the samples, the model performs very well\n\n\n# Weaknesses\n\n- I found the ablation study unconvincing: the MOS error bars intersect",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clearly written and easy to follow. Nevertheless, due to the large number of moving parts, when reading for the first time I was a bit confused about all the method applied. Not sure how to fix this. Then, some images are hard to read when printed in black and white.\n\nThe paper seems to have high quality, the methods are motivated well. The approach is sound. I am slightly unconvinced by the ablation studies. The Tab. 3 shows that many of the error bars on MOS intersect making rows 1, 5, 6, 7 identical as well as rows 2, 3, 4. What are the error bars for the CER and WER? The same is true for Tab. 4, it seems that all the error bars intersect here.\n\nThe paper is novel enough. While the paper builds upon the previous work, there are important changes and a clear end goal. From listening to the samples, I am convinced that the proposed method sounds better than the baselines.\n\nIt should be possible to reproduce the paper. But I still encourage the authors to release the source code of the paper.\n\n# Typos\n\n- Indonesian and French share some Latin(?) alphabets\n- p.4 variable information\n",
            "summary_of_the_review": "In summary, while there are some minor issues with the experimentation and the writing, the paper is very clearly written and contains some important results. I am convinced that the paper is well suited for the conference. Accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_G8SV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_G8SV"
        ]
    },
    {
        "id": "KIL2jGUwhX",
        "original": null,
        "number": 3,
        "cdate": 1666652752049,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652752049,
        "tmdate": 1666652752049,
        "tddate": null,
        "forum": "SbR9mpTuBn",
        "replyto": "SbR9mpTuBn",
        "invitation": "ICLR.cc/2023/Conference/Paper1638/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an unsupervised TTS pipeline that is achieved by assembling several pre-trained models and various methods in a pipeline, along with a few new tricks, to achieve impressive unsupervised TTS results.\n\nThe main contributions of this paper is examining different aspects of the proposed pipeline and quantifying the contributions of each component, as well as showing that when properly integrated together, this pipeline can achieve impressive results on the setup chosen by the paper.",
            "strength_and_weaknesses": "Strengths:\n- The final results are very good, especially for Indonesian\n- There is an extensive ablation study showing the effectiveness of each component\n- There are some innovations, such as the focus rate F\n\nWeaknesses:\n- The method is quite complex, combining several pre-trained models together with complex techniques, though it may be justified given the results achieved\n- Many of these tricks can also be applied to unsupervised ASR (thus improving unsupervised TTS based on these methods), such as voice conversion, access to a high quality rich resource language close to the target language, etc, and these drive significant improvements in accuracy of this method, as shown in table 3. While the authors have made attempts to make use of these methods for the baselines (as described in appendix C2), it is not very clear how effective these adaptations were without any ablation studies, nor is it clear how much effort the authors put in to make sure the baselines were modified in a an optimal way.\n\nOther:\n- While using the focus rate F to judge the quality of back-translation results is interesting, is there any study on how well F maps to quality (e.g. using a paired dataset for evaluation of F)?\n- In the given setup, speech and text are from same domain (CommonVoice dataset). Does this method still work if text and speech are from different domains? e,g, conversational speech + wikipedia text\n",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the clarity and quality of the presentation are good and easy to follow. The code is apparently available as well, which should make reproducibility straight forward. In terms of novelty, while this work achieves most of its results mostly by composing other methods and models, there actual composition itself to achieve the stated results is not trivial and the analysis is valuable.\n\nnit: table 3 caption references LM which is not in the table, but not Aug which is in the table",
            "summary_of_the_review": "This paper proposes a pipeline method to perform unsupervised TTS that does not rely on pseudo-labels produced by unsupervised ASR. However it does rely on several pre-trained models to achieve good results (as seen in ablations) which are pipelined together to build the final system. The main value of this paper, in my opinion, is showing how each component in the pipeline contributes to the final quality of the system and, together with released code for reproducibility, is certainly valuable to the community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_oNy6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1638/Reviewer_oNy6"
        ]
    }
]