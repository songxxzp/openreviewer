[
    {
        "id": "kS01v2iBtm5",
        "original": null,
        "number": 1,
        "cdate": 1666174667104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666174667104,
        "tmdate": 1669484496726,
        "tddate": null,
        "forum": "TC39w69m8bB",
        "replyto": "TC39w69m8bB",
        "invitation": "ICLR.cc/2023/Conference/Paper1970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes to train low-rank CNNs from scratch. To achieve this, the authors propose Tucker-2 decomposition with low-rankness, and improve the training with orthogonal weight regularizations.",
            "strength_and_weaknesses": "Strength:\n1) The paper is well-written and easy to follow.\n2) Questions in section 4 is well-motivated.\n3) It is good to have intensive experiments and latency measured on embedded GPUs.\n\n\nWeakness:\n1. The analysis for the question1 is not convincing. 1) There could be other formulations. For example, what if one perform SVD on each filter, instead of on the reshaped 2D weight matrix? 2) It would be better to show the averaged approximation error across all layers, instead of picking some specific layers.\n2. The analysis for question 3 is not novel. The original DSO method is not developed in this work. The authors just reiterate orthogonal regularizations from Bansal et al. (2018).\nThe two bullet points make Algo. 1 is not novel.\n3. No ablation studies on the regularization scaling factor $\\lambda_d$.\n4. Paper writing: 1) I think the authors used too many underlines, making it hard to find out which part did the authors really want to emphasize. 2) Minor issues: on the 4th row on page five, it should be \u201close\u201d, not \u201closs\u201d",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is well-organized and motivated.\nI think the novelty is very limited. Although the authors provide sufficient analysis to motivate their choices, the core methods are inherited from previous works.",
            "summary_of_the_review": "The authors provide sufficient analysis for their design choices and conduct intensive experiments with hardware measurements.\nHowever, as discussed above, my main concern is the core novelty of the method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_yVpF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_yVpF"
        ]
    },
    {
        "id": "WW9QZWyf_8b",
        "original": null,
        "number": 2,
        "cdate": 1666675089167,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675089167,
        "tmdate": 1666675089167,
        "tddate": null,
        "forum": "TC39w69m8bB",
        "replyto": "TC39w69m8bB",
        "invitation": "ICLR.cc/2023/Conference/Paper1970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "While there have been many studies using Low-rank approximation (compression) method for pre-trained models, this paper proposes a new training method from scratch using tucker decomposed models w/o full pre-training steps. Low-rank training from scratch has been conventionally viewed as an insufficient method, which can lead to lower accuracy, but this paper shows comparable accuracy with 2x-3x reduced FLOPs (maybe by adopting SO regularizer). ",
            "strength_and_weaknesses": "Strength\n- The proposed method, ELRT, can reduce training cost because the original networks are already decomposed.\n- Unlike conventional thinking, they show that low-rank training from scratch can lead to comparable accuracy for image classification models (ResNet/MobileNet).\n- They also show accelerated results on various H/W (V100 GPU, FPGA, ASIC, Jetson).\n\nWeakness\n- Most experiments are performed on the CIFAR-10 dataset (even including the VGG-16 model). It can show a method is working well, but it doesn\u2019t mean that the method is novel or works well for many NNs for practical applications. Because the key to improved accuracy seems to be adopting a regularizer to the current training system, training redundant models may cause much controversy. To prove the effectiveness of a method, the CIFAR-10 results and ResNet-50(w/ ImageNet) results are not adequate because this model may be not fully regularized. \n- There is only one ImageNet result w/ ResNet-50. How about ResNet-18 or other compact networks on ImageNet? \n- I think acceleration results are a good way to prove our compression methods are effective and practical. But, how about smaller models including ResNet-18, MobileNet, EfficientNet and so on? Decomposed layers w/ small weights can be not accelerated compared to dense models in highly-parallelized computing systems. \n- About Question 1, I have a concern on Figure 3 with MSEs. According to the analysis on MSE, tucker decomposition is selected as a main approach of this paper. However, how can we judge which training system from scratch is better by measuring the MSE of compression? This method is performed w/ decomposed layers and there is no compression step.\n- I think there is a lack of explanations and reasons that the orthogonality of U^1 and U^2 can lead to better NNs. But, I\u2019d like to follow other reviewer\u2019s opinions in this aspect.",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in weakness, I think some explanations should be supported by SO regularizer and question 1. I also have a concern about the novelty of a decomposed network from scratch. We can judge the success of a training system by using decomposed layers when there were already full-trained models. But if it is the first time to build NNs w/ new dataset (i.e. not benchmark models/datasets), it is hard to evaluate the performance of networks. It is the fundamental issue on low-precision training or similar approaches. To overcome this question, we need deeper understanding and various evidence on practical networks. ",
            "summary_of_the_review": "In my opinion, this paper should strengthen the experimental results and answer some questions on tucker decomposition and regularizer. I understand this method can be applicable to limited networks and dataset, but to prove higher novelty, it should be extended.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_Afr4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_Afr4"
        ]
    },
    {
        "id": "gVwFb5kQ73",
        "original": null,
        "number": 3,
        "cdate": 1666849988456,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666849988456,
        "tmdate": 1666849988456,
        "tddate": null,
        "forum": "TC39w69m8bB",
        "replyto": "TC39w69m8bB",
        "invitation": "ICLR.cc/2023/Conference/Paper1970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes ELRT, a low-rank training method that enables training low-rank factorized neural networks from scratch to attain faster training speed and smaller models. Different from the previously proposed low-rank factorization schemes designed specifically for CNNs,  ELRT leverages tensor decomposition. ELRT also utilizes Soft Orthogonal Regularization to further improve the accuracy of the low-rank training. Thorough experimental results on vision tasks are provided to demonstrate the effectiveness of ELRT.",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written. The research direction of speeding up large model training is important.\n- The experimental results are thorough and solid.\n\nWeakness:\nOverall, my major concern is that the statement \"the existing low-rank training solutions are still very limited and do not demonstrate their effectiveness for training modern low-rank CNN models in the large-scale dataset from scratch.\" is too strong. The papers of [1-3] (which are missing in the reference) have already demonstrated similar promising results. In such a sense, the authors are expected to demonstrate the advantages of ELRT against the low-rank schemes proposed in [1-3]. More weakness is summarized below:\n\n- Using tensor decomposition essentially factories each convolution layer into three \"thinner\" layers. However, it is well-known that deeper models are harder to train. Thus, does ELRT require any special (if not heavy) hyper-parameter tuning for training such deep low-rank neural networks?\n- Given that the transformer network is dominating many application domains currently. I wonder if ELRT also performs well for transformers. Though I understand that the factorization scheme can not be used directly for transformer networks.\n-  It was observed by [2] that factorizing the beginning layers can cause accuracy drops. Does ELRT encounter that?\n- It seems selecting the \"appropriate rank\" for different layers is of importance, otherwise, accuracy drops can be observed, e.g., in Table 2. Is there an approach to determine the ranks to select for the network layers for ELRT?\n\n\n[1] https://arxiv.org/abs/2105.01029\n[2] https://proceedings.mlsys.org/paper/2021/hash/84d9ee44e457ddef7f2c4f25dc8fa865-Abstract.html\n[3] https://arxiv.org/abs/2006.13347",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The demonstration and description of the paper are of good clarity.\nQuality: The writing and experimental results of the paper are of good quality.\nNovelty: The method proposed is of novelty. The importance might be somehow limited given the prior work has demonstrated similar results.\nReproducibility: The authors provided many details about the experimental study. Thus I believe the reproducibility of the paper is good.",
            "summary_of_the_review": "The proposed ELRT method is novel in the sense that it uses tensor decomposition with specialized regularization for training. The method might lack of importance given its performance is very similar to the previously proposed low-rank training schemes (please refer to \"Strength And Weaknesses\" for more details). I thus urge the authors to include comparisons between ELRT and the aforementioned low-rank training schemes.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_DfQU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_DfQU"
        ]
    },
    {
        "id": "kLQ7-VlRTCH",
        "original": null,
        "number": 4,
        "cdate": 1667244727051,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667244727051,
        "tmdate": 1667246624040,
        "tddate": null,
        "forum": "TC39w69m8bB",
        "replyto": "TC39w69m8bB",
        "invitation": "ICLR.cc/2023/Conference/Paper1970/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach to achieve low-rank training for CNNs. Concretely, the authors consider the form of tucker-2 decomposition to build the convolutional kernel, while during training the orthogonality regularization is imposed on the non-core matrices. The authors experimentally demonstrate the effectiveness of the proposed method using several well-known CNN structures for image classifcation tasks.",
            "strength_and_weaknesses": "Strength\n\n1. This paper provides a very neat and practical solution for low-ranking training with the tucker decomposition. Although the topic seems to overlap with former literature, to the best of my knowledge, the method still holds its novelty.\n\n2. The experiments show promising results, and the actual running times in edge devices convince the effectiveness of the proposed method.\n\nWeaknesses\n\n1. The authors should compare the proposed method with two more families of works if possible: i) the works that employ tensor networks straightforwardly, such as [1, 2]; ii) the works that are trained densely, and compressed with low-rank tensors, such as [3, 4].\n\n[1] Hayashi, K., Yamaguchi, T., Sugawara, Y., & Maeda, S. I. (2019). Exploring unexplored tensor network decompositions for convolutional neural networks. Advances in Neural Information Processing Systems, 32.\n\n[2] Su, Jiahao, et al. \"Compact Neural Architecture Designs by Tensor Representations.\" Frontiers in artificial intelligence 5 (2022).\n\n[3] Lin, Rui, et al. \"Hotcake: Higher order tucker articulated kernels for deeper CNN compression.\" 2020 IEEE 15th International Conference on Solid-State & Integrated Circuit Technology (ICSICT). IEEE, 2020.\n\n[4] Yu, Deli, Peipei Yang, and Cheng-Lin Liu. \"Learning-based Tensor Decomposition with Adaptive Rank Penalty for CNNs Compression.\" 2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE, 2021.\n\n2. The structures of CNNs are slightly out-of-date, the authors could try their method on search-based optimal network structures (eg. CoAtNet) or networks with unusual designs (eg. RepLKNet with 31x31 kernels) to enhance the effectiveness of the proposed method.\n\n3. The proposed method could result in matrix multiplications with non-optimal dimensions in practice. In Table 3, the theoretical reduction of FLOPs is somewhat larger than the reduction of running time. Hence the design of the network structure could be difficult and time-consuming.\n\n4. I am still not sure why orthogonality regularization can boost the performance by a large margin since it does not ensure the orthogonality on $\\mathbf{U}^{1}, \\mathbf{U}^{2}$. If this conclusion still holds under larger and wider CNNs?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is good in clarity, quality, and novelty. Also, the reproducibility should not be bad since the authors provide very detailed implementation instructions.",
            "summary_of_the_review": "This is a solid paper, it could be impactful in both the application of tensor decomposition and network compression and acceleration communities. I vote to accept this paper if the authors could address the aforementioned weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_rfLo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1970/Reviewer_rfLo"
        ]
    }
]