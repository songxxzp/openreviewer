[
    {
        "id": "T6MtJBAL7PA",
        "original": null,
        "number": 1,
        "cdate": 1666313532420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666313532420,
        "tmdate": 1666313532420,
        "tddate": null,
        "forum": "FZCFlj2_c7z",
        "replyto": "FZCFlj2_c7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2825/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose JSRL to explore and learn by continuously using existing polices to reach a state before adopting a learnable policy. The authors emphasize that when using existing policy to select the arrival state, one can consider gradually decreasing the time step of their decisions, thus providing a more stable learning process.",
            "strength_and_weaknesses": "\nStrength.\n* The paper is clearly written and very well understood\n* The method is helpful when reinforcement learning is performed for application\n* Experimental results show that the method proposed by the authors has clear advantages\n\nWeaknesses.\n* The paper emphasizes the shortcomings of the value-based approach, and in fact most of the experiments are based on the AC architecture approach.\n* The paper has some overclaims, mentioning \"It is also compatible with any RL algorithm and can be easily combined with existing offline and/or online RL methods\" several times in the paper, but it is worth noting that JSRL is only applicable to some specific scenarios, such as offline to online, and I think the authors should be more rigorous in expressing the advantages of the method.\n* The results of Fig2 are intuitive, and since critic is not learned, it is natural that efficient finetuning is not possible. in conjunction with Fig7 and Fig8 in the appendix, I believe that the core problem comes from the need for reasonable policy evaluation learning at the beginning of the next phase in order to proceed well with further learning, but I find the authors' formulation in the relevant section confusing .\n* The authors should have compared transfer learning and offline2online related methods instead of a few simple baseline algorithms.\n* In Fig3, it looks like there is no significant advantage of gradually decreasing methods compared with the randomly chosen time step methods\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The overall paper is clear\n\nQuality: Good enough\n\nNovelty: The author's choice of perspective is interesting\n\nReproducibility: Should be fine",
            "summary_of_the_review": "I think this paper is interesting enough that I will take into account the comments of other reviewers with the results of the rebuttal to adjust my score",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_hK9Y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_hK9Y"
        ]
    },
    {
        "id": "ezzm98oPn20",
        "original": null,
        "number": 2,
        "cdate": 1666639427822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666639427822,
        "tmdate": 1666639427822,
        "tddate": null,
        "forum": "FZCFlj2_c7z",
        "replyto": "FZCFlj2_c7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2825/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a general method called Just-Start Reinforcement Learning to utilize the pre-existing policy for better learning an RL policy. Two policies, a guide-policy and a exploration-policy, are used interactively to efficiently explore the spaces. The guide-policy is used at the beginning of each training episode. And an updated exploration policy will take charge of the other steps until the end of the time horizon. This paper theoretically analyzes the upper bound on the sample complexity of JSRL is polynomial. Furthermore, this paper presents simulated experiments on a set of robotics task. The proposed method significantly outperform related IL+RL works.",
            "strength_and_weaknesses": "The method proposed in this paper is a very general framework for efficiently and quickly learning an RL policy by utilizing the knowledge of an existing guide-policy. Any RL algorithm and policy model can be plugged into the proposed framework to improve its performance under an reasonable assumption. Strong theoretical proofs are provided to verity the advantages of this paper. From the experimental results, the improvement is significant compared to the existing methods. Different experimental setups are considered which gives a sufficient support for the theoretical result. \n\nI understand that Assumption 4.2 is relatively weak compared to the assumptions in related works. And this assumption is also essential for the theory. But in reality, I still think it is pretty strong. It could be hard to find a guide policy which only visits all good states in feature space.  ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very well written. The proof is easy to follow. The proposed method is novel.",
            "summary_of_the_review": "This paper proposes a novel frame work for efficiently exploring the state by using existing knowledge. Sufficient theoretical and empirical analysis is provided to support the statement in this paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_3rP7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_3rP7"
        ]
    },
    {
        "id": "sDVuXr25El",
        "original": null,
        "number": 3,
        "cdate": 1667292060435,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667292060435,
        "tmdate": 1667293308260,
        "tddate": null,
        "forum": "FZCFlj2_c7z",
        "replyto": "FZCFlj2_c7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2825/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses an interesting problem for RL i.e., sample efficiency. The author proposes a Jump-Start RL (JSRL) algorithm to leverage a prior policy of any form to give a head start for exploration in RL. The proposed algorithm rolls out a pre-existing guided policy, followed by self-improving exploration policy. The proposed method is evaluated on simulated tasks and vision-based robotic tasks. Also, the author provides the upper bound on the sample complexity for proposed method.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written.\n- The problem addressed in the paper is very important problem for RL to be used widely as a learning method.\n- The paper evaluates the proposed method on extensive simulated and vision-based robotic tasks.\n- The upper bound on sample complexity of the proposed method is good. \n\nWeaknesses/Questions:\n- The problem of sample complexity of RL is very useful in real-world robotic tasks. I have only seen authors mentioning about real-world robotic task in the end of the paper as Future Work. Can authors discuss how the proposed method would help in real-world robotic tasks?  If authors have some small experiment on a real-world robotic task to demonstrate effectiveness of the proposed method that would make the paper more strong?\n-  The guided policy induces some bias in learning the policy to learn a task. However, RL might find a better policy from scratch if the guided policy is not there? Can author comment on the possible bias induced due to the guided policy?\n- The guided policy is chosen from similar domain task as the new task? How would JSRL would perform if a guided policy is from different domain than the new task? For example, can we use a guided policy of grasping on a door closing task?\n- Could you please mention abbreviation of IQL? I know you guys are referring to Implicit Q-Learning. However, its good for the reader to know this terms in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and writing is good.",
            "summary_of_the_review": "already mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_5862"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_5862"
        ]
    },
    {
        "id": "zj6hJIKq70Y",
        "original": null,
        "number": 4,
        "cdate": 1667409657947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667409657947,
        "tmdate": 1667409657947,
        "tddate": null,
        "forum": "FZCFlj2_c7z",
        "replyto": "FZCFlj2_c7z",
        "invitation": "ICLR.cc/2023/Conference/Paper2825/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose Jump-Start Reinforcement Learning (JSRL) which utilizes a pre-trained guide policy to form a curriculum of starting states for a different exploration policy. Theoretical analysis shows that with a properly chosen training and evaluation algorithm, JSRL achieves a polynomial sample complexity. Empirical evaluations on D4RL and vision-based robotic tasks are provided to show the effectiveness of JSRL.",
            "strength_and_weaknesses": "Strength: JSRL is well-motivated to address the sample complexity requirement in training RL agents from scratch. The proposed algorithm is easy to understand and empirical evaluations show some effectiveness, especially in the low demo data regime. \n\nWeakness:\nThe empirical evaluation section could be improved. In Table 2, three baseline methods were not tested on low demo data regimes, only the 1 million standard setting. As some of them achieve better results than JSRL in the standard setting, we need to see the comparisons in the low data setting to evaluate how effective JSRL really is.\n\nMoreover, the improvement margins JSRL has over baseline methods are relatively small. In fact, in Table 1, for Instance Grasping, the result confidence intervals of 20 and 20k demos overlap with those of AW-Opt. The improvement in D4RL tasks is also small. So I am not convinced JSRL, in its current form, is really better than existing baselines.\n\nThere are some imprecise writings in the theoretical analysis, see my questions in the next section.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively easy to read. However, in the theoretical analysis, some languages appear imprecise.\n\n1. For Theorem 4.3, the authors wrote \u201cTo achieve a polynomial bound for JSRL, it suffices to take TrainPolicy as $\\epsilon$-greedy.\u201d\n\nThe TrainPolicy procedure updates a policy and a Q function. $\\epsilon$-greedy normally refers to an exploration method for policy. The authors should provide a more precise explanation of what an $\\epsilon$-greedy policy update procedure is.\n\n2. The distribution mismatch coefficient in Assumption 4.2 uses undefined quantities $d$. \n\n3. In Section 4.2, I think it should be $H_i \\in \\{1, 2, \\cdots, H\\}$.\n",
            "summary_of_the_review": "The major weakness is the evaluation results, which did not convince me that JSRL is stronger than baseline IL + RL methods.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_WwEP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2825/Reviewer_WwEP"
        ]
    }
]