[
    {
        "id": "5rFKQHOvuI",
        "original": null,
        "number": 1,
        "cdate": 1666460675201,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666460675201,
        "tmdate": 1669030368147,
        "tddate": null,
        "forum": "row6cEJ2aBT",
        "replyto": "row6cEJ2aBT",
        "invitation": "ICLR.cc/2023/Conference/Paper3480/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors address a challenging and important problem of feature selection. The new approach is an embedded-type method that relies on sparse max to impose sparsity on the input features of a model. They use 8 real datasets and compare to several baselines to demonstrate the effectiveness of the method. ",
            "strength_and_weaknesses": "The problem is well-studied but still important for the community. The English level is satisfactory, and overall I enjoyed reading the paper. The problem statement and solutions are clear. The evaluation seems clear, and they further provide an ablation study to demonstrate that the proposed components are beneficial. Related work is mostly well-written. Another strength is the ability of the method to select exactly k features. \n\nThe major weakness of the paper is its framing; the authors name their method Scalable FS\u2026 but I don\u2019t see any analysis of how scalable the method is.\nIn fact, the authors only apply it to dimensions that are not so large (D<1000). It is worthwhile to add the dimensions and sizes of all datasets. Furthermore, some strong, highly related baselines are missing, for example: \n\n[1] Lemhadri et al. Lassonet: Neural networks with feature sparsity\n\n[2] Yamada et al. Feature Selection using stochastic gates, 2020\n\n[3] Balin et al .Concrete autoencoders: Differentiable feature selection and reconstruction\n\n[4] Singh et al. Fsnet: Feature selection network on high-dimensional biological data\n\nIn the following, I provide detailed comments to help the authors improve the paper:\nIntroduction: \u201cvast majority of the feature selection methods cannot take advantage of end-to-end learning.\u201d This sentence is wrong and misleading, there are countless embedded methods that DO take advantage of this type of training, including 1-3, and many, many more.\n\nMissing works in related work 1-3.\n\nOn page 2, the authors say that local feature selection models can improve accuracy, in the next sentence, they say they it doesn\u2019t alleviate overfitting. This is somewhat of a contradiction.\n\nIn the algorithm please mention where \\ell and E losses are explained in the paper.\n\nSection 3.1 is called Sparse attention, but it has nothing to do with attention.\nAlso this section is not very clear.\n\nCommas missing in Eq.7. and after theorem 3.3\n\nWhy do you choose specifically 50 features for all datasets? This is very restrictive, and limits the analysis.\n\nFigure 2 -subfigure 3 is confusing. The variability between different models seems HUGE, this means that the model strongly depends on initialization? You need to elaborate on this and demonstrate.\n\nAre the results in table 1 the average over multiple runs? Or just the best run out of 300 runs based on the validation accuracy?\n\nStatistics are required to understand the performance of all methods.\n\nWhat are the train/ test validation sizes?\n\nAt least one simulation evaluating the FS capabilities of the method is required. For example, using synthetic data or datasets from the Nuerips 2003 FS challenge. Thus providing information to the reader about the model's ability to recover the informative features.\n\n Page 9 \u201cFeature importance intepretability\u201d the model is global; how do you interpret the FS locally for each sample? Also, nor example is given.\n\nHow do I interpret figure 2? Are all comparisons using FS at the same level?\nIf so, how are features selected without the MI without tempering? I can\u2019t judge these results without this explanation. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The introduction and problem statement is clear, the paper does provide some novelty tempering procedures. However, both probabilistic sparsity and tempering were previously studied for FS, and the authors don\u2019t mention those prior works [2] and [3]. The evaluation is very limited since the authors don\u2019t show averages and standard deviations over many experiments, also such results are hard to reproduce.",
            "summary_of_the_review": "Overall the authors present a new method for an important problem. They use sparse max with tempering and demonstrate the approach on 8 datasets. However, the experimental results are quite limited as they don\u2019t provide statistics and don\u2019t compare to highly related works (mentioned above). The MI component is interesting, but it\u2019s hard to judge its value since figure 2 is not well explained. I would expect a comparison between the proposed method and a baseline that incorporates the sparsemax mask into a cross-entropy loss guiding the selection of features. Based on these reasons, I can\u2019t recommend accepting the paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_DZWY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_DZWY"
        ]
    },
    {
        "id": "K3rydae0dZ2",
        "original": null,
        "number": 2,
        "cdate": 1666545066693,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666545066693,
        "tmdate": 1666545197280,
        "tddate": null,
        "forum": "row6cEJ2aBT",
        "replyto": "row6cEJ2aBT",
        "invitation": "ICLR.cc/2023/Conference/Paper3480/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method with the automatic mask scaling which can gradually temper this sparsity. The proposed SLM maximize a quadratic relaxation (with the help of Taylor approximation) of the mutual information between the selected features and the labels.",
            "strength_and_weaknesses": "### Strength\n- The quadratic relaxation of MI is interesting to some extent. \n- The experiments seems effective (even though the experiments may be insufficient). \n\n### Weakness\n- The novelty seems a little limited. Maybe I misunderstood something important due to the poor description of the motivation. Sections 3.2-3.4 are not attractive enough. The quadratic relaxation of MI may be interesting to some extent. Did the authors consider different estimators of MI, which have been extensively studied in representation learning? What is the primary advantage of the quadratic relaxation?\n- The writing of this paper could be further improved. For example, the formal definitions of mathematical notations could be summarized in an individual paragraph or section. The writing and poor motivation extremely limits the readability of this paper. \n- The experiments are not sufficient. Although the space is limited, the extra experiments with different selected features could be reported in appendix. ",
            "clarity,_quality,_novelty_and_reproducibility": "The writing quality of this paper could be improved. For example, a summary of mathematical notations will help a lot to improve the readability. \n\nThe novelty is a little limited.  ",
            "summary_of_the_review": "The main concerns come from the limited novelty. However, I would like to update score after discussion, if my concerns can be addressed.  ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_1pMd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_1pMd"
        ]
    },
    {
        "id": "gtI3oKFwEbr",
        "original": null,
        "number": 3,
        "cdate": 1666582015425,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666582015425,
        "tmdate": 1666582114444,
        "tddate": null,
        "forum": "row6cEJ2aBT",
        "replyto": "row6cEJ2aBT",
        "invitation": "ICLR.cc/2023/Conference/Paper3480/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors propose a method to dynamically select a subset of features in a neural network. In contrast to standard approaches, this set of features is _global_, and therefore the same subset of features will be ignored for all samples in the dataset.\nThe proposed method consists of three ingredients:\n1. Scaling sparsemax to the desired sparsity level.\n2. Linearly increase the desired level of sparsity during training.\n3. Optimize the task loss with a regularization term that captures the mutual information between the selected features and the label.\n\nResults on 8 datasets showcase the method's effectiveness, and ablation studies are performed to understand the impact of the sparsity level and the regularization term.",
            "strength_and_weaknesses": "Things that I liked in this paper:\n- The motivation is clear.\n- The idea of scaling inputs by sparsemax globally is exciting.\n- Lemma 3.2 is incredibly useful.\n- The results are impressive, and the ablation studies are very informative.\n\n\nHowever, there are many things to be improved:\n\n- The math notation makes the paper hard to follow. \n- Although I liked the connection to the classical literature on feature selection, the related work on masking covers a small number of recent works. See some important missing references below.\n- It is hard to assess if the subset of selected features is meaningful since the ground truth feature importance is unknown for the experimented datasets.\n- Linked to the above point, there are no concrete examples of the interpretability of the selected features. Are the selected features plausible? \n- Although the authors claim the regularizer can be computed efficiently, the paper can be improved by reporting the running costs.\n\nOn sparse masks with a controlled sparsity level:\n- Lei, Tao, Regina Barzilay, and Tommi Jaakkola. \"Rationalizing neural predictions.\" arXiv preprint arXiv:1606.04155 (2016). \n- Bastings, Jasmijn, Wilker Aziz, and Ivan Titov. \"Interpretable neural predictions with differentiable binary variables.\" arXiv preprint arXiv:1905.08160 (2019).\n- Guerreiro, Nuno Miguel, and Andr\u00e9 FT Martins. \"Spectra: Sparse structured text rationalization.\" arXiv preprint arXiv:2109.04552 (2021).\n- Paranjape, Bhargavi, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. \"An information bottleneck approach for controlling conciseness in rationale extraction.\" arXiv preprint arXiv:2005.00652 (2020)\n- Correia, Gon\u00e7alo, Vlad Niculae, Wilker Aziz, and Andr\u00e9 Martins. \"Efficient marginalization of discrete and structured latent variables via sparsity.\" Advances in Neural Information Processing Systems 33 (2020): 11789-11802.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I found the paper hard to follow. The introduction is too overwhelming and disconnected from the main part of the paper, while the math notation is confusing. For example: bold symbols are used to denote scalars (e.g., $\\mathbf{m}$), and $\\cdot$ between vectors is used to denote element-wise multiplication (e.g., $\\mathcal{M}_{sp} \\cdot \\mathbf{x}$). Moreover, it is unclear how equation 10 is computed efficiently. What is $b$ in its time complexity, batch size? What is $n$? What is $N$ (in sec. 3.2)? \n\nClarification question:\nHow exactly were the different methods of feature selection tested? Taking XGBoost as an example: were the \"selected\" features first discovered with XGBoost, and then later only these features were passed as input to an MLP?\n\nOther things:\n- The caption of Figure 2 is too close to the main text.\n- Multiplying the argument of sparsemax by $m$ is equivalent to use a temperature parameter $\\tau = 1/m$. This arrives from the property of temperature-scaling of Fenchel-Young Losses, which can be used to cast sparsemax as a regularized prediction function. See more in [1] and [2]. \n\n[1] Peters, Ben, Vlad Niculae, and Andr\u00e9 FT Martins. \"Sparse sequence-to-sequence models.\" arXiv preprint arXiv:1905.05702 (2019).\n\n[2] Blondel, Mathieu, Andr\u00e9 FT Martins, and Vlad Niculae. \"Learning with Fenchel-Young losses.\" J. Mach. Learn. Res. 21, no. 35 (2020): 1-69.",
            "summary_of_the_review": "Overall this paper provides a new method to induce a sparse global model. Although the sparsity can be exploited to interpret the model globally, the paper does not provide examples or evaluations toward this goal. However, the predictive performance is impressive across a wide range of datasets, and the ablation studies help illuminate the proposed approach's main contributions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_BDBF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3480/Reviewer_BDBF"
        ]
    }
]