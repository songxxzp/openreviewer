[
    {
        "id": "NKLA9TEpeY",
        "original": null,
        "number": 1,
        "cdate": 1665788511002,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665788511002,
        "tmdate": 1665788511002,
        "tddate": null,
        "forum": "WZH7099tgfM",
        "replyto": "WZH7099tgfM",
        "invitation": "ICLR.cc/2023/Conference/Paper929/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a new prompting techniques for large language models, called least-to-most prompting. It consists in two stages: (1) prompt the model to ask it to break down the given problem into easier subproblems; (2) prompt the model to solve each subproblem. Experimental results are shown for datasets of symbolic manipulation, compositional generalization, and math reasoning, with GPT-3 models.",
            "strength_and_weaknesses": "The proposed technique is new and interesting. It is a nice addition to the literature on prompting large language models to solve complex tasks.\n\nI see a major limitation with this approach: it looks like the prompt examples need to be designed specifically for each dataset. Additionally, the prompt examples add significant supervision on how the task should be solved, and this supervision is not present when using other methods (such as standard prompting).\n\nFor instance, in the symbolic manipulation task, the examples for least-to-most prompting are constructed by exploiting the property \"concatenating the last letters of $n$ words is the same as concatenating the last letters of the first $n-1$ words, and then concatenating the result with the last letter of the $n$-th word\". In my opinion, learning this property is an important part of learning to solve the task algorithmically.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is interesting and original.\n\nIn terms of clarity, it can be improved further:\n- It's not easy to understand how the full pipeline would work for a complex problem (which needs to be broken down recursively many times). Does stage 1 consists of a single pass through the model, to directly output how to break down the problem into the easy pieces?\n- In stage 2, when solving each subproblem, is the statement of the subproblem copied from the output of stage 1 or is it generated by the model (in which case probably the output of stage 1 is part of the prompt for stage 2)?\n\nIt would help to have one full example, where each forward pass through the model is clear, and in each forward pass it is clear what is the prompt and what is the model's continuation.",
            "summary_of_the_review": "The proposed approach is interesting, but it also appear to have a significant limitation (you need to provide supervision that provides information on how the task needs to be solved). This limitation seems to make it difficult to apply the approach more broadly (dataset-specific supervision is required) and makes comparisons with other methods not completely fair (because this approach has effectively more supervision about the task).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper929/Reviewer_Vyt2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper929/Reviewer_Vyt2"
        ]
    },
    {
        "id": "vyvgi9yokm",
        "original": null,
        "number": 2,
        "cdate": 1666683763535,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683763535,
        "tmdate": 1668507048954,
        "tddate": null,
        "forum": "WZH7099tgfM",
        "replyto": "WZH7099tgfM",
        "invitation": "ICLR.cc/2023/Conference/Paper929/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper follows-up on recent work around \"chain-of-thought\" (CoT) prompting, where the expected output of the model is not only the desired output, but also rationales that would reflect reasoning steps that a human would use to solve that same problem.\n\nThe additional insight of this paper is that this process can be made hierarchical, by first requesting the model to output a series of subproblem such that solving those subproblems would naturally lead to the solution of the initial problem. After this, each subproblem is solved sequentially, with the previous answer appended to the prompt before requesting to solve the next sub-problem.\n\n\n========= OLD REVIEW - PRE ANSWER BY AUTHORS ========= \n\nI would like to hold back with my review until the authors get an opportunity to reply (long live review discussion!). So please disregard the rest of the fields, including final assessment. Apologies if my question is somehow obvious, but I could not find an answer to this through repetitive reading of the paper:\n\nCould the authors please clarify which parth is given as prompt to the LM, and which part is generated? From the colored highlight it seems to me that:\n - in Fig 1, the subquestion 2 is provided by the human, and not generated by the LLM.\n - my understanding is that in Table 2 and 5, everything that comes before \"So,\" is given as a prompt (in addition of the few-shot examples of course) in the different calls to the API. This would mean for example that in Table 2:\n     * you call OpenAI's API a first time, with the prompt before the right-side of Table 1 + everything before the first blue highlighted text (up to 'leads to \"de\". So,').\n     * you call OpenAI's API a second time, with everything up to the first red highlighted text: up to 'leads to \"dem\", So,'\n\nIf that is the case, then this result seems underwhelming to me - and not corresponding to the claims. Because in that case the exercise seems rather trivial as all the heavy-lifting is done by the prompts which - again if my understanding is correct - are manually written and not machine-generated.\n\nthanks a lot!",
            "strength_and_weaknesses": "STRENGHTS\n+ As in CoT the proposed method does not require any training or fine-tuning. The experiments reported here can be done solely by interacting with the API of a large language model provider\n+ The performance jump is significant on problems considered difficult\n\nWEAKNESSES\n- The prompt are designed task-specific. It is not clear what was the validation set to create those prompts. A robustness study on using different prompts would be welcome and strengthen the message that the results hold across a variety of lexical variations of the same idea \n- Clarity of the paper (see below)",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe reply of the authors was greatly appreciated and helped me understand the paper better. It points however to the intrinsic difficulty of explaining the experimental setup when this one involves a large amount of text and successive calls to a LM that modifies some parts of the prompt. I encourage the authors to experiment with different and simple visualization techniques that could convey this better. Related to this - and a detail -: unifying the colours used to highlight the re-use of answers in Fig 1 and the various tables would improve clarity as as well.\n\n**Novelty**\nVery good\n\n**Reproducibility**\nThe authors made sure to show results not only on their proprietary model, but also on an API - whose access and underlying model however can change abruptly. The authors have limited influence on that however\n\n**Quality**\nThe evaluated datasets are limited, and the impact of prompt engineering is not assessed. Having said that, and considering the fast-moving pace of research into the capabilities of LLM, I consider that this is a worthwhile addition to the ICLR roster, and constitutes a paper which will be analyzed and cited in the immediate future\n",
            "summary_of_the_review": "Another surprising discovery into capabilities of LLM, showing great improvement on existing tasks without the need to finetune.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper929/Reviewer_d1S7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper929/Reviewer_d1S7"
        ]
    },
    {
        "id": "j3A7iNoaDhO",
        "original": null,
        "number": 3,
        "cdate": 1666695158153,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695158153,
        "tmdate": 1671355678395,
        "tddate": null,
        "forum": "WZH7099tgfM",
        "replyto": "WZH7099tgfM",
        "invitation": "ICLR.cc/2023/Conference/Paper929/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Recently chain-of-thought prompting (\"lets think step by step\") has demonstrated big language models can perform several reasoning tasks well, e.g. match word problem. However, it is known to not perform well when the problem to solve is harder than the few-shot examples in the prompt. To address the problem, this paper proposes a new prompting method called least-to-most prompting. It consists of two steps. The first step is to reduce a complex problem into a list of subproblems while the second one is to sequentially solve each sub-problem. Answers to previous subproblems are available to solve the next one. The paper evaluates on three types of reasoning tasks, symbolic manipulation, compositional generalization and math reasoning. The paper shows least-to-most prompting outperforms chain-of-thought prompting by a large margin. It is worth noting that GPT- 3 code-davinci-002 model with least-to-most-prompting solves the SCAN benchmark with an accuracy of 99.7% using 14 examples while chain-of-thought prompting only achieves an accuracy of 16.2%.",
            "strength_and_weaknesses": "Strength\n1. The paper proposes a new prompting strategy for big language models to solve multi-step reasoning tasks. It breaks the problems into sub-problems by querying the language model, and then solves each sub-problem sequentially.\n\n2. The paper shows that the proposed least-to-most prompting significantly improves over the prior work, chain-of-thought prompting on three tasks, symbolic manipulation, compositional generalization and math reasoning. \n\nWeaknesses\n1. A major limitation of the paper is the problem reduction step. Since the three datasets are relatively constrained, the few demonstration examples may cover most of the cases on how to break into sub-problems. In other words, the model may only need to copy patterns from the few-shot demos without any generalization. \n\n2. The paper lacks insights on key questions. Does the model have ability to decompose questions very different from demo examples into correct sub-problems? Even the model solves each sub-problem correctly, what makes the model compose the final solution correctly?\n\n3. The paper needs to evaluate on more reasoning problems (e.g. StrategyQA, high school reading comprehension) or clearly state the type of reasoning problem the method works well. It needs a better limitation section that what is stated in the conclusion and discussion section. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear. However, examples in the main paper do not have few-shot demos which made it a bit hard to understand.\n\nThe least-to-most prompting idea seems to be novel, somewhat straightforward. There are more principled and better ideas now, e.g.\nMaieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\nhttps://arxiv.org/pdf/2205.11822\n\nI believe maieutic prompting is considered concurrent work.\n\nEven though I consider the paper has substantial originality, the paper lacks depth and deep insight on whether this method represents genuine progress in improving big model's reasoning ability or is very dataset dependent. ",
            "summary_of_the_review": "Big language models have achieved impressive results in NLP. It is natural to think they can solve simple-step problems. To solve hard problems, one just decompose them into sub-problems. This paper presents experimental results on this straight-forward idea. Although the evaluation is very limited, it does have one very impressive results, GPT- 3 code-davinci-002 model with least-to-most-prompting solves the SCAN benchmark with an accuracy of 99.7% using 14 examples while chain-of-thought prompting only achieves an accuracy of 16.2% and neural-symbolic models in the literature specialized for solving SCAN are trained with the full training set of more than 15,000 examples.\n\nTo build confidence and understand its limitations, I would encourage the authors to evaluate on other types of reasoning tasks such as StrategyQA, TimeDial, high school reading comprehension, and many others in Bigbench (Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models https://arxiv.org/pdf/2206.04615.pdf).\n\nI would also encourage the authors to provide deeper insights and analysis as stated in the weakness section. \n\n== Post authors' response\n\nThe response provided more examples as evidences of generalization. It is still not clear to me why L2M is able to generalize. The paper only offered a tenuous connection to educational psychology. I think the results are intriguing and L2M worth further research. Therefore, I upgrade my rating from marginal accept to accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper929/Reviewer_YZYL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper929/Reviewer_YZYL"
        ]
    },
    {
        "id": "toPOQbIVcD",
        "original": null,
        "number": 4,
        "cdate": 1666731690673,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731690673,
        "tmdate": 1669234379341,
        "tddate": null,
        "forum": "WZH7099tgfM",
        "replyto": "WZH7099tgfM",
        "invitation": "ICLR.cc/2023/Conference/Paper929/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new prompting method, called \"Least To Most\" prompting. Building upon the success of the \"chain-of-thought\" prompting method, the authors propose to split the prompting process into two stages: problem reduction, and problem solving. The former focusing on decomposing the task at hand into its constituent parts, the latter corresponding to solving each subproblem and combining the results into the final answer. The authors show that their model outperforms \"chain of thought\" prompting on a number of tasks, including both simulated tasks like  last letter concatenation and the SCAN dataset and on more realistic mathematical reasoning problems (Drop and GSM8k).",
            "strength_and_weaknesses": "Strengths:\n- The problem of teaching language models new skills \"on the fly\" is extremely relevant\n- The paper has numerous illustrations, which helps to understand the approach and model behavior\n- The approach is novel and original\n\nWeaknesses:\n- There are a number of issues with experimental design. I detail those in the \"quality\" section of the review.\n- In terms of reproducibility, some elements of the approach are not very clearly reported, specifically, in the first two experiments, the authors say that either a script or simple prompting can be used (for decomposition in experiment 1, and for expanding python expressions), and it's not specified what was actually used.\n\nHere is one of the relevant quotes: \n\"To generate the final results, we either run a postprocessing script or prompt language models to expand the Python expressions. It is straightforward to teach language models to expand Python expressions via a few demonstrations.\nHowever, I can not find a more detailed description nor the results on the performance of these straightforward expansions. While straightforward from the prompting standpoint, it might still result in less than perfect accuracy & providing exact prompts is crucial for replication.\n\n- While thorough, the abundance of tables and model input/output printouts is, at times, getting in the way of readability.",
            "clarity,_quality,_novelty_and_reproducibility": "\n** Clarity **\n\nThe paper is generally clearly written and is a pleasure to read. There are some minor issues (which I list below), but they did not strongly affect my overall assessment of the paper.\n\nFirst, the phrase at the end of page 5 (\"We find that the ...) is continued amid a number of tables on page 6 (\"... solving rate remains the same\"), and then almost immediately followed by another table. It'd be better to move text or tables around so that it's a bit easier to read.\n\nAppendices are extremely thorough (which is a plus), but they also feature a lot of material that is difficult to navigate. It may, potentially, be better to move some of the additional results and demonstrations into supplementary materials.\n\nThe abundance of model input/output printouts in the main body of the paper is helpful, but I think that some of them may be condensed and explained verbally in text.\n\nOverall, the clarity is up to the standards of the ICLR conference.\n\n** Quality **\n\nThe experiments cover a range of substantially different tasks, both simulated and realistic. There are a few issues with experiment design, however, some of which I find substantial.\n\nExperiment 1 (symbolic manipulation):\n- The prompts differ not only in the recursive part ('So, \u201cthink, machine\u201d outputs\n\u201cke\u201d'), but also in the examples themselves. The chain-of-thought prompts in table 5 use completely different sets of words for length 2 and length 3 prompts. Because of that, it's impossible to tell whether it's the selection of words or the inclusion of the \"reduction\"/\"recursive\" phrase that is key for the method to work.\n- \"Such a trivial reduction can be done straightforwardly by a manually written script or prompting language models with several\ndemonstrations.\" regarding breaking a multi-word problem in to smaller constituents. However, if done with a script, the comparison with the regular \"chain-of-thought\" method is unfair. At the same time, if done using prompting, while straightforward, it can still break down on longer prompts. As far as I can tell, no detail is given in the paper on the \"straightforward\" prompting, and so the generalization performance of such prompting on longer sequences can not be assessed. Therefore, it may be that the improvements we see are due to better decomposition that may be delegated to an external script in the least-to-most prompting case and that has to be done internally by the language model in the case of \"chain-of-thought\" prompting.\n\nBecause of these concerns, the results from the first experiment can not be fully interpreted.\n\nExperiment 2 (SCAN):\nThe description of the experimental procedure is, again, partially unclear:\n\"To generate the final results, we either run a postprocessing script or prompt language models to expand the Python expressions. It is straightforward to teach language models to expand Python expressions via a few demonstrations.\"\nIt's unclear which method was used and whether there was a difference in performance. Again, while prompting may be straightforward,\nlanguage models are known to fail in unexpected ways on longer sequences. If scripts were used as part of Python expression expansion,\nit makes the comparison between the models unfair.\n\nWhat makes me especially worried is that in the SCAN experiment, it's not clear, why having Python notation such as \" * 2\" would be better than having verbal notation (such as \"twice\"), so it makes me worried that the benefit may be in reducing the errors during the decoding stage.\n\nExperiment 3:\nThis issue applies to different experiments too, but is most evident in experiment 3.\nI believe that for a fair comparison between prompting methods it is reasonable to normalize by the amount of information given. Least-to-most prompting essentially gives more prompts than the \"chain of thought\" one. They are partially redundant, but, nevertheless, in a way, the least-to-most prompting can be benefitting from the sheer number of examples (a solution to each subproblem may be seen as an example). \n\nFrom the practical standpoint, it is also reasonable to normalize by the amount of instruction provided to a language model, and I don't think it is 100% fair to say that \"least-to-most prompting\" and \"chain-of-thought\" prompting receive equivalent amount of instruction because both are two-shot, when each \"least-to-most\" prompting \"shot\" takes twice or more space in terms of volume of text.\n\nWhile not all of these issues are crucial, I believe that overall, they create an inconsistent impression, and because of that, I feel that the quality of the contribution is marginally below the requirements of the ICLR conference.\n\n** Novelty **\n\nThe work is substantially novel and original, and expand upon the \"chain-of-thought\" prompting in a meaningful way.\n\n** Reproducibility **\n\nI have mentioned some of the reproducibility issues in the \"quality\" section. The authors also do not provide the code, which could have helped to mitigate the issue.\n\n** Other suggestions **\n\nRegarding the last sentence on page 4 -- \"These look somewhat reminiscent of careless mistakes by humans\". I find that the paper will benefit if this sentence is removed.\n\nFirst, I disagree that even a careless human would end up with \"wsgss\" when concatenating \"wsg\" and \"s\". Second, the subtle implication that the model is somehow humanlike in its behaviour is very ambitious. When it's based on such loose grounds, it gives a very speculative impression, which overall damages the perceived credibility and rigour of the paper.\n\nThis is a minor suggestion that did not affect my overall assessment.",
            "summary_of_the_review": "Overall, I enjoyed reading the paper, and I believe that it has a great potential. I am very torn when it comes to its assessment, since while many of the issues I've mentioned are minor, they accumulate to give the paper an inconsistent impression. The experimental section issues are the most important. Experiment 3 is, overall, the most convincing, but given that it also has some methodological concerns, and, given the issues with experiment 1 and 2, I lean towards a weak rejection. I will, however, stay open to revise my assessment based on other reviews and authors' rebuttal.\n\n** UPD **\nMany of my concerns were properly addressed, therefore I increase the score, pushing it above the acceptance threshold. I would have given it a 7, but the interface does not allow it.\n\nThat being said, I believe that additional results and clarifications dramatically improve the quality of the paper, and that it is now above the acceptance threshold and is of interest to a large portion of the ICLR community. In fact, I hope that the paper is accepted.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Other reasons (please specify below)"
            ],
            "details_of_ethics_concerns": "The authors reference additional experiments on a 540 billion parameter language model in the appendix. It's not central to the contribution, and the added value of this comparison is dubious.\n\nThe reason I mention it is that I believe that it might place additional pressure on reviewers, since it is well known that there is only one such model (PaLM) and only one research group with access to it (Google Research).",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper929/Reviewer_JgLG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper929/Reviewer_JgLG"
        ]
    }
]