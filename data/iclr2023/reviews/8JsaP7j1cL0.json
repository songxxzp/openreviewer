[
    {
        "id": "NOm1jr17eWY",
        "original": null,
        "number": 1,
        "cdate": 1666539592841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666539592841,
        "tmdate": 1666539592841,
        "tddate": null,
        "forum": "8JsaP7j1cL0",
        "replyto": "8JsaP7j1cL0",
        "invitation": "ICLR.cc/2023/Conference/Paper4342/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper develops an online algorithm with local update rules for blind source separation using information maximization in terms of second-order statistics and knowledge of the domain of the sources.\n\nLike previous biologically plausible algorithms, the online algorithm can be instantiated using only Hebbian and anti-Hebbian learning in two or three layer networks. \n\nIn particular, when the domain of the sources are known to be a polytope (L1-ball for sparse vectors), L$\\infty$-ball for dense vectors, possibly non-negative (yielding simplex for the former), the Lagrangian of the constrained maximization problem is shown to induce different activation functions. \n\nResults show that the method is competitive to other prior work on synthetic problems where the true domain of the sources is known. ",
            "strength_and_weaknesses": "Strengths:\nThe work builds off of a solid foundation for BSS using second-order statistics and is logically presented.\nThe method compares well to related work in ideal cases. \nThe local updates seem reasonable based on the assumptions. \n\nWeaknesses: \nThe figures of networks are not clear from their captions.\nThe overloaded notation is quite cumbersome at times.  \nThe method is not compared to other approaches on the real data.\nAlthough dependent sources are discussed a real world example of dependent sources is not illustrated. \nThere is a relatively large number of hyper parameters. It is not clear if the hyper parameter selection was done in a reproducible manner. I.e. if the result cannot be evaluated by a metric or qualitatively can the user practically tune the hyper-parameters? If so with what criteria? ",
            "clarity,_quality,_novelty_and_reproducibility": "\nClarity: \nIn the abstract, the understanding of domain and \"their presumed sets\" needs to be made more explicit. \n\nFigure 1 is very detailed but it is presented without sufficient introduction to the notations, these should be in the caption. \n\nAmbiguous 'they' in \"or example, the lateral connections of the outputs in Bozkurt et al. (2022) are based on the output autocorrelation matrix, while they are based on the inverse of the output autocorrelatin matrix\" (also note the typo).\n\nEquation 3e can be interpreted as a surrogate for H(Y) - H(Y|X), which is discussed in the appendix but should be included in the body. \n\nSection 4.2 \n\"to exclude the negative frame effect\"\n\nNovelty:\nThe work builds on an existing objective and follows other biologically plausible networks. Thus, this is incremental novelty. \n\nQuality:\nThe derivations appear sound and the overall presentation is logical. \n\nAlthough dependent sources are discussed a real world example of dependent sources is not illustrated. \n\nThe case of separation with as many observations as sources is not discussed.   I.e. 5 mixtures of 3 sources is demonstrated. How well does it work for 3 mixtures of 3 sources? \n\nReproducibility: \nThere is a relatively large number of hyper parameters. It is not clear if the hyper parameter selection was done in a reproducible manner. I.e. if the result cannot be evaluated by a metric or qualitatively can the user practically tune the hyper-parameters? If so with what criteria? ",
            "summary_of_the_review": "The paper proposes an online algorithm for blind source separation using a log-det based information maximization principle under a second order statistic.  The method builds from previous work, and is logical and appears correct. Paper does thorough job in describing how this leads to biologically plausible cases. But the clarity of the presentation at times could be improved. Overall novelty is mostly incremental. Results are detailed but challenging real world cases are not covered. The knowledge of the domain of the sources is required, and it is unclear how a user would be able to blindly select hyper-parameters. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_C2ij"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_C2ij"
        ]
    },
    {
        "id": "4n_pg0bMFb",
        "original": null,
        "number": 2,
        "cdate": 1666694616232,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666694616232,
        "tmdate": 1666694616232,
        "tddate": null,
        "forum": "8JsaP7j1cL0",
        "replyto": "8JsaP7j1cL0",
        "invitation": "ICLR.cc/2023/Conference/Paper4342/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an online algorithm for blind source separation that is represented by a multi-layer network with biologically plausible weight/activity dynamics.",
            "strength_and_weaknesses": "### Strengths\n\nTheoretically grounded algorithm for blind source separation.\n\nDerivation of network dynamics that can implement this algorithm online.\n\nGood performance despite the online setting and correlation matrix approximations.\n\n### Weaknesses\n\nWriting is lacking in some parts (see below), but contribution-wise I don't see any issues. ",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nOverall, the paper is decently written. However, some of the results/derivations in my opinion should be moved to the appendix to leave more space. \n\nThis space can be used to, for instance, write down dynamics of all weights and activations for the whole network in one place. Right now the dynamics are presented one by one, which is somewhat hard to follow.\n\nMinor comments:\n\n\\hat R in Eq.3 should be defined in the main text.\n\n(personal preference) Adding Eq. when referencing equations is more readable.\n\nLabelling your methods as (ours) in tables/plots should improve readability.\n\n### Quality \n\nThe paper is technically solid. Moreover, the experiments are very extensive and cover several source separation setups, showing decent performance of the proposed algorithms.\n\n### Novelty\n\nThe work is, to my knowledge, novel.\n\nThe discussion around Eqs. 14-15 reminds me of the FORCE rule [Sussillo and Abbott, 2009], which ended up with the same issue of approximating recursive least squares (RLS) to make it more plausible. The RLS derivation is similar to what you have in Appendix B. Probably it\u2019s worth mentioning as a background citation? I think they ended up with a similar more plausible approximation to RLS, so some approximations in that or the follow-up work could improve your algorithm too, although it might be out of the scope of this work. See, for instance,\n\nEmergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning\nGM Hoerzer, R Legenstein, W Maass, 2014\n\n### Reproducibility\nThe code is provided but not checked. Additionally, the hyperparameters are listed in the appendix.\n",
            "summary_of_the_review": "Good paper with a solid technical and experimental contributions. I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_LCHG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_LCHG"
        ]
    },
    {
        "id": "aMRd0Wn_zz",
        "original": null,
        "number": 3,
        "cdate": 1666754352104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754352104,
        "tmdate": 1668267036679,
        "tddate": null,
        "forum": "8JsaP7j1cL0",
        "replyto": "8JsaP7j1cL0",
        "invitation": "ICLR.cc/2023/Conference/Paper4342/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "I thank the authors for providing a thoughtful reply addressing most of my concerns. Together with the clarifications I am now convinced that this work is a good contribution. I have adjusted my score accordingly.\n\nThe paper proposes a biologically plausible source separation method with local updates and batched online learning.\n\nI can see that the paper is based on a recent publication by Erdogan (2022). Generally, this work cites a large number of previous works by that author. I cannot judge how novel the contribution in this paper is compared to those previous works. The framework and mathematical formulation are extremely complex and I was not able, within reasonable time investment, to digest and asses the full breadth of it. The applications seem somewhat niche. And the only real data application does, as far as I can tell, not include the interesting setting of correlated sources. By the way, the same setting (in more interesting nonlinear scenarios) is also investigated in this (https://proceedings.mlr.press/v139/trauble21a.html) work.",
            "strength_and_weaknesses": "The paper is well written, with a thorough mathematical derivation. It also cites many relevant existing works.\n\nThe paper is extremely dense, possibly incremental on previous work, somewhat niche.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is not clear how important the biological plausibility is and that does not seem be further explore by the paper.\n\nNits:\n\nBoth the title and the abstract are very hard to understand and one is left wondering what the paper is about. Latent causes are not defined, domains are not defined, what are presumed sets?\n\nSecond paragraph: The fact that biological receptive fields look like ICA filters should not be confused with the much stronger claim that the brain is doing BSS.\n\nAll figures need more explanation in the caption.\n\n1.1.1 second to last sentence: \u2018while\u2019 ? \u2018they\u2019; autocorrelation\n\n2.1.iii superfluous dot?\n\n2.1 after list the \u201c\u201d quotation marks are wrongly formatted\n\nFig3 define the axis labels in the caption",
            "summary_of_the_review": "The work is hard to digest and it is not clear how incremental it is compared to previous work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_hwJs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4342/Reviewer_hwJs"
        ]
    }
]