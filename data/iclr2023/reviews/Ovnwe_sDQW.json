[
    {
        "id": "FNAZrLtRC44",
        "original": null,
        "number": 1,
        "cdate": 1666590676109,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590676109,
        "tmdate": 1666590676109,
        "tddate": null,
        "forum": "Ovnwe_sDQW",
        "replyto": "Ovnwe_sDQW",
        "invitation": "ICLR.cc/2023/Conference/Paper5413/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Providing informative rewards is crucial for effective reinforcement learning. Prior IRL methods overfit to demonstrations and fail to learn generalizable rewards. To combat this issue, this paper proposes BC-IRL, which uses gradient-based bi-level optimization to learn the reward. The authors evaluate on two continuous control tasks against IRL and imitation learning methods.",
            "strength_and_weaknesses": "Strengths:\n- The paper proposes a novel algorithm for learning rewards with bi-level gradient-based optimization.\n\nWeaknesses:\n- Unclear in what sense the authors are referring to when they use the term \u201cgeneralization\u201d. In the experiments, the generalization seems like it is limited to slightly different start and goal distributions. This should be made clearer.\n- GAIL is a method that trains a policy that matches the state-action distribution of the expert data, it should be clarified how the goal here is different since this method also aims to learn a policy that matches the behavior of the expert.\n- Experimental evaluation is very limited. Only 2 tasks of a similar nature are evaluated, and the tasks are reaching, which is very simple. Also, the environments have a low-dimensional state space, despite the simplicity of the tasks.\n- How does this compare to AIRL or GAIL where the discriminator learned uses mixup regularization or spectral norm? These are 2 common techniques (among others) for making the discriminator less brittle. Much more experimentation is needed to conclude how much more generalizable the rewards learned by BC-IRL are.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality need to be greatly improved, see above for more detailed discussion of weaknesses.",
            "summary_of_the_review": "Due to issues with clarity and quality, this paper does not seem ready for acceptance.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_pza4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_pza4"
        ]
    },
    {
        "id": "XDhsznZIDm",
        "original": null,
        "number": 2,
        "cdate": 1666641421362,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641421362,
        "tmdate": 1668419441401,
        "tddate": null,
        "forum": "Ovnwe_sDQW",
        "replyto": "Ovnwe_sDQW",
        "invitation": "ICLR.cc/2023/Conference/Paper5413/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a new form of IRL in which the learned reward is not based on the state occupancy, but is instead computed in order to lead a policy-gradient learner at imitating the demonstration.\nThis is done with a meta-learning approach, in which the inner loop updates the policy (with a policy-gradient objective) and the outer loop updates the reward (with a BC objective).\nThey observe that such a reward that is not based on occupancy is better at generalising behaviours and is more robust to changes of the initial distribution.\n",
            "strength_and_weaknesses": "The method is novel, easy to understand and implement and the results looks impressive.\n\nI however have a concern regarding the main tool-experiment as shown in Figure 1 and 2.\nEven if the reward is only on the states that are occupied by the demonstration (resulting in a X-shape), the learned behaviour with such a reward should still learn to reach the highest rewarding states. \n\nFor example, a Value-function associated with such a reward would diffuse values outside of the X cross.\nIn this experiment there is no value-function, but they use PPO that directs policies updates with generalized advantage estimators which behave exactly like a value-function (it is a non-biased estimator). Therefor the fact there is no reward outside the cross should not significantly affect the optimal behaviour.\n\nI would understand that the reward obtained by BC-IRL is much more dense, and so accelerates the learning. But AIRL well implemented should not fail on this task (at least with enough learning steps). Does AIRL ends up learning the good behaviour after a (much) longer training?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, the method is simple to understand and well described. \nI have never seen such an approach for IRL before.\n",
            "summary_of_the_review": "I think this paper is definitely novel and shows impressive improvements of SOTA approaches, but I have a strong concern regarding the implementations and behaviour of the baselines. In that doubt I'm giving a low score, but I am ready to significantly increase it if authors adresse my comments with a convincing explanation.\n\n\nAfter reading authors answer and revisions, I've increased my socre.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_9FVR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_9FVR"
        ]
    },
    {
        "id": "izTNRmXmBtQ",
        "original": null,
        "number": 3,
        "cdate": 1666800429269,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666800429269,
        "tmdate": 1666800983077,
        "tddate": null,
        "forum": "Ovnwe_sDQW",
        "replyto": "Ovnwe_sDQW",
        "invitation": "ICLR.cc/2023/Conference/Paper5413/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission proposes a novel IRL algorithm.  \nInstead of prior methods optimizing maximum entropy objective or adversarial objective for reward learning, the proposed method (BC-IRL) seeks reward function that makes a policy updated from the reward be close to expert behavior.\nTo optimize such an objective, the few-step updated policy parameters are bi-level optimized with respect to reward parameters.\nLearned reward function shows improved generalization ability than existing IRL methods and thus it can able to train more robust policy.",
            "strength_and_weaknesses": "### Strength\n- The proposed method meaningfully improves the performance of previous imitation learning methods.\nBoth the generalization quality and robustness of learned policy are consistently outperforming prior imitation learning methods.  \n- The qualitative results clearly depict the benefit of the method and how it can improve generalization power.\n- Paper is well written and easy to follow.\n\n### Weakness\n- As clearly described in the paper, BC-IRL should be used with a backbone RL algorithm that can update policy using reward parameters.\nThis limitation will exclude a meaningful portion of conventional RL algorithms.\n- Evaluations are done in a relatively short-horizon and less diverse set of the domain. Few more results on other domains can strengthen the experimental support.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper has high clarity and quality.\nAll text and graphics are well-polished and structured.\nCode, hyperparameters, and experimental details are provided for reproduction.",
            "summary_of_the_review": "This paper is very clearly written and the proposed method is strongly supported by experimental results.\nThere is a methodological limitation but it doesn't outweigh the benefit of sharing this idea with the community.\nThus, I vote to accept this paper, and I'd be convinced stronger if I can see results in a few more challenging evaluation domains.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_RnMY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5413/Reviewer_RnMY"
        ]
    }
]