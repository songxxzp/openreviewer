[
    {
        "id": "3p12Qeii5I",
        "original": null,
        "number": 1,
        "cdate": 1666637023104,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637023104,
        "tmdate": 1668953613099,
        "tddate": null,
        "forum": "xkSlKCYyV_",
        "replyto": "xkSlKCYyV_",
        "invitation": "ICLR.cc/2023/Conference/Paper4459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for being able to train high-quality policies on Atari benchmarks while having an experienced memory that is two orders of magnitude smaller. the motivation is that storing large amounts of data and experience memory, especially for image-based environments, can be very memory intensive. If you can reduce this demand for memory, we might be able to reduce the hardware requirements needed to run these algorithms. The method proposes combining types of experience replay methods for surprise and measuring on-policiness to instead remove tuples from the replay buffer that have the lowest combination of these two metrics. The method includes some analysis on the Atari environments that may indicate that performance is better or similar to prior methods that use significantly more memory stored in their experience replay buffer.",
            "strength_and_weaknesses": "pros\n- The method proposed in the paper is clean and very easy to understand and should be fairly easy to reproduce\n- Some of the motivations are also very clear. Being able to reduce the memory requirements for the algorithms we use these days can have many important impacts on making algorithms easier to use and more widely accessible to people.\n\ncons\n- One challenge with this method also has to do with the experimental analysis that it performs on the Atari environments. There are few examples where researchers are concerned with not having enough available memory to train in the Atari environments. Including additional environments for which these issues occur would help boost the paper's motivation. \n- The novelty of the method can also be seen as incremental in that it appears to be a combination of two prior well-understood methods for doing experience replay but instead is used for removing experience from the replay buffer instead of picking which ones to train the current policy.",
            "clarity,_quality,_novelty_and_reproducibility": "\n- One of the important points of training an off-policy algorithm is having data that is less on-policy. In order for training to progress the correlation between the data and the current policy needs to be broken to help preserve the IID assumption. How does the preference for keeping on-policy data not interfere with the off-plicy training.\n- Have the authors considered instead compressing the image information? Most image information is stored as raw uncompressed data on the computer it would be good to understand the relation between this concept and that method that could instead reduce the amount of memory required by properly compressing the images in the data set.\n- The method proposed in the paper is clear and concise, but it suffers from showcasing a large amount of novelty. The work could appear to be a combination of one method for computing surprise and another method for evaluating on-policiness and then just using that as a replay buffer prioritization metric. The importance of this novelty should be described better.\n- Does the method proposed in the paper, in particular the measure for on-policiness, imply that it is only designed to work with the DQN algorithm and will not work for continuous action-type environments that use TD3 and SAC?\n- The variance over the results in figure 3 is significant. This is implying that it's unclear if one method is performing better than the other, given the noisy results in those figures. To better understand if one method is performing better than the other, the author should run a series of statistical tests to see whether or not there is a meaningful difference between the distributions across the compared algorithms.\n- Why are the Atari environments a good set of environments for the comparison of this algorithm? There is little prior work that evaluates over those particular environments and is concerned about having a lack of memory available to train a policy. It would make much more sense to try and evaluate these methods over some types of robotics or real-time environments that can have real and quite limited hardware constraints.\n- Why is 10,000 experience memory samples being used for the comparisons in this paper? It is important to be able to justify this particular number and why this is a reasonable amount of experience memory compared to other values.\n- It is not clear if the results displayed in figure 3 and figure 4 are converging to at least locally optimal policies. The experiments in this section should be run longer such that we can truly understand the more long-term performance comparisons between these algorithms.\n- In addition, the results indicating that learning is unstable after further training is concerning. The goal of reducing the memory requirement for the algorithm is justified, but not at the cost of the training stability.\n",
            "summary_of_the_review": "The proposed method for reducing the memory requirement for training DQN is admerable. The analysis in the paper needs more data to understand better how the stability of the new algorithm. Also, the novelty of the method appears to be limited.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_Lf8C"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_Lf8C"
        ]
    },
    {
        "id": "mhx30sY-Tc",
        "original": null,
        "number": 2,
        "cdate": 1666878637940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666878637940,
        "tmdate": 1666878637940,
        "tddate": null,
        "forum": "xkSlKCYyV_",
        "replyto": "xkSlKCYyV_",
        "invitation": "ICLR.cc/2023/Conference/Paper4459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to make off-policy RL algorithms more memory-efficient by reducing the size of the experience replay buffer. Specifically, the proposed algorithm keeps experience tuples with higher importance and discards unimportant ones. The importance of a tuple is measured by its \"surprise\" (TD error) and \"on-policyness\". Experimental results in Atari games show that the proposed algorithm achieves comparable performance with only 1% size of the original replay buffer, compared with the baseline method (Rainbow).",
            "strength_and_weaknesses": "The paper is well-motivated and the writing is really good. The proposed method is indeed much more memory-efficient than Rainbow in Atari games when agents are trained with 1M steps.\n\nHowever, as also pointed out by the authors, when agents are trained longer (10M steps), the proposed algorithm actually performs much worse than the baseline in some tasks, even using a replay buffer with the same size as the one in baseline. The discussion of the limitation in Section 6 is not convincing or not supported by experiments. This paper can be much better if further experiments are conducted to show and analyze the limitation of instability with longer training. Moreover, a study of the affect of different buffer sizes (e.g., 1k, 10k, 100k, 1M) is necessary in order to see the whole picture.\n\nMinor issues and questions:\n- In Section 4.3, the function $f^{priority}$ is not defined.\n- In Figure 5(a), it seems that the state coverage of the proposed method (with 10K samples) is much better than the baseline (with 1M samples). How is it possible? Or maybe I interpret it incorrectly.\n- In Algorithm 1, will all priority scores in the replay buffer be recomputed after the Q function is updated? If so, will this slow down the training process? Can you show the wall-clock training time? If not, since the priority calculation function depends on the current Q value function, the pseudo-code hints that all priority scores in the replay buffer are recomputed, as shown in Line 8 & 11. It might be better to replace $f(\\tau)$ with some other symbol to show that the scores are not recomputed in Line 8 & 11.\n\nSuggestions\n- Instead of moving straight to Atari games, conducting experiments in toy environments might provide more insights. For example, no results are presented to support the claim that \"The surprise promotes the buffer to store experience tuples that cover the state space\" in Section 6. This claim can be better verified in low-dimensional tasks, such as Mountain Car.\n- My personal experience suggests that a less frequent update for Q function helps improve agent performance when the replay buffer is small.\n- In Figure 4, it seems that adding \"surprise\" is not that helpful and \"on-policyness\" is main reason for performance improvement. Did you try removing \"surprise\" (thus reduce negative effects as mentioned in Section 6) and redo the experiments in Figure 6?\n- Some methods listed in this [paper](http://arxiv.org/abs/1809.05922) might help to improve the performance of the proposed algorithm, such as reservoir sampling.",
            "clarity,_quality,_novelty_and_reproducibility": "Good.",
            "summary_of_the_review": "Overall, this paper is well-motivated, focusing on an important problem. However, the performance drop of longer training limits the application of the proposed method. I believe this paper can be further improved with more analytical experiments in toy tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_FE4B"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_FE4B"
        ]
    },
    {
        "id": "dJ0jJeQsf_c",
        "original": null,
        "number": 3,
        "cdate": 1666983469541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666983469541,
        "tmdate": 1671058558356,
        "tddate": null,
        "forum": "xkSlKCYyV_",
        "replyto": "xkSlKCYyV_",
        "invitation": "ICLR.cc/2023/Conference/Paper4459/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a mechanism for selecting transitions to remove/replace from the replay buffer during training of an RL agent. The samples are selected based on a priority function that is a multiplication of surprise (TD-error) and a on-policy weight of the transition. Using this priority function, the paper claims that for shorter settings, their approach outperforms agents with comparably sized replays and is able to match agents with much larger replays on most Atari games. ",
            "strength_and_weaknesses": "Strengths:\n- This paper is very well written, easy to follow, and well motivated. \n- The experiments are done on a wide range of the Atari games. For most of the games (although not all), this method does outperform the simple FIFO replacement strategy of a comparably sized buffer, and is able to close the performance gap with a larger sized buffer.\n- The ablation experiments show the contributions of each component in the priority weight, and show that both surprise and on-policyness do contribute to better performance.\n- This paper also tried running on longer timescales, with buffer size the same as what is typically used for Atari in the literature, and showed that their method actually does worse than traditional buffers on several environments on this longer timescale. While this is a limitation, it's good that the authors do talk about it, and it potentially highlights a future direction of work.\n\nLimitations/Questions:\n- How does your method work with n-step returns in Rainbow? If you remove a transition in the middle of a trajectory, does that invalidate several other transitions as well? If so, it might be worth exploring this relationship of the number of invalidated transitions and the horizon value in n-step, and see if a replacement strategy could be devised to work around that.\n- As mentioned above, the paper does mention limitations of the method when learning in longer timescales.\n\nEdit:\nAfter the discussion with other reviewers, I am lowering my score to marginal reject. I still think that this is an idea worth pursuing, but I think you need a few more experiments/analysis.\n- As other reviewers pointed out, using 10k is a bit arbitrary. You should do a sweep over different replay sizes to figure out the effect of replay size on different tasks. \n- If you are using Atari, you should show some runs with the full 50m steps.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of experiments in this paper are very good. It was easy to read, and everything was well motivated. Reproducibility seems good, as they provide the code for their experiments. In terms of novelty, this work does seem a bit incremental, but still novel.",
            "summary_of_the_review": "This is a well written paper with good experiments supporting their methods ability enable learning of agents with small replay buffers at short timescales. I recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_HQEE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_HQEE"
        ]
    },
    {
        "id": "A5MebyxZu_",
        "original": null,
        "number": 4,
        "cdate": 1667097348661,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667097348661,
        "tmdate": 1667097834648,
        "tddate": null,
        "forum": "xkSlKCYyV_",
        "replyto": "xkSlKCYyV_",
        "invitation": "ICLR.cc/2023/Conference/Paper4459/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to make Rainbow more memory efficient. In particular, it proposes a way to only save data which seems more important using the combination of on-policyness and surprise metrics. Surprise metric is built based on TD-error and on-policyness\u00a0measures how far behavioral policy is from target policy. One major difference between priority replay buffer and this method is priority replay buffer does not discard samples and saves all data until capacity allows; however, this method discards samples which are unimportant. To show the effectiveness of their method, this paper uses\u00a052 games\u00a0of\u00a0Atari benchmark.\u00a0",
            "strength_and_weaknesses": "While the idea of only saving high valued samples in the reply buffer and using smaller replay size is interesting and can be valuable, I have a couple of concerns with this paper:\n\n- Computational complexity:\u00a0\nThe main motivation for this paper is to \"make RL training more applicable to low-resource environments''. Although this is the right problem to study, \u00a0the current method doesn't come for free and it increases computational complexity of a method as priority function requires to search through the replay buffer at each time step and estimate the score ( search can be more expensive here). The paper \u00a0doesn't analyze the complexity of the proposed method and does't provide insights about computation and memory trade-off.\n\n- Baseline methods:\nIn section 5.1, even though authors discuss various baseline methods, they compare their method with these baseline only on 6 environments. While I understand running Atari is expensive, since the contribution of this paper heavily relies on the results, the current results are not conclusive. I'd like to note that authors reported results with smaller replay buffers but the same rainbow method, these results are not as important as comparing with other baselines. Moreover, important baseline like \"Improving computational efficiency in visual reinforcement learning via stored embeddings.\" is missing in the experiments. Also, likelihood free ration and importance sampling weighting can be used as other baselines.  \n\n\n- Figure 2:\nIt is not clear to me what the baseline method in Figure 2? If it is 1K or 10K baseline, they are not appropriate baseline methods. It should be compared with methods which are discussed in the paper. \n\n- Validity of Results:\nComparing Baseline (1M) reported in Figure 8 with Dopamine ( https://google.github.io/dopamine/baselines/atari/plots.html), I notice a huge discrepancy between this paper's results and Dopamine which makes me anxious about validity of this results. Take Gopher as example, Dopmaine shows rainbow gets to average return of 10000 after 200M samples while this paper shows 1000. The same is true about other environments. \n\n- Writing:\nWhile the paper is written well in general, there are notable issues. Q-function is distribution but the equations treat it as not being a distribution. I think this should be fixed. Also, this paper discusses continual learning in the related work sections which I have a hard time to find any relationship with. Instead, this paper should have focused on related works which are relevant to this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The priority metrics proposed in this paper are mainly based on previous works (i.e. TD-error in \"Prioritized experience replay\" paper, policyness weight in \"Soft-actor critic\"), but their combination can be considered new. However, since this is an empirical paper, the results should provide a strong signal about the significance of this method which they don't at the moment.  ",
            "summary_of_the_review": "The main and critical concern about this paper is the experiments which are not conclusive considering the fact that this is an empirical paper. Another concern is this paper only studies pixel-based environments.  I don't understand why this paper can not be used with state-based environments like MuJoCo. I'd make my final recommendation after rebuttal as I'd like to hear the authors' rebuttal. That being said, this paper does not meet ICLR acceptance bar as it stands.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_f3A3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4459/Reviewer_f3A3"
        ]
    }
]