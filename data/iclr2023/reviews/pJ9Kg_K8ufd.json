[
    {
        "id": "agVGQToPq8",
        "original": null,
        "number": 1,
        "cdate": 1666020379964,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666020379964,
        "tmdate": 1666020379964,
        "tddate": null,
        "forum": "pJ9Kg_K8ufd",
        "replyto": "pJ9Kg_K8ufd",
        "invitation": "ICLR.cc/2023/Conference/Paper3210/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces DI-Nets, a formulation of neural networks that is invariant to discretization of the input space. More specifically, the authors consider building models that take neural fields as input, and unlike other recent work, do not depend on the specific parameterization of that neural field, which is desirable. The authors prove various theoretical properties of their model, including discretization invariance for the resulting networks and their gradients as well as a universal approximation theorem. The authors test an instantiation of their model in various toy experiments. \n",
            "strength_and_weaknesses": "**Strengths**\n\n- As neural fields are becoming increasingly popular, the paper tackles an interesting and timely problem, namely that of performing various machine learning tasks on neural fields.\n- The proposed model is invariant to the parameterization of the neural field, which is a desirable property. The experiments training on SIRENs and testing on Fourier feature networks highlight this nicely.\n- The paper is well written and polished and the figures are nice.\n- The authors provide a thorough theoretical analysis of the proposed model.\n- The appendix is thorough and well done, containing lots of details and exciting ideas for future work.\n\n**Weaknesses**\n\n- While the authors claim their model is discretization invariant, this statement is contradicted by experimental results. Of course, the model is discretization invariant in the sense of Definition 2, but the both the variation of f and discrepancy of X can be large, which means that the model is not discretization invariant in the practical sense of the term. This is demonstrated in numerous experiments throughout the paper, e.g. Table 2 and Table 4. I think it would therefore be good to temper the claims about discretization invariance or at least highlight early on that the theoretical definition of discretization invariance does not necessarily imply discretization invariance in practice.\n- In general, it feels like the theory is so \"general\" that it doesn't really provide any particularly useful guidance on how to actually instantiate a model. Indeed, the resulting models are very similar to various continuous convolution models.\n- The experimental results are quite weak (this is also acknowledged by the authors). The authors only perform small scale experiments and compare their results to fairly weak baselines (e.g. a 2 layer CNN). While it is okay to underperform CNNs designed for image classification, it would be interesting to include comparisons to other neural field based approaches (or explain why they are not relevant).\n- One of the advantages of using neural fields is that they typically provide a very compact representation of data (which was the motivation for the original 3D works on neural fields). However, using DI-Nets, we are back to sampling the original \"data space\" which, for 3D experiments for example, still would require a very large number of samples and so defeats the purpose of using the compact neural field representation in the first place. In contrast, other methods that act directly on some compact parameterization of a neural field are likely to scale better. The scaling of DI-Nets is then similar to just acting directly on the discrete signal (as mentioned in the appendix, \"DI-Net's complexity is similar to that of a discrete model with an equivalent architecture\"). I believe it would be worth discussing this when comparing to related work.\n- This might be a style preference, but the language/formulation sometimes seems unnecessarily formal, and a more intuitive presentation might have worked better.\n\n**Typos**\n\n- The figure reference on page 26 in the appendix is wrong (fig:layers left)\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThe paper is well written and the figures are nice. This is likely a personal preference, but the paper often feels unnecessarily formal, when a more intuitive approach might have been more effective. \n\n**Quality**\n\nThe paper is well presented, experiments are quite thorough (although small scale) and the paper openly acknowledges limitations. As such, I believe this is quite a high quality paper.\n\n**Novelty**\n\nThe proposed method is novel in the sense that it provides a new way to handle deep learning on neural fields. However, the resulting models are, in practice, quite similar to existing models for continuous convolutions.\n\n**Reproducibility**\n\nThe details and explanations provided in the paper seem to be sufficient for reproducing the results in the paper. The appendix is detailed.",
            "summary_of_the_review": "Overall, I think this is a fairly good paper, which will be of interest to the ICLR community. The paper is well written, the method is sound and the experiments are fairly interesting. However, there are some issues around discretization invariance as well as empirical performance in practice. I therefore recommend a weak accept.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No ethical concerns.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_XJni"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_XJni"
        ]
    },
    {
        "id": "GQ88hAZc7gG",
        "original": null,
        "number": 2,
        "cdate": 1666575584604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575584604,
        "tmdate": 1666575584604,
        "tddate": null,
        "forum": "pJ9Kg_K8ufd",
        "replyto": "pJ9Kg_K8ufd",
        "invitation": "ICLR.cc/2023/Conference/Paper3210/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a general framework of discretization invariant network, DI-Net. The\nauthors give a theoretical upper bound w.r.t. variation and discrepancy in the finite samples case.\nThey also reveal that DI-Net is a generalization of existing neural fields networks such as CNN.\nThey demonstrate the performance of DI-Net against CNN and FCN with different discretization\nstrategies.",
            "strength_and_weaknesses": "Strength\nThis paper gives a general idea of discretization invariant learning. It connects its key idea\nDI-Nets with existing popular architectures, such as CNN and neural operators. It offers a general\nperspective of these models.\n\nWeakness\nThe pseudo code of algorithm 1 and 2 in page 7 is negligible as they are the standard procedure\nof general learning process.\nNeural operators, as the authors mentioned in the paper, is an important category of discretiza-\ntion invariant network. But the numerical experiments focus mainly on the computer vision, e.g.\nclassification and segmentation. I think it would be better if there were an experiment on neural\noperators. Personally, I think such experiments are an important aspect of discretization invariant\nlearning.\nIn the numerical experiments part, the authors mainly compare different methods of dis-\ncretizaion, such as MC, QMC, grid and shrunk. But there were few attention on the discretization\nlevel. There was only a partial comparison in Table 1 on 32*32 and 64*64. I think it would be\nbetter if there were comparison between more sampling levels (of the same sampling method).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well structured, but I think it would be more helpful to add more details about\nnetwork implementation.\nI think the DI-Net might not be stable, at least numerically. As the authors point out in\nsection 5.1, CNN outperforms DI-Net in low-resolution. And I think the superiority of DI-Net in\nhigher-resolution is not obvious enough to convince me. Table 3 also shows that DI-Net can not\ndefeat FCN under certain condition.\nThe authors did not provide their code, instead they offered some configurations of experiments,\nbut not all. They offered information about depth, learning rate, dataset, learning rate, optimizer\nand discretization level. But details of the network implementation were not mentioned. One\nmight encounter difficulties when reproducing the results.",
            "summary_of_the_review": "I think this paper offers a general idea of discretization invariant learning on neural fields and\nsupplies plenty of theoretical results. However, I think the detail of the DI-Net is not abundant\nenough for readers. Also, personally the numerical results are not good enough to convince me the\nsuperiority of DI-Net.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_NWbG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_NWbG"
        ]
    },
    {
        "id": "Uvbj6HkwZ3",
        "original": null,
        "number": 3,
        "cdate": 1666636706047,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666636706047,
        "tmdate": 1669439756703,
        "tddate": null,
        "forum": "pJ9Kg_K8ufd",
        "replyto": "pJ9Kg_K8ufd",
        "invitation": "ICLR.cc/2023/Conference/Paper3210/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel approach for learning over neural fields that is discretization invariant. The authors propose a mathematical framework and design a network following this definition. They show experimental results on several vision benchmarks.",
            "strength_and_weaknesses": "Strengths:\n- The paper addresses an important problem, the dependency of standard learning approaches on discretization.\n- The work in novel and very interesting\n\nWeaknesses:\n- The paper is not well written. \n(1) Lots of space wasted on trivial points, like prop.1 that simply says that the function is well defined. Or the list of operations later that doesn't really helps understand the work proposed. Also Prop. 2 is trivial from definition (eq. 1).\n(2) The main technical part of the paper is glossed over, 3 lines in bullet point 3 in page 6. Everything up to that point was simply replacing an integral with an average.\n(3) The whole mathematical machinery feels redundant or under-utilized. How is it different then convolutions on a non-uniform grid?\n- The experimental part should compare to previous works such as Jiang et al \"CONVOLUTIONAL NEURAL NETWORKS ON NONUNIFORM GEOMETRICAL SIGNALS USING EUCLIDEAN SPECTRAL TRANSFORMATION\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is novel and the quality of the work seems high\nMy main concerns are clarity and reproducibility",
            "summary_of_the_review": "The paper proposes an interesting and novel approach to an important problem, but I do not think it is clear enough and I have very low confidence in my ability to reproduce the results from the papers description.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_6WWK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_6WWK"
        ]
    },
    {
        "id": "gqfMSlxYw0f",
        "original": null,
        "number": 4,
        "cdate": 1666714621006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666714621006,
        "tmdate": 1669301955617,
        "tddate": null,
        "forum": "pJ9Kg_K8ufd",
        "replyto": "pJ9Kg_K8ufd",
        "invitation": "ICLR.cc/2023/Conference/Paper3210/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a general framework for building neural networks that take neural fields (NFs) - vector fields over compact spaces - as input and/or output. Processing NFs requires choosing a discretization on the compact space. The paper studies the sensitivity of maps between NFs on the choice of discretization. They define \"Discretization Invariant\" (DI) maps between NFs to be those for which the discretization error on a finite sample X is bounded by the variation of the input NF and the discrepancy of X.\n\nThe paper then proposes a family of DI invariant maps to be continuously defined convolution-like maps between fields on a particular discretization. The authors prove that this construction is universal and that the gradients can be estimated consistently on finite samples.\n\nVarious train/test discretizations are evaluated on classification on a NF version of imagenet, segmentation and signed distance function prediction.",
            "strength_and_weaknesses": "Strength:\n- Appears a thorough and novel theoretical analysis of maps between NFs, proving desirable properties\n- Gives an interesting empirical insight into how to choose discretizations.\n- The method is evaluated on three different interesting problem domains.\n\nWeaknesses:\n- I'm a quite confused about the weak derivatives in eqns (3) and (6) and the assumption that the NF is in the Sobolev space. Is the existence of the weak derivatives assumed throughout? If so, that's not made very explicit. Step 2 in the sketch of the proof of Theorem 2 also only mentions function evaluation, not taking the weak derivatives. Does it matter for the universality if the DI-Net takes gradients as input? It'd be great if the authors could clarify the role of these weak gradients.\n- The paper cites other methods to process NFs, such as via hypernetworks, but only compares grid discretizations to other discretizations.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is hard to read and uses many mathematical concepts without clear introduction in the main text. Examples include:\n  - ideal, top of page 4\n  - variation and discrepancy in def 2\n  - weak derivative\n  - Fr\u00e9chet differentiability\n\nQuality: as far as I can tell, the mathematics are sound.\n\nNovelty: the theoretical insights are novel, but the methodological innovation seems somewhat limited.\n\nReproducibility: the paper appears reproducible.",
            "summary_of_the_review": "The paper does a novel in-depth analysis of maps between NFs, but it is difficult to follow and lacks comparisons to baslines.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_pEzC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3210/Reviewer_pEzC"
        ]
    }
]