[
    {
        "id": "pbHQRZNx_B",
        "original": null,
        "number": 1,
        "cdate": 1666173430433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666173430433,
        "tmdate": 1666173646242,
        "tddate": null,
        "forum": "J_Cja7cpgW",
        "replyto": "J_Cja7cpgW",
        "invitation": "ICLR.cc/2023/Conference/Paper4100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposed **consolidator** to efficiently fine-tune transformers. Consolidator has multi-branches structures, which can be merged into a single matrix and saved on the disk. Extensive experiments demonstrate the efficiency and effectiveness of consolidator on various vision tasks.",
            "strength_and_weaknesses": "#### Strength\n\n1. The design of the consolidator is parameter-efficient since its mergeable property. By extending a single branch to multiple branches, the consolidator has better flexibility and transferability.\n\n2.  The two-stage process maximizes the adaptation capacity without extra inference costs.\n\n3. The paper is well-written and easy to follow.\n\n#### Weaknesses\n\n1. The novelty of this work is limited. The mergeable design is proposed by LoRA [1] for efficient parameter finetuing. Besides, what's the main difference between the **Channel Reorder** and **shuffle** operator in ShuffleNet[2]?\n\n2. Some recent related works are not discussed, e.g. [3][4][5].\n\n3. The improvement is marginal compared with NOAH in Table 1.\n\n\n[1] Hu, Edward J., et al. \"Lora: Low-rank adaptation of large language models.\" arXiv 2021.\n\n[2] Zhang, Xiangyu, et al. \"Shufflenet: An extremely efficient convolutional neural network for mobile devices.\" CVPR 2018.\n\n[3] Bahng, Hyojin, et al. \"Exploring visual prompts for adapting large-scale models.\" arXiv 2022.\n\n[4] Chen, Shoufa, et al. \"AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.\" NeurIPS 2022.\n\n[5] Jie, Shibo, and Zhi-Hong Deng. \"Convolutional bypasses are better vision transformer adapters.\" arXiv 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow.\n\nA typo in Sec 3.2: **aim ti**.\n\nThere is no source code available so I can not evaluate the reproducibility currently.\n",
            "summary_of_the_review": "See above.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_2eph"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_2eph"
        ]
    },
    {
        "id": "-2ry0EpHKN",
        "original": null,
        "number": 2,
        "cdate": 1666623694103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623694103,
        "tmdate": 1666623726384,
        "tddate": null,
        "forum": "J_Cja7cpgW",
        "replyto": "J_Cja7cpgW",
        "invitation": "ICLR.cc/2023/Conference/Paper4100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new fine-tuning method for vision transformers that only trains and stores very few parameters. The fine-tuning method achieves both efficiency and effectiveness through its design of mergeable multiple branches and parameter shifting. Extensive experiments show the method surpasses other adapter methods and full fine-tuning on several transfer learning tasks. ",
            "strength_and_weaknesses": "Strength:\n1. Consolidator is a very effective method. Compared with other adapter methods and full parameter fine-tuning methods, Consolidator can achieve an almost optimal transfer learning effect.\n2. Compared to the adapter methods, the design in this paper can be merged with the original model so that the inference phase does not increase the cost of memory and computation time.\n\nWeakness:\n1. The prior of ChannelReorder operation is strong but the explanation of its effectiveness is weak. \n2. The parameters in all tables are stored parameters for Consolidator but not trainable parameters.\n3. It should be possible to save time compared to the full parameter finetune during training, I wonder why this is not reported, nor is it compared to other baseline training time.\n4. The value of saving storage space is not so great, especially for the base-level model, and there are few experiments on the Large model.\n5. There are slightly more spelling errors and grammatical errors. P3 workss; P4 ti, thw; P5 the all the; P7 accomodate.\n",
            "clarity,_quality,_novelty_and_reproducibility": "See Strength And Weaknesses",
            "summary_of_the_review": "See Strength And Weaknesses",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_zLNL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_zLNL"
        ]
    },
    {
        "id": "xjOydozyBjE",
        "original": null,
        "number": 3,
        "cdate": 1666724528938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666724528938,
        "tmdate": 1670935232005,
        "tddate": null,
        "forum": "J_Cja7cpgW",
        "replyto": "J_Cja7cpgW",
        "invitation": "ICLR.cc/2023/Conference/Paper4100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a consolidator that modifies the pretrained model with the addition of a small set of tunable parameters to store the task-specific knowledge while freezing the backbone model during adaptation. A two-stage consolidation process is designed by merging corresponding parameters in the training-storage phase and loading-inference phase. Extensive experiments are conducted on various downstream tasks and the results outperform state-of-the-art methods with fewer stored parameters but superior performance. ",
            "strength_and_weaknesses": "Strength:\n\n(1) The authors propose a basic module, a consolidator for efficient transfer learning.\n\n(2) A two-stage consolidation process is designed. \n\n(3) Extensive experiments on various downstream tasks show state-of-the-art performance.\n\nWeaknesses:\nThis work aims to design a mergeable adapter for efficient transfer learning and proposes a two-stage consolidation process by merging corresponding parameters. I have some questions as follows. \n\n(1)  In Figure 1, the authors split channels and use zero padding, which might cause a heavy computational cost. Thus, the training time/speed should be shown for comparison with other tuning methods. Besides, I notice that the authors insert a consolidator in the FC layer in Figure 1. Will performance be further enhanced if the consolidator is inserted in other locations? Some recent tuning methods [1, 2] should also be discussed and compared in Table 1. \n\n(2)  Some other tuning baselines are missing in Table 4. The authors should conduct more experiments with other tuning methods for fair comparisons. \n\n(3)  The consolidator is designed for a linear layer. Can it be applied to other operational structures, such as self attention. Further, the current consolidator module is designed mainly in the vision transformer. In Table 1, Table 4 and Table 5, the proposed method obtains promising performance compared to the Full tuning. Push it forward, can it be extended to other model families such as CNNs or MLP architectures [3,4,5]? These will be interesting experiments. Due to the rebuttal time constraints, the authors do not need to add these extension experiments, but feel free to discuss them. \n\n(4)  There exists a large number of typos in the current version, the authors are able to polish it further. \n\n[1] \u2018AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition\u2019, in NeurIPS2022. \n\n[2] \u2018Scaling & Shifting Your Features: A New Baseline for Efficient Model Tuning\u2019, in NeurIPS2022. \n\n[3] \u2018A ConvNet for the 2020s\u2019, in CVPR2022.\n\n[4] \u2018AS-MLP: An Axial Shifted MLP Architecture for Vision\u2019, in ICLR2022.\n\n[5] \u2018RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\u2019, in CVPR2022. \n",
            "clarity,_quality,_novelty_and_reproducibility": "It should not be hard to reproduce this work because the authors describe the specific details for training and hyper parameters. The authors also propose consolidation process, which is also original in efficient transfer learning. ",
            "summary_of_the_review": "This work proposes a two-stage consolidation process for efficient transfer learning. It outperforms state-of-the-art methods with fewer stored parameters but superior performance. I tend to vote marginally above the acceptance threshold. Some additional experiments should be conducted to further improve the quality of this manuscript.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_ULMu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_ULMu"
        ]
    },
    {
        "id": "INbJAssxSUP",
        "original": null,
        "number": 4,
        "cdate": 1666770434360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770434360,
        "tmdate": 1666770702792,
        "tddate": null,
        "forum": "J_Cja7cpgW",
        "replyto": "J_Cja7cpgW",
        "invitation": "ICLR.cc/2023/Conference/Paper4100/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the adaptation of the well-trained transformer to downstream tasks on resource-limited devices. The authors introduced grouped connections with re-parameterization technology into the training process. The experimental results on serval vision tasks demonstrate the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "+ Compared to the baselines and other methods, the proposed methods could bring some improvements in accuracy and computation cost.\n+ Some ablation experiments are introduced to prove the effectiveness of the proposed method.\n\nThe main concerns are listed below. \n- Strictly restricted application scenario. The author introduces the application scenario in the abstract where the resource-limited devices cannot store a full copy of parameters but could provide computation to train/fine-tune the large models. It is a bit strange the devices with the ability to train DL models do not have sufficient storage.\n- The proposed method is irrelevant to Transformer. It is closer to the re-parameterization technology. \n- Some recent literature on the adaptation of Transformers is missing.\n\n[a] AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition, NeurIPS 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "I'm Ok with the quality, clarity, and originality of the work. ",
            "summary_of_the_review": "As written in the Strength And Weaknesses, the strictly restricted application scenario lowers the practical value of the proposed method.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_HhUN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4100/Reviewer_HhUN"
        ]
    }
]