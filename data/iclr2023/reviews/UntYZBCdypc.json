[
    {
        "id": "9o1FY9l7WJ0",
        "original": null,
        "number": 1,
        "cdate": 1666651165877,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651165877,
        "tmdate": 1666652068726,
        "tddate": null,
        "forum": "UntYZBCdypc",
        "replyto": "UntYZBCdypc",
        "invitation": "ICLR.cc/2023/Conference/Paper5392/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes S-mixup, an approach for using the well-known mixup methods on graphs. The authors present calculating an assignment matrix for the corresponding nodes between the two graphs so that they can use the image-like mixup approach in this domain. ",
            "strength_and_weaknesses": "**Strengths**:\n- The paper is well-written and clear.\n- The method is straightforward and easy to implement.\n\n\n**Weaknesses**:\n- Pairwise node correspondence calculation of the graphs is expensive. There should be a thorough time and computation complexity analysis between the proposed method and the baselines.\n- The performance improvement is incremental with respect to the baseline method, especially when the encoder is slightly more powerful (GIN instead of GCN). This raises the question that with large graphs and more powerful graph encoders, how much S-mixup can be practically useful.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear.\\\nThe novelty is limited and since the method is straightforward, I believe reproducibility wouldn't be an issue here. ",
            "summary_of_the_review": "Most of the points are mentioned in the strength and weaknesses section but in summary, although S-mixup shows performance improvements by considering the calculation cost and the limited novelty, I think that the method isn't ready for being published at this stage. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_bSxi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_bSxi"
        ]
    },
    {
        "id": "elb7rhcbp-",
        "original": null,
        "number": 2,
        "cdate": 1666677760941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666677760941,
        "tmdate": 1666677760941,
        "tddate": null,
        "forum": "UntYZBCdypc",
        "replyto": "UntYZBCdypc",
        "invitation": "ICLR.cc/2023/Conference/Paper5392/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new data augmentation method for graph-structured data. Unlike Euclidean spaces, on graphs, it is difficult to define interpolation due to the lack of mapping between two different spaces. Using soft alignments, the authors map one graph to the other and perform Mixup. The authors claim that the proposed method preserves important subgraphs (motifs) and the graph size. Also, unlike manifold Mixup, it performs data augmentation in the input data space at an instance level.",
            "strength_and_weaknesses": "Strengths:\n* The paper is well-written, and it has a comprehensive discussion about related work and the important aspects of data augmentation for graph-structured datasets.\n* This paper theoretically analyzes the discrepancy between the Mixup ratio and graph edit distance. It is not clear whether data augmentation (or Mixup) respecting Graph Edit Distance will be more effective or not. But it is a start to explore Mixup with graph-related distance measures.\n* The proposed method is simple and effective, and easy to implement. Compared to strong/recent baselines, the proposed method shows competitive performance. \n\t\t\nWeaknesses:\n* In data augmentation, controlling the strength and diversity of augmentation is crucial. In this sense, preserving graph size and motif may limit the diversity of augmentation. Graph size preserving is not always desirable. In some applications, one may want to generate samples with diverse graph sizes. This paper needs discussion about it.\n* The proposed method needs to be compared with broader related work such Hu et al. [1] GraphMix [2], Gaug [3] etc.\n* Figure 2, (c) has links between red and blue nodes. It can be viewed that the cycle motif is broken. Does motif preserving mean here keep all the edges of one of the original graphs? So, unlike standard Mixup, in this framework, the information from other graphs can be added to the target sample as additive noise on the adjacency matrix and features. So, then is no edge/node drop of the target graph possible in this framework?\n\n[1] Hu, Weihua, et al. \"Strategies for pre-training graph neural networks.\" arXiv preprint arXiv:1905.12265 (2019).\n[2] Verma, Vikas, et al. \"Graphmix: Improved training of gnns for semi-supervised learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 11. 2021.\n[3] Zhao, Tong, et al. \"Data augmentation for graph neural networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\nQuestions:\n1.  In figure 3, the IMDB-BINARY case, vanilla achieved the best performance at the 100th epoch. How was the model selection performed? One purpose of data augmentation is regularization to prevent overfitting, but early stopping or model selection are handy ways to address this problem. If the model at the last epoch was selected for evaluation, it might be misleading.\n2. The proposed method is limited to class-level tasks. Is it extensible to other tasks on graphs beyond graph classification?\n3. In eq (4), M is used. Did you consider the square root of M as in the normalized graph Laplacian matrix?\n4. What alpha was used for the beta distribution? How did you tune the hyperparameter? How sensitive is the performance gain to the hyperparameter tuning?\n5. How do you make sure that the augmented samples have the same labels? Or do you interpolate labels as well?\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: Overall, the method is simple, and the paper reads well.\n* Quality: The paper compares the proposed method with mixup techniques for graphs. The main idea is simple, interesting, and more importantly, effective. \n* Novelty: It has some interesting contributions, although the framework is not general enough to apply other tasks on graphs.\n* Reproducibility: The pipeline clearly was introduced, and I believe that it is reproducible. ",
            "summary_of_the_review": "Overall, this paper is well-written and studies the less-explored problem, data augmentation for graphs. The framework is simple and effective. The properties and behaviors of the proposed method have been well analyzed. More qualitative analyses comparing augmented samples by baselines will be useful to understand the value of this work. Also, the limitation of this work is not well discussed like when this work fails and when this work is effective. With some minor updates incorporating reviewers' comments, this paper can be further improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_KdGK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_KdGK"
        ]
    },
    {
        "id": "zUBSh9EQyD",
        "original": null,
        "number": 3,
        "cdate": 1667470596886,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667470596886,
        "tmdate": 1667470596886,
        "tddate": null,
        "forum": "UntYZBCdypc",
        "replyto": "UntYZBCdypc",
        "invitation": "ICLR.cc/2023/Conference/Paper5392/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a node assignmet based technique for graph data mixup, as mixup is a standard, popular and effective way of data augmentation for images, and its effectiveness is also recently explored in the graph area. The experimental results are convincing.",
            "strength_and_weaknesses": "Pros:\n\n1) The paper is clearly written and the idea is interesting and novel.\n2) The experimental results are convincing with state-of-the-art performance on public benchmarks.\n3) The current setting is basically supervised learning, while I believe it can be extended to general cases like self-supervised learning (see the following suggestion on related works ECCV22, SIGKDD22).\n\nCons:\n\nI did not find any specific concerns about the technical part of the paper. While the paper may acknowledge some more existing works to better position this work. \n\nI have some suggestions on expanding the related work part (if space permits or put some to the appendix).\n\nIn the related work part, the authors may also briefly discuss:\n1) the works on graph matching, whose purpose is finding node correspondence either softly or in a hard manner. The authors may not be aware a recent ECCV22 paper [a] on graph augmentation for self-supervised graph matching, which I think is worth mentioning as it also augments multiple copies of graph with node correspondence.\n\n[a] Self-supervised Learning of Visual Graph Matching, ECCV 2022\n\n2) As graph edit distance (which is a more flexible technique to expand the mixup space) is also used in the paper, such related works are also needed to discuss in related work part. There are some recent papers on learning-based graph edit distance computing.\n\n3) For mix-up based graph augmentation, there is another missed reference:\n[b] m-mix: Generating Hard Negatives via Multi-sample Mixing for Contrastive Learning, SIGKDD 2022",
            "clarity,_quality,_novelty_and_reproducibility": "Good.",
            "summary_of_the_review": "See above comments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_pxKa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_pxKa"
        ]
    },
    {
        "id": "A_Mrp-2PPz",
        "original": null,
        "number": 4,
        "cdate": 1667478653974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667478653974,
        "tmdate": 1667478653974,
        "tddate": null,
        "forum": "UntYZBCdypc",
        "replyto": "UntYZBCdypc",
        "invitation": "ICLR.cc/2023/Conference/Paper5392/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new way of graph mixup by enforcing the mixed-up graphs to be softly aligned, via a node-level graph matching network. The graph matching network is based on graph-level similarity learning supervision, whereby its node-level features are utilized to compute the node-wise similarity, followed by a softmax normalization. The authors conduct extensive experiments with different datasets and different mix-up methods, and the improvements brought by the graph alignment step seem convincing",
            "strength_and_weaknesses": "**Strengths**\n\n1. Enforcing node-level alignment in graph mixup tasks is an interesting and convincing effort.\n2. The empirical improvements brought by the proposed alignment method seem significant and consistent w.r.t. other mixup strategies. \n\n**Weaknesses**\n\n1. My major concern about this paper is that one of the main technical contributions of this paper, which is the node-level graph matching network, does not seem novel. The proposed graph matching network is simply adapted from Li et al. 2019, adding a softmax-based output after node-feature similarity computation. However, graph matching has been a research topic lasting for decades, and there exists both traditional algorithms and learning-based networks for the matching task. The authors seem to have not fully survived the graph-matching literature, and I believe there exist many papers that offer well-developed solutions for the alignment task in graph mixup. \n\n    For example, I suggest the authors try traditional graph matching methods, for example, RRWM (Cho et al. 2020), IPFP (Leordeanu et al. 2009), or FGM (Zhou et al. 2016), if the previous two methods cannot handle your problem because your graphs are too large. There are also recent deep graph matching neural networks (support node-matching) that are better motivated than the proposed approach, for example, nearly all deep graph matching networks use the Sinkhorn method instead of the proposed softmax method for normalization. The following two deep graph matching papers are suggested for the authors: (Wang et al. 2022) and (Fey et al. 2020).\n\n2. Some graph mixup methods are introduced and discussed in the related work, but they seem to be not compared in experiments, such as SubMix, ifMixUp.  Can the authors explain why they are not compared?\n\nReferences:\n1. Li et al. Graph Matching Networks for Learning the Similarity of Graph Structured Objects. ICML 2019.\n1. Cho et al. Reweighted random walks for graph matching. ECCV 2010.\n1. Leordeanu et al. An integer projected fixed point method for graph matching and map inference. NIPS 2009. \n1. Zhou et al. Factorized Graph Matching. PAMI 2016.\n1. Wang et al. Neural Graph Matching Network: Learning Lawler's Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching. PAMI 2022.\n1. Fey et al. Deep Graph Matching Consensus. ICLR 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. ",
            "summary_of_the_review": "This paper presents a graph node alignment-based graph mixup method, and the empirical results seem to be better than non-aligned graph mixup methods. The graph node matching network, which is one of the major technical contributions of this paper, does not seem novel considering the existence of both learning-free traditional graph matching solvers and learning-based graph matching networks that supports node matching. I suggest the authors to further survey the graph matching literature and improve the technical contributions of this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_t5LQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_t5LQ"
        ]
    },
    {
        "id": "_v58Fn-QOK",
        "original": null,
        "number": 5,
        "cdate": 1667529335793,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667529335793,
        "tmdate": 1667529335793,
        "tddate": null,
        "forum": "UntYZBCdypc",
        "replyto": "UntYZBCdypc",
        "invitation": "ICLR.cc/2023/Conference/Paper5392/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces S-Mixup. It is a graph Mixup method, that can better utilize the node-level information between the pair-wise graph.\n",
            "strength_and_weaknesses": "## Strengths\n1. The problem is well-motivated. Indeed, graph data is highly structured, and the standard augmentation methods can easily destroy the key substructures, which can lead to unexpected biases.\n2. The method is quite simple and straightforward. It first applies a graph pair-wise attention to learn a soft alignment score matrix. Then it utilizes this matrix to map graph 2 to the space of graph 1 for alignment.\n\n\n## Weaknesses\n1. Some key notations can be polished up. For example, in eq(5), authors are using A_2 to mix A_1, so the notations can be changed to $X_1\u2019, A_1\u2019, y_1\u2019$.\n2. Another question I want to confirm with the authors is, how this S-Mixup can be better than previous methods? I am aware that intuitively, it makes sense, but empirically, I think more qualitative results are required. Also in table 1, why S-Mixup can preserve motif? Can authors also help explain this further?\n3. The logic of the placement of Sec 5 is a little confusing to me. Why do the authors want to study the node-level correspondence and graph transformation problems? They seem to jump out without any introduction. Besides, though the conclusion of graph transformation seems correct to me, but I don\u2019t quite follow on why only the graph transformation is studied here? (the first sentence in Sec 5.2) A more common case is when the graph pairs have different number of nodes right?\n4. Sec 5.1 and 6.4 can be merged.\n",
            "clarity,_quality,_novelty_and_reproducibility": "There are some sentences not clearly explained to me:\n1. In the abstract, can authors explain what is \u201c... any pair of graphs can be mixed directly to generate an augmented graph\u201d?\n2. In the introduction, what is \u201c...  and perform mixup as in the case of images\u201d?\n3. The last sentence in te introduction can be polished up.\n",
            "summary_of_the_review": "I\u2019m not an expert on graph with muxip. So I may leave the comments on this point (baselines and experiments) to the other reviewers. The other comments are put above.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_CYp4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5392/Reviewer_CYp4"
        ]
    }
]