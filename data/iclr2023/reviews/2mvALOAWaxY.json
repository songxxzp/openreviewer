[
    {
        "id": "sfubfOdMsH",
        "original": null,
        "number": 1,
        "cdate": 1666537902312,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537902312,
        "tmdate": 1666537902312,
        "tddate": null,
        "forum": "2mvALOAWaxY",
        "replyto": "2mvALOAWaxY",
        "invitation": "ICLR.cc/2023/Conference/Paper4013/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper \"Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes\" derives a theoretical result and proves that log_2(n) hidden layers are necessary to compute the maximum of n input numbers. This is achieved by building on previous theoretical work that links neural networks to tropical geometry, and uses this connection to link neural networks with integer weights to lattice polytopes. Employing subdivisions of these polytopes, Minkowski sums and convex hulls, allows to construct all polytopes corresponding to a neural network of a given depth. Further, by looking at the normalized volume, it shown that not all lattice polytopes can be constructed that way, which implies a lower-bound on the depth of neural network (log_2(n)) to calculate the maximum of n numbers - matching the previously known upper bound.",
            "strength_and_weaknesses": "Strengths.\n- the paper gives a thorough and long introduction into the preliminaries (until mid of p.7); this is great to give a more self-contained presentation of the topic.\n- the paper proposes a novel theoretical insight into neural networks\n- limitations of the study are clearly stated\n\nWeaknesses - only minor things:\n- as the authors state clearly in the paper the analysis is limited to the case of integer-valued neural networks. It is unclear how to extend the analysis to the general case, but the methods provided might be instructive towards this target.\n- p.4. in conv(X): it should probably read 'lambda_i \\geq 0 \\forall i'?\n- the structure of the paper doesn't conform to typical ICLR papers, in particular, a Conclusion section is missing.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is clearly written, though not completely self-contained (not possible given the topic though)\n- The paper is of very high quality and novel to my knowledge",
            "summary_of_the_review": "The paper provides a novel theoretical insight into neural networks, and a proof construction that is well laid out. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_U42b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_U42b"
        ]
    },
    {
        "id": "lzhzG3oGUXO",
        "original": null,
        "number": 2,
        "cdate": 1666637305599,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666637305599,
        "tmdate": 1666637305599,
        "tddate": null,
        "forum": "2mvALOAWaxY",
        "replyto": "2mvALOAWaxY",
        "invitation": "ICLR.cc/2023/Conference/Paper4013/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to provide a non-constant lower bound on the depth of ReLU neural networks without any restriction on the width. Mostly, the authors prove that the conjecture by Hertrich et al. (2021) is true for all n \u2208 N with an additional assumption on the weighs of the network to be integer numbers.",
            "strength_and_weaknesses": "This general topic is appropriate for the machine learning community and the manuscript\u2019s proofs are well-written. I found the general concept of finding a logarithmic lower bound for the ReLU networks interesting and of value, since finding a lower bound for this seems to be the first and the ReLU networks are the most frequently used activation function in the world right now. While I did find this work interesting, there are several concerns to be addressed before further consideration.\n\n1-\tRegarding Theorem 3\u2019s proof, I could not find any precise justification for the used reasoning. It will be better to propose a more systematic procedure for the proof of this theorem and give a more detailed explanation of how \u201cThe arguments in Hertrich et al. (2021) can be adapted to show that the equivalence between the two conjectures is also valid in the integer case\u201d\n\n2-\t\u201cTranslating this result back to the world of CPWL functions computed by neural networks, we obtain that k hidden layers are not sufficent\u201d clause used in the manuscript needs more discussion about how they assumed it. Also, it has a typo. (Sufficient)\n\n3-\tThe \u201cFURTHER RELATED WORK\u201d section is just naming some papers without a brief description. It would be better if for some cases the author gives a brief description in manuscript.\n\n4-\tIt would be better if the author adds the proof of Theorem 8 to the appendix, since the paper should be self completed.\n\n5-\tThe theorem 12 proof is better to be more accurate and complete.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The manuscript\u2019s proofs are well-written and complete. The authors described most of the manuscript with a good quality and clarity. Furthermore, I found the general concept of finding a logarithmic lower bound for the ReLU networks interesting and of value.",
            "summary_of_the_review": "Although I found the general concept of finding a logarithmic lower bound for the ReLU networks interesting and of value, there are several concerns needs to be addressed before publication.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_iGLm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_iGLm"
        ]
    },
    {
        "id": "vHoDmVjFcu",
        "original": null,
        "number": 3,
        "cdate": 1666696997208,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696997208,
        "tmdate": 1669288304563,
        "tddate": null,
        "forum": "2mvALOAWaxY",
        "replyto": "2mvALOAWaxY",
        "invitation": "ICLR.cc/2023/Conference/Paper4013/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In (Arora et al. 2018), it was shown that $\\lceil \\log(n+1) \\rceil$-depth was sufficient for representing $n-dim$ CPWL functions. Is this necessary? Authors say yes, under the assumptions that weights are integral. That is, there exists functions that cannot be represented by depth less than $\\log(n+1)$, under the conditions that the weights are integral. To show this they exploit the fact that the weights are integral and the hard function is $\\text{max}[0,x_1,x_2,\\ldots,x_n]$, where $n=2^k$ and this is not expressible by a ReLU with integral weights and depth $k \\leq \\log(n)$.",
            "strength_and_weaknesses": "Strengths:\n\n It is an open problem to characterize the necessity of $\\lceil \\log(n+1) \\rceil$-depth for dimensions $n \\geq 4$ and also to get depth separations in high-dimensions, that are not constant depth, such as depth 3 vs depth 2 as shown in many works such as (Eldan and Shamir 2016, Daniely 2017, Safran et al.2019). This work tries to address this problem and gives a partial resolution and increased evidence for this. It does this by providing connections to the high-dimensional convex geometry of lattices. \n\nWeakness:\nThe main weakness is that of the integrality assumption. Now as the author points out that this is impractical in training and that perhaps new techniques are needed to deal with real-valued weights. The main issue is that the separation is argued by saying that a ReLU network of a depth smaller than this threshold does not exactly represent the hard function that is provided. However, results such as (Telgarsky, 2016) focus on L1-inapproximability by networks whose depths are $o(threshold-depth)$ and that have polynomial width. Could the authors clarify if allowed for real-valued weights, if they could still get separation on depths that are say $o(\\log(n+1))$ and show a stronger inapproximability result, rather than the exact representation of the function? Could the authors comment if ReLU(k), where $k=\\log(n)$ and allowed for real-valued weights can still provide an arbitrarily good approximation of the hard function?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear and the problem is important but some of the techniques may be tied to the specific assumption of integral ReLU networks.",
            "summary_of_the_review": "Overall, I think it is a good attempt at a big open question w.r.t the depth separation results, but I feel there are some questions as mentioned above, that requires some explanation and probably a bit more work.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_TovE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_TovE"
        ]
    },
    {
        "id": "09jhiQ_wLIL",
        "original": null,
        "number": 4,
        "cdate": 1667014974883,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667014974883,
        "tmdate": 1667014974883,
        "tddate": null,
        "forum": "2mvALOAWaxY",
        "replyto": "2mvALOAWaxY",
        "invitation": "ICLR.cc/2023/Conference/Paper4013/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the depth requirements for exactly representing the maximum of $n$ inputs using networks with integer weights. They show that $\\lceil \\log_2 n \\rceil$ depth is necessary. Note that the simple binary tree construction that computes maximum of two inputs at a time achieves this depth. For this class of integer weight NNs this shows a depth separation result for all depths less than $\\lceil \\log_2 n \\rceil$. The lower bound sheds light on the importance of using max pooling layers to remove depth dependence.",
            "strength_and_weaknesses": "**Strengths**: \n- The paper takes a good step towards understanding the depth separation of exact function computation for ReLU networks. Unlike approximation results where one non-linear layer suffices for approximating functions, the question of exact representation is not so straightforward. Though the paper does not solve the question in its generality, understanding even the integer weight constraint is useful, and hopefully helpful for the general setting.\n- Showing separation for max of linear functions is insightful since it suggests that adding max-pooling layers (which are popular in practice) might be a computationally useful operation.\n\n**Weaknesses**: \n- Despite being mathematically interesting, exact representability is not the most interesting from the point of learning in-distribution. Approximate approximation suffices for that.\n- There is no mention of width of the network. As has been seen in Telgarsky 2015, width and depth do have their interesting trade-offs.\n- The overall writing of the paper is not very friendly to readers without a tropical geometry background.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly clear, writing can be improved though (see below). The results are novel, and the proofs seem correct.\n\nSuggestions for improving writing:\n- All figures: Add descriptions in the caption to explain the figures, they are not easy for a reader without background to understand\n- Better notation for $\\tilde{L}$ parallel to $L$ (just above section 2.3)\n- Proof of Theorem 3 uses Proposition 17 before it is even defined. Please reorganize to ensure a linear flow. Also add more explanations of the proof, and high level ideas.\n\nQuestions:\n- How does the integer weight assumption compare to the condition used in Hertrich et al. 2021? Are they comparable?\n- Have you explored the width and depth tradeoff for these results?",
            "summary_of_the_review": "Overall, I found the problem studied in the paper and the results mathematically interesting. I think they are a nice addition to the rich literature of representation results known for neural networks despite the limitation of exact representability. Therefore, I vote to accept the paper. I encourage the authors to improve the writing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_dCJW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4013/Reviewer_dCJW"
        ]
    }
]