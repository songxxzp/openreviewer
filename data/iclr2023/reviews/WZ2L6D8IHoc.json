[
    {
        "id": "p80j_Zexe1",
        "original": null,
        "number": 1,
        "cdate": 1666513888003,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513888003,
        "tmdate": 1666513888003,
        "tddate": null,
        "forum": "WZ2L6D8IHoc",
        "replyto": "WZ2L6D8IHoc",
        "invitation": "ICLR.cc/2023/Conference/Paper801/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To learn symbolic policies that generalize better while maintaining\ninterpretability, the authors of this paper propose to use neural\nnetworks to generate parameters of a symbolic network which is then\n\"pruned\" through a path selector. Both the generator and the path\nselector are conditioned on context variables in the setting of\ncontextual meta RL. The primary finding is that the learned policies can\nbe interpretable and generalize better to different contexts, such as\ndiffering mass and forces in a control problem.\n\n",
            "strength_and_weaknesses": "# Strengths\n\n-   An interesting approach to learning interpretable policies. Using\n    both a generator and a path selector for learning a symbolic network\n    seems new, especially for contextual meta-RL\n\n# Weaknesses\n\n-   The main motivation is unclear. While the paper motivates the need\n    for symbolic networks by saying that overparameterized neural\n    networks can lead to overfitting and generalization, this is never\n    established and the current literature does not support that claim.\n    In addition, the way that the symbolic network is learned is, in the\n    end, with two neural networks. This seems to further undermine the\n    motivation.\n-   It is not that obvious how symbolic networks, as described in the\n    plain and densely-arranged architecture, differ from neural networks\n    aside from differing activation functions. If this is the only\n    difference, then this seems like a rather limited distinction from\n    regular neural networks. While in continuous control problems, your\n    choice of activation functions may lead to a more interpretable\n    policy - this may not be the case for some other problem for which\n    the needed non-linear activation function is not represented in your\n    library.\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Detailed Comments (Clarity, Quality, Novetly, Reproducibility)\n\n-   Section 1: \"Context-based Meta-RL methods are attractive because of\n    their empirically higher performance and higher efficiency compared\n    with previous methods which update the whole model.\" It is not clear\n    what \"previous methods\" are being compared to against here. Do you\n    mean RNN based approaches (Duan et al. 2016)?\n\n-   Section 1, \"This kind of NN-based policy usually involves thousands\n    of parameters, which may bring training difficulties, possibly\n    result in overfitting and hurt the generalization performance\"\n\n    Thousands of parameters is not an inordinate amount -\n    over-parameterization is the de-facto approach. It is also not\n    evident that this hurts generalization.\n\n-   Section 4.1: \"Different from traditional neural networks, the\n    activation functions of the symbolic network is replaced by symbolic\n    operators, e.g. trigonometric functions and exponential functions.\"\n\n    I do not see how these are different from activation functions,\n    because an activation function can be any non-linearity. Only that\n    sinusoids and exponentials are not commonly used activation\n    functions.\n\n-   Section 4.1 (Plain structure vs densely connected): It is not clear\n    how a plain structure differs from the densely connected structure,\n    given that the plain structure has an identity operator which allows\n    skip connections. Also, minor, but densely connected structure is a\n    misnomer, as a plain structure seems closer to a traditional dense\n    layer in a neural network and your \"densely connected\" structure\n    introduces skip connections.\n\n-   Section 4.1 (Symbolic Networks in general): I do not see much\n    difference between symbolic networks and regular feed-forward neural\n    networks. There are certain design choices made, such as limiting\n    the operations to some library, but this is not much different from\n    feed-forward neural networks. In fact, the issues with composing\n    arbitrary operations in your library seems more cumbersome for\n    little added benefit.\n\n-   Section 4.2: This is essentially a type of hypernetwork, which\n    outputs the parameters of the symbolic network, correct? But if \u03a6 is\n    a neural network, the the meta-policy is not interpretable and\n    suffers from all the issues described previously. While any\n    particular policy may be interpretable, that will not provide\n    interpratability (or as you claim, generalization) to new tasks -\n    which is precisely the type of generalization that matters in meta\n    RL.\n\n-   Section 4.2 (path selector vs parameter generator): I do not\n    understand the roles of these two components. While it is true that\n    you can model a policy using a subset of the symbolic network, it\n    does not seem obviously necessary to do so. In fact, this sacrifices\n    generalization because you will no longer be sharing parameters at\n    the level of the symbolic network. Instead, generalization comes\n    from the neural network which generate the parameters and select\n    paths - but again this is what you set out to avoid.\n\n-   Section 5.2: \"For each training step, we sample a new symbolic\n    policy\"\n\n    What is the definition of training step here, is it the same as the\n    meta-training epoch? It doesn't seem to make sense to resample the\n    policy per environment step, and the earlier sentence seems to\n    suggest that you \"use the sampled policy for the following steps of\n    the iteration\"\n\n-   Section 6.1: \"In Hopper-params, Walker2d-params and\n    InvDoublePend-params, CSP outperforms PEARL and Hyper during the\n    whole training process\"\n\n    While the mean performance is slightly higher, this is not enough to\n    claim that CSP outperforms PEARL and Hyper during the whole training\n    process. For much of the training process, the confidence intervals\n    are overlapping, which suggests that there is not a statistically\n    significant difference between them. The results earlier in training\n    on WAlker suggest that CSP learns quicker early on.\n\n-   Section 6.1 (Efficiency Comparison): I am somewhat surprised that\n    the differences in FLOPs and time can be so significant. Does this\n    account for the fact that the CSP is re-sampled\n\n-   Section 6.2 (ablation): From the figure, it does not seem that there\n    is a statistically significant difference between CSP with relu,\n    tanh or your proposed activation functions,\n\n    Is a CSP with relu or tanh activations functionally equivalent to a\n    traditional neural network? If so, it is possible that the neural\n    network is not large enough to represent complex nonlinearities that\n    CSP provides with its various activation functions.\n\n\n",
            "summary_of_the_review": "# Decision\n\nThe goal of this paper is to learn policies for meta-learning that are\nmore interpretable and more generalizable, This is a valuable goal to\nachieve in meta-RL and the research direction, using symbolic networks\nthat are generated by context, is interesting. While interpretability is\nsomewhat demonstrated, improvements in generalization are not addressed.\nIn addition, the symbolic network itself does not seem very different\nfrom a regular neural network that is arranged with special activation\nfunctions. The similarity to regular neural networks, as well as the use\nof regular neural networks to generate the symbolic policy, cast doubt\non the motivation and the broad effectiveness of symbolic networks.\nOverall, the research direction is not well-motivated and I find that\nthe paper further undermines the motivation. As a result, I am rating\nthe paper at a weak reject.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper801/Reviewer_EqMi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper801/Reviewer_EqMi"
        ]
    },
    {
        "id": "gKPTeMFPL3b",
        "original": null,
        "number": 2,
        "cdate": 1666659123428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659123428,
        "tmdate": 1666659123428,
        "tddate": null,
        "forum": "WZ2L6D8IHoc",
        "replyto": "WZ2L6D8IHoc",
        "invitation": "ICLR.cc/2023/Conference/Paper801/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes to parametrize meta-RL policies with a symbolic policy in the hopes of improving generalization, interpretability, and efficiency.",
            "strength_and_weaknesses": "This work has two main weaknesses. First, the clarity of the writing and presentation could be significantly improved, and is currently challenging to understand. I elaborate upon that in the next box. Second, this work does not meet the main claims that it sets out of increasing generalization and interpretability. More specifically, the experimental evaluation of these items is not sufficiently thorough:\n- It is true that in the reported experiments, the baselines' performance degrades with further training on the \"OOD\" tasks, while the proposed method achieves high performance throughout training. However, early in training the baselines do still achieve near-optimal results on the OOD test set, and its further performance drop can easily be remedied with early-stopping, which makes this sort of generalization gap less significant. More significant to the community would be considering domains where existing methods cannot already easily generalize out of distribution.\n- The claims about interpretability of the system are unclear. It is somewhat interesting that the symbolic policy produces an equation that relates to physics on the cartpole domain, as reported in the experiments. However, can this only happen when there are no dense layers interleaved with the policy? If the \"symbolic\" operations occur in between dense layers, how can a tight analytic equation be extracted? Additionally, it is unclear to me that compositions of operations such as sin, cos, division, etc., are any more interpretable than large matrix multiplications, especially when such compositions are long. Finally, in image-based domains, such \"symbolic\" operations seem unlikely to be helpful anyway.\n- The experimental evaluation setup could generally be improved to be more informative. Specifically, existing works already evaluate on common MuJoCo benchmarks, and it seems useful to evaluate on those rather than creating new ones. It's unclear to me if the Hopper and Walker evaluations indeed are the ones that have been used in other works, or if they are variants created newly in this paper. If they are the common ones, my concern here is alleviated. Additionally, though, it's challenging to evaluate CSP's performance without comparison to other existing methods. For example, \"Recurrent Model-Free RL Can Be a Strong Baseline for Many POMDPs\" reports strong performance of RL^2, and VariBAD also reports stronger-than-PEARL results. Inclusion of these methods would help understand whether CSP indeed offers superior performance or not.",
            "clarity,_quality,_novelty_and_reproducibility": "I generally find that the clarity could be significantly improved. In its existing form, many of the high-level ideas and motivation are completely absent or are hidden amongst the details, which makes it difficult to understand this work. For example:\n- Why should we expect that \"symbolic\" operators based on sin, cos., etc. provide greater utility or interpretability than standard NN building blocks, especially when there are already fully connected layers interleaved in between?\n- The full algorithm is not precisely defined in the main text, though I note that there is pseudocode in the Appendix, which helps\n- Certain details are jumped into without appropriate motivation or justification / explanation, such as the discussion of minimizing the $L_0$ norm. This requires a lot of work on the readers' part to infer the high-level ideas.\n- Terms, such as $\\mathcal{L}_{g_{i, j, k}}$ are never defined\n\nThe proposed ideas do not appear to be particularly novel: ultimately, the proposed approach is a new parametrization of meta-RL policies based on defining paths through \"symbolic\" operator graphs. Though, I note that I do not find novelty to be one of the most major concerns for this work.",
            "summary_of_the_review": "Overall, this work does not meet or test the claims that it sets out and could significantly be improved in quality and presentation. Therefore, I do not think it currently is ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper801/Reviewer_KRNC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper801/Reviewer_KRNC"
        ]
    },
    {
        "id": "WZRoSudDCC",
        "original": null,
        "number": 3,
        "cdate": 1666860801128,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666860801128,
        "tmdate": 1666860801128,
        "tddate": null,
        "forum": "WZ2L6D8IHoc",
        "replyto": "WZ2L6D8IHoc",
        "invitation": "ICLR.cc/2023/Conference/Paper801/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes Contextual Symbolic Policy learning for Meta-Reinforcement Learning, in which a symbolic network represents some search space in the policy space, conditioned on the context represented by encoding previous transitions using an encoder network. In addition, a path generator neural network is trained to select the \"correct\" symbolic form of the policy, by learning to mask out irrelevant nodes and edges in the symbolic policy network, with the encoded context as input. A parameter generator neural network is also trained, with the encoded context as input, to generate parameters of the symbolic policy network. In general, the whole pipeline, starts with design of the symbolic network architecture (plain, densely connected or arranged densely connected structures), learns to select the appropriate form of the symbolic policy by masking out irrelevant nodes and edges, and also learns the weights on the relevant edges. The pipeline is trained end-to-end.\n\nThe proposed approach is evaluated on a select benchmarks Meta-RL tasks and was shown to be better and sometimes competitive to the baselines.",
            "strength_and_weaknesses": "Strengths\n\nThe learned symbolic policies are more interpretable than pure deep learning policies.\nThe learned symbolic polices have been demonstrated to be lightweight and easier to deploy.\n\nWeaknesses\n\nWhile the efficiency of inference is shown to be better, no training efficiency is demonstrated. The proposed approach contains two more neural networks to be trained.\nThe evaluation is only demonstrated on toy examples. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and of good quality. Experimental setup is well described and model hyper-parameters are provided, which aids in reproducibility. ",
            "summary_of_the_review": "While the paper proposes an approach that allows extraction of symbolic policy expressions and has efficient inference speed, no analysis of the training efficiency is provided, especially given it has additional neural networks that need to be trained. Furthermore, it's not clear if policies of more complex environments will be any interpretable and if performance will scale to large benchmark problems. \n\nThe paper claims that by combining different kinds of operators to form the symbolic policy, CSP can handle more complex environments but this is not supported anywhere in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper801/Reviewer_SCBW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper801/Reviewer_SCBW"
        ]
    },
    {
        "id": "8NEuO0ejuK",
        "original": null,
        "number": 4,
        "cdate": 1667179011149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667179011149,
        "tmdate": 1667179011149,
        "tddate": null,
        "forum": "WZ2L6D8IHoc",
        "replyto": "WZ2L6D8IHoc",
        "invitation": "ICLR.cc/2023/Conference/Paper801/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the issues in the neural network-based policy in meta reinforcement learning (RL), such as overfitting \\& poor generalization ability, difficult/inefficient to deploy with limited computational resources, and poor interpretability. To address those issues, the framework of Contextual Symbolic Policy (CSP) is proposed by learning a contextual policy with a symbolic form based on the context variables for unseen tasks in meta-RL. Finally, experiments were conducted on several continuous control problems, with results demonstrating its effectiveness in terms of return, FLOPs, and interpretability.\n",
            "strength_and_weaknesses": "Strengths:\n- Symbolic representation is attractive since it has shown some promising in mitigating the issues regarding generalization ability, sample efficiency, and interpretability. This work targets to applying the symbolic representation to the meta RL setting in order to improve the generalization ability, reduce the computational cost, and increase the interpretability. Specifically, a gradient-based learning method is proposed to learn the contextual symbolic policy from scratch in an end-to-end differentiable way, which consists of a symbolic network, a path selector and a parameter generator.\n\n\nWeaknesses:\n- The literature review could include some previous work of combining symbolic representation with reinforcement learning in different ways, such as [1,2,3], although this work focuses on achieving the symbolic representation in a differentiable manner.\n- The abuse notation about the context of $c_{T}$: such as $c$ and $c_{\\kappa}$.\n- In Figure 1: it's not clear where do actor loss and critic loss come from. Is the \"context symbolic policy\" network only for the policy network? Does $Q(s,a,z)$ represent the value network? If this is the case, what's the architecture for the value network?\n- Are the symbolic operators listed in the paper applicable to all different tasks? If not, there should indicate what kinds of tasks can work with this symbolic operator library.\n- What's the experimental setup? Are those tasks trained alternatively or separately?\n- What are the differences for the paths in Table 2? Currently, I don't understand what are those.\n- The empirical evaluation seems weak and the current form of results is somewhat far from what was said to close the gaps in the introduction section, such as overfitting \\& poor generalization issue, inefficient deployment issues with limited computational resources, and explainability issue. For example, the problem size looks small. Even for the PEARL, the FLOPs are only in the level of KBytes and this might not be a big issue w.r.t. the deployment. For interpretability, it works well for the classical control problems since the original policy itself has a direct relationship between the state features and the action features. Although the proposed method can learn a symbolic policy, it looks to me that this is only an explicit expression for the learned policy which has partial interpretability rather than only numerical numbers from neural networks. The learned expression could vary if there are more different symbolic operators included.\n\n\nReferences:\n- [1] Garnelo, M., Arulkumaran, K., & Shanahan, M. (2016). Towards deep symbolic reinforcement learning. arXiv preprint arXiv:1609.05518.\n- [2] Garcez, A. D. A., Dutra, A. R. R., & Alonso, E. (2018). Towards symbolic reinforcement learning with common sense. arXiv preprint arXiv:1804.08597.\n- [3] Lyu, D., Yang, F., Liu, B., & Gustafson, S. (2019). SDRL: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 33, No. 01, pp. 2970-2977).",
            "clarity,_quality,_novelty_and_reproducibility": "- There are some questions that could be clarified as I mentioned in the main Weaknesses.\n- The proposed method is somewhat novel, although it is built on the previous work of (Rakelly et al., 2019) and (Larma et al., 2021).",
            "summary_of_the_review": "According to my comments in both the main Weaknesses and the section of Clarity, Quality, Novelty, I feel this is a paper where reasons to reject outweigh reasons to accept.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper801/Reviewer_8Fo8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper801/Reviewer_8Fo8"
        ]
    }
]