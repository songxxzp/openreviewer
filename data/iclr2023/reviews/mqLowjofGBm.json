[
    {
        "id": "Hx8rQlr9-d4",
        "original": null,
        "number": 1,
        "cdate": 1666234822090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666234822090,
        "tmdate": 1669265961679,
        "tddate": null,
        "forum": "mqLowjofGBm",
        "replyto": "mqLowjofGBm",
        "invitation": "ICLR.cc/2023/Conference/Paper3711/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed to solve the self-supervised imbalanced learning by enforcing category-level uniformity instead of sample-level uniformity.",
            "strength_and_weaknesses": "Strength:\n\n- The paper presents a strong motivation, which is to achieve class-level feature uniformity instead of instance-level feature uniformity. The overall method is closely built around this motivation. The toy example in Figure 1. is a good illustration of the paper's idea. The idea is new to the field.\n\n- The empirical results are sufficiently strong. Improvements are observed on multiple datasets (CIFAR-LT, ImageNet-LT, and Places-LT) on top of several SSL methods.\n\n\nWeakness:\n\n- I find the method part convoluted:\n\n  - I have difficulty understanding the importance of Simplex ETF. To my understanding, Simplex ETF is a fixed linear classifier that outperforms the standard learnable linear classifiers in end-to-end __supervised__ imbalanced learning. However, why is it necessary in the unsupervised setting? Should other alternatives to \"measure the geometric characteristics\", e.g., a fixed random linear classifier, be discussed and compared?\n\n  - The motivation for logit adjustment is not very clear. Why is logit adjustment required on top of Simplex ETF? \"All vectors in a Simplex ETF have the same pair-wise angle\", is Simplex EFT alone sufficient to ensure the class-level uniformity already?\n\n  -  Why is Eqn. (4) the objective? Is it to find some surrogate labels $q(t|x)$ to compute the cross-entropy loss with $p(t|x)$, so that the marginal $p(t)$ can be adjusted to $p(\\tilde{t})$? This part may need more clarity.\n\n  - The notions are not clear sometimes. Should the expression $p(\\tilde{t}|x) \\propto p(t|x)p(t)^{1+\\tau}$ be $p(\\tilde{t}|x) \\propto p(\\tilde{t}|x)p(\\tilde{t})^{1+\\tau}$? $q$ is also not defined in Eqn. (4) until later. \n\n  - __Overall__, I believe that the mixing of statistical interpretations (logit adjustment, Bayes) and geometric interpretations (Simplex ETF, geometric span) is what makes the method part hard to understand. Currently, I don't see how the two interpretations can be linked, i.e., the relation between \"skewed geometric distribution\" and the \"skewed statistical distribution\". If the authors are just trying to \"shrink the feature span of head classes for the category-level uniformity\", I would suggest the authors stick to the geometric interpretations and view \"logit adjustment\" as a margin loss.\n\n- I may have missed the part about how focal loss is used for self-supervised learning. It would be better if more related works besides Lin et al. 2017  can be provided.\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: can be improved in the method part.\n- Quality: good.\n- Originality: good.\n- Reproducibility: good, code is promised to be provided.",
            "summary_of_the_review": "The idea of the paper is new and comes with a clear motivation, and the experiment results seem sufficiently strong. However, I find the method part of the paper very difficult to understand which heavily affects my assessment of its correctness. I believe that the method part can be clearer with more polishing efforts, but based on the current version, I would recommend a weak rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_bvPJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_bvPJ"
        ]
    },
    {
        "id": "svWXd1qxe8L",
        "original": null,
        "number": 2,
        "cdate": 1666546623541,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666546623541,
        "tmdate": 1666546623541,
        "tddate": null,
        "forum": "mqLowjofGBm",
        "replyto": "mqLowjofGBm",
        "invitation": "ICLR.cc/2023/Conference/Paper3711/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Given the research progress on logit adjustment for supervised long-tailed learning, this paper presents a new method S^2LA to conduct self-supervised long-tailed learning  from the geometric perspective. In fact, without knowing the class distribution, S^2LA uses a constant simplex ETF to measure the geometric characteristics of the embedding space. Using a surrogate label allocation to represent the target distribution, this paper intends to compress the space of the head classes whilst avoiding the collapse of the tail classes. Treating the above transformation as a balancing and optimal transport problem, the proposed solution is intended to effectively handle the category-level uniformity problem. ",
            "strength_and_weaknesses": "Strengths - \n\n1. The presentation of the proposed method is generally understandable. The discussion on the theoretical analysis and the experimental is clear. \n2. The balance between the proposed approach and the evaluation section look reasonable. Many results have been provided to support the arguments in the theoretical analysis.\n3. The mathematical principles and basics have been clearly defined and explained.  \n\nWeaknesses - \n\n1. The major concern is that the proposed approach seems incremental, compared to the established works and similar technologies. \n2. The evaluations seem to support the arguments that the proposed technology outperforms the other state of the art methods, however, the justification is not comprehensive and requires much additional evidence to support the overall statements.\n3. The experimental results show that the proposed algorithm can only achieve slightly better performance (often < 1%) than the other state of the art technologies.\n\nActions to be taken:\n\n1. Since there is no ground-truth annotation of each sample, and using geometric labels tends to generate trivial unconstrained probabilities, a surrogate label allocation with logit adjustment is used in this paper. Eq. (4) is proposed and then needs to be solved using Eqs. (5)-(6). Finally, the overall objective function is expressed as Eq. (7). However, Eq. (7) has a from similar to that of Zhou et al. (2022). The only difference between them is that the current paper introduces Eq. (4) to its final objective function, which plays as a constraint term. It is not clear if or not this objective function is correctly derived in this paper. If it is, how much difference between the two papers indeed?\n2. In the ablation experiments, Fig. 4(c) is not of much value. This is because presenting different distributions without showing the classification results is not convincing.\n3. In Fig. 5, increasing skewness also leads to higher accuracy. However, the improvement of accuracy is fairly mild, normally less than 1%. If this is a successful mechanism, the increment should be significant, is not it? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The value and importance of the work seems clear in the current form. The presentation of the proposed method seems understandable. The explanations to the experimental results are sensible. However, the derivation of the objective function in the proposed algorithm needs further interrogation so that the reader understands the technical details. \n\nQuality: The idea and concept of the proposed algorithm look quite plain. It seems that the proposed approach is a straightforward extension of similar approaches. The proposed research seems acceptable in terms of the motivation but lacks significance in theory and experiments. \n\nOriginality: Logit adjustment is a reasonable choice and the current paper uses a number of mathematical means to deal with the objective functions. The proposed framework, based on several works with similar drives, has made incremental contribution on solving the SSL problems with the SK algorithm. The originality of the proposed method is very minor.\n\nReproducibility: There is no indication that the source code and other corresponding materials will be published. ",
            "summary_of_the_review": "In summary, this paper presents an interesting topic, worth of being investigated. The theoretical analysis sounds reasonable but lacks certain explanations to the proposed objective function and its derivation. The experimental results are quite considerate but some of the systematic parameters have not been properly justified in the examination.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "This proposed research uses publicly accessible datasets so there is no concern on ethics. ",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_jKEp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_jKEp"
        ]
    },
    {
        "id": "0uMdHyDscUm",
        "original": null,
        "number": 3,
        "cdate": 1666678751927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678751927,
        "tmdate": 1670479849648,
        "tddate": null,
        "forum": "mqLowjofGBm",
        "replyto": "mqLowjofGBm",
        "invitation": "ICLR.cc/2023/Conference/Paper3711/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses an important problem of self-supervised learning on data with imbalanced classes. SSL trained on sample-level uniformity will learn representation for common classes. However, less represented classes will suffer due to bias for \"head\" classes. The paper introduces a method to achieve category-level uniformity by using surrogate classes. A distinct property of the method is that it can be applied to any SSL method by modifying the loss function along with the training procedure. Authors demonstrate strong empirical results of the method.",
            "strength_and_weaknesses": "Strength:\n1. An important problem of SSL on imbalanced classes.\n2. An original idea of using surrogate classes to prevent bias due \"head\" classes.\n3. Substantial empirical results.\n4. Authors share code for reproducibility.\n\nWeaknesses:\n1. Mathematical exposition of the material is poor.\n2. Ideas are not communicated clearly. \n3. There is no discussion on the technique's limitations.",
            "clarity,_quality,_novelty_and_reproducibility": "Communication of the ideas can be improved. I don't think I can reproduce results from the paper itself, however authors release the code which somewhat mitigates the issue.",
            "summary_of_the_review": "After addressing the feedback, communication of ideas somewhat improved. I am giving a conditional acceptance, if authors can add a discussion on the limitations of the approach. Such a discussion will bring forth implicit assumption where the method expected to work.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_EKLC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_EKLC"
        ]
    },
    {
        "id": "GSZh01hG92",
        "original": null,
        "number": 4,
        "cdate": 1666766524921,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666766524921,
        "tmdate": 1666766724971,
        "tddate": null,
        "forum": "mqLowjofGBm",
        "replyto": "mqLowjofGBm",
        "invitation": "ICLR.cc/2023/Conference/Paper3711/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers the problem of self-supervised feature learning for downstream tasks where the unsupervised dataset follows a long-tail distribution.\nIn particular, it seeks to address the issue of head classes covering much more of the feature space than the tail classes (expansion/collapse).\nThe proposed method is to obtain initial surrogate class likelihoods for each example, transform these by applying reverse post-hoc logit adjustment based on the surrogate class frequencies, and then obtain final (soft) surrogate labels by solving an optimal transport problem with the Sinkhorn-Knopp algorithm.\nThe initial surrogate class likelihoods are obtained using a Simplex ETF (linear, constant, orthogonal) classifier and the final confidence of the assigned surrogate label is used to weight an additional loss term that encourages the Simplex ETF to be more confident.\nThe marginal likelihoods of the surrogate labels are recomputed every epoch, and the logit adjustment and optimal transport are computed in every minibatch.\nEmpirical studies show that this technique generally improves the accuracy of several baseline algorithms (SimCLR, Focal, SDCLR, BCL) across the Many, Med and Few class subsets for several datasets with a wide range of imbalance (CIFAR100-LT-10, -50, -100, ImageNet-LT, Places-LT).\nIt is applied after an initial warmup phase that uses the baseline algorithm alone.",
            "strength_and_weaknesses": "**Strengths**\n\n1. Dealing with long-tail data in unsupervised pre-training seems like an important and difficult problem which has not received much attention. Indeed, it's hard to even measure the tail since the dataset does not have labels.\n1. Including focal loss as a baseline was good as it may help with class imbalance.\n1. The proposed method is shown to significantly complement a wide range of algorithms for a wide range of datasets.\n1. The method does not incur a high computational cost.\n\n**Weaknesses**\n\n1. The motivation and description of the Simplex ETF classifier, the transformation of its predictions to obtain surrogate labels and the additional loss term is extremely unclear. Similarly, the motivation for applying logit adjustment in reverse is unclear. If I understood correctly, the idea is to make the predictions of head classes even more confident, such that they are moved further from their (fixed) decision boundary. It might be possible to achieve this more elegantly using the logit-adjusted softmax from that same paper (rather than post-hoc logit adjustment)? Also, could the single $q \\log p$ term in the loss be replaced with the more familiar cross-entropy over all labels $t$?\n1. It wasn't clear what the \"trivial unconstrained probabilities\" and \"trivial solution\" to eq. 4 were. One-hot distributions? This was used to motivate the use of Optimal Transport. Can't we just use the (normalized) logit-adjusted likelihoods $p(\\tilde{t})$ as targets? Does this cause the method to fail?\n1. I found it surprising that the proposed approach achieved significant gains for the \"Many\" subset as well as the \"Few\" subset. This was not explained. Perhaps the method is achieving its improvement by some mechanism other than accounting for imbalance? Perhaps it's accounting for imbalance in some unlabelled attribute? This might be revealed by inspecting the clusters. Is it possible to run some experiments in the balanced setting and see if the method still yields gains?\n1. No empirical comparison to the method of Asano et al. (SeLa), which does consider the imbalanced setting.\n1. The values for $\\tau$ seemed much lower than in the logit adjustment paper (0.02-0.07 vs. 1-4). How can this be explained?\n1. While the method is claimed to be end-to-end, it involves non-differentiable computations in each epoch and in each mini-batch.\n1. Does 300 iterations of Sinkhorn-Knopp really not have a significant impact on the training time? How long does this take compared to the typical time for a minibatch?\n1. The number of training epochs seemed very high (e.g. for CIFAR-100-LT, 1000 epochs plus 500 warmup epochs). Is this typical for self-supervised training?\n1. Figure 4(a), which shows a comparison to temperature scaling, would benefit from error-bars, since the values seem noisy and S2LA is not that far from the baseline.\n1. Related work section does not include discussion of Supervised Long-tail Learning. For example, the paper \"Decoupling Representation and Classifier for Long-Tailed Recognition\" seems relevant.\n1. Why was 100-shot evaluation used for ImageNet-LT and Places-LT? Wouldn't this impair the accuracy of the \"Many\" classes compared to a linear classifier trained with all examples?\n1. While Algorithm 1 in the appendix makes the method much more clear, it shouldn't be necessary to refer to the appendix to understand the method. For example, it wasn't clear in the main text that the surrogate class prior was re-estimated once per epoch.\n\n**Nitpicks**\n\n1. Self-supervised learning includes some methods that have labels (i.e. automatically generated labels). The paper often uses \"self-supervised learning\" to refer to *contrastive* methods exclusively. It would be good to be more clear.\n1. The acronyms BCL and SDCLR were not defined.\n1. Needs to be proofread for grammar.\n1. I was confused by the use of bold characters for the single labels $\\boldsymbol{t}$ and $\\tilde{\\boldsymbol{t}}$. This is typically used to denote a vector.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and quality.** The description and motivation of the method is unclear, see concerns above. The empirical evaluation seems to be of a high standard. However, it would be good to perform multiple trials and include confidence intervals for the most important results.\n\n**Novelty.** The method seems novel. Note that the use of Sinkhorn-Knopp for self-supervised learning via self-labelling was introduced in Asano et al. (2020). While this was not claimed as a contribution, I feel that this should be acknowledged in the paper.\n\n**Reproducibility.** I believe there is sufficient information to reproduce the experiments.",
            "summary_of_the_review": "While the results are strong, I find that the description and motivation of the method to be very unclear. I am leaning negative but I'll consider upgrading my review if these concerns can be mitigated in the process of the rebuttal and discussions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_1EQS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3711/Reviewer_1EQS"
        ]
    }
]