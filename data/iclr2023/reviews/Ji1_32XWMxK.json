[
    {
        "id": "s3klvKRvJn",
        "original": null,
        "number": 1,
        "cdate": 1666627095730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666627095730,
        "tmdate": 1666627095730,
        "tddate": null,
        "forum": "Ji1_32XWMxK",
        "replyto": "Ji1_32XWMxK",
        "invitation": "ICLR.cc/2023/Conference/Paper6083/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to combine human input in the form of a set of predicates and objects that they associate with a given problem, and realising the fact that for every deterministic MDPs there is a symbolic representation that they dub optimistic representation that reaches the goal state. They provide  the theoretical underpinnings necessary to refine and update a model that will solve the task when combined with a complete planner.\n\n",
            "strength_and_weaknesses": "Strengths: This paper is clearly written and provides an interesting approach to Exploration by lifting the action representation to a symbolic level and combining exploration with planning. \nThe approach seems very novel to the best of my knowledge.\nWeaknesses: the experimental section is lacking in comparison with other HRL approaches and how is Q learning performing in the tasks, how far does it go or where does it fail? It would also help understand the significance of the contribution if the results were given in terms of samples as well.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper introduces notation on an inverse mapping function that did not seem to be used later on, where was this lifting of the symbolic model being used? And how exactly is it being learned?\nSuggestions:\nThe mapping between states and labels C(s) is applied to actions as well although it is defined over states can the authors clarify? What does it mean C(a) is a predicate true or false in the MDP ? when performing an action? But this is also dependent on the state where it is being executed /equivalent initial and seq of actions in a deterministic env.\nThe delete_{i+1} =del^a_{i} U (C(s\u2019)\\C(s)) reads equals to the del set union with the set of true predicates in (s\u2019) set difference set of true predicates in s? It does not seem to match the text description that the set of predicates that are false in s\u2019. It would help if the authors could clarify on the meaning of the operator \\.\nHow is the model being pruned exactly? And how sensitive is the approach to the planner?\nHow sensitive is the optimistic representation to the size of the sets? Is the min_add max_del faster?\nTypos:\n-bee -> been pg 9\n",
            "summary_of_the_review": "This paper proposes a novel way of doing exploration using symbolic reasoning and combines it with planning. The main contributions seem to be in the model update by leveraging human input to formulate preconditions, deletion and addition of predicates for every state, and actions and using update rules to guarantee the existence of a path from start to goal conditions as long as a complete planner is in place. The experimental section only compares the proposed model with and without symbolic model lifting with Q learning in a time budget constraints. The paper would benefit if results would also be presented in terms of samples. Also no HRL baseline is provided, which would seem a more fair comparison.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_m6BR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_m6BR"
        ]
    },
    {
        "id": "JM1Yzivnrn3",
        "original": null,
        "number": 2,
        "cdate": 1666652183180,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652183180,
        "tmdate": 1666652183180,
        "tddate": null,
        "forum": "Ji1_32XWMxK",
        "replyto": "Ji1_32XWMxK",
        "invitation": "ICLR.cc/2023/Conference/Paper6083/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper focuses on the issue of requiring to access a complete and correct symbolic model when combining symbolic methods with reinforcement learning (RL). To address the issue, this work proposes a new method for learning optimistic symbolic approximations of the underlying model in an online fashion. Besides, theoretical results are provided to further analyze the proposed algorithm. Finally, experiments were conducted on three traditional planning domains (Blocks, Elevator, Gripper) and one traditional RL benchmark (Minigrid), with results demonstrating its effectiveness in terms of solvability, the average time, and the number of samples.",
            "strength_and_weaknesses": "Strengths:\n- Symbolic representation has been shown promising in guiding RL for the efficient exploration, explainability, and human-machine interaction. This is an interesting work targets to learning a symbolic representation for effective exploration within RL, particularly under the sparse reward setting. It would be intriguing to both the communities of symbolic planning and RL.\n\n\nWeaknesses:\n- The presentation of the paper could be improved. For example, how the symbolic model in this work is learned in an online fashion? As mentioned in the related work, I notice some of them allowing to learn the symbolic representation in an online setting, what's the difference between the proposed method and the existing work? How human guidance/ advice interact with this symbolic representation in practice? There lacks more technical details. For the pseudocode of algorithm 1, it would be more clear if the procedures, such as PruneModel, UpdateModel, can be referred to a more specific steps/ equations/ definitions. Besides, I am wondering how does this learned representation interact with RL since I didn't find too many details about this in the paper. Also, how does this learned representation help RL w.r.t. the effective exploration? Does it aim to reduce the exploration space by using prior knowledge or guide RL to explore more meaningful states?\n- Current empirical results could not well support the claims on addressing the problem of effective exploration and speeding up the learning process with human minimal input. Comparing with Q-learning with the proposed method (w/wo lifting) in Table 1 looks unfair to me. For the planning domains, there should be included some planning baseline in addition to the proposed method. Particularly, I am quite curious why hierarchical RL fails in Minigrid domain? Even if hierarchical RL doesn't work, it at least has a better performance than flat Q-learning. Can you elaborate more on this?\n\n\nMinors:\n- \u2019goal\u2018 on pp.3 should be 'goal'.\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper could be improved by re-organizing the papers and showing more technical details as I mentioned in the main Weaknesses. Besides, there are some questions could be clarified.\n- Novelty: this work provides some insights in the symbolic method side. However, it's unclear how it will help the RL side.\n",
            "summary_of_the_review": "According to my comments in both the main Weaknesses and the section of Clarity, Quality, Novelty, I feel this is a paper where reasons to reject outweigh reasons to accept.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_dzDP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_dzDP"
        ]
    },
    {
        "id": "LBdAnlomtB",
        "original": null,
        "number": 3,
        "cdate": 1666667492094,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667492094,
        "tmdate": 1666667492094,
        "tddate": null,
        "forum": "Ji1_32XWMxK",
        "replyto": "Ji1_32XWMxK",
        "invitation": "ICLR.cc/2023/Conference/Paper6083/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn a new exploration technique to learn a symbolic model of an environment. This model is then used to speed up goal directed exploration. The proposed technique is then evaluated against Q-learning with epsilon-greedy and R-max as well as a hierarchical RL method. Experiments are done on blockworld and minigrid and show that using the optimistic symbolic model improves results over the baselines",
            "strength_and_weaknesses": "While the paper proposes an interesting technique to solve the specifics problems presented in the paper it is unclear how applicable the proposed technique are to other problems.\nFor example the paper chooses to ignore the challenge of stochasticity in exploration in reinforcement which is one of the main cause of the difficulty of exploration in reinforcement learning\n\"We specifically chose a setting, which forefronts\nthe problems related to exploration, while placing less emphasis on other aspects of an RL problem\".\n\nThis paper also relies on human input and a goal directed planner, those two assumptions do not seem realistic to solve more general problems.\n\nI also do not understand why the paper decides to introduce and focus on blockworld. The paper does not properly justify why this is an interesting problem and we should care about solving as opposed to the larger number of existing reinforcement learning environments.\n\nIt appears that the paper is using a tabular version of Q-learning ? In that case can the proposed method scale to problems that require function approximation? In that setting it would be interesting to use more recent algorithms as baselines such as RND and/or NoveID.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall I found that the paper was really difficult to follow.\nSection 3.1 spends a lot of time to introduce the blacksworld environment even though the information presented is not particularly interesting and could be pushed to the appendix.\n\n\"In fact, possibly the only effective methods in the mainstream RL repertoire we can use are curiosity driven or intrinsic reward based methods\" \nPosterior sampling methods such as Thompson sampling could be used.\n\n\"For the symbolic model, we will be using an untyped normalized PDDL task model Helmert (2009).\"\nNormalized PDDL were not introduced.\nI found section 3.3 really hard to understand, a plot / figure of the symbolic model would have been helpful to help the reader.\n\n\"The basis of our approach is an observation that every deterministic MDP has a precise symbolic\nrepresentation.\" what does precise means?",
            "summary_of_the_review": "Overall while the proposed idea is interesting I do not think the paper present a compelling evidence that their method can be useful to improve exploration in most environments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_mjER"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_mjER"
        ]
    },
    {
        "id": "jW7EhMZaeG",
        "original": null,
        "number": 4,
        "cdate": 1667533485683,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667533485683,
        "tmdate": 1667533485683,
        "tddate": null,
        "forum": "Ji1_32XWMxK",
        "replyto": "Ji1_32XWMxK",
        "invitation": "ICLR.cc/2023/Conference/Paper6083/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new algorithm for integrating off-the-shelf automatic symbolic planners with online RL for efficient exploration in the sparse reward setting. The authors propose to itteratively refine a symbolic representation that a planner uses to predict paths towards a goal, which gets rolled out in the environment. It is theoretically proven that the proposed algorithm terminates, provided some conditions on the planner are satisfied. The authors show results comparing their method to Q-learning and show an improvement in the outlined domains. ",
            "strength_and_weaknesses": "Strengths:\n\nThe paper presents the problem setting and background in a very rigorous manner, and the theoretical guarantees are relevant to the proposed task. The paper includes code for reproduction.\n\nWeaknesses:\n\nThe paper is lacking in thorough experimentation and results discussion. Additionally, a major limitation comes from the constraint to deterministic MDPs (which the authors acknowledge). Additionally, the paper assumes human input for annotation of the actions, which begs the question why the baseline is a Q-learning model, as opposed to something that similarly uses human guidance, i.e. imitation learning. Additionally, the use of online RL, as opposed to the more prevalent offline RL setting within the symbolic representations, is not well justified. It would be good to compare against other existing symbolic planning methods as well. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and clearly written. The presented approach is novel although perhaps of limited practicality. The appendix contains sufficient implementation details for reproducibility.",
            "summary_of_the_review": "The paper is clear and well written with good theoretical justification, however it is questionable if the proposed approach could scale to more complex and realistic domains (such as the kitchen domain from Danfei Xu, Roberto Mart\u00edn-Mart\u00edn, De-An Huang, Yuke Zhu, Silvio Savarese, Li Fei-Fei, Regression Planning Networks). I would be willing to increase my score if more detailed experiments are provided.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_DcwG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6083/Reviewer_DcwG"
        ]
    }
]