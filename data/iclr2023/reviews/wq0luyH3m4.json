[
    {
        "id": "w2_RRFB20k3",
        "original": null,
        "number": 1,
        "cdate": 1666647224404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666647224404,
        "tmdate": 1666649687477,
        "tddate": null,
        "forum": "wq0luyH3m4",
        "replyto": "wq0luyH3m4",
        "invitation": "ICLR.cc/2023/Conference/Paper2881/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a new algorithm to build efficiently a few-shot classification benchmark, named HARD-META-DATASET++ tailored to difficult tasks. In particular, the proposed algorithm is able to sample difficult tasks from various large-scale vision datasets in a deterministic way using a constrained optimization problem. The principle is to learn, through this constrained optimization problem, selection weights for each instance in a search pool in order to build a support set associated with a given query set. Compared to previous related approaches that use greedy search, the proposed algorithm is faster and thus enables to sample tasks in large-scale vision datasets. Using this algorithm, they build HARD-META-DATASET++ by sampling difficult tasks on META-DATASET, ORBIT, CURE-OR and OBJECT-NET. They then proposed to test various few-shot classification state-of-the-art approaches on HARD-META-DATASET++ and they show that, on average, these approaches fail on these hard tasks. ",
            "strength_and_weaknesses": "**Strengths**\n\n+ This paper targets an important issue in the few-shot classification community: the lack of benchmarks that are enough representative of real-world few-shot tasks. This problem is currently understudied. In particular, the proposed approach focuses on hard tasks defined as tasks for which a given few-shot classifier performs worse than for all other tasks (on average).\n+ The paper is easy to follow and well-structured.\n+ The resulting HARD-META-DATASET++ if it is publically released.\n\n**Weaknesses**\n+ My main concern is related to the proposed definition of a difficult task. The proposed definition is relative to other tasks but does not take into account the content of the task itself, i.e. the labels and the image. Indeed, in real-world scenarios, hard tasks are for instance fine-grained classification ones or tasks with shifts between the support sets and the query set.  \n+ Moreover, the proposed approach to sample hard tasks depends on the chosen loss, here proto-loss but also on a meta-trained based model. This point should be more discussed and in particular what is the impact of the choice of these two ingredients on the findings of the paper.\n+ The paper title mentioned understanding few-shot performance but the \"understanding part\" is not enough detailed and developed in the paper.\n+ Some important references are missing such that the paper of [(Benequin el al, 2022)](https://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Bennequin_Few-Shot_Image_Classification_Benchmarks_Are_Too_Far_From_Reality_Build_CVPRW_2022_paper.html) (Few-Shot Image Classification Benchmarks are Too Far From Reality: Build Back Better with Semantic Task Sampling, that also tackles the weaknesses of current few shot benchmarks for real-world applications. Moreover, various approaches have been proposed to define a task such as for instance [Task2vec](https://arxiv.org/abs/1902.03545) (Achille et al, 2019) and the paper should mention them and position compared to them.\n+  Transductive inference is widely used in few-shot learning. What about transductive few-shot learning in the proposed approach?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and well-structured. The proposed algorithm is well-detailed and pseudo-code is provided. The paper also gives enough information and details on the experimental part: design choices are motivated and all the hyper-parameters are given.\n\nThe novelty of the paper is low : it extends META-DATASET with hard tasks but the issues of the hard task in few-shot have been ever studied (Aggarwal et al, 2021). The paper just proposes a more efficient algorithm than this previous one to sample hard tasks.",
            "summary_of_the_review": "The paper proposes a constrained optimization algorithm for few-shot classification hard task sampling.  As such, I believe that the contributions of this paper are not significant enough to be published at the ICLR. In particular, as such the paper just brings a new benchmark on which state-of-the art approaches are weaker but it does not address at all the problem of better understanding and definition of why a task is difficult in few-shot, in particular in real-world settings. More insights are needed on this particular point and in relation with the images and labels of the targeted task.\n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_tAM3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_tAM3"
        ]
    },
    {
        "id": "svUyE9evadm",
        "original": null,
        "number": 2,
        "cdate": 1666657479500,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657479500,
        "tmdate": 1666657479500,
        "tddate": null,
        "forum": "wq0luyH3m4",
        "replyto": "wq0luyH3m4",
        "invitation": "ICLR.cc/2023/Conference/Paper2881/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a novel efficient algorithm to extract hard tasks from existing large-scale datasets.\n\nFinding the difficult episodes, (support set, query set) pairs, is a combinatorial problem, and the authors propose to use a model\u2019s loss on the task\u2019s query set, as a proxy for the difficulty of the task, and cast the problem as an optimization problem.\n\nUsing the proposed method, the authors extracted a dataset from (MetaDataset, ORBIT, CURE-OR and OBJECTNET). \n\nThe resulting dataset is used as a test-only few-shot classification benchmark, and is named HARD-META-DATASET++. The authors evaluate a few popular few-shot classification methods on this benchmark.",
            "strength_and_weaknesses": "Strength\n- casted the difficult support set extraction as a constrained optimization problem, to optimize the \"selection weights\" (eq 1.)\n- solved the optimization problem using:\n   1) ProtoNet as feature extractor\n   2) applies 1-step gradient descent w.r.t the \"selection weights\"\n   3) projection w and uses l1 relaxation, and solved the relaxed version using its dual form\n- The difficult episodes are obtained by drawing the examples with the highest \"selection weights\"\n- The algorithm can speedups of difficult episode search by 20x\n- The constructed hard datasets is difficult for FSL models (fig 3)\n\n\nWeakness\n- Does number of steps have any effect on the final set? E.g., the current optimization of the \"selection weights\" uses 1 projection step.\n- The optimization involves using f, and its set to a ProtoNet. Does the selection of f affect the accuracy of the difficult set selection?\n- There are other methods to extract hard episodes, have the author compared with existing approaches?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to read and follow. \n\nCasting the hard episode selection as an optimization problem is interesting. \n\nThe algorithm should be easy to reproduce.",
            "summary_of_the_review": "The paper presents an interesting way to construct the difficult subset from large scale datasets. The prospered methods shows promising effectiveness in construction such difficult set. The authors successfully constructed the HARD-META-DATASET++, and conducted the evaluation of various feature extractors (fig 4). However, the paper lacks comparison between the proposed approach and existing approaches. The proposed method also has some constraints: hand-picked f (ProtoNet), and the authors used 1 gradient step to solve the optimization wo/ justification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_yM4m"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_yM4m"
        ]
    },
    {
        "id": "GBkKVLP1FH",
        "original": null,
        "number": 3,
        "cdate": 1666687593930,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687593930,
        "tmdate": 1666687593930,
        "tddate": null,
        "forum": "wq0luyH3m4",
        "replyto": "wq0luyH3m4",
        "invitation": "ICLR.cc/2023/Conference/Paper2881/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper mainly focuses on few-shot learning. Different from the previous methods pursuing good overall accuracy, the authors study the performance on hard few-shot tasks. Specifically they propose a novel algorithm based on constrained optimization to find difficult support samples given query samples. Such a method is much efficient than greedy search. Based on the proposed method, the authors conduct extensive experiments on a new version of Meta-Dataset using existing methods and provide many interesting results.",
            "strength_and_weaknesses": "Strength:\n1.\tThe idea of analyze the performance on hard tasks is practical.\n2.\tThe proposed algorithm is simple yes effective.\n\n\nWeaknesses:\n1.\tWhile the authors provide abundant results, the corresponding analysis is not so insightful. It is common sense that there are many potential reasons that can lead to low accuracy on query samples. For example, the support samples can either be hard to fit or easy to overfit, which are totally different when considering the properties of these hard tasks. It is hard to understand why these previous methods have large performance gap between the original MD and Hard-MD++ solely based on the average accuracy. It would be better if other metrics can be provided, e.g. the training loss curves on each dataset, the relation between the performance and shot/way number, etc.\n\n2.\tThe paper would be more comprehensive if other adaptation methods like DCM [1], TSA [2] and eTT [3] can be used in the experiments.\n\n3.\tI wonder if it is possible to measure the average \u2018hardness\u2019 of the original experiment setting on MD, i.e. 600 episodes for each dataset. This is helpful for indicating the chance of handling such hard tasks in real-life scenarios where hard and easy tasks are mixed.\n\n[1] Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling. AAAI 2022\n[2] Cross-domain few-shot learning with task-specific adapters. CVPR 2022\n[3] Exploring Efficient Few-shot Adaptation for Vision Transformers. TMLR 2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, and of good quality. The originality of the work is good.",
            "summary_of_the_review": "This paper presents a new Few-shot learning setting, in order to better understand the difficulty of FSL. \nMy personal suggestion is that some insightful analysis may be appreciated, and importantly some recent adaptation methods may be compared in this setting. This can show that this setting is really meaningful.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_trsF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_trsF"
        ]
    },
    {
        "id": "VVpqG16A5E",
        "original": null,
        "number": 4,
        "cdate": 1666710864172,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710864172,
        "tmdate": 1666710864172,
        "tddate": null,
        "forum": "wq0luyH3m4",
        "replyto": "wq0luyH3m4",
        "invitation": "ICLR.cc/2023/Conference/Paper2881/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the failure cases in FSL and proposes an efficient algorithm to extract the difficult tasks from large-scale datasets. Based on the proposed algorithm, the paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++.",
            "strength_and_weaknesses": "Strength:\n1) The paper is writing clearly and easy to read.\n2) The paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++. \n\nWeaknesses:\n\n1) The support samples of \"difficult task\" is selected by fixing the query samples which means the difficulty of each task is highly correlated with the fixed query data. The hypothesis is somewhat unreasonable, since the FSL aims to adapt to the whole new class not the fixed query samples. The reviewer supposes that the tasks in the HARD-META-DATASET++ may not be a reliable estimator in the test-time in FSL. \n\n2) The proposed method can be seen as an extension of the paper[1], since the paper[1] have already proposed that FSL are extremely sensitive to the data used for adaptation and used a greedy algorithm to find those difficult tasks. Although the proposed method is more efficient, the novelty is somewhat limited.\n\n3)The evaluation process consists 200 tasks using Prototypical Network, which is not enough(suffering from high randomness). In recent literature, the number of evaluation tasks is usually more than 2000.\n\n\nAs for questions, I would like to ask:\n\n1 In FSL, we usually report the mean and the variance of the accuracy over 2000 tasks. The variance of the accuracy also denotes the model's performance of the challenging tasks(The higher variance, the lower accuracy on more difficult tasks). Besides, the average accuracy of several worst cases can also be a good estimator. Why is the accuracy on HARD-MD  a better criterion for evaluation?\n\n2 How to use the \"difficult task\" in the training phase in FSL? Can the \"difficult task\" in base classes help the model improve the generality to novel classes?",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe paper is well written. However, the novelty is somewhat limited, since it can be considered as an extension of the paper[1]. \n\nReproducibility is unclear since there are no codes or links provided.",
            "summary_of_the_review": "This paper builds a new test-only few-shot classification benchmark named HARD-META-DATASET++. However, I have some concerns that whether the HARD-META-DATASET++ can be a reliable estimation (Please refer to 1) in Weaknesses). \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_Q1sQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2881/Reviewer_Q1sQ"
        ]
    }
]