[
    {
        "id": "B-tEDS4W1e",
        "original": null,
        "number": 1,
        "cdate": 1666710090059,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710090059,
        "tmdate": 1670955829619,
        "tddate": null,
        "forum": "n7lFF_zE8nm",
        "replyto": "n7lFF_zE8nm",
        "invitation": "ICLR.cc/2023/Conference/Paper1054/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of critical sampling for a dynamical system by sampling from the high error regions of the state space. Since the underlying system and hence the error is not known in the region where data does not exist, the authors propose to use a surrogate error given by the reciprocal prediction error of the model. The model effective consists of two parts, a forward prediction module to predict $x_{t+1}$ from $x_t$ and a backward module to predict $x_t$ from $x_{t+1}$. The model is trained both from samples and also a reciprocal error measuring how well the prediction is going forward using the forward module for $K$ time steps and then going backward using the backward module for $K$ time steps. Some theoretical results and empirical simulation results are shown for ODEs and PDEs showing that the proposed model outperforms the baseline.",
            "strength_and_weaknesses": "**Major Weaknesses**\n* The literature review is incomplete, many other methods for learning ODEs are not mentioned. Examples are HMMs, and linear and nonlinear state space models (LDS, SLDS, fLDS, SVAE, RNNs, etc.).\n* The original motivation for the presented method is that the existing models require large sample sizes to learn the dynamics and converge, however supporting figures are not accompanied. In time series and dynamical systems specifically, the critical sample size depends on many factors including the initial condition, sampling frequency, dynamical regime, etc. Further investigation is needed to address under what conditions a new method for critical sampling is helpful.\n* It is unclear how can the presented method be applied in practice for physical systems. Even if we know what the critical (high error) samples are, it's often the case that we cannot simply change the state of the system towards those samples. The results are only shown in the simulations, questioning whether the method is applicable to real physical systems or not.\n* Above the methods section, 4 contributions are elaborated. Contributions 1 and 2 are not independent and can be listed under the same number. Contribution 3 is not new, there is a large list of papers learning forward-backward models, i.e. simultaneously learning the forward and inverse models. In addition, the connections to HMM models are not explained, since the inference algorithms in HMM and LDS consist of a forward-backward loop, it's important to explore whether there is a connection. Importantly, HMM and LDS can handle noise, whereas in my understanding the presented model works in the noiseless regime (which is usually not the case for the real physical systems). Regarding contribution 4, I'm not quite convinced that this is fully addressed through empirical studies. The systems that are considered are very specific, comparisons are done against a specific baseline and many other models are not considered. The sampling frequency is not chosen according to the best practices.\n* The systems considered in this paper are all autonomous, this limits the scope of the presented models as there are many physical systems that aren't fully autonomous. In addition, the systems considered are low-dimensional. Since the high error regions are determined by sampling a grid in the state space, in the high dimensional cases it's unclear that the method scales properly.\n* The finding about the reciprocal error being correlated with the network modeling error is highly dependent on the underlying system, properties of the underlying flow field, learning algorithm, initial sample, dynamical regime, etc. It's unclear whether this finding holds for other physical systems.\n\n**Minor Issues**\n* The visualizations can be dramatically improved. The plots are Python's basic plots and the fonts are very small. The plots can be made more professionally.\n* What does the following mean in the second paragraph after the methods section?\n> The solution evolution is uniquely defined without bifurcation\n* In the first paragraph on page 3, \"effective\" should be changed to \"effectively\".\n* There is a grammar error in the following:\n> Learning the local spatial dynamics. The dynamical system may exist a highly nonlinear and complex behavior in the whole spatial domain ...\n\n## Post Rebuttal (Round 1)\nI thank the authors for their detailed responses and new additions to the paper. Below I reply to specific points raised by the authors in response to my criticisms.\n\n> Response: In the revised paper, we have included a separate section for related work (Section 2), and included a review of these statistical modeling/learning methods. It should be noted that, in this work, our focus is to study the problem of data-driven modeling of unknown dynamical systems whose behaviors are controlled by their underlying governing equations. These methods focus on statistical learning or modeling of more generic system observation data.\n\nOne interpretation of the models I mentioned such as HMMs and linear and nonlinear state space models (LDS, SLDS, fLDS, SVAE, RNNs, etc.) is that they are approximating an underlying dynamical system or ODE using a certain function family determined by the model. For example, in the case of LDS this function family is linear ($x_{t+1} = Ax_t$) whereas for SLDS this function family is piecewise linear and for RNN it can be a generic nonlinear function parameterized by a neural network $x_{t+1} = f(x_t)$. In this sense, I don't see a difference between the articulated model and these existing frameworks and I don't find the reason convincing enough to exclude a comparison with these models.\n\n\n> Response: Thanks for pointing out this. In existing work on data-driven modeling of dynamic systems, they need large sets of training samples to successfully train the deep neural network to achieve a reasonably accurate approximation of the system evolution trajectories. Table 1 of the revised paper summarizes the number of needed training samples for the method to achieve the desired model prediction error. For example, for the first Damped Pendulum system, the baseline method needs 14400 samples to achieve an average prediction error of 0.0263 so that the predicted system trajectory is fairly close to the true trajectory.\n\nAs I mentioned in my original reply, intuitively the number of samples needed depends on many factors, including but not limited to the sampling frequency, dimension, and dynamical regime (e.g. chaotic, stationary, stable, etc). Since the main motivation for this work is that models require large sample sizes, this should be illustrated more thoroughly through empirical results on multiple systems when best practices are used (e.g. optimal sampling frequency and models with lower sampling requirements). A vanilla comparison with an ad hoc method where no consideration of the best practices does not sound like a convincing baseline, to begin with.\n\n> Response: We really appreciate these insightful comments! We totally agree with you that in practical systems, it is not straightforward to change the state of the system towards those samples. Here are our thoughts and responses to address the issues that you pointed out ...\n\nI was expecting to see some results on physical systems in response to this question. If this is applicable only to simulation I don't find the contribution significant enough for publication. If this is in fact applicable to physical systems (as the authors argue it is) results must be accompanied.\n\n> Response: ... Sorry for the confusion. Our idea and method are quite different from the HMM and LDS. Specifically, in our work, we construct a multi-step reciprocal prediction network structure, which performs K-step forward prediction followed by K-step backward prediction by concatenating K forward prediction networks plus K backward prediction networks together. Interestingly, we find that this multi-step reciprocal prediction error obtained from this prediction loop is highly correlated with the model prediction error.\n\nI understand the motivation and the presented method and how they are different from LDS and HMM. I apologize for the unclear question and I hope this clarifies the question further. Regardless of the motivation of your paper, the algorithm that you've presented has forward and backward parts where you run the model forward through the forward mapping and then backward through the inverse mapping and evaluate the error and backpropagate through the parameters of the model. Indeed you use the error to efficiently sample from the dynamical system for the next iterations and hence the active learning framework. Now my question is, the forward-backward algorithm appears in other time series models as well (such as Kalman filter, LDS, HMM, etc.)? It would be interesting to see if there are connections between the two sets of algorithms.\n\n\n> Summary of Responses ...\n\nI read the responses from other reviewers as well and I appreciated their input about the work. I do think however that some of the points that I raised remain concerning and unanswered and they are independent of the points raised by other reviewers. In summary, I think the empirical results do not fully support the claims in the paper and more systematic empirical evaluation is needed for backing up the arguments. Specifically, considering other models for comparisons, using best practices as the baseline, and considering underlying dynamical systems with different characteristics (dynamical regime, dimension, presence of noise, etc.) is critical for the claims to hold. Given this, I tend to keep my score as is.\n\n\n## Post Rebuttal (Round 2)\n\nI thank the authors for further explanations, in the following I wish to clarify my standpoint and the reasoning behind my score further.\n\n**Regarding the motivation of the paper,** it is mentioned that learning ODEs using deep networks require large sample sizes and critical sampling helps with reducing the sample size. While I think that this statement is true in general when a baseline model and vanilla sampling schemes are used for comparison, I think it remains unclear under what circumstances it's worthwhile to use the proposed framework. There are ad-hoc methods for reducing the sample size. For example, one could subsample the input data using a frequency given by the auto-correlation time of the time series. Or one can run a clustering and subsample within a cluster.\n\nI think the answer to this question depends on many factors, some of which are listed below.\n\nIt depends on the underlying model. If the underlying deep network utilizes linear functions, the sample size required for learning the dynamics is as low as the dimension of the system. While the linear model is just an example, it shows that certain deep learning function spaces (determined by the neural network architecture, choice of nonlinearity, number of nodes, and regularization) will inherently require fewer data sizes to be trained compared to others. If the effect of inductive biases of the function space on the sample size is not systematically studied it is not possible for a practitioner to decide if this framework will be useful for their case or not.\n\nIt depends on the dynamic properties of the data. For example, if the data is sampled from an attractor type or stationary ODE, intuitively, fewer samples should be needed to learn the dynamics compared to a chaotic attractor (e.g. Lorenz model). It's important to show under what regimes the claims hold so that the practitioners are able to determine if the method is applicable to their case or not.\n\n**Regarding the significance of this work,** I wish to clarify that I do not think that every paper needs to have experimental results in it. I have given high scores to papers with no figures and no data before and I do not question the importance of having empirical studies on simulations and theoretical studies. However, I do believe that if a method is developed mainly for practical settings, accompanying it with real data will make it stronger. In my opinion, the methodological novelty of this paper is limited. Leaning forward and backward functions existed before. The main novelty comes from the empirical finding that the multi-step error is correlated with the ground truth error, hence utilizing it during sampling will help reduce the error faster than vanilla sampling. Given that the novelty lies in this empirical observation, at the very least a systematic investigation of the conditions under which the claim holds is required.\n\nGiven that in the simulated cases the constraints imposed by the computational resources are less severe and we can train deep learning models in a short time, the main merit of this work lies in experimental settings where we are time and budget limited. Given the context I provided, unless some results on real-world data are not shown (by that I certainly do not mean that the authors should do the experiments themselves) I do believe that the scope of this work is limited. In addition, noise is an inseparable component of real-world data, in the presence of which learning the forward and backward model might be more challenging and the critical sampling framework might not hold.\n\nIf we accept that the scope is limited, and we base the judgment only on the simulation results, then more controls are needed to show that if we account for time complexity and other factors, then the presented framework has better inductive biases and achieves better test/generalization error. Creating a grid of points and running the forward-backward model for a few time points is a time-consuming process. To bring the comparisons to the same footing, one should consider the time spent on the optimization and not just the number of samples used for training. Additionally, deep learning models are proven useful mainly in high-dimensional settings. There is no systematic study of how much of the claims hold in higher dimensions and the feasibility of applying the method to higher dimensional settings in case of evaluating the error on a grid and training the forward and backward functions. I think that for the acceptance of an empirical paper in ICLR, a solid empirical investigation (including the ones I mentioned above) is necessary.\n\n\n\n## Post Rebuttal (Round 3)\n\n> Response: Thanks for pointing out this! It should be noted that, in this paper, we are learning deep neural network models of unknown dynamical systems ...\n\nThe auto-correlation is easily estimated from the data, it is the correlation of a signal with a delayed copy of itself as a function of delay. There is no need for the equations to be known. Regarding clustering, one can think of clustering as a way to sparsify the data samples. Start from a large sample size, run a simple clustering algorithm such as k-means and use the cluster centroids or data points that are close to cluster centroids for training a neural network. These are the simplest ad-hoc algorithms to reduce the data size, but more complex ones can also be considered. My point is, in your work you have a data selection process, which is to evaluate the error on a grid of samples and choose the high-error data points for training. This data selection process should be compared to other data selection processes such as the ones I mentioned above as opposed to the vanilla fixed-interval sampling scheme used in the comparisons to motivate the paper.\n\n\n> Response: Thanks for this insightful comment! We totally agree with you that the number of needed training samples does depend ...\n\n> Response: We agree with you that experiments on real-world data will be very helpful. We will follow your comment to ...\n\nI cannot base my judgment on the results that I have not seen. Once the new results are included I will re-assess the paper accordingly.\n\n\n> Response: We totally agree with you that learning different ODEs would require different amounts of data. In our problem, ...\n\nThis needs to be carefully evaluated and investigated. I understand that in the problem statement you are assuming that the governing equations are not known. But ground truth studies can investigate under what dynamical regimes (with known equations) the method performs better than others. \n\n> Response: Thanks for pointing out this! In this paper, we study the important problem of selecting ...\n\nGiven that in the simulated environment the proposed method takes longer to run compared to baseline methods, now I firmly believe that the main merit of the paper is in the real-world scenarios where data collection is challenging and costly. In the simulated setting, one is not concerned with the computational resources of using a larger data sample for training. In fact, this just shows another example of the no-free-lunch-theorem. You are using the computational resources in a different way (computing multi-step error for a grid of samples) compared to using the computational resources for processing batches of data.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The problem considered is new but in my opinion it is not significant. I did not try to reproduce the results but the parameter configurations are mentioned and one should be able to reproduce the results. The presentation can be improved, there are frequent grammatical errors but the logical flow of the paper is clear.",
            "summary_of_the_review": "I am not convinced about the motivations behind developing this technique, whether the proposed methods can be used in real physical scenarios, and conditions under which this method can be useful and outperform other models. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_6jX7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_6jX7"
        ]
    },
    {
        "id": "6TN68uQAym",
        "original": null,
        "number": 2,
        "cdate": 1666710475939,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710475939,
        "tmdate": 1666710475939,
        "tddate": null,
        "forum": "n7lFF_zE8nm",
        "replyto": "n7lFF_zE8nm",
        "invitation": "ICLR.cc/2023/Conference/Paper1054/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studied an interesting and important problem and tried to answer the question: given an unknown dynamical system, how to predict its evolution behavior accurately with minimum number of samples and how to select these samples. To be specific, the paper introduces a multi-step reciprocal prediction network, where a forward evolution network and a backward evolution network are designed to learn and predict the temporal evolution behavior in the forward and backward time directions, respectively. In addition to that, the paper also proposed a joint spatial-temporal evolution network which extends the framework to the space. The numerical result is appealing and demonstrates promising performance compared to baseline approach. \n",
            "strength_and_weaknesses": "This is an interesting paper, which studied an important problem in dynamical system modeling. The methodology is well presented and easy to follow. I also want to commend on the numerical results and their figures, which are easy to be interpreted and demonstrate promising performance. The finding of strong correlation between the network modeling error and the multi-step reciprocal prediction error is super interesting and it is also very smart to use the multi-step reciprocal prediction error to approximate the desired network modeling error. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and well presented",
            "summary_of_the_review": "See above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_mGbk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_mGbk"
        ]
    },
    {
        "id": "PwqeBUBHb4",
        "original": null,
        "number": 3,
        "cdate": 1667009540368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667009540368,
        "tmdate": 1670790399930,
        "tddate": null,
        "forum": "n7lFF_zE8nm",
        "replyto": "n7lFF_zE8nm",
        "invitation": "ICLR.cc/2023/Conference/Paper1054/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents an approach to predicting a dynamical system with a spatio-temporal neural network and an active learning method for choosing a limited number of training samples to collect. They are able to greatly reduce the number of samples needed to predict a dynamical system. ",
            "strength_and_weaknesses": "Strengths:\n\nI think the problem of strategically choosing a limited training set for dynamical systems is important, and I'm glad to see more work done in this direction. The approach in this paper seems novel and creative to me. \n\nIf I understand correctly, the reported errors are on very long trajectories, whereas the temporal evolution model is trained on just one step. Long-term prediction of dynamical systems is difficult, so it's impressive to see comparisons with long-term errors. \n\nWeaknesses:\n\nI think that the writing could be improved in terms of clarity, reproducibility, and the writing on related work. (I have more details on that below.) Also, if I understand correctly, the errors reported are on 1-5 trajectories, which is very low in my experience. I would like to see errors averaged across a larger number of trajectories to see if the network generalizes well. I have additional comments in the next box.",
            "clarity,_quality,_novelty_and_reproducibility": "Novelty: as mentioned above, this seems like a novel and creative approach to this problem. However, the literature review could be improved and I don't agree that \"this work is one of the first efforts to address this challenge.\" I found [A] and [B] quickly as examples, but saw many other papers that are addressing this problem, sometimes with a particular application area of dynamical systems in mind. \n\nThe literature review and vocabulary could be improved. \n1. I found the paragraph from the Introduction beginning with \"In recent years, data-driven discovery of the governing equations of physical systems from measurement data has emerged as an important area of research. There are two major approaches that have been explored\" confusing. Firstly, I had a hard time understanding the distinction of the two approaches. Is the core distinction between learning the equations vs. learning the operator? If that's the point you're trying to make, then I think it's confusing to specify \"classic sparse regression methods and modern neural networks\" in the introductory sentence of the first approach and \"train a deep neural network\" in the introductory sentence of the second approach, since that's more specific than a lot of the literature, including the citations that follow. Also, \"data-driven discovery of the governing equations\" sounds to me like at the end you have equations that you can write down. In that case, I would include references for symbolic regression. I also wouldn't include citations for approaches that end in a black-box model without equations because that's a different task (perhaps \"approximation\" of the governing equations or surrogate modeling rather than \"discovery\" of the equations). Maybe you want to use broader language than \"data-driven discovery of the governing equations\" for this paragraph or split off the black-box approaches into a different paragraph? (I later found that there is a section in the appendix that repeats some of this, and the wording there is more clear.) \n2. Since this is a machine learning conference, it would be helpful to use machine learning terminology by mentioning that this is an \"active learning\" problem. This is not mentioned until the very end when the paper says it's \"related\" to active learning methods. I think that this \"Further Discussion\" paragraph at the end that mentions the relationship to a few other methods would make more sense in the introduction when other related work is discussed. I also don't see any citations for other papers that work on the same problem of choosing good samples for learning a dynamical system. Note that some related work would be under names like \"optimal experiment design\" and \"system identification.\" A couple of recent examples from a quick search are [A] and [B]. \n\n\n[A] Mania, Horia, Michael I. Jordan, and Benjamin Recht. \"Active Learning for Nonlinear System Identification with Guarantees.\" J. Mach. Learn. Res. 23 (2022): 32-1.\n\n[B] Huang, Yu, et al. \"Physics-Coupled Spatio-Temporal Active Learning for Dynamical Systems.\" IEEE Access (2022).\n\n\n\nClarity:\n\nI found the \"Sample augmentation based on local spatial dynamics\" section confusing. If points are successfully predicted by the spatial dynamics network, they are added to the sample set. What's the definition of being successfully predicted by the spatial dynamics network? Does that mean that you already know the future state for these points? If so, then doesn't this invalidate the goal of being able to know which samples to add without already knowing the ground-truth? \n\nAlgorithm 1 (in the Appendix) adds some clarity, but I found it still vague. For example, is the multi-step reciprocal prediction error averaged across \\bar{S_F^n}? How big is the \"large set of samples\" V_F? The main paper references adding points if they are successfully predicted by the spatial dynamics network. Is that incorporated somewhere in Algorithm 1? Within \"generate a large set of samples\"? \u2028\nUntil the appendix, I couldn't find any reference to how the final errors were defined as in Table 1, Figure 5, etc.. Based on Section A.3 in the appendix, I assume that those errors in Table 1 & Figure 5 are the errors on the 1-5 trajectories mentioned in this section, and evolving forward much longer (the intervals in this section)? Is that correct? Are those trajectories ones that are not in the training set? I think the errors shown in the main paper should have some definition, since this is confusing, even if the full details are in the appendix. \n\nI found it confusing when I got to the definition of S^n_F, which uses J_n. I eventually figured out that n is used in two ways: dimensionality of the system, and an index for the step in the active learning process. It would be helpful to choose a different letter and to explain J_n. (This is not defined until the appendix.) \n\nThe fonts in the figures are often too small to read. \n\n\"With this, we are able to perform dynamic selection of critical samples from regions with high network modeling errors and develop an adaptive sampling-learning method for dynamical systems based on the spatial-temporal evolution network.\" Since you have two ways of adding more training samples, how did you balance the two? (The interaction of these two ideas was somewhat explained once I got to the Appendix, Section A.4, but I'm still confused. I think this should be more clear in general, but also that there should be some explanation of this in the main paper.) \n\nIn Section A.3: a sentence abruptly ends: \"is used and.\"\n\n\nQuality: \n\n\"This number is empirically chosen since it is needed for the network to achieve a reasonably accurate and robust learning performance.\" How did you define the number of samples needed by the baseline method? There would be a tradeoff between number of samples and error, just like with your method. Did you test how few samples you could use while reaching a target error? This is important to clarify in order to see if the claims are accurate. \n\nThe paper emphasizes that the problem of needing more training data is much worse in higher dimensions. However, the examples in this paper are primarily 2-3 dimensional. At first, I thought that the Burgers equation example would be high-dimensional (the dimensionality of the spatial discretization). However, in the appendix, we learn that the PDE is tranformed into a lower-dimensional problem (I think 9-dimensional.) I think this should be stated in the main paper for fairness. \n\n\nReproducibility: The code and data are not shared. As described above, there are many aspects of this method that I found unclear, so I would not be able to reproduce this paper.\n\nOf note: I did not check the proofs.\n\n**Post Rebuttal**\n\nThanks for your responses! I think that most of my concerns were addressed, and I will raise my score. I look forward to trying this method on my problems after it is published.\n",
            "summary_of_the_review": "I think that this paper is interesting (important problem and novel approach). However, the writing needs quite a bit of work, and I have some clarification questions that would help me understand how strong the claims are. As mentioned above, I would also like to see errors averaged over more trajectories that were not used for training. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_C4XT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1054/Reviewer_C4XT"
        ]
    }
]