[
    {
        "id": "gVw-WH_pcfD",
        "original": null,
        "number": 1,
        "cdate": 1666641719785,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641719785,
        "tmdate": 1666641719785,
        "tddate": null,
        "forum": "8yVy6LdhER4",
        "replyto": "8yVy6LdhER4",
        "invitation": "ICLR.cc/2023/Conference/Paper3558/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a framework explaining how attention might be learned. First, the model would get the knowledge to translate individual words (KTIW) based on word co-occurences, which can be learned if the attention weights are uniform. KTIW then drives the learning of the attention mechanism.",
            "strength_and_weaknesses": "Strengths\n\nThe experiment showing that copying sequences is a difficult task under some constraints is insightful.\n\nThe paper shows how multi-head attention can improve learning dynamics.\n\nWeaknesses\n\nThe paper presents a plausible 2-stage learning approach for attention, but it doesn't really show that this happens in practice. For example, we could monitor the entropies of the output distributions and of the attention weights during training.\n\nThe simplifying assumptions are quite strong. Under the proposed framework, we may not be able to explain phenomena such as Figure 9 in [1], where attention does not necessarily match work alignment.\n\n[1] Koehn and Knowles. Six Challenges for Neural Machine Translation. First Workshop on Neural Machine Translation. 2017",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper would increase if it more clearly demonstrated that the proposed framework matches learning dynamics on non-toy tasks. The paper is mostly clear. To my knowledge, the work is original, but the contributions are arguably limited. The paper should be mostly reproducible with the attached code.",
            "summary_of_the_review": "The paper proposes an interesting and plausible framework of how attention is learnt. However, it does not clearly show that the 2 stages of learning happen in practice (is it really sequential?). Some of the assumptions may be too restrictive.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_AtcN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_AtcN"
        ]
    },
    {
        "id": "6nDKruhWf7",
        "original": null,
        "number": 2,
        "cdate": 1666787614146,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666787614146,
        "tmdate": 1666787614146,
        "tddate": null,
        "forum": "8yVy6LdhER4",
        "replyto": "8yVy6LdhER4",
        "invitation": "ICLR.cc/2023/Conference/Paper3558/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigates how seq2seq attention weights are learned in a machine translation setting. The paper defines a proxy to measure a model\u2019s ability to translate individual words, which they shorten to KTIW. The paper claims that this measurable quantity is a driver for the model to learn to attend well. The evidence presented in favour of this hypothesis is that KTIW can be learned (it is similar to learned attention weights) when attention weights are frozen and  uniform, but that when KTIW is not learned, neither are the attention weights. This suggests a sort of necessary condition on KTIW for attention learning, which the paper claims is a causal driver for attention learning.\n\nThe paper suggests that it is therefore possible to reduce the problem of understanding attention learning to the problem of understanding KTIW (which does not seem to be entirely true, as KTIW may be necessary but not sufficient condition for learning of attention weights).\nThe paper thus proposes a proxy model for approximating KTIW learning (that is, a proxy for the proxy for attention weight learning), and verifies this. The paper claims that attention weights are learned in two stages: first KTIW is learned through word co-occurence statistics, and second, a learned KTIW drives the learning of attention.\n",
            "strength_and_weaknesses": "*Strengths:*\n- Aiming for a theoretical understanding of learning dynamics and attention is an admirable goal. The high level topic would be of interest to many in the community.\n\n*Weaknesses:*\n-  The major weakness of this work is that I found the text and presentation of results really difficult to parse. It is also difficult to establish what exactly each experiment contributes to the logic of the paper, why they were designed in that way, or the takeaways from each expreriment, which made it quite a difficult read.\n\n- What are the implications of the results in this paper? I think I am missing the major takeaway from this work. It is not clear to me how defining this proxy for machine translation is useful for interpretability and understanding machine translation learning dynamics.\n\n- The KTIW proxy seems to assume a 1:1 ratio of input to output words for translation, which surely cannot hold true in the general MT case? Perhaps I am missing something here. \n\n- If I understand correctly, the logic is section 5.3 seems to be circular. The paper defines the initialisation of an attention head as \u2018good\u2019 if training with that single head converges (on a task that requires only a single attention head). The paper then constructs multi-head attention scenarios with all bad initialisations, all random, or only-one-good head initialisation. They present the result that if all initialisations are bad, they model does not learn, but if at least on is \u2018good\u2019 then the model will converge. As the task is learnable with a single attention head, this logic seems circular and trivially true. It is not clear to me what it adds to the paper and seems off-topic.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The text is quite hard to parse, and the introduction doesn\u2019t do a great job at justifying the work. The paper would benefit from some work to improve the logical flow in the presentation of ideas and arguments.\n\nThe title suggests that the paper is broadly about understanding attention mechanisms but really the paper focuses on machine translation and seq2seq models. It would be helpful for the reader if this information was made explicit in the title and abstract.\n",
            "summary_of_the_review": "This paper investigates how seq2seq attention weights are learned in a machine translation setting. The paper defines a proxy to measure a model\u2019s ability to translate individual words, which they shorten to KTIW. I found this paper very hard to follow, as the writing, justifications and logical flow of the word hid the scientific contributions. The paper would benefit from significant clarifying and removal of experiments (e.g. section 5.3) that detract from the points the authors wish the readership to takeaway - this might enable more readers to see the scientific contributions of the paper to the wider literature more clearly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_H7cW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_H7cW"
        ]
    },
    {
        "id": "M83_iemXCc",
        "original": null,
        "number": 3,
        "cdate": 1666900541376,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900541376,
        "tmdate": 1666900541376,
        "tddate": null,
        "forum": "8yVy6LdhER4",
        "replyto": "8yVy6LdhER4",
        "invitation": "ICLR.cc/2023/Conference/Paper3558/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper aims to understand the training dynamics of the Attention mechanism through lexical prob $\\beta$ and its learning proxy model.\n",
            "strength_and_weaknesses": "Strength:\n1. The paper did some rigorous analysis of the attention training mechanism and drew a connection to the word alignment of the classical models.\n2. There is a connection to interpretable classification for the mechanism described in the paper.\n\nWeakness:\n1. The applicability of such an understanding is now clear.\n2. The analysis has a lot of underlying assumption (almost all of them are pointed out by the authors) which does not usually hold.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Why a symmetric KL between the distributions is not used for measuring agreement? Why only look at the top value if only important in this scenario?  Especially when the assumption - \"the word's information is only bounded into its local hidden state\", does not hold then just looking at top attention weight might not be the correct thing to do. \n2. Does the inductive bias of LSTM plays any role in this analysis?\n3. Even if you prove that training of attention does not occur until the training of the proxy of the KTIW, does that immediately mean KTIW drives the attention training?\n",
            "summary_of_the_review": "All the observations made are interesting, and the intuitive connections are also very fascinating. However, I still fear that these analyses might not have many practical applications and even won't hold in many different learning dynamics.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_8y4q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3558/Reviewer_8y4q"
        ]
    }
]