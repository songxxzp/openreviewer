[
    {
        "id": "OPRGXeuedU1",
        "original": null,
        "number": 1,
        "cdate": 1666235218602,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666235218602,
        "tmdate": 1666411227221,
        "tddate": null,
        "forum": "-p5ZEVGtojQ",
        "replyto": "-p5ZEVGtojQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4980/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a continuous formulation of an RNN for modeling sequential data. The contribution of this work is a mechanism to continuously increase depth of recently proposed ODE-based RNN models, as well as a model that is motivated by a 1D-Heat equation. Two benchmark problems (person activity recognition and Walker2d kinematics) are used to show that the proposed models can outperform related models on irregularly sampled sequential data. ",
            "strength_and_weaknesses": "The ideas presented in this paper are interesting, and the results demonstrate the effectiveness of the proposed models on 2 tasks. However, I feel that the innovations are somewhat incremental. Moreover, I have several concerns that I would like to see addressed:  \n\n* It is not clear to me how the individual components that have been proposed affect the performance, since a detailed ablation study is missing. Is the performance boost due to the specific form of the gates, or due to increasing depth, or because the model has at least twice as many weights as a simpler ODE-GRU. At a minimum it would be helpful to list the parameters for all the models. Then, it would be nice to see how the performance of the proposed model changes as a function of depth.  \n\n* You claim that your model is continuously evolving in both the temporal and depth directions, but I am not sure what this means. Where do you show that the model is learning something continuously?  Once you discretize your model for training, you yield a discrete computational graph. There have been several recent works that show that Neural ODEs discretized by explicit Euler do not provide a useful representation for the underlying continuous dynamics of the system of interest. Also, based on your results, it seems that it doesn't matter too much which discretization scheme is used. So, I assume something else must be going on. \n\n* A general problem of RNNs (whether formulated as a discrete or continuous model) are vanishing and exploding gradients during training. Gates certainly mitigate this issue, but it would be good to understand how the proposed model performs on tasks that involve longer sequences. \n\n* For an experimental paper, two sets of experiments that demonstrate the performance of a new model are somewhat weak. \n\n* The paper is missing to discuss several recent state-of-the-art RNNs, based on ODEs, that have appeared in ICML, ICLR, such as coRNN, UnICORNN, incremental RNN,  LEM, and several other related works. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the quality of presentation is okay. In some parts it is difficult to follow the arguments (a better organization could help). The ideas presented in this paper are novel, but I don\u2019t feel that the work proposes innovations that are very significant. My main concern is that it is difficult to understand the effects of the different proposed components on the performance, due to a limited discussion and a missing ablation study. Further, I feel that two experiments are not sufficient to demonstrate that this model is useful more generally. The authors do not provide code, to reproduce results. I feel that it would take quite some time to reproduce the results based on just the technical description. ",
            "summary_of_the_review": "In summary, this is an interesting paper with some potential, but I feel that this paper is not ready for publication in its current form. It requires a major revision to better present the impact and significance of the different aspects of the proposed innovations. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_FygP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_FygP"
        ]
    },
    {
        "id": "kKnsd4t01YR",
        "original": null,
        "number": 2,
        "cdate": 1666358129951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358129951,
        "tmdate": 1666418080890,
        "tddate": null,
        "forum": "-p5ZEVGtojQ",
        "replyto": "-p5ZEVGtojQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4980/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a new type of recurrent neural network that operates over continuous time and continuous depth.\nThe continuous depth is achieved through an auxiliary variable t' that is integrated in each step from zero to a certain Tmax. The paper also proposes a heat equation PDE model to decode both time variables.",
            "strength_and_weaknesses": "# Strenghts\n- The paper presents an interesting idea on a relevant topic (time-continuous sequential data).\n- The paper presents a novel PDE model for resolving the problem of having two different time variables.\n\n# Weaknesses\nThe experimental evaluation of the paper is insufficient for ICLR.\nFirst of all, the paper lacks discussion and comparison with significant related works.\nFor instance, [1,2,3,4,5,6,7] propose models for processing irregularly sampled time series. All of these models have demonstrated significant improvements in continuous-time modeling in the past two years alone.\n\nMoreover, the selection of benchmark datasets is insufficient. Particularly, the paper only evaluates the model on two datasets. Additionally, both evaluations seem to be sourced from (Lechner & Hasani, 2020), which did not perform any hyperparameter tuning, evaluated on five datasets, and does not include any method published after 2020.\n\n[1] Romero et al. CKConv: Continuous Kernel Convolution For Sequential Data. ICLR 2022.  \n[2] Gu et al. Efficiently Modeling Long Sequences with Structured State Spaces. ICLR 2022.  \n[3] Kidger et al. Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.  \n[4] Morrill et al. Neural Rough Differential Equations for Long Time Series. ICML 2020.  \n[5] Gu et al. HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS 2020.  \n[6] Kidger et al. Efficient and Accurate Gradients for Neural SDEs. NeurIPS 2021.  \n[7] Shukla et al. Multi-Time Attention Networks for Irregularly Sampled Time Series. ICLR 2021.  \n",
            "clarity,_quality,_novelty_and_reproducibility": "Regarding novelty, the combination of ODE in time and depth is not a too novel contribution. The main novelties lie in the heat PDE model I belive.\n\nThe writing could be sharpened. Specifically, the use of the word \"labeling\" to describe the processing of data by a neural network is inconsistent with the literature on ML. The common understanding in ML is that labeling describes the process of obtaining targets (i.e., \"labels\") for the training of the network. Moreover, the use of \"and sequence data\" in the very first sentence is unnecessary here as \"in sequence processing tasks\" already require sequence data. More minor things like that make reading the paper unpleasant. \n",
            "summary_of_the_review": "Overall, minor but interesting contribution with a bit of unconventional writing and insufficient experimental evaluation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_itQc"
        ]
    },
    {
        "id": "SPYsPzElZN",
        "original": null,
        "number": 3,
        "cdate": 1666740291056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740291056,
        "tmdate": 1666740291056,
        "tddate": null,
        "forum": "-p5ZEVGtojQ",
        "replyto": "-p5ZEVGtojQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4980/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on learning of multivariate irregularly sampled time series data. Recently, several works have introduced continuous version of RNNs based on Neural ODEs to solve the problem of irregular sampling. This paper generalizes the recurrent Neural ODE models by continuously evolving the hidden states in both temporal and depth directions. It employs two separate differential equations to evolve the hidden states in both horizontal and vertical direction alternatively. The paper also proposes heat equation based model which treats the hidden state computation as solving heat equation over time. This model is also able to learn a better representation by utilizing information from past as well as future. Experiments show that the proposed models outperform the RNNs and neural ODE based models on two datasets.",
            "strength_and_weaknesses": "### Strengths\n1. The proposed ideas of learning continuous depth model as well as using second order partial differential equation to evolve hidden states are novel and interesting.\n2.  Experimental results show the effectiveness of the approach when compared to baseline approaches.\n3. The paper focuses on the task of learning from irregularly sampled data which is important in many domains.\n\n### Weaknesses\n1. Although it is interesting work, it is not well motivated. Why would continuous in depth potentially be useful for modeling irregularly sampled time series data?\n2. Another key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. The paper only experiments on a single real-world dataset and missing experimentation of benchmark datasets (e.g. PhysioNet, MIMIC-III).\n3. The paper is missing important related work and comparison with the SOTA in modeling irregularly sampled time series [1]. The paper also seems to underperform compared to [1] on person activity recognition.\n4. In the human activity dataset, which is a per time-step classification problem, it seems that the sequence was fed into the proposed model as a whole, thus the model has access to future observations for making a classification prediction on \"past\" time steps. This would mean that it has an unfair advantage over the RNN based models. \n5. RNN and ODE based recurrent approaches can be used for whole-time series classification, imputation, interpolation and extrapolation tasks. The paper only focuses on per time point classification/regression task which is very limiting. It is also not clear how the run time of the proposed approach compare to that of ODE-RNN and other baselines.\n\n\n#### References\n1. Satya Narayan Shukla and Benjamin Marlin. Multi-time attention networks for irregularly sampled time series. In International Conference on Learning Representations, 2021.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Reproducibility**: Not reproducible. Although the paper includes the hyperparameters, the code to reproduce the results is missing.\n\n**Novelty:** The proposed ideas of learning continuous depth model as well as using second order partial differential equation to evolve hidden states are novel and interesting. It is not well motivated though.",
            "summary_of_the_review": "My recommendation for this paper is reject. Although the paper proposes a novel solution, motivation is lacking behind the proposed approach, rigorous experimentation is missing.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_Ngo4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4980/Reviewer_Ngo4"
        ]
    }
]