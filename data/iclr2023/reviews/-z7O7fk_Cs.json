[
    {
        "id": "wwVnyykeKm",
        "original": null,
        "number": 1,
        "cdate": 1666605113952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605113952,
        "tmdate": 1669211590472,
        "tddate": null,
        "forum": "-z7O7fk_Cs",
        "replyto": "-z7O7fk_Cs",
        "invitation": "ICLR.cc/2023/Conference/Paper3140/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper under consideration proposes a normalizing flow model (called JKO-Flow) which combines JKO scheme and Continuous Normalizing Flows. The previous approaches which dealt with JKO scheme parameterized the pushforward transform $T(x)$  arising in JKO objective either by ICNNs [1, 2] or general NNs of the form $F : \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ [2] . The main idea of the authors is to replace the pushforward $T(x)$ with continuous dynamics $T(x) = x + \\int\\limits_{t_k}^{t_{k + 1}} f_k(x(t)) d t$ modeled via the techniques developed for Continuous Normalizing Flows.\n\n[1] Mokrov et. al. Large-Scale Wasserstein gradient flows, https://arxiv.org/pdf/2106.00736.pdf\n\n[2] Fan et. al., Variational Wasserstein gradient flows, https://arxiv.org/pdf/2112.02424.pdf\n\n**Post-rebuttal:** I increase my score by 1",
            "strength_and_weaknesses": "**Strength**\n\n- To the best of my knowledge, the proposed idea is fresh. It seems that the continuous dynamic parameterization helps to leverage some difficulties faced by previous approaches (like $\\log \\det J_{T}$ estimation [1] or $\\min \\max$ objective [2]). Given a good integral estimator the learned JKO-Flow could be easily inverted in a neural ODE manner without need of specific optimization procedures [1]. \n- The idea to optimize block-composed Neural Network by sort of \"divide and conquer\" block-wise algorithm also seems to be interesting.\n\n**Weaknesses**\n- No comparison with  [2]. The proposed JKO-Flow approach seems to be directly related to their method. \n- It would be interesting to see the generative properties of the JKO-Flow approach directly on data space of nontrivial image datasets like CIFAR10 / CelebA. As I understand, the proposed approachcan be straightforwardly adopted for such (image-data) scenarios.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Questions** \n- The objective (8) in the article is actually the $l_2$- regularized (or, $W_2$ - regularized) objective of the original MLE optimization problem for normalizing flows. And this observation reveals some questions regarding the usefulness of the proposed methodology. What you actually do: you separately train $\\textit{weak}$ ResNet blocks (weak in the sense that they are weaker than the ResNet network they constitute) with the objective which doesn\u2019t seem to be much more tractable than the original MLE optimization problem. In light of this I have the following questions: At first, if each ResNet block is too powerful to solve the regularized problem can we just use one (or small number of) ResNet blocks to solve the original unregularized problem? In other words, do we really need 74K params (for POWER), 76K params (for GAS), 112K params (for MINIBONE) and 396K params (for BSDS300) of OT-Flow and FFJORD models to achieve the competitive performance (compared to JKO-Flow)? On the other hand, if the ResNet block is too weak to solve the unregularized problem, how to estimate the number of steps and step sizes $h_k$ used at each iteration. Which $h_k$ still makes the regularized problem tractable for the single ResNet block?\n- Since we substitute the real ODE with the sequence of JKO problems and use the ResNet block parametrization the real solution and the obtained solution of ODE could diverge. Can we guarantee that the sequence of JKO problems still converges to the target distribution $p_z$?\n\n**Clarity:** The paper is more-or-less well-written.\n- The link to the article [2] should be added. Moreover, the authors of [2] called their method \u201cJKO-Flow\u201d which is similar to your approach\n- I don\u2019t understand the discussions regarding invertibility of ResNet blocks $f(x, t)$ (section 2.3). We don\u2019t need $f(x, t)$ to be invertible since we model the pushforward transform as $x \\rightarrow x + \\int\\limits_{t_k}^{t_{k + }} f(x(t)) d t$. For the same reason I don\u2019t understand the operation $f := f_k \\circ f_{k - 1} \\dots \\circ f_0$ in the Algorithm 1. The functions $f_i$ don\u2019t correspond to pushforward transform and the composition operations don\u2019t make sense. \n- The appendix B (which describes trajectory enhancement) is hard to follow. From my point of view, it can be rewritten in a more pleasant way. (it is just minor comment and it does not affect the recommended score)\n- How many ResNet blocks and which $h_k$ were used for high-dimensional tabular data and Mnist experiments?\n\n**Novelty:** The novelty is highlighted in the \u201cstrengths\u201d section\n\n**Quality:** The technical quality issues are raised in the \u201cweaknesses\u201d and \u201cquestions\u201d sections. Also there are some clarity points-to-fix.\n \n**Reproducibility:** I didn\u2019t run the code provided but it seems to be clearly written and I have no special doubts regarding reproducibility. \n",
            "summary_of_the_review": "The authors present a new idea which combines CNFs and JKO scheme. However, there are some questions regarding the usefulness of the proposed approach. Moreover, It is recommended to stress-test the method on the image generation problems which are standard benchmarks for many modern Normalizing flows models.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "none",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_T8Fn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_T8Fn"
        ]
    },
    {
        "id": "I9CRgBcGFcD",
        "original": null,
        "number": 2,
        "cdate": 1666611267992,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611267992,
        "tmdate": 1669217813586,
        "tddate": null,
        "forum": "-z7O7fk_Cs",
        "replyto": "-z7O7fk_Cs",
        "invitation": "ICLR.cc/2023/Conference/Paper3140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a continuous-time normalizing flow model based on the JKO scheme for gradient flows in the Wasserstein-2 space. A neural ODE flow network is trained to approximate the JKO scheme. This results in a block-wise training procedure where each block approximates a JKO step. Several such blocks are trained sequentially and the final flow is obtained via a composition of the blocks. Experiments demonstrate that the proposed method performs competitively against baseline models while being more computationally efficient. ",
            "strength_and_weaknesses": "[Strengths]\n* The paper presents an interesting method of training a continuous-time normalizing flow to approximate the JKO scheme. This allows the model blocks to be trained sequentially with potentially larger batch sizes. \n* While the JKO scheme has been explored before in the machine learning context for large-scale gradient flows (Mokrov et al., 2021), the proposed framework is novel to the best of my knowledge.\n \n[Weaknesses]\n* The quality and clarity of the write-up and the overall presentation can be improved.\n    * The need for the discussion in Section 2.2 is unclear. The result in Eq. (12) can be directly arrived at by using the fact that JKO scheme recovers the FPK equation which gives us the steepest descent curve. Then a direct matching of the terms in FPK for the OU process with the Liouville Equation gives  the result in Eq. (12). Also see the discussion in Appendix D of Song et al., 2021.\n    * Section 3.2 can be better explained. I did not understand what \"progressive refinement\" is and how exactly does it help the overall training.\n    * The scores can be reported in a more readable format. The \"e\" notation is messy.\n    * The paper ends abruptly after the MNIST experiment without a follow-up discussion or conclusion.\n    * Minor issues:\n        * What is meant by \"trajectory obtained by **rest blocks**?\"\n        * Rephrase \"can be **trained by iteslf**\".\n        * \"Th theoretical assumption\" -> \"The...\"\n        * \"space of probability\" -> \"space of probability measures\".\n        * The \"T\" for termination criterion in Algorithm 1 can be replaced with a different symbol. Currently it coincides with the \"T\" for the optimal transport map.\n        * A few other minor typos here and there. Please proofread.\n* On the claims in introduction:\n    * The authors discussed non-uniqueness of the flow as a motivation. However, it is unclear how/if the flow induced by the proposed method is unique. How does minimization via Eq. (8) ensure uniqueness? Just empirical evidence is not enough. Previous methods have also provided such evidence.\n    * It's unclear if the motivation of pruning unused blocks is actually relevant to other models. This seems to be a issue with the proposed model wherein a naive implementation results in unused blocks.\n* On experiments:\n    * The experiment results are not convincing enough. Most of the setups considered in the paper are toyish in nature. \n    * Why do the MMD results in Table 2 differ significantly from the results reported in Onken et al., 2021? If this is due to a difference in setups, what happens if the setup described in Onken et al., 2021 is used for experiments? What motivated the choice of specific bandwidth in MMD? In my experience, MMD is very sensitive to bandwidth choices. Can a better metric be used?\n    * Is it possible to evaluate the log-likelihood under this model? If yes, why did the authors not compare the log-likelihoods of the models?  \n* On the relation to prior work:\n    * With the specific choice of the OU process and the discussion around Eq. (12), the relation to probability flow ODE in Song et al., 2021 should be better discussed. \n\n**Post Rebuttal Edit**: Increased score from 3 to 5.",
            "clarity,_quality,_novelty_and_reproducibility": "* The quality of the write-up and presentation can be improved. See weaknesses above for specifics.\n* The paper presents a technically interesting idea. However, not all the claims in the introduction have been justified. Furthermore, the quality of the empirical evaluation can be improved.\n* The JKO scheme has been explored before but the specific framework proposed in this paper is new.\n* The authors have released their code which aids reproducibility. That said, certain experiment setup and metric choices can be better justified. ",
            "summary_of_the_review": "The paper presents a technically interesting idea of training continuous normalizing flows using the JKO scheme. The proposed method results in improved computation efficiency during training. However, this work has issues with the presentation, justification of claims, and quality of empirical evaluation. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_y2DU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_y2DU"
        ]
    },
    {
        "id": "qBcZQlBIkDX",
        "original": null,
        "number": 3,
        "cdate": 1667198976963,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667198976963,
        "tmdate": 1667198976963,
        "tddate": null,
        "forum": "-z7O7fk_Cs",
        "replyto": "-z7O7fk_Cs",
        "invitation": "ICLR.cc/2023/Conference/Paper3140/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes using Wasserstein gradient flow of KL dviergence  to construct the flow model. ",
            "strength_and_weaknesses": "First of all, the paper has a bad definition of notations. For example, the definition of V (is the potential of the equilibrium density) appears in the preliminaries. I think the paper should organize a problem-setting section to make all things clear.\n\nHowever, in objective (8) there is a V (is the potential of the equilibrium density) included. How to get the V for general distribution? V includes fitting a score, which is very similar to yang's ICLR paper. \n\nThe main concern is the realtionship to paper [1,2]. [1] consider the gradient flow of f-divergence and gives out a flow with velocity field f''*nabla (p/p_t). Thrid, how this paper differs from [3]. The only difference is the objective for training but not the gradient flow aiming to approximate. ( If the author claim their objective is better, I need to see evidence)\n[1] https://arxiv.org/abs/2012.06094\n[3] Johnson R, Zhang T. A framework of composite functional gradient methods for generative adversarial models[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 43(1): 17-32.\n\nIn terms of the literature review, this paper only discusses how they are different in terms of motivation (or how to derive the model), but have no discussion of how the derived model are different from each other. \n\nMissing experiments on CelebA. Missing baseline using diffusion model.\n\nMissing reference:\n[1] Liutkus A, Simsekli U, Majewski S, et al. Sliced-Wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions[C]//International Conference on Machine Learning. PMLR, 2019: 4104-4113.\n[2] Zhang L, Wang L. Monge-amp\\ere flow for generative modeling[J]. arXiv preprint arXiv:1809.10188, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper did a bad job in literature review and notation. ",
            "summary_of_the_review": "See above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_tb5k"
        ]
    },
    {
        "id": "Q8n6PPMqYS",
        "original": null,
        "number": 4,
        "cdate": 1667238346710,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667238346710,
        "tmdate": 1667238346710,
        "tddate": null,
        "forum": "-z7O7fk_Cs",
        "replyto": "-z7O7fk_Cs",
        "invitation": "ICLR.cc/2023/Conference/Paper3140/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This current paper proposes a normalizing flow algorithm that implements the JKO scheme using neural ODE flow blocks. At each time interval, an optimization over a vector field parameterized as ResNet is done to obtain the velocity field that can be integrated to obtain samples at the next step. The normalizing flow is invertible and the objective functional of the JKO scheme is taken to be the KL divergence with respect to a standard Gaussian so that by inverting the flow we can generate the data distribution. The proposed method appears to be more efficient than the alternatives as demonstrated by the experiments.\n",
            "strength_and_weaknesses": "## Strengths:\n* Optimizing the velocity field of the ODE for JKO instead of transport maps that are more common in the existing literature is novel.\n* The idea of using one block for each JKO step is interesting and it seems to have resulted in efficiency.\n\n\n## Weaknesses:\n* I found the novelty of the proposed method somewhat limited. The only difference from [Alvarez-Meliset et al. 2021], [Mokrov et al. 2021] is that instead of parameterizing the pushforward map as ICNN (note in either work it's not necessary to use ICNN; arbitrary networks can also be used, following the same reasoning at Lemma A.1), the current work uses neural ODE. I'm not convinced by the superiority of using neural ODE over a pushforward map, which the current work does not compare against. In my opinion, a more severe problem is not addressed: namely at step $k$ all these algorithms need to push initial samples by $k$ steps to obtain samples for the current iteration, which scales overall quadratically in $k$.\n* The section 2.2 is not new to my knowledge and references are missing. See a similar derivation in Theorem 3.1 of Liu et al. \"Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\".\n* I appreciate that two heuristic enhancements are proposed in Section 3.2. However, at first glance, these two enhancements seem contradictory. On the one hand, trajectory reparameterization wants to remove blocks, but on the other hand, progressive refinement requires adding intermediate blocks for large time steps. It would be great to have some ablation studies illustrating the effectiveness of either enhancement. \n* Since the target application is generative modeling, and the overall idea follows diffusion models (namely mapping the data distribution to a standard Gaussian, and then inverting the process), I think the authors should compare with diffusion models as well. \n* The writing is a bit sloppy overall. There is almost no assumption stated for Lemma A.1 or Proposition 2.1. At the very least we probably need $p$ and $q$ to have a finite second moment along with differentiability assumptions. The proof of 2.1 does not seem novel to me. It is essentially a direct application of the instantaneous change of variable from [Chen et al. 2018] (which is by itself just rewriting the continuity equation). There is missing reference on this formula (see the equation under (17)), and moreover as written this formula is not correct. The derivative needs to be a total derivative, i.e., it should be $d/dt (\\log \\rho(x(t), t)) = -\\nabla \\cdot f(x(t), t)$. The sloppiness of the writing is also manifested in the many handwavy sentences in the main text. To give a few examples:\n    - Above (4), \"Under generic conditions\" --- what conditions?\n    - In the second paragraph of 2.1, \"The solution of (1) ... gives a one-to-one mapping\" this should only be true if the time interval is small enough, by Picard\u2013Lindel\u00f6f theorem\n    - In the last paragraph of page 5, \"... it will also have bounded Lipschitz constant\". How is this true? There is no assumption on $\\rho_t$ having bounded the Lipschitz constant.\n    - At the end of page 5, \"The analysis is postponed here\", where more details could have been given\n* For the conditional generation experiments, more details will be helpful. I don't understand what it means by \"we evaluate $V$ for a Gaussian mixture $H | Y$\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is okay, although more details and more precise language can further improve the clarity. The source code is given but I did not run it.\n",
            "summary_of_the_review": "Overall I think the paper lacks novelty and needs more comparison with other methods (other JKO methods, diffusion models) to demonstrate its effectiveness. The writing can also be improved.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_ffZu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3140/Reviewer_ffZu"
        ]
    }
]