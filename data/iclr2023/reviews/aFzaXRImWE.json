[
    {
        "id": "wz0QRgXWfw",
        "original": null,
        "number": 1,
        "cdate": 1666692347018,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666692347018,
        "tmdate": 1669447676729,
        "tddate": null,
        "forum": "aFzaXRImWE",
        "replyto": "aFzaXRImWE",
        "invitation": "ICLR.cc/2023/Conference/Paper1154/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for deep learning with noisy labels. The method is based on the independent noise transition assumption, i.e., the label noise is independent of the input data. Existing methods, either with or without anchors, attempt to find a minimum enclosing convex hull for the noise transition matrix T. However, the authors show that this approach is vulnerable to errors in posterior estimation. To resolve this issue, the authors propose to bypass this problem using a bilevel formulation; Now, we are minimizing a robust loss function that can give the same parameter (theta) for both noisy labels and clean labels, but the parameter itself is defined based on another forward-correction-based optimization problem, where T is the variable. The authors claim that this can also alleviate the difficulty of minimizing the robust loss function directly with theta. The experiments show that the proposed method achieves state-of-the-art results for various benchmark sets.",
            "strength_and_weaknesses": "The main motivation of the paper is valid, i.e., MGEO methods are vulnerable to errors in posterior estimation. The authors provide this with a proposition, and the main reason for this vulnerability is the strict inequality constraint for the \"enclosing convex hull\" part. The idea for resolving this issue is novel, i.e., combining robust loss functions with the estimation of T in a bilevel formulation. The upper-level problem finds theta (the parameter) based on the robust loss function while the lower-level problem defines theta based on T with a forward-correction-based problem. The authors claim that this kills two birds with one stone in that (i) the difficulty of direct optimization for theta with the robust loss can be alleviated and (ii) the vulnerability of T estimation in MGEO can be bypassed (since now T is determined by the robust loss function through theta). This is quite an interesting approach, and the good performance makes it more convincing.\n\nAlthough the approach is interesting, it seems some of the positive effects of the proposed method are not fully justified. Among the above two advantages, the benefits of (ii) are clear: Now T is not defined based on the strict convex hull inequality constraint, so it is less sensitive to errors in posterior estimation as also shown in Proposition 2. However, (i) is somewhat unclear. The authors argue that reparametrizing theta with T as in (7) will make the optimization of the robust loss function easier, but the exact mechanism for this is not explained. A clear justification is needed for this part.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It seems that some description of the forward-correction problem is somewhat misleading (in the introduction and the proof of Theorem 1). The authors repeatedly say in the paper that the forward-correction-based method gives the same theta as the clean loss, but in my understanding, this is not entirely correct. According to [Patrini et al. 2017], what is guaranteed for forward-correction problems is much weaker than that for backward-correction problems, i.e., the minimizer will be the same for both problems if f_theta is allowed to be any functions. However, in practice, f_theta is limited to a certain class, and in that case, the actual solutions of theta can be different. Please correct me if I'm wrong.\n\nThe actual optimization procedure for the bilevel problem is not described in the paper. Is the complete computation graph for the lower-level problem constructed and used for the upper-level problem? Or, do the authors simply use an alternative optimization? Although the authors provide the code, some basic explanations regarding this can be discussed in the paper since this is related to the \"heaviness\" of the training procedure. I have only briefly gone through the code, and the former seems to be the case. How heavy is the computational burden in this regard?\n\nRegarding reproducibility, the authors have provided the code.",
            "summary_of_the_review": "The main idea for bypassing two difficulties (one for robust loss functions, another for vulnerability of the MGEO problem) at once is novel and interesting. However, the justification that the proposed method can alleviate the former difficulty is not clearly given in the paper. The performance is good, which makes the method convincing.\n\n[After rebuttal] I'm satisfied with the authors' answers and maintain the original score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_FXEG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_FXEG"
        ]
    },
    {
        "id": "7lJ3odBCtk5",
        "original": null,
        "number": 2,
        "cdate": 1667071978680,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667071978680,
        "tmdate": 1667071978680,
        "tddate": null,
        "forum": "aFzaXRImWE",
        "replyto": "aFzaXRImWE",
        "invitation": "ICLR.cc/2023/Conference/Paper1154/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of estimating instance independent noise transition matrix ($T$), which is utilized in learning with noisy labels. They unify existing methods for $T$ estimation, under a framework called Minimum Geometric Envelope Operator and show that these methods are prone to failing ( they are in-consistent estimators) when the noise posterior estimation is imperfect. To address this issue, they propose a novel T-estimation framework (ROBOT) based bilevel optimization and show that their proposal has nice theoretical properties like identifiably, consistency and finite-sample generalization guarantees. They also argue that unlike the MEGO methods their method doesn't require assumptions on the existence of anchor points. Empirical evaluation on multiple benchmarks shows the superiority of the proposed method. ",
            "strength_and_weaknesses": "Strengths:\nThe paper addresses an important problem of learning with noisy labels. Though I am not super familiar with the literature on noise transition matrix estimation, the characterization of existing works under a simple framework (MGEO) seems nice. Then, they highlight the problems with existing works -- anchor point assumptions and in-consistency when noise posterior estimation is imperfect. In addition the paper also gives a new solution to this problem and it is shown that the proposed solution has nice properties like identifiability, consistence and generalization. Empirical evaluation on multiple datasets shows significant improvement over the other baselines. \n\nWeaknesses/Questions:\nCould you please provide some background/preliminaries on the methods that are being put under MGEO? For the readers who are not super familiar with this line of work. I could see a formal statement on the in-consistency of MGEO methods, it is discussed using an example in section 4. Is it possible to have an exact statement on the in-consistency -- even restating previous results could be helpful. In Theorem 2 why are the results in terms of the covering number of $\\mathcal{F}$? Can't the standard generalization error bounds be instantiated here? What is $\\epsilon$ in Theorem 2? Is there a bound on the estimation error of $T$  and do you need any assumptions on the noise level (original $T$) for the method to work? I believe if the noise level is too high then the estimation might be hard, is it true?\n ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Mostly clear.  \nQuality: Good. \nReproducibility: Code and proofs are there, but not fully sure of reproducibility. ",
            "summary_of_the_review": "Overall a good paper. It provides an abstraction and highlights issues with existing methods for estimation of noise transition matrix and also proposes a solution to fix those issues. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_Ukut"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_Ukut"
        ]
    },
    {
        "id": "NrozIbi8SOr",
        "original": null,
        "number": 3,
        "cdate": 1667148251871,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667148251871,
        "tmdate": 1667148251871,
        "tddate": null,
        "forum": "aFzaXRImWE",
        "replyto": "aFzaXRImWE",
        "invitation": "ICLR.cc/2023/Conference/Paper1154/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The contributions of this paper are two-fold: firstly, a unifying view of the noise transition matrix is presented which combines both anchor-based and anchor-free methods into a single framework called MEGO which is based on the minimum-volume convex hull. The authors then propose an improved bi-level optimization method called ROBOT which improves the performance of the method in the presence of imperfect posterior estimation. The results show that the method outperforms the baselines by some margin.\n\n",
            "strength_and_weaknesses": "Strengths\n\n1. Handling label noise in deep learning is a significant and practically important problem as it is well-known that obtaining good posteriors from deep learning models is a challenge.  The propsed method is therefore a welcome contribution in this direction.\n\n2. The unified view of anchor-based and anchor-free, MEGO, methods provides some interesting insights into the existing approaches for handling noisy labels and is a good way of summarizing these approaches.\n\n3. The proposed method for overcoming the limitations of MEGO has good theoretical guarantees which are described in great detail in the paper. \n\n4. The empirical results show that the method consistently outperforms existing approaches across a range of datasets. \n\nWeaknesses\n\n1. While the proposed method is quite simple, the paper itself is a bit difficult to read and follow. For example, there is no summary of the approach and some implementation details aren\u2019t mentioned. How is $\\mathcal{L}_{rob}$ computed?\n\n2. The evaluation is on rather small datasets that don\u2019t really have much practical significance in the era of large models. How does the method scale in terms of dataset size and how computationally expensive is the method? ",
            "clarity,_quality,_novelty_and_reproducibility": "While the grammar and expressions are clear, the exposition of the method is a bit convoluted. The paper could be significantly strengthened by an overview of the proposed approach and an explanation of how each term is implemented. As far as I can tell, the proposed approach is novel.",
            "summary_of_the_review": "Overall, this paper makes a good contribution both in terms of providing a unified view of existing noise transition matrix estimation approaches and proposing a new effective method for dealing with noisy posterior estimation. The only major weakness is there is no evaluation of the significance of the approach on real-world large scale datasets.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_nwAC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_nwAC"
        ]
    },
    {
        "id": "b19oX_iqoRN",
        "original": null,
        "number": 4,
        "cdate": 1667202153086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667202153086,
        "tmdate": 1667202153086,
        "tddate": null,
        "forum": "aFzaXRImWE",
        "replyto": "aFzaXRImWE",
        "invitation": "ICLR.cc/2023/Conference/Paper1154/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "To construct the noise probability matrix, existing methods assume that there exists anchor points which belong to a certain class probability one or assume a perfect posterior estimator for the noisy examples. A new transition matrix estimation is proposed that utilizes an alternate optimisation routine of the neural network and the transition matrix parameters. ",
            "strength_and_weaknesses": "Strength\n\n* The paper is backed by enough theoretical evidence. \n* Results are strong. \n\nWeakness \n\n* In the last sentence in page-5, text(T) was used with out defining the term. \n* If I understood the eq 7 correctly, one needs to optimise the objective and the constraint in an iterative manner.  The neural network parameters are optimised using the noisy training set and the current estimation of the transition matrix. The transition matrix is then updated using the validation set. It is not clear why one would need to divide the set into training and validation and why can not we use the same noisy annotation to estimate both iteratively? \n* The theorem 2 is valid if eq-7 is well solved. I could not find any convergence guarantees of eq-7. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "* The clarity of the paper could be improved \n* The method is certainly novel and easy to reproduce ",
            "summary_of_the_review": "The paper shows enough theoretical evidence of the method. However, some parts will not be easy to be accessible by non-expert readers. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_5YUk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_5YUk"
        ]
    },
    {
        "id": "HMc8Yd3MYb",
        "original": null,
        "number": 5,
        "cdate": 1667347865429,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667347865429,
        "tmdate": 1667347865429,
        "tddate": null,
        "forum": "aFzaXRImWE",
        "replyto": "aFzaXRImWE",
        "invitation": "ICLR.cc/2023/Conference/Paper1154/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "In spite of my efforts I am unable to provide an acceptable review of this paper.",
            "strength_and_weaknesses": "In spite of my efforts I am unable to provide an acceptable review of this paper.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In spite of my efforts I am unable to provide an acceptable review of this paper.\n",
            "summary_of_the_review": "In spite of my efforts I am unable to provide an acceptable review of this paper.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_NebR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1154/Reviewer_NebR"
        ]
    }
]