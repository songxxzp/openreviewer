[
    {
        "id": "I6Wo84pOnB",
        "original": null,
        "number": 1,
        "cdate": 1666479434457,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666479434457,
        "tmdate": 1667905598916,
        "tddate": null,
        "forum": "TMYzh1hsHd",
        "replyto": "TMYzh1hsHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a modification to IQL called MA2QL in which each agent alternately updates the parameters rather than the simultaneous update. The main contribution of this paper is the theoretical analysis of this modification. In experiments, MA2QL is demonstrated to outperform IQL in several environments.",
            "strength_and_weaknesses": "## Strength\n1. The motivation of this paper is clear and the discussion of the related works are sufficient.\n2. The theoretical analysis of this paper is easy to follow and correct.\n3. The experiment design is comparatively good which covers variants of scenarios.\n4. The proposed algorithm is neat.\n\n## Weaknesses\n1. Simultaneous move (update) and iterative move (update) are two typical paradigms existing in both game theory and multi-agent learning. Iterative move (update), e.g. the original version of Brown's fictitious play, has been proved to converge to Nash equilirbium in potential game. The team reward game considered in this paper is a special case of the potential game. Even equipped with Markov dynamics, there is also some existing work to show the same idea as the paper proposes [1]. In addition, in the recent paper [2] the alternate update (or rollout in sequence) has also been discussed. Therefore, the theoretical contribution of this paper is minor in my view. From the multi-agent learning side, there is a past paper [3] that also shares the similar idea.\n2. As for the practical aspect, the proposed MA2QL will increase the times of the interaction with the environment to n when there are n agents. It will lead to extra cost as the number of agents increases. It is meaningless to practice in my view, unless it can exceed the state-of-the-art methods largely.\n\n\n## Reference\n[1] Hu, Ruimeng. \"Deep fictitious play for stochastic differential games.\" arXiv preprint arXiv:1903.09376 (2019).\n\n[2] Bertsekas, Dimitri. \"Multiagent reinforcement learning: Rollout and policy iteration.\" IEEE/CAA Journal of Automatica Sinica 8.2 (2021): 249-272.\n\n[3] Lauer, Martin, and Martin Riedmiller. \"An algorithm for distributed reinforcement learning in cooperative multi-agent systems.\" In Proceedings of the Seventeenth International Conference on Machine Learning. 2000.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper is good, both theoretical analysis and experimental analysis are clear.\n\nAs for the originality, although this idea should be independently raised from the authors, the similar ones have appeared so many times in the past. To me, the theoretical result shown in this paper is unsurprising and old.\n\nThe reproducibility is good, since the authors provide the experimental settings.\n\nOverall, the quality of this paper is good from the presentation, but not good enough from the contents.",
            "summary_of_the_review": "This paper proposes a simple modification of IQL and give a complete theoretical analysis to show the convergence property. However, the similar theoretical results have appeared in the past several times, so the novelty of this paper faces a challenge. From the practical side, the proposed alternative udpate will lead to n times interactions with the environment which is a huge cost as the number of agents increases.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_xmYF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_xmYF"
        ]
    },
    {
        "id": "od3_hhgUOPO",
        "original": null,
        "number": 2,
        "cdate": 1666542139496,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666542139496,
        "tmdate": 1666609858649,
        "tddate": null,
        "forum": "TMYzh1hsHd",
        "replyto": "TMYzh1hsHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes multi-agent alternate Q-learning (MA2QL) to tackle the non-stationarity problem in decentralized MARL. The main difference between MA2QL and existing method IQL is that MA2QL agents take turns to update their Q-functions while IQL agents simultaneously update their Q-functions. The authors prove that, in MA2QL, when each agent guarantees $\\epsilon$-convergence at each turn, their joint policy converges to a Nash equilibrium. They empirically show that MA2QL outperforms IQL in a variety of cooperative multi-agent tasks with discrete and continuous action spaces. ",
            "strength_and_weaknesses": "- Strengths:\n\t- The main approach proposed is theoretically grounded, with some good experimental results.\n\t- The paper is well-written and easy to follow overall.\n- Weaknesses:\n\t- The specific \"fully decentralized\" multi-agent setting considered (where parameter sharing is not allowed) in this work is neither well motivated nor clearly explained, which is critical for justifying the significance of this work.\n\t- The main idea of  the proposed algorithm (i.e., \"fixing the policies of other agents while one agent is learning\") is not new. For instance, it has been used in existing works in competitive MARL [1].\n\t- The experiments are not sufficiently comprehensive to me (e.g., MA2QL is only compared against IQL).",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity overall, although the definition of the specific problem setting considered is not clear to me (and this is one major problem). Regarding novelty, I think the main idea of \"fixing the policies of other agents while one agent is learning\" in MA2QL is not new. Very similar idea has been used in previous works on competitive MARL [1].",
            "summary_of_the_review": "My biggest concern of this paper is the specific \"fully decentralized\" multi-agent setting considered (where parameter sharing is not allowed) is not well motivated and clearly explained. It is not clear to me what the practical use cases for this setting are. I can think of some real-world scenarios where there are separate machines/robots who are not able to/not allowed to share parameters with each other. But in MA2QL, you seem to be implicitly assuming some kind of synchronization mechanism that enables the agents to reach an agreement on who should update their policy/Q-function at each learning stage. In that sense, is it really \"fully decentralized\" based on your definition?\n\nMy another main concern is the idea introduced in MA2QL is not really new as I've mentioned above. In addition, for the experiments, while I appreciate the evaluation of MA2QL on a variety of cooperative multi-agent tasks with discrete and continuous action spaces (and full/partial observability), I think comparing only against IQL is not enough. What about other independent learning baselines such as independent actor-critic, independent DDPG, and independent PPO? For instance, does MA2QL outperform independent PPO in SMAC? \n\nHere are some other comments/questions:\n- \"we do not use parameter-sharing, which should not be allowed in decentralized settings (Terry et al., 2020).\" I do not understand why parameter sharing \"should not be allowed in decentralized settings.\" \n- In Multi-Agent MuJoCo, using names MA2QL and IQL for the algorithms while DDPG is actually used are misleading. \n- In MA2QL, how do you decide the order in which each agent updates their Q-functions? Is it a random order? Do agents need to fully agree on who should update their Q-functions next?\n- For the hyperparameter K used in MA2QL, it is set to be 30 in MPE but 8000 in Multi-Agent MuJoCo. The range of values looks quite large. How do you efficiently tune this hyperparameter? \n\n[1] Lanctot, Marc, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. In Advances in Neural Information Processing Systems, 2017.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_p9BU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_p9BU"
        ]
    },
    {
        "id": "KGbpL8RCKg",
        "original": null,
        "number": 3,
        "cdate": 1666635835205,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635835205,
        "tmdate": 1668725706862,
        "tddate": null,
        "forum": "TMYzh1hsHd",
        "replyto": "TMYzh1hsHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a simple modification to Independent Q-Learning to make it better address the non-stationarity issue in credit assignment. The modification involves allowing agents to take turns in updating their individual Q-tables or Q-functions. This is as opposed to synchronous updates in IQL implementations. This paper motivates this approach through theory (in the tabular value iteration and Q-learning) and experiments (across several standard benchmarks).    ",
            "strength_and_weaknesses": "**Strengths:**\n\n- The idea is simple and elegant.\n\n- The idea is more realistic than standard IQL, in which updates have to be synchronized across agents. Their approach MA2QL, on the other hand, does not require synchrony and in fact is closer to what would happen in a practical physical multiagent system, where agents will receive updates at semi-fixed intervals of time without a synchronized starting time.\n\n- Improving Independent Learning is much harder than CTDE, and advances in this direction could prove more useful in real-world learning scenarios where no good simulator is available or communication during training is strictly limited.   \n\n\n**Weaknesses:**\n\n- While the paper is generally well-written, I struggled to obtain answers to many questions: partly due to the way things are explained/described, and partly due to missing analysis/experiments. Please refer to my questions in the next section to help clarify some of these concerns.  \n\n- Novelty might be limited with some related work in the context of MARL focusing on the same concept (see Appendix E for some such connections discussed by the authors, but not in great detail). I hope the authors can discuss this more.",
            "clarity,_quality,_novelty_and_reproducibility": "**Questions:**\n\n- Regarding this statement: \"*One may notice that the performance of MA2QL is better than MA2QL-DP. This may be attributed to sampling and exploration of MA2QL, which induces a better Nash equilibrium.*\"\n\n   1. How did you perform DP updates? Full sweeps over the entire set of states in every update? Or were the backup states sampled according to, e.g., the buffered transitions?\n\n   2. I cannot fully understand what you exactly mean in the statement above. Could you elaborate?\n\n- \"*To investigate continuous action space in both partially and fully observable environments, we configure 2-agent HalfCheetah and 3-agent Hopper as fully observable, and 3-agent Walker2d as partially observable.*\"\n\n   - Could you please add results for the fully-observable case with a higher number of agents in this benchmark?\n\n- \"*Here, we show that MA2QL can still outperform IQL in three maps with various difficulties, which indicates that MA2QL can\nalso tackle the non-stationarity problem and bring performance gain in more complex tasks.*\"\n\n   - Is there additional non-stationarity in SMAC tasks beyond the non-stationarity that exists in general for IQL?\n\n- Do any of these benchmarks have stochastic transition dynamics and stochastic reward functions? A big problem with non-stationarity in MARL is when there is stochasticity in the environment, whereby the challenge is partly that the agents cannot distinguish between the stochasticity in the environment from that induced due to changing policies of other cooperative teammates. I believe some experiments with the stochastic version of these environments, or at least a subset of the tasks are required. Also, it would even be better if one experiment compares IQL vs MA2QL with varying stochasticity levels in the environment.\n\n- What is the exploration strategy schedule? Are there any experiments with different exploration schemes? For e.g. what if we use IQL against MA2QL on a task with linearly-decayed epsilon of varying decay rates and with varying fixed-epsilon/final-epsilon levels? If MA2QL shows more robustness across varying exploration schemes, then it would highly support the main arguments of the paper.\n\n- Could you also test IQL vs. MA2QL on a tabular dilemma problem concerning \"relative overgeneralization (or shadowed equilibria)\"? E.g., consider the following 2-agent tabular dilemma task with the reward function:  \n```\n 11  |  -30   |  0\n-30  |   7    |  6\n 0   |   0    |  7\n```\nI'm curious what would be the learned Q-tables for the agents and the implied policy over the joint action space for MA2QL using $\\epsilon$-greedy exploration with epsilon=1.0.\n\n- Is there any intuition/results that could give an idea of whether a similar scheme could also help improve VDN (or Value Decomposition Networks)? In the appendix, there is some argument that MAPPO (CTDE) does not benefit from Alternate Updates (as in MA2PPO). I'm not quite sure why this is, and I also would've really preferred seeing this in the context of VDN. \n\n- What is \"the CTDE algorithm MAPPO\"? I cannot easily find any summary of this agent (I don't know what MAPPO does to be CTDE beyond IPPO).\n\n- \"*The reason is that their experiences are generated in different manners.*\"\n\nThis is confusing to me: are the experiences generated with only one agent potentially exploring and others exploiting, or do you mean something else here?\n\n- \"*MA2QL obtains experiences with only one agent learning, so the variation for the joint policy is smaller than that of IQL. Thus, in\ngeneral, the divergence between $\u03c0$ and $\u03c0_D$ is smaller for MA2QL, which is beneficial to the learning.*\"\n\nDoesn't the same argument benefit training of VDN? Why?\n\n- Regarding Appendix E (Additional Related Works): Could you add more info about what these papers are about, and what do they explore, especially those in the context of MARL?",
            "summary_of_the_review": "I generally like the paper, but I have several concerns that I hope the authors can address in their rebuttal. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_SmxM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_SmxM"
        ]
    },
    {
        "id": "SYJrumAztwO",
        "original": null,
        "number": 4,
        "cdate": 1666664319957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664319957,
        "tmdate": 1666699710444,
        "tddate": null,
        "forum": "TMYzh1hsHd",
        "replyto": "TMYzh1hsHd",
        "invitation": "ICLR.cc/2023/Conference/Paper3882/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors consider a fully decentralized cooperative multi-agent reinforcement learning (MARL). To alleviate the non-stationarity in decentralized MARL, this paper proposes to learn agents by letting each agent take turns updating each action-value function. Based on this simple idea, multi-agent alternate Q-learning (MA2QL) is proposed for practical implementation, and a theoretical explanation is described. Numerical results show that MA2QL can perform better than independent Q-learning in the considered cooperative MARL environments under fully decentralized settings.",
            "strength_and_weaknesses": "While this paper is well-organized, easy to follow, and theoretically supported, there is a fundamental debate on alternate learning concepts which should be clearly explained. In addition, there is a gap between theory and practice which is not properly dealt with. Experiments should be expanded for further analysis, and finally, reproducibility cannot be verified.\n\nQ1. Fundamental discussions on alternate learning \n\nThe core concept of this paper is alternate learning. There are substantial issues regarding the alternate learning concept.\n- Imagine we consider an extremely large distributed system such as decentralized traffic control. Training each agent alternatingly requires an extreme amount of time and each agent may not adequately take action at each moment.\n- If $n$ is extremely big, each agent is not fully trained because the period is long. In fact, in the wall-clock time sense, IQL can conduct more gradient steps within the same wall-clock time to improve performance. In this sense, \u201ca fair comparison\u201d in this paper may be questionable.\n- This paper does not explicitly describe the effect of agent order. Can the order be random at each round, or the order should be predefined? Can the authors show any numerical study showing whether order matters or not? If $n$ is extremely large, there can be one good agent and the other bad agents, which can cause asymmetric bias. Then it can be sensitive to the order.\n- Though controversial, defining agent order may not correspond to fully decentralized learning (unless every agent agrees to predefined order \u201cprotocol\u201d before starting training) since each agent should know whether a certain turn is \u201cmy turn\u201d or \u201cother\u2019s turn.\u201d \n- Can MA2QL apply to non-cooperative MARL settings? (i.e., Markov Game)\n\nQ2. Unclear description of \u2018fair comparison\u2019 between MA2QL and IQL\n\n- In Multi-agent Mujoco, $K=8000$. In 2-agent HalfCheetah, for example, for $2K=16000$ environment steps, each agent is trained with MA2QL. Then how IQL is fairly trained? Are the two agents simultaneously trained every 2 environment steps during $2K=16000$ environment steps?\n- I am also confused with MPE and SMAC. At agent $i$'s turn, each agent $j$ stores the $m$ transition samples ($m$ environment steps), and agent $i$ conducts $K$ training steps. In total, each of $n$ agents conducts $K$ training steps for $mn$ environment steps. \nThen how IQL is fairly trained? I think IQL agents simultaneously update Q-function one step (i.e., one gradient step) every $\\frac{mn}{K}$ environmental step so that during $mn$ environment steps, each of $n$ agents conducts $K$ training steps.\nIf my understanding is correct, what is the value of $m$ in MPE and SMAC? What is the effect of the value of $m$? Is IQL properly tuned in this setting?\n - Clarification with pictorial examples can help to understand.\n\n\nQ3. Regarding sampling issues in learning Q-network \n\nAs stated on page 5, when learning Q-network, MA2QL permits containing samples from past transitions in a single replay memory. The authors did not provide any remedy reducing the gap between theoretical on-policy learning and single replay memory usage. It would be better to try any remedy to reduce the logical gap.\n\nQ4. Multiple Nash equilibria\n\nThe main claim relies on the assumption that there is a unique Nash equilibrium, and the authors described a short remedy dealing with multiple Nash equilibria on page 4. Can we verify the remedy experimentally? For example, we may construct a matrix game with multiple Nash equilibria to check whether the proposed method can be extended in practical cases. \n\n\nQ5. Comparison with hysteretic Q-learning\n\nThe authors need to compare MA2QL with hysteretic Q-learning since hysteretic Q-learning is pre-existing baseline in fully decentralized learning. The considered environments by **Zhang et al. (2020) are different from those in this paper. \n** Zhang et al., Bi-Level Actor-Critic for Multi-Agent Coordination, AAAI 20.\n\n\nQ6. Environments\n- Can MA2QL be effective with Multi-agent Mujoco when agent_obsk=0? \n- Please explicitly write whether each environment is partially observable or not in each title in Figure 4 for readers.\n- In SMAC, is there any case where IQL fails but only MA2QL succeeds?\n\nQ7. Reproducibility\n\nThe authors did not provide their implementation codes for verification.\n\n\nNotation: $\\gamma$ is missing in the third paragraph of Section 5.2.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally easy to read and well-structured. However, more discussions are required to support the effectiveness of alternate learning. Other than that, naively using alternate learning may suppress the novelty. In addition, several experiments should be performed for more clarification. Finally, reproducibility should be improved.",
            "summary_of_the_review": "It is crucial to study new learning methods in fully decentralized multi-agent reinforcement learning. However, the technical novelty of the newly developed algorithm remains questionable, and there are many unclear aspects of the core idea of alternate learning. Discussions and experiments should be expanded. Thus, a major revision is necessary both theoretically and experimentally.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_J8oo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3882/Reviewer_J8oo"
        ]
    }
]