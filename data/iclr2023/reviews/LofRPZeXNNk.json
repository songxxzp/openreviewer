[
    {
        "id": "9NWqXkRbRq",
        "original": null,
        "number": 1,
        "cdate": 1666307651639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666307651639,
        "tmdate": 1666307651639,
        "tddate": null,
        "forum": "LofRPZeXNNk",
        "replyto": "LofRPZeXNNk",
        "invitation": "ICLR.cc/2023/Conference/Paper4201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Adversarial attack methods on text classifiers fail to preserve semantics of the sentence in some cases, because existing unsupervised sentence encoders fall short in distinguishing synonyms against antonyms. This paper proposes Semantics Preserving Encoder (SPE) which takes the average of multiple sentence embeddings extracted from different text classifiers to address this issue. Experiment results show the proposed method can improve the rASR metric. \n",
            "strength_and_weaknesses": "**Strength:**\n\n- This paper tackles a critical challenge in textual adversarial attack -- to preserve the semantic meaning of a sentence.\n- The paper is mostly clear and easy to follow. \n\n\n**Weaknesses:**\n\nSeveral technical decisions are not justified.\n- Why use average of embeddings? \n- Why use 7 datasets? Does some datasets more helpful then other datasets? For example, what if we remove the semantic classifiers from SPE when attacking Yelp dataset?\n- Why use 2.5 as the threshold for rASR? The sentences are just ``slightly similar'' if it is annotated as 3.\n\nThere are several issues with the experiments\n- Why not verify the method on standard sentence similarity benchmarks such as SemEval? SPE can be considered as a sentence similarity metric. However, this paper neither verifies its performance on standard sentence similarity benchmarks nor justifies that this metric is only suitable for adversarial attack situations.\n- Why use distilBERT on Rotten tomato dataset, while using RoBERTa on other datasets?\n- There are many other adversarial attack methods, such as BERT-Attack, CLARE, SememePSO, etc. Does SPE also improves the quality for these methods?\n- More insights is needed to demonstrate the improvement. For example, certain type of changes are eliminated by applying SPE. \n\nOther questions:\n- I'm not sure if Eq. (1) is correct. Why does the complexity depend on the vocabulary size rather then the length of the sentence?\n- What do you mean \"do not preserve the semantics and even the meaning of the text\"?\n- What do you mean \u201celiminate the problem of vocabulary words\u201d?\n\nMore philosophical question: Is SPE trying to improve the semantic similarity or to improve prediction consistency? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow. \n",
            "summary_of_the_review": "This paper proposes SPE to improve the quality of adversarial sentences. The idea is original, but many technical decisions of the method are unjustified, and the experiments need to be significantly improved. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_6oWy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_6oWy"
        ]
    },
    {
        "id": "4avuvrFQ5Wc",
        "original": null,
        "number": 2,
        "cdate": 1666520166897,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666520166897,
        "tmdate": 1670835926830,
        "tddate": null,
        "forum": "LofRPZeXNNk",
        "replyto": "LofRPZeXNNk",
        "invitation": "ICLR.cc/2023/Conference/Paper4201/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a simple yet effective sentence-encoder for the semantic preserving test in the field of word-substitution-based adversarial sample generation.\nGenerally, the idea is simple since it uses the annotated dataset to train a simple sentence encoder to judge whether the semantics is significantly changed by the attack algorithms.\nCompared with previous methods such as USE, the proposed method SPE is simple and efficient.\nWith a human-involved experiment and a proper metric that measures the real-attack success rate, the proposed method is proven effective to serve as a semantic checking tool in the attack generation process.\n",
            "strength_and_weaknesses": "Strength:\nA.\t\nthe method is very simple and straightforward, aiming to tackle a key problem in substitution-based adversarial sample generation.\nThe semantic-preserving is indeed an important problem that previous attack methods such as GA, Textfooler, BERT-Attack, PSO-attacks may have trouble solving. \nThe proposed SPE uses labeled datasets to train an ensemble of classifiers and use all these classification results as the semantic score checking.\n\nB. \nThe setup of the experiments is reasonable, the human-involved experiment and the real ASR metric are convincing.\n\nWeakness:\n\nA.\t\nMy first concern is though the experiments are properly designed, the comparison might not be fair.\nThat is, as explored thoroughly in prior works such as Textfooler and BERT-Attack, the transferability of the adversarial samples is somewhat weak, that is, the adversarial samples are not universal to different classifiers.\nWhile the proposed SPE is in fact trained with datasets including NLI, cola, rte, sst-2, yelp, etc.,  these datasets can be transferable, therefore, the SPE score can be viewed as another classifier in a way.\nI am concerned that the SPE similarity is in fact a reflection of classification, not as illustrated as \u2018semantic-preserving\u2019.\nIf it is the case, then the advantage of low cost in the proposed scorer is less convincing since we can always train some simple classification models as a scorer to measure the adversarial sample quality.\n\nB. semantic preserving concept:\nAs illustrated in Table 1, the authors intend to illustrate the concept of sentence semantic similarity when the sentence is perturbed.\nThese cases, which mostly are synonyms to antonyms change, are less observed in the similarity-based word-embedding used in the Textfooler but actually constantly happen in BERT-Attack generated substitutes, which cannot be properly recognized by sentence-level semantic scorers such as USE or BERT-Score.\nTherefore, the word-level and sentence-level semantic preserving performances can be different in measuring the quality of adversarial samples yet the paper seems to mix these two concepts.\n \n\nC. The novelty of the proposed method:\nIt is true that using USE or BERT-Score in evaluating the quality of the generated adversarial samples is costly and cannot obtain promising results.\nYet the proposed method is somewhat trivial since it only uses an ensemble of classifiers and uses the cosine similarity to calculate the final similarity score calculation.\nHow can the proposed SPE be used in more general scenarios as a semantic preserving sentence similarity checker is not discussed in the paper.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the clarity is good, and the paper is easy to understand.\nQuality: the paper has a good motivation that catches a small yet vital problem in adversarial attacks in texts and proposes a very simple method as a solution, yet the evaluation is somewhat unfair which may hurt the quality evolution of the method.\nNovelty: the paper catches a problem that is not fully explored in previous works, yet the proposed method is not novel.\nReproducibility: the method can be easily produced.\n",
            "summary_of_the_review": "This paper is easy to understand and the proposed method is clearly illustrated and simple to implement. \nThe main concern is fairness in the evaluation process, the authors should provide some analysis of the transferability of attack methods and the connection to their methods.\n---------------\nI have read the authors' responses and decided to keep my score unchanged. The technical quality of the paper would benefit from a major revision.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_QUPU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_QUPU"
        ]
    },
    {
        "id": "m_7ZcyzHNf",
        "original": null,
        "number": 3,
        "cdate": 1666642139391,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642139391,
        "tmdate": 1670787072903,
        "tddate": null,
        "forum": "LofRPZeXNNk",
        "replyto": "LofRPZeXNNk",
        "invitation": "ICLR.cc/2023/Conference/Paper4201/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a sentence encoder, SPE, to improve the quality of adversarial examples in terms of minimizing semantics change. Specifically, the authors train multiple text classifiers and average the output embeddings from these classifiers to get the final representation of an input sentence. The paper also proposes an rASR metric that employs humans to evaluate how similar an adversarial example is to the vanilla sample.",
            "strength_and_weaknesses": "Strength\n\n1. The method is clearly presented.\n\n2. The code is available.\n\nWeaknesses\n\n1. I am worried about the novelty. It seems that the proposed method just takes an average of embeddings from many different encoders.\n\n2. The motivation is not clear. The paper found that previous encoders fail to deal well with antonyms, but it seems that the proposed method did nothing special about antonyms either.\n\n3. Some related word-level attacks should be discussed, e.g., [1], [2], [3].\n\n4. Some designs need more justifications. E.g., what is the criterion when choosing the 7 datasets and why the threshold is just set as 2.5.\n\n[1] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In EMNLP, 2018.\n\n[2] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial examples through probability weighted word saliency. In ACL, 2019.\n\n[3] Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural language word substitutions. In ICLR, 2021.",
            "clarity,_quality,_novelty_and_reproducibility": "The method is clearly presented. The novelty may not be enough. The code is available so I assume the reproducibility is good.",
            "summary_of_the_review": "Given the strength and weaknesses, I tend to reject.\n\n======================After rebuttal==========================\n\nThank the authors for the effort in answering my questions. After reading all the review comments and responses, I decided to keep my score unchanged.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_dmKS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4201/Reviewer_dmKS"
        ]
    }
]