[
    {
        "id": "d-nJ59CPJT",
        "original": null,
        "number": 1,
        "cdate": 1666632144856,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632144856,
        "tmdate": 1669823216024,
        "tddate": null,
        "forum": "6BO4lP8K1N1",
        "replyto": "6BO4lP8K1N1",
        "invitation": "ICLR.cc/2023/Conference/Paper316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "As neural networks get deeper, end-to-end backpropagation requires computing the gradients along longer paths, thus making gradient computations more prone to numerical instabilities and vanishing/exploding gradients. This paper proposes a new approach which divides the network into different submodules that are independently trained. Specifically, inner modules are trained using supervised contrastive learning, while the last layer is trained as usual. This enables the computation of short gradients, as well as the asynchronous training of modules, which beats the current state-of-the-art method and obtains comparatively results with end-to-end backpropagation.",
            "strength_and_weaknesses": "**Strengths:**\n- S1. The approach is clear and simple, and provides results at times better than back-propagation, which I find remarkable.\n- S2. The proposed method is a clear improvement over the competing method, beating it in every aspect as far as I can tell.\n- S3. The ablation study is nice and agrees with existing knowledge in contrastive learning.\n\n**Weaknesses:**\n- W1. The motivation is rather weak. While the asynchronous training is interesting, having long gradients does not seem as much of a problem, as it can be overcome with architectural changes (which is clearly shown by the performance difference of the existing approaches in the ResNet experiments).\n- W2. Despite contrastive learning being quite intense in computations, there are no mentions to memory and time consumption.\n- W3. Needed work is left as future work. Specifically, pipelining as well as understanding where comes the improvement in performance come from.\n- W4. Despite talking all the paper about gradients, there is not a single experiment exploring the gradients and showing that there was actually a problem in the chosen experimental setups.\n\n**Comments:**\n- C1. While parallelism is trivialized in the proposed approach, pipelining is also possible in traditional methods, as demonstrated by companies such as [OpenAI](https://openai.com/blog/techniques-for-training-large-neural-networks/).\n- C2. Doesn't the fact that Early Exit perform well in CIFAR-100 show that it is not an appropriate experimental setup?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity and quality.** While the methodology is clear, the writing would benefit from a bit of polishing. Parts like the abstract, or short phrases extremely similar to the table captions (e.g. Table 5 and below) could be improved.\n\n**Novelty.** Here lies my biggest concern: the proposed method is extremely similar (to my eyes) to Greedy InfoMax. If I understood correctly, the only difference between both approaches is that the proposed method uses _supervised_ contrastive learning, while Greedy InfoMax uses _unsupervised_ contrastive learning. The authors should better clarify what makes this work novel.\n\n**Reproducibility.** There is no code provided until paper acceptance, neither there is an appendix. Therefore, the paper lacks all the experiemtal details to perfectly reproduce the reported results.",
            "summary_of_the_review": "While the proposed approach is novel in the supervised setting, I have serious concerns of its novelty with respect to Greedy InfoMax. While this would not directly imply a rejection, the deferred extra work and the lack of proper experiments to fully understand the benefits and hurdles of the proposed approach in the specific setting it is used for, makes me lean towards rejection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper316/Reviewer_UdqE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper316/Reviewer_UdqE"
        ]
    },
    {
        "id": "0F4O2ivSh0",
        "original": null,
        "number": 2,
        "cdate": 1666645387569,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645387569,
        "tmdate": 1669698449768,
        "tddate": null,
        "forum": "6BO4lP8K1N1",
        "replyto": "6BO4lP8K1N1",
        "invitation": "ICLR.cc/2023/Conference/Paper316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "UPDATE: FOLLOWING THE AUTHOR'S RESPONSE, I CHANGE MY SCORE FROM 5 to 6. \n\nThe authors describe a method to reduce the problems associated with gradients in settings where backpropagation happens through many layers. In the context of supervised contrastive learning, the authors propose to compute contrastive losses not only at the output layer, but also at intermediate layers. This allows the authors to segment the neural network into blocks that each have their own losses, and can therefore be trained nearly independently (of course, layers closer to the output still must adapt to changes in the closer-to-input layers during training). The authors compare their method with the Associated Learning method, and find that their method performs about as well, but is easier to work with. The authors also compare with vanilla back propagation, and find that their method (as well as the associated learning baseline) outperform back propagation on a number of tasks and network layer types (but not on residual networks). ",
            "strength_and_weaknesses": "# Strengths:\n\n- The paper addresses a question that remains relevant: how do we best minimize losses in deep neural networks? How do we prevent gradients from vanishing? \n\n- The paper is very clear and well-written, and easy to read. \n\n# Weaknesses:\n\n- The authors did not include the code with their submission, and mentioned no intent to do so. There is also no appendix with experimental details such as layer sizes and learning rates. Reproducing the experiments would be practically impossible. \n\n- Besides the methods described in the submission, skip connections (e.g. in highway networks https://arxiv.org/pdf/1505.00387.pdf or residual networks) are an effective tool to reduce vanishing gradient problems. The authors should highlight this in their literature review, and ideally also perform experiments to check if adding skip connections would remove the need for the method described in this paper. It is interesting that the authors find that their method (as well as the AL benchmark method) do not outperform backpropagation when ResNets are used. This suggests that skip connections may remove the need for methods such as the one proposed in this paper, as well as the AL baseline. It would be an interesting experiment to check how e.g. VGG with added skip connections performs with and without the proposed Delog-SCL method. Skip connections have also been used in LSTMs, e.g. in https://aclanthology.org/C16-1020.pdf and https://ojs.aaai.org/index.php/AAAI/article/view/4613/4491 . \n\n- The authors write \u201cWe omitted Early Exit because this method performs much worse on more complicated datasets\u201d. In my opinion, it would be better to include the Early Exist performance numbers, and not need the explanation of why Early Exit was not included. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The main body of the paper is very clear, and of high quality. \n\n- The paper is not reproducible, as no code was included, and hyper parameters such as learning rates are not included in the paper or abstract. I recommend that the authors release the code, and additionally describe experiments in detail in an appendix. ",
            "summary_of_the_review": "UPDATE: FOLLOWING THE AUTHOR'S RESPONSE, I CHANGE MY SCORE FROM 5 to 6. \n\nFor now, I mark this submission \"5: marginally below the acceptance threshold\". \n\nThe main idea presented in the paper is interesting and useful. However, before the paper can be accepted, the authors should improve the reproducibility of the paper by releasing code and describing experiments in more detail in an appendix. The authors should also investigate whether skip connections could achieve the same that their method does. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper316/Reviewer_Xd2U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper316/Reviewer_Xd2U"
        ]
    },
    {
        "id": "7LHy-rFGmt",
        "original": null,
        "number": 3,
        "cdate": 1666658163960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666658163960,
        "tmdate": 1666662015736,
        "tddate": null,
        "forum": "6BO4lP8K1N1",
        "replyto": "6BO4lP8K1N1",
        "invitation": "ICLR.cc/2023/Conference/Paper316/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a novel method that decouples back propagation gradient computation in artificial neural networks. The authors identify issues that result from long gradient flows in back propagation, and propose a method that tackles some issues, which include vanishing/exploding gradients, and update locking. In short, the proposed method cuts long gradient flows into several local gradients with a \u2018read-out\u2019 at each layer and a corresponding contrastive loss.",
            "strength_and_weaknesses": "Strengths\n-----------\n\nDelog-SCL provides competitive results against Backpropagation and is parameter efficient in comparison to related work (AL). Naturally, as a consequence of gradient decoupling, Delog-SCL circumvents the gradient update locking problem found in Backpropagation.\n\nThe algorithm is promising, lots of insights can be teased from this method and it is a fair addition to the literature on decoupled gradient optimization in artificial neural networks.\n\nWeaknesses\n----------------\nThere is a lack of clarity on the novel contributions of the proposed method, Delog-SCL. There is no clear consensus on the prevailing issue that Delog-SCL aims to tackle, a host of issues have been presented, some of which some are unrelated, in particular the Vanishing/Exploding gradient problem. The issue of vanishing and exploding gradients is prematurely explored, for example there is little mention of Delog-SCL being extended to recurrent nets.\n\nSome of the issues highlighted are naturally tackled by contemporary backprop decoupling methods, there was some justification that Delog-SCL is more flexible and simpler than AL, but this was not raised in light of a critical issue (beyond efficient parallelization).\n\nSimilarly, the claim that Delog-SCL tackles the issue of unstable gradients in the early layers was mentioned in the introduction and conclusion with little to no explanation on the reasons behind the instability and how this relates to Delog-SCL. This was not explored in the experiments nor methodology section.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity\n------------\n\nThe paper lacks clarity on its contribution. This makes it slightly tricky to follow the manuscript.\n\nQuality\n-------------\nI find the quality of the paper to be good but with lots of room for improvement.\n\nNovelty\n------------\nI find the work to be novel.\n\nReproducibility\n-------------------\nIt appears that there has been no code that has been released yet but practically the method is simple and can probably be reproduced.",
            "summary_of_the_review": "The work is an interesting alternative to the backpropagation algorithm as well as other existing decoupled gradient computation algorithms. However, the biggest shortfall of this manuscript is the lack of clarity on its contributions. The manuscript lacks a problem statement.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "Nothing to add.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper316/Reviewer_XPBU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper316/Reviewer_XPBU"
        ]
    },
    {
        "id": "g6uTxpPnrQ",
        "original": null,
        "number": 4,
        "cdate": 1666799430651,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666799430651,
        "tmdate": 1666826038048,
        "tddate": null,
        "forum": "6BO4lP8K1N1",
        "replyto": "6BO4lP8K1N1",
        "invitation": "ICLR.cc/2023/Conference/Paper316/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the author proposed a supervised contrastive learning (SCL)-based training method for deep neural networks in which the gradients only flow locally in each layer, solving some of the problems associated with long gradient flows in backpropagation. The authors also benchmarked its performance against early exit, associated learning (AL) and backpropagation (BP) for training some standard feedforward network architectures for classification tasks. The results showed that the proposed method outperforms early exit, AL on most tasks while BP is still superior for ResNet. \n\nIn addition, the author also did some analysis on the effects of batch size on the final test performance and found larger batch size generally works better. The author also found that a nonlinear projection head performs better than identity or linear heads. \n",
            "strength_and_weaknesses": "This work has several strengths:\n\n1. The proposed method is able to cut the long gradient flow in training deep neural networks.\n\n2. The proposed method decouples the parameter updating across layers, making parallel training possible, though it needs follow-up work to validate this claim.\n\n3. The proposed method requires fewer parameter than similar method such as AL in training phase, and has the same effective parameter in inference phase. \n\n4. The test accuracy on simpler tasks and simpler architectures are better than AL and comparable to BP.\n\nWeaknesses\n\n1. One important aspect of learning procedures are the learning dynamics, which is not thoroughly studied. What are the learning curves like for the methods compared? How much longer or shorter does the proposed method converge compared to AL and BP? Is the training stable? Do we need to use different learning steps for each layer? How does the parameters change during training and use this to validate whether it actually addresses some problems with long gradient flows, such as the exploding/vanishing gradient problem for long gradient flows?\n\nThe research community would also benefit from some potential follow-up work such as the pipelining implementation and its possible extension to training other architectures such as RNNs. \n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the paper is well written. The motivations are clearly explained. The experimental design is also sound. One novelty of this paper is that it brings supervised contrastive learning to solve the problem of long gradient flow, complementing some existing approaches such work such as target propagation and AL. The source code is not provided for reproducibility.\n\nSome minor issues and typos:\n\n1. Figure 1 notations are a little bit confusing. \n\n2. Page 4, line 7 after equation (1): $]r_1^{(i)}$.\n",
            "summary_of_the_review": "The authors proposed a supervised contrastive learning inspired approach to train deep neural networks without long gradient flows. The proposed method has several characteristics such as 1) it only requires local gradient computation and parameter updates 2) it enables parallel training via pipelining 3) it requires fewer additional parameters than similar methods in training phase and same number of parameters as BP in inference phase 4) the test accuracy are comparable to BP. The authors also studied the effects of batch size, non-linear projection head. However, a thorough study and comparison of the learning dynamics are missing. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper316/Reviewer_tuDH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper316/Reviewer_tuDH"
        ]
    }
]