[
    {
        "id": "uzRTORufZR",
        "original": null,
        "number": 1,
        "cdate": 1666687510000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687510000,
        "tmdate": 1666687510000,
        "tddate": null,
        "forum": "AW0i0lOhzqJ",
        "replyto": "AW0i0lOhzqJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2973/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors  propose FOCA (First-Order Context-based Adaptation), a learning framework to model sets of systems governed by common but unknown laws that differentiate themselves by their context. FOCA  learns to represent the common law through shared parameters and relies on online optimization to compute system-specific context and its training involves a bi-level optimization problem. This paper use an exponential moving average (EMA)-based method that allows for fast training using only first-order derivatives. ",
            "strength_and_weaknesses": "Pros: \nThis paper is well-written and present a new contextual meta-learning method FOCA.  FOCA jointly learns model and context parameters by solving online bi-level optimization efficiently. Instead of relying on higher-order derivatives, it employs EMA for faster and more stable training. The authors empirically demonstrated in both static and dynamic regression tasks on complex dynamical systems that FOCA outperforms or is competitive to baselines when evaluated on both in- and out-of-distribution contexts.\nCons: \nAs the authors say in the last part,  FOCA only models systems that are identifiable from the given dataset. In the setting where the systems are sampled from nearly identical systems, FOCA may not be able to infer context properly. ",
            "clarity,_quality,_novelty_and_reproducibility": "All good in this part. ",
            "summary_of_the_review": "I would suggest to accept this paper. It certainly is above the average. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_fp24"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_fp24"
        ]
    },
    {
        "id": "gQZZb5tkqZ",
        "original": null,
        "number": 2,
        "cdate": 1667065761295,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667065761295,
        "tmdate": 1667065761295,
        "tddate": null,
        "forum": "AW0i0lOhzqJ",
        "replyto": "AW0i0lOhzqJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2973/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper investigates the problem of generalizing to new dynamical systems through an efficient first-order context-based algorithm (FOCA). The algorithm is based on a bi-level learning objective obtained through the modeling-then-identification approach. An EMA step is taken to accelerate the training process.",
            "strength_and_weaknesses": "The most confusing part of this manuscript is the *dynamical system* that is claimed to be adapting. But throughout the derivation I'm not seeing why it is even a dynamical system and what makes the adaptation of a dynamical system special in the derivation. I suspect that this is the largest weakness of the manuscript. Some detailed questions follow.\n\nQuestions:\n\n1. As suggested by the title of this paper, FOCA should be a method that focuses on the generalization of new dynamical systems. The referred related works such as GBML and CoDA are also works that are specific to dynamical systems. However, from the problem setting described in the paper, it is not very clear to me how $f$ is a dynamic system (or perhaps the relationship between $x$ and $y$). This characterization will be very important as it will set apart this problem from the general setting of Meta-learning, where the datasets are often required to have i.i.d. data. \n2. Following the last question, when $f$ represents a general dynamic system where data are not i.i.d. Solving (6) or line 9 of the algorithm with mean squared loss is perhaps non-trivial as one can no longer directly uses ERM which assumes data are i.i.d. It is indeed discussed in the CoDA paper that in general the MSE objective of (6) is intractable and can only be approximated. \n3. In section 3, it is stated that the learning objective for meta-learning is to \u201cminimize the generalization error on $D_{tr}$. This is confusing as usually the generalization error is defined with the testing dataset while $D_{tr}$ denotes the training dataset. The paper actually did not define the test dataset for the generalization problem studied, thus this left me confused. \n4. For the time-series prediction tasks, FOCA is compared against CoDA, CAVIA, and encoder. However, it seems that CAVIA and encoder were originally designed for the general meta-learning setting and thus it is reasonable to have suboptimal performance for generalization to new dynamical systems. Therefore FOCA can be evaluated more comprehensively by comparing it with other algorithms designed specifically for learning dynamical systems, such as [1] and [2]. Besides, in the CoDA paper, two versions of the algorithm are proposed (with $\\ell_1$ or $\\ell_2$ constraints), could you state which version of the CoDA is used in this work as a baseline?\n\n[1]: Yin, Y., Ayed, I., de Bezenac, E., Baskiotis, N., and Gallinari, P. LEADS: Learning dynamical systems that generalize across environments. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. Advances in Neural Information Processing Systems, 2021a\n[2]: Wang, R., Walters, R., and Yu, R. Meta-learning dynamics forecasting using task inference. CoRR, abs/2102.10271, 2021c.\n\nMiscellaneous questions:\n1. For Table 1, it is better to include references to each algorithm in the caption. Otherwise, it can be a bit confusing (e.g. encoder can be a general terminology used while here it is used to refer to a specific algorithm?) The use of absolute value ($| \\cdot |$) to donate memory usage is also confusing. To my understanding, $\\theta$, $\\psi$, and $W$ are parameters that can be a vector/matrix. Therefore it is unclear what the definition of the absolute value of a vector/matrix is. \n2. For Figure 5, could you report the standard deviation of the prediction outcome obtained with the algorithms?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the manuscript is overall in good quality.",
            "summary_of_the_review": "The manuscript is overall in good quality but I'm not aware of its relevance towards dynamical systems and its novelty thereafter.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_dXrp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_dXrp"
        ]
    },
    {
        "id": "OYMt_CVTfIB",
        "original": null,
        "number": 3,
        "cdate": 1667949466081,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667949466081,
        "tmdate": 1667949466081,
        "tddate": null,
        "forum": "AW0i0lOhzqJ",
        "replyto": "AW0i0lOhzqJ",
        "invitation": "ICLR.cc/2023/Conference/Paper2973/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper investigates a first order optimization-based framework for context-based meta-learning of static or dynamical systems. In the considered problem setup, the authors look into settings where a large class of models can be expressed as a combination of a function parametrized by shared parameters $\\theta$ and task-specific context parameters $c$. The authors propose FOCA as the framework which uses typical first order optimization methods for online optimization for training with a key ingredient being the use of asymmetric configurations of $\\theta$ to learn the shared parameters and the setting $\\bar\\theta$ used to identify the best context parameter set $c^i$ for any task $i$ during training. To maintain stability over training, the authors propose to use a slowly exponentially moving average of $\\theta$ as a proxy for $\\bar\\theta$ instead of drastically moving best values of $\\theta$. Finally, the authors test their proposed framework on a toy static regression problem and dynamical system datasets based on well-known ODE/PDE systems.",
            "strength_and_weaknesses": "Strengths:\n1. The overall idea behind FOCA is well-motivated along with its connections of using a target network in RL and other works in supervised learning. Addressing the stability of bilevel optimization problems, even outside of meta-learning is a common issue, and this work provides empirical evidence of the efficacy of a simple method.\n2. The empirical results for FOCA look quite promising on the datasets considered in the paper. However, there are related issues for comparison (see below).\n\nWeaknesses:\n1. Overall, despite the empirical focus of this paper, the discussion about weaknesses of other approaches to handle the dependence of best fit $\\hat c^i$ is rather weak. Backpropagating through the optimization trajectory in MAML style methods is an issue but various alternatives have been studied in recent years, some of which are even referenced in this paper. Did the authors investigate any such variations empirically on these datasets? Further, implicit gradients 'ideally' would require solving the inner optimization accurately but can be used as an approximate way of solving the $\\hat c$ dependency issue. However, I do agree that adding an exponentially moving average with large values of $\\tau$ in this case would benefit as well. Also, can a simpler two timescale learning rate scheme for updating the inner variable $c$ at a slower rate work? It will be worth investigating that.\n2. In the experiments, it is not clear as to what extent the context variables capture the task specific parameters vs shared parameters capturing the underlying functional form. For instance, is there any result on how the dimensionality of the $c$ affect performance when it is known that the actual cardinality of the context set if small. Similarly, changes in the overall architecture and investigating the distribution of errors across the tasks in the test set will indicate any undesired behavior.\n3. Evaluating CAVIA: How is CAVIA run during test inference? Have the authors ran a few gradient steps $K$ during test time as well? If yes, can the alternative of finding the best fit $\\hat c^i$ be tested as well? That way, we can actually decouple the effect of meta-training procedure and not induce indirect effects of the meta-test step.\n4. Architectural differences with CODA: In the experiments, the authors use $\\theta^i = \\theta + W\\hat c^i$ as the task-adapted parameter for CODA. This is starkly different from how the counterpart for FOCA is structured: $\\hat f^i(x) = f_\\theta(x,\\hat c^i)$. The main difference between CODA and FOCA can be said to be joint training of $\\theta,c$ vs interleaved training using EMA respectively. As such, I do not see the linear approximation for the model as anything inherent to CODA. I recommend the authors to have the same setup for comparison.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper often seems to lack clarity in terms of the experimental setup and discussion of results. A few comments for example:\n1. The axes in most plots are not labels or described which makes it difficult to judge the actual evaluation criteria. \n2. In the static regression example, it is not clear as to what components $\\theta$ and $c$ are capturing. Given the dimensionality of the c parameters $64$, it is not even clear if this is just a spurious overparameterization of the network as the range of linear or quadratic functions in Fig 4 doesn't seem to be diverse at all for a given $\\theta$.\n3. Distinguishing context based adaptation and a few-shot learning based meta-learning objective is important and hasn't been discussed well in the current draft. In the test phase, $\\theta$ is kept fixed and only context inference is allowed. However, can the comparison with other procedures be made when a few steps for updating $\\theta$ are also allowed?",
            "summary_of_the_review": "Overall, the paper investigates an interesting idea and shows promising empirical results. However, I feel that the paper needs improvement on the thoroughness of experiments, adequate comparison with potential baselines and a better ablation study of the key component providing the improvement (if stability is the main issue, it can be combined with other baselines as well). Further, the tests for whether the context captures task specific information is rather weak and needs further evidence.\n\nNOTE: I want to take this space to note that the review is being submitted outside the stipulated timeframe, and I apologize from my end for the delay. I hope that the authors do find my comments helpful.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_t6gz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2973/Reviewer_t6gz"
        ]
    }
]