[
    {
        "id": "KExpC49vymR",
        "original": null,
        "number": 1,
        "cdate": 1666634239496,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666634239496,
        "tmdate": 1666634239496,
        "tddate": null,
        "forum": "Dvs-a3aymPe",
        "replyto": "Dvs-a3aymPe",
        "invitation": "ICLR.cc/2023/Conference/Paper2504/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considered scaling up Graph Neural Networks (GNNs) via the input graph and model (GNNs) compression while maintaining model performance. The authors considered a dual problem of the original Graph Lottery Ticket hypothesis problem, i.e., transforming a random pair of input graphs and networks into a graph lottery ticket. The proposed framework called the Dual Graph Lottery Ticket (DGLT), utilizes the $L_2$ regularization to achieve sparsity and a hierarchical graph representation learning procedure to learn the sparse subgraph. In addition, a one-shot pre-train and finetune step are applied to prune the GNNs. The authors also provided some theoretical analyses to show that their proposed DGLT could achieve better subgraphs compared with sampling-based methods. Experiment results are provided on the proposed method\u2019s effectiveness, scalability on large graphs and ablation study, etc.",
            "strength_and_weaknesses": "Strengths:\n1.\tAs far as I know, this paper delivered the first attempt to generalize the Dual Lottery Ticket Hypothesis onto graph data and GNNs to obtain the graph lottery ticket. \n2.\tEmpirical study is clean and shows promising results on some datasets.\n3.\tThis paper is well-written with a nice presentation. Many sections are easy to follow.\n\nWeaknesses:\n1.\tAs far as I understand, the finally found dual graph lottery ticket is obtained after the training of the GNNs, i.e., the ticket contains fully trained parameters. This makes me feel the method proposed in this paper is more like a graph/model pruning method where we simultaneously train and prune a GNN until we finally find a sparse subgraph/subnetwork with non-dropped accuracy. If so, the dual perspective of the lottery ticket hypothesis does not that novel or interesting to me. I would like to see more explanations or justifications from the authors on this point.\n2.\tIf my understanding above is true, the authors are also encouraged to compare their method with standard compression-based methods.\n3.\tAs stated in the introduction, the Graph Lottery Ticket hypothesis can simultaneously simplify the input graph and prune the GNNs without compromising model performance. However, there exists an unignorable gap between the performance of the pruned GNNs with a pruning ratio above ~60% or 70% and the full-graph baselines on Citeseer and Ogbl-Collab. Is there any possible explanation for this phenomenon?\n\nMinor concerns:\n1.\tAll experiment results in the current manuscript do not have standard deviations. Authors are strongly encouraged to add them by multiple random runs since on several datasets the margin between some methods is very close. Also, single-run-based results could sometimes be tricky due to e.g., cherry-picking.\n2.\tSince this paper also tackles the compression of input graphs, I believe it is more meaningful to include \u201ctruly\u201d large-scale graphs in the experiments. I believe nowadays large-scale graphs are typically at least in the scale of the millions in terms of the number of nodes, e.g., OGB-Products and ogbl-citation2.\n3.\tMissing references to some (potentially) very related paper: Inductive Lottery Ticket Learning for Graph Neural Networks, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, this paper looks novel to me (with the first attempt to apply DLTH onto graphs and GNNs), with good clarity and a nice presentation. I found this paper interesting but not that exciting, mainly because of the way the graph lottery ticket is obtained (please refer to my first weaknesses). Some other concerns are over the experimental settings and results. ",
            "summary_of_the_review": "Overall, I found this paper interesting in exploring DLTH under the GNN setting. However, there are some concerns over the technical novelty as well as some empirical results. My final scores will be largely upon the discussion with other reviewers. I am open to hearing from the authors and willing to change my scores if I made any misunderstandings.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_yVH3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_yVH3"
        ]
    },
    {
        "id": "1jpPEw2KOHu",
        "original": null,
        "number": 2,
        "cdate": 1666710965088,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666710965088,
        "tmdate": 1669526941981,
        "tddate": null,
        "forum": "Dvs-a3aymPe",
        "replyto": "Dvs-a3aymPe",
        "invitation": "ICLR.cc/2023/Conference/Paper2504/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the Dual Graph Lottery Ticket framework to obtain the triple-win graph lottery ticket. Regularization-based network pruning and hierarchical graph sparsification are designed to jointly get the sparsified graph and subnetwork. What\u2019s more, the graph information theory guarantees the explainability of this triple-win lottery ticket.",
            "strength_and_weaknesses": "Strength:\n* It is interesting to extend the dual lottery tickets hypothesis into graph learning and jointly get well-performed sparse subgraphs and subnetworks from randomly selected ones.\n* The design of hierarchical graph sparsification makes it possible to perform graph classification tasks.\n\nWeakness:\n* The idea of the dual lottery ticket is to prove the universality of tickets in neural networks. However, the design of DGLT does not contain a \u2018random graph\u2019. The authors haven\u2019t mentioned how to predefine the subgraph structure, instead, they only constrain the number of nodes in this graph.\n* The notion of hierarchical graph sparsification may be misleading. Graph sparsification means nodes with fewer connections rather than reducing the number of nodes in one graph with the hierarchical graph strategy. Besides, how to evaluate prediction for the hierarchical graph is vague because the label for nodes in the hierarchical graph is unknown. \n* The strategy of gradually increased regularization has already been proposed in [1]. It would be better if the authors could provide more differences for this strategy between DGLT and DLT.\n* Some technical detail needs further illustration. For example, how to uniformly sample subnetworks in DGLT should be described in more information. Meanwhile, good explainability is expected to have some empirical results to support it along with the GIB-based analysis. \n\n[1] Dual Lottery Ticket Hypothesis, ICLR 2022\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation and organization are clear but lack some technique detail. Adding more graph property related designs would be better to improve the novelty.",
            "summary_of_the_review": "It\u2019s really interesting to transform a random ticket into a graph lottery one and the authors introduce gradually increased regularization terms and hierarchical graph sparsification to achieve this goal. Besides, the Graph Information Bottleneck theory is utilized to guarantee a more explainable ticket. However, the design of the random ticket and the technical detail to find DGLT need to be further illustrated and the explainability of DGLT needs more experiments to support it. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_PXxs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_PXxs"
        ]
    },
    {
        "id": "5zH-A7ZNIR3",
        "original": null,
        "number": 3,
        "cdate": 1666872358742,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666872358742,
        "tmdate": 1666872358742,
        "tddate": null,
        "forum": "Dvs-a3aymPe",
        "replyto": "Dvs-a3aymPe",
        "invitation": "ICLR.cc/2023/Conference/Paper2504/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies a new formulation of the LTH problem on graphs. Instead of searching for a sparse subnetwork with good performance, it poses the question of how to transform a randomly selected ticket into a lottery one. Although this problem has been studied before in DNNs, the paper proposes a solution for GNNs. Specifically, the proposed methodology, called DGLT,  comprises a hierarchical graph sparsification module based on hierarchical graph pooling and a training module following a suitable adjusted L2 regularization component. In addition, the paper borrows ideas from the Information Bottleneck theory to provide performance guarantees. The model is empirically evaluated on the task of node classification.",
            "strength_and_weaknesses": "**Strengths:**\n* The paper presents a rigorous formulation of the GLT problem on graphs. Efforts have also been made to theoretically explain the performance of the model.\n\n* Detailed empirical analysis, studying different aspects of the methodology.\n\n**Weaknesses:**\n* A point that is not very clear in the paper has to do with the practical utility of the theoretical observations that are made in Sec. 3.3. I cannot really see how these observations have a direct impact on the performance of the model in practice. It would be helpful if the authors could further clarify this point.\n\n* Another point has to do with the ablation study. A core component of the methodology is based on the hierarchical graph sparsification module which is implemented as a trainable pooling operator. Oftentimes, especially on graphs with no clear modular structure, the benefit of hierarchical pooling is not always clear. The impact of this component on the performance of the model should be further analyzed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well-written and most of the different concepts are clearly presented. At the same time, the paper keeps a good balance between theoretical contributions and empirical evaluation. Regarding the methodology, I believe the paper makes a novel contribution to the field.",
            "summary_of_the_review": "The paper makes an interesting contribution toward further understanding graph structure simplification and model compression in GNNs using lottery tickets. Nevertheless, I still have some concerns about the theoretical analysis derived in the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_rozM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_rozM"
        ]
    },
    {
        "id": "kYTPNBCWRt",
        "original": null,
        "number": 4,
        "cdate": 1667420863849,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667420863849,
        "tmdate": 1669514423432,
        "tddate": null,
        "forum": "Dvs-a3aymPe",
        "replyto": "Dvs-a3aymPe",
        "invitation": "ICLR.cc/2023/Conference/Paper2504/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the dual lottery ticket hypothesis on graphs. Specifically, a pair of core subgraph and sparse subnetwork is jointly uncovered via regularization-based network pruning and hierarchical graph sparsification. The method is also analyzed from the perspective of graph information bottleneck. Experiments on node classification, graph classification and link prediction tasks show the effectiveness of the proposed method compared with GLT.",
            "strength_and_weaknesses": "**Strength**\n- The dual lottery ticket hypothesis is firstly studied on graph lottery ticket.\n- The graph pooling technique is introduced to replace the sampling-based graph sparsification.\n- Extensive experiments are conducted for multiple applications.\n\n**Weaknesses**\n- Important details about the training loss are not provided, especially when the dimension of the sparsified graph is changed by pooling operations.\n- Some claims made by the paper are not well justified, e.g. better explainability, pooling is better than sampling-based algorithms.\n- The theoretical analysis of the proposed method is not clear enough.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the paper presentation is generally satisfactory, and can be improved by providing more details/justifications about the questions I raised in the summary. The novelty and technical contribution is limited, as the dual lottery ticket hypothesis and pooling-based sparsification are not newly proposed in this work, but it is still good to find that such a combination provides empirical improvement. The codes have been included for reproducibility.",
            "summary_of_the_review": "The paper introduces dual lottery ticket hypothesis and graph pooling in graph lottery ticket. In general the paper is well written with clear motivation and extensive experiments are conducted. However, I have concerns about the limited technical contribution and unclear details. \n\n=== Method ===\n\n1. The example in Figure 1 is not convincing. Aggregation failure may also happen in the proposed sparsification, since mask matrices are also applied to the adjacency matrix (e.g. $m_A$) which removes edges among clusters/communities. \n2. The reason for imposing a mask $m^{(l)}_A$ in each layer is not well justified. If the graph is already pooled, why does it need to be further sparsified? What is the performance if removing the masks $m_A$?\n\n3. The calculation of the cross-entropy loss in Eq. (4) is crucial to understand how the method can serve for different tasks, but is not explained at all. If this is a node classification task, the label is per-node. However, after pooling and sparsification, the final subgraph $A^{(L)}$ and $X^{(L)}$ is no longer node-wise but cluster-wise (e.g. the dimension of $A^{(L)}$ is $n_{L}\\times n_{L}$). There is clearly a gap between node-wise labels and cluster-wise embeddings. What is the specific form of the cross-entropy loss for node classification, link prediction and graph classification task, respectively?\n\n4. The claim of better explainability is not demonstrated. It usually has a specific meaning of providing explanations to the model predictions.  \n\n5. Is $G_{sub}$ the same as $G_{hgs}$?\n\n6. The theoretical analysis is a bit vague, especially when it comes to the proposed method. For example, the logic of why the training mask will make $I(\\hat{\\mathcal{G}}^*_s, Y)>I(\\mathcal{G}^*_s, Y)$ is unclear. The analysis is more like intuition instead of rigorous theoretical proof. Meanwhile, Figure 3 is hard to understand. After reading the analysis, it is still unclear to me why DGLT can be comparable/better than sampling-based algorithms.\n\n=== Experiment ===\n\n7. What is the definition of weight sparsity and graph sparsity in the experiment?\n\n8. How sensitive is the method to the pooling-related hyperparameters, e.g. $n_{l}$?\n\n=== Typos ===\n\nThere are also some typos. To name a few:\n- In the paragraph after Eq. (2), \u201cassignment matrix S^{(l)}... can projecting coarsened subgraph xxx\u201d -> \u201cAssignment \u2026 can project \u2026\u201d\n- Duplicate \u201cand\u201d in the paragraph before Lemma 1.\n- \u201ca admirable\u201d -> \u201can admirable\u201d in the paragraph before Lemma 1.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_xzm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_xzm4"
        ]
    },
    {
        "id": "PES76ZOxgU",
        "original": null,
        "number": 5,
        "cdate": 1667475542952,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667475542952,
        "tmdate": 1667475542952,
        "tddate": null,
        "forum": "Dvs-a3aymPe",
        "replyto": "Dvs-a3aymPe",
        "invitation": "ICLR.cc/2023/Conference/Paper2504/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper generalizes the Dual Lottery Ticket Hypothesis in GNN network pruning and graph sparsification and proposes a Dual Graph Lottery Ticket framework. Their experimental results demonstrate the effectiveness of their framework.",
            "strength_and_weaknesses": "Strongness\n1. The paper is well-written, and the motivation is intuitive and clear.\n2. It is interesting to introduce Lottery Ticket Hypothesis to simplify the input graph, and it is contributive to propose a Dual Graph Lottery Ticket for GNN pruning and graph sparsification.\n3. Extensive experiments demonstrate the effectiveness of the framework in terms of sparsity improvement, scalability, and performance\n\nWeakness:\n1. The model complexity in terms of training time and parameter volumes is not discussed and investigated in the paper. It is necessary to discuss the complexity of DGLT especially considering the non-shared GNNs to learn the node embeddings and the assignment matrix.\n2. The authors did not investigate the performance of random pruning on node classification. Whether the pruning would affect the performance of DGLT is not detailed.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and easy to follow. For novelty, the authors propose a novel DGLT framework for GNN network pruning and graph sparsification. For reproducibility, the authors should provide source code to help better understand the proposed model.",
            "summary_of_the_review": "In general, it is a good paper with a novel framework, solid technical, and significant results. It is a modest-to-high impact paper in a subarea of graph machine learning, especially graph sparsification.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_cfDA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2504/Reviewer_cfDA"
        ]
    }
]