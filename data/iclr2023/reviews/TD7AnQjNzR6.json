[
    {
        "id": "rTEV2Y5jVc",
        "original": null,
        "number": 1,
        "cdate": 1666478534789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666478534789,
        "tmdate": 1669057514619,
        "tddate": null,
        "forum": "TD7AnQjNzR6",
        "replyto": "TD7AnQjNzR6",
        "invitation": "ICLR.cc/2023/Conference/Paper1578/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper analyzes the statistical efficiency of the score estimate obtained by the score matching objective and compares it with the maximum likelihood based approach. Specifically, relation between the loss optimized in score matching J has been compared to the KL divergence between the true distribution p and the estimated distribution q. The paper shows that the bound on the KL depends on the log-Sobolev constant, the score matching estimator converges to a distribution that depends on other quantities like Poincare and smoothness of sufficient statistics. Later this experiments support the main idea of this theorem.",
            "strength_and_weaknesses": "Many results of the paper are interesting and insightful. And they are fairly novel since this work is probably the first to discuss the statistical efficiency of score estimator.\n\nHowever, the clarity of the paper could be improved. I have a few suggestions below. Also, with some more insightful experiments, the motivation of the paper could be boosted. ",
            "clarity,_quality,_novelty_and_reproducibility": "Generally, the paper is clear and results are of high quality. Also, the statistical efficiency is a new direction in score estimation, so the paper has novel ideas. Perhaps reproducibility could be improved. I have some comments below that might help improve the paper:\nI believe there are a couple of things that need to be clarified. First and foremost, while the paper nicely motivates citing the example how new score estimation techniques circumvent many problems associated with probability density estimation and estimation of partition function, their result does not exactly discuss the property of the score function, say s, but rather the estimated distribution q, which is quite different from the score s, i.e. the derivative of log of q. It is unclear at the moment whether these results translate easily to the properties of score s. That is, as is customary in recent score based generative models, score function, s, is modeled as a neural network and minimizes the loss J as defined in eq. 1 of the paper.  Another important and related matter that needs to be clarified in the paper is that the estimation that the paper analyzes is different from the recent developments in score based generative models where to facilitate the estimation of score, distribution is first convolved with a series of Gaussian noise and score is estimated for all the noisy probability distribution indexed by time. While this paper does not discuss this setup, it is better to explicitly clarify this point since score based generative models are quite common and can immediately confuse readers.\n\nA couple of suggestion for the authors to improve the clarity and presentation:\n1. The distribution q first mentioned in section 2, what is it and how is it related to the score?? After reading a couple of sentence, I guessed that the score being estimated is the grad of log of q. But, it is better to clarify that relation and introduce q first. Also, this is very critical here. I am still puzzled why analyzing property of q, other than perhaps that is well studied, is relevant in contrast to studying the property of score directly??\n2. From the definition of log Sobolev constant in eq. 2, it seems like the log Sobolev constant does depend on the distribution p. But, later in remark 2, authors state that log Sobolev constant does not depend on p, which is what I was expecting. But, it's better to clarify that in Defn 2 and explain why. \n\n3. The paper is packed with a lot of information, which is hard to unpack for the reader. Perhaps it is better to let go some of the contents and better explain other more relevant ones. For example, how does the score matching estimator in Proposition 2, page 5 come from. Authors have cited the paper by Hyvarinen 2007, but it's better to derive it for clarity.\n4. Related to the above comment, pseudolikelihood in section 5 might be a bit less relevant to the analysis of score matching.\n5. I found the experiments illuminating and would have liked more of it. Plus, at the moment it only supports theorem 2, something like that for Theorem 1 would have been nice.\n6. Lastly,  score estimation in generative models have been linked to log likelihood, which makes me wonder if there is a connection between these results and those. See the following paper:\n\nChen, T., Liu, G.H. and Theodorou, E.A.. Likelihood Training of Schr\\\" odinger Bridge using Forward-Backward SDEs Theory. ICLR 2022.\n",
            "summary_of_the_review": "I generally liked the new perspective of analyzing the statistical efficiency of the score matching objective discussed in the paper. Many theoretical results have been presented. However, there were some concerns in presentation and a few clarifying questions. Hence, I am on the positive side.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_R2sB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_R2sB"
        ]
    },
    {
        "id": "jm7nYQZ2u5",
        "original": null,
        "number": 2,
        "cdate": 1666649865371,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649865371,
        "tmdate": 1666649865371,
        "tddate": null,
        "forum": "TD7AnQjNzR6",
        "replyto": "TD7AnQjNzR6",
        "invitation": "ICLR.cc/2023/Conference/Paper1578/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the statistical properties of the score matching from a geometry viewpoint. The authors show an upper bound for the KL divergence between the underlying data distribution and the score-matching estimator using the log-Sobolev inequality. Moreover, with a focus on the exponential family, they analyzed the asymptotic efficiency of the score-matching estimator compared to the one of the MLE using The Poincar\u00e9 inequality and isoperimetric inequality.",
            "strength_and_weaknesses": "**Strength:**\n1. Score matching is a widely used technique and its statistical analysis is of high interest to the community.\n1. Their theoretical results seem novel and technically solid to me. I found the isoperimetric viewpoint quite interesting.\n1. I found the paper well-written and easy to follow.\n\n**Major comments:**\n1. Why is Thm. 3 about an unknown direction? It makes sense to me if the goal is to estimate $||\\theta||$ or $w_0^\\top \\theta$ for a prescribed $w_0$. The fact that the lower bound is proved for an unknown vector $w$ is confusing to me. Is it possible that there exists some $w'$ such that the score-matching estimator achieves better efficiency?\n1. The paper could benefit from adding more explanations and discussions. For example,\n    1. How large is $C_{LS}(q)$ compared to $C_{LS}(q, \\mathcal{P})$ depending on $\\mathcal{P}$? Can you give an example?\n    1. Can you provide an example for Thm. 2, just like Thms. 1 and 3?\n    1. It would be helpful to give some intuition on the tools used in Sec. 2.\n1. From (1), $J_p(q) - J_p(p)$ is equal to $1/2$ of the RHS of (6). Prop. 1 seems to be missing a factor of 2. This also holds true in the latter results like Thm. 1.\n\n**Minor comments:**\n1. Should $p$ be smooth in Def. 2?\n1. What is $f$ in Def. 3?\n1. The notation $I_d$ in the paragraph Mollifiers can be confused with the identity matrix.\n1. $J_p(q)$ should be $J_p(p)$ in the equation below (6).\n1. The subscripts MLE and SM in Def. 6 and Prop. 2 are too large. Maybe use \\text{}?\n1. In conclusion, \"In tihs paper\" --> \"In this paper\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and easy to follow. The claims seem to be technically solid and novel. I only have a few concerns which are stated above.",
            "summary_of_the_review": "This paper provides useful insights into the popular score-matching estimator. The results seem novel and technically solid to me. I thus vote for acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_tSNd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_tSNd"
        ]
    },
    {
        "id": "RoEXiiQM4K",
        "original": null,
        "number": 3,
        "cdate": 1666706422721,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666706422721,
        "tmdate": 1667505973372,
        "tddate": null,
        "forum": "TD7AnQjNzR6",
        "replyto": "TD7AnQjNzR6",
        "invitation": "ICLR.cc/2023/Conference/Paper1578/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors provide a theoretical comparison between maximum likelihood estimators and score matching estimators in the case where the distributions are from the exponential family.\n",
            "strength_and_weaknesses": "Strengths:\n- Proofs are clean and straightforward.\n- Theory corroborates with practical observation, that score matching on multimodal data requires noise perturbations.\n\nWeaknesses:\n- Theory is mainly descriptive rather than prescriptive --- it is well known that noise-perturbation is crucial for properly learning scores [1].\n- Non-asymptotic theory is limited to comparing loss values (KL divergence vs expected square error of scores), which does not provide a clear insight into how the resulting densities themselves compare.\n- Asymptotic theory is limited to learning on distributions in the exponential family.\n\n[1] Song, Y. and Ermon, S., 2019. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the proofs are straightforward and easy to follow, even for non-experts in isoperimetric analysis.\n\nTo my knowledge, the theoretical connections are novel, though the practical observations they describe have been well-known at the outset of the development of score-based generative models [1].",
            "summary_of_the_review": "This work provides a suitable theoretical premise for the well-known observation in score-based generative modeling that noise-perturbations are critical for good performance. For this reason, the most relevant result to me is Theorem 2, which provides theory for this phenomenon under the (albeit simplistic) assumption that the data distribution is exponential.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_gNn2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1578/Reviewer_gNn2"
        ]
    }
]