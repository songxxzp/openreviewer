[
    {
        "id": "QIuvTc_iyT",
        "original": null,
        "number": 1,
        "cdate": 1666515770642,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666515770642,
        "tmdate": 1666515770642,
        "tddate": null,
        "forum": "JpbLyEI5EwW",
        "replyto": "JpbLyEI5EwW",
        "invitation": "ICLR.cc/2023/Conference/Paper6233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers learning high dimensional linearly separable data with a one-hidden layer network with Leaky ReLU activations. The high dimensional data assumption entails that the norm of the data points is significantly larger than the pairwise correlation between different points. The assumption holds for random high dimensional data such as a Gaussian distribution. It is shown that a network where the first layer is trained with gradient flow has the following properties: (1) It converges to a zero training error solution (2) The solution has rank at most 2. (3) The solution has a linear decision boundary (4) It converges to a certain max margin solution which is not necessarily the max margin linear separator. Results for gradient descent are also presented where a bound on the stable rank is proved.",
            "strength_and_weaknesses": "Strengths:\n1. Strong theoretical results in a challenging setup.\n2. New theoretical techniques that might be useful in other settings.\n3. Very well written.\n4. Novel empirical findings.\n\n\nWeaknesses:\n\nNothing major. \nTwo minor weaknesses:\n1. The fact that \u201chigh dimensional data\u201d is used throughout can be a bit of an over statement. For example, consider classifying MNIST digits, e.g., 0 digit vs. 1 digit. This problem is high dimensional, while I guess that the assumption in the paper is not satisfied. For example, there can be two data points of a 0 digit that have a large pairwise correlation. Maybe \u201crandom high dimensional data\u201d is more accurate.\n2. There are only a few experiments. There are many interesting experiments that can be included. For example, the theoretical finding on D_XOR is very interesting because usually networks solve the XOR problem. I guess that this is because of the high noise and low norm of the cluster centers. In any case, it would be interesting to see that the network indeed fails on this data empirically and converges to a linear classifier.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As described above, the work is of high quality, very clear and has novel theoretical results.",
            "summary_of_the_review": "Strong and novel theoretical results in a challenging setting and very well written. Highly recommend for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_BzfX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_BzfX"
        ]
    },
    {
        "id": "3r_PXooaTj",
        "original": null,
        "number": 2,
        "cdate": 1666591632940,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591632940,
        "tmdate": 1666591632940,
        "tddate": null,
        "forum": "JpbLyEI5EwW",
        "replyto": "JpbLyEI5EwW",
        "invitation": "ICLR.cc/2023/Conference/Paper6233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors studied the implicit bias of gradient flow and gradient descent for two-layer leaky-ReLU networks with nearly orthogonal data. It is shown that gradient flow will asymptotically converge to a network with rank at most 2 and linear decision boundary. Such a network is also approximately a max-margin linear predictor. For gradient descent, it is shown that with small enough initialization, a single step could reduce the rank of weight matrix to constant and the rank of weight matrix will remain constant throughout training. Experiments are also provided to support the theoretical results.",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written and easy to follow.\n- Understand the implicit bias of neural networks is an important problem and current paper provides an answer in the two-layer leaky-ReLU network with nearly orthogonal data setting.\n- The results are clearly stated with proof ideas discussed in the main text and full proofs in the appendix.\n- I found the section 5 that discusses the implication of the result quite interesting: it not only discusses the benefits of such implicit bias, but also gives examples where such implicit bias is harmful.\n\nWeaknesses:\n- As mentioned in the conclusion section by authors, the results heavily rely on leaky ReLU and cannot be applied to ReLU. Such leaky parameter $\\gamma$ also has impact on the convergence point, such as point 7 in Theorem 3.2. It is unclear how to generalize the results to other activations and non-nearly-orthogonal data.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n- The paper is overall well-written and easy to follow.\n\nQuality:\n- The quality of paper is overall good. The results are clearly stated with proof ideas discussed in the main text and full proofs in the appendix.\n\t\n\nNovelty:\n- The results appear to be interesting and novel. \n\nReproducibility:\n- This is a theoretical work so there are no experiments to be reproduced. The full proof are given in the appendix (I didn\u2019t check them).\n",
            "summary_of_the_review": "In summary, this work studied the implicit bias of two-layer leaky ReLU network with nearly orthogonal data. It shows that the convergence point has a low (stable) rank weight matrix and a linear decision boundary. The results seem to be new and interesting. Therefore, I\u2019m currently leaning towards accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_kRaJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_kRaJ"
        ]
    },
    {
        "id": "ojqZL57v0e",
        "original": null,
        "number": 3,
        "cdate": 1666622222751,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666622222751,
        "tmdate": 1670407613218,
        "tddate": null,
        "forum": "JpbLyEI5EwW",
        "replyto": "JpbLyEI5EwW",
        "invitation": "ICLR.cc/2023/Conference/Paper6233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is concerned with the implicit bias of gradient flow and gradient descent in two-layer fully connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. For gradient flow, this paper leverages the implicit bias for homogeneous neural networks to show that gradient flow produces a neural network with rank at most two. For gradient descent, this paper shows that a single step of gradient descent suffices to sufficiently reduce the rank of the network and the rank remains small throughout training. Finally, several experiments are provided to verify the effectiveness of the proposed method.\n",
            "strength_and_weaknesses": "Strengths: the paper characterizes the implicit bias of common gradient-based optimization algorithms for two-layer leaky ReLU networks trained on high-dimensional datasets. It presents that small initialization variance is important for gradient descent\u2019s ability to quickly produce low-rank networks.\nWeaknesses: The clear and precise definition of \"high-dimensional data\" is missing. What exactly are \"high-dimensional data\"? I understand random data in high-dimensions are almost always nearly orthogonal to each other. Please be more careful when stating the 'near-orthogonality' of data dealt with in the paper. Can we quantify the required \"near-orthogonality\" of data? Furthermore, the idea, analysis, and technical tools used in this paper are mainly adapted from the literature, in e.g., Lyu & Li (2019), Ji & Telgarsky (2020), Lyu et al. (2021). Third, I am not sure whether this so-called \"implict bias\" pertains to the network itself intrinsically, since if some of the assumptions on the data/gradient flow algorithms would make the \"implict bias\" not hold. Some existing works on two-layer ReLU or leaky ReLU learning are missing and closely related. See e.g., the landscaple and local/global optimality of (Leaky) ReLU networks in Laurent, et al, The multilinear structure of ReLU networks, in ICML, 2018; Laurent, et al, Deep linear networks with arbitrary loss: All local minima are global, in ICML, 2018; as well as the global optimality of SGD on training ReLU networks and the rank-2 solutions of regularized relu training in e.g., Wang, et al. Learning relu networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP, 2019, and Yang, et al. Learning two-layer relu networks is nearly as easy as learning linear classifiers on separable data. IEEE TSP, 2021.  Ergen & Pilanci, (2020, June). Convex geometry of two-layer relu networks: Implicit autoencoding and interpretable models. In International Conference on Artificial Intelligence and Statistics. PMLR. Last, can the results generalzie to three-layer or deep ReLU networks of practical interest? Or please at least discuss its implications for training deep relu networks.\n\nComments:\ni) This paper investigates the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. However, the paper still focuses on two-layer neural networks and also only the leaky ReLU activations. More comparison of the work in context shall be included, as well as its practical hints for ReLU networks and deeper networks.\nii) On Page 1, \"We consider fully-connected two-layer networks with m neurons where the first layer weights are trained and the second layer weights are fixed at their random initialization.\" Although fixing the second-layer weights does seem to harm the representation of the 2-layer network, which is indeed used in most existing works, this (considerably) makes the learning problem much easier. Nonconvex optimization often comes from the symmetry or rotation, if one fixes the second-layer and breaks the symmetry/or elliminates the scaling issue. It would be much more meaningful to study the landscape and learning behaviour of the two-layer networks.\niii) Could you please list the pros and cons of your offline method compared with the online method in Lyu & Li (2019) and Ji & Telgarsky (2020)? In addition, could you explain how does different choices affect the result?\niv) In the experiments, this paper verifies the theoretical results in two-layer ReLU networks with bias terms trained by SGD only on CIFAR-10. More datasets and tests shall be conducted for validation.\nv) There are some mistakes in grammar and sentences, such as \"...and consider training that starts from a...\" and \"...when the labels y are some nonlinear function of t...\" These indistinct expressions abate the readability of this paper.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, but the presentation can be improved. The quality of this paper looks fine, but some unclear \"asumptions\" on the data along with the treat of ReLU networks only undermine the contributions of the paper. Plus, lack of discussions of related works further makes it difficult to judge the novelty of the results. \n\n",
            "summary_of_the_review": "This paper is concerned with the implicit bias of gradient flow and gradient descent in two-layer fully connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal. I believe the paper shalle be improved in providing a detailed comparison of related works on assumptions on the data, activation functions, global/local optimality, and other interesting findings. Overall, the contribution of the present paper is below the bar for ICLR.\n------------------------------------------------------\nI have looked at the the authors' replies as well as the comments of other reviewers. I agree that the paper contributes new ideas and technical proofs for understanding training of leaky ReLU networks. I have updated my score. It would be good to see all the corrections in the revision. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_ewuA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_ewuA"
        ]
    },
    {
        "id": "jSaasMzGc3",
        "original": null,
        "number": 4,
        "cdate": 1666653372501,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653372501,
        "tmdate": 1666653372501,
        "tddate": null,
        "forum": "JpbLyEI5EwW",
        "replyto": "JpbLyEI5EwW",
        "invitation": "ICLR.cc/2023/Conference/Paper6233/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper analyzes the implicit bias of gradient flow/descent on a two-layer leaky-ReLU network. Specifically, assuming the data points are nearly orthogonal, Theorem 3.2 gives an accurate characterization of the KKT point of the margin maximization problem: all data points are support vectors, the first layer is of rank 2, and a closed-form solution is given. In Theorem 3.4, it is further shown that gradient flow converges to this solution. Section 4 focuses on gradient descent, and shows that with nearly orthogonal data and smoothed leaky-ReLU activation, the stable rank of the network is bounded by a universal constant. Empirical supports are also provided.",
            "strength_and_weaknesses": "This paper provides interesting and accurate characterizations of the implicit bias of gradient flow/descent with nearly-orthogonal data and leaky-ReLU activation. Specifically, although the network can be highly over-parameterized, this paper shows that the classical or stable rank stays small. One weakness is that the near orthogonality condition is a little restrictive, as it implies linear separability. Lemma 3.3 shows that it holds with Gaussian inputs, but it requires $d\\ge n^2$, which does not hold in practice. Can you discuss how this assumption is used in your proof, and why linear separability is not enough?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written.",
            "summary_of_the_review": "This paper gives interesting results on the implicit bias of gradient descent with nearly-orthogonal data and leaky-ReLU activation. I recommend for acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_MUCK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6233/Reviewer_MUCK"
        ]
    }
]