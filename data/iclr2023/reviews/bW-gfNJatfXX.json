[
    {
        "id": "CBxSmB3iVCA",
        "original": null,
        "number": 1,
        "cdate": 1665802078788,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665802078788,
        "tmdate": 1665802078788,
        "tddate": null,
        "forum": "bW-gfNJatfXX",
        "replyto": "bW-gfNJatfXX",
        "invitation": "ICLR.cc/2023/Conference/Paper4327/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a robust RL policy training framework which applies adversarial training using coordinated traffic flow. For building the traffic flow, it uses the so-called \u201cSocial Value Orientations (SVOs)\u201d (which is a weight that balances the proportion of each agent\u2019s own driving reward and its surrounding agents driving rewards in the agent\u2019s aggregated optimization reward) and allows the communication of SVOs among the background agents in the traffic flow. Adversary emerges when the SVOs of the driving agent sent to the traffic flow agents are perturbed. The problem can then be turned into a min-max optimization problem where the driving policy maximizes its reward and a spurious adversarial policy minimizes it through changing the SVOs seen by the agents. The proposed method is evaluated in terms of the safety, speed, and success rate of the generated traffic flows, the effect of adversary based on coordinated traffic flow, and the zero-shot transfer of the driving policy.",
            "strength_and_weaknesses": "## Pros:\n\n1.Leveraging SVOs for adversarial attack are moderately novel and its effect on improving zero-shot transfer of the driving policy seems promising.\n\n2.generalization of the training policy is an important problem\n\n## Cons:\n\n1.Communicating SVOs among agents seem to be a relatively incremental improvement over [Peng 2021].\n\n2.One issue is that the realism evaluation of the settings is missing \nAre the generated trajectories realistic and initial positions realistic? One way to evaluate if the trajectories are realistic is to look into acceleration (both longitudinal and latitudinal) and jerk distributions and compare them against some similar scenarios in real world datasets (e.g., nuScenes and Waymo Open).\n\n3.some details are missing\nsee \u201cClarity, Quality, Novelty And Reproducibility\u201d below.\n\n4.effect of adversary is not that large\nTable 1 success rates are not influenced much percentage wise. One usually expects the attack to have a stronger effect.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nSome parts are not very clear. For example, the usage of SVOs and the key difference with [Peng 2021] need to be further clarified and highlighted. Experiment setting details are also missing.\n\n\n1.Can you further clarify the key difference (using the notations in Sec 3.1), between the current work and [Peng 2021] in terms of \u201cagents in their traffic flow have no access to other agents\u2019 SVOs, leading to conservative behaviors\u201d? Is it basically the policy beta will only have O_i and C_i as the inputs but not the C_j for others? Are all the C_i for each agent i also learned during the training?\n\n2.How many different settings were used for each scenario during the evaluation?\n\n3.in (2), why the optimization for beta is to solve n separate optimization objectives rather than their sum?\n\n## Quality\n\nThe quantitative and qualitative results are moderately decent. One drawback is the realism issue I mentioned earlier. Scalability is also suspicious given it only has very limited scenario settings. However, this is somewhat acceptable given previous works e.g., [Peng 2021] along this line also does not evaluate realism and has limited scenarios to show effectiveness.\n\n## Reproducibility\n\nIt is hard to reproduce given code is not available, details of simulator and experiment settings are all missing.\n\n\n## Minor Issues:\n\n-Sec 3.2 - state space - last two lines, probably should not overload notation c_i here.\n",
            "summary_of_the_review": "Overall the approach is moderately interesting and the results seem to be promising. However, some clarification needs to be made regarding the novelty of the method and the details of evaluation for a better evaluation of the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_MJTo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_MJTo"
        ]
    },
    {
        "id": "OYb1lprf_Y",
        "original": null,
        "number": 2,
        "cdate": 1666709527564,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709527564,
        "tmdate": 1666709527564,
        "tddate": null,
        "forum": "bW-gfNJatfXX",
        "replyto": "bW-gfNJatfXX",
        "invitation": "ICLR.cc/2023/Conference/Paper4327/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work builds a framework for traffic simulation and propose a form of adversarial training to improve driving policies.  \nIn their traffic flow, they assume the agents allowed to communicate *social value orientations* (SVOs) and introduce an adversarial agent that tries to minimize the overall reward for the driving agent by only modifying the SVO.\nThey show strong improvements on their coordinated traffic flow task on a variety of subtasks (intersection, bottleneck, merge, roundabout).",
            "strength_and_weaknesses": "### Strengths\n\n* Experiments show clear improvements over IDM, FLOW, COPO\n* Overall writing is good, mathematical notion is for the most part clear\n\n### Weaknesses\n\n* Reproducibility - all experiments were performed on \"our internal driving simulator\". Can the authors give more details on this? This is a major factor in my decision.\n* The experiments were all done in this one simulator and it is unclear if the effectiveness of this strategy translates to other domains.\n* Writing/figure quality (more details below) - a couple figure/tables are quite hard to understand and could be presented better.\n\n### Minor Issues / Questions\n\n* What is the mathematical equation for the reward function? Were any other reward functions tried?\n* Algorithm 1: in my opinion this is described well by Sec 4.2 and can be taken out.\n* Table 1: Highlighted colors seem arbitrary - why is the \"0.0\" colored in \"Ours + Wrong Lane\"?\n* Figure 6 took quite a while to understand what I was looking at. FailMaker seems bad and can be taken out and just described in text.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity, Quality:** overall good quality and clarity if some of the figures/tables are edited.  \n**Novelty:** their method for adversarially attacking SVO seems relatively novel.  \n**Reproducibility:** the authors provide details about their architecture and learning algorithm, but I am unsure if this is reproducible without more training details and especially the simulator.",
            "summary_of_the_review": "This work introduces a fairly novel approach and overall shows improved performance across various scenarios in their simulator. The major factor in my current decision is on reproducibility.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_o4m3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_o4m3"
        ]
    },
    {
        "id": "gIZ2TY_Sev8",
        "original": null,
        "number": 3,
        "cdate": 1666754777701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666754777701,
        "tmdate": 1666762279220,
        "tddate": null,
        "forum": "bW-gfNJatfXX",
        "replyto": "bW-gfNJatfXX",
        "invitation": "ICLR.cc/2023/Conference/Paper4327/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a novel adversarial framework to train driving policies under learned traffic flow in simulation. It first extends traffic flow learned with reinforcement learning by exposing each other\u2019s intrinsic social value orientations (SVO). Next, it uses this exposed SVO to train adversarial agents as well as a policy. In particular, the traffic flow accesses the driving policy\u2019s SVO but not vice versa (misunderstanding). Experiments show that 1. The new traffic flow with exposed SVO to each other learns better behavior 2. The proposed adversarial training effectively reduces driving policy\u2019s performance and 3. Driving policy has better zero-shot transferability across different traffic flows when trained with the proposed method. \n",
            "strength_and_weaknesses": "## Strengths \n+ The paper is well-written and the presentation is strong.\n+ The proposed method demonstrates convincing empirical results.\n+ The paper proposes a novel way to adversarially train driving policies in traffic flow, and actually demonstrates that agents trained with the proposed method drive (transfers) better, which is a big plus.\n\n## Weaknesses\n- I can\u2019t seem to find sample complexity information on traffic flow training in the paper. I am curious to see whether exposing SVO improves/decreases sample complexity with respect to the metrics in Table 1, and how it compares to the baseline CoPO.\n- Table 1 compares how the proposed method impedes the driving policy. I am curious to see whether this result generalizes across different driving policies/architectures. \n- I would like to see an ablation that justifies the \u201cmisunderstanding\u201d part of the claim/story. For example, an ablation in which the driving policy also sees the SVO of the nearby traffic flow.\n- The adversarial training currently strictly comes from the adversarial agent. Since the background traffic already coordinates with each other, I am wondering why not directly alternate between training between the driving policy and background traffic flow, and have the traffic flow perform multi-agent coordinated attacks to the driving policy.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Great; the paper is well written\n\nQuality: Good\n\nOriginality: Good\n\nReproducibility: the method seems to extend upon prior work CoPO which is open-sourced; it hence seems to be reproducible based on information provided in the paper.",
            "summary_of_the_review": "The paper presents a novel framework to train driving policies under coordinated traffic flow. Experiments are thorough and demonstrate **constructive** usefulness of the presented approach. I personally think this paper is worth acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_hLos"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_hLos"
        ]
    },
    {
        "id": "oZRYGHnDRi",
        "original": null,
        "number": 4,
        "cdate": 1667420176219,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667420176219,
        "tmdate": 1667420176219,
        "tddate": null,
        "forum": "bW-gfNJatfXX",
        "replyto": "bW-gfNJatfXX",
        "invitation": "ICLR.cc/2023/Conference/Paper4327/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors build a traffic simulator in which each agent drives in order to maximize its reward as well as the mean reward of nearby agents. The extent to which the vehicle optimizes for the reward of other agents $c_\\beta$ can be adjusted. The authors then show that if a new agent is trained in such a simulator and during training the parameter for controlling $c_\\beta$ is adversarially optimized, the resultant policy transfers better than baselines to new traffic densities and test-time out-of-distribution NPC controllers (e.g. IDM).",
            "strength_and_weaknesses": "One strength of this paper is that it addresses multiple impactful problems in self-driving simulation. Generally speaking, adversarial NPCs exhibit unrealistic aggressive behavior. The idea of controlling the extent to which the NPCs optimize for the reward of nearby agents seems like a reasonable way to parameterize the space of plausible vehicle behaviors that an AV may interact with on real roads.\n\nThat being said, the main weakness for me reading this paper was that I found it hard to understand precisely what the authors claim as a contribution in this paper. My sense is that the main contribution is the use of $c_\\beta$ as the variable to adversarially optimize. However, my sense is another contribution is maybe the construction of NPCs that have such a $c_\\beta$? Or is the main contribution the policy that is ultimately trained using Algorithm 1 which they show can be transferred to new test-time simulators with out-of-distribution NPC controllers? I'm not seeing yet what is novel about Algorithm 1 - do the authors define what is different between \"misunderstanding-based adversarial learning\" and standard adversarial learning?\n\nI list my other minor edits and questions below:\n\n- state space - no notion of length or width?\n\n- \"Although IPL is prone to generate egoistic suboptimal behaviors\" - is there a citation for this claim?\n\n- Figure 3 - do the authors compare against the baseline of training the policy on a hard-coded uniform distribution for $c_\\beta$ instead of the adversarially optimized distribution that they find for $c_\\beta$?\n\n- Section 5.1 - clarify why would the initial poses being close together imply that it's harder for the agents to coordinate?\n\n- Figure 5 - I think it's easier to read if the same coordinate frame is used for each column\n\n- Figure 6 - I find this figure difficult to read. I think it would be better if the authors use the same colormap for all grid cells instead of using blue only for the diagonal.\n\n- Do the authors have videos they can share? It's hard to understand the effect of different $c_\\beta$ from frame shots.\n\n- Section 5.3 - I'm not sure what it means for the evaluation environment to be \"IID with training environment\"?\n\n- Figures and Tables - I feel \"ours\" is being used to sometimes denote NPC policies, sometimes ego policies, and sometimes training environments. Is there a way the authors could disambiguate each of these contributions?",
            "clarity,_quality,_novelty_and_reproducibility": "I think the authors can improve the clarity of the submission substantially. Specifically, I'm having trouble identifying the main result that the authors achieve using their method. I'm also somewhat confused about Section 3.3 - is there a reason the authors use this architecture compared to prior work? Can the authors define formally what it means for their framework to be \"misunderstanding-based\"? Once I have a better sense of the main takeaway of the paper, I will be more confident in commenting on the originality of the paper.",
            "summary_of_the_review": "I'm currently unclear on how to interpret some of the main claims of the paper, namely whether the authors are claiming that they have a general-purpose driving policy, or a general-purpose simulator in which robust driving policies can be trained. I also think that qualitative videos would make it much easier to judge the quality of the contribution, since most of the metrics are designed by the authors. If the authors clarify their contribution, I am willing to increase my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_7Hpn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4327/Reviewer_7Hpn"
        ]
    }
]