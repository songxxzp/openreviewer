[
    {
        "id": "puR556FqofC",
        "original": null,
        "number": 1,
        "cdate": 1666899088741,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666899088741,
        "tmdate": 1670652899912,
        "tddate": null,
        "forum": "dyifcA9UuRo",
        "replyto": "dyifcA9UuRo",
        "invitation": "ICLR.cc/2023/Conference/Paper6326/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces the neural probabilistic logic programming language DeepSeaProbLog, which\ncan be seen as an extension of DeepProbLog. To DeepProbLog it adds the ability to represent\nand use probability distributions of continuous and mixed continuous and discrete spaces.\nThe paper after describing the language provides an illustrative example of a classification\ntask which a neural baseline struggles on, but DeepSeaProbLog with its reasoning capabilities\nexcels at.\n",
            "strength_and_weaknesses": "The paper continues exploring the very interesting intersection of\ndeep learning and logic programming for solving problems each set of\napproaches would struggle on their own.\n\nUnfortunately, the paper feels far too incremental. Even the example seems fairly\nsimilar to the example used in Raedt et al's 2019 IJCAI workshop submission. With\nboth examples feeling fairly contrived. The neural baseline seems unusually weak.\nI would expect a more reasonable architecture to use a Spatial Transformer Network\nas those have been a mainstay of object localization for quite a while.\n\nI have some frustrations with large portions of the main text being essentially\nbackground information on logic programming. This lead to much of the unique\ncontributions of the paper being relegated to the appendix. While I understand the\nneed to explain logic programming to many of ICLR typical readers, I would have\nstill preferred a far more condensed background section.\n\nThis especially lead to issues as its not clear the paper really introduced the\nsemantics of DeepSeaProbLog. There were some definitions of how to calculate the probability\nof a possible world and a query, but further details are deferred to another paper.\nAs one of the major contributions of the paper more details should have stayed\nin the main text.\n\n Questions:\n\n What is the semantics of `normal` in Example 3.1? I expect normal to take two arguments\n but it only takes one. Does it's argument return a tuple?\n\n Minor:\n\n Not all code examples and figures have labels\n",
            "clarity,_quality,_novelty_and_reproducibility": " The work is fairly original if a bit incremental. The paper would greatly benefit from\n better organization of the material so that more of the novelty can move from the\n appendix back into the main text. I would have preferred more details on the semantics\n of the language than the brief sections included. The work while ostensibly isn't\n very reproducible is effectively describing a system which I expect to be publicly\n released.\n\nUpdate:\n\nI thank the authors for their response, but I still feel the clarity of the work could be greatly improved. If this paper is accepted, it would greatly benefit from more detail around the method itself. How is differentiation through WMI performed? Does knowledge compilation mean we don't have to worry about differentiating through discrete variables?",
            "summary_of_the_review": "Interesting work that would benefit from more clarity or an experiment that feels less contrived.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_6UoH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_6UoH"
        ]
    },
    {
        "id": "JmFHN26QN5",
        "original": null,
        "number": 2,
        "cdate": 1666984070828,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666984070828,
        "tmdate": 1666984070828,
        "tddate": null,
        "forum": "dyifcA9UuRo",
        "replyto": "dyifcA9UuRo",
        "invitation": "ICLR.cc/2023/Conference/Paper6326/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a probabilistic programming language based on extending a prolog-like language with probabilistic facts and equipping a query with probabilistic semantics. The language includes both continuous and discrete random variables and it includes parameterized neural networks. \n\nThe authors describe a fairly elaborate learning technique whereby gradients can be computed of the weighted model counting by using reparameterization of continuous random variables and relaxation of indicators. This allows them to train the neural network parameters.\n\nThe authors then show a couple of results demonstrating how a combination of probabilistic logic programs and neural networks  can be used to achieve some interesting results including computing the numerical difference of two digits shown pictorially as well as a variational encoder that reverses this.",
            "strength_and_weaknesses": "The strengths of the paper are they are putting together various well-studied concepts and techniques into one paper. However, this is also a weakness because it is hard to distinguish the key contributions of the authors. The techniques of using reparameterization of continuous random variables or relaxations are well known and the probabilistic logic program's semantics also seem to not be novel. So it is not obvious as to what is the key innovation here.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors don't give a good explanation of how they are handling discrete random variables despite claiming this in the abstract. There is a brief mention of using Dirac Delta distributions but more details would be needed to understand how they plan to reparameterize these Dirac Delta distributions. Categorical random variables that can take on multiple unordered values need more thought. I would have liked to see some experiments involving such variables.\n\nAlso please note that integrating out discrete random variables doesn't count as supporting discrete random variables in the language.\n\nWeighted model counting is a well known technique. It's unclear to this reviewer whether differentiation over weighted model counting by using relaxation of indicators counts as an innovation.\n\nA lot of the algorithms here such as the sum-product terms are referring to other papers which make it somewhat harder to reproduce.\n\nI had some difficulty following the subset of horn clauses that are supported or whether concepts like unification in Prolog are included. The example program on page 3 included a term \"Degree\" which was not used in the query. This needs some clarity is that being integrated out?\n\nThe example of digit differencing didn't seem very impressive for a language that is claiming to combine logic, probability and neural networks.",
            "summary_of_the_review": "The description of their work is reasonable but it is not clear what is the key novelty is among the hodge lodge of ideas that are included in this paper besides how do they even support discrete random variables is not well covered.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_ViX9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_ViX9"
        ]
    },
    {
        "id": "EadvEIzJwdI",
        "original": null,
        "number": 3,
        "cdate": 1667075133915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667075133915,
        "tmdate": 1667075133915,
        "tddate": null,
        "forum": "dyifcA9UuRo",
        "replyto": "dyifcA9UuRo",
        "invitation": "ICLR.cc/2023/Conference/Paper6326/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper describes DeepSeaProblog, a neuro-symbolic probabilistic\nlogic programming language and system that can handle continuous\nvariables.",
            "strength_and_weaknesses": "The work looks quite interesting and fills a gap related with the\nlack of probabilistic logic programming to handle continuous\nvariables. On the other hand, it looks like a small increment related\nwith DeepProblog.\n\nIt would be interesting to add more applications in the experimental \nsection. Also, what is the scalability of the implementation?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good\nQuality: good\nNovelty: it looks like an increment on top of DeepProblog\nReproducibility: authors provide material in the appendice that may be sufficient for reproducibility of experiments, but I haven't tested.",
            "summary_of_the_review": "This paper describes DeepSeaProblog, a neuro-symoblic probabilistic\nlogic programming language and system that can handle continuous\nvariables.\n\nThe work looks quite interesting and fills a gap related with the\nlack of probabilistic logic programming to handle continuous\nvariables. On the other hand, it looks like a small increment related\nwith DeepProblog.\n\nIt would be interesting to add more applications in the experimental\nsection. Also, what is the scalability of the implementation?\n\nObs:\nCOPPE Gerson Zaverucha in Garcez 2022 reference should be Gerson\nZaverucha. COPPE is the institution. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_tpMA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_tpMA"
        ]
    },
    {
        "id": "mxaNr6yw00q",
        "original": null,
        "number": 4,
        "cdate": 1667099519322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667099519322,
        "tmdate": 1667099519322,
        "tddate": null,
        "forum": "dyifcA9UuRo",
        "replyto": "dyifcA9UuRo",
        "invitation": "ICLR.cc/2023/Conference/Paper6326/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes DeepSeaProbLog, a NeSy algorithm that supports rules and facts specified in both discrete and continuous domains. DeepSeaProbLog falls into the category of NeSy method that injects (logical) constraints in neural networks. The main contribution of this paper is the generalization of supported logical programs from boolean variables to continuous variables (e.g., Gaussians). The resultant probabilistic program becomes an SMT formula. Since exact probabilistic inference over SMT formulas is intractable, the paper adopts several well-established methods to approximate the queried probability. The authors introduce a new task, i.e., MNIST subtraction, that requires algorithms to simultaneously learn the bounding boxes of MNIST digits and compute their subtracted values. On this task and two other existing NeSy tasks, DeepSeaProbLog performs better than neural networks and some other NeSy baselines.",
            "strength_and_weaknesses": "The main contribution of DeepSeaProbLog is to provide a more general language for formulating NeSy rules/programs. This naturally leads to broader applications of NeSy methods. However, one potential downside is that approximations need to be applied to ensure the algorithm's efficiency. \n\n- In terms of computing WMC/WMI, DeepProbLog uses a backward search-based exact solver while DeapSeaProbLog approximates the result by sampling, even if all variables in the probabilistic program are boolean. In this case, it would be nice to see the performance difference between the two methods. Efficiency comparison between (i) the compilation phase of DeepProbLog, (ii) the execution phase of DeepProbLog (evaluating the compiled logic circuit), and (iii) the execution phase of DeepSeaProbLog would also provide a better overview of the tradeoff between efficiency and performance.\n\n- Following the above comment, is it possible to combine knowledge compilation techniques with sampling to get the best of both worlds? That is, apply knowledge compilation to parts that can be efficiently turned into ``recursively nested sums of products'', and sample the part with complex variable dependencies.\n\n- In the MNIST subtraction experiment, some samples provided location supervision and curriculum learning. The authors mentioned that this is necessary for DeepSeaProbLog since otherwise it may converge to trivial cases. If this phenomenon also happens in baseline methods?\n\n- If a PCF contains only one variable, which is often the case in the probabilistic programs adopted in the paper, it seems that we can \"define\" this PCF as a boolean variable and simplifies the computation of the WMI by computing the probability of this PCF from the NN. Will this be able to achieve a better performance-efficiency tradeoff?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written, and the running examples are nice.\n\nThe technical part of the main paper is sound, though I did not check all details and the proofs in the appendix. In terms of empirical evaluation, there is one minor weakness: some auxiliary tasks are provided in the MNIST subtraction experiment. It is unclear how these tasks affect the performance of the baselines. Please refer to the detailed comments above.\n\nThe main novelty of this paper is to extend DeepProbLog to continuous domains. ",
            "summary_of_the_review": "In summary, I vote for acceptance of the paper as it proposes a NeSy algorithm that supports rules and facts specified in both discrete and continuous domains. The main weakness of the paper is the insufficient discussion of the performance-efficiency tradeoff, as detailed in the comments above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_qtFY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_qtFY"
        ]
    },
    {
        "id": "OjDLlp2tqIj",
        "original": null,
        "number": 5,
        "cdate": 1667181605287,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667181605287,
        "tmdate": 1667181605287,
        "tddate": null,
        "forum": "dyifcA9UuRo",
        "replyto": "dyifcA9UuRo",
        "invitation": "ICLR.cc/2023/Conference/Paper6326/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes a neural probabilistic programming language that supports both discrete and continuous variables, called DeepSeaProbLog. An implementation of DeepSeaProbLog that allows inference and gradient-based learning is further proposed, by leveraging a reduction to weighted model integration and differentiation through a weighted model integral. Empirical evaluations on DeepSeaProbLog are presented on a neural-symbolic object detection task, variational auto-encoder with a difference constraint in the latent space, and neural hybrid Bayesian networks.",
            "strength_and_weaknesses": "The motivation of this work to enable neural probabilistic programming to work in mixed discrete-continuous domains is tempting since it would allow for expressive modeling for real-world problems. The use of weighted model integration tool is novel to me and it is the key to tackle the mixed discrete-continuous domain challenge. The connection between the proposed DeepSeaProbLog and the existing work on neural probabilistic programming is nicely explained. Still, here are some of my concerns/suggestions:\n\n- Missing references to some of the current literature on weighted model integration solvers such as [1,2,3]. I think this work would benefit from a discussion on the choice of WMI solvers for performing inference in DeepSeaProbLog. For example, how different WMI solvers would support different inference performances of DeepSeaProbLog.\n- The proof of Prop 4.1 refers to Zuidberg Dos Martires et al. (2019) while it is unclear which results in Zuidberg Dos Martires et al. (2019) is related to the conclusion that Eq C.3 is indeed a weighted model integration problem.\n- Missing comparison in the pure discrete setting. When DeepSeaProbLog is applied to a pure discrete setting, there should be a bunch of neural probabilistic programming benchmarks as well as baselines for comparison. The authors might want to put such an empirical comparison to illustrate the discrete reasoning capability of DeepSeaProbLog in such settings.\n- In Sec 3.2, it seems that one limitation of DeepSeaProbLog is that each distributional fact must define a different random variable. I wonder why such an assumption is necessary. Also, are there any distributional assumptions on the continuous variables? It seems that the continuous variables are all assumed to be Gaussian.\n- In the neural-symbolic VAE experiment, it would be more convincing to include an ablation study where the VAE has no difference constraint but is still trained with difference as addition input. This ablation study is necessary since it might be possible that the VAE might simply learn the digit pair conditioned on the difference label and such an ablation study would help to see how much the DeepSeaProbLog help improve accuracy.\n- Another issue in the neural-symbolic VAE experiment is that when it shows that DeepSeaProbLog is able to answer conditional generative queries, only one example is presented. This can be further improved by presenting some metrics such as accuracy to measure the performance of answering such queries.\n\n[1] P. Morettin, A. Passerini, and R. Sebastiani. Efficient weighted model integration via SMT-based predicate abstraction. In IJCAI, 2017.\n[2] Z. Zeng, P. Morettin, F. Yan, A. Vergari,\nand G. Van den Broeck. Probabilistic inference with algebraic constraints: Theoretical limits and practical approximations. In NeurIPS, 2020.\n[3] Z. Zeng, P. Morettin, F. Yan, A. Vergari,\nand G. Van den Broeck. Scaling up hybrid probabilistic\ninference with logical and arithmetic constraints via message passing. In ICML, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is overall well-written and the contribution is solid and novel to me.",
            "summary_of_the_review": "The proposed DeepSeaProbLog is novel to me. However, my main concern is the empirical evaluation not being extensive and not so convincing.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_zi3P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6326/Reviewer_zi3P"
        ]
    }
]