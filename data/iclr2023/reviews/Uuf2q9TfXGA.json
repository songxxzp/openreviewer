[
    {
        "id": "2UCozqHYTmk",
        "original": null,
        "number": 1,
        "cdate": 1665794054568,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665794054568,
        "tmdate": 1665877118844,
        "tddate": null,
        "forum": "Uuf2q9TfXGA",
        "replyto": "Uuf2q9TfXGA",
        "invitation": "ICLR.cc/2023/Conference/Paper5565/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper investigated the superior performance of ensemble, knowledge distillation and self-distillation in the field of deep learning. As previous principles of ensemble on traditional machine learning algorithms do not apply to deep learning methods, a novel perspective of \"multi-view\" data was proposed to explain the phenomenon. Both empirical and theoretical analysis support multi-view theory for ensemble in deep learning.",
            "strength_and_weaknesses": "Strengths:\n- The paper analyzed an underexplored problem of how ensemble works in deep learning methods. It pointed out contradictions when applying previous principles of traditional algorithms and shed light on understanding deep learning.\n- For the theory side, the analysis was rigorous with clear definitions and concrete examples to help better digest the theorems.\n\nWeaknesses:\n- I am just curious whether the hypothesis of multi-view data structure can be extended to different data structures such as texts and graphs. Those data are very different from image data discussed in this paper and 'multi-view' might not be applicable to them.\n- It is not a good practice to include \"self-distillation\" in the title but defer all details about it to the appendix, which was somewhat misleading even though it was due to page limit. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clearly written and well organized. As it was primarily a theory paper, it was fine to include few empirical results in the main paper. The multi-view data structure was novel to the community and made the first attempt to interpret how ensemble and knowledge distillation works in the field of deep learning. In addition, the theoretical analysis was rigorous except for some minor typos in Contradiction 2 in page 2, where $f_i$ and $g_i$ were used mixedly.",
            "summary_of_the_review": "The paper made an attempt to explain the superior performance of ensemble, knowledge distillation and self-distillation with the theory of \"multi-view\" data structure and provided insights on understanding those techniques in deep learning. The analysis is rigorous and thus I tend to accept it.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_ZnbC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_ZnbC"
        ]
    },
    {
        "id": "uA3o3TQt7hc",
        "original": null,
        "number": 2,
        "cdate": 1666584829119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584829119,
        "tmdate": 1666584829119,
        "tddate": null,
        "forum": "Uuf2q9TfXGA",
        "replyto": "Uuf2q9TfXGA",
        "invitation": "ICLR.cc/2023/Conference/Paper5565/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the theories of ensemble and knowledge distillation. Insightful studies regarding NTKs and deep neural networks are performed and novel theoretical results based on the multi-view structure of data are established. I get many new insights after reading this paper.",
            "strength_and_weaknesses": "# Strength\n- The writing and presentation are good. It is a pleasure to read this paper. \n- The empirical results on neural tangent features and deep neural networks are novel and interesting. The authors then provide thorough explanations for them, which is appreciated.\n- The theoretical results are rigorous.\n\n# Weaknesses\n- We know new theories usually motivate new algorithms. I want to know if there are some potential future works to better exploit these theories. ",
            "clarity,_quality,_novelty_and_reproducibility": "All aspects are good.",
            "summary_of_the_review": "This is an outstanding paper in multiple aspects, so I suggest acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_GAwQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_GAwQ"
        ]
    },
    {
        "id": "FsEoteuji3y",
        "original": null,
        "number": 3,
        "cdate": 1666587366171,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666587366171,
        "tmdate": 1666587366171,
        "tddate": null,
        "forum": "Uuf2q9TfXGA",
        "replyto": "Uuf2q9TfXGA",
        "invitation": "ICLR.cc/2023/Conference/Paper5565/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the theoretical explanation of why the ensemble and knowledge distillation works for the deep learning model. It shows the different behavior of classic parameterization of neural networks and the random feature model (e.g. NTK). It then explains via the idea of multi-view data that the network infers via generalizable learning of a subset of features and memorizing another subset of data. Such analysis differs from the classic analysis that decomposes optimization and generalization.\n\nOverall, I find the insight of the paper quite interesting, and such intuition is well supported by the empirical evidence using the practical network. The construction of the multi-view data for analysis is closely related to the practical settings.\n",
            "strength_and_weaknesses": "Strength:\n\n1. The novelty of the paper is quite strong. I believe the such analysis is new and sound.\n\n2. There is quite a good match between theory and practice, especially the construction of the multiview dataset is an exciting and meaningful proxy for the practical case while being theory-friendly.\n\n3. The writing of the main context is well organized. It is easy for the reader to get to know the most critical intuition of the theory.\n\n4. I like the experiment used in this paper. It is concise but well supports the theory.\n\nWeakness:\n\n1. I am not able to open the supplementary of the paper but reviewing this paper, reading the appendix is very necessary and thus my reading of the appendix is via the arxiv. The structure of the appendix is good but I feel a more simplified overall technical/intuition is desirable.\n\n2. To obtain such a theory, there are still a lot of 'artificial' in the problem setup such as the use of smoothed ReLU, and specific restrictions of the data distribution. Those slightly increase the gap between theory and practice. But this point does not downgrade my point, understanding deep learning in theory is very hard and this paper has done excellent work toward that.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nQuality: Good\nNovelty: Godd",
            "summary_of_the_review": "Questions:\n\n1. Almost all the result suggests a 100% training accuracy but this is different from almost 0 cross-entropy loss. I feel all the results will not hold if the model is large and trained with almost zero cross-entropy loss? At least, in this case, the distillation via logits seems to be no more useful. Pls correct me if I am wrong. \n\n2. Following 1, how do the results look like if we consider regression rather than classification?\n\n3. Following 1 and 2, I was curious if we do distillation on the latent feature space rather than logit space, will the result become very different? From the feature learning perspective, distill on the feature space seems more reasonable?",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_qyvd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_qyvd"
        ]
    },
    {
        "id": "nEOoJqx-z7W",
        "original": null,
        "number": 4,
        "cdate": 1666646303182,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646303182,
        "tmdate": 1670342851033,
        "tddate": null,
        "forum": "Uuf2q9TfXGA",
        "replyto": "Uuf2q9TfXGA",
        "invitation": "ICLR.cc/2023/Conference/Paper5565/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper takes a step toward understanding ensemble and knowledge distillation. The authors consider the challenging setting where the teacher model is an average of several models of the same structure, or even the teacher has an identical structure as the student model. The authors developed a theory that the distillation from several independently trained neural networks can improve the performance when the data has a \"multi-view\" structure. ",
            "strength_and_weaknesses": "Strength:\n\n1. The authors present theoretical results showing that a single model is guaranteed to have 0 training error while having a high testing error with high probability; the ensemble can provably improve the testing accuracy.\n\n2. The author shows that the ensemble can be efficiently distilled into a single model. This understanding is fundamentally different from the standard NTK settings.\n\n3. The idea of \"multi-view data\" is intuitive and provides a natural and convincing explanation for various empirical observations for distillation.\n\nQuestions and weaknesses:\n\n1. One major concern is that the supplementary material cannot be opened due to a file format error, and I'm not able to see the results of self-distillation and empirical evaluation, which seem to be important aspects of this work.\n\n2. In the example of Section 2.3 and for a single model, why the network can only pick up one of the features in $v_1, v_2$ with the 90% of the data, and only memorize the remaining 10% of the data, instead of learning from the other relevant feature in $v_1, v_2$?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of \"multi-view\" is interesting, and provides an intuitive explanation for various observations in distillation.",
            "summary_of_the_review": "This paper presents an interesting and intuitive explanation for distillation. However, one major concern is that I'm not able to open the supplementary material due to some file format error. I'm not able to fully assess this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_Y3qu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5565/Reviewer_Y3qu"
        ]
    }
]