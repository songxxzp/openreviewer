[
    {
        "id": "wQyX2APbM9",
        "original": null,
        "number": 1,
        "cdate": 1666463726862,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666463726862,
        "tmdate": 1666463726862,
        "tddate": null,
        "forum": "fwn2Mqpy4pS",
        "replyto": "fwn2Mqpy4pS",
        "invitation": "ICLR.cc/2023/Conference/Paper4066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new GNN architecture, omega-GNN, which learns a filtering coefficient for each feature channel and each layer. The introduced omega parameter scales the graph convolution and thus enables \"sharpening\" operation in addition to the \"smoothing\" performed by traditional GNNs. By properly setting the omega parameters, the authors claim that omega-GCN does not oversmooth. A more expressive variant performing channel-wise scaling has been proposed and instantiated on the GCN and GAT backbones. Extensive experiments have been performed on both node-level and graph-level tasks to show the accuracy improvements. ",
            "strength_and_weaknesses": "## Summary of strengths\n\n+ The paper is well-structured and easy to read. The motivation is clearly stated and the problem is well defined. \n+ The overall architecture design is reasonable. \n+ Extensive experiments have been performed on many datasets and across different tasks. \n\n## Summary of weaknesses\n\n- Theoretical proof is problematic. The derivation is based on an improper discretization process that results in the residue error to accumulate. \n- The significance of the theoretical results is limited. First of all, it only discusses the basic version of omega-GNN with shared omega across channels. Further understanding is needed regarding the channel-wise scaling. Secondly, the analysis only discusses the prevention of oversmoothing. Many existing works can already address oversmoothing and it is not clear what are the advantages brought by omega-GNN. \n- Scaling the propagation matrix either channel-wise or layer-wise has already been proposed in the literature. \n- For experiments, most baselines are relatively old. Some more recent SOTA methods are missing. e.g., [a] which also addresses oversmoothing. \n\n## Detailed comments\n\n### Proof of Theorem 1 and Corollary 1 \n\nFirst, there is no guarantee that the Dirichlet energy of a multi-layer omega-GNN can converge to a target energy level, $E_{opt}$. It is true that Equation 16 defines a gradient descent operation with the Dirichlet energy as the minimization objective. However, iteration by Equation 16 is based on a constant step size omega, which is independent of both $E_{opt}$ and the iteration number (i.e., layer number). It is likely that $E(l+1) < E_{opt} < E(l)$ and thus there does not exist a layer $l$ to exactly achieve $E_{opt}$. \n\nSecond, the process of converting discrete form of Eq. 16 to the continuous form of Eq. 17, and then converting back to the discrete form of Eq. 20 is problematic. The discretization process introduces errors depending on omega, and such error accumulates with the layer number. Eventually, the second order term accumulated across the full GNN becomes a first order term and is not negligible. \n\nSpecifically, let $M = \\tilde{D}^{-1/2}L\\tilde{D}^{-1/2}$. Then Eq. 20 becomes \n\n$f(t_{l}) = exp(-\\omega M)f(t_{l-1}) = exp(-L\\omega M)f(t_0)$\n\nThe target $\\omega$ value is derived by setting $L\\omega=T$. \n\nHowever, if we look at the Taylor expansion:\n\n$f(t_{l+1}) = (I-\\omega M)f(t_l) + O(\\omega^2) =  (I-\\omega M)^2 f(t_{l-1}) + (I-\\omega M)O(\\omega^2) + O(\\omega^2)= ...$\n\nWhen the recursion is applied from $f(t_L)$ until $f(t_0)$, there will be $L$ terms of $O(\\omega^2)$. Since $L=T/\\omega$, then $L\\cdot O(\\omega^2) = O(\\omega)$ and the residue error becomes non-negligible due to the recursive expansion. \n\nTherefore,  Theorem 1 and Corollary 1 are problematic. \n\n### Related works with similar ideas\n\n* The idea of layerwise scaling the propagation matrix is known. Could you please discuss how omega-GNN improves methods such as GCNII?\n* Channel-wise scaling the propagation matrix is also known. For example, in [b]. Please discuss the relation between omega-GNN and [b]. \n* Oversmoothing is a classic issue and there have been many solutions proposed, from many different perspectives. In addition to the ones in Sec 3, [a] also provably avoids oversmoothing without changing the GCN architecture. The theoretical analysis presented in the paper mostly focuses on the oversmoothing perspective. Could you please illustrate the advantages of addressing oversmoothing via omega-GNN compared with the many existing solutions?\n\n### Experiments\n\nThanks for evaluating omega-GNN with many graphs and baselines. It would be more convincing if more recent SOTA methods (e.g., [a]) are included. \n\nFigure 4 does not really demonstrate the similarity between solid and dashed lines. I'm not sure if it can really validate the theorems. \n\n\n## References\n\n[a] Decoupling the depth and scope of graph neural networks. In NeurIPS 2021.\n\n[b] How Powerful are Spectral Graph Neural Networks. In ICML 2022. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written with the problem well-defined and techniques precisely described. \n\nMy main concern on the quality of the paper is in its theoretical proof. See above. \n\nI think the design is not entirely novel. As mentioned above, both layer-wise and channel-wise scaling of the propagation matrices have been proposed in the literature. \n\nThe experiments include reasonable amount of details on the setting and hyperparametes. The authors also provide source code in the supplementary materials. ",
            "summary_of_the_review": "In summary, I think this is a well-written paper with reasonable architecture design. The idea bears similarity with a few existing works. The theoretical analysis seems problematic. And it would be better to also (informally) analyze the benefits beyond addressing oversmoothing -- a phenomenon already with many solutions. \n\nOverall, in its current form, I think the paper is still below the bar of acceptance. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_shzp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_shzp"
        ]
    },
    {
        "id": "ZXGyT2Ehvy2",
        "original": null,
        "number": 2,
        "cdate": 1666630802244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630802244,
        "tmdate": 1666630802244,
        "tddate": null,
        "forum": "fwn2Mqpy4pS",
        "replyto": "fwn2Mqpy4pS",
        "invitation": "ICLR.cc/2023/Conference/Paper4066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper attempts to address the shortcomings in GNNs by modifying the propagation operator and thus propose an approach called $\\omega$GNN, as two instances, $\\omega$GCN and $\\omega$GAT. It is justified that $\\omega$GNN can prevent over-smoothing issue, and the parameterized propagation operator in $\\omega$GNN enable variant operators to use. Extensive experiments are conducted and shown superior accuracy. \n",
            "strength_and_weaknesses": "The strengths of the paper: \n\n+ It is quite interesting to develop a parameterized graph convolution operator that is able to prevent the over-smooth issue and is able to implement variant operators. \n\n+ The empirical evaluation is extensive and the experimental results show notable improvements. \n\nThe weaknesses of the paper: \n\n- Note that the graph convolution operation in GCN is a step of gradient-based node updating to optimize an objective in label propagation. Here, a parameter $\\omega$ introduced in Eq. (2), serves as a step size parameter. If the parameter $\\omega$ is learnable, does it still converge in the perspective of understanding the graph convolution as optimizing a label propagation objective (e.g. Ma et al.'21)? Where does it converge to? \n\nRef. \n[a] Yao Ma et al. 2021. A Unified View on Graph Neural Networks as Graph Signal Denoising. \n\n- Note that, the parameter $\\omega$ is a step size to control the degree of incorporating the graph convolution to modify the node feature. Of course, using a smaller $\\omega$, the smoothing issue could be slower as shown in Fig.3. However, it does not resolve the smoothing issue at all. When using a smaller $\\omega$, more layers of graph convolusions are needed to yield an equivalent node feature. Thus, it is NOT to avoid over-smoothing issue at all ---the over-smoothing issue is still hidding in the problem formulation! \nLooking into the curve in Fig.3(b) for $\\omega$GAT, it seems not showing a convergence. If many iteratons (i.e., layers) are conducted, does it converge or not? \n\n- It is interesting to have a theorem to justify the convergence for the proposed methods. However, the Theorem 1 strongly relies on the assumption that there is some optimal Direchlet energy of the final feature map that satisfies $0 < E_{opt} (f^{(L)}) < E(f^{(0)})$. Does it really exist an optimal value that is strictly greater than 0? In such case, what is the condition on the property of the objective function or on the graph Laplacian? Is the implicit optimization problem a well-defined, in the sense that not having a trivial solution? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-prepared, clearly written and easier to follow. Introducing a parameter into the graph convolution operator is novel and meaningful. ",
            "summary_of_the_review": "The paper is well-motivated and clearly written. The idea is interesting and novel. The empirical evaluation is extensive. Nevertheless, it is suspecious that the contribution is somewhat over-claimed. To be specific, the over-smoothing issue is just alleviated, but not resolved yet. For details, please refer the \"weaknesses\". ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_cjVH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_cjVH"
        ]
    },
    {
        "id": "p6OkeWT3D6",
        "original": null,
        "number": 3,
        "cdate": 1666638660800,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638660800,
        "tmdate": 1666638660800,
        "tddate": null,
        "forum": "fwn2Mqpy4pS",
        "replyto": "fwn2Mqpy4pS",
        "invitation": "ICLR.cc/2023/Conference/Paper4066/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Simple changes are proposed to overcome expressivity limits of GNN propagation. The changes involve new parameters $\\omega$, which very naturally allow both smoothing-type propagations and sharpening-type propagations. Instantiations of the proposed changes in GCNs and GATs are empirically tested in both node and graph level tasks.\n\n\n\n\n*Correctness\n3\n\n*Technical Novelty and Significance\n2\n\n*Empirical Novelty and Significance\n2\n\n*Flag for ethics review:\nNo\n\n*Recommendation\n3\n\n*Confidence\n4\n",
            "strength_and_weaknesses": "Strengths:\n1. Simple approach that is easy to implement\n2. Use of multiple omegas gives a flexible class of operators in an efficient manner\n3. Experiments on both node and graph classification tasks, while other works often do one or the other.\n\nWeaknesses:\n1. Should cite [1], which analyzes oversmoothing using the Dirichlet energy.\n2. Missing citations to other mixed sign propagations in GNNs, this is done by others besides Eliasof et al. 2022. See [2], [3], [7] for prominent examples. Moreover, this is also mentioned in [4]. Please compare and explain the differences.\n3. Why would you not also run your method on the Squirrel dataset in Pei et al. 2020?\n4. The datasets tested on for the most part have been noted to have a lot of issues. For instance, the Pei et al heterophily datasets were covered in [5] and Cora + Citeseer + Pubmed were covered in [6]. New datasets were introduced, which your method is clearly scalable enough to run. Further, many methods do quite well on PPI, and your gain is only 00.02 better than the next best method. Also, there are better baselines that outperform your method on ogbn-arxiv, such as Correct and Smooth [8] from 2020.\n\n\nOther notes:\n1. Should be $\\vec \\omega^{(l)} \\in \\mathbb{R}^c$ right before equation (3)\n\n[1] Cai and Wang. A Note on Over-Smoothing for Graph Neural Networks. https://arxiv.org/abs/2006.13318\n\n[2] Bo et al. Beyond Low-frequency Information in Graph Convolutional Networks. https://arxiv.org/abs/2101.00797\n\n[3] Yan et al. Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks. https://arxiv.org/abs/2102.06462\n\n[4] Di Giovanni et al. Graph Neural Networks as Gradient Flows: understanding graph convolutions via energy. https://arxiv.org/abs/2206.10991\n\n[5] Lim et al. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods. https://arxiv.org/abs/2110.14446\n\n[6] Shchur et al. Pitfalls of Graph Neural Network Evaluation. https://arxiv.org/abs/1811.05868\n\n[7] Yang et al. Diverse Message Passing for Attribute with Heterophily. NeurIPS 2021\n\n[8] Huang et al. Combining Label Propagation and Simple Models Out-performs Graph Neural Networks. https://arxiv.org/abs/2010.13993\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity of writing is in my opinion quite good. The novelty is not as large as claimed, as noted in the strengths + weaknesses section. The authors should contrast their method with the other mixed-sign methods that they did not initially cite.",
            "summary_of_the_review": "It is good to see simple and reasonable changes to GNNs that can help with some of the known issues of these models. However, the exact novelty is unclear, as several other similar methods (some of which are quite prominent) have been proposed. Moreover, the choice of datasets is not great for showing significant empirical benefits.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_UJFL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_UJFL"
        ]
    },
    {
        "id": "B3AAI2SQZC",
        "original": null,
        "number": 4,
        "cdate": 1667334584054,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667334584054,
        "tmdate": 1667334584054,
        "tddate": null,
        "forum": "fwn2Mqpy4pS",
        "replyto": "fwn2Mqpy4pS",
        "invitation": "ICLR.cc/2023/Conference/Paper4066/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an extension to GCN-style GNNs, where the propagation operator is extended with a \u03c9 factor. This allows for expressing non-smoothing transformations. The authors show theory backing the design of their model, and then perform a sequence of experiments to show its effectiveness. ",
            "strength_and_weaknesses": "=== Strengths === \n\n(S1): The addition of the \u03c9 component to the base GNNs is clearly explained and motivated, with useful visualization showing how different values of \u03c9 induce different kinds of behaviour. \n\n(S2): The theoretical results, showing how repeated application of the propagation operator with a given value of \u03c9 corresponds to gradient descent steps on an appropriately defined energy, are interesting. \n\n(S3): The paper is well-written and easy to read. \n\n \n\n=== Weaknesses === \n\n(W1): The paper seems to be biased towards GCN-style GNNs, and largely omits generalized message-passing-based ones. It focuses on GCN and GAT, as these are the kinds of GNNs where most of the statements made in this work apply. However, GNNs based on general message passing don't seem to be affected by the pitfalls discussed here. GCN in its plain form usually doesn't work very well anyway, due to being rather constrained (as the authors note), and often more flexible variants such as GGNN [1] or PNA [2] work better (in practice, the latter is a good default choice). Both the message function and the update function can be implemented with arbitrary MLPs, and as far as I understand, GNNs that do so don't suffer from the \"non-negativity\" problems that the authors seek to solve. The statement that the propagation is often shared across channels doesn't seem to apply to things like GGNN, while the statement that it is shared across layers doesn't seem to apply to most practical models (it seems un-tied weights tend to work better due to higher flexibility?). Things like PNA are available in `pytorch_geometric` (which the authors use), so a direct comparison could be straight-forward. In any case, I think the discussion could be adjusted to better position this work in relation to a larger body of papers on GNNs, noting how these other models don't suffer from the problems the authors focus on. \n\n(W2): While the empirical results look good compared to the models included in the tables, I'm not sure to what extent are these comparisons exhaustive. For example, [3] shows better results on PROTEINS, while [4] better results on ogbn-arxiv. Did the authors intend to compare to all the available results? Claiming that e.g. \u03c9GNNs are SotA among a particular style of GNNs is a reasonable thing to aim for too, but I'd like to understand exactly what the claim here is. \n\n \n\n=== References === \n\n[1] Li et al., \"Gated Graph Sequence Neural Networks\" \n\n[2] Corso et al., \"Principal Neighbourhood Aggregation for Graph Nets\" \n\n[3] Zhang et al., \"Hierarchical Graph Pooling with Structure Learning\" \n\n[4] Sun et al., \"Adaptive Graph Diffusion Networks\" \n\n \n\n=== Nitpicks === \n\nHere I include some final nitpicks, which did not affect my score; they are here just to help improve the paper. \n\n- \"indistinguishable from one and other\": maybe \"one another\"? \n\n- missing space before parenthesis in Section 2.1 \n\n- first sentence of Section 4.2 reads off ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is very clearly written and easy to understand. \n\n \n\nQuality: The theoretical and empirical results are thorough, although I am not sure about the exhaustiveness of the comparisons (see (W2)). \n\n \n\nReproducibility: The results looks reproducible and a lot of experimental details are provided. ",
            "summary_of_the_review": "While from the theoretical side the work looks strong, I am not yet sure if it's SotA in practice when compared to all the available results, and the current discussion of the GNN landscape seems biased. Therefore, for my initial evaluation I lean towards rejection, but I'm happy to adjust it after discussions with the authors and the other reviewers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_ZYQh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4066/Reviewer_ZYQh"
        ]
    }
]