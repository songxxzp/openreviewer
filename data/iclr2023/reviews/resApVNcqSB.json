[
    {
        "id": "biZ1teIVPyQ",
        "original": null,
        "number": 1,
        "cdate": 1666512102685,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512102685,
        "tmdate": 1666512102685,
        "tddate": null,
        "forum": "resApVNcqSB",
        "replyto": "resApVNcqSB",
        "invitation": "ICLR.cc/2023/Conference/Paper933/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work addressed the problem of HOI detection tasks under a weakly-supervised learning paradigm. The key inspiration is to leverage the rich vision-language model (i.e., CLIP) to improve the model learning on highly noisy and long-tailed HOI datasets. The key novelties are the novel local relation-specific HOI representation and the relatedness classification terms. The experimental results show that the proposed approach improves the corresponding baseline. ",
            "strength_and_weaknesses": "Strength:\n+ This work acknowledges the rich information encoded in the pre-trained CLIP model and proposes several modules to better leverage such information for the HOI detection task. This include (1) HOI knowledge bank, and (2) knowledge transfer network. \n+ Design a local relation-specific HOI representation that transfer relation-level semantic knowledge from CLIP to pair-wise representations. \n+ Design a model-guided relatedness classification loss term (i.e., some form of trustworthiness score) to address the problem in weakly supervised learning. \n+ The paper conducted several ablation studies to empirically show the efficacy of the proposed modules. Improvement on rare classes is more significant, demonstrate the benefit of having strong pre-trained model.\n\nWeakness:\n- The technical novelty of this work is relatively weak. The relatedness classification loss term is very straightforward and used in earlier work (e.g., using a threshold to remove low-quality prediction). In addition, this work largely benefits from the strong CLIP model (e.g., HOI knowledge bank, visual encoder, and text encoder). Effectively, the long-tailed issue is partially addressed as the encoder is trained with external dataset. The strength is that this work shows how to better leverage the CLIP model in the context of HOI detection. But I would argue that the technical novelty is welcome but limited.  \n\nOther concerns:\n- Introduction highlights that JOI annotations are prone to image-level labelling errors. Can the author elaborate on how can the model deal with labelling errors if it is presented in training data? \n- In Fig 1, please clearly annotate which are the HOI recognition network (is it both local and global branch?)\n- In Section 3.2, \"The global branch includes a backbone network (Sec.3.2.1) that generates human-\nobject proposals and extracts image features\". The human-object proposals seems to be generated with a pre-trained object detector and provided as an input to global brunch, please clarify this. \n- Please comment about the baseline model in ablation study. It seems that the performance is actually higher than the PPR-FCN (which use the same backbone) in Table 1. Is there some clear factors that give the baseline a higher performance?\n- Please change \"strongly supervised learning\" to \"supervised learning\". The term is well established in the community in the past decades. ",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed approach is detailed in the manuscript and should be able to reproduce without concerns. The author promise to release all codes and trained model for reproducibility purposes. ",
            "summary_of_the_review": "Overall, this paper is well-written and easy to follow. Generally, the work is highly beneficial from the strong visual-language model, where the object semantics are better learned in the pre-trained model. The key contribution of this work is to better exploit CLIP embedding for learning relational information. The designed model is intuitive and provides insights on how to leverage such information for a specific task. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper933/Reviewer_frLC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper933/Reviewer_frLC"
        ]
    },
    {
        "id": "m-UNBWXKjJe",
        "original": null,
        "number": 2,
        "cdate": 1666960589831,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666960589831,
        "tmdate": 1666960589831,
        "tddate": null,
        "forum": "resApVNcqSB",
        "replyto": "resApVNcqSB",
        "invitation": "ICLR.cc/2023/Conference/Paper933/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "This paper proposes a bi-level knowledge integration strategy that incorporates the prior knowledge from CLIP for weakly-supervised HOI detection.\n\nThis paper also exploits CLIP textual embeddings of HOI labels as a relational knowledge bank, which is adopted to enhance the HOI representation with an image-wise HOI recognition network and a pairwise knowledge transfer network.\n\nThis paper further proposes the addition of a self-taught binary pairwise relatedness classification loss to overcome ambiguous human-object association.\n\n",
            "strength_and_weaknesses": "The proposed approach achieves the new state of the art on both HICO-DET and V-COCO benchmarks under the weakly supervised setting.",
            "clarity,_quality,_novelty_and_reproducibility": "There is a lack of contribution and novelty.",
            "summary_of_the_review": " It lacks any innovative contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper933/Reviewer_q7LZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper933/Reviewer_q7LZ"
        ]
    },
    {
        "id": "S2_FCCIwpuo",
        "original": null,
        "number": 3,
        "cdate": 1667042882464,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667042882464,
        "tmdate": 1668780403048,
        "tddate": null,
        "forum": "resApVNcqSB",
        "replyto": "resApVNcqSB",
        "invitation": "ICLR.cc/2023/Conference/Paper933/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "For weakly-supervised HOI identification, this paper provides a bi-level knowledge integration technique that integrates the prior information from CLIP. To augment the HOI representation using an image-wise HOI recognition network and a pairwise knowledge transfer network,  the authors specifically use CLIP textual embeddings of HOI labels as a relational knowledge bank. To get over unclear human-object connection, the authors also suggest including a self-taught binary pairwise relatedness classification loss. Finally, in a weakly supervised context, the proposed method surpasses the previous state of the art on the HICO-DET and V-COCO benchmarks.",
            "strength_and_weaknesses": "Strength\n- The method achieves the sota performance among weak-supervised hoi detection benchmark.\n- Utilizing the prior knowledge from encoded prompt feature bank to enhance the human-object feature is interesting\n\nWeaknesses\n- Literature review should be enhanced, e.g., [1].\n[1] Weakly Supervised Learning of Interactions between Humans and Objects\n- From my humble view,  the score fusion is much better since union region brings lots of noise. Therefore, using the union region feature as  query to get attention score still be noisy.\n- It seems that over 50 percent images of HICO\u2014DET only contains one human-object pair (pls correct me if wrong). Therefore, HICO-DET is probably not the best choice to validate idea, especially for ablation study. \n- From my prospective, the main contribution of the paper is exploring how to introduce CLIP text prior but it is not strong related to weakly supervised learning. How about the result if taking into account fully supervision? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality, clarity and originality are all good to me.",
            "summary_of_the_review": "This paper looks good to me. The main concern is that CLIP text prior seems not to be strongly related to weakly supervised learning considering pseudo labels contribute only a little improvement.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper933/Reviewer_RZBb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper933/Reviewer_RZBb"
        ]
    },
    {
        "id": "otcC-Au4uv",
        "original": null,
        "number": 4,
        "cdate": 1667062005032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667062005032,
        "tmdate": 1669296462233,
        "tddate": null,
        "forum": "resApVNcqSB",
        "replyto": "resApVNcqSB",
        "invitation": "ICLR.cc/2023/Conference/Paper933/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new weakly supervised method in the human-object interaction task. The method exploits the prior knowledge from CLIP.",
            "strength_and_weaknesses": "[Strength] \nThe idea of incorporating external knowledge from CLIP is interesting. \n\n[Weakness]\n1. The authors promise to release source codes. This would eliminate the issue of reproducibility. However, the paper presentation is bad, which makes readers hard to fully understand and hence, potentially hard to reproduce the model based on the main text.\n\n2. The paper presentation is bad (see more points below).\n\n3. There exists an unfair comparison with baselines in the experiments (see elaborations below).\n\nI elaborated on the specific points below:\n[clarity] \nThe paper presentation is very bad. I have trouble understanding some parts of the model designs and the problem setting. The authors are strongly encouraged to explicitly list out what are the supervised signals in the problem setting in both fully supervised and weakly supervised cases. Please clearly define every math notation before using them. I list out several key confusions:\n\n1.1. What is the detection score s_h defined in sec 3.1? Ground truth bounding box coordinates? How many scores are there?\n\n1.2. Define relatedness and interaction scores in the introduction. \n\n1.3. Add vg defined in sec3.2.1 to fig1. Also including several other key variables in Fig1 would help readers understand the designs better.\n\n1.4. I have trouble understanding this relatedness score. Up to 3.2.5, there is no definition of sb. What is the relatedness score exactly? Is this binary score given as ground truth? If so, the authors should spell it out at the beginning of the paper.\n\n1.5. In sec3.3, it talks about self-taught relatedness classification loss. What is exactly given in this case? If binary cross-entropy loss is used and bm is given, this relatedness acts as the ground truth and it should be explicitly introduced in the problem setting in Sec 3.1.\n\n1.6. what is given in the weakly supervised setting? What are the image-level annotations only? Go back to the tuples O defined in Sec3.1, what is exactly missing or not given in a weakly supervised setting?\n\n\n1.7. In table1, what are APS1 and S2 in the rightmost column?\n\n[other questions]\n2. I am familiar with scene graph generation. HOI seems to be a subset of that problem. Would this weakly supervised method generalize to scene graph generation?\n\n3. I found the comparisons of methods are unfair and the results are puzzling. In table 1, baselines are using different backbones and object detectors. If so, it is very hard to say whether the performance improvement is simply because of the change of backbones and detectors, or because the proposed weakly supervised methods are efficient.\n\n4. Utilizing the external knowledge from CLIP is an interesting idea! However, would this put other methods at disadvantage as CLIP has been presented with more data than existing methods?",
            "clarity,_quality,_novelty_and_reproducibility": "I have problems with the paper presentation (points above in weakness).\nThe paper is hard to follow. Many notations are used without clear definitions beforehand.",
            "summary_of_the_review": "The paper presentation is bad which creates troubles for me to fully understand the method section.\nThis subsequently creates problems for me to appreciate the novelty of the method.\nI am also having issues with the baseline comparisons. I do not think it is fair to some extent (see elaborations above).\n\nI would vote for REJECT. However, it is possible that I might revise my ratings based on the authors' feedback after the rebuttal period.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "none as far as i can see",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper933/Reviewer_qCHU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper933/Reviewer_qCHU"
        ]
    },
    {
        "id": "FHX-vXV9CU9",
        "original": null,
        "number": 5,
        "cdate": 1667159472300,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667159472300,
        "tmdate": 1667170524770,
        "tddate": null,
        "forum": "resApVNcqSB",
        "replyto": "resApVNcqSB",
        "invitation": "ICLR.cc/2023/Conference/Paper933/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper targets on HOI detection in a weakly supervised manner (i.e., only image description is provided, no instance-level annotation). To improve the generalization towards fine-grained human-object interactions and mitigate the biased learning caused by imbalanced data distribution, this paper proposes a training strategy to distill the knowledge from CLIP model. Specifically, they incorporate CLIP in HOI at both the image and the instance levels. At the image level, they generate a global HOI knowledge bank and predict image-level HOI descriptions. At the instance/object level, the attention mechanism is employed to enrich the representation of HOI features and then improve the performance. A comprehensive evaluation is conducted and a detailed analysis is provided. ",
            "strength_and_weaknesses": "Strength:\n1.\tThe motivation is clear. Applying CLIP in weakly supervised learning and facilitating the learning of an HOI detector is essential. \n2.\tThe approach is interesting. Though combining the detector with CLIP has been studied, but combining them for weakly-supervised learning has been less explored.\n3.\tThe experimental study is promising and comprehensive. The visualizations are also provided.\n4.\tThe ablation study is in detail.\n\nWeakness:\n1.\tRegarding the whole framework, which part is vital for using CLIP to guide weakly supervised learning? I think the discussion is necessary (but I didn\u2019t find clear answer in the discussion) and help this paper to be distinguished from the other related work.\n2.\tThe knowledge bank is based on classes appearing in the full dataset and is defined by the text. Can you explain how the size of the knowledge bank affects performance? After all, when there exists a brunch of interaction classes, I am not sure about the training efficiency and workload.\n3.\tSRC only does not help much with the detection, according to Table 2. It is not very effective and is kind of counterintuitive. I would recommend providing a more detailed explanation or removing the SRC part.\n4. When a CLIP model is used, it is always necessary to explain the potential issue of fair comparison. After all, CLIP has seen quite a lot of training data during pretraining, and there is a risk of potential data leakage. As such, explanation is necessary.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The approach is new and interesting. It could be useful for weakly-supervised learning.\nThe implementation details are useful for reproduction.",
            "summary_of_the_review": "The overall presentation of this paper is good. However, a few more discussions should be provided. Please see details in weakness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper933/Reviewer_S5GP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper933/Reviewer_S5GP"
        ]
    }
]