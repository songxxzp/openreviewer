[
    {
        "id": "Aa2D80dIPnr",
        "original": null,
        "number": 1,
        "cdate": 1665881840240,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665881840240,
        "tmdate": 1669431859903,
        "tddate": null,
        "forum": "wwRjJScpsOO",
        "replyto": "wwRjJScpsOO",
        "invitation": "ICLR.cc/2023/Conference/Paper866/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a targeted adversarial training method for non-contrastive self-supervised learning (SSL) to improve the performance. Using the proposed algorithm, non-contrastive SSL gets an improvement in the robustness of the downstream tasks.",
            "strength_and_weaknesses": "Strength:\n\nThe paper explains the intuition and numerical experiments clearly.\n\nWeakness:\n\n[1] From the intuition, it seems that the idea behind the proposed method is similar to contrastive learning. Both of them consider separate samples of different properties (e.g, class, or similarity). Could the authors provide some more insights on the difference between the proposed algorithm and the idea behind contrastive learning? Is there any interesting novel thoughts in the new method?\n\n[2] Is there any specific reason on why this paper considers improving non-contrastive SSL? The numerical performance of the proposed algorithm does not outperform compared to contrastive learning. In all Table 3, 4, 5, the new method may either have a similar (or slightly worse) performance compared to contrastive based approach, or has a better robustness while sacrificing the clean testing performance (i.e., clean and adversarial trade-off). \n\nThe authors need to provide some explanation why they want to compare the proposed method only with non-contrastive SSL, but nor contrastive-SSL. For example, the authors may want to explain whether there is any concern to utilize contrastive learning. Or is there any particular area of study which cannot use contrastive learning? Or is there any potential advantage of using non-contrastive SSL? Or can the proposed method be applied in contrastive learning?\n\nIt is acceptable to not consider non-contrastive SSL for a theory paper. But since this paper aims to propose a new method, it is essential to demonstrate that the proposed method is good enough. There may be some scenarios that non-contrastive SSL is used instead of contrastive SSL, but it is not clarified in this paper. In all the experiments in this paper, it seems that we can always use contrastive SSL and get a better result.\n\nPlease also emphasize that the paper works on non-contrastive SSL in the abstract. My first impression was that this paper studies contrastive learning with targeted SSL. It is slightly confusing.\n\n[3] Similarly, the observation in Figure 5 is not apperant.\n\n[4] In the tables, the bold numbers distract readers from catching the groups with the best performance. Please adjust the tables to make them clear.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: the abstract is confusing. Other parts are clear.\n\nQuality: good.\n\nNovelty: the intuition is interesting.\n\nReproducibility: the intuition looks reasonable to me. It says \" The code will be available in Anonymous\" but seems there is no link. The other experiment results from other Github repositories (e.g., RoCL) looks good to me.",
            "summary_of_the_review": "The main concern is to provide some illustration on why we want to consider the proposed algorithm. From all the numerical experiments in the current presentation of the paper, it seems that contrastive-SSL is a good method. The authors may consider adapting the proposed method into contrastive learning to further improve the performance, or provide some motivation why we some times can only use non-contrastive learning. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper866/Reviewer_yLsz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper866/Reviewer_yLsz"
        ]
    },
    {
        "id": "0qzA2TsXoi",
        "original": null,
        "number": 2,
        "cdate": 1666424517613,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666424517613,
        "tmdate": 1666424517613,
        "tddate": null,
        "forum": "wwRjJScpsOO",
        "replyto": "wwRjJScpsOO",
        "invitation": "ICLR.cc/2023/Conference/Paper866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the problem of adversarial self-supervised learning is studied, where the goal is to improve the robustness of the self-supervised model. While adversarial example can be created under the supervised scenario, creating adversarial example is not trivial for self-supervised training because no label is available. This paper addresses this problem by proposing a score function that measure the similarities between unlabeled samples. Given an unlabeled sample, the negative sample can then be selected and then used to guide the direction of the adversarial perturbation. Experiment demonstrates the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strength\n(1).\t The motivation of this work is strong.\n(2).\t Figure 1 clearly illustrates the difference between creating adversarial examples under the supervised setting and unsupervised setting. More specifically, the difference between creating adversarial examples in contrastive based self-supervised learning and positive only self-supervised learning is highlighted.\n(3).\t The overall paper is clear and simple to follow.\n\n\nWeaknesses\n(1).\tWhile the proposed method is useful under the scenario where only positive data is used for self-supervised learning, it is unclear why it cannot be applied to contrastive based self-supervised learning (e.g. SimCLR). If applicable to contrastive based self-supervised learning, how does the proposed method compared to prior works (e.g. RoCL)?\n(2).\tThe author is suggested to explain why CIFAR5 is used in Table 1, instead of Cifar10. Moreover, how does table 1 in this paper compares to the Table 1 in RoCL? If the author is using Cifar 10, are Table 1 in this work comparable to Table 1 in RoCL?\n(3).\tTypically, self-supervised learning assumes that the pretraining feature can be applied to various downstream task. However, for the experiment in Figure3, the author assumes that the downstream task is known and use it to ``probe\u201d and select the optimal range of the similarity score. The author is suggested to explain why such operation make sense in general self-supervised learning scenario. Will the selected similarity score range transfer across dataset? How does the similarity score selection affect the final performance?\n(4).\tIt is unclear why the score function looks like Eq 6. Can we use the cosine similarity or the entropy term only? Which term is more important for the training? Any ablation on the weighted version between two terms?\n(5).\tTable 2 of RoCL also conducts experiment on black box attacks on Cifar10. What is the proposed method on black box attacks?\n\n\nTypo:\n(1).\t\u201cbecase\u201d : typo near table 2\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The manuscript is simple to read and follow.\nQuality: The paper is interesting with strong motivation. However, some clarification and experiments are needed.\nNovelty: The paper is novel from my best understanding.\nReproducibility: The presentation contains many details for reproducing the work.\n",
            "summary_of_the_review": "The author is suggested to address the limitation in the weakness section. It seems that some of the settings in this work are different than the prior works and several ablations are missing. The score might be adjusted if these concerns are not well addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper866/Reviewer_MNdh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper866/Reviewer_MNdh"
        ]
    },
    {
        "id": "iv27hk-BZ7Z",
        "original": null,
        "number": 3,
        "cdate": 1666586738111,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666586738111,
        "tmdate": 1666586738111,
        "tddate": null,
        "forum": "wwRjJScpsOO",
        "replyto": "wwRjJScpsOO",
        "invitation": "ICLR.cc/2023/Conference/Paper866/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a new unsupervised adversarial training method. Unlike previous work, this method leveraged label information and thus be categorized as the targeted attack. The core idea is the target selection algorithm, which is a combination of similarity and entropy. By running adversarial training on such perturbations, the authors show that this method enhances robustness of models trained with semi-supervised learning.",
            "strength_and_weaknesses": "Overall the idea has decent novelty and the overall idea of this paper is easy to grasp. What I like about this paper are:\n1. The motivation is clearly stated: the untargeted attack fails to generate useful adversarial training data for improving robustness.\n2. The experimental results are clearly in line of the arguments.\n\nThe limitations are also quite clear though, including:\n1. The design of the score function (combing similarity and cross entropy) lacks justification. Although authors mentioned that we want to maximize the similarity but still unnatural to me why.\n2. Some details are lacking. For instance the authors mentioned K-means multiple times, but based on what distance metric? How does this affect the training efficiency?\n3. The title seems too broad - this paper concerns about SSL using positive pairs (specifically BYOL and Simsiam), but SSL itself is a very broad term containing many other methodologies as well. So the title might confuse others.\n4. The comparison in the experiments are also concerning: the baseline is based on SimCLR (Jiang, Kim) but this paper is based on BYOL and Simsiam. Could this be the reason causing the performance improve? Authors might want to find more support.\n5. The datasets are a bit small and limited, authors might want to find more realistic datasets to verify.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the clarity is good and the quality is fair. The idea of the paper is easy to follow, the quality is affected by the factors I mentioned above.\n",
            "summary_of_the_review": "Please address some of my points above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper866/Reviewer_y32g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper866/Reviewer_y32g"
        ]
    },
    {
        "id": "zegYKNKqN88",
        "original": null,
        "number": 4,
        "cdate": 1666613626116,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666613626116,
        "tmdate": 1669871716317,
        "tddate": null,
        "forum": "wwRjJScpsOO",
        "replyto": "wwRjJScpsOO",
        "invitation": "ICLR.cc/2023/Conference/Paper866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper discusses the problem of adversarial training for non-contrastive self-supervised learning (NC-SSL) approaches like BYOL.\nThey propose a modification of the loss function that involves an adversarial attack for a pair of objects that, with high probability, lie in different classes.\nSuch a method allows to beat straightforward adversarial training for NC-SSL, but is not Pareto-optimal given a clean and robust accuracy compared to C-SSL methods like SimCLR.",
            "strength_and_weaknesses": "Strengths:\n- The problem the authors propose is interesting and lacks a straightforward solution\n- The authors propose an interesting idea on how to incorporate probably-negative objects in the framework\n\nWeaknesses:\n- The authors only partially solve the adversarial training for NC-CCL. C-SSL methods work better according to my interpretation of the presented results\n- There is no principal explanation of the usage of the approach at hand. There is some experimental analysis, but, e.g. it is missing if we can achieve similar results with different types of losses inspired by similar ideas.\n\nAdditional comments: \nThe idea of selecting a far-away image as a starting point resemble many black-box adversarial attacks.",
            "clarity,_quality,_novelty_and_reproducibility": "Writing and clarity are not the strong points of the paper. The notation is confusing and often wrong. The text misses many important details that hinder the main authors' findings. \n\nTechnical comments:\n- to obtain significantly high robustness -> to obtain significantly *higher* robustness\n- \"SimSiam detaches the gradient on the z\" - it is better to say that it stops gradients for z-part\n- p_2 and z_1 are not defined\n- in formula (6) indexes should be inside the brackets $x)_i$ -> $x_i)$\n- I also suppose that indexes for T_i are not correct in the same formula and around it\n- Table 3 - what is the used metric?\n- I think it is unacceptable to use bold font in Table to articulate the results of your approach, as the bold font is often used to show the best results, which is not the case in most experiments\n- Visualization of embeddings space: I suppose that the authors use tSNE but never mention it",
            "summary_of_the_review": "The idea is good. The method is not good enough from theoretical and practical points of view, with little novelty in the current approach. \nI also note that may be other approaches applied to robustness improvement can work in this case, like SAM or Differential Privacy algorithms with a proper algorithm.\nI suggest authors provide a deeper analysis of why they obtained these results and why NC-SSL fails to provide robust solutions. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper866/Reviewer_ngAb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper866/Reviewer_ngAb"
        ]
    },
    {
        "id": "PocgOeJYCFp",
        "original": null,
        "number": 5,
        "cdate": 1670643901506,
        "mdate": 1670643901506,
        "ddate": null,
        "tcdate": 1670643901506,
        "tmdate": 1670643901506,
        "tddate": null,
        "forum": "wwRjJScpsOO",
        "replyto": "wwRjJScpsOO",
        "invitation": "ICLR.cc/2023/Conference/Paper866/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to improve the robustness of adversarial self-supervised learning (SSL) by leveraging the proposed targeted adversarial data for adversarial SSL. The targeted adversarial data is generated by updating the natural data towards a targeted sample selected by K-mean or similarity. The paper empirically validates that target attacks can improve the adversarial robustness of several previous adversarial SSL methods including BYORL and SimSiam. ",
            "strength_and_weaknesses": "Strength:\n[Motivation for target attacks is clear.] Figure 1 clearly illustrates that targeted adversarial data can help find effective adversarial data in the setting of adversarial SSL with only positive pairs.\n\nWeakness:\n[Writing is poor.]\nThe definitions of some terms in the abstract and introduction like \u201cpositive-only adversarial SSL\u201d are unclear to readers. In addition, Authors cannot assume the reader fully read \u201cBYORL\u201d. The paper does not introduce BYROL in detail. The paper should be self-contained. Besides, please write a full name before using the abbreviations. \n\n[Motivation is unclear.]\nThis paper tries to propose a method to improve adversarial self-supervised learning. There already exists effective adversarial contrastive learning based on SimCLR. However, this paper tries to improve a suboptimal method BYORL. \n\n[Effectiveness of the proposed method is not well validated.]\n-In the introduction, it seems that Authors tried to improve non-contrastive methods since non-contrastive methods are more efficient. However, Authors do not show the efficiency of the proposed targeted SSL compared to BYORL. Observed that TAROSS spends almost twice of the training time of ACL from the results in Author\u2019s response (https://openreview.net/forum?id=wwRjJScpsOO&noteId=BU4byVrzGsk), it seems that the proposed targeted SSL is more inefficiency.\n-The scalability and efficiency of the targeted SSL on the large-scale datasets (e.g., ImageNet) are unknown. It would be more difficult to select an appropriate target sample in a large-scale dataset, which could degrade the efficiency and scalability of the proposed method.\n-The clean accuracy of the proposed targeted SSL is significantly degraded on some datasets (such as STL10 and CIFAR100 in Table 4). The degradation of clean accuracy is even higher than the improvement in robust accuracy. Therefore, targeted SSL could be problematic.\n",
            "clarity,_quality,_novelty_and_reproducibility": "[Clarity is poor] As stated in Weakness, Authors do not provide the definitions of some terms and some important preliminaries.\n\n[Quality is poor] As stated in Weakness, the effectiveness of the proposed method is not well validated. In addition, the target selection method is a bit tricky and lacks theoretical justification, which makes its quality further degrade.\n\n[Novelty is marginal]: The paper replaces the label with a carefully-selected target data for generating targeted adversarial data in the setting of adversarial SSL, which is a marginal improvement in novelty.  \n\n[Reproducibility seems to be good.] The paper provides experimental details in the section of Reproducibility Statement.\n",
            "summary_of_the_review": "This paper proposes a target attack for improving the robustness of adversarial SSL. However, the paper\u2019s weighting is poor, its motivation is unclear, and it lacks theoretical justification and enough empirical results to validate its effectiveness. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper866/Reviewer_h3Xt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper866/Reviewer_h3Xt"
        ]
    }
]