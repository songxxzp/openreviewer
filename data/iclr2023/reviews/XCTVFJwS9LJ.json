[
    {
        "id": "BvhIMhItuS",
        "original": null,
        "number": 1,
        "cdate": 1666410990189,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666410990189,
        "tmdate": 1666410990189,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to train normalizing flows with annealed importance sampling (AIS). The loss function, which is the 2-divergence, is approximated using importance weighted AIS samples targeted at the distribution with the zero-variance IS estimator. A replay buffer is introduced to reduce the computational burden of the AIS step. The algorithm has good performance on approximating a 2-d Gaussian mixture distribution and the Boltzmann distribution of alanine dipeptide.",
            "strength_and_weaknesses": "**Strength**\n- The proposed algorithm introduces AIS in the training of normalizing flows. With importance sampling, one is able to sample from the region with larger loss. The computation cost of AIS is reduced by introducing a replay buffer.\n- The algorithm is competitive in the reported numerical experiments.\n\n**Weakness**\n- There are a lot of tuning parameters in the AIS part of the algorithm: the number of intermediate distributions in AIS, $\\beta_i$ for each intermediate distribution; number of MCMC steps used to sample from each intermediate distribution; number of gradient steps per AIS step $L$; $M$. The paper does not discuss how sensitive the performance is to these parameters or how should one choose the parameters.\n- It seems that the algorithm should work for other $\\alpha$. It is not clear why the paper only considers $\\alpha=2$. Some discussion or empirical results with different $\\alpha$ would help.\n- The AIS step is trying to fit $p^2/q$, which is as challenging as the original problem of fitting $p$. One might wonder how the proposed method compares with pure AIS. For example, flow without AIS has bad performance as shown in Figure 8. So one might ask what's the performance of AIS alone.\n- The \"bootstrap\" in the paper is not the bootstrap commonly seen. There is only one line of explanation in Section 3.1. I think the paper could benefit from more elaborate explanations of why the procedure is called \"bootstrap\", since it's in the name of the algorithm.\n\n**Minor comments**\n- The notation $N$ is abused. In Equation (3), $N$ is the number of intermediate distributions. In other places, $N$ is the number of samples generated by AIS.\n- One might expect a plot with $\\alpha=2$ in Figure 1 since the whole paper is about $\\alpha=2$.",
            "clarity,_quality,_novelty_and_reproducibility": "The idea of combining AIS with flows and introducing a replay buffer to reduce cost is new. The paper is well structured and clearly written. The paper provides enough details of the experiments.\n",
            "summary_of_the_review": "The paper provides a novel algorithm for training normalizing flows. The algorithm is competitive in the reported experiments. The writing is clear. The weakness includes the complexity of tuning parameters and the lack of comparison with the baseline of AIS alone.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_xTnc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_xTnc"
        ]
    },
    {
        "id": "u7eYLuRV527",
        "original": null,
        "number": 2,
        "cdate": 1666470047412,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666470047412,
        "tmdate": 1666470047412,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors use the 2-divergence (Chi^2) as the objective function for sampling target distribution. And they parameterize the distribution by a normalizing flow. They use importance sampling ideas to approximate the Chi^2 divergence. By solving the proposed optimization method numerically, they directly simulate the Gibbs distribution from Molecular Dynamics. Numerical results demonstrate the simplicity of the method.  ",
            "strength_and_weaknesses": "Strength: The paper is written well with clear mathematics.\n\nWeakness: The authors need to demonstrate (motivate) why alpha=2 works better.  Does the author try other f-divergences? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear. ",
            "summary_of_the_review": "Overall, the paper is written well. Some analytical examples of Chi^2 divergences can be studied. This could explain or motivate why it works better in numerics. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_tRrj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_tRrj"
        ]
    },
    {
        "id": "ji7DbDqVESK",
        "original": null,
        "number": 3,
        "cdate": 1666857713205,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666857713205,
        "tmdate": 1666857713205,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The normalizing Flow-based method is used to estimate complex densities.  However, current methods for training flows have some drawbacks, including expensive computing with MCMC simulations. In this work, the authors propose low AIS Bootstrap method to generate samples with a theoretical and numerical guarantees. \n\n\n\n",
            "strength_and_weaknesses": "1. In section 3.1, they claim \u201cp^2/q is the minimum variance importance sampling distribution for D_{\u03b1=2}(p||q)\u201d. Can the authors explain further about this point?  \n2. In Many Well Experiments(Appendix E), it seems that their method can not beat the Flow w/ ML method based on Table 4. The Ep(x)[log q(x)], Mean log q (xmodes ) and DKL[p||q] results of those two methods are exactly the same. \n3. Could the authors provide computation time for different methods? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The question of how to estimate the flow-based method is important, and the proposed method is clear.",
            "summary_of_the_review": "\nOverall, the writing is easy to follow and the proposed method seems to work well in 2-d case.\nHowever, the method does not demonstrate improvements over the baseline method in higher\ndimensional cases.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_1QZD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_1QZD"
        ]
    },
    {
        "id": "Dx-y5MuDtD7",
        "original": null,
        "number": 4,
        "cdate": 1667515829778,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667515829778,
        "tmdate": 1667686509757,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method for training normalizing flows using a mass-covering objective which does not require samples from the target distribution.   Existing methods often optimize a mode-seeking objective when target samples are not available, although the experiments section highlights applications (such a learning mixtures of Gaussians or a molecular Boltzmann distribution) where mode-seeking optimization fails to match a multi-modal target.\n\nThe current work overcomes these limitations by evaluating flow density training gradients on approximate samples from the the optimal proposal for estimating the \u03b1-divergence with \u03b1=2.    Minimizing this divergence as a function of q corresponds to minimizing the variance of the importance weights when sampling under q.   Annealed Importance Sampling (with a replay buffer) is used to transform samples generated by the flow into samples from the optimal \u03b1=2 proposal, which emphasizes regions in the sample space where the current flow model does not match the target.\n\n",
            "strength_and_weaknesses": "**Strengths:**  The paper is well-motivated in proposing a method for training mass-covering flows without access to target samples.   I am happy to see work utilizing the importance sampling variance-minimization interpretation of the \u03b1=2 divergence.   \n\nThe authors essentially solve estimation of the Boltzmann dist. for alaine dipeptide molecule.   I do not fully appreciate the significance, but this does represent a 'state-of-the-art'-style result, in which the motivating aspects of the proposed method figure prominently.\n\n\n**Weaknesses:**\nOne question is to what extent the proposed \u03b1=2 objective is preferred to MLE with $KL[p : q]$ in general / practice.   Similar derivations should apply using AIS to sample from p and minimize $KL[p : q]$(the only thing that should change is the AIS target and importance weights, if I understand correctly).\n\nResults are nearly identical for \u03b1=2 vs. MLE with exact or MD samples (both essentially solve the problems), so it doesn't seem absolutely necessary to run additional baselines (or to open up the discussion to less justified values of \u03b1).     Still, the exposition should state the generality of the AIS bootstrap approach in applying to these divergences, especially since accurate sampling for higher values of \u03b1 should be more challenging in practice.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Most comments here are with respect to clarity.   Novelty and quality of the work is high and the appendix appears to have extensive experimental details, although I did not review.\n\n\n**Sec 3** \nOn first pass, I found myself conflating two notions of 'minimum variance', which I think could be clarified by the exposition.    Minimizing the \u03b1=2 divergence corresponds to minimizing variance of IS weights when sampling under $q$, but $g$ is the 'minimum variance' is the minimum variance proposal for estimation of the \u03b1=2 divergence.     One way to emphasize / summarize might be: at test time, we'd like to sample from flow model $q$ (=> minimize \u03b1=2), but \u03b1=2 is hard to estimate during training (=> sample from $g$).   \n\n* The last sentence under Eq. 2 could more clearly specify that samples are drawn from $q$.\n\n* In Sec. 3.1, I think it would be useful to discuss alternatives to the minimum variance proposal $g$, especially since sampling from $q$ and $p$ directly end up forming baselines later in the paper.    Only after rejecting these options might the reader fully appreciate the choice of $g$!     \n\n\n\n**Experiments** \nI found the experiment sections difficult to follow.   It may be useful to demarcate \"experimental design\" details and \"analysis of results\".\n\n* In particular,  I left Sec. 4.2 with very little sense for what \u03a6 and \u03c8 are, how they fit into the sampling/estimation task, etc.   More high-level definition of the problem (perhaps with fewer preprocessing or experimental details, for space) would be useful here.\n\n* Please clarify which KL divergence is being calculated in Table 2 ($KL[q:p]$?).    I assume reweighting in Fig 4 is for samples from q, using importance weights wrt p (?)\n\n\nMinor comments:  \n\n* A concrete expression for estimating expected values using AIS weights would be useful in Sec 2 or App A (e.g. to ease Eq 12).\n* Probably best to use to term \"perfect transitions\" for exact, independent AIS samples\n",
            "summary_of_the_review": "I am happy with this paper overall.   The experiments show success in proposing a mass-covering loss for normalizing flow training which can be used when target samples are not available.    Some concerns regarding presentation have been raised above.  ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_8zPW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_8zPW"
        ]
    },
    {
        "id": "A0Jw4cdAXJH",
        "original": null,
        "number": 5,
        "cdate": 1667540702556,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667540702556,
        "tmdate": 1667757570916,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a method, Flow AIS Bootstrap (FAB) that uses AIS to train flows to approximate complex, unnormalized distributions. Authors used. Alpha divergence with alpha=2 as the objective as it encourages mode covering and minimizes importance weight variance. Authors further reduce the computational cost of FAB by introducing a replay buffer with various tricks, and the authors show empirical evidence of the effectiveness of FAB on toy distributions and on the more sophisticated Boltzmann distribution of alanine dipeptide. \n\n",
            "strength_and_weaknesses": "Strength:\n\n1. I find FAB to be well-motivated. I wasn\u2019t familiar with Boltzmann generators but I find the adoption of AIS to train Flows for this problem to be clever. AIS samples are usually pretty good, but mostly it was only used to estimate the partition function. I have previously thought that the AIS samples could be put to better use, and in this paper the alanine dipeptide problem is a perfect use case because we have access to only the unnormalized density, but to train an explicit generative model we need samples. Yet directly training with the ML objective has various problems, AIS comes to the rescue.\n\n2. I find the use of Alpha divergence with alpha=2 to be clever and appropriate as it encourages mode covering and minimizes importance weight variance at the same time. \n\n3. I find the decomposition of HMC step sizes to be clever (In Appendix E.3). I don\u2019t recall seeing it in prior works, is it a novel contribution by the authors? \n\n4. The FAB method is theoretically sound.\n\n5. It seems authors have sufficiently explored various ways to empirically improve FAB, tricks including the replay buffer (and various nuanced decisions went into improving that including using mini batching with probability proportional to old AIS weights and reweighting etc) are well-motivated and empirically shown to be useful.\n\n6. As a result, the FAB method is empirically strong and convincing. On the toy example, Figure2 is convincing at showing the effectiveness of FAB with buffer, without ground truth samples, it performs as well as Flow trained with ML objective with access to ground truth samples. Similarly Table 1, Table 2 and Figure 3, Figure 4 show strong evidence of the effectiveness of the method. \n\nWeakness:\n\n1. The step sizes tuning for the HMC is tricky, because to conform to mathematical correctness, it is necessary to load the step sizes from a given run and repeat the same run with different random seeds, so that the Markovian property is not violated. I don\u2019t think the authors did this? Granted it probably doesn\u2019t make a big difference empirically, but authors could consider at least verifying how much of a difference this would make. For more details, see Appendix C.3 of https://arxiv.org/pdf/2008.06653.pdf \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: I find this paper to be well-written and clear.\nQuality: See strength 4,5,6\nNovelty: See strength 1,2,3\nReproducibility: The code is included, going through the code it seems all experiments are included. But I don\u2019t have enough time to run the code unfortunately. However based on going through the appendix, it seems the results will be reproducible. \n",
            "summary_of_the_review": "In summary, I think this paper is novel both in terms of the method itself and in terms of the specific use case. And authors have shown strong empirical evidence of the superiority of the method.  I would recommend accept. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_itRu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_itRu"
        ]
    },
    {
        "id": "eUzk5BPZW0",
        "original": null,
        "number": 6,
        "cdate": 1667676049882,
        "mdate": 1667676049882,
        "ddate": null,
        "tcdate": 1667676049882,
        "tmdate": 1667676049882,
        "tddate": null,
        "forum": "XCTVFJwS9LJ",
        "replyto": "XCTVFJwS9LJ",
        "invitation": "ICLR.cc/2023/Conference/Paper1902/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a strategy to augment flows with annealed importance sampling (AIS) with the objective to minimize importance weight variance. AIS is responsible to generate samples in regions where the flow struggles. The proposed FAB was shown to produce\nbetter results than training via maximum likelihood on MD samples with less computational demand. ",
            "strength_and_weaknesses": "Strength: I like the idea of using AIS to cover for regions in the sample space where the current flow model struggle. The experimental results also supported the claim.\n\nWeakness: My concern is over the particular choice of \u03b1=2 objective. Why does minimizing variance indicate that this objective works better for the flow?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall this paper is clear to read, well written and the novelty is high. The code is included, so I assume that the reproducibility should be high. ",
            "summary_of_the_review": "In summary, I think this paper is novel both in terms of the method itself and in terms of the specific use case. And authors have shown strong empirical evidence of the superiority of the method. I would recommend accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I like this paper overall. Clearly sufficient novelty and the experimental part is also convincng. Therefore, I would recommend accept.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_A5bQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1902/Reviewer_A5bQ"
        ]
    }
]