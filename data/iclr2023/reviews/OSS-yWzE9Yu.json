[
    {
        "id": "0B6zKlXXCi",
        "original": null,
        "number": 1,
        "cdate": 1666521272739,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666521272739,
        "tmdate": 1666521272739,
        "tddate": null,
        "forum": "OSS-yWzE9Yu",
        "replyto": "OSS-yWzE9Yu",
        "invitation": "ICLR.cc/2023/Conference/Paper1615/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper first claimed that element arrangement in forward's feature/parameter multiplication is different from the counterpart in backward, which leads to the inefficiency in previous sparse training acceleration method. Based on this observation, the paper proposed a block grouping method to aggregate non-zero elements in feature/parameter.",
            "strength_and_weaknesses": "Weaknesses:\n1. From my point of view, the most important matter of the submission is whether $dW=dY  X^\\top$ can be applied in backward directly. Though it is claimed and visualized in the content (for example, Figure 1) that the resulting $W^{'}$ is not $W^\\top$, I have conducted gradient derivation by manually multiplying gradient of output and activation and my experience implied that dW can be arrived by matrix multiplication between dY and X. My past work may not be fully solid. However, one very obvious question will be: why padding is necessary in backward ? In another words, I am still confused about the submission's foundational motivation.",
            "clarity,_quality,_novelty_and_reproducibility": "There are some confusing points, please refer to Strength And Weaknesses.",
            "summary_of_the_review": "The submission take effort to visualize the idea and make itself understood but it fails on me. Hope I can be clarified in the following discussion. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_5xYn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_5xYn"
        ]
    },
    {
        "id": "3JlYYndywG",
        "original": null,
        "number": 2,
        "cdate": 1666741903177,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666741903177,
        "tmdate": 1666741903177,
        "tddate": null,
        "forum": "OSS-yWzE9Yu",
        "replyto": "OSS-yWzE9Yu",
        "invitation": "ICLR.cc/2023/Conference/Paper1615/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper aims to accelerate the training workflow of convolution operators through sparsity. While recent work has explored structured/semi-unstructured sparsity to accelerate inference, this work targets training. The main challenge in leveraging sparsity in both forward and backward pass is crafting a sparsity mask that is transposable (or in the case of convolution the sparsity patterns are preserved after reforming the kernels). To achieve that, the paper suggests to apply sparsity mask in a kernel-wise manner, which arguable the main difference between this work and Rumi et al.). Leveraging this form of sparsity enables to preserve a hardware-friendly sparsity pattern for both forward and backward pass unlocking the opportunity to accelerate convolutional neural network training.",
            "strength_and_weaknesses": "**Strength**\n\n- Unifying sparsity for both forward and backward pass that enables better utilization of existing hardware platforms for acceleration.\n\n- Results on few convolutional neural network shows promising results both in terms of accelerating training and preserving model accuracy, comparable to unstructured sparsity accuracy.\n\n**Weaknesses**\n\n- The paper lacks discussion on how the proposed form of sparsity can be extended or applied to other (convolution/non-convolution) operations.\n\n- The paper did not cover (or discuss the limitations) other form of neural networks from prior work (recurrent neural network in SNIP or deeper ResNet models).",
            "clarity,_quality,_novelty_and_reproducibility": "(1) Can you provide some intuitions on whether the proposed form of sparsity is applicable to other neural network operations? DepthWise convolution? Self-Attention? etc.?\n\n(2) Prior work studied other neural networks (recurrent, deeper resnets). Have you considered applying your method on these neural network to better show its generality? Or is this a limitation of your method that can only be applied to reasonably small-sized models?  \n\n(3) While it is good to use a broad terms for the paper title, however I would suggest the title of the paper to clearly focus on the target goals of the paper. The paper reads as it is a technique that is general enough for different NN operations, but as the method and results demonstrate, it mainly targets convolution operations. \n\n(4) The results in Table 1 demonstrate that HRBP is seemingly more robust across multiple iterations of training (less variations) -- I agree that it may not be notable difference, but do you have any intuition why it is the case?\n\n(5) The paper covers reasonably high sparsity regime (e.g. 90% and 95% sparsity). What are the benefits of your method for other sparsity ratios? An ablation study---similar to prior work which you have cited---would provide insights about the trade-offs between accuracy-speedup. This is critically important because not all neural networks can be sparsified to the degree that you have studied.\n\n\n ",
            "summary_of_the_review": "The paper tackles an interesting problem, accelerating sparse training. However, the domain and target workloads seem to be very narrow and not generally applicable to different neural network operations and workloads (or at least is not supported by the results). Even for the models covered in the paper, they are arguable old models and dataset, which still valuable, but raises the concerns of application of such method to a broad range of applications.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_fvk3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_fvk3"
        ]
    },
    {
        "id": "FW-JAkuNOlh",
        "original": null,
        "number": 3,
        "cdate": 1667396664580,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667396664580,
        "tmdate": 1667397273987,
        "tddate": null,
        "forum": "OSS-yWzE9Yu",
        "replyto": "OSS-yWzE9Yu",
        "invitation": "ICLR.cc/2023/Conference/Paper1615/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the Hardware-friendly regrouping towards block-based pruning (HRBP) method, and the HRBP++ method. It tests the method on CIFAR-10, CIFAR-100 and ImageNet dataset. ",
            "strength_and_weaknesses": "Strength: \n1. The network pruning is an important topic, and the author has given sufficient background introduction.\n2. The paper illustrate the implementation flow chart of HRBP and HRBP++ clearly.\n\nWeaknesses:\n1. There are spelling errors in the paper.\n2. The paper mix up the programming languages and the math notations. It makes the presented methods difficult to follow. The definitions of symbols and math notations are missing.\n3. The advantages of using the presented methods are unclear. What is the compression rate by using the methods? How is the impact on the accuracy? What is the current state of the art? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and quality: The presentation of the proposed methods is clear, but the definitions of symbols are missing, such as the range of C_O, C_I, etc. It is better to summerize the methods into explicit algorithms. How is the propose method affect the network structure? \n\nIt would be better to put the implementation illustration on the appendix, and summerize the method into an algorithm format. \n\nNovelty: It can be further illustrated. What is the improvement on compression rate? How is the accuracy?\n\n\n",
            "summary_of_the_review": "This paper presents the Hardware-friendly regrouping towards block-based pruning (HRBP) method, and the HRBP++ method. It tests the method on CIFAR-10, CIFAR-100 and ImageNet dataset. The paper can be further improved by clarifying the notations, and clarifying the propose method impact on data structure, compression rate and accuracy. It is also better to illustrate the initialization impact on the proposed method. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_oxCu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_oxCu"
        ]
    },
    {
        "id": "r-sQPporVYC",
        "original": null,
        "number": 4,
        "cdate": 1667438504285,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667438504285,
        "tmdate": 1667483877931,
        "tddate": null,
        "forum": "OSS-yWzE9Yu",
        "replyto": "OSS-yWzE9Yu",
        "invitation": "ICLR.cc/2023/Conference/Paper1615/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies the hardware-friendly regrouping in order to make structured sparsity. It aims at brining the training acceleration on GPU. The algorithm is conducted  on the kernel-wise mask. Some experiments are validated the show the efficacy. ",
            "strength_and_weaknesses": "Strength:\n\n1, generally this paper is well written,\n2, the presented method is interesting.\n\nweakness:\n\n1, I had some doubts whether the compared baselines are reasonable. Why not directly using the group lasso penalty on the each convolutional filters like [r1,r2]. This method should be the naive way of pruning the filters.\n[r1]Exploring Structural Sparsity of Deep Networks via Inverse Scale Spaces, TPAMI 2022\n[r2] DessiLBI: Exploring Structural Sparsity on Deep Network via Differential Inclusion Paths. ICML2020\nThese should be compared, as group lass is exploring the structure sparsity of CNN filters.\nI think the method of these papers is pretty strong competitor to the proposed method in this paper.\n\n2, Page 4, HRBP paragraph, the third line: \"weights, which is obtained by counting Thus, the\". It seems missing something here?\n\n3, As transformers make very good performance in many task, is it possible to apply the algorithm to transformer?  CNNs are a bit out of date (less attractive).\n\n4, As this paper advocates the hardware-friendly algorithm. It should give some more implementation details: is the HRBP/HRBP++ implemented by cuda? How's the implementation of baselines in Fig. 5? Is the comparison apple-to-apple? Are the authours using C++ or Python to run the baselines in Fig. 5? \n\n5. Fig.6 should be results from HRBP++. However the caption refers to HRBP instead.\n6. It would be better if the authors could have some discussion about the practical usage of the proposed method. It seems that it is not so worth it  having only 2 times faster speed while dropping almost all of the original parameters and suffering from significant performance drop.",
            "clarity,_quality,_novelty_and_reproducibility": "the paper is clean, with limited novelty.",
            "summary_of_the_review": "Please clarify my questions about the weakness.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_WyN2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1615/Reviewer_WyN2"
        ]
    }
]