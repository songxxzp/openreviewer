[
    {
        "id": "ttkr8EhV5Ea",
        "original": null,
        "number": 1,
        "cdate": 1666656896034,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656896034,
        "tmdate": 1666656896034,
        "tddate": null,
        "forum": "4NT3umNU3D0",
        "replyto": "4NT3umNU3D0",
        "invitation": "ICLR.cc/2023/Conference/Paper2713/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new perspective of looking at backdoor attacks as features\nrather than outliers. It states that backdoor attacks are impossible to detect\nfrom natural features without structural information on training data\ndistribution. It also assumes that a backdoor attack is the strongest feature\nand designs a defense framework based on this assumption.",
            "strength_and_weaknesses": "It is an interesting perspective to look at backdoors.\nThe paper presentation is good.\n\nIts definition seems to be oversimplified.\nThe assumption it makes seems to be wrong.\nEvaluation is weak.",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe perspective of defining backdoor attacks as the strongest features is\ninteresting.\n\nIts definitions of features and feature supports seem to be oversimplified. In\nsection 3, this paper defines a feature as a map from an input to a binary\nvalue of either 0 or 1 and defines its feature support as whether the feature\nis present. These definitions overly simplify the problem and discussion. In\npractice, the presence of a feature is hard to measure or define -- moreover,\na feature is also hard to define or measure. Using a single scalar value to\ndenote its existence or not is overly simple. Moreover, the feature itself is\nan ambiguous concept. In many cases, humans cannot even determine if a feature\nexists or not without enough context. For example, it mentions feature support\nof a dog's feature can be whether it contains \"dog fur\" -- my question is, do\nyou consider a short white line in an image as dog fur or not? These\nproperties of its defined feature support also affect the decision of whether\na feature is present. This paper directly defines a feature as a\nrepresentation of having something or not, which to me, tries to convert the\nanalysis of a DNN on image data to the analysis of tabular data. I am not\nconvinced that an image in DNN can be analyzed this way. If so, some hard\nproblems in DNN will become easy.\n\nAnother fundamental issue of this work is that it assumes that backdoor\nattacks are the strongest features without considering the learning process.\nThis paper assumes that backdoors as stronger features than natural features\noriginally occur in input samples. This is wrong. I want to point out that\nmany datasets do have strong natural features. In backdoor research, this is\nknown as the natural backdoor. The research goes back to ABS by Liu et al. CCS\n2019 where the authors show natural backdoors for deers. Further, existing\nwork Ex-ray by Liu et al. CVPR 2022 have shown that natural backdoors, whose\nfeature are not as strong as input features, can still manipulate models to\nperform successful backdoor attacks. Another line of work is trying to develop\na robust learning method, e.g., ABL by Li et al. NeurIPS 2021, in the face of\npoisoning data. They also prove that the learning process can pick up more\ndesirable features in training to produce more robust models. This also shows\nthat the assumption of this work does not hold in practice.\n\nOne reason I think its experiment result is good is that it only considers\npatch triggers which are easier to present backdoor features at the input. It\nis important to consider different types of backdoor triggers like Blended\nattack [1], WaNet [2], and Invisible attack [3], whose features at input space\nare not obvious.\n\n[1] Chen et al. \"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.\" arXiv 2017\n[2] Nguyen et al. \"WaNet - Imperceptible Warping-based Backdoor Attack\" ICLR 2021\n[3] Li et al. \"Invisible Backdoor Attack with Sample-Specific Triggers\" ICCV 2021\n",
            "summary_of_the_review": "This paper defines features in a limited way and assumes backdoor attacks as\nthe strongest features without model learnability consideration. These seem to\nbe fundamental limitations.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_9GLS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_9GLS"
        ]
    },
    {
        "id": "JS351zgZWgv",
        "original": null,
        "number": 2,
        "cdate": 1666738967503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666738967503,
        "tmdate": 1666738967503,
        "tddate": null,
        "forum": "4NT3umNU3D0",
        "replyto": "4NT3umNU3D0",
        "invitation": "ICLR.cc/2023/Conference/Paper2713/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper gives a new perspective on backdoor attacks and how to identify and defend them. In back-door attacks, an adversary first performs a poisoning attack, usually by adding a specific pattern to examples that are then labeled carefully. Then, at the test time, the adversary can control the predicted label by minimally modifying the example (e.g., by again adding the pattern).\n\nThe paper focuses on the most common way of doing the backdoors (i.e., by adding patterns) and makes the following contributions:\n\n1. Aiming to identify what backdoor attacks are: The paper starts by making the case that \u201cfeature adding\u201d attacks are, in general, hard to identify as those features look like other features. Then the paper goes ahead and (somehow contradicts this thesis and) says that such adversarial features have special forms of features that could be identified as follows. The paper defines a notion of \u201csenstiivty\u201d for test examples with respect to a specific feature (where a feature is generally thought of as a boolean function over the training set) for a given data set and an integer k. Intuitively, this captures how much the answer to that test flips, when we go from \u201cimage\u201d set of size k to \u201cimage\u201d set of size k+1, where \u201cimage set\u201d is the set of examples from the training set that have the feature.\n\n2. Preventing the backdoor attacks: once we accept the thesis of item (1), that backdoor attacks will be identifiable by finding the \u201cfeature\u201d with maximum \u201cstrength\u201d. Then, the paper shows a heuristic that aims to identify such feature by (heavily) relying on the recent work of \u201cdata models\u201d (from 2022) that allows one to predict the change of labels through a vector in R^|S| where S is the training set. This vector, once used in an inner product with the characteristic function of a subset, will give the result of training on that subset. Using this strong tool (which is conjectured to work well for neural nets) the paper defines an optimization task that heuristically solves to identify a feature/subset that is adversarial.\n\n3. Finally, the paper performs some experiments to see how well their defense against two *previously known* attacks.\n",
            "strength_and_weaknesses": "The strength is to give a new perspective on backdoor attacks. The perspective is intuitive and might lead to useful defenses.\n\nWeakness: There is basically no real theoretical result in this paper. Everything is based on (numerous) assumptions, which expectedly confirms that the defense works, but there are so many assumptions that are thrown without much deep justifications.\n\nOne can argue that the theoretical part would not matter as long as this defense truly works in practice. I agree with that sentiment, but here comes the second (main) weakness:\n\nThe paper only studies previous attacks, while clearly an adversary might have this defense in mind when attacking. This has happened over and over in adversarial learning tha defenses carefully study previous attacks, while attacks are defined adaptively. At the bare minimum, the paper should try to experiment with attackers who try to, e.g., keep the margin and sensitivity bounds of their adversarially added examples below a certain threshold to fool this defense. That is why I cannot support this paper for the top venue of ICLR if it does not follow this (by now well known) standard to judge defenses.\n\nAnother weakness is that: the paper really focuses on a specific type of backdoor attacks. In general, a backdoor attack can do *any* form of poisoning as long as (1) it has few poison points (2) it can control the prediction by a small amount of perturbation. This paper, on the other hand, focuses only on the \u201cmost common\u201d way of doing the backdoor, which is by adding a feature to a bunch of examples.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty is in the new perspective on (the most common form of) backdoor attacks as well as using the \u201cdata model\u201d tool from the recent work of Ilyas et al. for their heuristic defense, which looks like a nice application of this tool.\n\nThe theoretical section is too dense and many parts of it seem not fully baked to me. \n\nSome comments about the writing that might be helpful to the authors:\nYou keep mentioning the \u201cindistinguishability\u201d claim, but it is never formalized.\nAt the minimum, you could discuss the connection to \u201cclean label\u201d attacks. See here for the literature:\nhttps://arxiv.org/pdf/2012.10544.pdf\n\nYou mention the \u201crobust statistics\u201d as the default source of literature on poisoning attacks. This is not true. You are working in the supervised setting, and there is already a large body of work that is much more relevant to your setting. For example, look up the words ``malicious noise\u201d or \u201cnasty noise\u201d from the classical works Valiant and others.\n\nI do not understand the claims about why the claim about the \u201cslopes\u201d in Figure 2 holds. I think the slopes are always there and exist, but it can be the case that in Def 3, the event that you condition on has zero probability. So something does not match.\n",
            "summary_of_the_review": "This paper gives a new interesting perspective on backdoor attacks. It then uses this perspective to come up with a new heuristic defense. The theory part is a bit sketchy and the experimental part does not aim for studying attacks that have this defense in mind.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_RpQm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_RpQm"
        ]
    },
    {
        "id": "tW6YgGA3W1",
        "original": null,
        "number": 3,
        "cdate": 1667265263628,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667265263628,
        "tmdate": 1667265263628,
        "tddate": null,
        "forum": "4NT3umNU3D0",
        "replyto": "4NT3umNU3D0",
        "invitation": "ICLR.cc/2023/Conference/Paper2713/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce a defense to backdoor attacks based on detecting \"strong\" features in training data, and removing backdoored training data by filtering along these lines.",
            "strength_and_weaknesses": "### Strengths:\n1. I like how the authors began their work in a principled way by discussing difficulties with categorizing backdoor attacks.\n2. The approach seems fairly intuitive and well motivated.\n\n### Weaknesses:\n1. I don't think the authors can claim contribution for presenting a \"new perspective\" on backdoor attacks as it is common knowledge that backdoor triggers can be discriminative features of the data. Also, I think this analysis applies more to the \"clean label\" setting - a point I would make clearer in the manuscript.\n2. The work only deals with rudimentary backdoor attacks. It would have been nice to test on modern backdoor attacks - some of which even disguise the \"trigger\" e.g. [1,2].\n3.  The empirical results seem lukewarm - compared to some older defenses, the proposed approach wins out, but often it is not the most successful defense. \n4. There is a lot of clunkiness with the definitions/assumptions. For example:\n    * You might be missing an $S'$ in the Definition 2 - $f(x;S')$ vs. $f(x;S)$\n    * I don't like the presentation for Definition 1. If you're calling $\\phi$ the feature $\\phi:S \\rightarrow \\{0, 1\\}$, then define $\\textit{another}$ symbol to refer to the preimage of $\\{1\\}$. When you reuse $\\phi$ to define $\\phi(S) = \\{ x | \\overline{\\phi(x)} = 1 \\}$ Also, what is $\\overline{\\phi(x)\\}$? Also what is feature $f$? Is this a typo?\n    * Why have $\\mathcal{S} = (X \\times Y)^n$? This seems unnecessary. Just define $\\mathcal{S}$ as $X \\times Y$ for some input space $X$ and label space $Y$.\n\n[1] Saha, Aniruddha, Akshayvarun Subramanya, and Hamed Pirsiavash. \"Hidden trigger backdoor attacks.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 07. 2020.\n\n[2] Souri, Hossein, et al. \"Sleeper agent: Scalable hidden trigger backdoors for neural networks trained from scratch.\" arXiv preprint arXiv:2106.08970 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "The work was fairly clear, but seems to contain several typos. The work is novel as far as I know.",
            "summary_of_the_review": "The lukewarm results and less than ideal presentation makes me lean toward rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_nR1F"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_nR1F"
        ]
    },
    {
        "id": "GvViXdBhhQf",
        "original": null,
        "number": 4,
        "cdate": 1667538052518,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667538052518,
        "tmdate": 1667538052518,
        "tddate": null,
        "forum": "4NT3umNU3D0",
        "replyto": "4NT3umNU3D0",
        "invitation": "ICLR.cc/2023/Conference/Paper2713/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper approached the data poisoning problem from a feature perspective. It argued that the backdoor is no more than the strongest feature in the dataset and thus can be detected based on its high sensitivity. The authors proposed using datamodels to approximate the sensitivity of features and tested the methods on dirty-label and clean-label attacks with either fixed pattern or m-way triggers. Compared to Inverse Self-Paced Learning (ISPL), Spectral Signatures and SPECTRE on CIFAR-10 and ResNet-9, the proposed algorithm successfully defended seven out of eight attacks. Still, it didn't outperform these existing methods on poisoned accuracy. ",
            "strength_and_weaknesses": "__Strength__:\n\n* This paper is well-structured with all implementation details provided. \n* Intuitition of theoritical claims is always given to help the audience understand the concepts. \n* The idea of using datamodel for defending backdoor attacks is novel, and the proposed algorithm is very simple after the problem is provably reduced to a maximum-sum sub-matrix problem. \n\n__Weaknesses__:\n\n* Basic notions should be better defined and explained in the data poisoning setting. \n* Connections between definitions and assumptions in Section 3 are weak and hard to follow. \n* As a result of the bullet point above, the theoritical intuitions cannot convincably motivate the algorithm. \n* The experiment section lacks more up-to-date attacks and defenses and more larger datasets and models. The results are also not significantly better than compared methods.",
            "clarity,_quality,_novelty_and_reproducibility": "__Clarity:__\n\n($+$) This paper has a well-organized structure and is concisely written.\n\n($-$) The main formulation seems to be borrowed from (Ilyas et al., 2022) which is not properly referred to. For example, definition 2 (margin function) is the same as Eq. 7 (correct-class margin) in (Ilyas et al., 2022). As this metric is heuristically chosen by (Ilyas et al., 2022), I believe it should be cited. \n\n($-$) Definitions are mostly not written formally in math but only with a line of words. It is helpful to have words to help the audience understand the concepts but they should only follow the mathematical formulations, not by themselves alone. (Ilyas et al., 2022) also used similar text definitions, but always accompanied with mathematical notations. \n\n($-$) Some definitions/notations are confusing to me:\n  - Informal Assumption 1: \"We assume that adding a backdoor example ($x:\\phi_p(x) = 1$) to the training set changes predictions on the backdoor examples  $\\phi_p(S)$ more than adding an example $x : \\phi_(x) = 1$ changes predictions on  $\\phi_(S)$.\"\n    - $\\phi_p(S)$ is not formally defined. Is it possible that $\\phi_(x)=\\phi_p(x) \\ \\forall x\\in\\mathcal{X}$? In this case, does this statement still hold? \n  - Definition 3: \n    - I don't know if \"$S'\\sim\\alpha S$\" is a valid expression for \"sampling a random \u03b1-fraction subset of S.\" $S'\\sim S, |S'|=\\alpha |S|$ could be a better one. \n    - A definition of $k$ should be given near Definition 3. \n  - Assumption 1: \n    - I didn't get what $p$ is given the definition $p:=|\\phi_p(S)|=|\\phi(S)|$ . Why is $|\\phi_p(S)|=|\\phi(S)|$? Does it mean that a feature $\\phi$ is considered in this assumption if and only if $|\\phi_p(S)|=|\\phi(S)|$? \n    - $S_p$ is defined but not used in this assumption. \n    - $\\delta$ is not defined. \n\n($\\cdot$) I suggest adding a pseudocode for the algorithm. \n\n__Quality__:\n\n($\\cdot$) I have a few questions regarding how the formal assumption, which is claimed as one of the contributions of this paper, is derived. \n  * In the context of data poisoning, I don't know how I should understand the margin. If I understood the definition of margin correctly, it should be that the higher the margin, the better job the model is doing, right? Then why in Figure 2 do we see the average margin of the poisoned image increase when there are more poisoned samples? Can the authors help provide some intuitions about this increase? \n  * Also for Figure 2, why should we look at the change in the margin of clean images when the number of poison images increases? \n  * Why does Assumption 1 _sum_ sensitivity over the supports? I didn't follow how we arrive at Assumption 1 from Definition 3. It would be helpful if the authors can explain more. \n\n($-$) Backdoor attacks and defenses considered in the experiments are limited and outdated. The model and the dataset options are also very limited: the only model used in the experiments is ResNet-9 and the only dataset tested is CIFAR-10. Experiments on more and larger models and datasets are needed. \n\n__Novelty__:\n\n($+$) Defending against backdoor attacks seems to be a novel usage of datamodels. \n\n($-$) The empirical results are not significant compared to existing defenses like ISPL. The theoretical contribution is also marginally novel.\n\n__Reproducibility__:\n\n($+$) Very detailed information is provided for reproducing the results (though the code is not attached). ",
            "summary_of_the_review": "In my opinion, this is a work in progress rather than a complete work that meets the conference standard. The theoretical part is lacking in both novelty and clarity, and there should also be more experiments on the SOTA attacks and defenses with larger and more models and datasets to empirically validate the proposed algorithm. Therefore, I recommend rejection and encourage the authors to re-submit in the future after careful revision. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_kC2s"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2713/Reviewer_kC2s"
        ]
    }
]