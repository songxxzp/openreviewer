[
    {
        "id": "el8ieBmM8hq",
        "original": null,
        "number": 1,
        "cdate": 1666649879663,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666649879663,
        "tmdate": 1666649879663,
        "tddate": null,
        "forum": "k71IGLC8cfc",
        "replyto": "k71IGLC8cfc",
        "invitation": "ICLR.cc/2023/Conference/Paper2945/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces an SDP based formulation for the development of 1-Lipschitz architecture. They show that many of the existing techniques for generating 1-Lipschitz neural networks can cast as solution to this SGD condition (W^TW < T).\n\nFurthermore, based on the SDP formulation the authors derive new conditions for Lipschitz Layers (based on different values of T) and show through experimentations that the SDP based lipschitz layers get better performance than previous approaches on natural and certified accuracy. \n\nBased on their setup, the paper also provides motivation for why Almost-Orthogonal-Layers promote orthogonality of layers during training.",
            "strength_and_weaknesses": "[Strengths]\n\n- The paper is well written and easy to follow, and provide a theoretical framework that can unify the existing methodologies to get 1-Lipschitz functions.\n- Theorem 1 provides shows that spectral normalization and convex potential layers can be looked at as the linear and residual block for the same value of T.\n- The authors apply theorem 3 to come up with a new formulation of T with a new diagonal matrix Q.\n\n[Weakness]\n\n- The empirical verification is lacking. The authors have provided with a general framework for designing Lipschitz Layers, however they only show empirical results with one result from this formulation.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and easy to follow, and very well motivated. \n\nOne minor point is that it would be useful to write the statement for Gershgorin circle theorem so that the proof for Theorem3 could be easier to follow.",
            "summary_of_the_review": "I think the unifying framework introduced by the authors is interesting. However, the paper lacks enough empirical validation since the authors only show results for one type of layer from their SDP formulation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_rDGJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_rDGJ"
        ]
    },
    {
        "id": "_qVN3W0irRM",
        "original": null,
        "number": 2,
        "cdate": 1666669237701,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666669237701,
        "tmdate": 1668929536864,
        "tddate": null,
        "forum": "k71IGLC8cfc",
        "replyto": "k71IGLC8cfc",
        "invitation": "ICLR.cc/2023/Conference/Paper2945/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a unified framework for a group of well-known Lipschitz regularization methods in deep learning. Theorem 1 presents the algebraic condition on normalizing matrix $T$ which requires $WW^\\top - T\\preceq \\mathbf{0}$. As a result, the framework includes spectral normalization, orthogonal regularization, Almost-Orthogonal-layer (AOL), Convex Potential Layers (CPL) schemes. Subsequently, Theorem 2 gives a new interpretation of AOL methods by observing that the chosen matrix $T$ results in the closest normalized matrix to the set of orthogonal matrices which is an important step toward achieving a $1$-Lipschitz network. Then, in Section 5 the authors generalize the CPL method by introducing new optimization variables in a diagonal matrix $Q$ which is simultaneously optimized with weight matrix $W$. The paper discusses several numerical results to show the merits of the proposed SLL method on robust learning problems over CIFAR-10 and CIFAR-100 datasets.\n\n**Review Update**\n\nThe authors' response satisfactorily addresses my comments and I therefore raise my score to 8. ",
            "strength_and_weaknesses": "Strengths:\n\n1- The paper has an interesting theoretical contribution and proposes a nice unified framework for Lipschitz regularization methods in deep learning.\n\n2- The paper is well-written and easy to follow. The theorems are sufficiently explained and well supported by several examples.\n\nWeaknesses:\n\n1- The numerical results are somewhat preliminary and discussed for only CIFAR-10 and CIFAR-100 datasets. Discussing the results for one or two large-scale image dataset could provide a stronger support for the SLL approach. Also, the performance scores for the SLL method and AOL baseline are quite close and in several cases, AOL performs better than the proposed SLL apporach.\n\n2- While the theorem proofs are interesting, I think they could have been presented in the Appendix. The proofs' space could have been used to discuss additional numerical results.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: The paper is written clearly and it is rather easy to follow the paper's discussion.\n\n**Quality**: The paper discusses an interesting unified framework for Lipschitz regularization methods.\n\n**Novelty**: While the paper revisits several existing Lipschitz regularization schemes, the unified framework and discussed extensions look novel to me.\n\n**Reproducibility**: While I have not checked the details of numerical experiments, the numerical evaluation seems reproducible to me. ",
            "summary_of_the_review": "This paper proposes an interesting unified framework for Lipschitz regularization methods, and the theorems help extend and generalize existing Lipschitz regularization schemes. The numerical contribution could be further improved by experimenting the proposed methods on more datasets and finding factors in which they outperform the baseline AOL.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_VQD9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_VQD9"
        ]
    },
    {
        "id": "TXrWw0_JKwK",
        "original": null,
        "number": 3,
        "cdate": 1666680487699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666680487699,
        "tmdate": 1666680487699,
        "tddate": null,
        "forum": "k71IGLC8cfc",
        "replyto": "k71IGLC8cfc",
        "invitation": "ICLR.cc/2023/Conference/Paper2945/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a unified perspective on 1-Lipschitz layers. The 1-Lipschitz layers in previous literature can be recovered by the algebraic conditions in Theorem 1. The theoretical results also lead to new parameterizations of 1-Lipschitz neural network layers. Experiments on certified robustness shows the effectiveness of the new paramterizations.",
            "strength_and_weaknesses": "Strength:\n1) The authors provide a unified perspective on the development of 1-Lipschitz layers, which provides better understanding of the connections between different methods, and allows for new parameterizations of 1-Lipschitz layer.\n2) Theorem 1 can recover a wide range of previous methods on parameterization 1-Lipschitz layers, and leads to varaitions of new parameterizations.\n3) The authors show some computational efficient parameterizations as an extension to CPL. The performance on certified robustness surpasses previous 1-Lipschitz neural networks and further reduces the gap between certified accuracy and AutoAttack accuracy, implying the new parameterization is more expressive. \n\nWeaknesses:\n1) The architectures that are used for SLL seem to be much larger than those used in OrthoConv (Trockman et al., 2021). For instance, Cayley-Large uses an architecture with 4 convolutional layers and 3 fully connected layers, while in Table 3, SLL-small has 20 convolutional layers and 7 fully connected layers. It may be good to also put number of parameters in the table performance comparison table, and comment on the effectiveness of different architectures of 1-Lipschitz neural network.\n2) What activation functions do the authors use for SLL? It seems that the authors follow the architecture choice in Meunier et al. (2022) but it'd be good to mention the activation function choice because it is important for training 1-Lipschitz neural networks. Some previous works (e.g. Trockman et al., 2021) finds that gradient-norm-preserving GroupSort is much more effective than ReLU in training 1-Lipschitz neural networks. I am curious on whether this is also the case for the residual architecture. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written, and the proposed perspective and techniques are novel. \nHowever, the template of the paper seem to be wrong and should be fixed.",
            "summary_of_the_review": "This paper proposes a unified perspective on the development of 1-Lipschitz neural network layer, and provides new parameterizations of 1-Lipschitz layer. The experiments show strong certified robustness results which demonstrate the flexibility of the new parameterization.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_PHyU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_PHyU"
        ]
    },
    {
        "id": "EtDzWGFcJU",
        "original": null,
        "number": 4,
        "cdate": 1667036089196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667036089196,
        "tmdate": 1667036089196,
        "tddate": null,
        "forum": "k71IGLC8cfc",
        "replyto": "k71IGLC8cfc",
        "invitation": "ICLR.cc/2023/Conference/Paper2945/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new formulation for designing 1-Lipschitz neural networks, which is more general than existing ones. To guarantee 1-Lipschitzness, existing works use a variety of parameterizations. This paper proposes a more general parameterization which covers existing ones (existing works can be seen as special cases of this parameterization), and the new parameterization allows more flexibility on designing the network. Additionally, experimental results on CIFAR-10 and CIFAR-100 show that the 1-Lipschitz network trained using the new parameterization performs better compared to baselines.",
            "strength_and_weaknesses": "Strengths:\n\n1. The new formulation for representing the 1-Lipschitz layer is novel. The theoretical analysis provide many insights on how to design neural network layers that are one Lipschitz.\n\n2. Empirical results look very promising - the proposed method achieves better provable robustness (one main application of 1-Lipschitz network) than prior works consistently, although in some cases the improvements are marginal.\n\n\nWeaknesses:\n\n1. One question is not clearly answered by this paper: Is it possible to show that the formulation proposed in this paper can universally approximate any 1-Lipschitz functions? If it is hard to prove theoretically, we hope to know if the new factorization for 1-Lipschitz neural network is more representative compared to previous ones. The paper provides some experimental comparisons, however some baselines use models of different sizes. Ideally, for a fair comparison the number of optimizable parameters should be same in each method.\n\n2. Table 1 should include other recent baselines for provable robustness for L2 norm, such as (Leino et al., 2021) and (Huan et al., 2021); it should be easy to add the results directly. In addition, I think it is important to report training time of each method.\n\nReferences:\n\n[1] Leino, Klas, Zifan Wang, and Matt Fredrikson. \"Globally-robust neural networks.\" International Conference on Machine Learning. PMLR, 2021.  \n[2] Huang, Yujia, et al. \"Training certifiably robust neural networks with efficient local lipschitz bounds.\" Advances in Neural Information Processing Systems 34 (2021): 22745-22757.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall well written and has good novelty. The paper provides good insights on how to design 1-Lipschitz neural networks. Sufficient details on experiments are given in appendix.",
            "summary_of_the_review": "Training 1-Lipschitz neural network is an important and challenging problem and I like the new formulation in this paper because it provides new insights to this problem. The empirical results also look positive. So I tend to accept this paper. I hope the authors can address the weaknesses questions above.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_BFUj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2945/Reviewer_BFUj"
        ]
    }
]