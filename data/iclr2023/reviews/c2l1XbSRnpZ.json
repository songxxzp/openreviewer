[
    {
        "id": "Kj-n5ACUjwU",
        "original": null,
        "number": 1,
        "cdate": 1666092450517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666092450517,
        "tmdate": 1666384356233,
        "tddate": null,
        "forum": "c2l1XbSRnpZ",
        "replyto": "c2l1XbSRnpZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4662/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present a complex methodology of how to generate compact MLP-models that would highly accurately and smoothly represent mathematical functions. \nCore of the methodology (contributions):\n1. a novel hierarchical MLP-model called WGN (weight generating network) - one part of the model (\"weight generator\") generates parameters of the later part (\"main network\"). \n2. a novel activation function ISLU - a smooth version of ELU, concurrent to Softplus (it promises \"less wavy\" second derivatives)\n3. a meta-batch concept - meta-parameters of the represented mathematical function are used as hints during the training \n4. \"smoothness score\" - an approximate measure that evaluates how \"wavy\" a network function is\n\nExperimental evaluation comprises three relatively simple artificial datasets. It compares the MLP and WGN models with \"smoothness score\" as the tested criterion.\n\n",
            "strength_and_weaknesses": "Plus\n- The paper looks at accurate regression from an interesting point of view. The authors are looking for \"unwavy\" network functions (they call them \"smooth\" which is slightly misleading according to the common sense of the word) \n- The concept of meta-parameters of the represented mathematical function that can serve as hints during training or to create meta-batches is interesting and worth further research.\n- The WGN-model seems to be novel enough, its combination with with smooth activation function and the presented meta-batch concept to achieve better results looks reasonable.\n\nMinus\n1) The experiments are not well chosen to highlight advances and usefulness of the presented model: \n- I am not sure, whether the presented model/concept isn't unnecessarily complex to succeed on the presented artificial tasks that are relatively simple. \n  I would guess, that similar compact and smooth models could be created in a much simpler way (than WGN)\n  by one (or a combination) of the classical MLP-techniques for generalization improvement and/or for reduction of model VC-dimension. \n  What about classical learning from hints - the variant where meta-parameters are used as hint-outputs during training seems promising. Also pruning, well-chosen regularization,... \n  Further experiments comprising at least learning with hint outputs could rebut such suspicion. \n- The experimental evaluation doesn't comprise any real world data. Many models work well for simple artificial tasks but have problems on real world data that is is usually less regular/noise-corrupted/not perfectly distributed etc. Could you enhance the evaluation by such data? \n- The only tested criterion is \"smoothness score\" that measures \"waviness\" of the network function. The measure is approximate and measures presence of just one type of waves. It might thus unfairly advantage one technique over others. \n   If you alter the \"smoothness score\" to e.g., identify also peaks, it might lead to more fair comparison.\n-  There is another argument, why it is not good to use \"smoothness score\" as the only criterion - best (zero) scores will be achieved by MLPs with only linear or even constant activation functions that however won't be able to learn the task at al.\n  - Therefore, why do you omit common measures, such as generalization error or sensitivity (i.e., derivative of the model outputs with respect to its inputs), as further tested criteria? \n  If the network function is \"too wavy,\" its vc-dimension is \"higher then enough\" and it is indicated also by greater generalization error. Smooth network function is also characterized by low sensitivity.\n\n2) The paper omits classical research on the topic - there are several general methods how to form compact, smooth and accurate MLP-models (i.e., regularization, pruning, learning from hints, sensitivity analysis,...). \n   A citation and discussion of such techniques would be appropriate. E.g., the concepts presented in 2.2. and 2.3 (or even 3.2) are obvious variants of learning from hints.\n3) The core of the paper - Sections 3.1 and Section 3.2 are not written comprehensibly. The details of the model and the training process are not described adequately.  \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and reproducibility\nThe comprehensibility of the paper differs from one section to another. Some parts of the paper are written clearly with well-chosen illustrative Figures, while others are not:\n- Namely the WGN model and meta-batch concept (Section 3.1, 3.2) are not described comprehensibly and in enough detail. This fact also lowers down reproducibility of the results. \nWhat is the inner structure of the Weight Generator sub-network?  I also don't understand the principle of how exactly the two parts of the network are connected.  Even the Figure 10 does not help much. \nCould you describe the model, its parts and their inputs more clearly? Explanation on task <1> might help.  \n- The training algorithm and the way how the error is propagated through the model should also be explained in more detail (e.g., using a pseudo-code).\n- It is not clear how the concepts explained in 2.2 and 2.3 and in Equation (1) are used in the WGN model.  \n    \n- The Abstract is a bit confusing. It is more common for a scientific publication to place such a section-wise list of content at the end of the Introduction section rather than into the Abstract. \n \n- The paper should be proof-read for language issues (especially Section 1).\n\nNovelty\n- The solved task is interesting. The presented WGN model and the methodology as a whole seem to be reasonably novel. However, it is questionable, how useful and significant the contribution is (better experimental evaluation might help to answer this question).\n- Some important relative work is not mentioned and analyzed (see \"Strength And Weaknesses\" for details).\n\nQuality\n- The paper suffers for lower technical quality from the experimental point of view.  See \"Strength And Weaknesses\" section for details. \n\nQuestions\n- You argue that ISLU activation function leads to smoother and less wavy network function. \nTo better show the smoothness of ISLU, could you provide a Figure (similar to Figure 3) with also second derivatives of ISLU compared to the other mentioned activation functions (softplus, ELU, hyperbolic tangent,...)? Can you explain your claim that the problem of the hyperbolic tangent function is, that \"the output function bends in two places after each layer.\"?\n\n\n",
            "summary_of_the_review": "The solved task is interesting and the presented model seems to be novel enough. \nIf the authors provide an adequate experimental evaluation to show usefulness of their approach and if they improve both description of the WGN model and settle their approach better into the related work, I would change my score towards accepting the paper.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_uCcd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_uCcd"
        ]
    },
    {
        "id": "f9Tx42f5eA",
        "original": null,
        "number": 2,
        "cdate": 1666319154660,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666319154660,
        "tmdate": 1666319154660,
        "tddate": null,
        "forum": "c2l1XbSRnpZ",
        "replyto": "c2l1XbSRnpZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4662/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new activation function named integrated sigmoid linear unit (ISLU), and evaluates its performance on a regression task.",
            "strength_and_weaknesses": "Strength\n* None\n\nWeakness\n* The paper is not well structured nor formally described. For example:\n    * What is the formal definition of 'well-developed' in p.2\n    * What is the meaning of 'good' in p.3\n    * Most figures are not cited from the main manuscript\n    * It is very hard to understand the experimental settings\n    * It is very hard to understand the definition of 'metadata'\n",
            "clarity,_quality,_novelty_and_reproducibility": "See 'Strength And Weaknesses' section",
            "summary_of_the_review": "See 'Strength And Weaknesses' section",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_Mwpn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_Mwpn"
        ]
    },
    {
        "id": "2Mmn25yk8n",
        "original": null,
        "number": 3,
        "cdate": 1666423470947,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666423470947,
        "tmdate": 1666423470947,
        "tddate": null,
        "forum": "c2l1XbSRnpZ",
        "replyto": "c2l1XbSRnpZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4662/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The authors have worked on NNs that can generate or rather approximate accurate and smooth function mainly to solve problems related to regression. The NN uses a few weight parameters, for regression.  They have reinterpreted the outputs of NNs and have proposed a new activation function\u2013integrated sigmoid linear unit (ISLU). \nThey have captured the essence of metadata, meta-parameters (called fictitious meta-parameters) for improving the performance of neural networks used in regression.  \t\nSome calculations of the form of the activation presented in the paper reveal the following:\nlog(alpha + exp(beta))/beta  - log(1+alpha)/beta\nlog[alpha/(alpha+1)+ exp(beta*x)/(1+ alpha)]\nlog[(alpha(1+alpha))(1 + exp(beta*x)/alpha)]\nSome constant + log(1 + exp(beta*x)/alpha) which is a minor variation of the activation function log(1 + exp(beta*x)) which is known to have a saturation problem. I don\u2019t see a major contribution in the existing cottage industry of activations.\n",
            "strength_and_weaknesses": "An idea is presented by the authors to augment data and meta-parameters for improving the performance of NN. A novel activation function which works very close to ELU is also worth exploring. \n\nWeaknesses \nThere is no theoretical foundation of the work. When the authors say that the presence of meta parameters improves the performance of NN, is there any theory that validates the claim. If so then they must present it. Otherwise the improved performance could be just a mere coincidence on the data presented in the manuscript. The data set used by the authors is generated from the formula in the physical concept of a spring connected to a support at one end and has a mass m attached at the other end.\nAnother validation needed in the manuscript is the proof of Universal approximation theorem for the new activation function ISLU. Does ISLU satisfy UAT! \nISLU is a just modified version of ELU with additional parameters added to ELU, alpha and beta, to tune the curvature of the output function at every layer.  If there is more to it, the authors need to mention that. Like how is the AF unique? \nRegarding the architecture of NN, it is expected from authors to explore why they have chosen 4 layers for training adding/reducing layers can affect the results! Or Is it because the NN performs fine with 4 layers for that specific dataset used by authors. So the model will work fine if the dataset is changed?\nIt is important to analyse if the AF, and the concept of meta parameters and weight generating network works well for all types of datasets and on the other regression tasks. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper is missing, the English used is weak. However the notations and nomenclatures are fine. \nNovelty in the manuscript is missing because the proposed activation function is just a modified ELU with additional parameters for better tuning. Moreover, as meta-parameters are important for improving the performance, the authors have not presented a robust method of computing them. \nThe method is not reproducible; since the authors have not shared their code via online repositories like github. At least, I can't find anything except the PDF. I'm usually not rigid but it's unlikely that my opinion about the paper will change.\n",
            "summary_of_the_review": "The manuscript is a bit weak with respect to empirical as well as theoretical validations. Empirically, testing a model requires division of data into train and validation sets to ensure that there is no bias while experimenting. Moreover, there is just one dataset used during experimentation which is generated via a physical system of a spring attached at one end. Such physical systems (spring-mass) are now repeatedly used to describe activation functions (DiffAct, Saha et al, IJCNN 2021). The solution to the system is a polynomial where coefficients can be learned from data.\nThe NN regression should be rigorously checked on multiple datasets. The authors have not clarified the method used by the authors to generate hyper-parameters in their manuscript. Theoretical validations of the new activation function are missing as well. The authors need to show if the new AF can approximate any function with a fair degree of accuracy. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_QykB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_QykB"
        ]
    },
    {
        "id": "lqXAQ6ZfeG",
        "original": null,
        "number": 5,
        "cdate": 1666577273612,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577273612,
        "tmdate": 1666577273612,
        "tddate": null,
        "forum": "c2l1XbSRnpZ",
        "replyto": "c2l1XbSRnpZ",
        "invitation": "ICLR.cc/2023/Conference/Paper4662/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "I don\u2019t think this paper is finished and ready to be reviewed.  ",
            "strength_and_weaknesses": "Strength: Perhaps after this paper is finished there will be strengths, but currently I can\u2019t find any.\n\nWeakness: See next section\n",
            "clarity,_quality,_novelty_and_reproducibility": "I find this paper very confusing. Despite my efforts, I could not understand the problem setup or the significance, and I needed to constantly guess what the authors are trying to say and what was being described. Here is a list of things that are either syntactically incorrect or semantically confusing. \n\nAbstract\n\u201cThis is paper for\u201d This feels really weird to read, I think this entire sentence doesn\u2019t convery any additional information given the paper title, so this sentence can be safely removed. \n\u201cIn this study, we get NNs\u201d It\u2019s a bit confusing here, what do you mean by \u201cwe get\u201d? do you mean you trained, or you took some pretrained/existing NNs from somewhere?\nIn the same sentence, \u201cthrough discussing a few topics about regression\u201d,  you mean you get NNs through discussing a few topics about regression? Sorry that makes no sense to me.\n\u201cthe one of a simple hierarchical NN that generate models substituting mathematical function is presented\u201d\n\u201cthe new batch concept \u201cmeta-batch\" which improves the performance of NN several times more is introduced.\u201d This sentence makes partial sense, but I think it\u2019s much better if authors don\u2019t use the passive voice here: \u201c\u201cwe introduce a new batch concept \u201cmeta-batch\" which improves the performance of NN several times more.\u201d \n\nIntroduction\nAfter reading the introduction, I\u2019m still pretty confused about what this paper is about. Key concepts like metaprameters and meta-batch were not explained at all. \n\n\nNNs for Regression\n\u201cThey can be seen as basis functions\u201d Do authors mean basis functions in the sense of rigorous mathematics? If so, authors would need to provide theoretical justification or point to prior literature. If not, authors should explain what exactly do they mean by basis functions. \nI\u2019m not sure what is the point of Figure 2. I\u2019m not sure what\u2019s the point of Figure 1 either. \n\u201cIf a one-dimensional regression problem is modeled with a simple MLP that has (k+1) layers with nodes [N0, N1, N2..Nk], the output function will bend more than N0 \u2217 N1...Nk.\u201d What do authors mean by \u201cbend more than\u201d?\n\n\u201cThus, the question is which activation function can develop the intermediate basis functions well? If the activation function starts as a linear function and bends at an appropriate curvature after each layer, the final result will be good.\u201d This seems to be the motivation for the new activation function proposed by the author, but I couldn\u2019t understand this sentence. I understand every word, but I can\u2019t understand what\u2019s going on.\n\n\u201cThere is a significant difference in performance between SoftPlus and ISLU.\u201d What difference? Is it desired, or not desired? What\u2019s the hypothesis/prediction of the authors\u2019 theory?\n\n\n\nI would advise authors to read Steven Pinker\u2019s The Sense of Style and revise the paper according to the principles described in the book. That\u2019ll be a gift to make the lives of the reviewers and potential readers much better. For example, I noticed that authors excessively used passive voice, passive voice is not always better, and it sometimes makes the writing much harder to read. See the Sense of Style for more details.\n\nAt the very least, authors could consider using grammar tools like Grammarly or word tune to check for grammatical mistakes, or proofread before submitting. This is only fair to the reviewer/reader\u2019s time. \n\n\u201cWhen using metadata, the performance is improved because biases are determined by referring to various data.\u201d What is metadata? It is still not described though it has been referred to many times. \n\t\n2.1 PERSPECTIVES OF METADATA\nFinally metadata is introduced, but it is still not clear to me what is the problem set up and I needed to guess. Do the authors mean that for the regression problem we have access to parameters of the data-generating process? And those parameters are metadata? But if we already have parameters and the data-generating process, it doesn\u2019t seem like a significant problem to begin with, so I remain extremely uncertain of what is going on.\n\n2.3 LEARNING FUNCTION WITH RESTRICTED METADATA\nThis section also makes no sense to me. The comparison seems to be to be mostly about intrinsic data dimension. If you call representative points on a polynomial metaparameters, you can also call representative images metaparameters? Like eigen faces in a small face image dataset? \n3.1 WGN\n\u201cWe consider the one of the structure of a function-generating network called weight generating network(WGN) in this study.\u201d This sentence makes no sense to me either, and this is the point where I decided to give up \u2026 I\u2019m sorry but I don\u2019t think authors have finished writing this paper, and spending any more time on reviewing an unfinished, not readable paper is not justified and not fair for reviewer\u2019s time. \n\n",
            "summary_of_the_review": "I don\u2019t think this paper is finished and ready to be reviewed. ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_DP91"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4662/Reviewer_DP91"
        ]
    }
]