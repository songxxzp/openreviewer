[
    {
        "id": "nCoKqFh0n71",
        "original": null,
        "number": 1,
        "cdate": 1666550402080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666550402080,
        "tmdate": 1670369905579,
        "tddate": null,
        "forum": "8KYeilT3Ow",
        "replyto": "8KYeilT3Ow",
        "invitation": "ICLR.cc/2023/Conference/Paper3965/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new neighborhood aggregation method based on the number of hops. With this method, a graph transformer is developed with node-wise input sequences. Moreover, a self-attention readout function is applied to integrate the representations learned from the transformer module.",
            "strength_and_weaknesses": "Strengths:\n\n1. With node-wise sequences as inputs, mini-batch can be used, and training becomes more scalable.\n2. Extensive experiments are conducted to compare NAGphormer with many baseline models in small to large-scale tasks. Additionally, an ablation study on the effect of different readout functions and propagation steps is performed.\n3. The empirical performance of NAGphormer is outstanding.\n\n\nWeaknesses:\n\n1. The authors propose that with Hop2Token, NAGphormer is more expressive than decoupled GCN family. However, in the experimental section, no decoupled GCN models are compared. Therefore, it will be more persuasive if the authors can add some of the decoupled GCN as baselines.\n2. Given the good performance of NAGphormer, from the ablation study over the depth of the model, it seems that NAGphormer also suffers from oversmoothing or oversquashing problems since as the depth increases, the performance deteriorates. \n",
            "clarity,_quality,_novelty_and_reproducibility": "1. The paper is written clearly.\n2. The code of the experiments is submitted.\n",
            "summary_of_the_review": "The idea of node-wise sequence input is novel, and the proposed model is competitive compared with current SOTAs.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_b2CU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_b2CU"
        ]
    },
    {
        "id": "XICIbG86_p",
        "original": null,
        "number": 2,
        "cdate": 1666594446730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594446730,
        "tmdate": 1666594446730,
        "tddate": null,
        "forum": "8KYeilT3Ow",
        "replyto": "8KYeilT3Ow",
        "invitation": "ICLR.cc/2023/Conference/Paper3965/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Hop2Token to scale a graph transformer to larger graphs by using multi-hop aggregated features of each node as the input tokens to graph transformer, instead of using all nodes as the tokens. The overall transformer using Hop2Token is called NAGphormer, which also uses structural encoding and attention-based readout function. The method is tested on both small and large node classification datasets and demonstrates empirical gains.",
            "strength_and_weaknesses": "Strength\n- The proposed Hop2Token method is novel, simple, and can be efficiently implemented.\n- NAGphormer demonstrates empirical gains over thorough experiments and ablations.\n- The paper is clearly written and easy to follow.\n\nWeaknesses\n- Several important previous works are not mentioned and compared against, most notably \"Recipe for a General, Powerful, Scalable Graph Transformer\", which proposes reducing computation costs using a linear transformer. Other recent graph transformers like K-Subgraph SAT and EGT should also be compared if the authors wish to support the claim \"consistently outperforms existing graph Transformers and mainstream GNNs\".\n- The expressivity result against decoupled GCN does not seem very strong, since decoupled GCN is very simple and limited itself (probably by WL1 test, also https://arxiv.org/pdf/2010.12408.pdf shows that it can be seen as two times label propagations.)\n- Hop2Token fundamentally tries to compress an exponentially more amount of nodes information to a single token as the hop k increases. This is known to suffer from over-squashing issue. Although this may not be shown in the experimented datasets (maybe due to high homophily), this is a concern conceptually. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. To the best of my knowledge, Hop2Token is novel, although heavily related to the decoupled GCN and APPNP lines of work. The NAGphormer is less novel beyond Hop2Token and seems mainly serves as a way to adapt a standard transformer to Hop2Token (e.g. structural encoding has already been studied in previous works). ",
            "summary_of_the_review": "Overall, the proposed Hop2Token seems a conceptually interesting and promising way to enable mini-batch training over larger graphs. However, several important baselines are missing and the method in its current form could suffer from severe over-squashing problems. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_zQFG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_zQFG"
        ]
    },
    {
        "id": "a-76SZtI1c",
        "original": null,
        "number": 3,
        "cdate": 1666618047453,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666618047453,
        "tmdate": 1668505361173,
        "tddate": null,
        "forum": "8KYeilT3Ow",
        "replyto": "8KYeilT3Ow",
        "invitation": "ICLR.cc/2023/Conference/Paper3965/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a Graph Transformer architecture. It aggregates the neighborhood features from different hops and treat them as a sequence of token vectors for Transformer.",
            "strength_and_weaknesses": "Strengths:\n\n- Different from existing graph transformer methods, this work treats different hops of neighborhood representations as tokens. This can make the proposed method scale to large graphs.\n\n- The paper is written and organized well, and it is easy to follow.\n\n- The provided source code facilitates good reproducibility of this work.\n\n- The proposed method seems to have a promising performance in experiments.\n\nWeaknesses:\n\n- The authors argue that existing message passing-based GNNs have limitations such as over-smoothing and over-squashing. However, recently, there emerges a number of works e.g. [1,2,3,4,5] that address these limitations.\n\n- The experiments are not extensive. Specifically, the running time cost and the memory cost of all the methods should be compared to better show the efficiency of the proposed method, since the efficiency of this work is highlighted throughout the text.\n\n- This paper repeats itself in some pieces of text.\n\nRefs:\n\n[1] Huang, W., Rong, Y., Xu, T., Sun, F., & Huang, J. (2020). Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864.\n\n[2] Lu, W., Zhan, Y., Guan, Z., Liu, L., Yu, B., Zhao, W., ... & Tao, D. (2021). SkipNode: On alleviating over-smoothing for deep graph convolutional networks. arXiv preprint arXiv:2112.11628.\n\n[3]\tYang, C., Wang, R., Yao, S., Liu, S., & Abdelzaher, T. (2020). Revisiting over-smoothing in deep GCNs. arXiv preprint arXiv:2003.13663.\n\n[4] Huang, W., Rong, Y., Xu, T., Sun, F., & Huang, J. (2020). Tackling over-smoothing for general graph convolutional networks. arXiv preprint arXiv:2008.09864.\n\n[5] Sun, Q., Li, J., Yuan, H., Fu, X., Peng, H., Ji, C., ... & Yu, P. S. (2022). Position-aware Structure Learning for Graph Topology-imbalance by Relieving Under-reaching and Over-squashing. arXiv preprint arXiv:2208.08302.",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: easy to follow\n\n- Quality: seem to be sound\n\n- Novelty: fair\n\n- Reproducibility: provided source code facilitates good reproducibility \n",
            "summary_of_the_review": "The idea is good. Although some necessary experiments are missing, I encourage the authors to supplement these experiments during the discussion phase, to well support their arguments.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_gZ4g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_gZ4g"
        ]
    },
    {
        "id": "fd7TEqSBLX",
        "original": null,
        "number": 4,
        "cdate": 1666628584052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628584052,
        "tmdate": 1666655568368,
        "tddate": null,
        "forum": "8KYeilT3Ow",
        "replyto": "8KYeilT3Ow",
        "invitation": "ICLR.cc/2023/Conference/Paper3965/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new graph Transformer architecture that contains a sequence of tokens obtained by aggregating neighbors' features. It shows improved expressive power with information from multi-hop neighborhoods. So, the model treats a node as a sequence of tokens. The authors claim that the model has better scalability due to its mini-bash training. The authors theoretically analyze that the proposed method has more expressive power compared to decoupled GCN.",
            "strength_and_weaknesses": "Strengths:\n\n* The proposed method shows competitive performance on many benchmark datasets.\n* The multi-view/multi-scale representation of a node (a sequence of aggregated node features) is an interesting idea. The late fusion of the node features is new. However,  the attention-based readout can be viewed as deformable convolution with an adaptive receptive field since it varies neighborhood from 0-hop to K-hops. From this perspective, the authors may want to compare the proposed method with similar approaches in the literature. \n\nWeaknesses:\n\n* Unlike the authors' presentation, the model does not utilize order information at all. Indeed, the model can be viewed as an ensemble model with features from different layers of a GNN while sharing the transformer's parameters. \n* One of the main contributions is \"a sequence of tokens\" for a node. But no ablation study is provided. For instance, simple feature concatenation is one na\u00efve approach. Compared to straightforward construction, the authors should show the performance gain of the sequence of token node representation.\n* Mini-batch training for large-scale graphs has already been studied in the literature. Unless the authors present a new technique, which significantly improves scalability, It is hard to consider the improved scalability as the contribution of this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "* Overall, the paper reads well, and the architecture is clearly presented with figures.\n* Since the method is simple, it doesn't seem to have a problem with reproducibility.\n* Novelty is somewhat limited. But processing the multiple tokens for a node separately and merging them in the attention-based readout layer is interesting. To prove the value of this architecture, the authors need to provide ablation studies or additional analyses.\n* The authors claim that the proposed method is theoretically superior to Decoupled GCN. But decoupled GCN is not clearly introduced. \nAlthough the proposed method exhibits empirically strong performance, technical contributions and novelty are limited.",
            "summary_of_the_review": "The proposed method shows a significant performance gain. Also, the multi-hop representations for each node and merging transformed node features in the attention-based readout layer are interesting ideas. However, the authors did not provide experimental results to justify their architecture. Lastly, scalability is improved by well-known mini-batch training. Overall, this paper has limited novelty and a few missing analyses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_wvBc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3965/Reviewer_wvBc"
        ]
    }
]