[
    {
        "id": "zdVwH2TL1LC",
        "original": null,
        "number": 1,
        "cdate": 1666558736215,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558736215,
        "tmdate": 1666716892689,
        "tddate": null,
        "forum": "WhwtdGkbaDr",
        "replyto": "WhwtdGkbaDr",
        "invitation": "ICLR.cc/2023/Conference/Paper3983/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the problem of providing generalization bounds using the disintegrated PAC-Bayesian framework. The main idea behind the paper is that we can start with an arbitrary complexity measure which provides a hierarchy over the hypotheses. Then the authors show that a particular learning algorithm based on the Gibbs posterior can be used for learning. Moreover, using the PAC-Bayesian framework we can show that the complexity measure arises in the upper bound on the generalization.  ",
            "strength_and_weaknesses": "The paper is very-well written, and the idea is presented very clearly.\n\nI think there is a mismatch between the motivation explained in the intro and the results: In particular, in the intro there is a discussion on the challenges behind proving generalization of \"SGD\". The results mentioned in the intro state that proving generalization is difficult because most of the theoretical results for \"SGD\" empirically do not correlate with the generalization gap.\n\nLater in the paper the authors provide an abstract learning algorithm based on \"Gibbs posterior\" which takes into account the complexity measure. Also, there are some technical details such as, the loss function is not cross entropy,....\n\nAnother point to consider is that in the learning theory community, the main question is why does SGD without any \"regularization\" generalize? It is quite different from the results in the paper since the proposed algorithm is not SGD, also the proposed algorithm is based on using an \"explicit\" form of regularization. Again you can contrast it with the implicit bias results for SGD.\n\nQuestions:\n\n1- For a given problem what is the best complexity measure? Is there any theoretical insight on that?\n\n2- Generalization can't show the learnability. I would suggest that the authors compare the performance of their algorithm with SGD in terms of the \"population error\"?\n\n3- Do the results suggest that we should use the proposed algorithm compared to SGD? I agree that the proposed algorithm comes with theoretical guarantees. \n\n4- You can't sample perfectly from the Gibbs distribution. It is interesting to study the impact of the stochastic mala on the generalization. There are a few results in the literature on the finite time generalization performance of such a samplers:\n\n[1] Negrea, Jeffrey, et al. \"Information-theoretic generalization bounds for SGLD via data-dependent estimates.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[2] Mou, Wenlong, et al. \"Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints.\" Conference on Learning Theory. PMLR, 2018.",
            "clarity,_quality,_novelty_and_reproducibility": "In general, I found this paper really well-written, and all the results are clear.",
            "summary_of_the_review": "This paper is interesting. However, there is a big gap between the motivation behind the paper and the actual results. It is my main concern. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_g1xt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_g1xt"
        ]
    },
    {
        "id": "0WxAYKjdnp",
        "original": null,
        "number": 2,
        "cdate": 1666561583281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666561583281,
        "tmdate": 1666561583281,
        "tddate": null,
        "forum": "WhwtdGkbaDr",
        "replyto": "WhwtdGkbaDr",
        "invitation": "ICLR.cc/2023/Conference/Paper3983/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper leverages the framework of disintegrated PAC-Bayes bounds to derive a generalization bound for the Gibbs algorithm that involves an arbitrary complexity measure. The proposed bound stands in probability jointly over the hypotheses and the learning sample, which can be used to tighten the complexity for a given generalization gap since it can be set to fit both the hypothesis class and the task.",
            "strength_and_weaknesses": "Strength:\nThe theoretical part of the paper is very clear. Although Thm 3.1 highly depends on the structure of the Gibbs algorithm, the resulting form is correct and novel.\n\nWeaknesses:\nOne major drawback is the tightness of the bound. From Figures 1 and 2, it seems that the proposed bounds are loose in most cases and are only informative when the empirical risks are high, which is not the current operating regime for deep learning. \n\nA lot of important references are missing. This paper completely ignores a recent line of work on information-theoretic generalization bounds. Though the original forms of these bounds are in expectation [1,2], more recent results [3] can provide high-probability generalization bounds and should be mentioned in the discussion. \n\n[1] Xu, Aolin, and Maxim Raginsky. \"Information-theoretic analysis of generalization capability of learning algorithms.\" Advances in Neural Information Processing Systems 30 (2017). \n\n[2] Bu, Yuheng, Shaofeng Zou, and Venugopal V. Veeravalli. \"Tightening mutual information-based bounds on generalization error.\" IEEE Journal on Selected Areas in Information Theory 1, no. 1 (2020): 121-130.\n\n[3] Hellstr\u00f6m, Fredrik, and Giuseppe Durisi. \"Generalization bounds via information density and conditional information density.\" IEEE Journal on Selected Areas in Information Theory 1, no. 3 (2020): 824-839.\n\nMore specifically, the following two papers on the generalization error for the Gibbs algorithm that provides 1/m in expectation bounds are not discussed. \n\n[4] Kuzborskij, Ilja, Nicol\u00f2 Cesa-Bianchi, and Csaba Szepesv\u00e1ri. \"Distribution-dependent analysis of Gibbs-ERM principle.\" In Conference on Learning Theory, pp. 2028-2054. PMLR, 2019.\n\n[5] Aminian, Gholamali, Yuheng Bu, Laura Toni, Miguel Rodrigues, and Gregory Wornell. \"An exact characterization of the generalization error for the Gibbs algorithm.\" Advances in Neural Information Processing Systems 34 (2021): 8106-8118.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is novel and Reproducible, as proofs and codes are provided.\n\nHowever, there are a lot of details that need to be clarified. \n\nI do not fully understand the training process. To my understanding, you first train different models with SGD using the bounded cross-entropy loss from Dziugaite & Roy (2018). Then, using the output of SGD as initialization in Algorithm 1, a new $w$ is sampled from the Gibbs algorithm with different complexity measures $\\mu(h_w,S)$. What is the loss function used in Algorithm 1 (or eq(9))? The zero-one loss or the bounded cross-entropy loss? It is mentioned in the paper that $\\rho_S(w)$ (Gibbs posterior distribution) is hard to compute, then how do you compute $\\rho_U(w)$ in Line 6 of Algorithm 1? \n\nEventually, all different complexity measures will induce a different Gibbs algorithm, so what is the point of the comparison in Figures 1 and 2? Dist L2 and Sum Fro are better regularizers in training compared to the others? Or the proposed bounds are tighter for these complexity measures? The authors are encouraged to elaborate on the insights readers can obtain from these experiments.\n",
            "summary_of_the_review": "Overall, it is an interesting paper with some novel results. However, I feel that the authors oversell their contribution in the title and abstract. The proposed results only work for Gibbs distribution. If I understand correctly, different complexity measures $\\mu(h,S)$ will induce different learning algorithms, which are hard to implement and compare in practice. Also, I would expect that the structure of the Gibbs distribution could provide a tighter bound or better converge rate, but the resulting bounds are not as tight as I mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_H8Gk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_H8Gk"
        ]
    },
    {
        "id": "iSKeOZ010Dz",
        "original": null,
        "number": 3,
        "cdate": 1666675461553,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666675461553,
        "tmdate": 1666680806010,
        "tddate": null,
        "forum": "WhwtdGkbaDr",
        "replyto": "WhwtdGkbaDr",
        "invitation": "ICLR.cc/2023/Conference/Paper3983/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a PAC-bayes bound that is able to utilize and arbitrary complexity measure. They define a complexity measure, $\\mu$ as a function that maps a pair $(h, S)$ to a real number, where $h \\in \\mathbb{H}$ is a hypothesis, and $S$ is a labeled training sample. This notion generalizes common complexities such as the norm-weight, or the Rademacher complexity. They then show that the following posterior distribution, $\\rho_S(h) \\propto \\exp \\left[ -\\alpha R_S(h) - \\mu(h, S)\\right]$ has a generalization gap that can be bounded with high probability. Their bound is highly general, due to the abstraction of $\\mu$. \n\nTo empirically validate their results, they also give a way of approximately sampling from the posterior distribution above (which is quite non-trivial due to the nature of the distribution). They then apply this to several natural complexity  measures.",
            "strength_and_weaknesses": "This paper offers an interesting general theoretical result that leverages its generality to increase its practical relevance. In particular, it allows a practitioner to understand their algorithms convergence for arbitrary complexity measures. This is particularly relevant because it is well known that traditional complexity measures often do not provide relevant bounds for practical forms of generalization.\n\nI think this paper could have included a further discussion of complexity measures -- both including more examples as well as providing more context to their formalism. In particular, I believe that outlining common properties that one would expect the function $\\mu$ to have could be very helpful for building better intuition.  ",
            "clarity,_quality,_novelty_and_reproducibility": "This is a highly technical yet relevant work. It operates at a very high level of abstraction, which makes understanding a little bit difficult. Nevertheless, I believe this to be an overall strength of the paper -- the entire point of the paper is that it applies to arbitrary complexity measures. I am not familiar with the literature in this area, but this work appears to be novel for me. ",
            "summary_of_the_review": "I recommend this paper for acceptance. I think that it could benefit from increased intuition and more examples, but both of these are easily addressable. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_m9hB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_m9hB"
        ]
    },
    {
        "id": "-GY83Ocq-Td",
        "original": null,
        "number": 4,
        "cdate": 1666770714754,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666770714754,
        "tmdate": 1666770714754,
        "tddate": null,
        "forum": "WhwtdGkbaDr",
        "replyto": "WhwtdGkbaDr",
        "invitation": "ICLR.cc/2023/Conference/Paper3983/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "\nA general framework to compute PAC-Bayesian Generalization bounds is provided. The bounds are \u201cdisintegrated\u201d in the sense that they bound the generalization gap by a function which involves both the trained hypothesis h and another hypothesis h\u2019 drawn from the prior distribution. The bounds hold with high probability over the joint draw of the sample, h\u2019 and h, where h is obtained via the Gibbs distribution from classic PAC Bayesian bounds. Sampling from this distribution is analogous to performing empirical risk minimization with added randomness/regularization which depends on the definition of a regularizing function mu.  \n\nThe main theorems are Theorem 3.1, which applies to any priori, and Corollary 3.1, which applies to the case of a uniform prior over a set \\mathcal{H}. Theorem 3.1 bounds the generalization gap with high probability by the sum of three terms: the last term is a function of delta and is similar to an expected generalization gap of a sample from the prior (modulo exponentials and logs): in more concrete situations such as Corollary 3.1, this term is O(constant/\\sqrt{m}\\delta^2) and therefore doesn\u2019t actually depend on the prior pi. The control of this quantity in Cor 3.1 is obtained via a result of Maurer (cf. appendix F). The second term in Theorem 3.1 is the log of the ratio of the prior probabilities of h\u2019 and h, which is a disintegrated analog of the KL divergence which would appear in classic PAC Bayesian bounds. Finally, the first term in Theorem 3.1 is the difference between the regularized risks of h (\u201ctrained\u201d, i.e. drawn from Gibbs) and h\u2019, drawn from the prior. This term can be interpreted as an empirically computable complexity measure, and is the main term left in corollary 3.1. The main ingredient in the proof of Theorem 3.1 is Theorem 2.1 which is taken from Rivasplata et al. \nExperiments show that the bounds provide a good control of the true risk at some data-rich regimes where the generalization gap is nearly non existent. Sampling from the Gibbs distribution is achieved via a \u201cstochastic\u201d (i.e. batch-version of the)  Metropolis Adjusted Langevin Algorithm. Further experiments investigate the effects of the regularization parameter alpha. \n\nThere is an attempt to show that the proposed setting supersumes the uniform convergence approach (as well as algorithm dependent approaches) to generalization bounds, but the arguments are not convincing at all. \n\n",
            "strength_and_weaknesses": "Strengths: \n\nDisintegrated PAC Bayesian bounds are quite interesting and the main result theorem 3.1 seems original and worthwhile \nI like the fact that the appendix always puts a copy of the theorems to prove, which means the reader doesn\u2019t have to keep going back and forth between the paper and the appendix. \nThe writing is good in general and there are not many grammatical mistakes. \nThe proof of Theorem 3.1 appears correct to me, and so does the proof of Corollary 3.1.  \nThe stochastic MALA algorithm seems to be correctly implemented.\n\nWeaknesses (including main points to be addressed in a rebuttal, numbered for the authors\u2019 convenience): \n\n1.\t(TL;DR: state the exact page of Nagaragan and Kolter where Proposition D.2 appears, substantially rewrite and clarify Section D)\nThe paper claims that their bounds supersume the uniform convergence approach, but It is very hard to make sense of the \u201cproofs\u201d. The claim is also misleading in general since no bound analogous to uniform convergence bounds is derived for any concrete architecture.  There are errors in the \u201cdefinitions\u201d as well. Definition 3.1 is not very well written, but it is still technically possible to make sense of it. On the other hand, Section D makes very little sense. \n     1.1 **The \u201cproofs\u201d of propositions D.2 and D.3 (which are almost exactly identical) merely seem to go back and forth through tautologies**. Although I admit I may have missed something due to my lack of familiarity with PAC Bayesian bounds, I am quite certain that the proofs are at best very unclearly executed: Proposition D.2 seems to be a reformulation of the concept of uniform generalization bounds taken from Nagarajan and Kolter 19. The authors say that NK19 failed to provide a proof, but it seems unlikely that the result in NK19 needs proof and the current treatment is messy. **Prop. D.2 states that \\Phi_u needs to fulfil definition D.1, which includes equation (14).  This seems to be an assumption of the proposition. The proposition statement than says that equation (14) (from the assumed definition D.1) is equivalent to another trivial reformulation of it.**  \n\n1.2\tThe nearly identical Corollaries D.1 and D.2 are not better. First of all, the corollaries claim to use Theorem 3.1 but do not explicitly state where they do so, though I believe it is exactly after the line \u201cfor any hypothesis h\u2026 the definition of the parametric function\u2026 bound\u201d.  The conclusion there does not seem sound either: Theorem 3.1 is implicitly used there in a form that doesn\u2019t involve the probability over h\u2019 being drawn from the prior. It is also not clear to me why the definition of mu is different depending on whether h=h\u2019 or not (please explain). \n2.\tIn Section 4.2 (experiments), you claim that the set H  \u201ccorresponds to the hypotheses h_w that can be obtained from this initialization\u201d (the initialization being that of He et al. 2015). This initialization requires drawing from gaussian distributions, but corollary 3.1 only applies to a uniform distribution on H. Am I correct in assuming that you use the scaling/normalizing factor of the normal initializations from He et al. 2015 to define lower and upper thresholds that define a uniform distribution over the weights? If so, this needs to be explained thoroughly. It is also very unclear how this would maintain the benefits of He initialization since uniformly sampling from a hypercube is radically different from sampling from a multivariate gaussian in terms of dimensional dependence. \n3.\t It seems like most of the experiments are conducted at a regime where there is so much data that the generalization gap is near zero, which makes the bounds far less informative.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written. However, there are substantial issues with Section D of the supplementary (see weaknesses).  Would also like to hear from the authors regarding point 2 in the \u201cweaknesses\u201d. \n\nI also personally found it a little hard to end up with a good grasp of what the three terms in Theorem 2.1 correspond to, but it may be because I have less exposure to the PAC Bayesian setting. \n",
            "summary_of_the_review": "I enjoyed some parts of the paper, including the main theorem and corollary. The paper is generally well written, but some definitions are imprecise. The comparison to uniform convergences bounds unfairly places the current work above them without providing a concrete framework which could allow one to derive bounds as concrete as uniform convergence ones (without having to sample from the prior or estimate other quantities). Section D in the supplementary is sloppy and repetitive. Overall, only the short proof of Theorem 3.1  counts as an original theoretical contribution, which is not much for as prestigious a conference as ICLR. \n\nI apologize if I missed something as I am not an expert on PAC Bayesian bounds, I am looking forward to the thorough rebuttal and willing to change my score if the other reviews and the rebuttal warrant it. \n\n\n\n=================*More minor* comments=================\n\n1. I would really appreciate a more specific reference (exact page and theorem) for Theorem 2.1 from Rivasplata et al. A proof is provided in the present paper and it seems correct, but it would be nice to see where the result is from (I couldn\u2019t find the exact statement easily when browsing through the reference).\n\n2. In the proof of Theorem 2.1, you might want to explain how you get to the last line in terms of the change of measure which absorbs the $ln(pi(h\u2019)/\\rho_S(h\u2019))$ term. This is especially important since there are other minor typos in the proof (\\mathcal{S} and \\mathbb{S} are used interchangeably to refer to the same quantity and $\\phi(h,S)$ should be $\\phi(h\u2019,S)$ in the exponential at the end of the first equation.\n\n3. On page 13 \u201cby simplifying the left-hand side \u2026\u201d I think you mean \u201cright-hand\u201d. Note that the last two equations immediately above that line are exactly identical. It seems there is a missing simplification (it is not hard to fill the gap though). \nIt would be nice to have a pointer to the proof of Prop. D.1 in section D.2, since the proof is in section D.4. \nIn the preliminaries (\u201csetting\u201d), in the definition of the risk, it seems that you are restricting yourself to classification, which needs to be made clearer earlier on. It seems the assumption is necessary to define kl the quantities in Corollary 3.1. \n\n4. The notation in the main Theorem 3.1 includes two uses of h\u2019 as a different dummy variable.\n\n   5.Below corollary 3.1, \u201ckl\u201d (the KL divergence of the Bernouilli distributions induced by the numbers (\\in (0,1)) provided as arguments ) is not defined. A pointer to page 27 would be nice. \n\n6. For readability, it might be nice to repeat the fact that the quantity at line six of the algorithm can be computed despite the fact that \\rho_U cannot, due to the simplification of the normalization constants in the numerator and denominator. \n\n7. Do you know whether there are guarantees analogous to those of Chib and Greenberg for the stochastic variant of MALA you use? Is the stochastic version original to your work or does it appear elsewhere?\n\n\n=======================Very minor typos etc.======================\n\n\n1. Capital letter at \u201cTheorem 3.1\u201d on line two of the proof of Corollary 3.1 \nThere are a few somewhat inelegant sentences such as: \n\n 2. Section 2.2 just above definition 2.1: \u201cobtained after obtaining\u201d\n\n  3.  Page 5: \u201cConcerning the tightness of the bounds, it may appear loose.\u201d\n\n   4. Top of  Page 15: \u201c The generality of our framework can thus generalize\u201d\n\n\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_g9Hx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3983/Reviewer_g9Hx"
        ]
    }
]