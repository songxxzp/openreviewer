[
    {
        "id": "BKk5hA1IOp",
        "original": null,
        "number": 1,
        "cdate": 1666681176427,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666681176427,
        "tmdate": 1666681176427,
        "tddate": null,
        "forum": "P4bXCawRi5J",
        "replyto": "P4bXCawRi5J",
        "invitation": "ICLR.cc/2023/Conference/Paper1298/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "CLIP has shown remarkable performance on zero-shot visual recognition. However, adversarial examples still greatly affect CLIP's performance. This work propose a text-guided contrastive adversarial training loss to adopt CLIP to attain adversarial robustness for the datasets that are not seen during adversarial training. They show that a naive adversarial training of CLIP on ImageNet achieves the best performance on ImageNet but fails on other classification datasets. Their approach, on the other hand, performs much better for zero-shot tasks, despite being slightly worse on ImageNet than the naive adversarial training. They evaluate their zero-shot performance on 15 image datasets, and perform comprehensive ablation studies of their method.",
            "strength_and_weaknesses": "# Strength\n\n- Zero-shot adversarial robustness is an important problem, given the fact that large-scale models are becoming some sort of infrastructure in practice.\n- Their approach empirically performs well compared to other baselines.\n- They even show that their approach doesn't require ground truth labels, and using psudo-labels (via CLIP image-to-text retrieval) is enough to attain similar performance.\n- They have done comprehensive ablation study including:\n\t- The effect of text supervision in contrastive adversarial training.\n\t- The effect of visual prompt design (e.g. appending an additional token is better than adding a learnable noise to the raw input image.)\n\t- These ablation studies provide useful insights for future research in zero-shot adversarial robustness.\n\n# Weaknesses \n\n- No major weaknesses. See Clarify and Minor for small issues.",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\n-  For contributions: \"our analysis provides useful lessons to understand the problem of zero-shot adversarial robustness.\"\n\t-  Their contributions would be easier to understand if the authors could list specific lessons instead (or highlight one or two most important lessons). \n- I assume y_ij = -1 when i is not equal to j, but this is not explicitly mentioned.\n\n# Novelty\n- The problem setting (e.g. zero-shot adversarial robustness) and their approach is novel. \n\n# Reproducibility\n- They have enough details (e.g. Implementation Details section) to reproduce their results.\n\n## Minor\n- \"finetuning has higher gains than VPT as more parameters are tuned.\" -> \"Finetuning has...\"\n- Figure 7:  \"By change(-ing) the interpolation ratio for the adapted CLIP and the vanilla CLIP,\"",
            "summary_of_the_review": "The problem of zero-shot adversarial robustness is important and this work illustrates how naive adversarial training cannot deal with zero-shot settings. Then they propose a new approach based on image-text embedding alignment to achieve superior zero-shot adversarial robustness. They also conduct a comprehensive set of ablation studies which yield useful insight for future research.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_8Zoa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_8Zoa"
        ]
    },
    {
        "id": "NETlOgJgHq",
        "original": null,
        "number": 2,
        "cdate": 1666701617281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666701617281,
        "tmdate": 1666701617281,
        "tddate": null,
        "forum": "P4bXCawRi5J",
        "replyto": "P4bXCawRi5J",
        "invitation": "ICLR.cc/2023/Conference/Paper1298/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper explores the problem of adapting large-scale pre-trained models for adversarially robust zero-shot classification. It is found that vanilla adversarial training on a single task may reduce the zero-shot capability of the pre-trained model. To improve the zero-shot adversarial robustness, a text-guided contrastive adversarial training (TeCoA) is proposed, which aligns the image embeddings of adversarial examples and the text embeddings of the standard prompts for zero-shot predictions. Experiments validate the effectiveness of TeCoA.",
            "strength_and_weaknesses": "[Strength]\n1. The problem of zero-shot adversarial robustness is important in practice and has not been well explored.\n2. Some of the results are interesting and may inspire future works on this problem, e.g., Figure 1 and Table 2.\n\n[Weaknesses]\n1. Technical novelty. The proposed TeCoA is nearly identical to: (1) first construct a zero-shot linear classifier head with the pre-defined text prompts for zero-shot classification, as done in (Wortsman et al., 2022), and then (2) perform vanilla adversarial training (using CE loss) with the classification head frozen. Hence the main technical contribution may be that the initialization (and/or freezing) of the classification head is important for adversarial fine-tuning.\n2. Missing important experimental details. The missing implementation details include model architecture, training epochs and which visual prompt design is used. Besides, the generation of pseudo text labels for images (for results in Table 2) needs further explanation.\n3. Robustness evaluation. PGD-100 may not be a reliable attack for robustness evaluation. It would be better to consider stronger attacks like AutoAttack (or at least the two APGD attacks used in AutoAttack) [1]. Besides, the main results in Table 1 are based on eps=1/255, which may be too small. Using larger eps like 4/255 as in [1] may be more convincing.\n\n[1] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n4. Explanation of abnormal results. As shown in Table 1, while FT (TeCoA) is the best on most datasets, LP (CE) and VPT (adv.) outperform other methods by a large margin on HateMemes and PACM. Is there any possible explanation?\n5. Table 1 and Table 3 are inconsistent. Especially, there are two rows starting with \"+VP (TeCoA)\", which is confusing.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is readable, but some important details are missing. The novelty is limited as discussed in the Weaknesses. The reproducibility may be questionable due to some missing experimental details and that code is not provided.",
            "summary_of_the_review": "While this paper studies an interesting and not well-explored problem, and provides some experimental results that may promote the understanding of zero-shot adversarial robustness, there is a lack of insight and technical novelty. Besides, the missing details and weak robustness evaluation reduce the reliability of the results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_J4qy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_J4qy"
        ]
    },
    {
        "id": "tMfUPSgCjF",
        "original": null,
        "number": 3,
        "cdate": 1666702971556,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666702971556,
        "tmdate": 1666702971556,
        "tddate": null,
        "forum": "P4bXCawRi5J",
        "replyto": "P4bXCawRi5J",
        "invitation": "ICLR.cc/2023/Conference/Paper1298/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Authors are the first to show that zero-shot image recognition models built on top of CLIP are still susceptible to adversarial attacks, and that standard adversarial training objective, while effective at preventing adversarial attacks, destroys the rich image-language capability of CLIP, making the defended model useless for zero-shot recognition on different tasks/datasets. Authors propose TeCoA - a novel objective that takes the image-language nature of the model into account during adversarial training via visual prompt tuning (VPT) and contrastive learning. Authors show how the proposed objective interacts with various task adaptation techniques, including linear probing, full and partial finetuning, image and token-level prompting. More specifically, authors propose to perform an adversarial attack on the standard contrastive image-text alignment objective used in CLIP - an adversarial image perturbation aims to make the correct image-text pair to have lower cosine similarity then an incorrect one. Authors propose two ablations that verifies that observed gains indeed come from robustifying the vision-language model, and not from robustifying the vision backbone itself - contrastive alignment to one-hot vectors and an image-to-image loss. Authors show that when it comes to zero-shot adversarial robustness, the proposed approach beats the baseline adversarial training and both ablations, and that finetuning outperforms visual prompt tuning. Authors also investigate the effect of the dataset size, attack strength, prompt design, number of adapted parameters during fine-tuning, and the of pareto optimum the clear-vs-robust accuracy.",
            "strength_and_weaknesses": "The paper is very well written and easy to follow: the motivation is clear, the objective and the training procedure are clearly defined. All experiments and ablations are clearly motivated, described, and reported as well. The results are promising. The provided intuition for why the proposed method helps while baselines fail makes intuitive sense.\n\nThe only part of the paper that I found to be not entirely clear and self-contained is the specifics of the image-to-image loss used in ImgCoAdv experiments.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very well written and should be reproducible from the description in the paper alone. I lack the background in zero-shot learning to judge the novelty of this work compared to prior work. ",
            "summary_of_the_review": "\nI think this is a great paper - it establishes an impactful problem, explores a simple solution in great detail with a lot of ablation experiments to verify their intuition, the paper is very well written.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_Uucq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_Uucq"
        ]
    },
    {
        "id": "3EmdrRTbaXp",
        "original": null,
        "number": 4,
        "cdate": 1666726866428,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666726866428,
        "tmdate": 1666726866428,
        "tddate": null,
        "forum": "P4bXCawRi5J",
        "replyto": "P4bXCawRi5J",
        "invitation": "ICLR.cc/2023/Conference/Paper1298/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of zero-shot adversarial robustness: adapting pretrained large-scale vision-language models to unseen target tasks with high robust accuracies. With the conjecture that language encoder plays an important role in achieving good zero-shot generalization ability, a contrastive based adversarial training objective is proposed to contrast between image and text embeddings. Several adaptation methods are analyzed, and some interesting discoveries are made for the visual prompt tunning method.",
            "strength_and_weaknesses": "Strength:\n\nThe proposed problem is new and under-explored.\n\nThe paper conducts rich experiments to compare diverse adaptations methods and several possible training loss functions. Many datasets are employed to evaluate zero-shot generalization.\n\nThe proposed loss function is well motivated by the observation that text encoder is important to zero-shot transferability. Ablation study verifies that the proposed method using text embedding for contrastive learning is important. \n\n\nWeakness:\n\nAlthough the setting is new, the proposed loss function is a direct use of contrastive loss for two modalities. Moreover, the adaptation methods used are all existing techniques.\n\nImplementation details are not sufficient. For example, it is unclear how exactly the adaptation methods are employed during training and what the objective functions of CoAdv or ImgCoAdv are. It would be better to give mathematical formulations of these methods in appendix.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Most explanations of the proposed method are clear and are presented with high quality. The technique is not novel and some implementation details are not given.",
            "summary_of_the_review": "This paper targets the problem of zero-shot adversarial robustness for CLIP, and it seems that it is a \u201cfirst do A+B\u201d type of work. Techniques used in the proposed methods are not new, while comprehensive experimental results show that the proposed method is effective through comparisons with reasonable baseline methods.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_eUU6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1298/Reviewer_eUU6"
        ]
    }
]