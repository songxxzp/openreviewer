[
    {
        "id": "2JDdA0B74gs",
        "original": null,
        "number": 1,
        "cdate": 1666417331577,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666417331577,
        "tmdate": 1666417331577,
        "tddate": null,
        "forum": "rVM8wD2G7Dy",
        "replyto": "rVM8wD2G7Dy",
        "invitation": "ICLR.cc/2023/Conference/Paper3691/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a semi-supervised learning algorithm for unbalanced data. The key component is a bias adaptive classifier to alleviate the problem that the model is usually biased towards the majority classes. The bias adaptive classifier, together with the regular classifier, are learned through a bi-level training protocol, where the model us updated first and a sampled balanced set is used to update the bias attractor parameters. Experiments on several long-tailed dataset show the outstanding performance compared to existing solutions.\n",
            "strength_and_weaknesses": "This paper is well written and easy to follow. The idea of using another classifier to rectify bias introduced in the original classifier is intuitively sound. Moreover, the formula and design is actually very straightforward yet effective, allowing others to easily reproduce the proposed method and add the proposed module into any SSL framework. Technically, the approach of bi-level training makes a lot of sense to me, when considering the cost function to be enforcing the similarity between gradient of a sample and the average gradient of a sampled balanced set. In addition, this architecture seems works pretty well under various imbalance conditions. \n\nThis work relaxes the assumption used by some existing works that unlabeled data and labeled data should follow similar distribution, and experiments show that it can handle both this case and reversed case (unlabeled data distribution is reversed from labeled data distribution). Apart from consistent gain over compared algorithms, this paper also provides solid ablation study to highlight a few important observations on how the proposed algorithm performs regarding different perspectives, such as pseudo-label quality improvement, capability of handling extreme label ratios. Such study is valuable and helps evaluate the contribution of this work, making this paper convincing.\n\nThere are several minor weakness, listed as follows:\n\n- Although the related works section covers most of recent SSL algorithms for class imbalanced problem, there are more papers that should be cited:\n\n   -  [1] Class-Imbalanced Semi-Supervised Learning with Adaptive Thresholding, 2022\n   -  [2] Debiased Learning from Naturally Imbalanced Pseudo-Labels, 2022\n\n  In particular, [2] also talks about debasing for pseudo-labels under class imbalance scenario, which is quite related to this paper. It would be good to include and discuss about them.\n\n-  The motivation of the proposed bias adaptive classifier is still a bit vague to me, although the structure looks simple. I understand that it takes into account the second order gradient when computing the upper training loss $L_{bal}$, but could not draw a crystal clear connection between updating bias attractive parameters and enforcing model to make more prediction towards the minority classes. How such architecture, i.e., residual connection, linear and non-linear classifier design, and bi-level training helps alleviate classifier bias? More intuitive explanations would help. Additionally, how does the sampled balanced set affect performance of the bias adaptive classifier?\n\nSome minor grammatical errors:\n\n- Note that CReST+ (Wei et al., 2021) fail in \u2026 -> fails in\n- In summary, such an nested training\u2026 -> such a\n- design a be-level learning algorithm \u2026 -> a bi-level",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and organized. It proposes novel solution for class-imbalanced SSL and the proposed method is relatively straightforward. With detailed information on training recipe, I think it would not be difficult to reproduce it.",
            "summary_of_the_review": "In summary, this is overall a well-written, well-organized paper and the proposed method is technically sound. Despite its simplicity, the proposed method consistently outperforms existing works on SSL for class imbalance problem, and the improvement is quite significant. There are a few unclear pieces regarding motivation of the bias attractor parameters and missing references.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_5NiX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_5NiX"
        ]
    },
    {
        "id": "OGOjjdLQcp",
        "original": null,
        "number": 2,
        "cdate": 1666516373569,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666516373569,
        "tmdate": 1670861309366,
        "tddate": null,
        "forum": "rVM8wD2G7Dy",
        "replyto": "rVM8wD2G7Dy",
        "invitation": "ICLR.cc/2023/Conference/Paper3691/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed a semi-supervised learning (SSL) method, especially for imbalanced classification. Since standard SSL methods based on pseudo labels often assign labels of head classes to unlabeled data, the obtained pseudo labels tend to have a small number of labels for tail classes, resulting in low classification performance. To cope with the problem, the bias adaptive classifier is proposed to reduce bias from imbalanced data. To train neural networks with the bias adaptive classifier, a bi-level optimization approach is employed. Through extensive numerical experiments, the proposed method outperformed various recent SSL methods. Also, the ablation study showed the usefulness of the proposed bi-level training with the bias adaptive classifier.",
            "strength_and_weaknesses": "##### Strengths\n- The idea of the proposed bias adaptive classifier is novel and has an impact on imbalanced SSL.\n- The bi-level training is important to train a bias adaptive classifier and its effectiveness is carefully investigated through an ablation study.\n- The superior performance of the proposed method was demonstrated on extensive numerical experiments and various recent SSL methods were compared with the proposed method.\n\n##### Weaknesses\n- The current explanation of the proposed method is not really clear. The presentation of the proposed method could be improved. \n- It seems that the proposed method requires intensive computation compared with the other methods because the proposed method requires a second-order gradient. If the computation time is provided during the rebuttal phase, I will reconsider this point.",
            "clarity,_quality,_novelty_and_reproducibility": "The explanation of the proposed method could be improved. Despite the novelty and quality of the proposed method, the explanation in Section 3.2 does not provide clear intuition why the proposed method works and the idea is effective. The current explanation says that a residual connection is key. However, the explanation, such as \"the proposed bias attractor can assimilate a wide range of training biases\", does not provide an in-depth understanding of why the proposed method works. My current understanding is that $\\Delta f_\\omega$ absorbs the bias from imbalanced distributions. I would be grateful if more in-depth explanations were provided for the proposed method.\n\nSome notations are ambiguous. In Eq.(2), $\\boldsymbol{I}$ and $\\circ$ seem not to be defined. Also, $|_{\\phi^t}^T$ in $G_i$ in Proposition 3.1.\n\nIn Figure 5, $\\gamma_l=100$ and $\\gamma_u=1$ were chosen for t-SNE visualization. It is interesting to see the results of the other values of $\\gamma_l$ and $\\gamma_u$. \nSuch a result might show that a high-quality representation can often be obtained by the proposed method, i.e., the result in Figure 5 is not owing to the specific choice of the hyperparameters.\n\nAppendices B and C seems to be useful information for reproducibility. Also, the authors declare that the source code will be made publicly available in Section 1. The results in this paper might be able to be reproducible by the code.",
            "summary_of_the_review": "This paper proposed the bias adaptive classifier for imbalanced semi-supervised learning. The idea looks novel, and the empirical results support its effectiveness. The reproducibility seems high. If the clarity of this paper is improved, the overall score will be increased.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_ihj1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_ihj1"
        ]
    },
    {
        "id": "B_gmsZw62z",
        "original": null,
        "number": 3,
        "cdate": 1666663319660,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663319660,
        "tmdate": 1666663319660,
        "tddate": null,
        "forum": "rVM8wD2G7Dy",
        "replyto": "rVM8wD2G7Dy",
        "invitation": "ICLR.cc/2023/Conference/Paper3691/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a novel imbalanced SSL method with pseudo-labeling.\nThe authors introduce a bias-adaptive module that can be trained to minimize\nclassification loss on a balanced dataset with bi-level optimization.",
            "strength_and_weaknesses": "### Pros\n\n1. The paper is easy to follow and well-written.\n2. The problem is well-motivated and intuitively makes sense.\n3. The reported performance exceeds the baselines significantly.\n\n### Cons\n\n1. The balance loss L_bal is just a subset of the original pseudo-labeling loss L \n   (balanced dataset B is a subset of the labeled dataset D_l).\n   Thus, if L is minimized, L_bal will also be minimized (if we are overfitting L).\n   The effect of introducing a balanced dataset here is unclear.\n   If L_bal is a hold-out dataset (instead of \"dynamically sampled from the labeled training set\"), \n   there may be some differences.\n\n2. A much simpler method is to keep the bias-adaptive module fixed when updating other parameters\n   and update the bias-adaptive module on the balanced dataset. \n   I suggest the authors compare this baseline method and discuss the difference compared with\n   the proposed method. The bi-level optimization may be overkill.\n\n3. Theoretical analysis of the convergence of such a bi-level optimizing method is necessary,\n   which may help understand the problem in 2.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: easy to understand.\n\nQuality: good.\n\nNovelty: fair.\n\nReproducibility: good.",
            "summary_of_the_review": "The paper is overall good-looking, but the method does not make much sense to me.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_juK4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3691/Reviewer_juK4"
        ]
    }
]