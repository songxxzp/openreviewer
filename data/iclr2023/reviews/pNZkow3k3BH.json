[
    {
        "id": "tMBU5K3PCLF",
        "original": null,
        "number": 1,
        "cdate": 1666106105659,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666106105659,
        "tmdate": 1672425263854,
        "tddate": null,
        "forum": "pNZkow3k3BH",
        "replyto": "pNZkow3k3BH",
        "invitation": "ICLR.cc/2023/Conference/Paper3410/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper finds that when backdoors exist in transformer models, particularly BERT, attention concentrates more on trigger tokens compared to clean tokens. They leverage this observation to improve the sample efficiency of backdoor attacks by adding an explicit loss term that encourages this observed behavior. Empirically, this leads to almost equally strong attacks (esp. in the clean-label attack case) with (1) less data and (2) without giving out information about the backdoor triggers.",
            "strength_and_weaknesses": "### Things that I liked\n\n- The paper is well written making it easy to understand.\n\n- The authors did a good job at describing the related work and situating their contribution in that context.\n\n- The authors are able to show that adding the loss term improves the sample efficiency of the data-poisoning attacks.\n\n- The authors consider a set of emprirical analysis to ensure the additional loss term does not result in easier detectability. For this, they observe that the clean input accuracy and attention scores remain similar to a vanilla (i.e. non-attacked) model. Futher, they consider how these attacks perform against defenders.\n\n### Things that need improvement / clarity\n\n- Why was the TAL not as effective for dirty-label scenarios? As per Fig 3, it only improved efficacy of the two invisible attacks; my guess is that the other attacks are already pretty sample efficient in the dirty-label setting and thus, the scope of improvment is small. Given the limited scope, I am not sure the impact of the paper.\n\n- Since the experiments are all done on the BERT cased model and evalutated on sentiment classification, the generalizability of the observation and thereby efficacy of the proposed loss is of concern. Given this is an empirical paper, serveral unanswered questions remail.\n  1. What happens on other sentence-level classification tasks beyond sentiment classification where the overall sentence structure can play a major role? Does the observation that attention scores concentrate on triggers still hold?\n  2. What about token-level tasks such as NER where the token-specific outputs of the transformers are considered as opposed to the [CLS] token encoding? (I did understand that some attacks do consider structure but since they are eventually applied to the sentence classification problem, it doesn't directly answer this question.)\n  2. What happen on other BERT based models? Does size of the transformer model play a role-- DistillBERT (smaller size), RoBERTA (similar size), GPT-2 (larger)?\n\n- The detection of these attacks, esp. due to lack of knowledge about triggers, seem to be the more difficult problem. Beyond simply echoing this sentiment, the authors simply choose to solve the simpler problem of improving the attack efficiency.\n\n- Given the observation that attention scores are concentrated on triggers, the TAL loss function is the first-order thing that comes to mind. Is there a way to improve the loss function? Does it need to be applied to the attention scores for the encoder layers? Why does the loss randomly pick as opposed to developing an attention pathway backdoor in the model (are such backdoor more detectable that the random sampling of attention weights & applying TAL on them)?\n\n- [Minor] Needs a proof reading (some mistakes follow):\n   - \"The the attack efficacy is robust to..\" -> \"The attack's efficacy is robust to..\"\n   - \"our TAL loss will not arise the attention abnormality\" -> our TAL loss will not give rise to an attention abnormality\"\n   - \"because of randomness data samples\" -> \"because of randomness in data samples\"",
            "clarity,_quality,_novelty_and_reproducibility": "The paper aids clarity and does a good job at situating its contribution in the realm of existing works. The additional materials aid understanding and reproducibility of the empirical results (although I did not run them at my end)",
            "summary_of_the_review": "The paper makes a niche contribution to improve attack efficacy but over states the generalizability of their results to transformer models with specific experiments [eg. the title itself talks about (all kinds of) transformers].\n\n--- AFTER REBUTTAL & CHANGES ---\n\nGiven this is primarily an empirical paper, I do believe the addition of results on other NLP base models like RoBERTA, DistilBERT, GPT-2 adds a bit more confidence on the efficacy of the TAL function.\n\nHaving said that, the authors bypass the more challenging questions on (1) detection of these attacks due to lack of knowledge about triggers and (2) TAL being the first-order approach that comes to mind given the findings about the attention scores. While the authors claim that their methods give more insights into the attention mechanism in such scenarios, they are not able to propose ways to detect these attacks. This also keeps my ethics concerns in place.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "The paper proposes a loss function to aid attackers in making the data poisoning attack sample efficient (thus, making the detection even more difficult). Unfortunately, beyond simply stating that this makes the already difficult detection problem all the more difficult, they don't solve it in anyway.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_Xodn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_Xodn"
        ]
    },
    {
        "id": "U2AOeLEgAr",
        "original": null,
        "number": 2,
        "cdate": 1666604330227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604330227,
        "tmdate": 1666604483086,
        "tddate": null,
        "forum": "pNZkow3k3BH",
        "replyto": "pNZkow3k3BH",
        "invitation": "ICLR.cc/2023/Conference/Paper3410/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a trojan attention loss for backoor attack in transformer-based NLP models. In the process of training, the loss manipulate the model to give the trigger word a higher attention weight, so as to improve the effect and efficiency of backdoor attack. They conducted the experiments to show TAL works for current different types of attack methods, and boosts the efficiency in both dirt-label and clean-label attack.",
            "strength_and_weaknesses": "Strengths:\n- The TAL loss is simple but effective method that can be applied to most of existing attack methods.\n\n- The experiments show that TAL significantly increased the attack efficacy of clean-label attack with very small poison rate compare to current attack methods.\n\n- This paper is well-written and easy to follow.\n\nWeaknesses:\n- Why just evaluating on sentiment analysis? It seems that more tasks should be applied to demonstrate the effetiveness of the proposed approach. Furthermore, why only choose BERT pre-trained model, the proposed approach is also applicable to other pre-trained models?\n\n-  The setting of the experiment is not sufficient, because TAL loss focuses on the effectiveness of the attack. In my opinion, the trend of ASR with the change of poison rate can be explored but not only conduct experiments with a specific small poison rate. By comparing the difference between attack method with TAL and without TAL, we can see the effectiveness of TAL more intuitively.\n\n-  This work is inspired by the paper Lyu et al. (2022), which analyzed the abnormal attention behavior in Trojan model. However, this paper also proposed a method to detect trojan model based on model\u2019s attention drift after inserting the trigger word. Theoretically, their method can detect the trojan model trained by your method very well. Why don't you try to use their method as defender in section 4.4?\n\n-  TAL enables the model to achieve a good attack effect with a small poison rate. However, based on the assumption that the attacker can access data and the training process, I believe that the performance of the attack is more important than the size of poison rate. Have you experimented with a high poison rate to verify that your method can still improve the performance of current attack method?",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity: The paper is easy to follow but still has several writing problems for example:\n\tSection 3.3, last paragraph: The the attack\u2026 -> The attack\n\n- Novelty: It\u2019s a novel and interesting method to manipulating the attention during training to improve attack efficacy.\n\n- Quality: The experiments and analysis part is not solid as there should be more detailed experiments to verify their methods.\n\n- Reproducibility: The experiments should be easy to reproduce if all data and models are reaseased.\n",
            "summary_of_the_review": "This paper proposes a trojan attention loss for backoor attack in transformer-based NLP models. Although it has confirmed its effectivenss in BERT model, I think the generalization of the proposed approach should be proved by other models and tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_XXQF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_XXQF"
        ]
    },
    {
        "id": "ywFIIKrzAs2",
        "original": null,
        "number": 3,
        "cdate": 1667471967238,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667471967238,
        "tmdate": 1667471967238,
        "tddate": null,
        "forum": "pNZkow3k3BH",
        "replyto": "pNZkow3k3BH",
        "invitation": "ICLR.cc/2023/Conference/Paper3410/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the attention distribution of an attacked Transformer models. Based on their findings, the authors propose a trojan attention loss to enhance the attack efficiency. The authors conducted a comprehensive study on this method, showing the effectiveness of their approach.",
            "strength_and_weaknesses": "Strength:\n- The method is well motivated by the study on the attention distribution\n- The designed loss is simple and easy to be used in many existing methods.\n- The experiments are comprehensive.\n\nWeakness:\n- The proposed method is designed for white-box attack. Is it possible to extend this method to black-box attack?\n- The backbone is only vanilla BERT. It would be better if the authors can show results on other pre-trained Transformers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: good, the paper is easy to read.\n\nQuality: good, the paper present a good quality research\n\nNovelty: good\n\nReproducibility: fair. The code is released and it seems to be easy to implement.",
            "summary_of_the_review": "This paper present a novel and well-motivated method to improve the attack efficiency. The results are promising, along with lots of analysis. Overall, I would recommend accept this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Privacy, security and safety",
                "Yes, Potentially harmful insights, methodologies and applications"
            ],
            "details_of_ethics_concerns": "As this is a work focus on Trojan attack, may be an ethics review is required to make sure this method will not be used to attack real world systems.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_qVRi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_qVRi"
        ]
    },
    {
        "id": "bVaXqzZlp3z",
        "original": null,
        "number": 4,
        "cdate": 1667482039999,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482039999,
        "tmdate": 1667482039999,
        "tddate": null,
        "forum": "pNZkow3k3BH",
        "replyto": "pNZkow3k3BH",
        "invitation": "ICLR.cc/2023/Conference/Paper3410/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on the backdoor attacks against transformers for NLP tasks.\nThe authors observe that backdoored transformer models (like BERT) have higher attention weights on trigger tokens.\nDue to this empirical observation,\nthe authors propose a new loss called Trojan Attention Loss (TAL) which further improves the attention weights of trigger words.\nWith experiments on different attack methods, the authors show that adding this loss term can enhance the efficiency of backdoor attacks with fewer data and fewer training epochs.\n",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper is well-written and easy to understand.\n2. The proposed loss term is simple and easy to follow. The observation is easy to understand, and the proposed method is intuitive.\n3. The proposed loss term is compatible with most attack methods.\n4. Experiments show adding their loss term can improve the attack efficacy (with fewer data and fewer training epochs).\n\n\nWeaknesses:\n\n1. The paper only considers BERT for experiments. It is a bit over-claimed since other transformer models are ignored. It is suggested to do experiments on other common transformer models like RoBERTA.\n\n2. The paper only considers sentiment analysis tasks for experiments. We expect more experiments on other NLP tasks. We believe these are quite significant to improve this empirical paper. Otherwise, it is difficult to evaluate the generalization of the proposed method and the impact of this paper.\n\n\n\nWriting Suggestions:\n\n1. It is better to add the explanations of Dirty-Label and Clean-Label in Section 4.1 like the caption of Table2.\n2. It is better to clearly state that Attn-x uses the proposed TAL loss while x does not in the experiment section.\n3. Figure 3: Drity-Label --> Dirty-Label",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity and clearly explains the contributions with enough background information.\nThe proposed method is simple and novel, but their experiments are not sufficient enough to fully support their claims.\nIt should be easy to reproduce since the authors have submitted their code.\n",
            "summary_of_the_review": "This paper proposes a novel Trojan Attention Loss to improve backdoor attack efficacy.\nHowever, the method is based on empirical observation. So it is still unknown whether the proposed loss term also has a good generalization ability for other transformers and NLP tasks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_5z1R"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3410/Reviewer_5z1R"
        ]
    }
]