[
    {
        "id": "gCbrzsGVnF",
        "original": null,
        "number": 1,
        "cdate": 1666708758006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666708758006,
        "tmdate": 1670514462848,
        "tddate": null,
        "forum": "yXk83o735o",
        "replyto": "yXk83o735o",
        "invitation": "ICLR.cc/2023/Conference/Paper4763/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on the situation where the training data are corrupted by noise, which often happens in practice, and evaluates to what extent these nuisances affect the uncertainty quantification based on the conformal prediction. The main conclusion is that one should not worry in practical cases. ",
            "strength_and_weaknesses": "The topic of the paper is highly interesting and relevant to the AI/ML community. \nMy main concern is that the text as written is quite confusing and I am not sure I understand the significance of their results. The assumptions seem to be very strong and unverifiable, contrary to some of the (over) claims in the text. See more details below.",
            "clarity,_quality,_novelty_and_reproducibility": "The proofs are densely written and quite hard to follow.\nSee some details in the section below.",
            "summary_of_the_review": "1) The reference to the probability distribution at play is quite confusing.\n   - In Equation 1, P refers to the tensorized probability over the calibration and test data.\n   - The same P is used in equation 2, however C_noisy involves the corrupted label that does not follow the same distribution\n   - The same P also defined the oracle classifier P(tilde Y | X) where no other calibration data are involved.\n\n2) Why is the assumption that access to the oracle classifier is reasonable in any way? It is not testable (oracle is never available) and this seems to be equivalent to the trained model classifying perfectly any noisy data. In that case, it seems quite natural that building a conformal prediction set based on a perfect classifier will behave perfectly. I am not sure to understand the insights/meaning of this result.\n\n3) Taking into account variation in the labels due to noise, seems to be a special case of distribution shift. How does the current contribution differ from the conformal prediction that accounts for such a shift? Also, the label noise seems to not affect a conformal set whose underlying model is invariant to slight variation in the labels. Does this relate to a conformal set based on stability assumptions? \n\n4) Some points on the Proofs.\nIn the proof of Lemma A1, the authors might want to clarify a bit formally \u201chat f be a classifier that ranks the classes in the same order as the oracle model\u201d and also how is this precisely used in the proof. Seems to be in the monotonicity assumption but it is unclear and hard to follow.\nSimilarly, the proof of proposition 5 is very unclear. Y and tilde Y are discrete random variables, assuming a density for their distribution is a bit too strong here. Also, the Proposition talks about any tilde Y not equal to Y (in distribution) but the proof assumes a substantially strong hypothesis. For example, the set A can be empty depending on p and tilde p. The design of the adversarial score is also unclear: the set A is independent of the input data x (but this is ok) and also independent of any output y. Then the sequence of s(x_i, y_i) are not exchangeable (and so not valid scores) since the score is invariant wrt to the data. The rest of the proof is also very confusing.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_mtdD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_mtdD"
        ]
    },
    {
        "id": "u7CRGAFWtpw",
        "original": null,
        "number": 2,
        "cdate": 1666811923682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666811923682,
        "tmdate": 1666811923682,
        "tddate": null,
        "forum": "yXk83o735o",
        "replyto": "yXk83o735o",
        "invitation": "ICLR.cc/2023/Conference/Paper4763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addressed an interesting question: is conformal prediction (CP) robust to label noise? They explored several situations including both regression and classification cases. They also constructed the corresponding theories to support their claim. The authors also included experiments on both synthetic and real examples, demonstrating the robustness of conformal prediction. The paper is easy to follow, and the writing is fluent. ",
            "strength_and_weaknesses": "1.\tI have some concerns with the assumption of Section 2.1. The labels of images are not usually randomly corrupted. For example, dogs are often misclassified as cats, and vice versa. The confusion matrix case looks more reasonable. However, it is not supported by any theoretical results. \n\n2. The second formula in Section 2.1 involves \\hat{\\pi}_y(x), which is never mentioned before use. Similarly, I can guess the meaning of \\hat{C}_{\\rm noisy}^{\\rm APS} in proposition (1). The authors should define the notations before use.\n\n3. In Figures 1, 2, 4, and 5, there are some bars between distributions. The authors didn\u2019t explain what they mean. The authors only mentioned the error bar in the caption of Figure 3. \n\n4. Possible typos: \n1) The second formula of section 2.1: \\hat{\\pi}_y(x)_y. The last subscript y looks redundant. \n2) The last sentence of the second last paragraph of section 3.1: \"be more robust to label noise then HPS\". Should it be \"than\"?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, with both theoretical and numerical support. ",
            "summary_of_the_review": "The topic that analyzing whether conformal prediction is trustful when label noise exists is important. Oveall, the authors provide solid study to address this question. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_tBnf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_tBnf"
        ]
    },
    {
        "id": "AgV1nLy_qqp",
        "original": null,
        "number": 3,
        "cdate": 1666844108309,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666844108309,
        "tmdate": 1666844108309,
        "tddate": null,
        "forum": "yXk83o735o",
        "replyto": "yXk83o735o",
        "invitation": "ICLR.cc/2023/Conference/Paper4763/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the robustness of conformal prediction to label noise, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels.",
            "strength_and_weaknesses": "Strength: This paper studies a very important problem and tries to provide some theoretical results about the label noise learning tasks.\n\nWeakness: The paper does not provide much explanation for how the findings in this paper can be applied to existing noise-label learning methods.\nThe adopted datasets are relatively small, there are real-world noise datasets such as Clothing1M and Webvision that are widely used for noisy label learning but are not adopted in this paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well organized, and the idea of studying the robustness of conformal prediction to label noise is relatively novel.",
            "summary_of_the_review": "The idea of studying the robustness of conformal prediction to label noise is relatively novel. However, The paper does not provide much explanation for how the findings in this paper can be applied to existing noise-label learning methods. Moreover, the adopted datasets are relatively small.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_DjSE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_DjSE"
        ]
    },
    {
        "id": "SkLuOLUWHx",
        "original": null,
        "number": 4,
        "cdate": 1667423922026,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667423922026,
        "tmdate": 1667423922026,
        "tddate": null,
        "forum": "yXk83o735o",
        "replyto": "yXk83o735o",
        "invitation": "ICLR.cc/2023/Conference/Paper4763/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies whether a coverage set is still valid under label noise. They study both regression and classification cases. Empirically, they demonstrated in some experiments that constructing coverage sets based on examples with corrupted labels is still valid as long as the noise is benign (not too adversarial). Theoretically, several noisy labeled cases are studied, for example, in 2.1, a commonly used corruption model is used to show their point, and in 2.2, a shifted noise is added for the regression model, where the noise is symmetric unimodal, e.g. standard normal. They also show in 2.4 that when noise is not benign, the coverage sets built upon noisy examples may not be valid anymore. Lastly, synthetic and real experiments are shown.",
            "strength_and_weaknesses": "Strength. The writing is quite clear. The topic is interesting as corrupted labels are commonly seen in real data sets. The discussion of whether some standard techniques based on well-specified models are applicable in general to real noisy data is always appealing. \n\nWeakness. However, this paper is insufficient in several aspects:\n1. The biggest concern of mine is that this paper only presents in all positive cases, the coverage set is valid (>= 1-alpha). However, one of the most important aspects of conformal prediction is to show the tightness of the coverage. One can always return a valid coverage set by applying a trivial one. One missing part is the analysis of how much the noise inflated the coverage set.\n\n2. The second concern of mine is that the theoretical results are built upon a very simple noise mechanism. For instance, the unimodal assumption is kind of strong. Also, the assumption on Z is not very commonly seen, more explanation is appreciated.\n\n3. The empirical result on real data is too simple. More datasets in large scale should be discussed. \n\n4. One minor point, in practice, domain shift is also commonly seen. It would be interesting to see some discussion on that.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, but the contribution may not be solid enough. See comments above.",
            "summary_of_the_review": "See comments above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_snZB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4763/Reviewer_snZB"
        ]
    }
]