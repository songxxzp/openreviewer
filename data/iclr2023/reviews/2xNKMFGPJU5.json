[
    {
        "id": "bxRkwBNeAB",
        "original": null,
        "number": 1,
        "cdate": 1666192662433,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666192662433,
        "tmdate": 1666192662433,
        "tddate": null,
        "forum": "2xNKMFGPJU5",
        "replyto": "2xNKMFGPJU5",
        "invitation": "ICLR.cc/2023/Conference/Paper237/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a VAE-like framework with SEM to represent causal relationships among latent variables, and train the model by linearization.  Experiments on images and video demonstrate an ability to discover basic causal relationships. \n",
            "strength_and_weaknesses": "Strengths: \n- Novel approach to modeling, building on well-established techniques like VAEs. \n- Experimental results support the authors' claims.\n- Well-written paper.\n\nWeaknesses: \n- would have been nice to see experiments on more complex data sets \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, with a concise overview of the methods,  experimental setup, and results.  It builds on existing VAEs, but is distinct and appears novel.  The experiments are on simple data sets, but indicate an ability to discover causal relationships.  \n",
            "summary_of_the_review": "I enjoyed reading this paper, and found the exposition clear and results interesting.  I see it as a solid contribution.  \n\nQuestion: In Figure 4, why do z6 and z7 not depend on z2 and z3? \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper237/Reviewer_8bM1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper237/Reviewer_8bM1"
        ]
    },
    {
        "id": "xhgwz7NpDEE",
        "original": null,
        "number": 2,
        "cdate": 1666541288071,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541288071,
        "tmdate": 1668819201087,
        "tddate": null,
        "forum": "2xNKMFGPJU5",
        "replyto": "2xNKMFGPJU5",
        "invitation": "ICLR.cc/2023/Conference/Paper237/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is concerned with learning causal relationship between latent variables of a deep generative model, used to model high-dimensional complex data such as images. This is an important and difficult topic in causal inference. The paper starts with background about VAEs, SEMs and then proceeds to describing their method. The proposed method consists in linearizing the SEM given by a \"maximal DAG\", in which the topological ordering is fixed, and each latent variable is densely connected to each of the ones appearing before it in the ordering. For linear SEMs, the likelihood function is directly computable. Then, two experiments on image datasets are presented, in which certain variables have causal relationships (e.g., position of sprites in images), and one would like the DGMs to learn those relationships. ",
            "strength_and_weaknesses": "Strength:\n\n(1) The problem that the authors are interested in is important. \n\n(2) The maximal dag idea allows to circumvent a lot of the cumbersome machinery in causal discovery learning for acyclicity, and search for topological ordering\n\n(3) The idea of linearization is interesting (I had not seen it in causal inference before)\n\n\nWeaknesses:\n\n(1) There is a discussion of identifiability in the introduction regarding the structural equation model for the latent variables (supposedly treating them as observed). Then, there is another discussion of identifiability in the setting of decoder + SEM setting, pointing out some previous work. However, to the best of my understanding, the 2008 paper is concerned with observed random variables. Because here the zs are latent, I do not think the theory actually holds. If it does, please explain it in details in the paper, as it is not straightforward. Also, the setting with temporality is another instance, and identifiability could hold in one setting, but not the other. \n\n(2) A discussion of time complexity is missing. In particular, what is the complexity of putting together the covariance matrix detailed in equation (8), and how does it affect scaling with respect to the number of latent variables? It may look prohibitively expensive for large dimensions. \n\n(3) There is no discussion of whether the algorithm has any form of theoretical guarantees after using a lower bound, and after linearization.\n\n(4) The experimental section seems rather like a discussion of disentanglement capabilities, rather than causal discovery learning. Why was 5 latent variables used in the first experiment, and not 2? After all, there are only one cause and one effect latent variable? Similarly, if the paper goes through all the process of explaining how to fit VAEs with latent SEMs, I would expect experiments with evaluation of the graph quality (e.g., does the method discover a sparse set of interactions between latent variables, is the conditional distribution exactly parabolic). \n\n(5) I could not find quantitative evaluation of methods in the main text. If the authors claim the VAE is identifiable, then could they report MCC and other metrics imported from non-linear ICA? I would also expect the authors to report metrics over the learned causal graphs for example.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is rather clear, although I have been confused by the discussion of identifiability, and the possibly inadequate claims. \n\nMinor comments:\n\n+ The theoretical concept of maximal DAG (in Prop 1), is perfectly acceptable, but is essentially equivalent to the idea that the set of DAGs can be reparameterized by the set of permutations (GSP, IGSP, and many algorithms used this idea before). \n\n",
            "summary_of_the_review": "I have found this paper to be concerned with an important problem, and proposing an interesting method. However, I think that the paper neither has solid theoretical discussion of the setting or of the accuracy of the method, and also only has weak empirical evaluations. Therefore, I am leaning towards rejection. I am looking forward to the discussion period w/ authors, as well as reviewers. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper237/Reviewer_qqHC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper237/Reviewer_qqHC"
        ]
    },
    {
        "id": "tAJjlaTidUd",
        "original": null,
        "number": 3,
        "cdate": 1666604636121,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604636121,
        "tmdate": 1668770160462,
        "tddate": null,
        "forum": "2xNKMFGPJU5",
        "replyto": "2xNKMFGPJU5",
        "invitation": "ICLR.cc/2023/Conference/Paper237/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers the problem of causal representation learning from observational data without any form of supervision. The authors assume that the latent causal structure follows a nonlinear additive noise model (ANM) and claim that it is then identifiable. In practice, they use a VAE with a causally structured prior. One novelty in this is that they propose to approximate the KL term in the ELBO by linearizing the causal mechanisms. Their approach is demonstrated on toy image data as well as time series data from a video game.",
            "strength_and_weaknesses": "Strengths:\n- The problem of causal representation learning is interesting and potentially very impactful.\n- The data regime that the authors tackle \u2013 unsupervised observational data \u2013 is the holy grail of causal representation learning: a strong identifiability result here could be immediately practically relevant.\n- The VAE ansatz with a fully connected graph is very sensible.\n- The paper is generally well written. I appreciate the thorough introductions to the background material.\n\nWeaknesses:\n- Unfortunately, I believe that the main premise of the paper is flawed. (I hope I am wrong and the authors can correct me!) The authors correctly point out that when the causal variables are given, nonlinear ANMs are identifiable. They then claim that this implies that *causal representations with a latent nonlinear ANM are identifiable*, i.e. one can identify both the map from low-level data to causal variables and the causal graph from data. As far as I understand it, they do this without making assumptions on the decoder (like that being linear). I am afraid this claim of identifiability is not correct.\n  - One known counterexample is the special case of the trivial graph. Locatello et al [\"Challenging common assumptions in the unsupervised learning of disentangled representations\", ICML 2019] and some other papers show that in this case the variables are not identifiable.\n  - Another counterexample can be constructed from any nonlinear ANM with causal variables $z_i$ and noise variables $\\epsilon_i$. We can define a second nonlinear ANM that has a trivial graph and causal variables $z'_i = \\epsilon_i$ such that the observational data distribution remains the same. (Of course, the map to the data space will then be different between the two causal models.)\n  - If I'm missing something here, I kindly ask the authors to add an identifiability argument (for the representations, not just the causal structure once the representations are known) to the paper.\n- I don't quite understand the reasoning behind the linearizing approximation. Why not just use a nonlinear density estimator like a normalizing flow for each conditional probability density in the prior? Then we can approximate the KL quite well by sampling. Unlike the linearization, this approach should be unbiased (in some limit) and is well established.\n- I find it difficult to draw conclusions from the experiments. It would be better if the results were analyzed more quantitatively, showing that the learned representations and graphs are correct.\n- In the related work section, several recent works that use VAEs with causal structure in the latent space are missed, for instance:\n  - von Kugelgen et al, \"Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style\", NeurIPS 2021\n  - Brehmer et al, \"Weakly supervised causal representation learning\"\n  - Ahuja et al, \"Interventional Causal Representation Learning\"\n  - as a review, it may also be useful to point to Sch\u00f6lkopf et al, \"Towards Causal Representation Learning\", IEEE",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: In my humble opinion, the paper suffers from a fatal flaw \u2013 I think causal representations cannot identifiable from unlabelled observational data alone, at least not without additional assumptions. If this is correct, then unfortunately I believe that there is no reason for the proposed method to work.\n\nNovelty: There are already several papers that propose causal representation learning with a VAE with a causally structured prior. The key novelty of the paper is the claimed identifiability from unsupervised observational data. If that result is correct and well supported, then that is certainly sufficient novelty (and potentially very impactful).\n\nClarity: The paper is clearly written.\n",
            "summary_of_the_review": "If the author's claim is true, this would be a great result. Unfortunately, I do not think that the main point \u2013 that causal representations can be identified from observational data if we just assume the latent causal structure to be a nonlinear ANM \u2013 is correct. I hope the authors can point me to what I'm missing, add an identifiability argument, or provide strong empirical evidence for this claim. Otherwise, I do not think that this paper can be accepted.\n\nUPDATE after rebuttal and discussion: I am grateful to the authors for constructively and quickly engaging with my questions and criticisms and even working out a new identifiability claim. Unfortunately, I am convinced that this claim is incorrect, but even if it were true, it would not solve the main issue with the paper. I am afraid that this paper will need more substantial work before acceptance, and recommend rejection at ICLR.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper237/Reviewer_mc4c"
        ]
    },
    {
        "id": "NW0ydJk63h",
        "original": null,
        "number": 4,
        "cdate": 1666729446975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666729446975,
        "tmdate": 1666729446975,
        "tddate": null,
        "forum": "2xNKMFGPJU5",
        "replyto": "2xNKMFGPJU5",
        "invitation": "ICLR.cc/2023/Conference/Paper237/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "In traditional causal inference, one often assumes access to the structured variables and the task is to discover causal relationships between them. In recent years, there has been a growing amount of interest to tackle the question when these variables themselves are unknown and this task is termed as \"causal representation learning\". To this end, the first task is to infer these ``causal variables\" themselves. Some existing works such as the causal VAE have tackled this question but in the context of linear structural equation models for the latents. In this work, the authors extend it to general non-linear SEMs. The authors leverage additive models as they can be identified with observational data. The method proposed by the authors extends standard VAEs that leverage independence of latents as prior to a non-linear SEM driven prior. To train VAEs, it is far more efficient to have a closed form expression for the KL divergences, which one cannot with standard non-linear SEM based prior. To tackle this the authors propose to break SEM into a piecewise linear SEM. The authors validate their method on synthetic datasets.",
            "strength_and_weaknesses": "**Strengths**\nThe authors study an important and timely problem. Currently, we lack methods that can recover latents with learnable causal priors.\nThe authors have done a good job of exposition starting with basics of the VAE and building the structure of the paper. \n\n**Weaknesses**\n\nI have many concerns with the paper that I highlight in a bulleted form below. \n\n1. **Method for local linearization** The authors state that they make a piecewise linear approximation of the non-linear SEM around pivot point. The authors do not state how they select the pivot points. Since the approximation is valid in a neighborhood only, it is important to have sufficiently many pivot points. This is central to the paper and it is appalling that it is missing. Further, if there was such a method used a natural question to ask would be if the method has some identification guarantees or it is completely ad-hoc. \n\n\n\n2. **Experiments have major issues** There are two sets of experiments that authors carry out. In the first experiment the authors use a quadratic relationship between the latents. \n\n   In the first set of experiments the authors use the known quadratic prior. If the prior is not learned then the experiment is a mere sanity check and does not provide any valuable insight. In fact, if the model is already aware that the relationship is quadratic, then the SEM effectively becomes linear by treating z^2 as the parent feature. If the SEM becomes linear, then doesn't that go against the whole point of the paper. \n\n   In the second set of experiments, the authors resort to using a linear SEM again. I am not sure why authors do that. If the point of the paper is to achieve identification for non-linear SEMs why use linear SEMs. Also, if it is linear and Gaussian would you not run into non-identification issues. \n\n2.  **Crucial comparisons are missing** The authors operate in time series settings to conduct their main experiments. For these experiments, why do they not compare against the work https://proceedings.mlr.press/v177/lachapelle22a/lachapelle22a.pdf. In the work referenced the authors learn a causal graph in time. See Section 4, Figure 3 of the paper. The authors rely on time-sparsity as the regularization to achieve identification. \n\n3.  **Lack of theoretical guarantees**  As highlighted above, the paper does not provide valuable experimental insights so I wanted to comment on theoretical guarantees. The paper also does not provide theoretical guarantees either. For instance, many of the works that authors cite do provide identification guarantees. The current theorems in the paper do not provide any new insights. \n\n4. I would recommend the authors to do a significant revamp of the experiments. Also, it should be made clear how you select pivots and manage the non-linear case. These changes will improve the paper. Finally, any insights from the theory even for two variable case would be valuable. \n\n5. The authors in Section 4.6 for some reason state the identification guarantees for non-linear SEM are empirically shown. There are many works and this https://arxiv.org/pdf/1205.2536.pdf is one example where identification has been shown in multivariate case. \n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written. There are major issues with the quality of the work, please refer to the weaknesses section. While the paper is original but the lack of proper description of the method, poor experimentation makes it very weak. ",
            "summary_of_the_review": "The current work proposes a new class of VAEs that accommondate a SEM based prior. The authors propose a variational approximation for it. However, the method used for arriving at the approximation is not described, central parts of it namely pivot selection is missing. The main contribution of the paper is supposed to be non-linear additive SEMs but the main experiments are conducted with linear SEMs. Key comparisons with other works are missing as well. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper237/Reviewer_WspD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper237/Reviewer_WspD"
        ]
    }
]