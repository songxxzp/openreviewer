[
    {
        "id": "cTPKdGIQXpk",
        "original": null,
        "number": 1,
        "cdate": 1666390306792,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666390306792,
        "tmdate": 1666390306792,
        "tddate": null,
        "forum": "cbpRzMy-UZH",
        "replyto": "cbpRzMy-UZH",
        "invitation": "ICLR.cc/2023/Conference/Paper723/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of improving self-supervised learning (SSL) methods on compact neural networks. The authors investigate the relations between data augmentation strength and sizes of neural networks in SSL, and propose a new objective and data augmentation scheme to improve the performance.",
            "strength_and_weaknesses": "Strengths:\n1. The research goal is well-motivated. The wider performance gap between supervised and self-supervised methods on smaller models is long observed and dubbed the benefits of larger model sizes. However, as many applications of deep learning are still confined by memory and compute limits, it is important to improve these methods in smaller models. \n2. The proposed method in the paper is motivated due to their careful experiments and observations. The authors conduct controlled experiments supporting their intuitions about the correlations of model sizes and data augmentation strength in SSL. They define a rather novel measure pixel scale to examine the effects of different view construction on the performances. And based on these results, they conjectured the smaller performance gap between SSL and supervised for larger models is due to that the smaller models lack the ability to learn on such adversarial augmented data. \n3.  The comparison between different methods on different downstream tasks are solid and convincing. The author compared their improved SSL method with others on classification, object detection, and segmentation, and over multiple architectures including resnet, mobilenetV2 and small transformers. The performance looks good and surpasses distillation-based methods in many cases, which is clearly an important contribution.\n\nWeaknesses:\n1. There are some clarity issues due to arrangement of contents, which are mentioned below.\n2. The authors can dig into the local vs global augmentations with more principled metrics. The $\\mathrm{mean}(PS_g/PS_{\\ell})$ seems rather arbitrary and less intuitive.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Most part of the paper is written clearly. There are some small confusions, as follows:\n1. The concepts of global vs local views are mentioned early but not well defined until when defining pixel scales. I would suggest the authors give some background on $S_g, S_{\\ell}$ shortly before introducing the pilot experiments. Assuming such knowledge from the readers would limit the range of the audience.\n2. Many numbers of accuracies, gaps, and hyper-parameters are mentioned in the paragraphs in Sec 3, it is possibly better to absorb them in compact tables and refer to them rather than listing these numbers directly, which slow downs reading quite a bit. The same happens in later sections where the authors constantly mention experiment details, which require careful reading to understand. By summarizing their intuitions rather than listing specific details, the paragraphs would look better and reader-friendly.\n",
            "summary_of_the_review": "This paper studies an important but less-explored aspect of self-supervised pretraining. They investigate the problem with interesting experiments to support intuitions and provide motivations, and their final results justify their contributions.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper723/Reviewer_aQkw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper723/Reviewer_aQkw"
        ]
    },
    {
        "id": "FhKLpxTGKWh",
        "original": null,
        "number": 2,
        "cdate": 1666717034643,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666717034643,
        "tmdate": 1670136448235,
        "tddate": null,
        "forum": "cbpRzMy-UZH",
        "replyto": "cbpRzMy-UZH",
        "invitation": "ICLR.cc/2023/Conference/Paper723/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a self-supervised learning approach in the context of low-compute deep-learning models (e.g., DeiT-Tiny). The authors observe that a weaker self-supervised target is beneficial for small networks, and propose to match multiple views in more comparable spatial scales and contexts. Experimental results are provided on ImageNet and COCO.",
            "strength_and_weaknesses": "Strength:\n\n- The experimental analysis in Section 3 is extensive.\n- The improvements in Table 3 are impressive.\n\nWeaknesses:\n\n- The authors find that a weaker training target is beneficial for the self-supervised learning of low-compute networks, and demonstrate that this idea can be implemented by adjusting the relative scale of different views. Personally, I do not think this is a significant scientific contribution. The conclusion seems straightforward, while the authors conduct a series of experiments, yielding an empirical principle for hyper-parameter searching. In general, I think that, currently, this paper may be too engineering-oriented to be published on ICLR.\n- Is the proposed method compatible with knowledge distillation? For low-compute networks, the most significant concern may be how to improve the accuracy without introducing additional computational costs during inference. I think that \"without distillation\" is not an important advantage.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general, the writing of this paper is clear. However, I have some concerns on the novelty and importance of this paper. See the weaknesses above. ",
            "summary_of_the_review": "The experimental analysis is extensive. However, I have some concerns on the novelty and importance of this paper. See the weaknesses above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper723/Reviewer_MNWJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper723/Reviewer_MNWJ"
        ]
    },
    {
        "id": "eY0608j6Efr",
        "original": null,
        "number": 3,
        "cdate": 1666735080740,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735080740,
        "tmdate": 1666735080740,
        "tddate": null,
        "forum": "cbpRzMy-UZH",
        "replyto": "cbpRzMy-UZH",
        "invitation": "ICLR.cc/2023/Conference/Paper723/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors investigate the problem of self-supervised pre-training on compact networks. They study the impact of view sampling strategy and design a strategy which can boost the SSL pretraining performance without knowledge distillation. The developed method consistently shows improvement on different compact models with different SSL frameworks.",
            "strength_and_weaknesses": "Strength:\n- This paper is well-motivated. The proposed method provides a good alternative to the current distillation based methods for ssl pretraining on low-compute networks.\n- The article clearly shows the thought process and is easy to follow.\n- The proposed method shows decent improvement over baseline results.\n\nWeakness:\n- I'm not convinced by how S_l and S_g are produced. 0.14 and 0.4 for S_l are quite different and I'm wondering whether it will influence the conclusion.\n- Lack of analyzes. Most parts of the method are only driven by empirical results.",
            "clarity,_quality,_novelty_and_reproducibility": "The experiments overall are well-designed, while the technical novelty is limited.",
            "summary_of_the_review": "In conclusion, this work improves the self-supervised pretraining on compact models by tuning several hyperparameters in the view sampling strategy, which is simple but shows decent performance boost. From my perspective, I would like to see more analyzes on how these changes influence the training, instead of just showing the empirical results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper723/Reviewer_rtAP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper723/Reviewer_rtAP"
        ]
    },
    {
        "id": "PJVri-A8w-O",
        "original": null,
        "number": 4,
        "cdate": 1667432416106,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667432416106,
        "tmdate": 1668026807791,
        "tddate": null,
        "forum": "cbpRzMy-UZH",
        "replyto": "cbpRzMy-UZH",
        "invitation": "ICLR.cc/2023/Conference/Paper723/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents  a training recipe for helping smaller computer vision models achieved improved performance from Self-Supervised Learning. The paper presents the success of said recipe as a evidence that it is not the capacity of the smaller models that limits their performance but rather the extreme regularization effects of some of the data augmentation procedures used in classical SSL training.",
            "strength_and_weaknesses": "**Strengths**\n1. The paper has extensive ablations that validate the impact of various modifications to the classical training procedure for vision-based SSL\n2. We see non-trivial improvements of the method across multiple benchmarks and on top of multiple SSL methods.\n\n**Weaknesses**\n\nMy main issue with the paper is that it is not properly positioned in that I do not believe it has provided adequate evidence for its major claim.  The paper claims to show that excessive regularization from the augmentation strategy is the reason why smaller models under-perform KD  with larger models. \nWhat the authors present however, is a set of  very useful **training recipes** for getting improved performance on smaller models. The existence and efficacy of this recipe however does not support their claim that capacity is not a bottleneck for smaller models. Specifically, to fully show this they would have to :\n1. Demonstrate that the present training recipe **does not improve results for larger models**. If it does lead to improved results for the larger model, then the authors have only presented a better training procedure but not an explanation for the performance gap. Improved results on a larger model roughly implies improved KD performance which would mean that the purported gap isn't actually closed but the baselines are just shifted up.\n\nTable 5 buttresses my suspicion that the procedure presented will benefit models at larger scales, given that the delta improvement actually **increases** from Resnet18 to Resnet34 \n\n[Edit - ] The authors rightly pointed out that Table 5 is an unfair comparison to their method. They directed me to table 3, which I agree is the better table to compare to.\nI have updated my score based on the rebuttal",
            "clarity,_quality,_novelty_and_reproducibility": "*Clarity* : The paper is clearly written, figures and presentation are easy to follow\n\n*Evaluation Quality* : The paper seems quite reproducible, with a relatively thorough ablation section providing key information about reproducing their results.",
            "summary_of_the_review": "The authors do a good job of presenting a training recipe that improves performance of SSL methods on smaller models.\nHowever, they do not establish the key claim they make in the paper which is that \n> We find that, contrary to accepted knowledge, there is no intrinsic architectural bottleneck, we diagnose that the performance bottleneck is related to the model complexity vs regularization strength trade-off\n\nFor my score to increase, I would have to be convinced that the positioning of the paper is correct. \n\n[Edit - ] The authors rightly pointed out that Table 5 is an unfair comparison to their method. They directed me to table 3, which I agree is the better table to compare to.\nI have updated my score based on the rebuttal",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper723/Reviewer_h6ge"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper723/Reviewer_h6ge"
        ]
    }
]