[
    {
        "id": "zdpyh8FdOq",
        "original": null,
        "number": 1,
        "cdate": 1666513460025,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666513460025,
        "tmdate": 1666513460025,
        "tddate": null,
        "forum": "iI8zWfCyCIQ",
        "replyto": "iI8zWfCyCIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4331/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new backup operator that exploits potential graph\nstructure in the observed transitions. The purpose of this backup\noperator is to increase data efficiency by allowing information to be\nbacked-up further along state transitions and providing better value\nestimates. Their results demonstrate, rather conclusively in the\naggregated score, that backup operators can improve data-efficiency. In\nparticular, a rainbow DQN agent augmented with graph backup tends to\noutperform one-step, n-step and tree-backup targets on MinAtar and Atari\n100k.\n",
            "strength_and_weaknesses": "# Strengths\n\n-   The empirical results presented seem strongly in favour of graph\n    backup. This is especially interesting because a priori I would not\n    expect that the results would be strong on Atari due to low expected\n    crossover.\n\n-   Overall easy to follow with a clear contribution. Graph backup is\n    well-connected to the literature, as a generalization in breadth\n    from tree-backup. Experiments also leverage an already strong\n    baseline, and tweak a single component making the contribution of\n    graph backup clear as well.\n\n# Weaknesses\n\n-   Despite the paper being overall clear and easy-to-follow, there are\n    some statements in the paper that are unclear. While I understand\n    the method, its motivation and its use in Rainbow DQN, I am not able\n    to follow the reasons for the large improvement. The difference\n    between tree backup and graph backup, in particular, is\n    significantly larger than I would expect. Little intuition, or\n    empirical insight, is given to explain this effectiveness. Although\n    the crossover rate for Atari is reported, I am still left wondering\n    why the method is as performant as reported.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Detailed Comments (Clarity, Quality, Novetly, Reproducibility)\n\n-   Section 1, (data-use): \"Data is normally stored in a buffer and only\n    used several times for learning before being discarded. This means\n    these algorithms underutilise the data that is generated from the\n    simulator\"\n\n    It is hard to argue that replay-buffer based training doesn't make\n    use of data. As far as minimizing TD errors of the current policy,\n    which is the goal of most value-based methods, the data has\n    contributed all it can in some sense. While the environment may\n    possess additional structure, this is not known apriori. Of course,\n    it can also be argued that the right back-up operator can minimize\n    these errors in fewer iterations.\n\n-   Section 2, (Connection to Model-based RL): Paragraph 3 lists some\n    model-based work and separates the proposed graph backup from MCTS\n    methods that also utilise tree-structure search algorithms. It is\n    would be good to discuss further what advantages graph backup\n    provides over background planning methods, that use a learned model\n    to provide samples. This would bring all problems - including those\n    without a simulator - closer to \"cheap data\" regime.\n\n-   Section 3, (Equation 1): It is not quite correct to refer to this as\n    a loss function. This is because temporal difference methods which\n    use a stop-gradient are not taking the gradient of any objective\n    function. There are gradient TD methods, but they minimize the\n    projected mean squared bellman error.\n\n-   Section 4 (counter function) : This seems to be the key function in\n    graph backup - but it is also the most problematic. Similar to\n    count-based exploration methods, accurately learning to count\n    visitation is a challenging problem.\n\n-   Section 4.2 (counterfactual): A tabular update to the value function\n    at one transition will not directly influence the value function at\n    other states from other trajectories. With function approximation,\n    however, there will be generalization which may allow for changes in\n    nearby states in feature space.\n\n-   Section 4.2 (spare reward settings): if there are truly no rewards\n    from a lack of successful trajectories, then both graph backup and\n    one-step backups will fail to learn anything meaningful. If there\n    are some trajectories that are successful, then this information\n    will still be propagated to other states from other trajectories\n    through generalization, as long as they are in the replay buffer. It\n    may take more iterations but this is not what you seem to argue in\n    Section 4.2 Paragraph 1.\n\n-   Section 4.3 (variance): this point about averaging over many\n    trajectories is interesting and valid. It is, in my opinion however,\n    lacking some nuance because it may not be the case that your counter\n    functions is accurate.\n\n-   Section 4.2 (bias): while you discuss how graph backup can reduce\n    variance, you also mention that multi-step backups can be biased for\n    Q-learning. I do not see a discussion of whether graph backup is\n    similarly biased. This would help situate graph-backup in the space\n    of backup operators described in section 3.\n\n-   Section 4.3 (Limiting expansions in Figure 1b). Could you explain\n    why the top node in figure 1b is faded? Its parent node only expands\n    one child node, so the top most node should satisfy the breadth\n    limit, unless the breadth is not at the expansion level but at the\n    tree level.\n\n-   Section 4.5 (crossover assumption): I think it would benefit more\n    broadly to discuss (or demonstrate empirically) the importance of\n    these assumptions. The Markov assumption is obviously tied to\n    effectiveness of not only the method but RL algorithms as whole. It\n    is not, however, obvious the degree that crossovers matter (for\n    example, if 1\\% vs 10\\% of states have a crossover).",
            "summary_of_the_review": "\nI am somewhat torn on this submission. On the one hand, the contribution\nis clear and the experiments are in favor of the proposed method (graph\nbackup). It is also well connected to previous literature as an\nextension of tree-backup. This suggests that graph-backup does improve\ndata-efficiency, and the main explanation is that this improvement is\ndue to exploiting the graph structure of some fixed breadth. Some\nstatements in the manuscript are still unclear, and the overall\nmotivation for why graph backup would benefit in environments like Atari\nis still non-obvious. In light of the large improvements that graph\nbackup can make, I think this finding needs to be better explained,\neither from further intuition or results on a simple problem (but with\nfunction approximation). I am currently rating this paper at a weak\nreject. However, I am very open to increasing my score up to an accept.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_iGcV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_iGcV"
        ]
    },
    {
        "id": "eck9yb0yDb",
        "original": null,
        "number": 2,
        "cdate": 1666643769406,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643769406,
        "tmdate": 1666643769406,
        "tddate": null,
        "forum": "iI8zWfCyCIQ",
        "replyto": "iI8zWfCyCIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4331/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the value function back-up estimation problem in data efficient reinforcement learning. The key idea is to the Graph Backup, with uses creates a tree-structured update of the value function based on multiple trajectories which have overlapping states. Because of the overlapping trajectories, it has advantages such as the counterfactual credit assignment and smaller variance. The empirical results show that the proposed Graph Backup method, when integrated into DQN or Rainbow, outperforms existing backup methods in data efficient settings.  ",
            "strength_and_weaknesses": "Strengths:\n1) The problem being studied is well-motivated: data efficient RL is a practical and important topic in many real-world RL tasks.\n2) The idea of using crossover trajectories in updating the value function is interesting and novel. I am a bit surprised that this has not been studies in previous works (from what I saw in the discussion of related works).\n3) The empirical results look promising. \n4) The analysis in Section 6 is an addition. Though there might be some limitations there. \n\nWeaknesses:\n1) The settings with overlapped (crossover) trajectories are often quite restricted. In the Atari game, the ratio of crossover trajectories is relatively high because the initial state is always the same. This is a strong assumption for the majority of the RL tasks/MDPs. Therefore, this has strongly limited the applicability of the proposed Graph Backup method. \n2) It is not clear why a data graph is necessary for the proposed backup in Equation (6), and it looks like that this critical information is only included in the Appendix. Therefore it is very hard to evaluate the motivation of the proposed data graph.\n3) In computing the new backup, there is a computational overhead since it requires more computations with the iterative update and multiple crossover trajectories. It would be good to add some discussions/evaluations of the computational overhead.\n4) Section 4.2: This is more like a discussion. While it is good to have such kind of discussions, the paper would be stronger if it includes some formal results for the described advantages.\n5) The writing of the paper needs significant work. While this is normally not the reason to reject a paper, it is making it really challenging to evaluate the paper itself when clarify is a serious issue. Examples:\n- Figure 2 is far from where it is first referred to. It does not really help understand why multi-step backup adds variance to the state value estimates.\n- The visualization of Figure 1(a) is good, but it would be better if there is explanation as to what the shape of the graph indicates, e.g., is it a subgraph that shares the same target state? It will be more informative if such kind of discussion/description is provided.\n- In Figure 1, \"The blue squares represent the state-action pairs\" -> they should just be the actions from what I understand\n- Equations (7) and (8) are in the appendix, but they are referred to in the main text.\n- Some typos and grammar issues. E.g., 2nd last line of Page 2: \"as they are also utilise\" -> remove \"are\"; 1st line of Page 3: \"Zhu et al. (2020) propose the using the MDP graph\" -> \"propose to use\"; 3rd line after Equation (3): \"in a off-policy\" -> \"an\"; 4th last line of Page 3, \"Despite what it\u2019s name suggests\"->\"its\". There are a few more which I did not list but strongly suggest the authors check them out. \n- Notations: \n1st line after Equation (1): $G^{a_T}$ should be $G^{a_t}$; Equation (2): \n$\\theta'$ is not explained? You did not mention anything about a separate network. Also, the superscript of G is not explained, and is inconsistent with Equations (3) (4); \nEquation (4): why do we compare t with n in the condition  of Equation (4)? by t<n do you mean the last time step in n-step return? In this case you should use a t' as the generic step, and the condition should be $t'<t+n$. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I use 4 levels of grades: strong, good, fair, poor\n\nClarity: poor. See point (5) of weaknesses.\nQuality: fair. \nNovelty: good.\nReproducibility: good.",
            "summary_of_the_review": "Main reasons of accepting the paper are points (1)(2)(3) of strengths. \nMain reasons of rejecting the paper are points (1)(2)(5) of weaknesses. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_zkNq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_zkNq"
        ]
    },
    {
        "id": "EOlTSkJL1G3",
        "original": null,
        "number": 3,
        "cdate": 1666655414733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655414733,
        "tmdate": 1666655414733,
        "tddate": null,
        "forum": "iI8zWfCyCIQ",
        "replyto": "iI8zWfCyCIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4331/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new target value estimate approach, Graph backup, based on tree-backup. The main idea is to leverage the crossover on state space to accelerate value propagation using a graph. In graph backup, a state-transition graph is built and the target value for each value backup is estimated by looking at the Q-values a few steps ahead on the graph. The empirical results show that graph backup can be a drop-in replacement to Rainbow (a DQN variant) and attains visible performance improvements, particularly in MiniGrid ( challenging sparse reward tasks).",
            "strength_and_weaknesses": "**Strength**\n\n* The method is well-motivated and novel and can be combined with many existing off-policy algorithms.\n* The empirical results are convincing and comprehensive (experiments on MiniGrid, MiniAtar, Atari100K).\n\n**Weakness**:\n\n* Some of the pseudocode is hard to follow (See my questions).\n* The implementation of the graph is complicated.\n* The applicability to continuous state space or more complex games with a high novel state ratio is unclear.\n* Computation costs might be big to build and loop over the graph?\n\nQuestions:\n* In Line 2, Algorithm 1, what does \u201cthe largest element\u201d mean?\n* Algorithm 2 in the Appendix is confusing. Where is $\\bar{G}^{a}_s$ used in the update?\n* Since graph backup performs recursive max-operation while looking ahead from a node, would graph backup aggravate over-estimation?\n* I wonder when the novel state ratio is low (i.e., many crossover states), would a discrete graph planning achieve competitive performance with deep RL?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper presents a novel graph backup algorithm and show consistent improvements on the experiments.",
            "summary_of_the_review": "Overall, I enjoy reading the paper and the analysis section in particular. The writing can be improved to make it easier to follow (especially the pseudocode). The algorithm has some novelty, but more work needs to be done in order to apply such a method to continuous state space. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_8RrV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_8RrV"
        ]
    },
    {
        "id": "D2OHSXBkSy",
        "original": null,
        "number": 4,
        "cdate": 1666735647182,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735647182,
        "tmdate": 1666735647182,
        "tddate": null,
        "forum": "iI8zWfCyCIQ",
        "replyto": "iI8zWfCyCIQ",
        "invitation": "ICLR.cc/2023/Conference/Paper4331/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper suggests that an important way to improve sample efficiency in reinforcement learning comes from bootstrapping from accurate target estimates. Although multistep methods such as TD(lambda) offer a bias-variance trade-off, the authors argue that such algorithms fail to exploit the underlying graph structure of the MDP. They therefore propose to explicitly build a graph structure out of the experience of the agent allowing for bootstrapping targets to be composed of various trajectories starting from the current state. The authors claim that this method leads to counterfactual credit assignment and reduced variance. They evaluate their method on variety of environments, from smaller grid-like worlds to the  Atari domain.",
            "strength_and_weaknesses": "# Strengths\n- The paper tackles an important problem\n- Trying to construct graph to better perform credit assignment can be an promising path in RL\n- Evaluation on a variety of environments\n\n# Weaknesses\n- The graph construction method seems inherently not scalable\n- The results are not really statistically significant in larger environments\n- Some claims should be more nuanced or supported by evidence",
            "clarity,_quality,_novelty_and_reproducibility": "Obtaining accurate bootstrapping targets is indeed essential in RL as most methods rely on this mechanism, either through value based methods or policy-based ones. Multistep TD methods generally only consider the sampled trajectory to perform updates, as a way to achieve scalability. My understanding is that the method the authors propose tries to construct the underlying graph of the MDP directly on observations, which inherently seems not scalable to large environments. By constructing this underlying graph, the method the authors propose to bootstrap from targets belonging to different possible trajectories, weighted by their probability of taking place. I am having a hard time understanding how this motivation differs from standard dynamic programming updates, yet there is no mention of it in the paper. Arguably, the authors do not try to bootstrap from all possible futures, only from the subset which has previously been observed. However, this strategy, based on modelling each observation as a node of the graph, faces the same curse of dimensionality. This is illustrated by the empirical results on Atari, which do not provide significant gain as the confidence intervals overlap, as well by how sparse the resulting graph is (.927 novel state ratio). I would really appreciated more discussion on this in the paper, in particular if the authors considered working in a (possibly discrete) latent space. For example, the authors mention that the transition matrix is estimate through counts. Are they also using counts for Atari, or instead are they using pseudo-counts? Moreover, how long does it take for the algorithm to perform updates, compared to other methods?\n\nThe authors claim that performing the Graph Backup update will lead to reduced variance. Although this is intuitive as expected updates usually do so (Expected SARSA or Expected Policy Gradients, which are not cited), I would have liked to see some experiments to support this. \n\nThroughout the paper, there are some fundamental concepts that seem wrong. For example in section 3 \"n-step-Q exploits the chain structure of the trajectories with little computational cost, but at a cost of biased target estimation\". It is not clear what is meant by this. Q-Learning add bias by boostrapping, as do all TD methods when lambda is not set to 1.\n\n\nRegarding experiments, the results on MinAtar and MiniGrid are good, however they are done with only 5 seeds which is very little considering how fast they can be performed. Moreover, the variance of the plots is very high, which brings into question the robustness of the method.\n\nThe paper adequately cites recent work such as Expected Eligibility Traces, however they do not provide a comparison to it in the experiments. There is in fact no comparison to any other method trying to performing credit assignment more efficiently, such as [1] or [2]. Recent work on leveraging the underlying structure of the graph for better information propagation would be a particularly good baseline [3]. In general I there isn't enough relevant baselines or other recent work on unifying multi step methods [4].\n\n\n[1] Counterfactual Credit Assignment in Model-Free Reinforcement Learning.  Mesnard et al. 2020\n[2] Hindsight Credit Assignment, Harutyunyan et al. 2019\n[3] Reward propagation using graph convolutional networks, Klissarov et al. 2020\n[4] Multi-step reinforcement learning: A unifying algorithm, De Asis et al., 2018   ",
            "summary_of_the_review": "The proposed method tackles an important challenge in RL, but the claims are sometimes hardly justified. Importantly, the paper proposes an inherently non-scalable method, and it is not clear how it can avoid the curse of dimensionality. On some environments the propose method provides significant gain, but on larger ones it doesnt. More important, those results on only 5 seeds and the CIs are sometimes not even presented.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_uNoE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4331/Reviewer_uNoE"
        ]
    }
]