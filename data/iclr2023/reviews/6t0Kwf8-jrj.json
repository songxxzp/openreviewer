[
    {
        "id": "ZcmpWXCuX8",
        "original": null,
        "number": 1,
        "cdate": 1666591150789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666591150789,
        "tmdate": 1666591150789,
        "tddate": null,
        "forum": "6t0Kwf8-jrj",
        "replyto": "6t0Kwf8-jrj",
        "invitation": "ICLR.cc/2023/Conference/Paper6019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes task vectors, which are subtraction of pre-trained weights from the weights fine-tuned on downstream tasks,\nand perform add or subtract the task vector to the pre-trained weight to steer pre-trained models. With the task vector, we can remove undesirable behavior  of pre-trained model (e.g. generation of toxic sentences) by negating the task vector and adding it to the pre-trained model. Moreover, we can perform multi-task learning, by obtaining  independently task vector for each task and summing all the task vectors to the pre-trained weight. Lastly, similar to word2vec, we can perform analogy with the task vector.",
            "strength_and_weaknesses": "## Strength\n- The idea seems to be novel and interesting. If we can add multiple task vectors without any interference, it would be very impactful result in terms of multi-task learning since task interference is one of the most important problem to be tackled in multi-task learning community.\n\n-  The authors have performed various experiments (image classification and natural language processing tasks) to verify the proposed method.\n\n## Weakness\n- The paper lacks of analysis about why the proposed methods work. For example, the authors show that multiple task vectors works better than fine-tuning the pre-trained model with multi-task learning objective. But they do not explain why the proposed method show better performance in terms of optimization.\n\n- Figure 2 does not seem to be informative enough. In section 4.1, they claim that the proposed method achieves higher accuracy than the multiple fine-tuned models, but Figure 2 does not show anything about the multiple fine-tuned models. Comparing the proposed method and pre-trained model is not informative since that comparison is too trivial and not surprising.\n\n- I think authors should compare the proposed method to stronger baselines to rigorously verify effectiveness of their method. For example, for the experiments with the addition of task vector, I highly recommend to add some multi-task learning baselines such as [1, 2]. Moreover, since the authors tune $\\lambda$ with validation set, we can take a set of snapshots of the model during finetuning and select one of the snapshot that achieves the best validation performance for fair comparison.\n \n\n[1] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" Advances in Neural Information Processing Systems 33 (2020): 5824-5836.\n\n[2] Navon, Aviv, et al. \"Multi-task learning as a bargaining game.\"ICML 2022.",
            "clarity,_quality,_novelty_and_reproducibility": "Presentation is clear, and idea seems to be novel.",
            "summary_of_the_review": "I am inclined to reject since the experiments are not thorough enough to verify the effectiveness of the proposed method and the paper lacks of analysis why the proposed method work. However, if the authors can address my concerns, I will be happy to raise my score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_t87y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_t87y"
        ]
    },
    {
        "id": "U5CgNd5btMi",
        "original": null,
        "number": 2,
        "cdate": 1666617133473,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666617133473,
        "tmdate": 1666617133473,
        "tddate": null,
        "forum": "6t0Kwf8-jrj",
        "replyto": "6t0Kwf8-jrj",
        "invitation": "ICLR.cc/2023/Conference/Paper6019/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work, they propose a new paradigm based on the task vectors.\nThis task vector implies a direction of the pre-trained model. Task vectors are defined as a subtraction between a pre-trained model and fine-tuned model from a same pre-trained model with a specific task. This vectors can be used with arithmetic operations such as negation and addition.\n",
            "strength_and_weaknesses": "Strength\n\nThe definition of a task vector might be a matter of course because the pre-trained model can be considered as an updated model with meaningful directions. However, arithmetic operations with these task vectors are very interesting. For example, \nCompared to traditional multitask learning which is updated from the total loss ( Loss A + Loss B) Learning via addition (Task vector A + Task vector B) can improve the pre-trained model on the tasks, too\n\n\n\nWeaknesses\n\nFrom the definition of a task vector, arithmetic operations can be applied to the same structure model because of the dimension of the model. \nIn the image classification task, more realistic dataset such as ImageNet and the deep model such as ResNet-50 are needed for verifying the effectiveness of the task vector and the possibility for the realistic dataset.\n",
            "clarity,_quality,_novelty_and_reproducibility": "In my opinion, arithmetic operations with task vectors are novel and the results are also interesting.  Code repo is  provided in the main paper for the reproducibility. However, I can not find the code at https://github.com/redacted and i am not sure this repo breaks the Blind Submission but i can find the name \u201cSteven Tobin\u201d and other information.\n",
            "summary_of_the_review": "In this work, they propose a new paradigm named task vectors. For various vision and NLP models, adding multiple specialized task vectors results in a single model that performs well on all target tasks. Arithmetic operations are very simple but effective.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_9yQa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_9yQa"
        ]
    },
    {
        "id": "WjRpTEcjNQL",
        "original": null,
        "number": 3,
        "cdate": 1666632747897,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632747897,
        "tmdate": 1666632747897,
        "tddate": null,
        "forum": "6t0Kwf8-jrj",
        "replyto": "6t0Kwf8-jrj",
        "invitation": "ICLR.cc/2023/Conference/Paper6019/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a very simple technique to edit pre-trained model using arithmetic on network weights.  From which, the paper evaluates three ways to do athematic: i) negation: to reduce the performance for a particular dataset, ii) addition: add the performance for a dataset which results a multi-task model, and iii) analogy: to improve performance for a dataset with fewer training data by adding and subtracting two related datasets via analogies. A similar concept like king = queen \u2013 women + men, as shown in semantic embedding in a language model. The paper verifies the proposed approach in both vision and language domain. ",
            "strength_and_weaknesses": "Strength. \n+ This paper proposes a very simple and general method which I can imagine has wide applications with different use cases.\n+ The method is supported with a wide range of experiments across image classification language generation.\n+ The visualisation of cosine similarity shows why the proposed method works and has very little interference. \n\nLimitations.\n+ The provided link points to a private github repo which might breaks the anonymity requirement for ICLR.\n+ The paper mentions the scaling term lambda, which is a learnable parameter based on validation performance. However, I did not see any additional experiments or explanation in the experiment section?\n+ There are also no explanations on the gradient ascent and random vector baselines. Is it doing simply gradient descent for the fine-tuning baseline but reverse the gradient signs? But if all task weights are orthogonal to each other why gradient ascent would affect control task performance? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The method is very simple and clear and should be easy to be reproduced.",
            "summary_of_the_review": "The paper proposes a simple method to modify a pre-trained model by weight arithmetic. The idea is simple and interesting and has the potential to be a good and general solution to a lot of applications. But the paper itself is very rushed to make, with some experiment details and settings missing. And I believe the authors break the anonymity requirement with a personal GitHub link.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_gv8P"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6019/Reviewer_gv8P"
        ]
    }
]