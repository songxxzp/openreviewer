[
    {
        "id": "7M-IOplJCn",
        "original": null,
        "number": 1,
        "cdate": 1666564911938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666564911938,
        "tmdate": 1666565325901,
        "tddate": null,
        "forum": "raU07GpP0P",
        "replyto": "raU07GpP0P",
        "invitation": "ICLR.cc/2023/Conference/Paper1858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors analyze the observation that reformulating regression as classification often yields better performance by better learning high-entropy feature representations. To retain the benefits of both high entropy and ordinality of feature learning, the authors propose an ordinal entropy loss to encourage higher-entropy feature spaces while maintaining ordinal relationships for regression tasks. Extensive experiments demonstrate the empirical improvement of the method. ",
            "strength_and_weaknesses": "Strength:\n\n(1) I like the analysis of regression and its reformulating classification through the lens of mutual information and entropy. It does theoretically explain the difference between regression penalized by MSE and regression penalized by cross-entropy. Besides the empirical improvement, I do value this theoretical finding. \n\n(2) The idea is well-presented and easy to follow. The experimental demonstration is also convincing and extensive. \n\nWeakness:\n\n(1) The authors should consider conducting an ablation study on $\\lambda_d$ and $\\lambda_t$. Is the final performance sensitive to these two hyperparameters? \n\n(2) Are Eq. (8) and (9) defined on the mini-batch or the entire dataset? How does adding tightness and diversity terms increase the overall training cost? \n\n(3) Missing related work [1]. The idea of preserving ordinality by using the label distances is interesting and technically sound. A similar idea in the case of preserving ordinality in regression is also used in [1]. The authors should discuss their difference.\n\n(4) Minor point: I encourage the authors to further proofread the manuscript. I can find some of the typos as follows:\n- Section 3.1 P2 L3 \u2018The simplest mapping The feature encoding\u2019\n- Sec. 3.2 P2 L2 \u2018being a being composed\u2019\n- Introduction Paragraph 4 \u2018and regress\u2019 to \u2018and regression\u2019\n- Section 4 P3 L2 \u2018is precision enough\u2019\n\n\n\n[1] Gong et al., RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression, ICML 2022",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: \n\n[+] The presentation of the idea is clear and easy to follow. The experimental demonstrations are also convincing and solid. However, the authors should proofread the manuscript and pay more attention to the typos. \n\nQuality:\n\n[+] The theoretical and empirical demonstrations are both clear and technically sound. \n\nOriginality: \n\n[+] To the best of my knowledge, I think this work is novel and can be a contribution to the ML community. \n\nReproducibility:\n\nIt would greatly improve the contribution if the authors can release the code. ",
            "summary_of_the_review": "The authors propose an ordinal entropy loss with the original regression task loss to improve the regression performance, based on the theoretical analysis that reformulating regression as a classification problem can often improve the performance. The method is also experimentally demonstrated on multiple different regression tasks. Therefore, I recommend acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_eTrs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_eTrs"
        ]
    },
    {
        "id": "am5V-KVLh4W",
        "original": null,
        "number": 2,
        "cdate": 1666671350568,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666671350568,
        "tmdate": 1666671350568,
        "tddate": null,
        "forum": "raU07GpP0P",
        "replyto": "raU07GpP0P",
        "invitation": "ICLR.cc/2023/Conference/Paper1858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents an analysis of regression's reformulation as a classification problem and proposes an ordinal entropy loss to learn high-entropy feature representations which preserve ordinality based on the analysis.\nThe proposed method outperforms the previous regression loss, classification loss, and other comparison methods in various benchmark datasets. A thorough ablation study also properly supports the author's claim.\n\n",
            "strength_and_weaknesses": "[Strenght]\n+ the analysis and finding of regression's reformulation as a classification problem.\n+ propose an ordinal entropy loss to learn high-entropy feature representations which preserve ordinality\n+ significantly improved performance on various datasets and tasks.\n",
            "clarity,_quality,_novelty_and_reproducibility": "[Clarity & Quality & Novelty]\n+ The paper is well-written and easy to follow.\n+ The analysis and proposed loss have high contribution and novelty.\n- The source code is not currently available.\n",
            "summary_of_the_review": "Based on the strength, weaknesses, and novelty, I think this paper is novel and has high contributions. But I am not absolutely certain about my decision, I will adjust my decision after checking other reviewers' opinions.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_ejHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_ejHX"
        ]
    },
    {
        "id": "maqqYSh-aU",
        "original": null,
        "number": 3,
        "cdate": 1666691723904,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691723904,
        "tmdate": 1666691723904,
        "tddate": null,
        "forum": "raU07GpP0P",
        "replyto": "raU07GpP0P",
        "invitation": "ICLR.cc/2023/Conference/Paper1858/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors analyze multiple deep learning prediction cases with continuous targets where discretizing the target into a classification problem results in better results compared to directly employing regression loss functions. The authors argue that the root cause of the problem is that the regression objective only optimizes the conditional entropy H(Z|Y) but not the marginal entropy H(Z), whereas the cross-entropy objective optimizes both. The authors then propose a remedy to the situation by incorporating a regularizer to also maximize H(Z). The author finally demonstrates that the regularized improve the performance of the regression model.",
            "strength_and_weaknesses": "**Strengths**:\n- The authors aim to provide an explanation of the phenomenon that discretizing continuous targets into classification often time performs better than optimizing the original loss.\n- The proposed regularizer is motivated by the ordinal entropy.\n- The author shows the performance benefit of the regularizer compared to just optimizing the MSE loss.\n\n**Weakness**:\n\nEven though the authors aim to provide an explanation of why converting continuous tasks into a classification is better than running a regression, I don't think the provided analysis is sufficient enough to establish the claim. First, the analysis that the regression objective only optimizes H(Z|Y) is an interpretation/approximation under certain assumptions. The authors establish the relation, which says that the MSE is a proxy for maximizing H(Z|Y). However, the analysis cannot say for sure that the MSE also optimizes something else (like a proxy to H(Z)) as the relations are not exact. In addition, the analysis is only conducted on a linear regressor, not a very non-linear deep network. An analysis that works for linear regressor cases does not always translate to deep neural network settings. Therefore, a more thorough theoretical and empirical analysis needs to be conducted to establish the such claim.  \n\nIn the experiment, the authors did not compare the results with classification-based networks. This is important to back up the claim made by the authors that by modifying the continuous loss, the regression objective can outperform the classification baselines.",
            "clarity,_quality,_novelty_and_reproducibility": "There are some clarity issues, like an unfinished sentence in the second paragraph of section 3.1. In Lemma 1, even though the statement mentioned \\eta as consideration, it does not appear in the stated inequality. The authors are recommended to present it such that it is clear to the reader.",
            "summary_of_the_review": "The paper contains some weaknesses in the claim.\nTherefore, I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_1mqu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_1mqu"
        ]
    },
    {
        "id": "BXVvgjM2nUY",
        "original": null,
        "number": 4,
        "cdate": 1666735818420,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666735818420,
        "tmdate": 1666735818420,
        "tddate": null,
        "forum": "raU07GpP0P",
        "replyto": "raU07GpP0P",
        "invitation": "ICLR.cc/2023/Conference/Paper1858/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed to loss term for deep regression models to increase the entropy of the learned representation and with that increase the performance of the predictive model. An additional term is also proposed to decrease the entropy of the class conditioned representation.",
            "strength_and_weaknesses": "Strengths\n-Well written.\n-Technically sound.\n-Analytical and intuitively well motivated.\n-Novel contributions to deep regression, of wide application.\n-Sound proposed methodology.\n\n\nWeaknesses\n-minor typos. For instance, \"The simplest mapping The feature encoding...\"\n-complexity/efficiency discussion. Since the proposed loss term seems to be quadratic in the number of observations, it would be interesting to discuss the efficiency or the impact on the batch size.\n-in ordinal regression (also known as ordinal classification), there are works adding a regression component to the classification loss. Although quite different from the current proposal, it may be interesting to comment and put into perspective in the related work.\nhttps://arxiv.org/abs/2202.05167\nhttps://aclanthology.org/2022.coling-1.407/\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Very good.\nClear, of high quality and mostly reproducible.",
            "summary_of_the_review": "see above",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_NgkU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1858/Reviewer_NgkU"
        ]
    }
]