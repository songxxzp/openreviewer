[
    {
        "id": "IG81ZAxxB-K",
        "original": null,
        "number": 1,
        "cdate": 1666327346392,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666327346392,
        "tmdate": 1666327346392,
        "tddate": null,
        "forum": "OTbRTIY4YS",
        "replyto": "OTbRTIY4YS",
        "invitation": "ICLR.cc/2023/Conference/Paper4005/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This practical paper proposes a method for computing global explanations for the predictions made by graph neural networks (GNNs). The approach hinges on the use of a local GNN explainer invoked for computing local explanations for the GNN's predictions in the form of sub-graphs of the inputs. Given a number of local explanations, the proposed approach then combines them into a single propositional formula serving as a global summary of the GNN's behavior. The experimental results obtained on three datasets demonstrate the faithfulness and accuracy as well as high interpretability of the proposed approach.",
            "strength_and_weaknesses": "First of all, I am not an expert on GNNs and their explainability and so I am afraid I am not the right person to judge the merits of this concrete work. For the same reason, I find this paper quite hard to follow.\n\nThis is a purely practical paper that describes how one can aggregate local explanations, provided in the form of sub-graphs of the input graphs, into a joint propositional formula that summarizes the behavior of the GNN model from the global perspective.\n\nOne of the upsides of the paper is that it is rigorous in terms of citations and seems to cover the prior work in the area quite extensively. It offers a large number of nicely drawn images illustrating the general flow and also examples in the experimental results, which is also a plus as they help a reader a lot.\n\nHaving said that, I have to mention that the proposed solution does not look like a sufficient contribution to me. Granted that this paper hinges on a significant amount of engineering work but the insight is a bit shallow. The fact that the approach is described in essentially a single page seems to confirm this issue. What is worse, the authors give no rationale behind the choices they propose to make. It is just a sequence of steps (1), (2), ..., followed by (n) and a non-expert reader like me is left wondering why these steps are better than the possible alternatives (if any). The steps themselves can be seen as a bunch of technicalities applied widely in the ML community and so the novelty seems a bit on the edge here (again, for a non-expert).\n\nExperimental results are thoroughly presented and discuss the accuracy and fidelity of the computed explanations. However, these concepts do not look sufficient for measuring the \"quality\" of explanations, especially from the perspective of logic and formal correctness of those. I believe in this case a user study is necessary to confirm to what extend the explanations offered by this approach actually help a human understand the behavior of the model.",
            "clarity,_quality,_novelty_and_reproducibility": "Language-wise and quality-wise the paper is well-written. The experimental results are described up to a sufficient level of detail. The experimental setup and the tool are unavailable and are promised to be released upon paper acceptance. As mentioned above, the novelty of the paper seems limited while the proposed approach represents a recipe applying the known and existing technology with no clear rationale provided.",
            "summary_of_the_review": "In my view, the paper is under-delivering on the novelty side and represents purely practical/engineering effort. I doubt that this should meet the quality standards of a flagship AI/ML conference like ICLR. However, given the lack of my expertise, I admit that I may overlook some important merits of the work, hence my score is not entirely negative.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_8iKd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_8iKd"
        ]
    },
    {
        "id": "hmio-Jg2zPf",
        "original": null,
        "number": 2,
        "cdate": 1666512077343,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666512077343,
        "tmdate": 1666512077343,
        "tddate": null,
        "forum": "OTbRTIY4YS",
        "replyto": "OTbRTIY4YS",
        "invitation": "ICLR.cc/2023/Conference/Paper4005/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes GLGExplainer (Global Logic-based GNN Explainer), which is aimed at capturing the behavior of the model as a whole, abstracting individual noisy local explanations in favor of a single robust overview of the GNN model by generating explanations as arbitrary Boolean combinations of learned human-understandable graphical concepts. GLGExplainer uses PGExplainer to extract local explanations (as subgraphs) from graphs and learns an embedding for each local explanation that allows to cluster together functionally similar local explanations, and then projects each local explanation embedding into a set of m prototypes, which are initialized randomly from a uniform distribution and are learned along with the other parameters. The projection for each embedding is thus a vector containing a probabilistic assignment of local explanation embedding. Finally, a Logic Explainable Network learns to map a concept activation vector to a class while encouraging a sparse use of concepts that allows to reliably extract Boolean formulas emulating the network behavior. The authors validated GLGExplainer on both synthetic and real-world datasets, and claims that GLGExplainer is able to accurately summarize the behavior of the model to explain in terms of\nconcise logic formulas.",
            "strength_and_weaknesses": "- Strength\n    - The paper is well written and organized, and readers can easily follow the content.\n    - The paper organically aggregates a series of established methods, such as PGExplainer for extracting local explanation subgraphs, and concept or prototype based learning for human-understandable graphical concepts, and E-LEN for generating explanations with compositionality in form of Boolean combinations, in order to address an interesting and useful problem of capturing the behavior of the model as a whole.\n- Weaknesses\n    - There is only single baseline XGNN in the experiments. This is understandable, after all, the previous work on Global Explainer is limited. The table 2 only shows the performance of GLGExplainer, but does not show that of the baseline. \n    - Although this paper mainly aggregates a series of established methods, it would be helpful if some specific details were explained more clearly in self-contained way.\n        - For example, the Equation 2 and E-LEN\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Generally, Clarity, Quality and Novelty of the paper is good.\n- Probably there will be better reproducibility when the source codes are available.\n",
            "summary_of_the_review": "GLGExplainer is the first Global Explainer for GNNs in terms of logic formulas, and it offers a helpful and human-understandable diagnostic tool for learned blackbox GNNs.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_CpLz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_CpLz"
        ]
    },
    {
        "id": "oxYavw-DXJ",
        "original": null,
        "number": 3,
        "cdate": 1666665705291,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665705291,
        "tmdate": 1666665705291,
        "tddate": null,
        "forum": "OTbRTIY4YS",
        "replyto": "OTbRTIY4YS",
        "invitation": "ICLR.cc/2023/Conference/Paper4005/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an approach to applying the Entropy-based Logic Explained Network (E-LEN) to generate global explanations for a GNN from computed local explanations of the GNN, where both local and global explanations are expressed by sub-graphs. Experimental results on three datasets demonstrate good performance in terms of fidelity, accuracy and concept purity.",
            "strength_and_weaknesses": "Strengths:\n\n(1) The work may have a significant impact since generating global explanations for GNNs is seldom studied by now. There seems only one notable approach to generating global explanations for GNNs, namely XGNN (Yuan et al., 2020), but this approach is based on reinforcement learning and thus is unstable and time consuming in course of training.\n\n(2) Out of three experimental datasets, two are proposed by the paper for the first time. The authors promise to make them publicly available upon acceptance.\n\nWeaknesses:\n\n(1) The novelty is limited since the proposed approach seems to be only an application of E-LEN. I suggest the authors to strengthen the novelty for clarifying which enhancements are made beyond the framework of E-LEN.\n\n(2) There is no comparison with XGNN in terms of the main metrics namely fidelity, accuracy and concept purity. In particular, the proposed approach seems inferior to XGNN on Mutagenicity in terms accuracy according to the accuracy reported in (Yuan et al., 2020).\n\n(3) The presentation is generally good but can be improved by clarifying the following two questions.\n\nQ1: How to convert a prototype vector (obtained in the concept projection step) to a subgraph (learnt concept) as shown in Figure 3?\n\nQ2: Why not to use the same set of metrics provided in (Yuan et al., 2020) but introduce a new metric concept purity? Besides, concept purity has not been clearly defined in the paper; it should be defined in a more formal way, e.g., by using formulas.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the paper is fine except a lack of comparison with XGNN in terms of main metrics and some presentation problems. The clarity is fair considering that two important questions mentioned above have not been answered in the paper. The originality is marginal since the main idea is merely an application of the E-LEN framework to generate global explanations for GNNs. There is no code or data provided in the supplemental material.",
            "summary_of_the_review": "The paper addresses an important yet insufficiently-studied problem by applying the E-LEN framework. New datasets are proposed to accelerate the on-going studies. However, the proposed approach lacks comparison with the state-of-the-arts in main metrics. Besides, the paper is unclear in some important aspects and need to be improved by clarifying them.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_gQT1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4005/Reviewer_gQT1"
        ]
    }
]