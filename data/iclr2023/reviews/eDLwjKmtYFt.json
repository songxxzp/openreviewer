[
    {
        "id": "1NGo_1REYi1",
        "original": null,
        "number": 1,
        "cdate": 1666385204179,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666385204179,
        "tmdate": 1669131588038,
        "tddate": null,
        "forum": "eDLwjKmtYFt",
        "replyto": "eDLwjKmtYFt",
        "invitation": "ICLR.cc/2023/Conference/Paper6438/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Proposes a modification of the data-augmentation approach to self-supervised learning where the parameters of the transformation used in data-augmentation can be used by the network to predict the latent variables of the un-augmented input.  This is advantageous when there exists a set of input features (such as color histogram) which would potentially cause the network to converge to bad local minima (shortcut solutions), yet could still be useful for classification.  Previous approaches would use data augmentations to enforce invariance to those features, which avoids the bad local minima but at the cost of losing the ability to use those features for classification.  Equimod therefore improves performance by combining a loss that encourages invariance to data augmentations and a loss that encourages the learning of a representation that is equivariant to data augmentations.  This is hypothesized to allow the network to avoid bad local minima without becoming totally invariant to useful features that are perturbed by data augmentations.  Experimental results show improved performance of SSL methods by modifying them to include the Equimod sub-network for predicting the effect of data augmentations on the latent space.",
            "strength_and_weaknesses": "Strengths\n+ Analyzes the key ideas behind data augmentation and identifies a key shortcoming of data augmentation.\n+ Proposes an innovative, elegant, and general solution to overcome the identified problem of data augmentation.\n+ Concept is carefully explained with aid of diagrams and appropriate mathematical notation.\n+ Evaluation shows meaningful improvements achieved using method.\n+ Evaluation includes an experiment that shows that Equimod indeed learns equivariances, not just invariances.\n+ Evaluation includes ablation study of architectures.\n\nWeaknesses\n- Evaluations could be made stronger.  Training for only 90 or 100 iterations is too short; methods should be training until convergence for a fair comparison with and without Equimod. [addressed by rebuttal]\n- Intuition for Equimod loss (eq 4) not fully explained.  How does the loss differ in the case of a positive pair vs a negative pair?  And how are indices i and j sampled (e..g uniformly at random)? [addressed by rebuttal]\n- Not clear how to tune the scaling of the Equimod loss.  The paper could be improved if the choice of lambda could be explored.  How is lambda set for the experiments?  Could an ablation on lambda (setting to different values and seeing the performance) be conducted? [addressed by rebuttal]\n- Relatedly, the theory for how well Equimod might work to avoid \"shortcut solutions\" and optimally trade off is totally heuristic at the moment.  We see that Equimod works, but the paper doesn't give us a clear idea of the details of how that is possible theoretically.  Perhaps a theoretical analysis of a toy model would help shed some light on the success of the method.\n- What is the computational cost of Equimod? [addressed by rebuttal]\n- Will the code be shared? [addressed by rebuttal]",
            "clarity,_quality,_novelty_and_reproducibility": "Quality\n\nI am very impressed by the ideas and the writing of this paper.  The method is simple and well-motivated, and successfully makes use of the concept of equivariance within the context of SSL.  The evaluations address many aspects of the method (influence of architecture, which transformations are actually learned) that a potential use would find relevant.\n\nClarity\n\nThe motivation and details of the method are clearly presented.  The conceptual figures 1 and 2 also aid in grasping the method quickly.\n\n\nOriginality\n\nThe ideas of the paper are strikingly original.  As noted, some other works have made use of equivariance before, but the elegance of this approach surpasses them.",
            "summary_of_the_review": "The paper presents a very interesting method for the field of SSL, and after rebuttal, provides more clarity and insight into the proposed method.  While a better evaluation (e.g. on problems such as flowers or trees) might allow the work to have a greater impact, I  think in its present form it will be useful for SSL researchers.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_URYd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_URYd"
        ]
    },
    {
        "id": "xsx63usfXt",
        "original": null,
        "number": 2,
        "cdate": 1666663134783,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663134783,
        "tmdate": 1666663134783,
        "tddate": null,
        "forum": "eDLwjKmtYFt",
        "replyto": "eDLwjKmtYFt",
        "invitation": "ICLR.cc/2023/Conference/Paper6438/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presented a new module to improve self-supervised contrastive visual representation learning. Specifically, the proposed module focused on equivariance in the leaned latent space. Experimental analysis on two public datasets showed that when applying the proposed module to existing methods (SimCLR, BYOL and Barlow Twins) the performance was improved. The main contribution of this paper is the proposed equivariance module.",
            "strength_and_weaknesses": "**Strengths**\n\n\\+ The idea of equivariance in the feature space to help self-supervised representation learning is interesting.\n\n\\+ The proposed method was shown to be effective on linear evaluation (Table 1), by showing a performance gain when added to some prior works.\n\n\n**Weaknesses**\n\n\\- The statement \"These methods rely on maximizing the similarity between...\" in the Abstract is inaccurate. Not all self-supervised visual representation learning methods are based on such a contrastive learning scheme. Instead, most of the early methods are based on pretext tasks. Suggest revising the corresponding claims and also the title.\n\n\\- The main idea of this work is based on the assumption of the self-supervised visual learning method being contrastive-based ones. Whereas there are quite a few non-contrastive learning approaches. As a result, the main contributions of this work may be a bit limited.\n\n\\- It is unclear how the defined u_\\psi was guaranteed to represent the transformation t. From the description, t was encoded by another set of layers, which are also learnable, making the equivariance here (esp. the \"equivariant predictor\") a bit misleading.\nThe encoding of the transformations (Sec. 2.3.2) is rather confusing. In the beginning, it looks like the authors used one-hot encoding, but later some of the augmentations directly use the coordinates or real values (i.e. 1, 2, 3). The generalization for the augmentation set, as a result, is another issue. What if new augmentations were included? How would they be encoded?\n\n\\- It is unclear why the final model still needs the conventional invariance loss (i.e. the first term in the final loss function), if the proposed new equivariance-based loss is as claimed to be effective. Jointly optimizing both invariance and equivariance also seems to be confusing. How does the model actually learn in this case?\n\n\\- It is unclear why the Barlow Twins was only evaluated on the small-scale dataset CIFAR10.\n\n\\- The quality of the learned representation was only evaluated on the linear evaluation setting, which is a bit insufficient to get a clear conclusion. There are quite a few other downstream tasks that could be used as reported in the literature (e.g. fine-tuning, detection, and segmentation to name a few).\n\n\\- The proposed method was motivated by the case where \"augmentation information is useful\" as claimed by the authors, and example cases are flowers and birds (as claimed in the Introduction). But this was not validated in the experiment. There are some fine-grained datasets for such categories (e.g. the Oxford 102 flowers dataset and the Caltech-UCSD Birds-200 dataset) that should have been used to validate the claims.\n\n\\- It is unclear why not use an objective similar to Eq. (5) or (6) to constrain the equivariance model training, but instead only maximize the similarity between z_i' and \\hat{z}_i'?\n\n\\- Would an absolute value makes more sense for Eq (5)? Otherwise, what does a negative value mean, as the second case \"H-flip\" shown in Fig. 3?\n\n\\- Line below Eq. (5), \"Fig. 4\" should be \"Fig. 3\".",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity and quality of the writing are generally okay, but need improvement. The idea is relatively new to self-supervised learning, but some key components were not well clarified with sufficient details and justification. The general idea is not hard to reproduce, but it may not be easy to reproduce the exact same architecture/results due to the lack of some technical details (e.g. the encoding scheme and the detailed network architectures).",
            "summary_of_the_review": "The main idea of this work about the equivariance in latent space is interesting and may have the potential to explore and contribute to the community. The shown experiments also suggested the effectiveness of the proposed method to some extent. But some technical design was not clearly presented without sufficient evidence to back them up. There are also several unclear claims and statements. The experiments are also insufficient to validate the claims. As a result, I would suggest the authors revise their paper accordingly for a future submission.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_9hFv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_9hFv"
        ]
    },
    {
        "id": "F0O9Xc2ulq",
        "original": null,
        "number": 3,
        "cdate": 1666833917682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833917682,
        "tmdate": 1666833917682,
        "tddate": null,
        "forum": "eDLwjKmtYFt",
        "replyto": "eDLwjKmtYFt",
        "invitation": "ICLR.cc/2023/Conference/Paper6438/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an equivariance regularizer as a modification to the usual invariance-inducing self-supervised losses. This is an interesting approach to enabling equivariance as there is no need to have a special architecture as prior work. The authors are able to train multiple self-supervised losses on a standard ResNet-50 and show good linear probe improvements over invariant baselines. ",
            "strength_and_weaknesses": "Strengths:\n\n1. A novel approach to enforcing equivariance is presented. The regularizer and modification to regular architectures is simple and effective.\n\n2. The performance of the technique is shown on multiple losses and \n\nWeaknesses/Questions:\n\n1. In Eq. (4) why is j not shown on the numerator? \n\n2. Is the only difference between the equivariance projection head and equivariant predictor the fact that the augmentation parameters are fed into the predictor? Otherwise equivariance projection head and predictor could have just been merged? Is equivariance projection head used elsewhere?\n\n3. In Figure 3 and 4, what is the value of these measures for a regularly trained (invariant) model? This will tell us the benefit of adding the regularizer. \n\n4. What is the reason that equivariance is stronger for color compared to other augmentations for ImageNet? Any insight into this behavior?\n\n5. Did you try any finetuning experiments?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, clearly written, and the method is novel. I imagine that it will be easy to reproduce given the details in the paper.",
            "summary_of_the_review": "Given the overall novelty and clean/simple idea which seems to work well, I recommend acceptance.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_A3J1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6438/Reviewer_A3J1"
        ]
    }
]