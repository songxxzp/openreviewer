[
    {
        "id": "zEe-x73Tv5",
        "original": null,
        "number": 1,
        "cdate": 1666278197881,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666278197881,
        "tmdate": 1666278197881,
        "tddate": null,
        "forum": "33csNbhVnD",
        "replyto": "33csNbhVnD",
        "invitation": "ICLR.cc/2023/Conference/Paper4098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The manuscript proposes to use homotopy optimization between a stabilized version of a neural ODE and the original neural ODE to learn dynamical systems from time series data. The proposed method is tested on three synthetic problems, where it is shown to outperform a naive gradient descent training of the neural ODE.",
            "strength_and_weaknesses": "**Strengths:**\n* The idea of using a homotopy to a problem with a more regular landscape to leverage difficulties in the training of neural ODEs is very attractive. Also the demonstration that such a problem can be constructed via synchronization is valuable.\n\n**Weaknesses:**\n* Apart from being an attractive idea, homotopy continuation with synchronization has been used for parameter identification in dynamical systems before. Hence, the contribution of the manuscript reduces to the application of an existing method for parameter identification to the special case of neural ODE training.\n* Being a purely empirical work, the experiments are not very strong. In particular, the proposed method is only compared against one baseline, which is the vanilla gradient descent training of the neural ODE.",
            "clarity,_quality,_novelty_and_reproducibility": "**Quality:**\nThe presented experiments are not sufficient enough to be conclusive. One major limitation is that the proposed method is only compare against one baseline, which is vanilla gradient descent. I do appreciate that the hyper parameters are roughly indicated. However, I believe that the hyper parameters for all methods must be optimization, which is not described in the manuscript. \n\nAt parts the presentation is not optimal. Sometimes, the manuscript is not very concise, for example, Subsection 2.2 could be significantly shortened without sacrificing clarity. Occasionally, some unfinished sentences, missing space and linguistic mistakes are present. At times, the tone of writing is rather informal, which I personally do not appreciate. \n\nThe discussion of related works is present, but not sufficient. Most importantly, it omits the literature on parameter identification for dynamical systems, which is a closely related task to the problem of regression of time series data with neural ODEs. \n\n**Repreducability:** The manuscript does provide most implementation details, but does not make the code available. \n\n**Clarity:**\nOverall, the ideas presented in the manuscript are easy to follow and mostly clearly presented. \n\n**Originality:**\nThe idea of using homotopy optimization is very intriguing but not original. It has been applied in different works for parameter identification in dynamical systems. Note that neural ODEs can be viewed as an instant of a general parameter estimation problem in a dynamical system. Whereas some works from the literature on parameter identification are referenced, I believe they are not credited enough, since they are neither mentioned in the introduction nor the related works section and are therefore hard to miss. Rather, the manuscript claims in the contribution section to introduce a novel optimization algorithm, where I would the manuscript applies the existing method of homotopy optimization with synchronization to train neural ODEs. That being said, I am not aware of papers pursuing this approach in the context of training neural ODEs and believe that strong enough experiments can justify a publication.\n",
            "summary_of_the_review": "The manuscript addresses the important and timely problem of learning dynamical systems from time series data using neural ODEs. \nI appreciate the idea of using a homotopy to a sychronized system in order to leverage training of neural ODEs. \nCurrently, I have the a few main concerns regarding the manuscript in its current form:\n* The idea of homotopy optimization with a synchronized system is not new. However, prior works from the parameter identification literature are only mentioned at the very end of Subsection 2.3 and neither in the introduction nor in the related works section. In general I would describe the contribution of the paper rather the application of this approach in the context of neural ODE training as they fall in the abstract setting of parameter identification as far as I can see it (see for example Sch\u00e4fer et. al. 2019). That being said, I don\u2019t mind applying an idea from another field, but I think the contribution of the manuscript in comparison to prior works have to be sufficiently clear.\n* Being an empirical paper, I am currently not convinced that the experiments are comprehensive enough. In particular, the proposed method is only compare against one baseline, which is vanilla gradient descent.  \n* The overall presentation can be improved. \n\nOverall, I believe that the manuscript is not ready for publication in its current form. Nevertheless, I want to highlight that I find the idea of using homotopy optimization for neural ODE training intriguing and believe that with stronger experiments, this can be an interesting direction.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_4PH8"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_4PH8"
        ]
    },
    {
        "id": "PGQCoGQTh_7",
        "original": null,
        "number": 2,
        "cdate": 1666498231626,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666498231626,
        "tmdate": 1666498231626,
        "tddate": null,
        "forum": "33csNbhVnD",
        "replyto": "33csNbhVnD",
        "invitation": "ICLR.cc/2023/Conference/Paper4098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new method for training Neural ODEs by employing techniques of synchronization and homotopy optimization. In the task of learning physical systems, the authors have shown that it is possible to learn appropriately for long duration data.",
            "strength_and_weaknesses": "S1. This paper is the first example of introducing synchronization and homotopy optimization for learning Neural ODEs.\nS2. The effectiveness of the proposed method is verified for multiple physical systems.\n\nW1. The proposed method is novel, however, it is somewhat unclear why the introduction of synchronization and homotopy optimization solves the problem. First of all, can you briefly explain why the learning becomes unstable for long duration data?\u3000Then, can you explain how the proposed method solves the problem? An intuitive introduction would also be helpful.\nW2. As the authors said, It is a well-known fact that learning is unstable for long duration data. For practical use, a time-window is introduced and a shortened sequence is used as a mini-batch for training. What are the advantages of the proposed method over such techniques?\nW3. Can the proposed method be applied to machine learning tasks (regression, classification, etc.)? If so, it would be good to discuss its effectiveness.",
            "clarity,_quality,_novelty_and_reproducibility": "- The manuscript is readable.\n- The proposed model is novel.\n- The validity of the proposed method is somewhat unclear.",
            "summary_of_the_review": "The proposed method is novel, but its validity and effectiveness are somewhat unclear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_89pi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_89pi"
        ]
    },
    {
        "id": "iTMC5LAAJZ",
        "original": null,
        "number": 3,
        "cdate": 1666557912041,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557912041,
        "tmdate": 1666557912041,
        "tddate": null,
        "forum": "33csNbhVnD",
        "replyto": "33csNbhVnD",
        "invitation": "ICLR.cc/2023/Conference/Paper4098/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new training paradigm for Neural ODEs by annealing a coupling term between the learnt system and the target system. This coupling allows for a more efficient training of the Neural ODE by simplifying the loss landscape. The authors then showcase their approach on synthetic data (Lorenz, LV and double pendulum). ",
            "strength_and_weaknesses": "### Strengths\n\n- The coupling of the dynamics to improve the training is a very nice and promising idea. \n- The paper is easy to read and understand\n\n### Weaknesses\n\nIn general, I think this paper does not go deep enough either theoretically or practically, therefore missing the impact it could have.\n\n- I believe it would be very interesting to have a more in-depth investigation of the properties of the loss including the coupling. You show graphically that this coupling results in a smoother loss landscape but your work requires more theoretical foundations to motivate the approach in general.  You state that previous works have found that it synchronizes but you should do the extra step of deriving what it implies for the loss.\n\n- From an experimental point of view, it would be desirable to have some real world data as well. It's ok if the theoretical motivation is not strong but then you need a solid experimental setup. Additionally, it seems you are using only single realization of time series. How does your model transfer to the case of multiple realization of a time series (say different clinical trajectories for instance) ?\n\n- In terms of limitations, the fact that coupling is required at every time step is significant. I think it would be interesting to know when the spline interpolation fails, when the sampling rate because lower and lower.  In particular, the sampling rate used in your experiments seems quite high. \n\n- The mean square errors you get from the vanilla Neural ODEs in Figure 4 seem unrealistic and might point towards some regularization issue in the Neural ODE.  It's not clear from your experiments how you used regularization for training. I think it should be fair to have an extensive hyperparameters search to compare against your method. Also, why are the results so bad for the Lokta Voltera but seems to be acceptable for Lorenz and Double Pendulum which are arguably much more difficult systems ?\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper is well written and clear.\n\n### Quality\n\nThe paper develops very intersting ideas but lacks the depth to provide a comprehensive motivation of the approach. In particular, theoretical motivation for the coupling as well as stress tests of the approach to understand when it fails are missing.\n\n### Novelty\n\nThe paper builds upon a long line of work in Neural ODEs and optimization. In that regard, I think the title of the paper is slightly off point as it focuses on homotopy training while the core contribution relies on the coupling in my opinion.\n\n### Reproducibility\n\nThe descriptions for reproducibility are present in the paper. It's not clear why the Neural ODEs results are so bad in the lokta-voltera case.",
            "summary_of_the_review": "Interesting and promising idea for improving the training of neural odes. Nevertheless, the paper currently lacks depth in the investigation. In particular, theoretical motivation for the more behaved loss and/or more comprehensive experiments. In its current form, the paper is nor theoretically nor experimentally convincing but I fully believe there is the potential for it to be in the future.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I have no ethical concerns.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_3MeW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_3MeW"
        ]
    },
    {
        "id": "3UHIR8GfBY",
        "original": null,
        "number": 4,
        "cdate": 1666602379689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666602379689,
        "tmdate": 1666678015536,
        "tddate": null,
        "forum": "33csNbhVnD",
        "replyto": "33csNbhVnD",
        "invitation": "ICLR.cc/2023/Conference/Paper4098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a novel training strategy for neural ordinary differential equations (NODEs) by introducing a coupling term between the true dynamics and NODEs. They formulate the coupling-based training framework by using the homotopy optimization, which optimizes a homotopy between the simple coupled loss landscape and the complicated uncoupled loss landscape. It results in stable and fast training of NODEs with respect to the targeted dynamical system. The authors validate their proposed learning framework for three standard benchmarks (Lotka-Volterra, Lorenz, and double pendulum) that frequently used in NODE literatures.",
            "strength_and_weaknesses": "Strength:\n1. The motivation of the introduced coupling term for synchronizing NODEs and true dynamics is clear and principled. The authors convincely explain the effectiveness of such a coupling term when estimating the optimal parameters of dynamical systems.\n\n2. The coupling-based synchronization cannot be used directly for training NODEs, because it gives an undesired model that depends on the unknown true dynamics. To overcome this issue, the authors introduce a homotopy between the coupled and uncoupled NODE loss functions. To me, the application of the homotopy optimization for training NODEs with the synchronization is novel.\n\n3. The paper is very well-written and easy-to-follow. \n\nWeakness:\n\n1. The proposed method has four additional hyper-parameters that should be tuned carefully. Although the authors provide rough guidelines for tuning such hyper-parameters, it is not entirely clear whether the proposed method is sufficiently robust and general.\n\n2. The experimental results on Lorenz and double pendulum are not sufficiently strong. There are some extrapolation errors. Considering that they are chaotic, it is acceptable (for me). For the double pendulum experiment, it might be interesting to apply the proposed homotopy optimization to Hamiltonian NODEs.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper is well-written with sufficient details.\n\nQuality:\n\nThe proposed method is technically sound. Because the proposed method is simple (principled), I found no concern in the correctness of it.\n\nThe experimental results seem to be weak as I mentioned in Weakness section.\n\nNovely:\n\nTo me, the proposed homotopy optimization for exploiting the synchronization of dynamical systems is novel. However, my evaluation might not exhaustive because I am not on top of the current literatures.\n\nReproducibility:\n\nThe paper contains experimental details in the main text and supplementary. Code is not made publicly available.\n",
            "summary_of_the_review": "I think the paper is interesting, as I mentioned above. I would like to vote to (weak) accept for this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_nRZJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_nRZJ"
        ]
    },
    {
        "id": "9yTK21i39g",
        "original": null,
        "number": 5,
        "cdate": 1666780321570,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666780321570,
        "tmdate": 1666780321570,
        "tddate": null,
        "forum": "33csNbhVnD",
        "replyto": "33csNbhVnD",
        "invitation": "ICLR.cc/2023/Conference/Paper4098/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a technique for matching the predicted trajectory to the trained ODE using a Coupling term. It also introduces the idea of Homotopy training, where we have the interpolation between a smooth, easy function and the target function that we want to optimise. Using these two techniques allow to better fit the Neural ODE to the target trajectory, including Lotka-Volterra models, Lorenz attractor and double pendulum. Using these techniques also lead to the better extrapolation of the predicted trajectory.",
            "strength_and_weaknesses": "**Strengths**\n\nBoth techniques introduced in the paper come from the classic optimisation theory. Intuitively, the idea is similar to simulated annealing \u2014 first optimise an easier function that is already close to the ground-truth, and then increase the difficulty of the optimised task.\n\nThe paper demonstrates the approach on notoriously challenging systems, such as Lorenz attractor and double pendulum. Since Lorenz attractor is a chaotic system, it requires a very accurate reconstruction of the trajectory and is hard to extrapolate correctly. The coupling term ensures that the trained ODE can accurately fit the target trajectory. Figure 2 also provides an insight on how the optimised function changes because of the coupling term.\n\n**Weaknesses**\n\n1. My main concern is the evaluation of the Neural ODE baselines. It is surprising that Neural ODE has extremely poor fit on Lotka-Volterra model in Figure 3 and Lorenz attractor on Figure 5, but can fit the double pendulum reasonably well (Figure 6).  I trust that the coupling term and the homotopy would help the ODE training (mostly based on results in Figure 6), but it is hard to evaluate the benefits of the proposed approach, if the baseline is not well trained.\n\n\n    To my understanding, the training is done on one trajectory, so Neural ODE should be able to overfit the training data, or at least have a better fit than just a constant function. For example, in the original paper the model was able to fit the oscillating data: https://arxiv.org/abs/1907.03907. This is the example where Neural ODE is able to fit the trajectory from Lotka-Volterra model: https://gist.github.com/ChrisRackauckas/a531030dc7ea5c96179c0f5f25de9979\n\n    Figures 3 and 5 suggest that the Neural ODE baseline is either under-parameterized, or not trained long enough.\n\n\n    Can the authors provide a similar experiment to figure 3, where the Neural ODE is able to fit at least the training part of the trajectory, or explain why they think the results are not consistent with the results of the previous papers?\n\n\n2. The proposed approach uses splines to impute the input data to construct the coupling term. The splines can provide the reasonable interpolation on densely-sampled data with low amounts of noise, but might require a more careful consideration in case of sparse data or data with higher amount of noise (a common scenario in the real-world data). However, this is not a big limitation, because the splines are only used in the coupling term, which is set to zero in the end of the training.\n\n**Minor**\nSection 2.1: \u201cDifferentiating the ODESolve operation \u2026 can be done by the \u201dadjoint method\u201d, which \u2026. can yield inaccurate gradients.\u201d\n\nCan the authors give examples or citations when the adjoint gradients can be inaccurate, but the \u201dsymplectic-adjoint\u201d integrators are accurate?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written and offers enough details to reproduce the method. It is novel: it proposes an approach that allows to make a match between the trained ODE and the ground-truth trajectory. These techniques have not been mentioned in the space of Neural ODE literature, to my knowledge.\n\nThe authors tested their model on a range of systems, including chaotic, and in the variety of scenarios, for example, with added noise. As mentioned above, I have concerns about the properly trained Neural ODE baseline, but otherwise I am happy with the paper quality.\n",
            "summary_of_the_review": "The paper introduces an interesting technique to enforce the match between the ground-truth trajectory and the. Trained ODE, as well a simulated-annealing-like technique to start the optimisation from an easier function.\n\nHowever, the results of the baseline (Neural ODE) are not consistent with the results from the previous papers and external sources, suggesting that it was not trained properly or under-parameterised. I am giving the weak-reject, but willing to greatly increase the score if the authors re-train the Neural ODE model on Lotka-Volterra and Lorenz attractor and show the better fit than just predicting a constant function, or explain why their Neural ODE results might be inconsistent with the previous works.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_zjmg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4098/Reviewer_zjmg"
        ]
    }
]