[
    {
        "id": "bv3_nk1dBtw",
        "original": null,
        "number": 1,
        "cdate": 1666600763130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600763130,
        "tmdate": 1666600763130,
        "tddate": null,
        "forum": "FBMLeaXpZN",
        "replyto": "FBMLeaXpZN",
        "invitation": "ICLR.cc/2023/Conference/Paper3420/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel objective, Spectral Decomposition Representation (SPEDER), that factorizes the state-action transition kernel to obtain policy-independent spectral features. The authors show how to use the representations obtained with SPEDER to perform sample efficient online and offline RL, as well as imitation learning. Moreover, the paper provides a thorough theoretical analysis of SPEDER and empirical comparisons on multiple RL benchmarks, demonstrating the effectiveness of SPEDER.",
            "strength_and_weaknesses": "The paper seems to contain interesting results. \nSome typos need to be corrected, for example, \"sample\" => \"simple in Introduction.\nThe presentation can be modified in order for the reader who is not familier with representation learning. \nFor example, what is the difference between representatoin learning and RL with function approximation? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems to contain interesting results. \nThe paper needs to be clarified more in order to improve the readability.\nOverall, the paper is well written, and it clearly compares the proposed method with existing ones through numerical examples. ",
            "summary_of_the_review": "The paper proposes a novel objective, Spectral Decomposition Representation (SPEDER), that factorizes the state-action transition kernel to obtain policy-independent spectral features.\nOverall, the paper is well written, and it clearly compares the proposed method with existing ones through numerical examples. \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_PTm5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_PTm5"
        ]
    },
    {
        "id": "YOtFTjvAuc",
        "original": null,
        "number": 2,
        "cdate": 1666821368303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666821368303,
        "tmdate": 1666821368303,
        "tddate": null,
        "forum": "FBMLeaXpZN",
        "replyto": "FBMLeaXpZN",
        "invitation": "ICLR.cc/2023/Conference/Paper3420/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a novel method of spectral representation learning that is compatible with stochastic gradient descent, allows for sample efficient online exploration, and \n\nA key novelty is that inter-state dependence is avoided by extracting spectral features some a policy independent state transition operator.\n\nThe paper provides analyses generalization bounds and sample complexities of the method.\n",
            "strength_and_weaknesses": "### Strengths\n- There is a good introduction to the spectral decomposition view. \n- Good theoretical analyses.\n- Fair literature review, with a description of why the present work is needed.\n- Remark on model based variant of this work was interesting to see.\n- Algorithmic steps are well clearly.\n\n\n### Weaknesses\n- The paper can be a little dense to read, but this could be because of the subject matter at hand. ",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nThe paper is sufficiently clear given the density of the problems it tackles.\n\n### Quality\nThe paper and research is of good quality. Some questions:\n\n### Novelty\nThis is novel analysis, to the best of my knowledge.\n\n### Reproducibility\nHyperparameters and experimental setup seem to have been provided. However, open sourced code would have been nice to see (I couldn't find any links in the paper).",
            "summary_of_the_review": "Based on my limited understanding of the related work in this line of thinking, I believe that this is good research. The background theory is well described, along with a good literature review. The algorithmic details of the method, as well as the theoretical analysis are presented well. I did not check the correctness of these analyses however.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_jSW9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_jSW9"
        ]
    },
    {
        "id": "wHLSYOScSpj",
        "original": null,
        "number": 3,
        "cdate": 1667116955281,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667116955281,
        "tmdate": 1667116955281,
        "tddate": null,
        "forum": "FBMLeaXpZN",
        "replyto": "FBMLeaXpZN",
        "invitation": "ICLR.cc/2023/Conference/Paper3420/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies representation learning in low-rank MDPs. In low-rank MDPs, the transition model $T(s' \\mid s, a)$ can be expressed as a low-rank decomposition $T(s' \\mid s, a) = \\mu(s')^\\top \\phi(s, a)$ and similarly the reward model $R(s, a) = \\theta^\\top \\phi(s, a)$. The proposed approach SPEDER takes an SVD approach to learn the representation $\\phi$ as opposed to performing maximum-likelihood estimation used in prior work. The paper claims that this leads to a simpler optimization. Once $\\phi$ is learned, the approach closely mimics existing work in using elliptic bonus, and planning a policy using the learned model with bonus. ",
            "strength_and_weaknesses": "Strength:\n\n1. Low-rank MDPs are widely studied and the present paper proposes a new approach that claims to be more computationally-efficient\n2. Experiments and theoretical statements show promise of this approach\n\nWeakness:\n\n1. It is unclear if the proposed approach is more efficient than RepUCB. For example, RepUCB does not use partition function since the model class is assumed to be normalized. Is that not possible in practice? The objective proposed in the paper seems quite complex and hard to optimize. While experiments show that it can be optimized, certain gaps exists between theory and practice which makes it hard to understand to what extent, Algorithm 1 is tried empirically. E.g., Algorithm 1 says that policy is found by using model-based planning on $\\hat{P}$, but the paragraph on the requirement on $\\hat{P}$ suggests that planning is done somehow using the replay memory data. A pseudocode of the algorithm that was run experimentally would be helpful.  \n\n2. The paper does not present all details which makes it hard to verify certain claims. To begin with, the main contribution is the SVD approach described in Equations 6 and 7. However, the very first line uses a notation that is not defined. Specifically, what is meant by $\\langle P(s' \\mid s, a), \\phi(s, a) \\rangle_{\\rho_0}$. Normally, one uses $\\langle x, y \\rangle_A$ to denote $x^\\top A y$ which is an inner product if $A$ is a positive definite.  However, I am not sure how $\\rho_0$ is represented as a $S \\times A$ matrix. Is it diagonal with $\\rho(s, a)$ as diagonal entries? Similarly, the explanation of Equation 7 can be further expanded. \n\n3. Lastly, even though the paper aims to improve upon algorithms like RepUCB, FLAMBE, and MOFFLE; not even one of these algorithms is experimentally tried. If these algorithms are impossible to try experimentally, then the paper should discuss why this is the case. Otherwise, including their performance will help understand the effectiveness of SPEDER. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity/Quality/Question:\n\n1. Adding a pseudocode of the algorithm that was ran experimentally would improve clarity. At the least, the empirical version of Equation 10 that was tried should be included. \n\n2. Adding more details for Equation 6 and Equation 7, and defining notations would also help clarity.\n\n3. Please explain how adding Equation 11 ensures that $\\hat{P}$ is a valid probability distribution?\n\n4. How is planning performed? If $\\hat{P}$ is not used for planning, as it may not be a valid distribution, then how is the replay dataset used to perform planning? \n\nNovelty:\nThe paper is very close to prior work on low-rank MDPs such as RepUCB in how the data is collected and elliptic bonus-based planning is done. However, the proposed representation learning approach as given by Equation 10 is novel. Further, the paper includes experiments unlike prior work. \n",
            "summary_of_the_review": "I am currently leaning towards a tentative score of weak reject due to a lack of clarity about the algorithm that was empirically run, and questions regarding both empirical and conceptual differences between RepUCB and SPEDER. Including empirical performance of RepUCB, or adding clarity on why Equation 10 is easier to optimize than the MLE objective in RepUCB, would most influence the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_uUqZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3420/Reviewer_uUqZ"
        ]
    }
]