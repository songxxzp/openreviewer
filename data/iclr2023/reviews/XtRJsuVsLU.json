[
    {
        "id": "goHv1vcidJL",
        "original": null,
        "number": 1,
        "cdate": 1666640806322,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640806322,
        "tmdate": 1666641646266,
        "tddate": null,
        "forum": "XtRJsuVsLU",
        "replyto": "XtRJsuVsLU",
        "invitation": "ICLR.cc/2023/Conference/Paper2370/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes an evaluation framework for approximate nearest nearest neighbor search algorithms in a dynamic setting (index set changes over time). It measures various algorithms in this space and concludes that is possible to tune the algorithms so that they are far better than bruteforce search for high dimensional data.",
            "strength_and_weaknesses": "Strengths:\n1. Authors list various parameters of importance to dynamic setting.\n2. Authors identify the lack of comparison for dynamic indices as a major missing piece on ANN literature. The research is timely.\n3. Authors identify that its possible to get substantial speed up over \n\nWeaknesses:\n1. The authors miss an opportunity to present clear and comprehensive experiments. A few examples of points important for dynamic indices but missed here:\n- How do batch-build vs streaming-build indices compare in quality? \n- No measurement of indexing effort vs search quality. There is a direct trade off between index construction complexity and \"search complexity vs recall\", the latter being the property studied in all plots. This is especially so for graph indices. A study of dynamic indices for graph indices could measure this trade off?\n2. Delete operation is ignored. This is important in practice, as a lack of delete operation forces a (expensive) rebuil of the indexafter substantial modification. While most indices measured in this paper do not support delete operation, there is recent work on deletes that this paper does not cite.  These papers also include algorithms at least as good as the ones studied in the paper for inserts.\n  - Proximity Graph Maintenance for Fast Online Nearest Neighbor Search [Zhaozhuo Xu, Weijie Zhao, Shulong Tan, Zhixin Zhou, Ping Li]\n  - FreshDiskANN: A Fast and Accurate Graph-Based ANN Index for Streaming Similarity Search [Aditi Singh, Suhas Jayaram Subramanya, Ravishankar Krishnaswamy, Harsha Vardhan Simhadri]\n  - Parallel Nearest Neighbors in Low Dimensions with Batch Updates [Magdalen Dobson, Guy Blelloch]\n3. Do these indices generalize as dataset changes or increases in size? How do we quantify this? I am especially concerned about the quantization and clustering schemes and how they are bootstrapped and maintained over time. These details are missing in the paper.\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Suggestions for improving paper:\n\n**Clarity**\n1. The settings \"online feature learning\" and \"online data collection\" need to be explained more clearly. What does it mean for the ANN index? Presumably \"online data collection\" just means that new points are inserted into the index? Does online feature learning mean that the underlying model is being trained and this \n2. More clarity needed on temporally dependent events -- what are the sources of these data and how are the points correlated?\n3. IVFPQ and SCANN: Is the quantization changed over time? How is the initial quantization computed when zero points are provided? Does the initial quantization generalize over time? Same question for clustering in IVF. \n\n\n**Quality**\n1. The main parameter varied in Fig 4 is inserts:search ratio. Isn't it clear that dynamic ANN indices can outperform bruteforce? I feel there are other experiments that merit more attention than this plot. See the remaining points:\n2. What is the total pool size of points from which the experiments sample data? You could use the 1B scale datasets released at big-ann-benchmarks.com and explore various permutations of insertions and resultant effects.\n3. Lack of parallelism. Support for parallelism is an important feature of algorithms and should be measured. In practice, a single dynamic index would be deployed on a multicore machine and could be supporting changes and searches at the same time.\n4. The conclusion could be more specific and focused on the learnings from the experiments as opposed to general comments on the ANN space.\n5. It unclear that FAISS IVFPQ as used in this paper has been tuned well. There are recent discussions on how FAISS IVFPQ can compete with SCANN: See https://medium.com/@kumon/similarity-search-scann-and-4-bit-pq-ab98766b32bd and https://github.com/facebookresearch/faiss/wiki/Fast-accumulation-of-PQ-and-AQ-codes-(FastScan)\n6. Please consider taking the best 2 or 3 algos for each dataset in ann-benchmarks.com to ground the choice of algorithms you benchmark.\n\n**Novelty**\n1. The authors don't propose novel algorithms but describe an evaluation framework, which is OK. Given the lack of evaluation for dynamic ANN, this work is a good first step.\n\n**Reproducibility**\n1. Consider organizing the paper and code around (A) APIs a new algo must support to  be measured in your framework, and (b) use cases and datasets that that the eval framework considers important. These eval code should describe and support these two orthogonally.\n2. What are the datasets used in this paper and their source? ",
            "summary_of_the_review": "While a greatly welcome first step in the direction of benchmarking dynamic ANNS indices, the work falls short on several fronts -- metrics, evaluation set up, choice of operations, quality of tuning, description and coverage of use cases. Therefore, it does not rise to the quality necessary to be a useful reference in this area.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_brtU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_brtU"
        ]
    },
    {
        "id": "4wqcIbYmJC2",
        "original": null,
        "number": 2,
        "cdate": 1666678316792,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666678316792,
        "tmdate": 1666678316792,
        "tddate": null,
        "forum": "XtRJsuVsLU",
        "replyto": "XtRJsuVsLU",
        "invitation": "ICLR.cc/2023/Conference/Paper2370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper identifies the key measures of high-dimensional search problems and utilizes them to benchmark the ANN methods for dynamic search. The paper benchmarks state-of-the-art ANN algorithms - HNSW, SCANN, k-d Tree, IVFPQ, and Annoy on speedup against the brute-force near neighbor. The online data collection (adding items online) and online feature learning (updating item features online) aspects are covered for the dynamic search. ",
            "strength_and_weaknesses": "Strengths:\n1) The paper addressed the much-needed benchmark space for the ANN algorithms. \n2) The benchmarks are provided for the additional metrics for dynamic search, i.e., Event type, event processing time, event frequency, event batching, Search frequency, and search batching.\n\nComments:\n1) I might have missed it, but the details about the datasets need to be clarified. \n2) More ANN algorithms: The authors have done a great job including many well-known ANN algorithms in the benchmark. They have included Quantization based, Tree-based, and Graph-based algorithms. Including pure partitioning-based methods like IVFFLAT, LSH table, or learned indices like Neural LSH (Dong et al., 2019) and BLISS (Gupta et al., 2022) will be great.\n3) Are there any similar benchmarks for dynamic search problems?\n4) It will be great to have a table like Table 1 with different methods listed with their performance against the given measures. The performances can be depicted by ranks or markers like ++ , + , - - etc. \n\n*Learning sublinear-time indexing for nearest neighbor search. arXiv preprint, arXiv:1901.08544, 2019. URL http://arxiv.org/ abs/1901.08544.\n\n*A billion scale index using iterative re-partitioning. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201922, pp. 486495, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written well and clear. The efforts made in this direction of benchmarking is needed. Code is provided.",
            "summary_of_the_review": "The paper is written well and addresses the need for benchmarking on dynamic search. Some additional experiments and clarity will make it an excellent paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_vEWF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_vEWF"
        ]
    },
    {
        "id": "ObI6xv1BB1v",
        "original": null,
        "number": 3,
        "cdate": 1666696552156,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696552156,
        "tmdate": 1666696552156,
        "tddate": null,
        "forum": "XtRJsuVsLU",
        "replyto": "XtRJsuVsLU",
        "invitation": "ICLR.cc/2023/Conference/Paper2370/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper looks at the Approximate Nearest Neighbor (ANN) search problem for high-dimensional dynamic data. The authors propose a new benchmarking framework to fill the gap in selecting the satisfied ANN method for dynamic ANN search. They focus on two requirements: online data collection and online feature learning, and identify six additional measures for the dynamic search problem. Finally, they evaluate five ANN methods and show that the proposed benchmarking framework can determine which ANN method is most satisfactory for a given dynamic search problem.  ",
            "strength_and_weaknesses": "Strengths:\n\n1. The motivation is clear. Even though there exist a few benchmarking frameworks for ANN search, they are designed for static search problem that assumes the dataset is fixed. It seems that this work is the first to consider the dynamic search problem. \n\n2. The authors identify many measures for the dynamic search problem and use experiments to confirm that these measures are valid for the benchmarking framework they proposed.\n\n3. The paper is well-written and easy to read.\n\nWeaknesses:\n\n1. My primary concern is that the technical depth seems somehow limited. I appreciate the six new measures they proposed and the experiments for the five ANN methods they conducted, but most of the measures are straightforward, and all five ANN methods are open-sourced. The framework will be more promising if they can build a cost/analytical model to analyze the relationships of different measures.\n\n2. In the 2nd paragraph of Section 2, they provide many examples of online data collection and online feature collection. It would be more convincing if they could add references for support. \n\n3. In the experiments, what are the datasets they used for online data collection and online feature learning? They provide the dataset statistics, but I cannot find what datasets they used.\n\n4. The experiments only report the curves of speedup and recall. Can they also report the results for other measures, such as the event processing time, initialization time, and memory footprint?\n\n5. For the last experiment (Figure 5), can you explain why the five ANN methods fail to exploit mutual information between consecutive search batches?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this is a well-written paper with a good framework that looks well-motivated. However, considering this paper's strengths and weaknesses, the quality and novelty do not seem very strong to me.\n\nFor the reproducibility concern, as they declare they will make the code available, the reproducibility should be somewhat assured. ",
            "summary_of_the_review": "In summary, I think this paper is well-written and has a clear motivation, but currently, I feel the technical depth is limited, and I have some concerns about the experiments. Thus, I initially provided a borderline reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_GRS7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_GRS7"
        ]
    },
    {
        "id": "vA5Y3YbdedF",
        "original": null,
        "number": 4,
        "cdate": 1666761705967,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666761705967,
        "tmdate": 1666761705967,
        "tddate": null,
        "forum": "XtRJsuVsLU",
        "replyto": "XtRJsuVsLU",
        "invitation": "ICLR.cc/2023/Conference/Paper2370/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper is concerned with the problem of evaluating approximate nearest neighbor (ANN) search strategies for use in high-dimensional and highly-dynamic settings, such as in deep learning, where features must be learned from sample neighbors in embedding spaces.  The authors propose and discuss a characterization of dynamic search problems in terms of 12 measures such as search accuracy, runtime, initialization time, etc.  Based on their characterization, a framework is proposed for specifying, evaluating, and performance-tuning ANN methods on dynamic search problems.  They then demonstrate their framework by providing an experimental evaluation of 5 representative ANN methods on 2 dynamic search benchmark data sets (1 data collection benchmark, and 1 online feature learning benchmark).\n",
            "strength_and_weaknesses": "Pros:\n\n1) A thorough and compelling motivation is given for the work, in light of the severe computational and resource implications of dynamic search, particularly as regards the context of deep learning. The categorization provided for dynamic search is well-argued, and a good case is made for the need of a benchmarking framework for parameter tuning of ANN methods in dynamic settings.\n\n2) The experimental evaluation does show that certain classes of ANN methods (in particular, graph-based methods) have greater prospects for handling high-dimensional and highly-dynamic data search problems, such as those that typically arrive in feature learning in ML/DL contexts.\n\n3) The work seems ready for deployment in ML/DL development. The authors intend to make their framework openly accessible to practitioners wishing to automate the runtime automation of parameter-tuning for ANN search.\n\nCons:\n\n1) Although the authors are careful to show a connection to ML/DL practice, the proposed framework is entirely generic as to the application - there are no unique issues that arise in learning contexts that require special treatment within the framework. As such, the topic is far from central to the interests of the ICLR community, and might be better suited to a different publication venue.\n\n2) The authors do not demonstrate their framework on dynamic data sets of the same scale as is commonly encountered in learning.  In their experimentation, they consider only two datasets: one having 100,000 samples in 128 dimensions, with 100,000 added events, and the other having 5000 samples in 96 dimensions with 100,000 update events.\n\n3) From the experiments, it is not clear whether `successful' parameter tuning is even capable of reducing the computational costs of ANN search methods enough to allow them to be used directly in deep learning. Practitioners and researchers often use fast heuristic methods such as minibatch sampling followed by brute-force search. Although the proposed framework can accommodate such heuristics, it is not clear that any of the more sophisticated ANN search methods would meet the time budgets available in practice, even when properly optimized.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The organization and clarity of the paper is excellent. The authors build their case carefully, showing why and how dynamic search cannot be accommodated by existing frameworks designed around static ANN search. However, the proposed framework can be regarded as a relatively straightforward extension of static benchmarking platforms. Although the paper does not give the full details of the implementation, the authors have stated their intention to make it freely accessible to practitioners.\n\n\n",
            "summary_of_the_review": "Although the work targets a practical need in ML/DL, it is doubtful whether existing non-trivial ANN methods can meet practical time budgets even after parameter tuning. The work is not specific to learning, and many might regard this topic as only incidental to the interests of the ICLR community.\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_nVSk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2370/Reviewer_nVSk"
        ]
    }
]