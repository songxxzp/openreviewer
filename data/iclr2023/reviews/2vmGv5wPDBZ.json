[
    {
        "id": "Vy_OIWUzdY",
        "original": null,
        "number": 1,
        "cdate": 1666603528841,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666603528841,
        "tmdate": 1670617048920,
        "tddate": null,
        "forum": "2vmGv5wPDBZ",
        "replyto": "2vmGv5wPDBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1743/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a volumetric motion estimation framework specifically in the context of fluids. The framework can be trained end-to-end from only single-view image sequences. Experiments show that the proposed framework can successfully generalize across different inputs and outperforms single-scene optimization-based approaches.",
            "strength_and_weaknesses": "### Strength\n* The paper provides a reasonable explanation for the depth ambiguity problem in single-view inputs. \n* Motion estimation for fluids is a challenging task, especially from single views. In general, the proposed method is technically sound to me. \n### Weakness\n* The framework figures are complicated and hard to understand. Too many notations, and some of them are unexplained. It would be better to simplify the diagrams and add some legends.\n*  It would be better to give more background and intuitive explanations for some technical designs. For example, why use RaLSGAN objective to train the discriminator, and why would CFL condition help constrain the magnitude?\n* It would be better to visualize the learned prototype volume. \n* The ablation study is confusing. In Figure 3, there are two \"fix pos, multi, \u00acD\"; one of them should probably be \"var pos, multi, \u00acD.\" It is unclear from Figure 3 that adding the adversarial loss can sharpen the result. It would be better to provide quantitative results for all ablation studies. \n* The paper mentioned that using a multi-view capture setup does not help the density ambiguity issue and would result in a suboptimally averaged solution. How about using a multi-view+the proposed discriminator? Showing such a comparison could strengthen the evaluation. \n* Some minor typos. Section 3.1 - \"leadsto\",  Section 4.1 - \"spacial\".",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\nAs mentioned in the weakness part, the diagrams are hard to understand, and some technical designs lack background and intuition. The paper can be understood only by audiences familiar with all related work and techniques.\n\n### Novelty\nThe contributions are significant and somewhat new. Aspects of the contributions exist in prior work.\n\n### Reproducibility\nThe paper provides some implementation details. However, the training flow seems complicated as it requires multiple stages. To reproduce the experiment, more training details including the range of the hyperparameters tuned, and the final selected hyperparameter values are required.",
            "summary_of_the_review": "In general, the proposed method is technically sound to me. However, the experiments are somewhat limited. Furthermore, the clarity of the paper needs to be improved. Please see the above sections for more details.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_TTZh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_TTZh"
        ]
    },
    {
        "id": "cKaGHtWjBDa",
        "original": null,
        "number": 2,
        "cdate": 1666630328032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666630328032,
        "tmdate": 1670617176774,
        "tddate": null,
        "forum": "2vmGv5wPDBZ",
        "replyto": "2vmGv5wPDBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1743/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a single-view video-based volumetric reconstruction pipeline for smoke. The method assumes known background (black color) and foreground (smoke with known color). It takes the video frames as input and estimates both volumetric density and velocity. The pipeline is trained end-to-end by comparing the captured incoming frame with the rendered image that is based on the estimated volumetric density and velocity. Additional regularizations are added to tackle the inherent ambiguity in single view volumetric estimation. ",
            "strength_and_weaknesses": "Strength\n\n+The usage of the transport function A() based on MacCormack advection  to render new frames adds physical inductive bias into the pipeline, so that the estimated states (density) can be propagated into the new timestamp in a physically correct way. It also helps to reduce the number of learnable parameters for the transition itself (eg. versus using GRU) and make the model more generalizable.\n\n+ The adversarial learning helps to reduce the estimation ambiguity. \n\n+ The pipeline trained end-to-end without GT volumetric density and velocity as supervisions, which is useful since GT volumetric data is hard to capture in real world.\n\nWeakness\n\n-Key assumption not explained: The rendering equation does not include the background color. If the background color changes, how to make sure this rendering equation is correct? If there is an additional assumption about the colors of the background and smoke, it should be claimed upfront since it is a very strong assumption\n\n-The writing needs improvement: \nWhat does it mean by c_z is the center along z (after Eq.6)? Without defining the start and end points along z for each ray, it is hard to understand it. \nEq.6: what is p? Is it a 3D point location? If so, how can we define L_z over one single 3D point? *\nEq.6: why multiplying 2 inside the second term eq.6 what is the intuition of the regularization?\nWhat is R^{-1} function in Eq.8\n\n-To tackle the ambiguities in the solution, the method assumes  the initial state is known. What if the initial state is not accurate? How can the subsequent estimation correct for the inaccurate initial state estimation? \n\n-Ablation study is not enough: ablations for the additional regularizations that tackle ambiguities are missing.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Quality and clarity:\nThe clarity needs to improve (see the weakness above for some confusions)\nAdditional ablation studies on the regularizations are needed.\n\nOriginality :\nAlthough the monocular 3D and scene flow estimation is not a new topic, the inclusion of the MacCormack Advection to model the inter-frame smoke transport is incrementally novel  ",
            "summary_of_the_review": "The paper demonstrates a monocular video based method for smoke volumetric density and velocity reconstruction. Although the method has shown to be working for smoke with strict experiment setup (known foreground and background colors), I\u2019m not sure if it can be extended to handle more general case (eg. smoke with real-world background, or even non smoke foreground). In addition, due to the confusions in the writing, the motivation for the design choice is not clear to me. Last but not least, the experiment is lacking in ablations. As a result, I would recommend weak rejection of the submission. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_8YBe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_8YBe"
        ]
    },
    {
        "id": "W8_lZLu8l4",
        "original": null,
        "number": 3,
        "cdate": 1666839105732,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666839105732,
        "tmdate": 1666839172418,
        "tddate": null,
        "forum": "2vmGv5wPDBZ",
        "replyto": "2vmGv5wPDBZ",
        "invitation": "ICLR.cc/2023/Conference/Paper1743/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new method to estimate the 3D flow and volumetric densities of a moving fluid from a monocular video. The proposed framework consists of two deep networks, one for predicting the volumetric densities from a single image, and one for predicting flow given the input images and predicted densities from two consecutive timestamps. Since ground truth 3D flow and densities are difficult to obtain, this paper adopts an unsupervised learning approach, which projects the inferred densities to the image space with a differentiable image formation operator, and then uses image-based losses to train the networks. To reduce the depth ambiguity, it further uses an adversarial loss to regularize the solution. Experiment results on benchmark datasets demonstrate the effectiveness and efficiency of the proposed method.",
            "strength_and_weaknesses": "Strength:\n\n+ A novelty of this paper is that it trains deep networks to predict 3D flow and volumetric densities in an end-to-end fashion. Compared to optimization-based methods, the proposed method is much faster. \n+ Technically, the proposed method appears to be sound and effective. It adopts an adversarial loss, similar to the one used in [Franz et al., 2021], to regularize the estimated densities across different views. To some extent, this helps avoid requiring videos from multiple views as input, as prior work [Qiu et al., 2021] does. It also proposes to predict a prototype volume, which is shown to be beneficial in stabilizing the network training over time. \n\nWeaknesses:\n\n- The quality of the estimated flow is not very good. In Table 1, the errors in estimated flow u is much larger than the other methods on the synthetic dataset. Looking at the videos in the supp material, there are also noticeable artifacts in the flow motion. Perhaps, the design of network and losses makes the proposed method focus more on synthesizing the densities \\rho than predicting the flow u?\n- What are the limitations of this work? This is not discussed in the paper.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing and clarity of the paper can be improved. \n\n- Notation: this paper uses a large amount of notation. Some are not properly defined. For example, in Figure 1, the loss L_u is not defined in the paper. Also, the notation \\nabla\\times is not explained.\n- Consistency: For example, in Eq.(3), the input to network G_u includes \\rho^t, I^{t+1}, but it is later shown in Eq.(8) that the input to G_u consists of five terms. This is quite confusing.\n- Grammar errors and typos:\npage 1: \"is a very appealing\"\npage 3: \"leadsto\"\n- I also suggest that some long sentences to be broken into shorter sentences to improve the readability.\n\nReproducibility: the authors indicates that code and data will be made public upon paper acceptance. ",
            "summary_of_the_review": "Overall, this work presents an effective and efficient solution to the challenging problem of estimating 3D flow and volumetric densities from a monocular video. Leveraging unsupervised learning and adversarial losses appears to be a promising direction. However, the quality of the estimated 3D flow is not as good. To improve the results, more work needs to be done to adjust the network and losses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_UynW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1743/Reviewer_UynW"
        ]
    }
]