[
    {
        "id": "eTWQ1tIzit",
        "original": null,
        "number": 1,
        "cdate": 1666525007528,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525007528,
        "tmdate": 1666525007528,
        "tddate": null,
        "forum": "8foynpwwRb",
        "replyto": "8foynpwwRb",
        "invitation": "ICLR.cc/2023/Conference/Paper6416/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper targets at improving the efficiency of sharpness aware optimizer (SAM) by assigning a probability where a normal SGD optimizer is used. The decision is based on Bernoulli trial. The authors further propose a general framework to make the proposed optimizer schedule usable for different architectures. The idea is intuitive and clearly expressed. The proposed method is simple. This would not be an issue if it is extremely effective. However, the paper has missed many efficient SAM baselines such as GSAM [1] and SAF [2]. Besides, the experiments on large dataset ImageNet is not solid. The selected baseline models are different from the experiments on CIFAR-10 and the are not representative enough.\n\n\n[1] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training.\n\n[2] Du, Jiawei, et al. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).",
            "strength_and_weaknesses": "Strength:\n1. The motivation of the proposed method is clear. SAM is indeed suffering from low computational efficiency.\n2. The proposed method is clearly presented. The reproducibility is not an issue.\n\nWeakness:\n1. The proposed algorithm is intuitive and not novel enough. It is adding a probability to replace some of the SAM steps with normal SGD optimization steps. This would not be an issue if the proposed method is verified to be super-effective. However,\n2. The experiments do not show the superiority of the proposed method. The paper has missed some relevant baselines as mentioned in the summary section. Also, I was confused that why the selected architectures are different on CIFAR and ImageNet dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written. The proposed method is clearly presented. However, the proposed method is not novel enough and the experiments are not solid enough.",
            "summary_of_the_review": "In summary, the proposed method is not well supported by the presented experiments mainly due to two points:\n\n1. The missing baselines on efficient SAM algorithms and \n2. The missing architectures on ImageNet dataset.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_5xxY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_5xxY"
        ]
    },
    {
        "id": "QHItPdGRQU5",
        "original": null,
        "number": 2,
        "cdate": 1666664952164,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664952164,
        "tmdate": 1668998994203,
        "tddate": null,
        "forum": "8foynpwwRb",
        "replyto": "8foynpwwRb",
        "invitation": "ICLR.cc/2023/Conference/Paper6416/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Sharpness-aware minimisation (SAM) has been shown powerful to train high-performance deep learning models, but it also incurs at least double computational cost due to the extra back-propagation for the sharpness estimation. To improve the efficiency of the vanilla SAM, this paper proposes an training scheme, dubbed Randomized Sharpness-Aware Training (RST). RST performs a Bernoulli trial at each iteration to choose randomly from base algorithms (SGD) and SAM. The probabilities of Bernoulli trials at each time are  determined by a predefined scheduling function $p(t)$. The average extra time is reduced from 1 to $\\sum p(t) / T$.",
            "strength_and_weaknesses": "Strength:\n- The proposed RST can reduce the extra computational cost of the vanilla SAM from 1 to $\\sum p(t) / T$ in average, and preserves similarly good performances.\n- The paper is easy to follow.\n\nWeakness:\n- The novelty of this paper is my only concern. The idea of applying SAM to a subset of parameters or iterations has been explored in several papers [Mi et al. 2022, Liu et al. 2022, Du et al. 2022]. For example, Liu et al. 2022 propose to only periodically calculate the inner gradient ascent across the training iterations; Liu et al. 2022 and Du et al. 2022 propose to select a subset of parameters to calculate the inner gradients in each iterations. Over expectations, randomly selecting a subset of parameters in iterations is same as randomly select interactions. And the later can be considered as a special case of the former --- alternatively masking out all parameters. \n\n\nMi et al. 2022, Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach\n\nDu et al. 2022, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks\n\nLiu et al. 2022, Towards Efficient and Scalable Sharpness-Aware Minimization",
            "clarity,_quality,_novelty_and_reproducibility": "Please see the weakness part. ",
            "summary_of_the_review": "The paper is easy to follow. The empirical experiments are enough to support the arguments. However, the novelty of this paper is the concern. The idea of selecting a subset of parameters or iterations for implementing SAM has been explored in several recent works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_hEbB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_hEbB"
        ]
    },
    {
        "id": "riq8IbU7SzK",
        "original": null,
        "number": 3,
        "cdate": 1666835445207,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666835445207,
        "tmdate": 1666835445207,
        "tddate": null,
        "forum": "8foynpwwRb",
        "replyto": "8foynpwwRb",
        "invitation": "ICLR.cc/2023/Conference/Paper6416/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Recently, sharpness-aware training such as SAM has drawn large attention, because of its provably guarantee its significant performance improvement. However, SAM requires a huge additional computation cost, and it is not easy to adopt SAM on a large-scale model or a real-time analysis system. This paper provides an efficient computation for SAM, RST, and G-RST. They adopt randomized sharpness-aware training. The idea is simple, and it works well.",
            "strength_and_weaknesses": "Strength points\n1. This paper aims to improve the efficiency of SAM, the important problems.\n2. This paper raises interesting ideas such as randomized sharpness and its general extension G-RST that adjust regularization degree freely for any scheduling function.\n\nWeakness points\n1. The concept of this paper is similar to the other papers [1]. This paper should cite and discuss the difference between this paper and another paper.\n2. This paper is necessary to include diverse related works and efficient computation of SAM. For example, there are three research works, [2], [3], [4]. This paper only cites and discusses the [4]. However, there is no experimental comparison with [4]. I suggest that this paper should discuss and compare the performance with [2], [3], [4].\n\n[1] Zhao, Yang, Hao Zhang, and Xiuyuan Hu. \"SS-SAM: Stochastic Scheduled Sharpness-Aware Minimization for Efficiently Training Deep Neural Networks.\" arXiv preprint arXiv:2203.09962 (2022).\n[2] Du, Jiawei, et al. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).\n[3] Liu, Yong, et al. \"Towards efficient and scalable sharpness-aware minimization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[4] Du, Jiawei, et al. \"Efficient sharpness-aware minimization for improved training of neural networks.\" arXiv preprint arXiv:2110.03141 (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written, and it is easy to follow the contents.\n\nThere is an implementation code, and it is easy to reproduce.",
            "summary_of_the_review": "This paper raises an interesting concept, but similar ideas were already suggested in other venues. \n\nDiscussion and additional experimental comparisons are necessary.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_eiyQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_eiyQ"
        ]
    },
    {
        "id": "Ua-w5CmuInh",
        "original": null,
        "number": 4,
        "cdate": 1667262995547,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667262995547,
        "tmdate": 1667262995547,
        "tddate": null,
        "forum": "8foynpwwRb",
        "replyto": "8foynpwwRb",
        "invitation": "ICLR.cc/2023/Conference/Paper6416/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents new training methods called RST and G-RST to extend the geometry inspired training method SAM and improve its computational efficiency.\nBased on randomized gradient boosting RST randomly selects between SGD and SAM with probability based on parameterized Bernoulli distribution.\nThe authors explore different parameterization schemes for such a scheduling function and analyze their effect on computations and performance trade-off.\nThe authors also develop RST\u2019s convergence properties for non-convex stochastic cases where the classes of objective functions are smooth and strongly convex.\nThe paper evaluates RST and G-RST on multiple image classification tasks showing that the proposed methods can save 50% computations while performing on-par or even better than the original SAM.\n",
            "strength_and_weaknesses": "- The paper is written in a very clear and professional way.\n- The paper is very solid with balanced views and analysis.\n- The paper provides convergence analysis which somehow lacks in SAM literature.\n- The resulting algorithm is simple and effective.\n- I literally didn\u2019t find any flaws in the paper but thought adding more experiments on large scale and different domains could make the paper even stronger.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very well written overall. The paper is also original to some extent in the sense that although there exist SAM variants attempting to improve on SAM\u2019s computational aspect, unlike most of these works this work develops based on matured techniques and optimisation characteristics and provides very well thought-out and reliable results.\n",
            "summary_of_the_review": "Highly recommended for interested readers on SAM literature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_cVKR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6416/Reviewer_cVKR"
        ]
    }
]