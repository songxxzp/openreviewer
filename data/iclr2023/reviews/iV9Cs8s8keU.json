[
    {
        "id": "QFHJkTn9QzE",
        "original": null,
        "number": 1,
        "cdate": 1666523883994,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666523883994,
        "tmdate": 1666523883994,
        "tddate": null,
        "forum": "iV9Cs8s8keU",
        "replyto": "iV9Cs8s8keU",
        "invitation": "ICLR.cc/2023/Conference/Paper1599/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to learn the positions of the no-zero entries in count-sketches for better approximation accuracy. It improves previous approaches learn only the value of the no-zero entries. It first proposes a greedy solution, which has high complexity, then designs simpler algorithms for low rank approximation and Hessian approximation. Theoretical analysis shows that the algorithms reduce the size of the sketch without harming worst case performance.",
            "strength_and_weaknesses": "Strength\n1. Learning the position of non-zero entries makes sense and is a good idea.\n2. The two algorithms are simple and thus practical, and come with theoretical guarantee.\n3. The algorithms significantly reduce the approximation errors in the experiments. \n\nWeakness\nNothing that I can think of.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Good clarity, quality and novelty. I believe the results are reproducible. ",
            "summary_of_the_review": "This paper proposes to learn the positions of non-zero entries in count-sketches. The proposed methods are practical and come with strong theoretical guarantees. The empirical results demonstrate the effectiveness of the proposed algorithms.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_oTN6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_oTN6"
        ]
    },
    {
        "id": "3CeIYKLpHjC",
        "original": null,
        "number": 2,
        "cdate": 1666648258090,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648258090,
        "tmdate": 1666648258090,
        "tddate": null,
        "forum": "iV9Cs8s8keU",
        "replyto": "iV9Cs8s8keU",
        "invitation": "ICLR.cc/2023/Conference/Paper1599/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper focuses on sketching for linear algebraic problems like LRA. They move the state-of-the-art boundary theoretically and support it empirically. The main idea is to learn positions of non-zeros in the countsketch matrix . The approach seems reasonable and as shown in experiments is valuable.  Some comments /questions\n\n1. not a learning heavy paper. Crux of contributing algorithms do not use learning and its target problems as well are linear. So not sure if it fits the call of papers. Something for Meta-Reviewers / Area chairs to decide. \n\n2. The setting is not clear. where there is a set of training matrices {A1, .. An} and the test is supposed to be on an unseen data from the same distribution. This is not very clear. For example when you say A \\in \\mathcal{D}, does it mean the order of rows in A is also important? Or does it mean \"each row of A\" \\in \\mathcal{D}. In the latter case, it is difficult ot imagine how a fixed S will work for unseen A. For example S learned on A will not work on a permutation of A.  Maybe it will help clarifying this better.\n\n3. I have not read much literature on learning of sketching matrices. so maybe its well known. I think it is important to point out the downside of these approaches. For example, the CS matrix is O(1) storage due to use of universal functions, however the storage of learned sketch matrix requires O(n) memory. It is important to point out this trade-off between CS vs learned sketch\n\n4. The proposed approach is compared with sketching baselines namely - vanilla CS, Indyk 2019. However, as the authors also use existing techniques like ridge-leverage score sampling etc. It seems important to evaluate a comparison between sampling approaches (which in some sense are sketching as well). For example, a comparison with cohen 2017 - both theoretical and empirical would be good to have. ",
            "strength_and_weaknesses": "[Strengths] The paper is well written and relatively easy to understand (except for some clarification needed. see above) . Intuitions are provided with associated theory which is very much appreaciated. \n[weakness] comparisions where only pure-sketching  methods are considered and the sampling methods such as leverage score sampling which is used by the algorithm itself is not used as a baseline. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The work seems novel ( i have not read a lot of recent papers in this field. so i am not sure). ",
            "summary_of_the_review": "I believe the paper is a good contribution to the field. I would like resolutions to some of the issues with writing and experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_Akjx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_Akjx"
        ]
    },
    {
        "id": "ZFuZFimD9O",
        "original": null,
        "number": 3,
        "cdate": 1666849722311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666849722311,
        "tmdate": 1666849722311,
        "tddate": null,
        "forum": "iV9Cs8s8keU",
        "replyto": "iV9Cs8s8keU",
        "invitation": "ICLR.cc/2023/Conference/Paper1599/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers sketching based algorithms for low rank approximation and second-order methods for regression where the sketching matrix is learned. If the underlying data matrix is drawn from some unknown distribution, one can use historical samples of the data to learn better sketch matrices rather than the classical, data independent Count Sketch. While prior learning-based sketches only learn the values of non-zero entries of the sketching matrix (and keep the positions random), the paper proposes a framework to also learn the positions of the non-zero entries of the sketching matrix. The proposed algorithm is fairly natural and outperforms prior learned sketching based approaches.",
            "strength_and_weaknesses": "Strengths:\n\n+ The paper tackles a natural drawback of existing approaches for learning-based sketching algorithms. It provides good intuitions regarding the central premise of learning positions of non-zero entries of the sketching matrix as opposed to just their values (as prior work)\n\n+ The paper is well-written and all claims are well justified. The paper is well-written and includes detailed algorithms for clarity. I feel the presentation of the paper got hurt significantly due to page restrictions, and I encourage the authors to publish a full version where the flow of text is not hurt due to page limits.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and all claims are well justified. ",
            "summary_of_the_review": "Overall I feel the paper makes an important contribution. The key ideas introduced are fairly natural and many of the proofs follow from standard techniques. Empirical results illustrate that learning positions of non-zeros along with their values significantly improves the approximation of classical CountSketch (at a similar sketch size).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_ZY3u"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_ZY3u"
        ]
    },
    {
        "id": "l39N5e-Bvp",
        "original": null,
        "number": 4,
        "cdate": 1666996196588,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666996196588,
        "tmdate": 1668623934637,
        "tddate": null,
        "forum": "iV9Cs8s8keU",
        "replyto": "iV9Cs8s8keU",
        "invitation": "ICLR.cc/2023/Conference/Paper1599/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper is about learning linear maps for dimensionality reduction that are based on count sketch. Existing works either use randomized sketches or learn the values in a count sketch matrix (with the positions drawn randomly). This paper develops on such methods by proposing to learn both the positions and the values of the count sketch matrix. The paper studies this idea in the context of two problems: low-rank approximation and iterative Hessian sketching. The contributions of the paper are:\n1. It shows that learning positions and values instead of just values leads to better performance. This is shown using a greedy search algorithm \n2. In order to overcome the high run time of the greedy search algorithm, the paper proposed algorithms to construct the count sketch matrix using ridge leverage score based sampling. \n3. The paper shows using experiments that the proposed ideas lead to better numerical performance. ",
            "strength_and_weaknesses": "Strengths:\n\n1. The idea of learning both the positions and the values is interesting and warrants a study. This paper does a good job of exploring this (previously unexplored) idea. \n\n2. The paper shows strong improvement over previous algorithms in terms of numerical performance.\n\nWeaknesses: \n\n1. The presentation of the paper can be improved a lot. Some examples of poor presentation are: \n    - Algorithms 1, 2 and 3 are placed in unnatural locations, with presumably liberal usage of \\vspace and \\hspace. The paper is visually \n        very dense and doesn't need to be.\n    - On page 3: \"Hence, in this work we consider the following algorithm that compresses both sides of A...\": which algorithm?\n    - Authors should consider completely moving the few-shot learning setting to the appendix and write more clearly about the two main \n        problems considered. \n    - Placement of the results tables are also not great. For example, Table 7.4 appear at a random location in between text. \n\n2. I have the following questions regarding the experiments:\n    - Shouldn't metrics always be relative errors? For example, in the low-rank approximation setting, instead of reporting ||A-A_k|| - ||A-X*||, \n        it should be ( ||A-A_k|| - ||A-X*||)/ ||A||. This would indicate how large the error is w.r.t the Frobenius norm of A itself and gives a better \n        idea of performance.\n    - Why are the errors in the few-shot learning case smaller than when the full training set is used? Intuitively, it should be the opposite. \n    - The authors mention that during training, only the average of the training matrices are used. However, this is not equivalent to \n        minimizing the expected error over the training set. It is also not clear if the baseline methods (especially IVY19) follow this procedure.  \n        Upon reading the IVY19 paper, it appears that they do minimize the error over all the training matrices. Maybe I am missing something, \n        but there is chance that the baseline methods have worse performance because of this? It would be better to present results (at least \n        on the baseline methods) where the training fully takes advantage of the training set and does not use just the average. \n    - Authors mention that they use torch.qr for backpropagating through QR factorization. Do the authors take any measures to ensure \n       that the first few columns are linearly independent (as required by torch.qr: https://pytorch.org/docs/stable/generated/torch.qr.html)?\n    ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper seems to be put together hurriedly. Presentation and flow can be improved quite a lot. See above for specific comments. \n\nQuality: Idea is quite interesting and warrants a study. The paper is technically sound.\n\nOriginality: The idea is original, although the proposed algorithms rely quite heavily on existing frameworks (ridge leverage score sampling and sketch monotonicity). \n\n",
            "summary_of_the_review": "The paper studies an interesting problem and presents good ideas. However, the presentation needs to be improved and the experiments should be explained more clearly. I have elaborated further in my comments above. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_qwvC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1599/Reviewer_qwvC"
        ]
    }
]