[
    {
        "id": "Q8dNk0GftjX",
        "original": null,
        "number": 1,
        "cdate": 1666463564957,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666463564957,
        "tmdate": 1666571654982,
        "tddate": null,
        "forum": "dNyDCl2FsvM",
        "replyto": "dNyDCl2FsvM",
        "invitation": "ICLR.cc/2023/Conference/Paper3129/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an analysis of a solution for achieving compositional generalization on the grounded SCAN dataset, in particular on its compositional split \"H\", i.e. the novel adverb-verb combination setting. The model is required to generalize to creating commands for instructions such as `pull X while spinning`, but has never seen `pull` and `while spinning` together during training. The authors approach this problem inheriting the Meta-Seq2Seq approach from Lake et al. (2019) where a Seq2Seq model is trained to use a batch of support instructions and target commands pairs to predict the output command for a given query instruction. They create the support set using an oracle function (which is actually a manual instantiation of GECA (Andreas, 2020)) and show that permuting the mapping from each instruction to the command (not clear how the permutation was done, but I can imagine by reading the Meta-Seq2Seq paper). They show that the resulting model can outperform the baselines on split H (and D) and provide an ablation of the most important components, namely the use of permutations and the oracle function to create support examples.\n",
            "strength_and_weaknesses": "Strengths:\n- *Strong performance on split H*: The paper presents a model that show promise in improving performance on split H for GScan. Improvements over recent baselines are promising.\n\n- *Nice ablation study*: The ablation study and the analysis are interesting to understand the failures of the method. I particularly appreciate when a particular problem is studied in detail and ablations are provided.\n\nWeaknesses:\n- *Limited clarity*: The paper could be more self-contained: I found it hard to figure out what is going on without re-reading the relevant literature. For example the GScan dataset and the original Meta-Seq2Seq model have not been introduced.\n\n- *Limited scope*: In-depth analysis of a particular generalization failure in only one setting, GScan. A more impactful paper would offer perspective on how the method can be fruitful to solve also other compositional generalization datasets (CFQ? Natural Questions?)\n\n- *Limited novelty*: An application of the Meta-Seq2Seq model from Lake et al. 2019 to GScan. I cannot identify whether there exist an intrinsic modelling novelty brought forward by this work, apart from the design of the oracle function that generates support sets. The oracle function resembles the methodology introduced by GECA.\n\n- *Limited applicability*: Ablation offered the key insight that permutations are important to solve split H, but afaicu, permutations require knowing a lot about the structure of the problem (e.g. that walk maps to I_WALK). Is this realistic for other datasets?\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Questions and Concerns\n- Did you try a retrieval based approach to mine support examples instead of using the oracle function?\n- Section 4.2: I am a bit unsatisfied by this section. I find the statement that \"the issue is not due to a lack of generalization\" a bit general as any performance reduction on the test set can be attributed to a lack of generalization. What do the authors mean? What is the problem if it is not \"generalization\"? What would you conclude from this analysis? Are we happy with a model that solves the H split at 86%? Has the model really learnt to compose the two concepts? I think maybe answering these questions could give a bit more depth to this section.\n\n## Typos and Clarity:\n- It would be great if you could first introduce GScan and Meta-Seq2Seq, then go on into describing how you modify the existing approach, then present the results.\n- Can you provide examples of the permutations applied to I and A? It is hard to understand what happens from the exposition, the description of the \"Permuter block\" has not been introduced.\n- The support set examples in Figure 2 seem to be all the same, is this an error?\n- Section 4: \"Each cell in the state S is encoded as a bag-of-words...\" I don't understand this sentence, the notion of \"state\" has not been introduced before, GScan has not been introduced.\n- In paragraph \"Transformers Actions\": \"The transformer cannot leak the target location for the south-west object\" , no such object was introduced before\n- ALRED should be ALFRED in the related works?\n- \"target target\" in the Section 4.2",
            "summary_of_the_review": "I do overall appreciate the relevance of the problem the paper is trying to solve and the ablations therein. Lack of novelty and limited scope indicate this paper might be more suited for a workshop. In its present form, the paper lacks clarity, it is not self-contained and thus it might not be ready for publication just yet.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_TWtT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_TWtT"
        ]
    },
    {
        "id": "gnAnv2BZ-c",
        "original": null,
        "number": 2,
        "cdate": 1666648234636,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648234636,
        "tmdate": 1666648430505,
        "tddate": null,
        "forum": "dNyDCl2FsvM",
        "replyto": "dNyDCl2FsvM",
        "invitation": "ICLR.cc/2023/Conference/Paper3129/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the generalization ability of language-instructed agents of gSCAN. Based on the Meta-Sequence-to-Sequence learning approach and meta-seq2seq architecture of Lake 2019, they issue the statistical action-bias of gSCAN split-H and extend the Meta-Sequence-to-Sequence learning approach to the specially case of split-H. They introduce \u201coracle templating function\u201d tune the model on the different in-distribution set of split-H. They also provide the model improvements, such as replacing RNNs with Transformers. They succeeded to tune their models to some specific test sets of Split-D and Split-H sacrificing performance in other testsets, using the preliminary known information of the testsets.\n \nEntirely authors include both the contribution of the proposed \u201coracle templating function\u201d in Sec 3.2 and the comparably minor model improvement (e.g., swapping RNNs with Transformer and minor neural network modification / engineering). It is regrettable that authors mix two parallel contributions and hence it becomes less clear what is the major contribution of the paper. Please clarify the differences from Lake 2019 concisely in the manuscript. In the current paper, authors raised many incremental improvements one by one in Sec 3.2 seemingly irreverent to the main contribution of \u201coracle templating function.\u201d The details and insights of \u201coracle templating function\u201d are not clear. Overall, the arguments of this paper are not still convincing yet.",
            "strength_and_weaknesses": "- The strength of this paper is that they obtained better performance than the previous models with \u201coracle templating function\u201d in limited test splits, although there are following problems as written in weakness.\n \n- The first problem is that the proposed model uses the periluminal knowledge of some testsets such as the different distributions of verbs and adverbs mentioned in Sec 3.2. It is prohibited in the previous studies compared in Table 2. The details of the contribution \u201coracle templating function\u201d is written in Appendix, not in the main paper. It still lacks details.\n \n- The second problem is that the contribution / observation is limited. In this paper, authors proved that tuning Meta-Sequence-to-Sequence learning approach on the limited testset of gSCAN is effective. However, this is not so surprising because some testsets of gSCAN are designed for different test situations. Therefore, we can tune our models for some specific test sets if we preliminary know the information of the testset, such as the different distributions of verbs and adverbs. When we turn some models into the specific distribution, we obviously sacrifice the performance in other test sets with different distributions. This is the observation of the main score Table 2 and is not a novel thing in the learning of the out-of-domain adaptation. Hence the overall paper contribution looks not obscure. In short, optimizing models just for a few good test sets is not a good contribution.\n \n- The third problem is indeed the largest problem: the motivation to access the statistics/bias of gSCAN test set and taking advantage of the existing bias in tuning model is not discussed well in this paper. I\u2019d like to know concrete applications or explanations how the proposed models / approaches for the limited OOD test sets are useful in the following studies or real applications. If the motivation in this paper relies on human compositional problem solving as written in the introduction, how the observations in the experiments, especially for the results of \u201coracle templating function,\u201d have contributed to it?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The details of  \u201coracle templating function\u201d lacks from at least of the main paper. I'm not confident readers can reproduce their results from these writings.\n\n- As a note, I didn\u2019t recommend that authors present the statistics of test sets on the paper because I allow readers to know (or even leak) the knowledge of the test set. Although in this paper it is somehow inevitable for discussion.\n \n- As a minor comment, the writing is grammatical, but not so well-composed or fluent. It sometimes lacks clear arguments.\n\n- Typo:\nALRED -> ALFRED\n",
            "summary_of_the_review": "Overall, the paper arguments are still not clear nor convincing yet. The motivation doesn't sound. I hope authors have further discussion on the motivation and the next directions of the entire paper possibly in other conferences or workshops.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_wsCX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_wsCX"
        ]
    },
    {
        "id": "qWdOzEI9EGz",
        "original": null,
        "number": 3,
        "cdate": 1666739895816,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666739895816,
        "tmdate": 1666739895816,
        "tddate": null,
        "forum": "dNyDCl2FsvM",
        "replyto": "dNyDCl2FsvM",
        "invitation": "ICLR.cc/2023/Conference/Paper3129/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper works on compositional generalization of language-instructed agents. \nThey apply meta-seq2seq method to this setting. \nThey conduct experiments on the gSCAN benchmark. ",
            "strength_and_weaknesses": "Strength \n\nThey study an important problem. \n\nUsing meta-learning for compositional generalization is reasonable. \n\nThe results on Split H are positive and they also conducted a range of ablation and error analysis. \n\nWeakness \n\nThe paper is poorly organized and very hard to follow. This is the key weakness, but this makes me find it really difficult to judge the overall technical quality and significance. \n\nE.g., after reading the related sections a few times, I still do not understand how meta-seq2seq is applied in their setting and how the technical components handle the gSCAN environments. It seems that many important introductions and remarks are missing. So I suggest that the authors give a technical introduction of the framework and more precisely discuss how it can solve the problem of interest, possibly with visual illustrations. \n\nSome important concepts are repeatedly used without a definition. E.g., what is a \"support\"? The paper has \"support set\" and \"support instructions\" at many places but it is unclear to me what it actually means. I also read the original gSCAN paper but they didn't use this term at all. Similarly, what is an \"oracle\"? What is \"in-distribution\" instructions and how would an instruction from non-oracle look to an instruction from oracle? \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "please see above",
            "summary_of_the_review": "The paper aims to address an interesting problem and their general technical idea (using meta-learning) is reasonable. \n\nBut the paper is poorly written and it is hard to give a good judgement on its current version. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_xpEo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_xpEo"
        ]
    },
    {
        "id": "3LDLC_SoDQ",
        "original": null,
        "number": 4,
        "cdate": 1666825062382,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666825062382,
        "tmdate": 1666825062382,
        "tddate": null,
        "forum": "dNyDCl2FsvM",
        "replyto": "dNyDCl2FsvM",
        "invitation": "ICLR.cc/2023/Conference/Paper3129/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper expands the Meta-Sequence-to-Sequence learning approach proposed by B. Lake in 2019, adapting it to be used with a modern transformer architecture and applying it to the gSCAN benchmark, utilizing the idea of random symbol/token permutation as a remedy against overfitting to specific sequences in the training set. The authors show that performance on two challenging splits of the dataset is improved, compared to previous works.",
            "strength_and_weaknesses": "Strengths:\n- The paper addresses a highly relevant problem\n- The proposed method is elegant and might allow to model abstraction and generalization\n- The proposed method shows promising results\n\nWeaknesses:\n- The method is very closely related to the work \"Compositional generalization through meta\nsequence-to-sequence learning\" by Lake (2019), which it builds upon. While adapting it to transformer architecture and a new task is not trivial, it does limit the conceptual novelty of the contribution.\n- Empirical results, while promising, are confined to a relatively narrow domain. This is especially unfortunate, since when novelty is relatively limited, the breadth and impact of empirical results becomes paramount.",
            "clarity,_quality,_novelty_and_reproducibility": "** Clarity ** \n\nThe paper is generally well written and is a pleasure to read. I especially enjoyed the literature review section which is both thorough and concise.\n\nI was a bit confused about Figure 4. I expected the support instructions to be different, not just a copy of the same support instruction. Maybe choosing a different example would be more insightful.\n\nAlso, the authors refer to appendices in the paper, but the paper does not have any (they are included in the supplementary materials instead). Generally appendices are usually included in the paper, while supplementary materials can be used to provide code or even more extra information that does not naturally fit into appendices. \n\nGenerally, these are minor points that did not affect my evaluation. Overall, the clarity of the paper satisfies ICLR standards.\n\n** Reproducibility **\n\nClear descriptions, together with the code that authors provide, make the paper satisfy the highest reproducibility standards.\n\n** Novelty **\n\nThe novelty and significance of the paper are, unfortunately, very much borderline. While the paper does show improvements on two splits in gSCAN, the value and the overall impact of this specialized architecture may not be high enough.\n\nAdditionally, while the paper does outperform another specialized architecture developed for the gSCAN dataset (Improving Systematic Generalization Through Modularity and Augmentation, Ruis, Lake, 2022), I don't believe that this improvement is sufficiently consequential, since that method is an archive pre-print, and, while providing valuable ideas, does not in itself establish a standard basis for comparison.\n\nThe method is, without a doubt, elegant, and it may provide a pathway towards modeling abstraction in language models, but its key idea was already known (Lake 2019). Therefore, we must resort to judging the contribution based on how impactful its applications are, and I am not sure if the impact is sufficient to meet the standards of the ICLR conference.\n\n** Quality **\n\nThe experiments fit the goal of the paper and are clearly reported. There is, however, one potential concern I have about the experimental design (see \"questions\") section below.\n\n** Questions **\n\nThe authors say \"The oracle never generates an example of the query instruction or instructions from test Split H,\" but it's not entirely clear whether it accounts for random permutations. I.e. the authors say \"critically, we make random permutations of word/symbol assignments for both the instruction and the support commands\". Is there a chance that an oracle generated something, then it was permuted, and the result actually does happen to be in the test Split H?\n",
            "summary_of_the_review": "I enjoyed reading the paper which addresses a highly important and ambitious problem of compositional generalization. I believe that the proposed method is interesting and has the potential to be generalized to model abstraction and generalization in a wider array of applications. Unfortunately, in its present state, I feel that the combination of relatively limited novelty with the relatively narrow range of applications puts this paper below the threshold of acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_EJmn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3129/Reviewer_EJmn"
        ]
    }
]