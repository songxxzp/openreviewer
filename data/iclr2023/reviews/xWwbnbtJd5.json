[
    {
        "id": "jy3sWYmQL_",
        "original": null,
        "number": 1,
        "cdate": 1666450624029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666450624029,
        "tmdate": 1666450624029,
        "tddate": null,
        "forum": "xWwbnbtJd5",
        "replyto": "xWwbnbtJd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3098/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In the paper, the author proposed a new Transformer variant with linear complexity, which is called Waveformer. In each Waveformer layer, the input will first project to the coefficient space using a forward discrete wavelet transform. Then a linearized attention operation is applied via random features. Finally, the outputs are transformed back to the \"contextual\" space using a backward discrete wavelet transformer step. The authors studied the expressiveness of the architecture and conducted experimental analysis of the model on standard benchmarks.",
            "strength_and_weaknesses": "Strength:\n\n1. The architecture is new as far as I know and studying the efficiency of the model is an important direction.\n2. It is useful to show the model is a universal approximator, which is the same as the original Transformer model.\n\nWeakness:\n\n[Novelty]\n1. Where is the linear complexity coming from?\n\nIf I understand correctly, the operation that reduces the quadratic complexity in attention is to use the random feature kernel developed in Choromanski et at., 2020, but not the proposed wavelet transform. \n\nIn the first round of reading, I got confused as in several parts of the paper, the authors emphasize that the linear-complexity model is a major contribution. I believe the authors have overclaimed that.\n\n2. Why you use wavelet transform?\n\nIn the work, the input will first project to the coefficient space using a forward discrete wavelet transform. Then a linearized attention operation is applied. Note that even if you don;t use the wavelet transform, the model is already a linear transformer with random features. So is the wavelet transform redundant and what is the benefit?\n\n[Universal approximation]\n\nOnce you use the random features, your model is, in fact, a \"randomized\" function. How did you deal with approximating a deterministic function by a \"randomized function\"? I didn't find any place in the proof coping with that. That being said, the proof is incomplete and lacks sufficient details. I don't think you can simply let the random feature dimension approach infinity and say, \"MSE error vanishes to 0\"\uff0c since we are concern about a finite-size Transformer's approximation ability\n\n[Experiments]\n\nThe author missed important effcient Transformer S4 (\"Efficiently Modeling Long Sequences with Structured State Spaces\"). In all of the LRA tasks, S4 is roughly better than the proposed model for 20 points. I have little reason to vote a paper for acceptance given the significant performance gap.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is generally clear. I have questions regarding the novelty and the significance of the experimental results.",
            "summary_of_the_review": "I believe proposing efficient Transformer is essential but the current quality of the work (motivation, empirical results) are not ready to publish in the venue, and I recommend rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_jCxJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_jCxJ"
        ]
    },
    {
        "id": "Dc6xvHiJSXy",
        "original": null,
        "number": 2,
        "cdate": 1666480576461,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666480576461,
        "tmdate": 1666480576461,
        "tddate": null,
        "forum": "xWwbnbtJd5",
        "replyto": "xWwbnbtJd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3098/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper innovates attention block by sandwiching it into wavelet forward and backward transformations. Empirical results suggest that this approach can improve upon vanilla softmax attention, as well as other approximated and more efficient attention mechanisms.",
            "strength_and_weaknesses": "The method is simple and interesting. Empirical results (especially Tab. 4) are encouraging. It encourages reader to think \"\nwhy attention in (wavelet) transformation domain can outperform the conventional one in time domain?\".\n\nHowever, the answer is still unknown after reading the paper. An in-depth analysis is lacking. Section 2.2 discusses the advantage of wavelet coefficients against Fourier coefficients. The authors suggests maintaining both time and frequency resolution is helpful. But that is a signal processing perspective, whose connection to representations learning is not clear. In that sense, is short-time Fourier transform also able to outperform Fourier transform? Theorem 1 is a combination of two existing theoretical results. It still cannot provide an answer to the question above.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is well written. A few clarity questions:\n\n1. Second to last paragraph in section 2.2: \"... perform discrete wavelet transform over each spatial dimension of the input ...\"--- by \"spatial dimension\" -- you mean each of the $d$ dimensions of the input feature?\n\n2. The wavelet transform will keep time axis while adding another frequency axis. How is that handled? 2D input becomes 3D, or simply \"flatten\"/\"reshape\" to 2D?\n\n",
            "summary_of_the_review": "Although an in-depth analysis and theoretical justification is lacking, the paper presents a simple method and promising results, which is novel and interesting.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_ciFm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_ciFm"
        ]
    },
    {
        "id": "EYHjwSq1c1",
        "original": null,
        "number": 3,
        "cdate": 1666623540699,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623540699,
        "tmdate": 1669285129052,
        "tddate": null,
        "forum": "xWwbnbtJd5",
        "replyto": "xWwbnbtJd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3098/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper \"WAVEFORMER: LINEAR-TIME ATTENTION WITH FORWARD AND BACKWARD WAVELET TRANSFORM\" proposes integrating analysis- and synthesis-wavelet transforms into transformers.\n\nThe main idea is to move attention computations into the wavelet domain, by framing the attention computation with a forward and backward wavelet transform. The attention module uses the randomized mapping known from the performer architecture.\nThe proposed network is evaluated on the Long Range Arena and the Code Understanding benchmarks.\n\nThe authors observe competitive or improved performance.",
            "strength_and_weaknesses": "### Strengths:\n- To the best of my knowledge the fast wavelet transform has not yet been studied in attention modules.\n- The theory is adequately discussed.\n- The reported results are competitive.\n\n### Weaknesses:\n- The novelty of the proposed approach is limited. Wavelet transforms have previously been studied in other contexts.\n- For a paper which studies the introduction of wavelet transforms, it would have been interesting to study the effect of the wavelet itself.\n- It seems different wavelets have not been explored.\n- Similarly, the depth of the wavelet transforms has not been studied. It would have been interesting to see if and how the level choice impacts the results. ",
            "clarity,_quality,_novelty_and_reproducibility": "## Quality and clarity:\n\n### Methods:\n- Equation (1) previously appeared in the attention is all you need paper. A citation would be in order.\n- Do equations 2, 4, 5, 7 and 8 appear in standard textbooks? Adding citations helps your readers find them. \n- Figure 2: The third plot in figure 2 titled 'Wavelet Transform,' looks like it was produced by a continuous transform. Are the wavelet-transforms in the paper discrete or continuous?\n- Section 2.2: The Short-Time-Fourier-Transform (STFT) is often used instead of wavelet-transforms to conserve temporal or spatial information. While the introduction to section 2.2 is true, not mentioning the STFT could be misleading.\n- Fnet [Lee-Thorp et al.] uses two-dimensional Fourier-Transforms. Are the wavelet transforms used in this work two-dimensional?\n- If a two-dimensional transform was used is the implementation using two 1D-transforms or a single two-dimensional transform? \n- If 1D wavelets are used, how are these augmented to be useful in 2D?\n\n### Experiments:\n- Table 1: Are the experiments reproducible? Are seed values for all experiments known?\n- Table 1: How big is the effect of different seeds? Do we know the variance on the numbers in table 1?\n- Table 1: Is the performer experiment identical to the waveformer experiment if the analysis and synthesis wavelet-transforms are removed?\n- Table 1: Are the results for the related work in table 1 reproductions or citations? If the values are cited a citation of the original paper should appear in the model column.\n- Which wavelet was used for the experiments? How many coefficients does it have? How was it chosen?\n- Which decomposition level do the transforms have? How was the level chosen?\n- Table 4: Are accuracy values tabulated in table 4?\n\n### Copy editing:\n- The paper requires additional copy editing. Minor flaws are mostly related to articles and include i.e:\n    - After the introduction:\n        - The first line could be either \"the transformer\" or \"transformers\".\n    - In the contribution section:\n        - i.e. we develop the waveformer, which requires ...\n\n## Originality:\n- Wavelet transforms are well-known in machine learning. The papers-related work section addresses this fact properly. In the context of attention introducing wavelets is original.\n",
            "summary_of_the_review": "This paper presents an interesting application of fast wavelet transforms in attention modules.\nThe results are competitive. The experimental part of the paper leaves many questions open.\nWavelet choice and the number of scales for example are not investigated.\nMean, and variance are unknown for all experiments. Experimentally the paper presents the bare minimum.\nI am recommending a weak reject for now, but am willing to adjust my rating depending on the rebuttal.\n\nEdit: In response to the rebuttal, I have raised my rating from five to six.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_DaL7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_DaL7"
        ]
    },
    {
        "id": "atwQ5aBi-Uo",
        "original": null,
        "number": 4,
        "cdate": 1666903198874,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666903198874,
        "tmdate": 1666905158850,
        "tddate": null,
        "forum": "xWwbnbtJd5",
        "replyto": "xWwbnbtJd5",
        "invitation": "ICLR.cc/2023/Conference/Paper3098/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a wavelet transformation based liner attention mechanism named Waveformer. Experiments on long-range arena benchmarks have been conducted to demonstrate the effectiveness of the proposed method.",
            "strength_and_weaknesses": "Strengths:\n* The authors focussed on an important issues in sequence modeling\n* The paper is organized well and easy to follow\n* Technical details are presented clearly\n\nWeakness:\n* The novelty needs to be further justified\n* The evaluation needs to be strengthen",
            "clarity,_quality,_novelty_and_reproducibility": "Sufficient technical details and background knowledge are presented, though the authors may need to further justify the novelty of the proposed method.",
            "summary_of_the_review": "My major concern is the novelty of the proposed method. Applying spectral-based transformation (Fourier or wavelet) for efficient computation has been widely adopted in ML (also indicated in Sec. 4.2 by the authors). The claim of non-linear transformation as a major contribution seems to be incremental. I suggest the authors to better justify the novelty of the proposed approach and the motivation of applying wavelet transformation.\n\nFor experiments, I suggest the authors explicitly indicate how many repetitions each model has been trained and also report the variance of accuracy. The authors choose random feature kernel in the paper and I wonder if the authors have considered other kernels or how different kernels may affect the model performance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_KfzV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3098/Reviewer_KfzV"
        ]
    }
]