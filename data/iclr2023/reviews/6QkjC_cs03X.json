[
    {
        "id": "RSuylogLQx",
        "original": null,
        "number": 1,
        "cdate": 1666501124976,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666501124976,
        "tmdate": 1666501124976,
        "tddate": null,
        "forum": "6QkjC_cs03X",
        "replyto": "6QkjC_cs03X",
        "invitation": "ICLR.cc/2023/Conference/Paper5204/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a VAE framework for that formulates attentions in transformers as a mixture of gaussians, where variational inference can be used to learn the framework. The proposed VAE with transformers as the encoder and decoder is used in text reconstruction and generation tasks. ",
            "strength_and_weaknesses": "Strength:\n\n1. The (re) formulation of attention to a gaussian mixture and the application of variational inference are interesting and intuitive.\n\n2. The paper provides comprehensive experiments on parameter sensitivity and ablation study.\n\nFeedbacks (not all about weakness)\n\n1. Although the formulation of a gaussian mixture of attention is interesting and contributes to the significance of the paper, I think the technical depth is not large enough for an accept yet. Using variational inference or VAE with gaussian mixtures is a well studied technique, which is directly applied given the formulation.\n\n2. I don't see it is necessary to highlight Bayesian nonparametric or Dirichlet processes. First of all, DPs are usually applied into latent variables so that the dimensions of them can grow with data. In the case of this paper, the dimension of Z is the batch size of data, which does not usually grow in training or testing. Secondly, although discussing Bayesian nonparametric or Dirichlet processes, the paper ends up with a vanilla Dirichlet gaussian mixture model, which does not have strong connections with Dirichlet processes. It is not even a truncated approximation (e.g. stick breaking process) of DP. The paper names this unbounded DP and claims it as an approximation of DP, which requires theoretical justification.\n\n3. In the experiments, the paper mainly compares with the baselines of different configurations of VAE with transformers. I think it is expected to put the proposed method in the category of text generation methods and compare it with other kinds of methods in the same category. This is to show how the method is positioned in the task.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe general clarity of the paper is OK. But I do have confusions on connecting the ELBO with the attention architecture of the transformer. The ELBO shown in Section 3.1 is a general form of gaussian mixture. It is unclear to me that how to learn the parameters of attention by maximizing the ELBO. Clearer formulations of p(x|F) and q(F|x) with the attention parameters need to be introduced.",
            "summary_of_the_review": "The idea of the paper is interesting but I feel the technical depth is not enougth.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_fw7U"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_fw7U"
        ]
    },
    {
        "id": "gjmllzZVrIq",
        "original": null,
        "number": 2,
        "cdate": 1666620154811,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666620154811,
        "tmdate": 1666620154811,
        "tddate": null,
        "forum": "6QkjC_cs03X",
        "replyto": "6QkjC_cs03X",
        "invitation": "ICLR.cc/2023/Conference/Paper5204/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this manuscript authors embedding space of Transformer encoders as mixture distributions and as such are able to formulate transformer encoder - decoder as a VAE model. Number of mixture distributions is variable to authors decide to use Bayesian nonparametrics to solve the modeling issue, namely they use bounded Dirichlet prior. ",
            "strength_and_weaknesses": "Strengths: \n- Novel probabilistic formulation of attention model in transformers. \n- Full derivation is shown. \n- Model has potential usefulness beyond the experiments shown in the paper. \n\nWeaknesses: \n- Implicit reparametrization, that provides exact reparametrization for Dirichlet is not used ( https://arxiv.org/abs/1805.08498) \n- Manuscript seems to be typeset in a hurry, for example p(F|x) and p(F) are both named as prior. AFAIK, prior should not have data-term conditioning. Please clarify this point. \n- Even though mathematical development is convincing, authors have not motivated their work. They should clearly explain in Introduction that for what reason this development was done. It cannot be that just this formulation has never been done before!",
            "clarity,_quality,_novelty_and_reproducibility": "Paper needs more work in finalization. ",
            "summary_of_the_review": "All in all, I find the paper quite interesting and potentially very useful. I think the way how transformer is reformulated as a VAE model is quite neat. More large-scale experiments obviously would make the paper much better as authors also state in the Conclusions. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_tsdp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_tsdp"
        ]
    },
    {
        "id": "h6i3LoXYVhr",
        "original": null,
        "number": 3,
        "cdate": 1666660019688,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666660019688,
        "tmdate": 1666660019688,
        "tddate": null,
        "forum": "6QkjC_cs03X",
        "replyto": "6QkjC_cs03X",
        "invitation": "ICLR.cc/2023/Conference/Paper5204/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a probabilistic mixture model representation of the attention mechanism in transformers. The probabilistic representation  allows the authors to define priors and posteriors over the mixture distributions and ultimately redefine the transformer as a nonparametric variational autoencoder. They show experimental results for the reconstruction of input sentences, generation by sampling from the prior, regularize the size of the latent space, and interpolate.",
            "strength_and_weaknesses": "Strengths:\n- The paper seems interesting and a rather novel concept. Their approach is sound and the results seem ok. There are no glaring flaws in the paper. \n\nWeaknesses:\n- The point / takeaways / motivations are not very clear. Is the main benefit the regularization ability and the computational savings of having a lower dimensional latent space?\n- While transfomer based models e.g. BERT / GPT-2 have been shown to be very useful, the paper is showing results on a a two layer Transformer encoder and decoder with a single attention-head. While this is the original proposed structure for using attention, it does not mimic either the encoder stack models e.g. BERT or the decoder stack models e.g. GPT-2 that are now the SOTA.\n- Furthermore, it is not clear whether learning a NVAE for each attention mechanism (BERT would have 12*12=144 head) is computationally feasible and whether this training process is significantly more expensive than the standard learning of the query, key, and value weights. \n- In the experimental results, the NVAE model is compared against VTP and VT (which don't seem to have any hyperparameters as only 1 best model was chosen?) and a hand-coded solution VTS. The performance of NVAE and VTS seems relatively comparable, but is VTS supposed to be an \"oracle\" or is there any further description on what \"hand coded\" means?\n- In tables 7 and 8, all generated samples seem equally senseless? Similarly the interpolation examples (tables 9-12) seem to show all models are comparable? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe grammar is kind of strangely worded, but overall fine. There is a lot of content, but sometimes key details seems to be glossed over (see above box).",
            "summary_of_the_review": "The paper is promising although it could be reworked to be stronger with a more clear focus and motivation. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_w4Qq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_w4Qq"
        ]
    },
    {
        "id": "L2S46RkpbUC",
        "original": null,
        "number": 4,
        "cdate": 1667223510280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667223510280,
        "tmdate": 1667223510280,
        "tddate": null,
        "forum": "6QkjC_cs03X",
        "replyto": "6QkjC_cs03X",
        "invitation": "ICLR.cc/2023/Conference/Paper5204/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper mainly proposed the nonparametric variational autoencoder (NVAE) by 'combining' VAEs and the transformer.\nFurther, the transformer encoder and decoder are used in the VAEs framework.\nDifferent from VAEs whose prior is the standard Gaussian distribution, the proposed NVAE applied the nonparametric variational information bottleneck (NVIB) regulariser for the latent embedding.\nFinally, the experiment shows NVAE could do reconstruction, generation, and regularisation tasks. The interpolation further explored the meaningfulness of the learned representation.",
            "strength_and_weaknesses": "My biggest concern is in the experiment part. \n\nFirst, maybe I am wrong, but in my opinion, this work may be compared with SOTA works to support their claim. I am not convinced at least at the current reviewing stage. I am not convinced why we should choose NVAE rather than the original VAE. Because, the VAE could also do the reconstruction, generation, interpolation, etc. It is clear to me the VAE is more complex compared with VAE, with more inductive bias, but it is not clear to me NVAE whether NVAE could reach a significant improvement in performance or could do something VAE couldn't.\n\nSecond, the prior is a mixture distribution, could different distributions learn different aspects of the input object? e.g. a disentangled latent representation? Some ablation studies or visualizations may be needed. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is very detailed. The author included dozens of experiment setups and math in the supplement material, which makes their claims convincing and reproducible.\n\nThe motivation for this paper is not clear to me. For example, in the first paragraph of the Introduction, I am not clear why we should combine the transformer and VAE. If the motivation is to combine the strength of transformers and VAEs, which aspect of VAE could be improved in theory? ",
            "summary_of_the_review": "As above, my main concern is about the experiment, and the second is the motivation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_Lu5K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5204/Reviewer_Lu5K"
        ]
    }
]