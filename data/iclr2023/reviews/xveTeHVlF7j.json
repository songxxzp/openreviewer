[
    {
        "id": "xX9zOEXnx1E",
        "original": null,
        "number": 1,
        "cdate": 1666250470178,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666250470178,
        "tmdate": 1666250470178,
        "tddate": null,
        "forum": "xveTeHVlF7j",
        "replyto": "xveTeHVlF7j",
        "invitation": "ICLR.cc/2023/Conference/Paper4425/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposed to use self-attention as ansatz in FermiNet. There are some minor changes, including using log distance and spin in the input feature. The proposed model is proved better than FermiNet in several experiments including third-row atoms, large molecules and benzene dimer.",
            "strength_and_weaknesses": "Strength: the main strength of this paper is that --- it really works. I believe there are many researchers actively working on incorporating complicated model structure in FermiNet, and many of them may have the idea to use self-attention. This paper is the first one to make this work, which is not a naive accomplishment. The runtime scaled cubically, which means for large systems, FermiNet and PsiFormer ran at nearly the same speed. Also the experiments are very detailed.\n\nWeaknesses: I don't see any major weaknesses, but I have several questions or comments below.\n\n1. Why do you only consider features of electrons? Can we include the features of atoms in the self-attention mechanism? (DeepWF https://arxiv.org/pdf/1807.07014.pdf did something like this.)\n2. More ablation study would be helpful for understanding how the number of determinants, number of attention heads, attention dimensions and etc (basically the hyperparameters listed in table 6) make contributions to the accuracy of calculation.\n3. The divergence of training energy as in Figure 6 without input rescaling seems quite outrageous. Does the authors see similar issue in FermiNet and FermiNet SchNet?\n4. The authors claim that this work is the first time that KFAC is applied to self-attention layer (in Section 4). However, https://arxiv.org/abs/1907.04164 (from KFAC authors) already claimed that this is the first work where KFAC is applied to transformer model which include self-attention.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has good clarity, quality, novelty and reproducibility.",
            "summary_of_the_review": "This paper introduce self-attention in the ansatz in FermiNet. I guess the idea may have been studied by many researchers, and this paper is the first one to make it work, which is a good accomplishment.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_GTMe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_GTMe"
        ]
    },
    {
        "id": "1cxzhBGqSg5",
        "original": null,
        "number": 2,
        "cdate": 1666330816767,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666330816767,
        "tmdate": 1666330816767,
        "tddate": null,
        "forum": "xveTeHVlF7j",
        "replyto": "xveTeHVlF7j",
        "invitation": "ICLR.cc/2023/Conference/Paper4425/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new attention-based neural architecture for solving many-electron Schrodinger equation by optimizing the variational quantum Monte Carlo objective. While previous works proposed architectures like FermiNet, the proposed PsiFormer differs by (1) using self-attention between $O(N)$ electron-nuclear features and (2) incorporating $O(N^{2})$ electron-electron features through (parameterized) Jastrow factor. The proposed algorithm empirically outperforms the prior works in various experiments.",
            "strength_and_weaknesses": "Strength 1: This paper studies a very important problem in computational chemistry: solving the many-electron Schrodinger equation. Even a small progress in this area is likely to be impactful for future research.\n\nStrength 2: The proposed self-attention mechanism is reasonable since the self-attention mechanism has shown success for many machine learning applications. \n\nStrength 3: The experiments are thorough and it looks like the authors did a good job at solving a complex problem that requires a lot of engineering. \n\nWeakness 1: The proposed idea itself is not very novel. To my knowledge, the novelty of this paper is limited to the self-attention mechanism and the Jastrow factor-based parameterization of electron-electron features. Both of them has already been considered in prior works, like the FermiNet.\n\nWeakness 2: I hope the authors could further highlight the running time of their algorithm in the main text. I think this is importance since (1) running time is important for this type of problem and (2) the proposed self-attention mechanism is known to be quite slow. Currently, I find that the authors report their training time in Table 9 of the appendix. I guess this is the time required to solve the Schrodinger equation for a molecule. How does this time compare to the existing DNN-based or non-DNN-based solvers?\n\nWeakness 3: I would like to have seen some justification behind using the self-attention mechanism. Besides the additional expressive power, is there any reason that self-attention is more useful for parameterization of the energy function? Is there any gating mechanism in \u201cground truth\u201d for solving the Schrodinger equation?\n\nWeakness 4: There is no explanation on which Markov chain Monte Carlo algorithm is used for the variational quantum Monte Carlo. \n\nWeakness 5: Since the authors do not provide a code repository for their paper, given complexity of the problem being solved, the possibility of reproducing this paper\u2019s experiment is very low.\n\nQuestion: There are some works on graph-based Transformer to incorporate pairwise features to self-attention like Graphormer. I wonder if the authors have already considered this type of incorporation for their work, but discarded due to poor performance.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper provides a nice overview of the problem being solved. There are some details missing (like the sampling algorithm for VMC), but I could understand the paper even though I come from a machine learning background.\n\nNovelty: The novelty is not very high since self-attention is not a very new concept. However, I think this is fine given importance of the problem.\n\nReproducibility: Without any repository, I do not think it is possible to reproduce the paper. ",
            "summary_of_the_review": "This paper proposes a simple idea for solving an important problem. I believe significance and complexity of the problem being solved is enough to compensate for the (arguable) lack of novelty. I am concerned about reproducibility of this paper. I do not think the follow-up works will be able to reproduce this paper without allocating a significant amount of time.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_RSBm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_RSBm"
        ]
    },
    {
        "id": "LjblF9qZJ0",
        "original": null,
        "number": 3,
        "cdate": 1666577722786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577722786,
        "tmdate": 1666577765489,
        "tddate": null,
        "forum": "xveTeHVlF7j",
        "replyto": "xveTeHVlF7j",
        "invitation": "ICLR.cc/2023/Conference/Paper4425/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This is an interdisciplinary research paper on AI for science, it proposes a neural network framework based on self-attention mechanism, which can get an Anstazes solving the many-electron Schrodinger equation. The authors scaled the input features and concatenate the numerical spin into the input feature vector. Then they project input features to the same dimension of attention inputs by linear mapping to add self-attention mechanism into neural network framework. ",
            "strength_and_weaknesses": "The idea is novel for AI+Science. However, there are some incomplete or vague expressions in this paper. Please improve the language presentation carefully. The motivation for joining self-attention is not clear enough. How does the attention mechanism work on gating interactions between electrons and make the framework better? ",
            "clarity,_quality,_novelty_and_reproducibility": "The language used in this paper is very professional, but the explanation of terms is too brief, which is not very friendly to non-major readers. For the experimental part, there are too few baselines selected, which makes the result not strong enough. It is recommended to introduce more baselines or consider adding different kinds of attention mechanisms in these frameworks to compare with PsiFormer. I believe that the results will be more convincing. ",
            "summary_of_the_review": "AI for science is novel for our community. However, the contribution of this work is to introduce self-attention. It is an incremental contribution.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_58NZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_58NZ"
        ]
    },
    {
        "id": "4X22z0c3uQ",
        "original": null,
        "number": 4,
        "cdate": 1666663178604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663178604,
        "tmdate": 1668660518441,
        "tddate": null,
        "forum": "xveTeHVlF7j",
        "replyto": "xveTeHVlF7j",
        "invitation": "ICLR.cc/2023/Conference/Paper4425/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a self-attention based architecture called PsiFormer for approximating many-electron Schr\u00f6dinger equation. The architecture uses self-attention transformer to learn the embedding of electrons and the pairwise features are incorporated through Jastrow factor. The authors did a series of experiments on multiple molecular systems and compared PsiFormer with FermiNet, the previous state-of-the-art system for this task.",
            "strength_and_weaknesses": "The strength of this paper is that PsiFormer shows improvement over the FermiNet, especially on larger atoms with more electrons. The weakness, however, is mostly about the experiment section. There are many parts of the experiment that needs further clarification. \n1. It's quite unclear whether the results are significant. The difference between FermiNet and PsiFormer is very close (within 0.01 for smaller atoms and 0.1 for larger atoms). Also, what's the ground truth for each electron system? Is it the result coming from VMC or DMC? If so, shouldn't you report the mean absolute error (MAE) between FermiNet/PsiFormer's prediction and VMC/DMC? Should a system get a lower MAE instead?\n2. What's the level of accuracy (or precision) needed for an algorithm to be useful? Is FermiNet good enough already? (again, improving results by 0.01 seems very small but I can be wrong).\n3. Why do you only report $\\Delta E_{mono}$ and $\\Delta E_{10A}$ for Benzene Dimer but not third-row atoms?",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the clarity of the experimental section can be much improved by explaining why the results in tables 1-3 are significant and how to interpret these results. The novelty of the work is moderate -- it uses self-attention and combine the pairwise features through Jastrow factors. It's also unclear why such design is the best. For example, why not use relational attention to incorporate pairwise features in the self-attention layer? Why do you include the pairwise features at the very end? Is this modeling choice guided by first principles?",
            "summary_of_the_review": "Overall, it is unclear to me how significant the results are and hard for me to draw a conclusion. I am not an expert in the field of quantum chemistry calculations so I am not confident in my judgment. I vote for weak rejection for now but open to suggestions during author feedback and discussion.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_x9Gw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4425/Reviewer_x9Gw"
        ]
    }
]