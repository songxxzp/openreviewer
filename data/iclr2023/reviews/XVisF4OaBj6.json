[
    {
        "id": "UvX9OhRGNa",
        "original": null,
        "number": 1,
        "cdate": 1666552401927,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666552401927,
        "tmdate": 1666552401927,
        "tddate": null,
        "forum": "XVisF4OaBj6",
        "replyto": "XVisF4OaBj6",
        "invitation": "ICLR.cc/2023/Conference/Paper1094/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper propose a neural network quantization method by mapping the mixed-precision layout problem into a traditional NP-hard problem, and solve the problem by some efficient implementation of relaxed solution to these NP-hard problem. ",
            "strength_and_weaknesses": "Weakness:\n\n\n- The paper is not well-written. \n\n- The empirical study is casual.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is difficult to track and does not follow the standard of academic writing. For example:\n- In the introduction, there is no citation to justify the statements. \n- Section 3 is cumbersome: for ICLR audience, these background should not take more than half-page.    \n\nThe empirical study is casual. There is a lack of presentation of the experimental setting, the central hypothesis, and the justification of the selection of the baseline (there are many works in neural network quantization, a more exhausted comparison should be included). Additionally, some claimed contribution from the introduction is not verified by the experiments, e.g., based on the description, it is unclear to me why the proposed method \"has clear interpretability\", what does interpretability refer to? \n\nThere is a lack of code release for reproducibility or a plan to do so.  ",
            "summary_of_the_review": "This paper is difficult to track, and the claimed contribution is not supported by the experiments.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_nDrD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_nDrD"
        ]
    },
    {
        "id": "nBBmQicXFn",
        "original": null,
        "number": 2,
        "cdate": 1666628811032,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628811032,
        "tmdate": 1666683050096,
        "tddate": null,
        "forum": "XVisF4OaBj6",
        "replyto": "XVisF4OaBj6",
        "invitation": "ICLR.cc/2023/Conference/Paper1094/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper lacks clarity, so I reproduce here my best understanding of its contents.\n\nThe paper proposes to search for the optimum bit width (or more generally, numerical precision) for each layer in a\nneural network during post-training quantization. To this end, the authors assume quantization induces an additive\nprice model -- <i>i.e.</i> each layer's quantization incurs an accuracy cost with respect to the full precision version,\nindependent on the quantization parameters of the other layers -- and an additive cost model -- i.e. layer computational\ncosts are independent on each other. Under these assumptions, finding the optimal bit widths can be cast as a 0-1\nknapsack problem. The price for quantizing each layer is estimated via a first-order linear approximation of the loss\nfunction with respect to the quantization-induced noise. The only experiment the authors report is the classification\nloss (not clear which loss is being reported) for ReseNet8 on CIFAR-10.\n",
            "strength_and_weaknesses": "* Strengths:\n\n  * In cases where the independence assumptions hold, the proposed algorithm should be computationally efficient. \n\n* Weaknesses:\n\n  * The paper lacks clarity and notational rigour (please see notes below on clarity).\n  * The assumption of additive accuracy price and computational cost model may not hold in practice and the authors\n  do not conduct a study on this. The total accuracy degradation is likely not a linear function of layer-wise accuracy\n  losses, as layers interact via feature maps. Furthermore, if one considers running time as computational cost,\n  inter-layer interactions arise from cache locality and bandwidth effects, leading to non-additive models.\n  * The authors simply report the loss (not clear what loss is being reported) and compare against a single baseline,\n  on a single (and not very challenging) classification dataset with two network architectures. This is hardly\n  convincing. No ablation studies are shown and the additive model hypothesis is not validated.",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity:\n\n  * I found the paper extremely hard to follow, as English is poor and lacks a top-down logical flow. Examples:\n    * \"we will show that how to mixed use round up/round down\" (page 3)\n    * \"to describe the conclusion for the sequential neural network is easily described\" (page 4)\n    * \"because of the natural of quantization is the trade of the performance and computation resource use\" (page 4)\n    * \"equation 7 is purity mathematical concept\" (page 6)\n  * The authors do not clearly state their contribution.\n  * Mathematical notation and rigour are lacking:\n    * Equation (5): this is a linear approximation (so the equal sign should be replaced by an approximation sign).\n    * Equation (6):\n      * $E$ is not defined, nor is $e$\n      * $l$ is the loss for training example $j$, it should take $(x_j, y_j)$ as arguments\n    * Equation (8):\n      * What is the minimum taken over?\n      * Is there a separate optimization problem here for each j? Or is j a function of i, that is, for each $j$\n      we look for a different $i$?\n      * Algorithm 1: Whenever a layer is quantized, it is not clear whether the previous layers remain full precision\n      or not. I would assume yes, but this is not specified.\n  * Some equations/formulas do not look correct to me:\n    * Equation (6): Assuming $e$ consists of a vector of $\\epsilon_i$ and $\\delta_i$ and $E$ is \n    the entire real domain (hard to tell, as $E$ is undefined), then the objective on the left\n    hand side is linear in $e$ and unbounded in general (one can set $\\epsilon_i$ to be arbitrarily large in absolute\n    value and to have sign opposite to the sign of\n    $\\sum_{i=1}^{n} \\sum_{(x_j, y_j) \\in \\mathbb{D}} \\frac{\\partial l}{\\partial h_{i+1}}$\n    and a similar expression exists for $\\delta_i$).\n    * Algorithm 1: The slopes on lines 3 and 5 are taken in absolute value. If the validation loss decreases (which can\n    sometimes happen in practice as quantization can act as a regularizer), there will still be a positive price\n    incurred on line 9.\n    * Equation (7): What variable is the minimum taken over ($w$, $\\epsilon$)?\n    * Why is there an \"or\" in equation (7)? Are these two alternative minimization problems?\n  * The authors mention \"What is more, in the model, which consists mostly of identity mapping [...]\". What do the\n  authors refer to by \"mostly\"? Is this a reference to the residual blocks in ResNet?\n    \n* Novelty:\n\n  * It is not clear what the contribution of this paper is. It mostly seems to propose an additive cost model, while the\n  rest of the algorithm simply relies on ACIQ and a linear approximation of the loss. It is fairly trivial, and no\n  studies are carried out to show that this assumption holds in practice. Hence, I believe there little technical\n  novelty.\n\n* Quality:\n\n  * Given the very limited experimental scope and that no validation of the additive model and/or linear\n  approximation hypotheses is considered, I do not think this work does not meet the scientific quality standards for\n  publication. \n\n* Reproducibility:\n\n  * Since there are so many unclear points with respect to the method, reproducibility is hardly possible, in my\n  opinion. The authors also do not mention the intention to release any public code. ",
            "summary_of_the_review": "This work does not seem to be technically novel. The presentation style and mathematical rigour needs to be\ngreatly improved. To me, there are many unclear points and questions regarding correctness. Empirical support for the\nmethod and hypotheses is lacking (and I would expect that the additive model does not hold in practice). Overall,\nI do not think this work meets the minimal standards for publication at ICLR. \n\n",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_hSyo"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_hSyo"
        ]
    },
    {
        "id": "C986ZR3nOs",
        "original": null,
        "number": 3,
        "cdate": 1666654791951,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654791951,
        "tmdate": 1666654791951,
        "tddate": null,
        "forum": "XVisF4OaBj6",
        "replyto": "XVisF4OaBj6",
        "invitation": "ICLR.cc/2023/Conference/Paper1094/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In  this paper,  the mixed-precision layer problem is formulated as a traditional NP hard problem and the problem can be solved by low cost methods without fine-tuning. The experimental results show that the proposed method is better than the current SOTA method.\n",
            "strength_and_weaknesses": "Strengths:\n1. Based on the total differential calculation, the paper proposed a theoretical framework incorporating bot the quantization error and data type conversion error. \n2. The paper also proposed the secant line method which fixed the large approximation error issue.\nWeaknesses:\n1. The method mainly focuses on the models which contain less residual structure. However, most of the popular models contain the residual structure such as transformer et al. \n2. The experimental results are not enough to show the strengths of the proposed framework: it only test simple architectures. It will be much better to show more results on more models, more baselines and more statistics such as accuracy and so on.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper proposed a novel framework to solve the mixed-precision inference quantization problem, it\u2019s clear and well written.",
            "summary_of_the_review": "To solve the challenging mixed-precision inference quantization issue, this paper proposed a theoretical framework to convert the original problem into a traditional NP hard problem, the derivation and writing is clear. However, the experimental results are limited to show the benefits on popular neural networks.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_y7Tw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1094/Reviewer_y7Tw"
        ]
    }
]