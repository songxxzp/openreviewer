[
    {
        "id": "4vys8zDn9y",
        "original": null,
        "number": 1,
        "cdate": 1666316422292,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666316422292,
        "tmdate": 1666675499875,
        "tddate": null,
        "forum": "iP77_axu0h3",
        "replyto": "iP77_axu0h3",
        "invitation": "ICLR.cc/2023/Conference/Paper2195/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper the authors propose to use energy-based model to improve the performance of Class-incremental learning. They try to make their model to be bi-directional compatible, i.e. forward compatible, which aims to enable old modules to sensitively capture the input distribution shift and backward compatible, which aims to enable the discrimination ability of old modules unaffected by new ones.Given data from new classes, a new energy-based model is first trained with a designed gradient equivalent objective function to avoid the intractable normalizing constant during the energy-based model expansion phase. And after that the new energy model is fused with the previous models with a designed fusing function. Through experimental results on CIFAR-100 and ImageNet-100, the authors demonstrate that their model work better than the baselines. ",
            "strength_and_weaknesses": "Strength:\n1. I think the idea that applies the energy based model (EBM) to class incremental learning (CIL) is interesting;\n2. The designs of the energy expansion objective and energy fusion function is meaningful;\n3. The experimental results seem to support their claim that the new proposed framework works better.\n\nPossible concerns:\n1. In the training objective (Eq,7), the constant $\\mu_{\\theta}$ and $\\lambda_{\\theta}$ are actually still intractable and related to the normalizing constant of EBM. And the way the authors solve this problem is simple treating it as a hyper-parameter to tune. Theoretically this may be incorrect. And in practice, introducing a new hyperparameter may mean that the performance will depends on the setting/tuning of this new hyperparameter. If we look at Figure 6 b, we can see that the setting of $\\lambda$ do influence the performance.\n\n2. Also there are other parameters like $\\alpha$ and $\\beta$ that need to be tuned, too. The authors tune these parameters on a tiny  sub-dataset. I would like to know how sensitive the performance is to these two parameters settings. Also, whether tuning on a tiny sub-dataset is feasible in practice and can give accurate enough results.\n\n3. For the energy-based model alignment part, the authors mention that they use Adam to accumulate gradient. Then I'm wondering whether they still include the noise term $\\omega_i$ in eq 13 or just do gradient desent or ascent. And how would the aligned images looks like comparing to the original images.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is in general well writen and easy to follow, while I will also suggest the authors to give more explanation for their figure 2 (Learning energy manifold and test-time energy alignment) \n\nBesides, for the learning of EBM, I think many important works of learning EBM as a generative model has not been included here, these include:\n\n[1] A Theory of Generative ConvNet ICML 2016\n\n[2] On learning non-convergent non-persistent short-run MCMC toward energy-based model. NeurIPS, 2019\n\n[3] Cooperative learning of descriptor and generator networks. PAMI, 2020\n\n[4] VAEBM: A symbiosis between variational autoencoders and energy-based models. ICLR, 2021 \n\n[5] Learning Energy-Based Generative Models via Coarse-to-Fine Expanding and Sampling. ICLR, 2021\n\n[6] Learning energy-based model with variational autoencoder as amortized sampler. AAAI 2021\n\n[7] A Tale of Two Flows: Cooperative Learning of Langevin Flow and Normalizing Flow Toward Energy-Based Model. ICLR, 2022\n",
            "summary_of_the_review": "In summary, I like the idea proposed in this paper that applies the energy-based model theory to the Class-Incremental Learning task. And the results seems to support their claims, while on the other hand, I also have some concerns that I may want authors to further clarify. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_zPVs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_zPVs"
        ]
    },
    {
        "id": "vtdnOhfOvev",
        "original": null,
        "number": 2,
        "cdate": 1666483878791,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666483878791,
        "tmdate": 1666483878791,
        "tddate": null,
        "forum": "iP77_axu0h3",
        "replyto": "iP77_axu0h3",
        "invitation": "ICLR.cc/2023/Conference/Paper2195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied the problem in dynamic-structure-based CIL methods, namely that coupled training of modules in different incremental steps leads to additional training costs and spoilage of eventual predictions. The authors proposed a unifying energy-based theory and framework called 3EF to train independent modules in a decoupled manner at each incremental step, and then fused the modules into a unifying classifier. Experimental results suggest that the proposed approach outperforms on the CIFAR100, ImageNet100, and ImageNet1000 benchmarks.",
            "strength_and_weaknesses": "Strength:\nThis paper proposed to train a specific module for new samples and then fuse it with the prior model. Based on theoretical inspiration from the energy-based model, the model performance is further improved.\n\nWeaknesses:\n1. This paper proposes a method so-called \u2018Efficient\u2019, where training new models do not require the forward propagation of old models for knowledge distillation. However, SGLD in EBM indeed increased the computation. Different modules are required in different steps, resulting in more parameters amount. Some quantitative analysis may be more helpful to understand. (eg. parameters amount)\n2. Comparison with recent SOTA methods including RMM [1], and Foster [2] are missing. I suspect that maybe the paper cannot achieve true state-of-the-art performance. Eg. in Foster CIFAR100 B0 10 steps: 72.90. But this paper achieved 71.94.\n3. Some detailed accuracy plots at each incremental step on magnet benchmarks should be presented in this paper to show the effectiveness of the method.\n4. This paper needs to be highly polished. Eg. section C.4 \u2018. Our method can is a bi-directional compatible method,\u2019\n\n\n[1] RMM: Reinforced Memory Management for Class-Incremental Learning\n[2] FOSTER: Feature Boosting and Compression for Class-Incremental Learning",
            "clarity,_quality,_novelty_and_reproducibility": "Fair: The paper is somewhat clear, but some important details are missing or unclear.",
            "summary_of_the_review": "The proposed method is somewhat novel and applying the energy-based model to CIL community is not new. On the other hand, I have doubts about the experimental results. My initial evaluation of this paper is \u201cweak reject\u201d.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_so2y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_so2y"
        ]
    },
    {
        "id": "nakyRAVn5K",
        "original": null,
        "number": 3,
        "cdate": 1666611514162,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666611514162,
        "tmdate": 1670489387642,
        "tddate": null,
        "forum": "iP77_axu0h3",
        "replyto": "iP77_axu0h3",
        "invitation": "ICLR.cc/2023/Conference/Paper2195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to learn dynamic-structure-based CIL models in a decoupled manner. It learns independent models for different tasks and then fuses them at a low cost to make phase-wise predictions. The idea is simple and seems effective compared to simple baseline methods such as ICARL and PODNet, but can not achieve the top performance compared to the recent work FOSTER (the citation is missing from this ICLR submission)---which is a more effective version of DER.",
            "strength_and_weaknesses": "Strength******\n\n1. The paper is clearly written and easy to follow. Though there are many formulations of entropies, the interpretation of each formula is quite clear and straightforward. It is easy to follow.\n2. Implementation details are well-presented in the paper. There is a compressive analysis of the model's sensitivity to hyperparameters.\n3. The model fusion and alignment modules are interesting and novel.\n\nWeaknesses*******\n\nThe main idea of this paper, \"using independent models for different learning phases\", is often taken as a baseline. See the \"independent\" setting in table 3 of this paper [Gradient Episodic Memory for Continual Learning, NIPS 2017]. It is straightforward and not fair to the models using a single model.\n\nThough this submission designs a novel module of model fusion and alignment based on entropy, its poor performance is still a concern, given it has used so many copies of backbone networks (i.e., a much higher number of network parameters compared to the related methods using a single model). For example, its performances on ImageNet-based datasets (all settings in ImageNet-100 and -1000) are consistently lower than those of FOSTER [1] -- where FOSTER is a more efficient model than DER, in terms of computational speed and parameter quantity.\n\n[1] FOSTER: Feature Boosting and Compression for Class-Incremental Learning. ECCV 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of this paper is high in terms of writing. Clarity is high.\nNovelty is poor.\nReproducibility is uncertain as no code is given.",
            "summary_of_the_review": "My main concerns are two-fold. 1) The unfair comparison between this paper and related works of using a single model for CIL. 2) The poor performance compared to an unmentioned sota method FOSTER.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NO",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_2BsG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_2BsG"
        ]
    },
    {
        "id": "j6IV27w5DW",
        "original": null,
        "number": 4,
        "cdate": 1666631727436,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631727436,
        "tmdate": 1670487822293,
        "tddate": null,
        "forum": "iP77_axu0h3",
        "replyto": "iP77_axu0h3",
        "invitation": "ICLR.cc/2023/Conference/Paper2195/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper, the authors propose a new CIL method, which trains independent modules in a decoupled manner and integrates the modules into a unifying classifier. They provide an energy-based explanation for the proposed method with theoretical analyses. Extensive experiment results on CIFAR-100, ImageNet-100, and ImageNet-1000 are provided. ",
            "strength_and_weaknesses": "### Strengths\n\n- Extensive experiment results are provided.\n- This paper is well-organized and easy to follow.\n- The proposed method is technically sound\n\n### Weaknesses\n\n- The memory usage of the model parameters might violate the class-incremental learning benchmark protocol. The authors propose to train independent modules and then integrate them. However, each module requires a memory budget to save them. We can also use the same memory budget to save more exemplars to improve performance. Based on this, the authors should provide ablation results showing saving the independent modules is more efficient compared to saving the exemplars. \n\n- There is no analysis of memory usage. As the independent modules also need to use the memory, it is important to provide the memory usage in Tables 1 and 2.\n\n- The authors claimed that their method aims to address the \u201ctraining cost\u201d issue in CIL. However, there is no analysis of the training cost. The authors need to provide a comparison of the training cost to show their method performs better than other baselines. \n\n- There is no open-source code. The authors didn\u2019t provide the implementation in the submission. It is important to provide the following researchers with enough materials to reproduce the results. \n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity & Quality\n\nThe paper is well-written and easy to follow. \n\n### Novelty \n\nThe energy-based method for CIL seems interesting. However, we still need to validate that the proposed method doesn\u2019t violate the memory usage protocol in CIL.\n\n### Reproducibility\n\nThe authors didn\u2019t provide the open-source code. \n",
            "summary_of_the_review": "Overall, this is an interesting paper. However, there is no open-source code. Besides, the analyses of memory usage and training cost are not provided. As these two points are very essential to the final contributions, the overall score is borderline reject.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_hrbR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_hrbR"
        ]
    },
    {
        "id": "X2njUKaGhR",
        "original": null,
        "number": 5,
        "cdate": 1666840549675,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840549675,
        "tmdate": 1666840598665,
        "tddate": null,
        "forum": "iP77_axu0h3",
        "replyto": "iP77_axu0h3",
        "invitation": "ICLR.cc/2023/Conference/Paper2195/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this work authors propose a novel dynamic-structure-based method for class incremental learning from the perspective of energy-based models. The proposed Efficient Energy-based Expanasion and Fusion (3EF) framework introduces the concept of bi-directional compatibility and decouples the training of different modules with a expansion and fusion two-stage training strategy. In addition, authors also show less requirement on the selection of  exemplar-set for rehearsal. Extensive experiments on three popular benchmarks show that the proposed method performs favorately against state-of-the-art methods.",
            "strength_and_weaknesses": "Pros:\n1. This work is well-motivated by the training efficiency of dynamic-structure-based methods and possible conflict among modules in addressing novel classes.\n2. A novel method from the perspective of energy-based methods is proposed. The proposed energy-based framework consists of two stages, i.e., expansion phase where new modules are trained to satisfy both backward and forward compatibility and fusion phase where a unified classifier for all seen categories is obtained.\n3. The proposed method also improves robustness to the imbalance or lack of some categories in exemplar-set. \n4. This work is well-written and the structure of the paper is well-organized. The symbols, equations and their descriptions are clear and easy to follow.\n5. The experiment setting is clearly explained. Extensive experiments, including both comparison with SOTAs and ablation studies, demonstrate the effectiveness of the proposed method.\n\n\nCons:\n1. In this work only one dynamic-structure-based method is compared with. More strong dynamic-structure-based methods with rehearsal are suggested to compare with.\n2. Performance of applying the proposed training strategy to more existing models.\n3. 'For benchmark ImageNet-100 ...' in Sec. 4.1 are suggested to put in another paragraph for clarity.",
            "clarity,_quality,_novelty_and_reproducibility": "This work is well motivated and well written.",
            "summary_of_the_review": "Overall, I think this work is well prepared and would inspire and advance the development of increamental learning field.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_6aJg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2195/Reviewer_6aJg"
        ]
    }
]