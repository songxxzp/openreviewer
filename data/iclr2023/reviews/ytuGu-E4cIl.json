[
    {
        "id": "8Uw-tq8926F",
        "original": null,
        "number": 1,
        "cdate": 1666832370537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666832370537,
        "tmdate": 1670600928667,
        "tddate": null,
        "forum": "ytuGu-E4cIl",
        "replyto": "ytuGu-E4cIl",
        "invitation": "ICLR.cc/2023/Conference/Paper4142/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the continual meta learning, widely used in low-resource setting.  Compared with the existing works, this work is motivated by two points. One is existing works assume the number of components of meta knowledge is mutually exclusive. Two, existing works usually only use a prior determined by Chinese Restaurant Processs, but do not make a posterior decision on number of components. To address the two issues, this method proposes an IBP prior to determine whether to increase the number of components, then leverages evidential theory to filter out the uninformative components.  \n\n\n--- post-rebuttal ---\\\nI have read the authors' response and other reviewers' comments. I think the current form of this paper is not ready for ICLR publication.",
            "strength_and_weaknesses": "Strength \n\n-Few shot continual learning is a hot research topic, the authors successfully identify two weaknesses of the existing works, and proposed a principal solution to solve them. \n\n-Evidential theory is a well suited equipment to solve the mutual exclusiveness among mixture components.  \n\n-Detailed theoretical derivation is provided.  \n\n \n\nWeakness \n\n-I have a little bit of hard time following this paper. For instance, the title highlights \u201cscalable\u201d and \u201cmulti-modal\u201d. However, I do see much description on these two terms, except in title and abstract. Not sure the meaning of \u201cmultimodal\u201d. \u201cScalable\u201d I guess it refers to the ability of filtering out the components. However, the current experiments do not well support this claim. \n\nAnother part is the technical description in evidential theory. I am not clear how to map the concepts in evidential theory to continual meta learning problem without explanation. This prevents me from understanding the technical details intuitively. Also, some terms in evidential theory lacks definition, e.g. focal set in page 5. \n\n \n\n-In page 1, the authors use user profiling as an example to explain the unreasonable assumption on mutual exclusiveness of meta knowledge components. However, the chosen experimental datasets don\u2019t reflect this to my understanding. Please explain (or better qualitatively illustrate) how the proposed method addresses the mutual exclusiveness on these test data. \n\n \n\n-The experimental results of VC-BML in Table 2 seems lower than the reported numbers in the original paper. Please clarify if there is any difference in experimental setting.  \n\n \n\n-Regarding the scalable claim, I would recommend conducting longer sequence of tasks/datasets to show the value of the proposed m method. The current reported results show a marginal improvement over existing methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "I think the paper has a lot of room for improvement in terms of presentation. The idea of applying evidential learning to choose meta-knowledge components is interesting. There are some concerns regarding experiments.",
            "summary_of_the_review": "Overall I think the idea of applying evidential theory to address the inappropriate mutual exclusiveness of mixture meta knowledge components is interesting. While unfortunately there are several concerns on the presentation and experiments, that needs to be addressed and clarified.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_MeHS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_MeHS"
        ]
    },
    {
        "id": "MUVKYTqtte",
        "original": null,
        "number": 2,
        "cdate": 1666935629149,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666935629149,
        "tmdate": 1666935629149,
        "tddate": null,
        "forum": "ytuGu-E4cIl",
        "replyto": "ytuGu-E4cIl",
        "invitation": "ICLR.cc/2023/Conference/Paper4142/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a scalable multi-model continual meta-learning algorithm. This method associates a cluster of similar tasks with a set of meta-knowledge components instead of one single component in previous approaches. If I understand correctly, \u201cmulti-modal\u201d in the title is due to a set of components being used. The authors proposed to use Indian buffet process to determine whether to add new components into the mixture when learning for a new task and leverage evidential theory to remove redundant components from the mixture. They evaluated their methods using four benchmark datasets. ",
            "strength_and_weaknesses": "Strength\n+ The idea is innovative and may be of value\n+ Based on the reported empirical results, the proposed method seems working. \n\nWeakness\n- The paper is poorly written, very difficult to follow. Especially in the mathematic derivation, the overloading and inconsistency choice of notations cause a lot of confusion. For example, in 3.2, what exact A is? Is it an element in Z or a set? The definition of the plausibility of A in Eq. (5) is confusing. In the paragraph following Eq. (7), what does exact k represent, a cluster or component? Awkward sentences and poor choices of worlding show up in so many places. \n- If I understand correctly, the posterior calculated to determine whether to remove components is only conditioned on the data at current time point as one has no access to previous data in a continual learning setting. If this is the case, the removal of components could easily lead to catastrophic forgetting.  \n- There is lack of clarity in the description of the setting of their experiment. Specially, at what point, the performance on a task was evaluated? Was it right after the training for that task was completed or after the training for all tasks were done. If the former, catastrophic forgetting was not properly evaluated. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Poorly written. The idea is innovative. The reproducibility is questionable given the lack of clarity and no code is provided. The authors did not mention whether they will ever provide code when the paper is published.",
            "summary_of_the_review": "The proposed idea may be interesting. However, poor presentation really affects a thorough understanding of the paper.     ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_XxCX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_XxCX"
        ]
    },
    {
        "id": "N_ztYvt0EGw",
        "original": null,
        "number": 3,
        "cdate": 1667459494733,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667459494733,
        "tmdate": 1667459494733,
        "tddate": null,
        "forum": "ytuGu-E4cIl",
        "replyto": "ytuGu-E4cIl",
        "invitation": "ICLR.cc/2023/Conference/Paper4142/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a continual meta-learning framework, Scalable Multi-Modal Continual Meta-Learning. Specifically, the authors employ the Indian Buffet Process for sharing meta-knowledge across different tasks and encourage evidential sparsity for parameter efficiency. The experiments validate the proposed method under the online non-stationary setting.",
            "strength_and_weaknesses": "Strength\n\n1. The paper aims at a very interesting and important topic.\n2. The use of IBP prior to handle the update of the number of mixture components looks novel.\n\nWeaknesses\n\n1. The writing can be improved. For example, it would help if the authors can elaborate on why the proposed method can allow the sharing of meta-knowledge and why previous methods cannot (as this is one major contribution summarized by the authors). The term \"multi-modal\" throughout the paper also seems obscure to me. Plus, by \"multi-modal knowledge\" I guess the authors do not want to mean something like knowledge from image and text but want to mean something relating to informativeness as in [1]. It would help if the authors can make this point more clear.\n2. I am not sure about the novelty of section 4.3. In section 4.3, many equations (Eqs 14-17) are very similar to section 2.2 of [1] without explicitly citing [1]. However, I am not an expert in this field. I would like to bring it to other reviewers' attention and would defer to their comments.\n\n\n[1] Masha Itkina, Boris Ivanovic, Ransalu Senanayake, Mykel J. Kochenderfer, and Marco Pavone. Evidential sparsification of multimodal latent spaces in conditional variational autoencoders. In NeurIPS 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of the paper can be improved. The IBP part looks novel to me but I am not sure about the novelty of the evidential sparsity part.",
            "summary_of_the_review": "Given the strength and weaknesses, I tend to rate this paper as marginally below the acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_yxSH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4142/Reviewer_yxSH"
        ]
    }
]