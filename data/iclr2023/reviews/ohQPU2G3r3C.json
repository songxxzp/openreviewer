[
    {
        "id": "R4gepBbRIw",
        "original": null,
        "number": 1,
        "cdate": 1666446620399,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666446620399,
        "tmdate": 1669702273374,
        "tddate": null,
        "forum": "ohQPU2G3r3C",
        "replyto": "ohQPU2G3r3C",
        "invitation": "ICLR.cc/2023/Conference/Paper5621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes the graph condensation method for hyperparameter search. The proposed HCDC algorithm optimizes synthetic validation data as well as training data so that the obtained optimal hyperparameter on synthetic data shows good performance on the original validation set. They theoretically analyze previous approaches and verify existing methods can overfit to specific models and fail to generalize over other models. To overcome this issue, the paper proposes a new objective that matches hyperparameter gradients. The paper shows the effectiveness of their method on GNN convolutional filter search.  ",
            "strength_and_weaknesses": "**Strength**\n- The proposed idea matching the hyperparameter gradient is novel.\n- The paper presents the problems of the existing methods theoretically and experimentally.\n- The experimental results on GNN convolutional filter (Table 3) are promising. \n\n**Weakness**\n- **It is hard to follow the paper.** There are too many notations that are often not consistent. For example, (page 3) $\\theta^S(S,\\theta_0)$ and $\\theta^T(\\theta_0)$, (page 5) $\\lambda^S$ and $\\lambda^S(S)$. Why do the same notations have a different number of arguments? This makes me confused.   \nAlso, the paper considers various hyperparameters (network type, width, learning rate; on page 5), but their mathematical notations of the paper only cover a very limited class of hyperparameters: $f_{\\theta, \\lambda}:R^{n\\times n}\\times R^{n\\times d} \\rightarrow R^{n\\times K}$, where $\\lambda$ is a hyperparameter. Based on this, they define the loss function as $L(\\theta, \\lambda)$, but general hyperparameters cannot be the argument of the loss function. For example, the learning rate is the argument of the learning algorithm, not the loss function. This inconsistency between the claims and explanations reduces the readability of the paper.   \n- **Algorithm is not described in detail.**  The paper uses the existing hypergradient method but differentiating the hypergradient is not trivial (see eq. (IFT) on page 7). However, the authors do not elaborate on the process. It would be informative if the authors provide the complexity analysis of the algorithm and error analysis by hypergradient approximation.   \nAlso, the authors should summarize and present the final algorithm, which is currently absent. It is unclear how the synthetic training set and validation set are optimized. (Do they optimized sequentially?)\n- **Experimental analyses are insufficient.** They conduct a synthetic experiment, finding the best cross-validation fold, but it is hard to understand the meaning of this experiment. (HCDC optimizes the validation set, and it is unclear what is the evaluation metric). Rather than synthetic experiments, the experiments in realistic settings (e.g., other types of hyperparameters and other domains or tasks) will be much more informative.   \nAlso, there is no computational cost analysis of the condensation algorithm. Figure 4 compares the search process speed on synthetic and original data, but it is unfair because it does not consider the time for optimizing the synthetic set. \n\nFurther questions.\n- The proposed method requires calculating hypergradient on the original training set. What is the benefit of the proposed method over the method directly using the hypergradient to optimize the hyperparameter? \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the paper is good. They conduct theoretical analysis and provide motivation for their work. However, it is not easy to follow the paper due to the complex and inconsistent notations. Also, the algorithmic details are absent and it is hard to fully understand their algorithm in detail. I believe applying hypergradient to dataset condensation is novel.",
            "summary_of_the_review": "The paper proposes a novel approach on dataset condensation by matching hypergradient. There are some promising experimental results. However, the paper requires consistent and more concise notations for readability. Also, the final algorithm is not described in detail and the paper lacks some important experimental analysis, e.g., time complexity, optimization results of other hyperparameters, comparison to hyperparameter optimization by hypergradient. Due to these reasons, it is hard to understand their method fully. \n\n-------\n\n**Post rebuttal**   \n   \nAfter reading the authors rebuttals, I still have some major concerns. \n- In response 4, authors said their main contribution is to find a condensed dataset that preserves the validation performance ranking of architectures, and thus it is not directly comparable to hypergradient optimization methods. However, I still believe the paper should compare with the hypergradient optimization methods to demonstrate the effectiveness of the proposed approach. (the proposed method requires to compute the hypergradient with the original training set, so if the direct use of the hypergradient is more effective, then why should we use the proposed approach for NAS?\n- In response 3.2, authors said it takes about 531 seconds for condensation before NAS. However, this is much larger than the time spent for search in Figure 4 (140 seconds) and it is unclear whether the proposed method is effective. Authors should reflect this on Figure 4. \n- In response 1.2, authors mention that the focus of this paper is on the architecture hyperparameters, not general hyperparameters (learning rate). Then, the terms like HPO in the paper seems not appropriate, and I think the writing should be revised by only considering NAS, not HPO. \n\nFrom the reasons above, I maintain my score.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_RKbd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_RKbd"
        ]
    },
    {
        "id": "CkcbNifYDu",
        "original": null,
        "number": 2,
        "cdate": 1666684034878,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684034878,
        "tmdate": 1666684034878,
        "tddate": null,
        "forum": "ohQPU2G3r3C",
        "replyto": "ohQPU2G3r3C",
        "invitation": "ICLR.cc/2023/Conference/Paper5621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper addresses the problem of node classification in graphs,\nesp. via graph condensation, i.e., finding a smaller graph on which\nmodels can be learnt and perform well on the whole graph\nnevertheless. The authors show that previous approaches (Jin et al.\n2021) that match loss gradients do not work well across different\ngraph neural network (GNN) architectures. Therefore they\npropose to match validation loss hyperparameter gradients\ninstead. They apply a method from the literature (Lorrainne et al. 2020)\nto compute those gradients efficiently. In experiments they\nshow that their method better ranks hyperparameters w.r.t.\ntheir performance on the whole graph and outperforms\nexisting baselines on four different datasets by up to 1.9\n(absolute) %.\n",
            "strength_and_weaknesses": "s1. interesting problem setting: graph condensation.\ns2. clearly identified limitation in current work (overfitting hyperparameters)\n  and plausible approach to overcome it. \ns3. good experimental results.\n\nw1. differences in the experiments w.r.t. published work.\nw2. claim of training speedup not substantiated.\nw3. the extended hyperparameter search space \\tilde\\Lambda is not clear\n  to me.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and well structured. The problem\nis well explained, the proposed method is plausible, and\nexperiments look promising.\n\nSmaller points:\n1. I did not understand how the graph condensation works from reading\n  just the paper and had to resort to Jin et al. 2021. Maybe explaining\n  the training process at least in the appendix would make the paper more\n  self contained.\n\n2. Several typos should be fixed, e.g.,\n  - p.9 \"methods which does\"\n  - p.9 \"we random split\"\n",
            "summary_of_the_review": "I see three issues:\n\nw1. differences in the experiments w.r.t. published work.\n  In the experiments, esp. tab. 3, performances are compared\n  against the baseline model GCond (Jin et al. 2021), but there are\n  major differences to the published numbers in the baseline paper, e.g.\n  a. the dataset Flickr is dropped. Often different reduction ratios\n      are used. \n  b. the whole graph performance is different, e.g., for\n      ogbn-arxiv you have 73.2, they have 71.4.\n  c. the performance for the random baseline and for the baseline\n      GCond differ, e.g., for ogbn-arix 0.25% and 0.5% you report\n      70.5 and 71.1, they report 63.2 and 64.0.\n\n  Is there an experimental condition changed that explains these\n  differences? And if so, could you reproduce their results and\n  compare head-to-head also?\n\nw2. claim of training speedup not substantiated.\n\n  The authors argue that by dataset condensation they could speed\n  up training.\n  While no one doubts that training on smaller graphs will take less time\n  than training on larger graphs, to substantiate the claim that dataset\n  condensation speeds up the training process one would have to compare\n  the sum of the training time on the smaller graph and the time needed\n  for its condensation to the training time on the larger graph. Is there\n  really a speedup? And if so, how large is it?\n\nw3. The extended hyperparameter search space \\tilde\\Lambda is not clear\n  to me.\n\n  What exactly is it, as a space, \\R^p ? And how do you map\n  between its elements and the p many hyperparameters in a\n  differentiable manner? \n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_K3vn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_K3vn"
        ]
    },
    {
        "id": "bsgo6ROG2Vp",
        "original": null,
        "number": 3,
        "cdate": 1666840462360,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666840462360,
        "tmdate": 1669598991902,
        "tddate": null,
        "forum": "ohQPU2G3r3C",
        "replyto": "ohQPU2G3r3C",
        "invitation": "ICLR.cc/2023/Conference/Paper5621/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper solves a problem of hyperparameter search for a graph neural network using calibrated dataset condensation. The authors propose a novel hyperparameter-calibrated dataset condensation framework by matching hyperparameter gradients in synthetic validation data generation. Unlike the standard dataset condensation, the proposed method is more robust for an overfitting issue, in particular for graph structures. Finally, the experimental results are demonstrated to show the validity of the proposed method.",
            "strength_and_weaknesses": "Here the strengths and weaknesses of this work are described.\n\n## Strengths\n\nIt solves an interesting problem regarding graph neural networks in the perspective of hyperparameter optimization.\n\nThe arguments of this work is well-supported by thorough analyses and diverse evidences.\n\n## Weaknesses\n\nI think that presentation and writing can be improved more. The current version is okay, but some expressions need to be polished. In particular, there are too many terms with a dash \"-\", which makes sentences complicated. Also, a calligraphic \"C\", which stands for another convolution filter, is somewhat confusing with \"C\". I know that you can prefer your own expressions to write your paper, but these can be improved regardless of my suggestions. Moreover, I think \"test\", \"cos\", or any similar terms should be a regular font; please check them out carefully.",
            "clarity,_quality,_novelty_and_reproducibility": "## Questions\n\n* Why did you restrict a message passing algorithm in a graph neural network as a convolution operation? Is it necessary for developing your algorithm?\n\n* How did you implement your optimization strategy in practice?\n\n* For Proposition 3, does this statement support failed generalization across graph neural networks appropriately? If a graph becomes bigger, is not it naturally getting bigger? It implies that it only says the size of graph, instead of the lower bound of errors.",
            "summary_of_the_review": "Please check the above text boxes.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_ny5k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_ny5k"
        ]
    },
    {
        "id": "xlATmJ0wBT7",
        "original": null,
        "number": 4,
        "cdate": 1667120537004,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667120537004,
        "tmdate": 1667120537004,
        "tddate": null,
        "forum": "ohQPU2G3r3C",
        "replyto": "ohQPU2G3r3C",
        "invitation": "ICLR.cc/2023/Conference/Paper5621/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed a new graph condensation method (HCDC) for fast hyperparameter/architecture search by aligning the hyperparameter gradients. As in the paper, the proposed method is theoretically and experimentally proven to be effective on preserving the validation performance rankings of GNNs.",
            "strength_and_weaknesses": "Strength:\n\nThe task is novel and there are many potential applications for the use cases of the proposed method.\n\nWeakness:\n\nThe paper is hard to read and have many confusing parts:\n\n1) The paper is about hyperparameter search on GNN, but the experiment is on Cifar10. \n\n2) The proposed method seems be used for constructing validation data only. According to the paper, the training data condensation is using other graph condensation methods. This brings the question of why not using the proposed method for training graph condensation? and what if performing NAS on the distilled graph generated by other graph condensation methods to search for hyperparameters?\n\n3) The experiment result also seems confusing. For example, in Table 3, are the results on the distilled training graph as the ratio is on the n_train? to my understanding, the training graph is generated by other graph condensation method. Then how to choose the graph condensation method for training?\n\n\n\n\n ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is hard to read due to large amount of equations without much explanations. Also the description of the experiment setting seems confusing.\n\nThe proposed method seems novel.\n\n",
            "summary_of_the_review": "I think the task is interesting, and due to the readability of the paper, it is hard to justify the contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_C3Kq"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5621/Reviewer_C3Kq"
        ]
    }
]