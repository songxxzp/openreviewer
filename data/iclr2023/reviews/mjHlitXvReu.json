[
    {
        "id": "AT8MEFpW4Ob",
        "original": null,
        "number": 1,
        "cdate": 1666575069508,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666575069508,
        "tmdate": 1666575069508,
        "tddate": null,
        "forum": "mjHlitXvReu",
        "replyto": "mjHlitXvReu",
        "invitation": "ICLR.cc/2023/Conference/Paper818/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes a method to use weak supervision (image-caption pairs) for open-vocabulary object detection (OVOD). This is in fact a problem of weakly-supervised object detection (WSOD). In stead of using the max-score or max-region matching in the literature, this paper proposes to formulate the problem of object-noun matching as a problem of bipartite matching problem, which can be solved by Hungarian algorithm. The experiments on COCO and LVIS have shown that this simple method can have big improvements. ",
            "strength_and_weaknesses": "**Strength**\n\n+Since the object-language annotation is expensive, it is reasonable to use weak supervision to improve OVOD performances.\n\n+The use of bipartite matching is reasonable and simple.\n\n+The improvements on COCO and LVIS seem nontrivial.\n\n\n**Weaknesses**\n\n-Using weak supervision to improve OVOD performance is not new, e.g. in RegionCLIP, GLIP, X-DETR, Detic. But those works only show some minor improvements, unlike 5 points gain on LVIS in this paper. I don't see anything fundamentally different, so I am not sure why the gains in this paper are so big.\n\n-Using bipartite matching for WSOD is not new, as seen in [1]. However, this paper doesn't discuss it.\n\n-This paper only shows the results on a base backbone, e.g. ResNet-50. However, many works have already shown high OVOD performances using some large models, e.g. [2]. I am wondering if the gains of this paper can be generalized to large models. \n\n-It seems using weak data can easily get 4-5 points improvement, but I don't see the ablations studies of getting those improvements step-by-step\n\n[1] Omni-DETR: Omni-Supervised Object Detection with Transformers, CVPR 2022\n\n[2] Simple Open-Vocabulary Object Detection with Vision Transformers\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nThis paper is clear and reads easily. But I still have some questions.\n- The last three lines of paragraph \"multi-modal object detection\" is not accurate.\n\n- How to use image-text caption is unclear. Do you extract the global image features by global pooling? And on which layer? More details are needed.\n\n- I am confused about the experiments of Table 3. If you only use category names, do you still use captions? If so, how? And there are only 6750 nouns for CC-3M dataset? And how to interpret those numbers? More details are required.\n\n- What's the row of using nothing in Table 5?\n\n- Do you generate the pseudo labels online? Do you use a student-teacher framework as in many semi-supervised and weakly-supervised object detection papers?\n\n**Novelty**\n\nOverall, I think the novelty is small, given using weak supervision and bipartite matching are already in the literature.\n\n**Reproducibility**\n\nAlthough the proposed algorithm is conceptually very simple, I am concerned about if we can reproduce the same improvements, given the detailed ablations are missing. Also the paper doesn't mention the code will be released.",
            "summary_of_the_review": "The idea is simple and easy to understand, and it gives some big improvements. But the technical novelty is small, no results comparing with the state-of-the-art, and I am concerned about the reproducibility. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper818/Reviewer_cyAh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper818/Reviewer_cyAh"
        ]
    },
    {
        "id": "qSJmjF_u4h",
        "original": null,
        "number": 2,
        "cdate": 1666635881170,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635881170,
        "tmdate": 1671023555912,
        "tddate": null,
        "forum": "mjHlitXvReu",
        "replyto": "mjHlitXvReu",
        "invitation": "ICLR.cc/2023/Conference/Paper818/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work proposes a novel method for open-vocabulary object detection (OVOD), where a model is trained from two datasets, one containing bounding box annotations for a set of base categories, and another one containing only free-form text descriptions (captions) of imagesf. While prior work typically uses grounded annotations (words in captions are associated with bounding boxes) or distills information from image-level pre-trained vision and language models, this work proposes a loss function to directly leverage image-text pairs (without grounding supervision). The architecture of the detector is similar to existing ones, with the only exception that text embeddings are predicted instead of a fixed-size probability distribution over a known label space. The model trains from both detection data and image-text pairs. The losses are the same as in standard detectors when training from detection data. For image-text pairs, a similarity is computed between each noun in the caption and each predicted region (region proposals). Bipartite matching is then applied on the similarity matrix to find a matching between regions and nouns, which then serves as ground truth for a standard binary cross-entropy loss. Experiments on the open-vocabulary setting for both COCO and LVIS datasets show improvement over prior baselines, specifically for novel categories.",
            "strength_and_weaknesses": "Strengths:\n- The topic is interesting\n- The paper is well written, easy to follow, figures a good\n- The method is easy and simple, but effective\n- The experiments are mostly clear and the method performs well\n- Ablation studies are reasonable.\n\nWeaknesses:\n- The dataset generalization experiment in Section 4.6 from COCO to Pascal VOC is limited. The 20 Pascal VOC categories are a subset of the 80 COCO categories. The question is how many of the 20 categories are part of the 48 base categories. Still, there will likely only be a few novel categories.\n- Did you try a different text embedding architecture that was not jointly trained on images and text? Would this make a difference?\n- The related work and the baselines can be improved\n  - I think paper [A] (concurrent work) can be discussed in the related work section.\n  - The paper [B] is another recent example for OVOD that can be discussed and compared with. It uses pseudo labels and gets strong results on COCO, although in a slightly different setting.\n  - The reference \"Gu et al.\" is now an ICLR 2022 paper\n- I'm missing a more thorough discussion on the region-word alignment loss\n  - It would be good to explain in words how the loss deals with incomplete captions, i.e., when some objects captured in the image are not referred to in the text. Looking at the loss and assuming a perfect matching, it seems such regions would just be ignored by the loss.\n  - The nouns in other captions of the same mini-batch (W') may contain the matching (positive) word. How likely does this happen? And why does W' not contain other words of the same caption?\n  - How do you deal with plural descriptions in a text, e.g., the caption \"Two cats near a dog\". Again, I think the loss would just ignore one of the regions that contain a cat (assuming a good matching). Still, I think it would be valuable for the paper to discuss such situations in text.\n\nReferences:\n- [A] Scaling Open-Vocabulary Image Segmentation with Image-Level Labels. Golnaz et al. ECCV 2022\n- [B] Exploiting Unlabeled Data with Vision and Language Models for Object Detection. Shao et al. ECCV 2022",
            "clarity,_quality,_novelty_and_reproducibility": "- Overall, I think the paper is of high quality, clearly written, and novel. The method seems easy to reproduce, but the authors seem to release code upon acceptance.\n- There are few parts of the text that can be improved, though:\n  - The first paragraph on page 5 about the image-text loss is not clear to me. My assumption is that the whole caption is represented by one embedding vector and the whole image as one region vector, which constitutes another (correct) match. However, I think the text in this paragraph could also be misinterpreted as actually having some image-text pairs with ground truth region-word alignment (like in a semi-supervised setting), which I assume is not the case given the whole motivation of the work.\n  - Also the paragraph on \"Object vocabulary\" on page 5 is not clear to me, without having read the corresponding paragraph in the ablation study. It might help to use the notation introduced in Section 3.1.\n  - The one-to-one vs. one-to-many experiment needs more clarification. First, the intuition for doing this is missing. I assume it relates to my comment above on how to deal with plural nouns in the text. From the definition of the loss function in Eq. 2, I assume the relation here means one (word) to many (regions). As a suggestion for further investigation, would it help to identify if nouns are plural or singular, and then choose the appropriate constraints (alignment algorithms) for mapping to either one or multiple regions?\n  - I think it should be made more clear in the beginning the that the model trains from both detection and image-caption data. Otherwise, the reader may wonder how the model creates sensible regions in the first place.\n  - Typo on page 8 last paragraph: \"leaning\" -> \"learning\"",
            "summary_of_the_review": "Overall, I think this a high-quality paper with clear impact to the research community on open-vocabulary object detection.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper818/Reviewer_y363"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper818/Reviewer_y363"
        ]
    },
    {
        "id": "aR9JBjtAdh",
        "original": null,
        "number": 3,
        "cdate": 1666750702995,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666750702995,
        "tmdate": 1666750702995,
        "tddate": null,
        "forum": "mjHlitXvReu",
        "replyto": "mjHlitXvReu",
        "invitation": "ICLR.cc/2023/Conference/Paper818/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces a set-matching approach for open-vocabulary object detection by jointly training on detection and image caption dataset. The approach achieves good results on the existing COCO and LVIS open-vocabulary detection benchmark.",
            "strength_and_weaknesses": "Strengths:\n* The idea of joint training with caption data is promising as captions are noisier and more scalable than image tags\n* Set matching is an intuitive approach to handle region-word alignment\n* The label space of COCO/LVIS are not used in training phase which follows the open-vocabulary detection settings\n* Performance on COCO/LVIS are both strong and the margin over Detic baseline is clear\n* Ablation studies are informative.\n\nWeakness:\n* Method - what are the constraints for region-text matching? Are all regions assigned to words or are all words assigned to regions? I can imagine some words or regions are noisy and do not have good matches.\n* Results - the scalability of the proposed approach needs further study in the axis of data size (e.g. CC12M, larger web-scale image-text data) and model size (larger backbones). It'd be very useful if this approach scales well with data and model size. A simple study is to subsample CC3M and observe the scaling benefits.\n* Looks like the proposed approach compromises base categories compared to the Detic baseline. Table 5 image-text loss only has the same overall AP suggests this as well. Is there any explanation for this?\n* Comparison with DetPro is unfair because DetPro uses ViLD detector as baseline while this work uses Detic CenterNet as baseline. Detic papers report a ViLD detector baseline which is lower. A more fair comparison would be to report a Mask R-CNN detector (ViLD style) baseline on LVIS.\n* SOTA on LVIS claim is not well supported because this study focuses on R50 backbone only.\n* [minor] how much overhead does the set-matching add to the training time?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - the paper is written clearly and easy to follow.\nQuality - the quality is good overall with some weaknesses as indicated above.\nOriginality - novelty is good because the set-matching loss hasn\u2019t been shown effective in the joint training setup to my knowledge.\nReproducibility - the authors promise code release and provide some details of implementation.\n\n",
            "summary_of_the_review": "The paper proposes an interesting approach to tackle open-vocabulary detection by set matching loss on caption data. There are some weaknesses in the paper but I think the merits outweigh the downsides and am leaning positive for the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper818/Reviewer_W8ar"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper818/Reviewer_W8ar"
        ]
    },
    {
        "id": "OaPmDzWABjr",
        "original": null,
        "number": 4,
        "cdate": 1666846337946,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666846337946,
        "tmdate": 1669747720948,
        "tddate": null,
        "forum": "mjHlitXvReu",
        "replyto": "mjHlitXvReu",
        "invitation": "ICLR.cc/2023/Conference/Paper818/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes to directly learn from large-scale image-text pair data for open-vocabulary object detection. \nIt formulates object-language alignment as a set matching problem.\nThe benefit of such approach is to allowing training on image-text pairs without the help of grounding information.\nThe author conducts experiments on COCO and LVIS to demonstrate its performance on novel categories.",
            "strength_and_weaknesses": "Strength\n+ Performance is slightly better than previous methods\n\nWeakness\n- The paper lacks necessary mathematics proof on solving object language alignments learning. How does bipartite matching is optimized end-to-end during training? Hungarian matching is not differentiable. How does it guarantee convergence on learning?\n- It also lacks implementation details. For example, in table 4, how does one-to-one or one-to-all affect the matching? In table 5, how does image-text pair loss implement?\n- For Table 1, compared with Base-only, the proposed alignment loss downgrades the performance on supervised base categories. What is the reason?  \n- For Figure 3, the author avoids example with multiple same instances. What will happen when apply to an image \"a basket of oranges\"?  ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity can be improved, especially on the details of method.",
            "summary_of_the_review": "Overall, I feel solving object language alignment without grounding information is the correct way. However, this paper doesn't prove the effectiveness of using bipartite matching nor well explain the details. \n\nAfter reading the rebuttal, I still don't get enough details on the correctness of the proposed method. Meanwhile, it also raised me another concern on its ability of handling dense detection. Hence, I would not recommend to accpet this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper818/Reviewer_fcRy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper818/Reviewer_fcRy"
        ]
    }
]