[
    {
        "id": "ROritB7N89N",
        "original": null,
        "number": 1,
        "cdate": 1666651488652,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666651488652,
        "tmdate": 1668822903982,
        "tddate": null,
        "forum": "G2GpzH1l9AC",
        "replyto": "G2GpzH1l9AC",
        "invitation": "ICLR.cc/2023/Conference/Paper3236/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors of this paper examine the notion of representation redundancy in large networks, and more specifically how it is diffused across the neurons of the representation. The authors quantify the measure of representation redundancy they use and demonstrate that dropping a large amount of neurons from the last layer randomly only affects the downstream accuracy by a small amount. The authors evaluate their observation on pretrained models on ImageNet, and demonstrate the level of redundancy present in these pretrained representations.",
            "strength_and_weaknesses": "Strengths:\n\n- The paper poses an interesting question overall, in that the authors consider whether the learned representations have any particular structure in how information is redundant in them, and demonstrate that the representations produced by the network are significantly redundant.\n\n- The experimental portion of the paper demonstrates the claim that the authors make, regarding redundancy in the representations of the network. More specifically, it can be seen that in most cases, the downstream accuracy in a given task decays slowly, as the number of neurons in the representation is decreased. The authors also perform a large set of experiments to show what precisely influences diffused redundancy (varying upstream/downstream tasks, layer width and existence of adversarial training).\n\nWeaknesses:\n\n- While I agree with the core observation of the paper, I also don\u2019t find it particularly surprising. It is expected that a large part of the representation produced by the network is redundant, and can be removed without significantly harming the performance of the network. The main point the authors make is that this can be done by removing random neurons of the representation, which shows that this redundancy is spread across the entire representation. However, one can reasonably expect that, since the representation is still in high dimensions, the samples will still be well separated, with only a few neurons kept. To improve this, I believe that the authors should further elaborate on the implications of their observation in the introductory parts of the paper. \n\n- In the experimental section, a simple baseline one can compare against is keeping only the principal directions of the data, via PCA. While this is more complex than simply dropping neurons (since it requires a linear combination of the weights) I believe it would be useful to include this comparison as well.\n\n- When comparing downstream tasks, I think it would also be useful to include the evaluation that arises when the downstream task is the same as the upstream (for example, ImageNet-1K). I think this would be useful to demonstrate whether the observation still holds in a more complicated downstream task.\n\n- The authors include a section in their paper that touches upon issues of fairness that may arise by minimizing the size of the representations of the model. However, I think this part of the paper should be further extended. As it is, it seems to be somewhat disjoint from the rest of the paper. This can easily be an important topic for future work, so I think it should be highlighted further.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper is clear. The point that the authors want to make (that it is possible to drop a large fraction of neurons from the network and still retain downstream performance) is clear, and the paper overall is easy to read. As a minor comment, I would suggest the authors rearrange the figures in page 6, so that they don't cut in the text.\n\nRegarding originality, as stated above the authors make an interesting observation, which however is not that surprising in my opinion. I believe that further analysis on the implications of this observation is needed to fully support this work.\n\nFinally, the authors provide in the Appendix the necessary hyperparameters to reproduce their results.",
            "summary_of_the_review": "Overall, I think this paper makes an interesting observation, which however requires further analysis on its implications. Currently, I think that there needs to be a better understanding on how the observation made by the authors can be used. Elaborating on the points I mentioned above would help improve this paper.\n\nUpdate post rebuttal: I have raised my score after the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_tTBB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_tTBB"
        ]
    },
    {
        "id": "tBbOCKV8_5",
        "original": null,
        "number": 2,
        "cdate": 1666665626275,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665626275,
        "tmdate": 1670204108232,
        "tddate": null,
        "forum": "G2GpzH1l9AC",
        "replyto": "G2GpzH1l9AC",
        "invitation": "ICLR.cc/2023/Conference/Paper3236/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "# Intro / Section 2\n- This paper introduces the property of diffused redundancy: any randomly chosen subset of a layer can perform similarly to the full layer on downstream tasks.\n- The degree of diffuse redundancy is investigated for the final layers of image classification models trained on imagenet.\n- Two downstream tasks are used to measure diffused redundancy, 1) representational similarity (CKA) between the whole layer and random subsets of the layer, and 2) classification accuracy when using the whole or a random subset of the layer to train a linear classifier for a downstream dataset.\n- Redundancy is when a high performance can be accomplished with a small fraction of neurons.\n- Diffusivity is when the variability of performance is low for different random subsets of neurons.\n\n# Section 2.1 / Figure 1 \n- Representational similarity and downstream accuracy are plotted for different sizes and random subsets of the full layer. A comparison is done between four downstream datasets (cifar10/100, flowers, oxford-iit-pets), and two models (ResNet50 with and without adversarial training).\n- Degree of diffusion and redundancy depends on the downstream dataset.\n- The adversarially trained model demonstrates greater diffused redundancy.\n\n# Section 2.2 / Figure 2\n- Plots of representation similarity between randomly chosen subsets.\n\n# Section 3 / Figure 3\n- Downstream accuracy vs subset sizes is compared for a larger collection imagenet classification models.\n\n# Section 3.1 / Figure 4\n- Downstream accuracy vs subset size is compared for models trained on imagenet1k and imagenet21k.\n- The imagenet21k models demonstrate greater downstream accuracy.\n\n# Section 3.2 / Figure 5\n- Downstream accuracy vs subset size is compared for models with varying final layer sizes.\n- The size of the final layer is proportional to downstream accuracy.\n\n# Section 3.3 / Figure 6\n- Downstream accuracy vs subset size is compared for models that are trained with MRL (smaller parts of the representation have structure).\n- The MRL-optimized representation performs well for very low numbers of neurons, but is outperformed for larger subsets.\n\n# Section 4 / Figure 7\n- The loss in downstream accuracy when reducing the size of the random subset is primarily in a few classes instead of being uniformly distributed.\n-This suggests that exploiting diffused redundancy has implications for fairness.",
            "strength_and_weaknesses": "# Strengths\n- Demonstrates an interesting property of image classifiers.\n- Comparisons are done between many different models.\n- Potential drawback of creating a class imbalance is highlighted, although this somewhat contradicts the diffused redundancy hypothesis.\n\n# Weaknesses\n- The application to model optimization is limited since this technique only reduces the size of the final layer. There could be some comparison in memory usage, floating point operations, or model size to motivate DR as an optimization technique.\n- It is not clear if there is any use for the definition of diffused redundancy (DR) (equation 1), while the definition states that it will be used to rigorously test the DR hypothesis. Figure 1a (downstream accuracy) and 1b (diffused redundancy) appear to convey the same information. The rest of the paper does not use this definition.\n- Section 2.2 claims to explain why any random subset will work for downstream tasks. Figure 2 shows that two randomly chosen subsets have high representational similarity to each other. This further confirms that any random subset works, but does it explain why? After seeing that a random subset has a similarity to the full layer in figure 1, it is not surprising that random subsets are also similar to each other.\n- In figures 3, 4, and 6 I believe the full representations all have different sizes, but they are not displayed. I am wondering if it is more important to show the absolute number of neurons on the x-axis instead of the fraction.",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity / Quality\n- In Figure 1, it is not stated whether diffused redundancy is using CKA or downstream accuracy as the task. The acronym for adversarial training (AT) is never explicitly defined.\n- Needs some proofreading. For example: \"hence right more point\" was probably supposed to be \"hence, the rightmost point\".\n\n# Novelty\n- I am not familiar enough with the related works to evaluate the novelty.\n\n# Reproducibility\n- Code and implementation details are provided.",
            "summary_of_the_review": "Diffused redundancy is an interesting property of the final layer of image classifiers, but its application is not clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_JaiD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_JaiD"
        ]
    },
    {
        "id": "oGCIt0916r",
        "original": null,
        "number": 3,
        "cdate": 1666833690254,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666833690254,
        "tmdate": 1666833690254,
        "tddate": null,
        "forum": "G2GpzH1l9AC",
        "replyto": "G2GpzH1l9AC",
        "invitation": "ICLR.cc/2023/Conference/Paper3236/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Pretrained deep representations from large datasets have a plethora of applications across vision and natural language. Understanding how information is encoded across the dimensions in the representation is important to design better algorithms for the respective downstream tasks. \n\nThis paper discusses that the learned representations exhibit significant redundancy. That is, not all the dimensions are needed for getting a decent performance on the downstream tasks, and propose a notion of diffused redundancy. The paper explores the existence of diffused redundancy across the architectures, upstream and downstream tasks, and learning algorithms. Based on the experiments, the paper concludes that the entire layer is not necessary for the downstream task, however, providing caution on how smaller representations sizes may cause unfair decisions",
            "strength_and_weaknesses": "I will go sequentially with the strengths and weaknesses\n\nStrengths:\n1) The Paper is well-motivated, decently written, and easy to understand. \n2) Systematic experimentation is done to understand redundancy in the representation of 4 smaller-scale datasets using ImageNet pre-trained models\n3) Experimental findings are consistent across the downstream datasets, pre-training data, architecture, and learning algorithms (normal training, MRL, adversarial). \n4) The trade-off between representation size and fairness is very important (and also a good extension to MRL -- Kusupati et al., NeurIPS 2022)\n5) The choice of definition of diffused redundancy with the assumption of $\\delta=0.9$ is a good starting point.\n\n\nWeakness:\n1) The existence of redundancy is not novel, in fact, MRL has also discussed it in their appendix implicitly (diffused redundancy experiments with particular feature size) - however, concretizing it is a good first step taken by this paper\n2) Baseline with correlation regularizer should\u2019ve been discussed - \u201cReducing Overfitting in Deep Networks by Decorrelating Representations\u201d\n3) The paper should cite and discuss papers with structured pruning which are very relevant to the potential redundancy in the hidden layer.\n4) Figure 1 should also compare algorithms, that is CKA v/s fraction to better understand if some algorithms have less redundancy than others. \n5) Plain CKA numbers are a bit hard to understand. That is, for certain tasks, a CKA of 0.7 may be good, but for other, it may not be. Therefore, a scatter plot downstream task performance v/s CKA at different neuron fractions perhaps could be more informative. \n6) On Figure 4: When we are comparing the pre-training datasets, it is difficult to draw conclusions if the accuracy difference for full set of neurons differs quite a bit (which is the case in this figure)\n7) The figures could be made more easily readable, it is hard to go through the densely packed plots. \n8) Lastly, while the experiments are a good first step, the datasets often are easy -- I would like to see two sets of ablations a) with the variation of $\\delta$ from 0.8 to 0.99 and b) experiments on harder datasets like ImageNetV2 and Places365. This would make the paper extremely strong and would help any future papers built on top of it. \n\nI am very excited about this paper and want to have a conversation with the authors through rebuttal and revision. I want the authors to take the weakness in good spirit to help improve the paper to make it worth a strong publication\n",
            "clarity,_quality,_novelty_and_reproducibility": "See the above section. The paper is highly reproducible with code in the supplementary. ",
            "summary_of_the_review": "Good step towards quantifying the redundancy in the representations. Experiments on small scale datasets and some missing baselines. However, a strong effort towards something useful at scale. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_9A8r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3236/Reviewer_9A8r"
        ]
    }
]