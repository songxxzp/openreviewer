[
    {
        "id": "Sh3OrhvbLNP",
        "original": null,
        "number": 1,
        "cdate": 1666176997529,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666176997529,
        "tmdate": 1666176997529,
        "tddate": null,
        "forum": "gfr5yILQc7_",
        "replyto": "gfr5yILQc7_",
        "invitation": "ICLR.cc/2023/Conference/Paper1536/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed MQSP, an optimized sequence parallelism technique for long sequence transformer. Finally, the evaluation result shows that MQSP can scale to longer sequences. In long sequence scenarios, MQSP can significantly reduce memory occupation and have higher throughput compared to Tensor Parallelism and Sequence Parallelism.",
            "strength_and_weaknesses": "Strengths\n\nWith Micro-Query, MQSP scales up sequence length linearly and shows a pretty good performance improvement. Greatly increases the length of sequences that can be handled in Sequence Parallelism.\nMQSP has a better communication pattern and significantly improved performance compared to previous sequential parallelism techniques.\nThis paper provides a thorough theoretical analysis, while comparing other methods including the most advanced Tensor Parallelism and Sequence Parallelism systems from theory and experiment.\n\n\nWeakness - major concerns\n\nIn the evaluation, only Bert-Large was used for testing. Testing with more models with different sizes can make the results more convincing.\nIn Section 3.1 \u201cMICRO -QUERY SEQUENCE PARALLELISM - Comparison\u201d, a comparison is made with ColAISP only. If a comparison can be made with Tensor Parallelism, it can give the reader more information to choose between Tensor Parallelism and Sequence Parallelism\nIn Section 4.3 \u201cMEMORY FOOTPRINT\u201d, the paper shows the performance of MQSP with the activation checkpoint turned on and off, but does not specify whether the activation checkpoint is turned on in other implementations\nIt can be observed that within a sequence length of 7500, Megatron-LM3 has a clear performance advantage over MQSP, and there needs to be a more prominent expression in the paper to demonstrate why we need long sequences for training, and how long we really need them. One concern is that extra-long sequences can lead to excessive computational cost without actually bringing much gain effect.\n\n\nWeakness - minor concerns\n\nIn Section 4.1 \u201cQUALITY OF CONVERGENCE\u201d, the setting of sequence length for MQSP is missing.\nFor some readers who have not been exposed to much tensor parallelism, the background of this paper is slightly brief. Consider adding more detailed background on Tensor Parallelism.\nSome letter symbols are used in advance in the figure, such as \"m\" in figure one. Maybe highlight the meaning of these letters in the article or caption, it will be easier to understand the article.",
            "clarity,_quality,_novelty_and_reproducibility": "OK",
            "summary_of_the_review": "Tend to reject; Please check my comments above for more information.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_nVbE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_nVbE"
        ]
    },
    {
        "id": "lBoyLW_3dTd",
        "original": null,
        "number": 2,
        "cdate": 1667186804006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667186804006,
        "tmdate": 1667186965068,
        "tddate": null,
        "forum": "gfr5yILQc7_",
        "replyto": "gfr5yILQc7_",
        "invitation": "ICLR.cc/2023/Conference/Paper1536/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work studies long sequence problem from the system perspective. The self-attention part of a Transformer requires quadratic memory cost, limiting the applicability of full-attention for thousands of tokens. There are major two lines of directions for tacking with the high complexity due to long sequence: 1) full-attention approximation such as sparse attention, and 2) model parallelism approach. This work focuses on the 2nd line. The existing works such as Megatron and ColAISP reduce memory complexity by n times, where n is the tensor parallel size. This paper proposed to further reduce the memory cost by splitting the attention computation into m micro-steps, and in each micro-step, a micro-context output is computed. In this way, the memory buffer can be reused and the memory cost of attention map is reduced by nm times. The experimental results show that the proposed method, dubbed MQSP, supports training a BERT-large model with 78848 sequence length on 32 A100 GPUs.",
            "strength_and_weaknesses": "Strength:\n1. The empirical results are promising. Supporting the training with 78848 sequence length is impressive.\n\nWeakness:\n1. This paper lacks sufficient discussion and comparison with the related works. Similar idea of splitting sequence into smaller chunks and incrementally accumulating results has been proposed in (Dao, Tri, et al., 2022). This work investigates the bottleneck of Transformer training from IO perspective. The resultant algorithm fuses the whole Transformer layer into a fused kernel and the memory cost of computing attention map is constant independent of the sequence length. This algorithm has been implemented in multiple libraries including OpenAI triton and Xformers https://github.com/facebookresearch/xformers/blob/91c7c0846455d78b934d6705026b4c7e35b89bb1/xformers/ops.py#L280. We can easily combine it with the Megatron implementation.\n2. Distributed softmax is also not new. This has been used in Megatron for computing output logits: https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/mpu/cross_entropy.py\n\nDao, Tri, et al. \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\" arXiv preprint arXiv:2205.14135 (2022).",
            "clarity,_quality,_novelty_and_reproducibility": "Though the experiments look promising, the novelty is limited and the discussion and empirical comparison with the highly related work FlashAttention are not provided (details are provided in Weakness).\n\nThe paper is hard to follow. There is little description of Figures 3 and 4. I found the current main text loosely coupled with the figures, giving me some difficulty in comprehension. I encourage the authors to put more efforts in explaining the proposed algorithm in details especially for pages 4 and 5. Also, I suggest that authors need to ensure that all the symbols occurred in the paper have definitions and shapes such as $c_i^j$ and $C_i$.\n\nIt is not clear to me where 2BDL comes from for MEGATRON-LM3 in Table 1. By using the sequence parallelism introduced in (Korthikanti, V., et al., 2022), the memory costs for all the intermediate activations including dropout and layernorm can be reduced by n times.\n\nI didn't run the code, so I do not have any data for reproducibility.\n\nKorthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., & Catanzaro, B. (2022). Reducing Activation Recomputation in Large Transformer Models. arXiv preprint arXiv:2205.05198.\n\n",
            "summary_of_the_review": "The paper studies an important problem since long sequence has a wide variety of applications such as question-answering and summarization. The empirical results are also promising and interesting. However, I am concerned with the novelty. The paper is lack of discussion and empirical comparison with FlashAttention. FlashAttention adopts very similar technique and fuses the computation of micro-chunks into a CUDA kernel, and its memory cost depends on the GPU SRAM size and is independent of the sequence length.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_h8ct"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_h8ct"
        ]
    },
    {
        "id": "HNh5LeoIM_T",
        "original": null,
        "number": 3,
        "cdate": 1667202117402,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667202117402,
        "tmdate": 1667202117402,
        "tddate": null,
        "forum": "gfr5yILQc7_",
        "replyto": "gfr5yILQc7_",
        "invitation": "ICLR.cc/2023/Conference/Paper1536/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new way to further reduce the memory footage of the self-attention mechanism over long sequences by parallelization on the sequence dimension. The paper proposed to communicate the query tensors to handle the sentence-dimension computation used in self-attention. ",
            "strength_and_weaknesses": "The paper proposed a more fine-grained method to partition the query dimension to reduce the memory footprint of the self-attention layer. The proposed method is new. However, I have doubts on the actual effectiveness of the method. Specifically,\n\n1. The $O(L^2)$ memory usage for the attention operator can be reduced to $O(L)$ by changing the way it was implemented in GPU without introducing any communication overhead across different GPUs. See the following two references:\n    1. [https://arxiv.org/pdf/2205.14135.pdf](https://arxiv.org/pdf/2205.14135.pdf)\n    2. [https://arxiv.org/pdf/1911.02150.pdf](https://arxiv.org/pdf/1911.02150.pdf)\n    \n    Please compare with these works and show the effectiveness of the proposed method since the biggest benefit of the proposed does not exist any more with the optimized single-GPU kernels, and the proposed method brings much more extra communication.\n    \n2. There is no analytical analysis on the extra communication brought by the newly proposed algorithm, which is the major overhead brought by the proposed method.\n3. Why do you need to evaluate the convergence of the proposed method in 4.1? The method does not change the behavior of the original self-attention algorithm.",
            "clarity,_quality,_novelty_and_reproducibility": "Many figures in the paper is not illustrative. I eventually figure out the following questions by \n\n1. Figure 1: Which dimension corresponds to queries, and which dimension corresponds to keys?\n2. Figure 2: I suppose $Q_i, K_i, V_i$ are all $L/n\\times d_k$ sized matrices, but one cannot tell the $d_k$ dimension from the figure.\n3. Figure 3: The size of $Q_i, K_i, V_i$ in the figure does not reflect their actual shape (which should be exactly the same!). Also $Q_i$ and $Q^j$ are of the same shape, which should not be the case. \n4. Figure 3 caption: \u201cThe red dotted arrow lines represent the aligned row for distributed softmax.\u201d Do you mean the orange dotted arrow? What does the arrow direction mean?  \n\nThe paper should be able to be reproduced. The proposed method from the paper is new.",
            "summary_of_the_review": "I have doubts about the effectiveness of the proposed method. I will raise my score if my concerns are addressed during the rebuttal.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_zrYy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_zrYy"
        ]
    },
    {
        "id": "XXpQZ5ELaY",
        "original": null,
        "number": 4,
        "cdate": 1667281166086,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667281166086,
        "tmdate": 1667281166086,
        "tddate": null,
        "forum": "gfr5yILQc7_",
        "replyto": "gfr5yILQc7_",
        "invitation": "ICLR.cc/2023/Conference/Paper1536/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces a novel dimension to partition self-attention layers for distributed training of long-sequence transformers. The proposed technique, called Micro-Query Sequence Parallelism (MQSP), maintains only partial columns of attention map and applies a low-cost distributed softmax. To reuse memory over multiple rows of the attention map, MQSP also utilizies a finer-grained query, Micro-Q, to scale memory overhead linearly instead of quadratically. Evaluation demonstrates MQSP can scale sequence length significantly longer than both ColAISP and Megatron-LM3.\n",
            "strength_and_weaknesses": "**Strengths** \n\n* The sequence length of transformers is likely to keep increasing, and a scalable solution to the sequence length has values.\n* The proposed solution effectively increases the sequence length over 70K on 32 A100 GPUs.\n\n** Weaknesses **\n\n* It is not clear whether this technique introduces another bottleneck (especially with increasing $m$) for inter-GPU communication and synchronization.\n* The paper lacks comparison against other works leveraging different forms of parallelism (e.g., pipeline parallelism + tensor parallelism).\n* The necessity of scaling sequence length needs to be better motivated.\n ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is mostly written clearly and the writing quality is good. As for originality, this is not the first work to propose sequence parallelism demonstrates superior scalability over the prior art leveraging sequence parallelism (ColAISP, in particular).  The idea of distributed softmax seemingly has been explored elsewhere.  \n\n",
            "summary_of_the_review": "This paper focuses on improving scalability of multi-head self-attention block with an increasing sequence length. The self-attention block is known to be a scalability bottleneck due to it's quadratic overhead for both memory capacity and computation.  That said, this paper tackles a timely problem.  Evaluation demonstrates the sequence parallelism can handle longer sequences and is more memory space-efficient than both ColAISP (leveraging sequence parallelism in a different way) and Megatron (leveraging tensor parallelism).\n\nWhile the results show some promise, I still have the following questions/concerns about this proposal:\n\n* *Performance Sensitivity* - In Figure 9, there is a tradeoff between reduction in memory usage and token (training) throughput. This makes sense as $m$ increases, attention maps are partitioned into finer-grained chunks to increase the overhead of communication limiting overall throughput. In this particular setup of *2n16g*, the throughput is maintained at a high level before the benefits of memory savings saturate. However, this might not necessarily true depending on the hardware configuration (e.g., # of nodes, # of GPUs, GPU generations, communication bandwidth, etc.).  Would the proposed technique provide robust performance for other system configurations with different compute/communication ratios?  \n* *Comparison against Existing Work* - This work compares mostly against ColAISP, which also leverages sequence parallelism.  I was not aware of this work, and hence do not know how strong a baseline it is. Comparison against Megatron-LM3 alleviates the concerns but I am still wondering how MQSP compares against other existing works, especially leveraging multiple forms of parallelism (e.g., pipeline parallelism together with tensor parallelism) in terms of overall token throughput. \n* *Motivation* - This work could have been better motivated by suggesting some realistic use cases requiring longer sequences. The authors suggest high-resolution image cases as an example, but wouldn't scaling down the input image be a more practical solution, especially for classification and segmentation tasks? What is the practical range of sequence lengths today (and how does it scale) and what kind of applications could potentially benefit from a long sequence with >78K tokens?\n\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_L8Ft"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1536/Reviewer_L8Ft"
        ]
    }
]