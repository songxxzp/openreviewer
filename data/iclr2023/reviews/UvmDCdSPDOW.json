[
    {
        "id": "256QLf7Tu8",
        "original": null,
        "number": 1,
        "cdate": 1666022058716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666022058716,
        "tmdate": 1666910087293,
        "tddate": null,
        "forum": "UvmDCdSPDOW",
        "replyto": "UvmDCdSPDOW",
        "invitation": "ICLR.cc/2023/Conference/Paper5479/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes using information theory tools to derive the diffusion model's optimization object without using the variational approximation. It turns out the ELBO lower bound actually matches. This paper also discusses the problem of discrete probability estimator. Experiments are done to verify the findings.",
            "strength_and_weaknesses": "Strength:\n1. Using the information-theoretic approach to derive the object of the diffusion model seems interesting. Significantly, the finding of ELBO should be exact is interesting.\n\n2. Discussion of the related theoretical property of the main result is complete.\n\nWeakness:\n\n1. The result that ELBO should be exact is actually not new. Existing works [1, 2] have also had such result based on the Girsanov theorem and some SDE analyses. This slightly decreases the impact of the result. Moreover, a discussion comparing this work and the existing works are necessary: I was wondering which result should be general. \n\n2. The work in [1,2] also discusses the issue of training a discrete probability estimator but seems to using a different approach that avoids rounding and inexact approximation.\n\n3. The sentence under equ (12): In that case, we should be able to return the exact, correct value for \\hat{x} = x, leading to zero error with high probability. Could you elaborate on how this can be true during the sampling phase without the high probability that our model will be close to the discrete state at the end?\n\n4. The experiment section is weak. I believe there should be some more intuitive/practical way to demonstrate the improvement: how about the change of some standard metric such as FID? Does your experiment supports the 4 findings you list in page 5?\n\n[1] Let us Build Bridges: Understanding and Extending Diffusion Generative Models\n\n[2] First Hitting Diffusion Models\n\n===============================================================\nAfter reading AC's comments. \n\nThanks AC for outputting extra information and pointing out that some presentations in this paper confused me on the `tightness of VLB`. Indeed, VLB is tight when we choose the optimal posterior is a standard result and I didn't notice that this paper also requires such optimal posterior. \n\nSince this significantly reduces the soundness and novelty, I downgrade my initial score from 6 -> 5.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe clarity is clear but I feel the writing should be improved. This paper took me more time to read compared with other submissions due to its organization. I feel it is better to start with some background: the existing result/ preliminary knowledge of some important derivation and the problem.  And then begin to give some technical introduction to the information-theoretic approach. Also highlighting the important findings given by the theory in a short and simple sentence at the beginning of the section might be helpful.\n\nNovelty:\noverall, the novelty is sound.\n\nReproducibility:\nNo code is given and thus not accessible. But I believe it is reproducible. \n",
            "summary_of_the_review": "Please see above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_j334"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_j334"
        ]
    },
    {
        "id": "eclt3NbHD_Y",
        "original": null,
        "number": 2,
        "cdate": 1666529556827,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666529556827,
        "tmdate": 1666529556827,
        "tddate": null,
        "forum": "UvmDCdSPDOW",
        "replyto": "UvmDCdSPDOW",
        "invitation": "ICLR.cc/2023/Conference/Paper5479/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper discusses an unrecognized relation between denoising diffusion probabilistic models and Information with Minimum Mean Square Error estimators (I-MMSE). By showing the equivalence of the KL divergence $KL[p(z_\\gamma|x)\\|p(z_\\gamma)]$ and the pointwise MMSE, this work presents an exact relation between the data probability densities and the optimal denoising functions, which reveals new perspective for improving diffusion models such as recognizing important SNRs. Finally, a practical implementation of the proposed density estimator achieves good test NLLs.",
            "strength_and_weaknesses": "Strength:\n\n- The paper presented a highly intriguing relationship between DPM and I-MMSE, which has the potential to have a significant impact on current research on diffusion-based generative models.\n- The paper is generally well-written, organized, and easy to follow.\n\nQuestions:\n- The KL terms in the original ELBO (as in DDPM) also involve MSE terms. Is there any close relation between the ELBO and the actual implementation of the proposed method (e.g. Eq.(14))? (The discussion in \"Variational bound connection\" seem to be not clearly enough.)\n- In Section 6, it is mentioned that estimating Eq.(10) does not require a sequential estimation of the scores at different SNRs. However, I am uncertain as to whether this is an advantage over Song et al. (2020), as estimating MMSE at each SNR requires estimating an expectation over all Gaussian noise. If only one Monte Carlo sample is used to estimate each MMSE, then it is surely the proposed method is more efficient than Song et al. (2020) due to the parallelism, but this seems to produce a much larger variance. Moreover, regarding the parallelism, the original DDPM ELBO can also be evaluated parallelly if we are able to estimate the expectations (appeared in KL terms) at all SNRs. Can you please explain more about this?\n- Eq.(8) looks a bit like the form of normalizing flows, does this mean that MMSE terms correspond to some kind of log determinant of Jacobian of invertible transformations?\n\nMinors:\n- In Eq.(3) the \"arg\" should be \"argmin\".\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and clear. The discovered connection and the technical contribution in the paper are novel and of good quality.",
            "summary_of_the_review": "Overall, this paper presents a very interesting explanation of diffusion models with I-MMSE. The technical contribution is solid.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_pxZh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_pxZh"
        ]
    },
    {
        "id": "aVFrJLK7Mbf",
        "original": null,
        "number": 3,
        "cdate": 1666641406729,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641406729,
        "tmdate": 1666641406729,
        "tddate": null,
        "forum": "UvmDCdSPDOW",
        "replyto": "UvmDCdSPDOW",
        "invitation": "ICLR.cc/2023/Conference/Paper5479/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a new theoretical perspective on diffusion models\nby using an exact information-theoretical expression for the data\nlikelihood in terms of minimum mean squared error estimators.  This\nallows to refine likelihood bounds of existing diffusion models by\nconsidering the MSE of the denoisers at different SNRs.\n",
            "strength_and_weaknesses": "**Strengths**\n- The paper is well written and establishes an interesting novel\n perspective (to the best of my knowledge) for the formalism of\n diffusion models, in terms of information and MMSE estimators.\n- The method allows to refine likelihood bounds for existing models\n  while being significantly more efficient in terms of required steps to compute the likelihood bounds.\n- The paper proposes a principled approach to improving the bounds by\n  ensembling models with the best MSE at different SNRs.\n\n**Weaknesses/Questions**\n- The resulting refined bounds seem to be mixed in terms of sometimes\n  increasing or decreasing the value from the variational bounds. I\n  wonder if the authors can comment further on practical implications\n  that could be derived from this study, e.g. for improving diffusion\n  models?\n- The authors mention categorical variables in the introduction, which\n  is an interesting case in light recent discrete diffusion approaches\n  [Austin et al. 2021, VQ-diffusion  Gu et al 2022]. However it\n  seems to me that due to the Gaussian channel model this method is\n  not amenable to non-ordinal discrete variables such as VQ encodings.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear, original and good quality. The appears to contain sufficient details for reproducibility.",
            "summary_of_the_review": "This paper presents an interesting and novel theoretical perspective\ninto diffusion models, which form a valuable contribution.\nThis contribution could be stronger if there were clear practical\nimplications to improve diffusion models based on this perspective.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_Ky8k"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_Ky8k"
        ]
    },
    {
        "id": "RdY7HvM7oQ",
        "original": null,
        "number": 4,
        "cdate": 1666648759536,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648759536,
        "tmdate": 1666648759536,
        "tddate": null,
        "forum": "UvmDCdSPDOW",
        "replyto": "UvmDCdSPDOW",
        "invitation": "ICLR.cc/2023/Conference/Paper5479/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper characterizes the marginal log-likelihood (MLL) as an integral of the minimum mean squared error (MMSE) between the input and the optimal denoiser output under the Gaussian noise channel (integrated over all values of signal-to-noise ratio (SNR)). The main technical tools are a pointwise generalization of the I-MMSE relation and the thermodynamic integration trick. The paper applies the result to estimating data likelihood under diffusion models. Empirical versions of the MLL characterization are developed and shown to yield slightly higher test likelihoods than existing variational methods.\n",
            "strength_and_weaknesses": "STRENGTHS\n- Useful extension of the classical I-MMSE relation of Guo et al.\n- Exact equality between the MLL and a function of the Gaussian MMSE (for a choice of base Gaussian), which sheds light on the success of MSE in variational objectives for diffusion models. \n- The above equality is derived for both continuous and discrete inputs. The latter admits a very simple equality of -log P(x) = 0.5 int_0^{infty} MMSE(x, snr) d(snr), which implies the result on Shannon entropy from Guo et al. \n- The overall approach of deriving an exact relation rather than a variational objective is refreshing. \n\nWEAKNESSES\n- This is a (mostly) theory paper. There is no (substantial) empirical contribution. \n- The experiments section could be more informative (e.g., the \"denoising MSE objective\" used for finetuning is never explicitly defined).\n- Performance of the empirical bound is not addressed, though I think it's beyond the scope of the paper. It would be interesting to discuss this issue in the context of the general difficulty of entropy estimation.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and well written. The presented perspective and insights seem novel and substantial.",
            "summary_of_the_review": "The paper presents a new exact relation between MLL and MMSE under Gaussian noise channels, and applies it to the likelihood estimation under diffusion models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_q1V3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5479/Reviewer_q1V3"
        ]
    }
]