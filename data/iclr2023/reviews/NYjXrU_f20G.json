[
    {
        "id": "vv7wBtVfWa",
        "original": null,
        "number": 1,
        "cdate": 1666108073398,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666108073398,
        "tmdate": 1668252447373,
        "tddate": null,
        "forum": "NYjXrU_f20G",
        "replyto": "NYjXrU_f20G",
        "invitation": "ICLR.cc/2023/Conference/Paper2769/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper tackles the problem of detection of adversarial examples targeting (deep) neural networks. Specifically, the paper models the underlying operations of neural networks as a \u201cdynamic process\u201d, wherein information is propagated \u201cover time\u201d (i.e., goes through layers of neurons). By acting upon such intuition, the paper proposes to detect adversarial examples by analyzing \u201cwhat happens\u201d in-between each layer, thereby modeling it as an \u201canomaly detection\u201d problem. The proposed method is grounded in solid theoretical analyses, and is then experimentally evaluated on neural networks trained over diverse benchmark datasets (all of images). The experiments are repeated multiple times, and the overall findings confirm the validity of the proposal, which outperform a reputable prior work (Lee et al. (NeurIPS 2018)). Furthermore, the paper also shows that by \u201ctweaking\u201d the proposed detection method, it is also possible to improve its generalization capabilities---i.e., detecting adversarial examples that do not conform to what the detector is expected to identify. Such a property is also found to be true via comprehensive experiments.",
            "strength_and_weaknesses": "STRENGTHS:\n+ Strong theoretical analysis\n+ Experiments on multiple datasets\n+ Repeated trials that allow statistical tests\n+ The \u201cruntime\u201d is evaluated, thereby allowing to gauge the overhead of the method\n+ Good writing\n+ Good presentation\n+ Interesting (and novel) idea\n\n\nWEAKNESSES\n- The focus on \u201csecurity\u201d is not sound\n- The evaluation (and the entire method) appears to be exclusively tailored for data in the form of \u201cimages\u201d\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the presentation of the paper is very good.\n\nThe quality of the English text is fair.\n\nFigures and Tables are of good quality.\n\nThe topic addressed by the manuscript is source of abundant literature, but still an open issue and relevant for ICLR.\n\nThe references are appropriate.\n\nThe contribution is significant (at least in the domain of Computer Vision).\n\nThe basic idea is intriguing and novel (to the best of my knowledge), and may potentially inspire future works.\n\nThe results are reproducible: the datasets are publicly available, and the manuscript provides a link to an (anonymous) repository containing the source-code.",
            "summary_of_the_review": "I liked the paper, a lot. I commend the authors for their work: despite not being \u201cgroundbreaking\u201d, I consider this paper to be a solid contribution to the state-of-the-art, which deserves to be accepted to ICLR. Even by assuming a very critical eye, I was not able to find any significant flaw in the paper.\n\nSomething that I particularly appreciated is reporting the \u201cruntime\u201d: such metric is often forgotten in ML papers, but it is of high importance for practitioners as it allows to gauge the \u201ccost\u201d of a proposed method. Moreover, I also loved Section 5.2: here, the authors describe what I consider to be some \u201cnegative results\u201d. I think more papers should include them. Props!\n\nHowever, I believe that the paper is currently affected by a single issue---which stems from me being a researcher with a \u201csecurity background,\u201d and hence being critical of this aspect. Although I do not consider such issue to be ground for rejection, I truly invite the authors to fix it. Let me elaborate, and propose actionable means of remediations.\n\n**The \u201cattack\u201d is misleading.**\nThe paper poses itself as tackling a \u201csecurity-related\u201d problem, but does not support such security focus in the appropriate way. Indeed, let me quote the first line of the abstract: \u201cAdversarial attacks are perturbations to the input that don\u2019t change its class for a human observer, but fool a neural network into changing its prediction.\u201d The key term, here, is \u201cattack\u201d: from a security standpoint, an \u201cattack\u201d requires an \u201cattacker\u201d, who \u201cdoes something (malicious) in order to achieve a given goal\u201d. However, the paper does not provide any support for the \u201cattacks\u201d considered in the paper: this is due to the lack of a proper threat model---which is a lack that is becoming endemic in adversarial ML literature (see [A], which reports the opinions of researchers, policy makers and practitioners). To my understanding, the paper (and its contribution) does not deal with the \u201cdetection of adversarial attacks\u201d, but rather with the \u201cdetection of adversarial examples\u201d. Although the two topics have several similarities, they do not overlap: my take is that the paper addresses the more general problem of \u201cadversarial examples,\u201d which is useful to assess the \u201crobustness\u201d of ML methods and not their overall \u201csecurity\u201d (I quote the term \u201cfalse sense of robustness\u201d used in a recent NeurIPS paper [B]). \nTo address this problem, I invite the authors to revise the paper by replacing \u201cadversarial attacks\u201d with \u201cadversarial examples\u201d whenever necessary. Alternatively, the authors can insert a proper \u201cthreat model\u201d: this could be done by moving the current Section 4.2.2 into the Appendix. My stance is that the paper is being oversold as being (also) a \"security paper\": the authors should prove that I'm wrong, or -- if I'm not -- then they should fix their paper accordingly.\n\n**Dubious utility of \u201cdetection\u201d.** \nThis is a direct consequence of the previous point. The goal of a \u201cdetection\u201d mechanism (in the context of  \u201cdetecting an attack\u201d) is to act as a \u201ctrap\u201d for the attacker. In other words, the detector \u201cexpects the attacker to do A, and if they do so, then the attacker falls into the trap and is therefore detected\u201d. However, the lack of a threat model describing what the attacker can/cannot do prevents from determining the utility of the detection mechanism as a security measure. Such a shortcoming further supports my previous observation of shifting the focus of the paper from \u201cdetecting adversarial attacks\u201d to \u201cdetecting adversarial examples\u201d. I point the authors to [C] for a good paper that truly models the problem of \u201cdetection of adversarial *attacks*\u201d.\nI acknowledge that the paper reproduces well-known white/black box attacks, but I still do not accept the lack of a threat model. A way to rectify the issue is by mentioning that \"we evaluate our method by assuming adversarial examples found by means of the white/black box attacks proposed in (ref)\".\n\n\n\n**General or Specific?**\nSomething that is not fully clear to me is whether the proposed idea (which I found very enticing!) can be applied to \u201cany\u201d neural network, or only to residual nets. Let me explain. The paper proposes a method whose main intuition lies in the modeling the \u201cdepth\u201d of a neural network as \u201ctime\u201d. However, this leads me to think: would this idea \u201cwork\u201d for those neural networks that are \u201ctime-aware\u201d, such as recurrent neural networks (RNN) and LSTM? This is because, by following the paper\u2019s idea, an input can go \u201cback in time\u201d. The paper does not take such architectures into account, so I am inclined to believe that it does not work. According to the introduction, the paper focuses on \u201cresidual networks\u201d because they are \u201cparticularly amenable to this analysis\u201d. The authors fairly state that \u201cthe analysis and implementation can extend immediately to any network where most layers have the same input and output dimensions.\u201d, but they do not mention whether this also applies to RNN or LSTM; moreover, in Section 3.2, the definitions clearly state \u201cResidual Networks\u201d.\nI hence ask the authors to clarify whether the proposed method can cover only \u201cresidual networks\u201d (as well as any model that approximates such networks), or not. This should be clearly stated in the Introduction.\n\n**Only Image data.**\nThis is self-explanatory: although the proposed methodology is (theoretically) applicable to any (residual?) neural network, the paper (and its evaluation) appears to be tailored for applications of neural networks that focus on image analysis. There are many domains in which neural networks are used today, and focusing on a single data-type prevents to assess the general applicability of the proposed technique. A potential fix can be, e.g., performing a proof of concept experiment on a dataset that does not include images---albeit this may be hard to find, since residual nets (which I believe to be \u201crequired\u201d for the proposed method) are mostly useful for images.\n\nSome additional issues:\n\n\u2022\tI recommend to change the title: as a \u201csecurity\u201d researcher, I was misled into thinking that the paper dealt with adversarial attacks in the \u201cnetwork traffic analysis\u201d domain (the term \u201ctransport\u201d is a key component in networking). Specifically, I endorse the authors to replace the term \u201ctransport\u201d with something else. Maybe \u201cupdates\u201d is a better fit?\n\n\u2022\tThe first sentence in Section 5.1 should be better contextualized \u201cFirst, the regularization improves generalizations.\u201d Instead of \"First\", it should report \"this result shows that...\"\n\n\u2022\tIn Section 4: \u201cuse the transport regularization (6) to improve detectability of adversarial attacks.\u201d It is not clear what \u201c(6)\u201d stands for.\n\n\u2022\tI was originally taken aback by the \u201cabstract\u201d. In truth, I did not like the abstract at all. I think the authors tried to blend \u201ctechnical\u201d details with \u201csimple\u201d jargon, but the result (imho) is something that does not give true credit to the paper\u2019s value. \n\nEXTERNAL REFERENCES\n\n[A]: Apruzzese, G., Laskov, P., de Oca, E. M., Mallouli, W., Rapa, L. B., Grammatopoulos, A. V., & Franco, F. D. (2022). The Role of Machine Learning in Cybersecurity. Digital Threats: Research and Practice.\n\n[B]: Pintor, M., Demetrio, L., Sotgiu, A., Manca, G., Demontis, A., Carlini, N., ... & Roli, F. (2022). Indicators of attack failure: Debugging and improving optimization of adversarial examples. NeurIPS 2022.\n\n[C]: Shan, S., Wenger, E., Wang, B., Li, B., Zheng, H., & Zhao, B. Y. (2020, October). Gotta catch'em all: Using honeypots to catch adversarial attacks on neural networks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security (pp. 67-83\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_Xhhn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_Xhhn"
        ]
    },
    {
        "id": "8rm6ktsANBi",
        "original": null,
        "number": 2,
        "cdate": 1666583450851,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666583450851,
        "tmdate": 1666583450851,
        "tddate": null,
        "forum": "NYjXrU_f20G",
        "replyto": "NYjXrU_f20G",
        "invitation": "ICLR.cc/2023/Conference/Paper2769/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper tries to detect adversarial examples from a perspective that models residual networks as discrete dynamical systems. The detector studies trajectories of samples in space, through time, to distinguish between clean and adversarial examples. Based on this rationale, the authors also apply transport regularization during training to make the detector perform better. Though this method does not necessarily make the model more robust to adversarial attacks, it makes the model's behavior on adversarial examples more distinguishable.",
            "strength_and_weaknesses": "Strength:\nThe proposed method has a solid theoretical background and the idea of applying trajectory regularization in optimal transportation to adversarial example detection is also very interesting. The theoretical studies look solid to me. The authors also conducted experiments to show the effectiveness of the proposed method.\nWeakness:\n1. My first concern is that the authors did not test on adaptive adversaries, which may have access to the detector itself. Similar issues also exist in other works on adversarial examples detection and defense. There are existing works that show that random forest and other decision tree-based models are also vulnerable to adversarial attacks (e.g. Kantechelian et al \"Evasion and Hardening of Tree Ensemble Classifiers\", ICML16). So I think if the attackers are able to design attacks on the detector, the proposed method can be bypassed. \n2. Even for non-adaptive attacks, SOTA attacks such as AutoAttack (Croce and Hein, ICML20) are not tested, either. I think it is necessary to test under strong attacks. For example, LID mentioned in this paper fails under high-confidence adversarial examples according to Athalye et al ICML18.\n3. Minor: Please refrain from only using color to distinguish points and bars as in Figure 1, Figure 2 and Figure 3, as it is not friendly to readers with color blindness.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow in general. I am not concerned about its originality and reproducibility, after checking the code that's released anonymously by the authors.",
            "summary_of_the_review": "In general, I think this paper proposed an adversarial detection method from an interesting perspective. The theoretical analysis is solid. However, the experimental part may not be enough due to the lack of adaptive attacks and strong attacks, which makes the proposed method less convincing. So I would recommend a borderline reject.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_jGWF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_jGWF"
        ]
    },
    {
        "id": "ZGBFxuxKvc",
        "original": null,
        "number": 3,
        "cdate": 1666648075042,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666648075042,
        "tmdate": 1666648075042,
        "tddate": null,
        "forum": "NYjXrU_f20G",
        "replyto": "NYjXrU_f20G",
        "invitation": "ICLR.cc/2023/Conference/Paper2769/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes an adversarial example detection mechanism for residual networks, as well as a regularization method to improve the detection performance. The detection mechanism is based upon viewing the deep network as a dynamical system. The regularization method is an existing method based on optimal transport that the authors show theoretically and empirically makes adversarial examples easier to detect.  ",
            "strength_and_weaknesses": "Strengths\n- The authors show their detection method performs better than the Mahalanobis detector on known and unknown white box attacks, and also on known black-box attacks. \n- The regularization method proposed also improves the performance of the existing Mahalanobis detector.\n- The authors give theoretical backing behind their choice of regularization method. \n\nWeaknesses\n- Overall I found the paper hard to follow. I think the paper would benefit from more explicitly stating how the detection is implemented. \n- The method is restricted to residual networks, whereas the detector being compared to (the Mahalanobis detector) I believe is applicable to different types of architectures.\n- I'm not sure whether you specify that you are considering the l-infinity norm threat model in section 5. \n- There are a lot of unclear statements and loose ends. For example, rather than saying \"We don\u2019t fine-tune these hyper-parameters much\", it'd be better to be more specific with respect to the details.\n- There was never much of an introduction or review of adversarial examples more formally -- you may wish to add this. \n- The computational complexity of the detection method was not made explicit. \n- You could more explicitly compare your method to the method of comparison (the Mahalanobis detector) in the related works section for better clarity. \n\nOther comments/questions:\n- Why do you use the test set for training the detector? It seems like it would be more reasonable to hold out part of the training set.\n- This paper focuses on detecting adversarial examples specifically, how does it compare to existing methods on other OOD detection tasks? ",
            "clarity,_quality,_novelty_and_reproducibility": "I found the clarity of the paper overall lacking. The paper introduces what I believe to be a novel detection framework, and a novel application of the particular regularization method. ",
            "summary_of_the_review": "I am leaning towards reject because I found the paper lacking clarity/difficult to follow in terms of the methods proposed. However, the final results of the paper are promising. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_J6gT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2769/Reviewer_J6gT"
        ]
    }
]