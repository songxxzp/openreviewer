[
    {
        "id": "A_-wQqDQzKZ",
        "original": null,
        "number": 1,
        "cdate": 1666147964832,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666147964832,
        "tmdate": 1666147964832,
        "tddate": null,
        "forum": "lVdvYoIxsXm",
        "replyto": "lVdvYoIxsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper5659/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a framework pre-training for robots (PTR) based on CQL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 10 demonstrations.\nIn particular, their method modifies CQL to include several crucial design choices that enable PTR to have strong generalization. These choices include using task indicators for policy and Q-function networks, using group normalization, and using mixed data of prior data and the target dataset.\nIn their experiments, they train the model on the Bridge dataset and finetune it with real-world demonstrations. Their method shows signi\ufb01cant improvement over prior RL-based pretraining and \ufb01netuning methods.\n",
            "strength_and_weaknesses": "### **Strength**\n\n* Generalization of offline reinforcement learning algorithms is an interesting topic. Although there is no new algorithm or framework in this paper, the exploration of useful combinations of existing technics still contributes to this area.\n\n* The presentation of this paper is good. All details are clearly explained. \n\n\n### **Weaknesses**\n\n* Missing baselines of offline meta-reinforcement learning [1][2]. Without comparing with existing methods, it is hard to position the empirical value of this paper.\n\n* Sort of limited novelty. As the authors mentioned, \u201cAlthough the individual components that constitute PTR are not especially innovative and are based closely on prior work, the combination of these components is novel.\u201dSince there are already some existing novel meta-RL methods, properly comparing them could empirically improve the novelty of this paper.\n\n---\n\n[1] Mitchell, Eric, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. \"Offline meta-reinforcement learning with advantage weighting.\" In International Conference on Machine Learning, pp. 7780-7791. PMLR, 2021.\n\n[2] Xu, Mengdi, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. \"Prompting decision transformer for few-shot policy generalization.\" In International Conference on Machine Learning, pp. 24631-24645. PMLR, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "### **Quality**\nThe quality of this paper is high for proposing an empirical framework for offline RL.\n\n### **Clarity**\nThe presentation is very clear.\n\n### **Novelty**\nThe novelty could be improved by comparing it with existing meta-RL methods.\n\n### **Reproducibility**\nCode is not released but the technics used in the method are either well-known or easy to implement. So I think it is easy to reproduce the results. \n\n",
            "summary_of_the_review": "I acknowledge the empirical contribution of this paper to the generalization problem of offline RL. However, without a fair comparison with state-of-the-art meta-RL algorithms, I suggest rejecting this paper for now.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_WMAS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_WMAS"
        ]
    },
    {
        "id": "sJkTk6i37-_",
        "original": null,
        "number": 2,
        "cdate": 1666492137123,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666492137123,
        "tmdate": 1669503698952,
        "tddate": null,
        "forum": "lVdvYoIxsXm",
        "replyto": "lVdvYoIxsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper5659/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper experimented with using offline RL to pretrain and fine-tune policy and Q function. The results show that using offline RL to first pretrain the policy and Q function on a diverse offline dataset can improve the policy adaptation speed on new tasks. The paper discussed about several important design choices, such as network architectures to achieve better performance. Overall, the paper is more like an engineered work that combines some ideas from prior works and achieves fast policy learning on new tasks.",
            "strength_and_weaknesses": "**Strength**:\n\n* The paper presents a few techniques, such as network architecture design and normalization trick, that are empirically shown to improve downstream task learning.\n\n* The paper shows real-world robot experiments. This is a big plus as such experiments do require a significant amount of effort.\n\n * The paper compares the proposed pipeline with a decent number of baselines, including behavior cloning and especially prior works on first doing representation learning and then doing policy learning such as R3M.\n\n**Weaknesses**:\n\n* As the paper also says itself, the techniques presented in the paper are not new.\n\n* Since the number of real-world tests is small (like 20?), I am not sure how one can report success rates with so many significant figures. Similarly, the paper tends to overclaim the method's effectiveness. For example, in page 5, the paper says that using \"learned spatial embeddings\" leads to `2x` improvement. However, if we actually look at table 10, we see that one case has 4/10 success rate and the other one has 7/10 success rate. With such a small number of tests, it is very confusing and not rigorous at all to make such claims (`2x` improvements). There are other similar claims in the paper. The authors should remove such claims.\n\n* In table 2, I cannot find the 95% confidence interval.\n\n* It's unclear how convincing the claim about generalizing to previously unseen domain is. From the video, I can only tell that the testing door is not that different from the training doors. Can authors add a figure showing all the training doors and the testing door? Since the claim is about generalization, it is important for readers to understand how much difference the training and testing distribution has. As a research paper, it is important to make each claim precise and accurate. While I enjoy reading the papers, especially seeing the real-world testings, reading such exaggerated claims only negatively affects my judgment.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper writing is clear, except it tends to exaggerate the claims or does not define the scope of the claims properly.",
            "summary_of_the_review": "Overall I think the paper presents a decent amount of experiments and compare its proposed pipeline with some reasonable baselines. However, the paper writing needs to be improved to make the descriptions accurate.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_D87g"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_D87g"
        ]
    },
    {
        "id": "3es6DGFqpRc",
        "original": null,
        "number": 3,
        "cdate": 1666638521199,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666638521199,
        "tmdate": 1666638521199,
        "tddate": null,
        "forum": "lVdvYoIxsXm",
        "replyto": "lVdvYoIxsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper5659/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of letting robots learn to solve novel target tasks by leveraging diverse offline datasets in combination with only small amounts of task-specific data. To this end, the paper proposes a framework that uses multi-task offline reinforcement learning approaches for pre-training and then fine-tuning its policy on target tasks. The experimental results show that the proposed framework outperforms behavior cloning and offline reinforcement learning baselines including CQL and COG. I am leaning toward accepting this paper since it studies a promising research direction and presents a reasonable framework to address the problem with supporting experimental results.",
            "strength_and_weaknesses": "## Paper strengths and contributions\n\n**Novelty**\nIn my opinion, the idea of bridging offline reinforcement learning and behavior cloning to solve novel target tasks in a new domain with only a few demonstrations is intuitive and convincing. This paper presents an effective way to implement this idea.\n\n**Clarity**\nThe overall writing is clear. The authors utilize figures well to illustrate the ideas.\n\n**Related work**\nThe authors give a clear description of the related prior works from both the perspectives of offline reinforcement learning and behavior cloning.\n\n**Experimental Results**\nThe experimental results show that the proposed framework outperforms behavior cloning and offline reinforcement learning baselines including CQL and COG.\n\n## Paper weaknesses and questions\n\n**Experimental results**\n- It is mentioned in scenario two that COG does not outperform BC (joint), which is pretty interesting. Unfortunately, COG was only compared to BC but not PTR with this setup. It would be more informative to give a brief explanation or an intuition of why COG was at a disadvantage in that particular scenario.\n- It is not entirely intuitive to me why the proposed framework outperforms BC baselines. It would be helpful if the authors give more intuitions.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "I am leaning toward accepting this paper since it studies a promising research direction and presents a reasonable framework to address the problem with supporting experimental results.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_mM3r"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_mM3r"
        ]
    },
    {
        "id": "j0zFGvO1FwS",
        "original": null,
        "number": 4,
        "cdate": 1667226399056,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667226399056,
        "tmdate": 1667226399056,
        "tddate": null,
        "forum": "lVdvYoIxsXm",
        "replyto": "lVdvYoIxsXm",
        "invitation": "ICLR.cc/2023/Conference/Paper5659/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper targets to solve real-world robotic manipulation tasks by leveraging a large demonstration dataset and a small amount of target task demonstrations.\nThe proposed method first pre-train a policy on the large dataset with multi-task offline RL, then fine-tune on the small demonstration data again with same objective.\nDuring the fine-tuning phase, a small amount of data is sampled from the dataset used for pre-training to prevent overfitting.\nThe fine-tuning process is early stopped by heuristic validation rule.\nIn experimental results, several challenging robotic manipulation tasks are solved by the proposed method using proper design choices of network architecture and network size.",
            "strength_and_weaknesses": "### Strength\n* Provided results on robotic manipulation tasks can provide empirical knowledge on handling robot control tasks using offline dataset.\n\n### Weakness\n* Novelty\n    - The problem setup of this paper is very relevant to fully offline meta-RL and the paper is stating \"our approach is much more data-efficient and simple\". However, any comparison or justification is not given in the paper, thereby misleading readers to be impressed as the problem setup is novel.\n    - Given architectural choices are interesting and reasonable, but neither surprising nor significant. Moreover, in order to introduce the network design as a technical novel of this paper, the architecture should be applied and evaluated on more diverse domains.\n    - The early stopping criteria is bizarre. The Q-value plot of passed example in Figure 3 is not monotonically increasing over the time. If there is an exact rule deciding whether it is \"almost\" monotonically increasing it is not described in the main text. If a human decide this, any of given results is reliable as there is no human-related study for this procedure.\n\n* Comparison\n    - As mentioned before, the comparison with offline meta-RL is missing. Also, any naive application of imitation learning method other than behavior cloning also deserves to be a baseline.\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity, Quality\nThe paper is clearly written and easy to follow.\n\n### Novelty\nMany aspect of papers can be found in prior works, but it is not enough addressed.",
            "summary_of_the_review": "Although this paper provides interesting results on challenging manipulation tasks leveraging offline data,\nthe relationship between the proposed method and existing methods is not enough addressed.\nMoreover, the proposed design choices are incremental but evaluated on a single domain.\nBecause the generality of the proposed method is not proven, it is difficult to be introduced to the community as a generic learning method for such a problem setup.\nThus, I vote to reject this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_tq7f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5659/Reviewer_tq7f"
        ]
    }
]