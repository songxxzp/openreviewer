[
    {
        "id": "NeG0pIoz9mN",
        "original": null,
        "number": 1,
        "cdate": 1666348151838,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666348151838,
        "tmdate": 1666348151838,
        "tddate": null,
        "forum": "0eSq84hbXhe",
        "replyto": "0eSq84hbXhe",
        "invitation": "ICLR.cc/2023/Conference/Paper3303/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work presents a graph structure refinement layer named GLAM based on a neighborhood attention schema slightly different from the Softmax attention, which disentangles the optimization of node embeddings and graph structures. The proposed GLAM layer learns more sparse subgraphs than prior arts by giving the known adjacency matrix.",
            "strength_and_weaknesses": "Pros:\n- The insight of the addressing the conflicts between structure learning and node embedding optimization is interesting\n- Simplicity of the methodology\n- The paper is well written and easy to follow despite some confusing points\n\nCons:\n- Lack of a more principled motivations and justifications behind the proposed design\n- Limited evaluation setup\n- Limited technical novelty\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this paper presents an attention mechanism slightly different from the Softmax attention as the graph structure learner extracts sparse subgraphs to be effective topological information to facilitate node embedding learning. I like the idea of disentangling the learning of graph structure and embeddings, but the analysis is less insightful and lacks more principled discussion in Sec. 3. Also, I do not see significant breakthroughs and novelty for the composition of the GLAM layer. For the claimed advantages, there are no substantial improvements or clear justifications in experiments.",
            "summary_of_the_review": "Following my comment above, there are several questions:\n\n- What are the exact conflicts between the learning of graph structures and node embeddings? Specifically, in Sec. 3, I have these questions: (1) What are the roles of the permutation invariant aggregator in hindering the use of $\\alpha_{ij}$ as the bases of structure learning? (2) I partially agree with your analysis about the Softmax attention, but how to get your conclusion of \u201cFor these reasons, \u2026, we should not do away with the Softmax activation to normalize attention coefficients\u2026\u201d? (3) I cannot see your principled analysis to answer why it is not desired to learn edge weights together with node embeddings, e.g., how do you estimate the noisy evolution of local neighbourhoods, and why the improvements over GAT are less significant if your conjectures hold?\n\n- I notice the authors mentioned graph sparsity several times, especially in Sec. 6, but there is no experiment to discuss the consequent benefits, e.g., efficiency and scalability. Have you evaluated the proposal on some larger real-world graphs, e.g., OGB datasets?\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_jLy4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_jLy4"
        ]
    },
    {
        "id": "EPycoifTdy9",
        "original": null,
        "number": 2,
        "cdate": 1666540110735,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666540110735,
        "tmdate": 1666729672962,
        "tddate": null,
        "forum": "0eSq84hbXhe",
        "replyto": "0eSq84hbXhe",
        "invitation": "ICLR.cc/2023/Conference/Paper3303/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses issues of GNN sensitivity to graph structures. Specifically how noisy edges degrade performance which cannot be mitigated without complete removal. This paper proposed a novel layer Graph Learning Attention Mechanism (GLAM) which isolates structure learning from node embeddings. This technique does not require using costly exogenous structural regularizers or edge-selection heuristics to learn optimal graph structures. Experiments on benchmark datasets including Cora, PubMed, Citeseer, Amazon Photo, and Amazon Computers show comparable classification performance with significantly higher sparsity in structure.",
            "strength_and_weaknesses": "The experiments indicate the GLAM layer when applied achieves comparable prediction accuracies to state-of-the-art methods on benchmark datasets but with a significantly sparser graph structure. \n\nSince the experiments do not show great improvement in prediction accuracies on benchmark datasets, the justification for using this method would rely on applications that can take full advantage of the edge removal capabilities in this paper.\n\nFuture experiments can uncover the reasoning behind the drastic differences in the number of removed edges between the first and second GLAM layers. \n\nDuring experiments, each dataset has a drastically different % of edges removed during training. Further research may be able to explain this occurrence.\n\nDetermining cutoffs for low homophily thresholds for usable datasets can open the door for applications to more diverse datasets and applications.\n\nDoes the GLAM layer offer any runtime benefits during training?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written and organized with clearly defined objectives and motivations.\n\nThis work builds on existing methods including Graph Attention Networks (GAT). In breaking down the GLAM method, the similarity and novelty of this method from GAT are clearly noted. \n\nThe authors take note of the constraints on the datasets that would benefit from applying the GLAM layer for training.\n\nSource code and setup scripts are provided but redacted for review.\n",
            "summary_of_the_review": "The work in this paper is good with promising results in an interesting direction beyond plainly increasing prediction accuracy on benchmark datasets. Additional work may be needed to expand upon this research for publication. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_aSBH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_aSBH"
        ]
    },
    {
        "id": "eMIkuNtZzh",
        "original": null,
        "number": 3,
        "cdate": 1666657330451,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657330451,
        "tmdate": 1666657330451,
        "tddate": null,
        "forum": "0eSq84hbXhe",
        "replyto": "0eSq84hbXhe",
        "invitation": "ICLR.cc/2023/Conference/Paper3303/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work aims to tackle the challenges of noisy edges in graph learning problems. It first points out the finding that the demands of structure learning and node embedding are inherently conflicting with each other, so it is not proper to optimize both jointly.  Then, based on their finding, they propose the Graph Learning Attention Mechanism \uff08GLAM\uff09 layer, which is a non-heuristic differentiable network layer that can be inserted into existing GNN models and can focus on the graph structure learning objective to learn a task-informed graph structure and benefit the graph learning. ",
            "strength_and_weaknesses": "Strength:\n1. This work is clearly motivated, and the studied topic is meaningful.\n2. The writing of this work is easy-to-follow.\n\nWeakness:\n1. Model Evaluation\n1) The proposed GLAM is only examined with 3 baselines, which I think is not enough. \n- The authors mentioned that \"GSAT was not evaluated on our datasets\", I am wondering why GSAT is excluded.\n- I think it would be better if the authors can also compare with more challenging baselines, especially some SOTA models on structure learning.\n2) no ablation study is provided to examine the GLAM method comprehensively, for some examples, I am curious about the following aspects:\n- is it necessary to apply a GLAM layer before every GNN layer? \n- the authors claim that the GLAM can be incorporated with various GNN backbones, however, only an example with GAT is provided. I am curious to see how GLAM performs on other GNN models, such as GCN.\n\n2. Reproducibility\n- Though the authors provide the section to clarify the reproducibility and the experimental set-up details claimed to be provided in the code, the code link is missing. So essentially no useful information is provided in the paper, which makes the reproducibility fair.\n\n\nSome questions:\n1. In equations (5) and (6), what is the major benefit of adding the noise term u? is it for the Gumble softmax trick for differentiability?\n2. if we apply GAT after GLAM, it seems to me that we are still jointly considering the structure learning and node embedding tasks simultaneously as the model still uses the weight on each edge learned by GAT for message passing. Does it contradict the claim that these two objectives should be considered separately?\n3. It looks to me that, the major drawback of GAT is the softmax which expresses the neighbors' importance relatively to other nodes. Then, does it mean, as long as we come up with a way to get rid of this softmax, the limitation can be solved? \n4. In section 5, I don't quite understand, why it would be problematic to add penalties on retained edges?\n5. In section 6.2, the authors claims one of the benefits of GLAM is sparsification, I am wondering, is this sparsification mainly for efficiency purpose? If so, then how much speed/memory improvement can this sparsification bring to GAT? If not, then why is sparsification important here?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\n\nQuality: Fair\n- I think the evaluation of the proposed method is not good enough, which makes the quality of this work fair. Please refer to the weakness for more details. \n\nNovelty: Fine (between Fair and Good)\n- I appreciate the observation that the structure learning and node embedding, as claimed in the paper, this has never been done before. I feel this looks like the major novelty of this work.\n- The proposed method looks very similar to GAT, but replacing the LeakyReLU in eq.(1) and softmax in eq.(2) with the plus noise and sigmoid in eq.(5), I don't think this part has enough novelty.\n\nReproducibility: Fair\n- Please refer to the weakness for more details.\n",
            "summary_of_the_review": "This is a well-motivated work focusing on how to separate the structure learning and node embedding task to best solve the graph learning problems. Though the writing is clear and the explored topic is meaningful, I have major concerns about the method evaluation. First, it is not compared with comprehensive suitable baselines, and the benefit of the proposed method is not distinctive; second, the authors only provide one simple experimental result (in table 2), and no ablation studies are provided to examine the GLAM method comprehensively; another minor point is, neither the code link nor details of the experimental set-up are provided.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_7cxi"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_7cxi"
        ]
    },
    {
        "id": "8th6g7kuia",
        "original": null,
        "number": 4,
        "cdate": 1667122000438,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667122000438,
        "tmdate": 1667122000438,
        "tddate": null,
        "forum": "0eSq84hbXhe",
        "replyto": "0eSq84hbXhe",
        "invitation": "ICLR.cc/2023/Conference/Paper3303/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a topology optimization method for message passing-based graph learning models, termed Graph Learning Attention Mechanism (GLAM). The proposed model is differentiable and removes edges in an end-to-end manner. The method can be integrated into GCN and GAT. The experimental results show the proposed GLAM can improve the performance of GCNs.",
            "strength_and_weaknesses": "Strengths,\n\nS1: The proposed topology optimization method is differentiable;\n\nS2: This paper studies an interesting problem;\n\nS3: This paper is well-written.\n\nWeaknesses,\n\nW1: There is a lack of evidence to support the analysis of the conflations in Section 3, which weakens the motivation of this paper.\n\nW2: Topology optimization is not new in graph learning, especially edge movement.\n\nW3: The improvement from GLAM seems limited in experiments. And the empirical support for the effectiveness of GLAM is weak.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. Topology optimization is not new in graph learning, especially edge movement. The paper points out a conflict between node embedding and structure learning, which seems new but not convincing.",
            "summary_of_the_review": "I recommend a weak reject because of the weak motivation and validation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_BqHK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3303/Reviewer_BqHK"
        ]
    }
]