[
    {
        "id": "X2qpGvWL1D",
        "original": null,
        "number": 1,
        "cdate": 1666900654722,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666900654722,
        "tmdate": 1666900654722,
        "tddate": null,
        "forum": "XIIynqbMXgR",
        "replyto": "XIIynqbMXgR",
        "invitation": "ICLR.cc/2023/Conference/Paper2704/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a novel set of benchmark resources for discourse-aware evaluation of language undersatnding, translation, and generation, focusing on Chinese and English languages. The release includes benchmarking data, diagnostic test suite, pretraining corpora and pretrained models. The benchmark is applied on relevant models across a wide variety of tasks, showing benefits of document-level pretraining in Transformer architectures when it comes to discourse.",
            "strength_and_weaknesses": "Strengths:\n- Potentially a very useful resource for discourse-aware evaluation across tasks.\n- Very thorough description of the resources, including in-depth background in the appendix.\n- Considerable experiment breadth.\n- High-quality writeup.\n\nWeaknesses:\n- The paper does not reflect thoroughly on the linguistic bias introduced by choosing Chinese and English as the focus languages. There should be a Limitations section that addresses how the proposed approach would scale in low-resource scenarios or if extending the breadth of linguistic coverage in the datasets. What would it take to introduce another language? What if that language is low-resource? How would that support the findings? Can we simulate that with the existing data to an extent?\n- Experiment breadth is not backed with experiment depth, at least in the main part of the paper, as the Main Results and Analysis are rather superficial. This is a minor concern since the main focus of the paper is to provide the resource.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is exceptionally clear, of high quality, and while it is a resource paper so it does not contribute novel modelling, it does offer a very thorough account of discourse in natural language understanding, translation, and generation.",
            "summary_of_the_review": "Contained above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_zDLD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_zDLD"
        ]
    },
    {
        "id": "kzkLSa4Mqv",
        "original": null,
        "number": 2,
        "cdate": 1666919722687,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666919722687,
        "tmdate": 1666919722687,
        "tddate": null,
        "forum": "XIIynqbMXgR",
        "replyto": "XIIynqbMXgR",
        "invitation": "ICLR.cc/2023/Conference/Paper2704/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a new collection of datasets, the goal being to analyze discourse phenomena.",
            "strength_and_weaknesses": "Strengths: data set construction is valuable, and the goals of the work seem worthwhile.\n\nWeaknesses: first, there is no attempt to evaluate the quality of the resulting data, either through expert evaluations or inter-annotator consistency (I think the former is the only way to really check for this). Second, many of the details of the paper are unclear. Third, many claims are made without sufficent support. ",
            "clarity,_quality,_novelty_and_reproducibility": "Reproducibility is problematic given that no quality control/measurement is described.\n\nClarity is an issue in several places.",
            "summary_of_the_review": "Overall, this is worthwhile work, but there are a few issues:\n\n* the lack of any measures of quality of these datasets is problematic. I would be hesitant to use this data without such measures\n\n* Clarity is an issue in several places. what does \"aims to recover omitted pronouns in terms of position and form, according to its anaphora information in the given sentence\" mean?, for example\n\n* I think the argument that these are \"discourse level\" tasks is a stretch in several cases. For example, several of the datasets are conventional translation datasets.\n\nOverall I think the goal to release data is valuable, but the paper needs to be improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_CWMG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_CWMG"
        ]
    },
    {
        "id": "aeDDttvQZd",
        "original": null,
        "number": 3,
        "cdate": 1667051873936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667051873936,
        "tmdate": 1667051873936,
        "tddate": null,
        "forum": "XIIynqbMXgR",
        "replyto": "XIIynqbMXgR",
        "invitation": "ICLR.cc/2023/Conference/Paper2704/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents the GuoFeng benchmark consisting of i) 9 NLP tasks, ii) a contrastive testing set, iii) a large scale training data. The aim is to evaluate the abilities of models for cohesion and coherence across clauses and sentences. The authors also conduct experiments with transformer models using various settings, including randomly initialized ones, pre-trained models, and pre-trained models on the GuoFeng training data. ",
            "strength_and_weaknesses": "Strengths:\n\n* The datasets are valuable for the document-level NLP research, which need to address the coherence and cohesion across clauses and sentences.\n* The experiments provide baseline results for future research.\n\nWeaknesses:\n* I have difficulties to fully understand the settings of the 9 tasks. How to evaluate each task?\n     * SI: are the speakers always mentioned before the utterance of interest? What are the expected outputs if there are multiple utterances with quotation marks?\n     * ZPR: What are Chinese-English movie subtitles? Are they Chinese movies with English subtitles, English movies with Chinese subtitles, or both?\n     * NT: Is eclipsis not important for novel translation? How about concept consistency in scientice fictions?\n     * CCT: do you mean traditional Chinese? Novels written in traditional Chinese?\n     * PT: what's the target language? English?\n     * How to evaluate the generation tasks if there are multiple plausible answers?\n* What is the inter-annotator agreements for the tasks requiring human annotations?\n* What are the inter-annotator agreements for the discourse-aware test suite? How are the annotators instructed to construct adversarial examples or the noises are added by algorithms? What kind of noises?\n* Why coreference resolution models are not considered as part of or as baselines of applicable tasks? \n* It is not clear to me what are the new challenges arising from the benchmark? ",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity is certainly an issue, see the weaknesses.\n\n* Reproducibility is also an issue due to the lack of human annotation details and the guidelines for human annotators.",
            "summary_of_the_review": "Overall, this paper can benefit from a major revision, although the resources are interesting for the community. More insights from the experiments are expected, not just model A is better than model B, expecially about what are the new challenges that are not covered by the prior works, evidenced by the experimental results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_eMwY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2704/Reviewer_eMwY"
        ]
    }
]