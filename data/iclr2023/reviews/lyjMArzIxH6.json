[
    {
        "id": "jtYNX1oloe4",
        "original": null,
        "number": 1,
        "cdate": 1666508626670,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666508626670,
        "tmdate": 1666508626670,
        "tddate": null,
        "forum": "lyjMArzIxH6",
        "replyto": "lyjMArzIxH6",
        "invitation": "ICLR.cc/2023/Conference/Paper3445/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper develops a transformer-based model to learn representations of job sequences. The authors first leverage the transformer architecture to fit large-scale resume data, and then finetune the model with smaller, task-specific data, which achieves good performance in predicting job sequences. ",
            "strength_and_weaknesses": "a.\tStrength:\n1). This paper is well-presented.\n2). The authors propose a transformer-based framework for job prediction, which can effectively take use of large-scale resume data; the pretrain-finetune paradigm is reasonable.\n3). The experiments show that their proposed model can perform well on job prediction task. \n\nb.\tWeaknesses:\n1). Some related work is not mentioned and some potential baselines are missed. Actually person-job fit is quite a mature topic in data mining and information retrieval. The first two baselines (Markov regression and bag-of-jobs) seem to come from econometrics, and the third baseline (NEMO) is quite old in data mining. The authors should investigate more and discover more strong baselines to verify the effectiveness.  \n2). Depside the effectiveness of pretrain-finetune paradigm of transformer architecture, it has been well studied in other tasks such as NLP and CV. Therefore, the technical novelty is inadequate as the paper seems to be an application of transformer in job prediction task.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Presentation of the paper is quite clear, but the techincal novelty from the CS perspective is limited. ",
            "summary_of_the_review": "This paper leverages transformer in job prediction tasks and achieves well performance. However, some potential baselines in data mining are ignored. The technical contribution is inadequate as a direct application of transformer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_MpmD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_MpmD"
        ]
    },
    {
        "id": "95cpEZBsiua",
        "original": null,
        "number": 2,
        "cdate": 1666612153769,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666612153769,
        "tmdate": 1666666773453,
        "tddate": null,
        "forum": "lyjMArzIxH6",
        "replyto": "lyjMArzIxH6",
        "invitation": "ICLR.cc/2023/Conference/Paper3445/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper uses the transformer to leverage a sizeable online resume dataset by pretraining and then fine-tuning it on the small and carefully constructed longitudinal survey datasets. According to the results based on their experiments, their approach shows a significant improvement compared to the current state of the arts on the task of job sequence prediction. Besides, they also show that their approach can help a wage model to provide better performance.\n",
            "strength_and_weaknesses": "Strength\n+ Propose an inspiring method to apply the transformer to the prediction of labor data by pretraining the model on a large online resume dataset and then fine-tuning it on the small datasets.\n+ Conduct comprehensive experiments, including both cross-sectional and overtime experiments, demonstrating the usefulness of the approach.\n+ Well-written paper\n+ Reproducibility: Provide the source code with README and a detailed description of their experiments.\n\nWeakness\n- The main concern regards the technical novelty of the paper: The authors only made two minor changes to the transformers used in NLP.\n- Some related studies are missing. The used baselines are quite old.",
            "clarity,_quality,_novelty_and_reproducibility": "The presentation of the paper is quite clear, but the technical contribution is limited.",
            "summary_of_the_review": "As I mentioned in the previous parts, the authors deal with an interesting problem with the transformer-based models. The main concern regards the technical novelty of the paper, as the authors only made two minor changes to the transformers used in NLP. Moreover, some related studies in IR or data mining are missing, and the compared baselines are quite old.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_ZmR7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_ZmR7"
        ]
    },
    {
        "id": "4s561Hl-M2",
        "original": null,
        "number": 3,
        "cdate": 1666625480526,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666625480526,
        "tmdate": 1666625480526,
        "tddate": null,
        "forum": "lyjMArzIxH6",
        "replyto": "lyjMArzIxH6",
        "invitation": "ICLR.cc/2023/Conference/Paper3445/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposed and developed a clear and detailed transformer-based model called CAREER that uses transfer learning\nto learn representations of job sequences. The CAREER system was pretrained on a dataset of 24 million resumes and it is capable of outperforming standard econometric models for predicting and forecasting occupations. \n\n",
            "strength_and_weaknesses": "The paper is well written and very detailed description of both the development and operations of the model was provided. The appendix data was provided to clear the reproducibility concerns. \n\nWeakness: \n1. The deployment of this model was not presented \n2. The claimed incorporation of the model into the wage prediction models was not demonstrated \n",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the work is OK. The mathematical models are clearly explained. \n\n",
            "summary_of_the_review": "The paper gave both a good theoretical background and a practical application of the model. However, the deployment for end-user operations was not shown. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_DtGV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3445/Reviewer_DtGV"
        ]
    }
]