[
    {
        "id": "YKgmUEja4HP",
        "original": null,
        "number": 1,
        "cdate": 1666570291159,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666570291159,
        "tmdate": 1666570291159,
        "tddate": null,
        "forum": "VLnODGVVAsL",
        "replyto": "VLnODGVVAsL",
        "invitation": "ICLR.cc/2023/Conference/Paper3563/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a framework FedAMD, that disjoints the partial participants into anchor and miner groups. Clients in anchor group target to discover the bullseyes based on their local data distribution, while clients in the miner group perform multiple local updates and finally drive the update of the global model guided by the global bullseye. Under the partial-client scenario, this paper gives many theoretical proofs that FedAMD achieves sublinear speedup under non-convex objectives and linear speedup under the PL condition. Experimental results demonstrate that FedAMD is superior to SOTA works compared with BVR-L-SGD, FedAvg, FedPAGE, SCAFFOLD on Fashion MNIST.",
            "strength_and_weaknesses": "Strength:\n1.The proposed FedAMD achieves sublinear speedup under non-convex objectives and linear speedup under the PL condition.\n2.The proposed FedAMD is superior to SOTA works compared with BVR-L-SGD, FedAvg, FedPAGE, SCAFFOLD on Fashion MNIST.\n3.The proofs of theories are detailed and correct, and they are consistent with the experimental results.\n\nWeaknesses:\n1.It seems that there is no analysis about the influence of initial model (line 1) on the experimental results in this paper. In fact, the influence of initial model exists (assuming that the best model is set at the beginning). It is suggested to add analysis about the influence of initial model.\n2.The experiment is too simple. \"We train a convolutional neural network LeNet-5 using Fashion MNIST\". Such a specific experiment may affect the degree of confidence of the experiment results.\n3.This paper proposes two strategies(constant and sequential) for selecting anchor and miner groups. But it would be better if there were a strategy based on the performance of each client.\n4.The analysis of g(i)t,k (line 19)is reasonable, but it is worth discussing whether a factor \u03b1 is needed for second and third items.\n5.For the analysis of Figure 1, it seems that only Communication Rounds=400 is considered(why is 400?). Such analysis seems to be one-sided, and there are other considerations in fact, such as the change rate of the training loss.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: The proposed FedAMD is superior to state-of-the-art works, and there are many theories to support the experimental results in this paper.\nClarity: The algorithm 1 is detailed and reasonable, but some lines(line 1 and line 19)need additional analysis and discussion.\nOriginality: This work may be the first work to analyze the effectiveness of large batches under partial client participation.\n",
            "summary_of_the_review": "In general, it is a good paper. It has the SOTA results, many theoretical proofs and analysis. However, there are some weaknesses in the setting and analysis of the experiments section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_UWdf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_UWdf"
        ]
    },
    {
        "id": "ntAG4Ck5qz",
        "original": null,
        "number": 2,
        "cdate": 1666599548202,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666599548202,
        "tmdate": 1670623988042,
        "tddate": null,
        "forum": "VLnODGVVAsL",
        "replyto": "VLnODGVVAsL",
        "invitation": "ICLR.cc/2023/Conference/Paper3563/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the problem of improving convergence in Federated Learning especially under partial client participation settings. Differently from previous work, which either used large batch sizes or variance reduction techniques while resorting to full client participation, the authors propose a randomization of FedAvg and FedSGD. In that, the clients switch from being either anchor or minor groups. Convergence results are derived for non-convex losses under general conditions. Experimental results illustrate the efficacy of the approach. ",
            "strength_and_weaknesses": "Strengths\n+ The paper is very well written and the algorithm is well motivated.\n+ The convergence results show an $O\\left(\\frac{1}{\\epsilon}\\right)$ for general non-convex losses and linear convergence with the extra PL assumptions which is an improvement over the state of the art.\n+ The proposed algorithm FedAMP also shows the efficacy of schemes which resort to heterogeneous batch sizes and updating schemes while resorting to variance reduction.\n+ The experimental results are comprehensive in terms of comparison with other baselines and tends to outperform other baselines.\n\n\nWeaknesses\n\n- Though the algorithm claims to use partial client participation; it requires a full client participation at the first step which in turn requires each gradient from each client to be individually available to the server and that too persisted over time instead of being available in an aggregated manner. This severely undermines the privacy-preserving nature of FL as it makes each client susceptible to gradient inversion attacks. It is also not clear how can this be alleviated without resorting to additional storage in each device.\n- The convergence results while improving in terms of $\\epsilon$-accuracy of the error, is substantially worse as compared to other baselines in terms of dependence on the total number of clients. While other algorithms scale inversely with respect to the total number of clients, FedAMP scales linearly with the number of clients in the setup. Unless, the number of selected clients is always a function of the total number of clients, it seems the performance of FedAMP gets worse with increasing number of clients, if the number of clients selected per round is constant. It is also worth noting the performance benefit comes at the expense of additional assumption 3.\n- An inherent assumption made by the authors is that the number of samples across clients is the same. The method by which a client is assigned to being an anchor or minor is solely decided by a coin toss and not based on the number of samples. In the general case, some clients may have fewer samples, in the case of which the number of local epochs and the batch size can't be uniform. It is also not clear how the choice $b$ and $b^{'}$ affects the algorithm performance.\n- The authors miss comparing and contrasting with FedNova by Wang et.al., which especially takes care of the case where different clients undergo different number of rounds along with the flexibility of running different local solvers. It would be good to add comparisons with FedDyn and FedOPT which are highly performant baselines.\n- While a theoretically optimal $p$ is derived, it can't be derived in practice as quantities such as the number of samples on each device and $\\sigma^{2}$ is unknown to the server. \n- For the variance reduction phase, it requires additional storage of the past gradient which is the same size as that of the model. This can be particularly prohibitive with larger model and resource constrained devices.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written, easy to understand in terms of clarity and is of quality. The paper modifies well known techniques used in the context of FL and carefully tunes them so as to attain better rates. Hence, the originality is moderate.",
            "summary_of_the_review": "The paper has weaknesses in terms of the convergence proofs with the dependence on the total number of clients being significantly worse as compared to other schemes.  The applicability, usability and originality of the paper is undermined by some assumptions which either don't hold or are difficult to verify in practice. Moreover, the randomization of a client either doing a local-SGD style update or no update, can lead to stretches with very few clients contributing to the update. Having said that, I would be more than happy to increase my score if my concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_Moju"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_Moju"
        ]
    },
    {
        "id": "1Ah36DRJdF",
        "original": null,
        "number": 3,
        "cdate": 1666859850970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666859850970,
        "tmdate": 1666860025829,
        "tddate": null,
        "forum": "VLnODGVVAsL",
        "replyto": "VLnODGVVAsL",
        "invitation": "ICLR.cc/2023/Conference/Paper3563/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a new federated learning algorithm which allows clients to perform their updates on the model using the gradient of the model obtained by other clients. This involves data samples stored on other clients memory in updating the model locally by a client. The paper provides convergence analysis for the proposed algorithm and examines its performance through experiments.",
            "strength_and_weaknesses": "Strength: The paper proposes a novel way to enable clients to share their gradients and use this information to update the model locally. The paper proves the convergence of the proposed algorithm and provide convincing experimental results to validate the theoretical analysis provided in the paper.\n\nWeaknesses:\n\n- Using the proposed algorithm, the server has to store a caching gradient matrix whose dimensions scales with the number of clients. When the number of clients is so large, to implement the proposed algorithm a server with a very large memory is required.\n\n- It is beneficial for the paper if the convergence rate of the proposed FedAMD is compared to FedAvg to see what are the benefits of FedAMD compared to FedAvg in terms of convergence. FedAMD requires larger memory from the server side and it is interesting to see if FedAMD uses this memory successfully to obtain better convergence rate than that of FedAvg.\n\n- From reading the Algorithm 1 on page 4, the role of cached gradients in updating models is not clear. It would be great if Algorithm 1 can be revised such that the role of cached gradient become obvious.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and provides a novel algorithm. I could not check proofs carefully.",
            "summary_of_the_review": "In summary, the paper proposes a new federated learning algorithm at the cost of the need for a server with a large memory capacity. The paper proves the convergence of the algorithm. However, the paper lacks a detailed comparison of the proposed algorithm's convergence with the convergence of FedAvg. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_B281"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3563/Reviewer_B281"
        ]
    }
]