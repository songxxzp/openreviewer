[
    {
        "id": "SVnI_bsynt-",
        "original": null,
        "number": 1,
        "cdate": 1666424107174,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666424107174,
        "tmdate": 1666463565657,
        "tddate": null,
        "forum": "uzFQpkEzOo",
        "replyto": "uzFQpkEzOo",
        "invitation": "ICLR.cc/2023/Conference/Paper5071/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper analyzes the gradient flow learning of a particular mean-field (MF) network with more than 2 layers and shows that it can learn a radial target function (and a specific data distribution) with width polynomial in the data dimension $d$. This target function was previously shown to be inapproximable by 2-layer networks whose widths are $poly(d)$.",
            "strength_and_weaknesses": "\nThe result of the paper constitutes an interesting example where it shows the algorithmic advantage of a deep network against a shallow network. Global convergence results in the MF literature typically show (multilayer) neural nets can learn a certain generic class of target function or distribution, often without a rate and without an explicit comparison against shallower models. Here the paper focuses on a very specific setting and pulls through the main technical hurdle, which is to show the convergence time $T = poly(d)$ while avoiding the typical Gronwall\u2019s inequality bound on the width whose dependency is exponential in time, hence achieving a $poly(d)$ width guarantee. Though I haven\u2019t read the proof carefully, I quite like the analysis that takes a keener look at the role of the gradient flow in Lemma D.12 and D.14, which in particular helps keep the bound on $\\frac{d}{dt}(\\delta_{1,L_2}^{(2)})^2$ on the smaller, useful order (i.e. $\\frac{d}{dt}(\\delta_{1,L_2}^{(2)})^2 \\ll \\delta_{1,L_2}^{(2)}(\\delta_{1,L_\\infty}^{(2)})^2$ instead of $\\frac{d}{dt}(\\delta_{1,L_2}^{(2)})^2 \\ll \\delta_{1,L_2}^{(2)}\\delta_{1,L_\\infty}^{(2)}$, which would have been bad).\n\nThere are a few items that should warrant attention:\n\n- The type of MF networks proposed by the paper is actually a known idea. It is basically a concatenation of multiple 2-layer MF networks. Similar ideas exist, for example, MF Resnets in a paper by Jianfeng Lu, Lexing Ying and coauthors, which is one of the cited references. As far as I know, the same proposal was discussed at least in one talk by Eric Vanden-Eijnden.\n\n- There are several important parameters that have been chosen so as to \u201chint\u201d the learning at the desired solution. These choices could be unnatural. In particular, the initialization of $w_2$ is chosen to be very small and basically close to 0. As a result, this hints the network to learn an atomic solution. The initialization of $w_1$ (given by $\\sigma_1 = 1/\\sqrt{d}$ in Lemma C.1) and the unnatural architecture $F(x) = \\vert w \\vert \\sigma(w\\cdot x)$ again hint directly at the solution, as $\\bar{F}(x; t=0) = \\vert x \\vert$. Due to the small initial $w_2$, in the first stage, $w_1$ has almost no movement. Hence in the first stage, the network $F(x)$ stays \u201cat the right place\u201d, which is necessary for it to stay again at the right place in the second stage, as shown on page 43. \n\n  Zooming out, one can see that with the choice of parameters, most things are already at their right places from the beginning (the first layer $\\bar{F}$, the spread of $w_2$). Perhaps the only thing that has large movement through the learning is $\\bar{w}_2$, which is a scalar and does not quite convey the necessity of MF infinite-width networks. The same can be said about the bias term.\n\n- A more critical point is whether the proposed MF network is really important to establishing the result. The proof actually suggests that if the second width $m_2=1$, the result can be readily proven \u2014 perhaps with less technical work. That is, it is not necessary that one has $m_2\\to\\infty$. However when $m_2=1$, the \u201cMF\u201d part of the network would then be just a 2-layer MF network, not a multilayer one.\n\n  One may say it is more natural to consider large $m_2$ than $m_2=1$. But we should also recall the aforementioned design choices that hint at the desired solution. With these design choices, the consideration of large $m_2$ seems to demand heavier technical work and yet does not reveal insights on whether or why a large width is necessary.\n\n- The paper also advertises that the proposed MF network factors out neuron-permutation invariance due to the use of the distributions $\\mu_1$ and $\\mu_2$ in the representation, but the analysis does not make use of the representations $\\mu_1$ and $\\mu_2$. This is quite unlike the 2-layer MF literature, where the movement of the neuron distribution $\\mu$ is described by a Wasserstein gradient flow and where the analysis makes use of this fact.\n\n  More generally a typical MF framework goes by passing from the large-width MF network to the MF (infinite-width) limit and then using this MF limit as an analytical object. The advantage of this framework is that it removes the width out of the analysis and thus allows to exploit certain properties that only exist in the infinite-width limit. The paper does not follow this approach: it does not identify the MF limit and it analyzes directly the large-width MF network. Furthermore the analysis looks rather restricted to the particular problem setup under consideration. As such, at the time it is hard to judge whether this type of analysis can be expanded into a proper framework.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The result is new, though as said the proposed idea for MF networks is known. The writing of the paper is generally clear.\n\nOne thing to note is that the paper has not proved well-posedness of the gradient flow. For completeness, the paper should discuss this issue or at least state it as an assumption.",
            "summary_of_the_review": "The paper presents an interesting and encouraging result, though there are several downsides, including a somewhat restricted problem setting, the MF network idea being already known, several unnatural design choices of the neural net that already reveal quite a lot about the desired target function before learning takes place, and the questionable necessity of the proposed multilayer MF network.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_JRVm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_JRVm"
        ]
    },
    {
        "id": "l_P7IRnSB6",
        "original": null,
        "number": 2,
        "cdate": 1666577611271,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577611271,
        "tmdate": 1666577611271,
        "tddate": null,
        "forum": "uzFQpkEzOo",
        "replyto": "uzFQpkEzOo",
        "invitation": "ICLR.cc/2023/Conference/Paper5071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the benefits of Depth in neural networks with an emphasis on algorithmic separations. \n\nTypically, previous works have focused on proving depth separations by proving the existence of certain functions that on the one hand, can be easily approximated with a depth 3 neural net, yet cannot be approximated well by any depth-2 network. The paper here asks can we prove the analogous statement but with a learnable deep network. \n\nThe main result relies on previous work using a radially symmetric function (i.e., one that depends on the norm $||x||$ of the input) and importantly show how to train and converge to a network that provably approximates the function at hand. This requires several technical extensions of prior works to multilayer mean-field networks. The network studied is overparameterized and has polynomially many neurons. One interesting aspect of the analysis is to decompose the loss into the discretization loss (going from infinite width to fixed width) and the approximation error (w.r.t. to the hypothses classes and the function to be approximated).",
            "strength_and_weaknesses": "Strengths:\n+ nice technique that extends previous techniques based on mean field analysis of 1 layer.\n+ proving that one can train a network to find the hard-to-represent-by-shallow-nets function is itself a clean and nice result.\n+ it's interesting that his paper studies training of both the layers. Training dynamics for one layer while keeping the previous fixed, were studied but this paper deviates.\n+ perhaps the idea of truncating the mean field analysis, studying the introduced discretization error could be useful for future problems.\n\nWeaknesses:\n- the architecture itself is somewhat simple as the intermediate layer has dimension only 1. This suffices for learning the norm function, but it's not clear how crucial it is for the analysis. Would the analysis be able to be carried out for internal dimension d? What would the dependencies on d be?\n-Some omissions in the literature and how the paper compares to them: \n\n-What happens in the case where depth is not 3, but rather a parameter D and we want to learn a \"simple\" function? Telgarsky's paper on \"Benefits of depth in neural networks\" provides similar separation in the \"deep regime\" based on an admittedly very simple function, namely the triangle wave f(x)=2x for x in [0,0.5] and f(x) = 2(1-x) for x in [0.5,1]. \nCould your result somehow be made algorithmic even in the case of learning compositions of the triangle wave? \nMore generally, Chatziafratis et al. in \"Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem\" generalized Telgarsky's construction based on periodic 3 functions. Could this also be made algorithmic?\n\n-The authors should compare or at least mention connections to computational complexity and neural networks based on the work of Vardi, Shamir in \"Neural Networks with Small Weights and Depth-Separation Barriers\", where showing separations beyond depth 4 is connected to some basic questions in complexity.\n\n- The authors should also mention \"Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations\" by Awasthi et al. since they study the very related question on learning a ReLU net. Are the techniques used there similar to the ones in your paper?  ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, clear, and novel contribution, with a few omissions (see weaknesses) in the literature related to expressivity and optimization of ReLU nets. \nTypos:\n-page 3: see Equation Equation 5 --> drop one word\n",
            "summary_of_the_review": "The paper is a solid contribution to the theory of approximation/expressivity of neural nets. Many prior works have studied the depth separation question from an existential point of view, and this work shows that for some simple case (namely the radial function $||x||$), the result was indeed algorithmic. But to do so several technical obstacles are overcome in terms of the optimization process.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_5SVU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_5SVU"
        ]
    },
    {
        "id": "cwHLNt1NTS",
        "original": null,
        "number": 3,
        "cdate": 1666693914822,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666693914822,
        "tmdate": 1666693914822,
        "tddate": null,
        "forum": "uzFQpkEzOo",
        "replyto": "uzFQpkEzOo",
        "invitation": "ICLR.cc/2023/Conference/Paper5071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In (Safran et al. 2019), the authors describe a function which is $ReLU(1-\\lVert x\\rVert)$, which can be well approximated by a depth 3-network, while a depth 2-network requires $\\Omega(\\exp(d\\log d))$ width for the same. In this paper the authors show that, this separation is also, algorithmic, in the sense that one can learn the hard function $ReLU(1-\\lVert x\\rVert)$ using a depth 3-network in time polynomial in $(d, 1/\\varepsilon)$ using mean-field dynamics and an appropriate discretization scheme. ",
            "strength_and_weaknesses": "Strengths:\nPrior work by (Safran and Lee 2021), train GD on a depth 3-network, but for different activation functions other than the standard ReLU and their first layer-weights are fixed throughout the training. In this work, the authors use a splitting technique to deal with moving weights and also to ensure permutation invariance of the distribution of neurons. \nAfter initialization of the weights and under the infinite width regime they obtain a distribution over the weights, which is spherically symmetric and then they are able to show under an appropriate discretization scheme, they can transfer the results to a finite polynomial width network.  I think it is one of the few results that show polytime learnability with depth 3 network, albeit on a specific instance. Potentially some of the techniques could be of interest to the community.\n\nWeaknesses:\n1) As this a depth separation result, even though they train a deep network, the methods are more or less, specific to the particular function. Can the authors comment on whether there is a larger class of functions and input distributions such that a depth 3-network can learn in polytime?\n2) The equations in (4) describe the dynamics, please indicate what is $\\bar{v}_1$. One of the concerns is the assumption on the spherical symmetry of the distributions, which is satisfied in the beginning, but could the authors please clarify why it continues to be satisfied as the dynamics changes the weights? Does this always ensure that the assumption is valid and also when you discretize and deal with the finite width regime?\n3) Can the authors comment about dealing with polynomially many samples from the input distribution? Does one run standard GD, with gradient clipping in this case?\n",
            "clarity,_quality,_novelty_and_reproducibility": "It seems technically strong, but I feel it lacks some explanation when describing the dynamics for the infinite width. Some variables with bar on top does not seem to be defined or explained anywhere. I also feel the whole multi-layer mean field network can be described in the main paper just with D=1 and shift the generalized version to the appendix and focus a bit more on the loss partitioning and other novelties on the specific simpler architecture.",
            "summary_of_the_review": "Overall, even though it is tied to learning a specific hard function, I think the techniques could be useful to understand optimization with deeper architectures. So I feel it might be of interest to the DL theory community.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_wTzk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_wTzk"
        ]
    },
    {
        "id": "vJYbdggbctU",
        "original": null,
        "number": 4,
        "cdate": 1666695105609,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666695105609,
        "tmdate": 1666695105609,
        "tddate": null,
        "forum": "uzFQpkEzOo",
        "replyto": "uzFQpkEzOo",
        "invitation": "ICLR.cc/2023/Conference/Paper5071/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work studies the problem of learning the target function $f(x) = \\rm{relu}(1+||x||)$ under the following setting:\n\n- *Data*: Data is sampled from a specific distribution $x\\sim\\mathcal{D}$ (see Section 2, \"*Target Function and Input Distribution*\")\n- *Model*: A three layer neural-network with low-rank second layer weights.\n- *Algorithm*: Clipped gradient flow on the population mean-squared error, with small random initialisation.\n\nDespite specific, the interest in the target (and data distribution) above lies on a recent depth separation result by [Safran et al. '19] showing that it requires *at least* three-layers to be approximated with polynomially many neurons. The key theoretical result in this work is to show that indeed it can be learned algorithmically, i.e. the low-rank architecture converges to the target under clipped gradient flow from random initialisation in polynomial time.\n\nOn a technical note, the proof strategy introduces a mean-field approximation for multi-layer neural nets with low-rank middle layers which can be of independent interest.\n",
            "strength_and_weaknesses": "**Strengths**:\n- [+] Algorithmic separation and convergence rates results for neural-nets are scarce. Therefore, even if the setting is specific this is a very interesting result.\n- [+] The paper is well-written and easy to follow, even for a non-expert reader. In particular, the authors make an effort to intuitively explain every step in the proof, which is helpful.\n- [+] The extension of the mean-field limit to deeper (although low-rank) architectures can be of independent interest to the community.\n\n**Weaknesses**\n- [-] The proof is cumbersome, and despite the effort highlighted above, one sometimes get lost in the big scheme. It would be useful to have a bullet list in Section 2 summarising the key steps and pointers to the Lemmas / Theorems establishing them, e.g. \"First, we show that for spherical symmetry of initialisation is preserved by the first layer measure throughout the infinite-width dynamics\", etc.\n- [-] It is not so clear which of the assumptions needed for the result are crucial for the result to hold and which are not. e.g. what is the role of the cumbersome distribution of inputs?\n- [-] The setting is quite specific. However, as highlighted above I think this is a minor point.\n\n**Questions**:\n\n- **[Q1]**: The data distribution seems to play an important role in the theoretical analysis. Aside from the technical proof, would changing the distribution (e.g. $x\\sim\\mathcal{N}(0,I_{d})$ or spherical) completely change the phenomenology?\n- **[Q2]**: The separation of scales $T_{1}, T_{2}, T_{3}$ in the dynamics is an important element in the proof. If I understood correctly, these scales are independent of the hidden-layer widths $m_{1}, m_{2}$,and only scale with input dimension $d$. Can the authors clarify this scaling more explicitly?\n- **[Q3]**: Overparametrisation plays an important role in the proof. However, as noted by the authors in the discussion below eq. (1) a single second-layer neuron suffices to learn the target. Aside from the technical details of the theorem, can the authors comment on why overpamatrisation is benign to the optimisation in this setting?\n- **[Q4]**: If I understand it correctly, the initialisation of the first layer weights on the sphere is important to establish spherical symmetry of the first layer measure throughout the dynamics. Is this condition stable? For instance, would a small angular fluctuation (e.g. Gaussian initialisation) drives the dynamics away from the radial direction?\n\n**Comments**:\n\n- **[C1]**: I find Fig. 1 cryptic. First, the font size is very small and one needs to zoom a lot the pdf to read them. Second, the specific details of what is being plotted is not given. For instance: what are the red dashed vertical lines in Fig. 2 (left)? What is the input dimension $d$ in this simulation (this is quite relevant, since the authors claim convergence is polynomial in $d$)? Are these finite network simulations (if yes, what is $m_1$ and $m_2$?) or theoretical, infinite-width curves?\n\n- **[C2]**: In page 6, first equation $\\bar{v}$ is not defined explicitly.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: This work is clearly written and easy to follow even for a non-expert.\n\n**Quality**: The problem studied is theoretically relevant and the results are interesting.\n\n**Novelty**: This work builds on previous results on depth separation. However, it provides an algorithmic separation result which to my best knowledge is new and relevant.\n\n**Reproducibility**: Proof details are given in the appendix. However, code to reproduce the figures is not provided.",
            "summary_of_the_review": "This work studies the relevant problem of depth separation in neural networks. Despite considering a specific setting, it provides interesting results on the convergence of gradient flow for overparametrised three-layer neural networks in polynomial time. This sort of result is scarce in the theoretical literature. Therefore, I believe it is a submission of interest to the ICLR community.  \n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_i3qA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_i3qA"
        ]
    },
    {
        "id": "LY2BxoDMdT1",
        "original": null,
        "number": 5,
        "cdate": 1667428093648,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667428093648,
        "tmdate": 1667428093648,
        "tddate": null,
        "forum": "uzFQpkEzOo",
        "replyto": "uzFQpkEzOo",
        "invitation": "ICLR.cc/2023/Conference/Paper5071/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "\nThe paper uses a mean field type analysis on non shallow networks. The work is motivated by a result of Safran, Eldan, and Shamir (2019) which shows that shallow networks cannot capture sufficiently complex functions. The main result guarantees the learning of the function Relu(1-||x||^2), by a neural network with several multiple hidden layers with a polynomial number of units in each layer (which is a novelty)\n",
            "strength_and_weaknesses": "The paper is well written and is definitely interesting. My main concern is with (1) the relatively restrictive nature of the architecture and training machinery considered. The authors essentially focus on an example from Safran, Eldan, and Shamir (2019)  (which shows that shallow networks are limited in their expressive power and will fail to learn sufficiently complex functions such as Relu(1-||x||^2)) and consider a network that is designed specifically for that example which reduces the applicability of the result. (2) the fact that the readability/impact of the paper would benefit from a longer and more detailed exposition (see my comments below). This is a strong paper, why not submitting it to a journal?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear on the whole, neat and the result is definitely new (at least to the best of my knowledge). Some of the sections could be improved though as the reader sometimes has to go back several pages in order to retrieve the details of a notation.  \n",
            "summary_of_the_review": "\n\n- On page 2, last paragraph, the sentence \u201cRestricting the intermediate layer to have only one dimension \u201d is not very clear. What you do is your restrict the connections between the first and second layer  \n- In Theorem 1, I would recall the definition of the main parameters. This would clearly improve readability (Let d denote the input dimension, \u2026). Also, what do you mean exactly by poly(d, 1/eps)?\n- It would also be good to recall that gradient flow is an intractable algorithm in practice as it optimizes in the space of measure (which is infinite dimensional)\n- On page 4, when you introduce the distribution related to phi(x), it is not clear whether the impossibility to approximate f_* is related to the distribution or not. \n- Which loss are you using between (2) and (3) ? I would use distinct notations\n- In your main theorem, when you say gradient flow, you mean a flow algorithm on a polynomial number of particles, right? I think it would improve readability to say it that way or to add that somewhere. \n- page 6 first paragraph. what do you mean by \u201cthe change in norm is also uniform \u201d\n- your defintions of mu_1 and mu_2 are sometimes ambiguous. From what I understand, by mu_1 and mu_2 (at least when you derive the polynomial width), you mean multi-atomic distributions, don\u2019t you? if so, why not writing the corresponding decomposition mu_1 = sum_{i} alpha_i delta(w - w_i) somewhere?\n- On page 6, when you say that the dynamics of the first layer reduces to alpha, from what I understand, it is because you can initialize v1 to a given (let us say random) vector and then derive the update from the update on alpha ? I think you should expand a little more on this. E.g. provide one iteration, v_1 = v_1 + eta* d\\alpha/dt * (1/alpha)* v_1\n\n- On page 6, it took me some time, to get the flow on alpha and I think (although everything is sound) it would be good for the (general) reader  to recall the meaning of \\cdot{alpha} here. I.e. the fact that \\cdot{\\alpha} denotes the variation of alpha through the gradient iterations. Perphaps you could add something like alpha(t+eta) = \\alpha(t)+ eta d alpha/dw_1  * dw_1/dt and recall that dw_1/dt is the evolution of w_1 throughout the gradient iterations. I.e. indicate that dw_1/dt is computed as w_1(t+eta) = w_1(t) + eta* dL/dw_1\n- It would be good to clarify the notation E_x (Given that you have other averages with respect to the distribubtions mu_i and you sometimes use \\mathbb{E} without subscript). Perhaps add a line somewhere such as E_x \\left\\{ f(x)\\right\\} = \\int_{-\\infty}^{\\infty} f(x)\\; dx\n- On page 7, when you discuss the vanishing of the gradients, it would be easier for the reader to recall the definitions of f and F   \n- On page 7, the sentence \u201cAs a result, when f decreases sufficiently fast, f will become 0 before \u2225x\u2225 becomes large\u201d is unclear.\n- On page 7, when you introduce the times T_{1,1}, T_{1,2} and T_{1,3}, those times are critical to the analysis, I\u2019m wondering if they should not be introduced as a list. Moreover, just before the statement of lemma 4.2, when you introduce the T_{1,i}, I would emphasize that the projectors are not needed anymore after T_{1,1}. At the end of the day this is the point of the section. \n\nMy general comments for lemma 4.2. are the following: The statement of the lemma is perfect for a journal but there are too many details for conference proceedings. I would make it even more informal and simply say something like the first phase of the optimisation is characterized by small changes in alpha and (v_2, r_2) (including small changes in the mean \\overline{w}_2 and spread \\|v_2 - v_2\u2019\\| across the layer). Then if you really want you can give the estimates on \\overline{w}_2, r_2 and alpha but you should explain why those are important. \n\n- In lemma 4.2., is \\overline{v}_1 the same as \\mathbb{E} w_1 ?\n- In lemma 4.2., how do you control the variance \\mathbb{E}\\|w_1\\|^2 ?\n- In lemma 4.2. \u201cspread of the second layer\u201d and \u201cregularity condition\u201d, when you say for all (v_2, r_2) \\in \\mu_2, I guess you mean for all (v_2, r_2) \\in \\supp(\\mu_2) \n\n- On page 7, if I understand well, T_{1,1} is the stage where you still need the projectors ? if so I would indicate it. \n- In the statement of Lemma 4.1 I would recall the meaning of R_{v_1}, R_{v_2} and R_{r_2}.\n- On page 7, you use \u2018f(x)sigma\u2019(v_2 F(x) + r_2) \\approx f(x) \u2018 is that because d/dx relu(x) =1 on the positive x ? if so then what does the approximation mean?\n- On page 7, \u201cSince f is much flatter than f\u2217 , the RHS is always negative\u201d ? What does that even mean? I don\u2019t understand why the flatness of f_* vs f implies that f_* - f will be negative?\n- In fact at the end of p7 you say \u201cIn fact, we show that it is \u2212\u0398(\u03b1 log d) = \u2212\u0398(logd/d1.5) \u201c Where do you show this ?\n- \u201cOne also needs to show that \u03b42 cannot change much during Stages 1.1 and 1.2.\u201d \u2014> Why ?\n- I would recommend changing the sentence \u201cthe dynamics of v2 is approximately uniform in Stage 1.1 and Stage 1.2, and \u03b42 does not change much\u201d to the distance between successive iterates (v_2\u2019, r_2\u2019) and (v_2, r_2) does not change much\n- At the beginning of page 8, the sentence \u201cRecall that stages 1.1. and 1.2. only requires\u201d \u2014> \u201crecall that T_13\u201d corresponds to the time at which |w_2| = Theta(d)\\delta_2\n- \u201cWe can make \u03c32 small by selecting a small enough \u03c32\u201d the sentence does not make sense. I would replace by something like \u201cRecall that sigma_2 refers to the variance of the Gaussian distribution used to initialize w_1, by taking this parameter small enough, we can guarantee that the initial value of w_1 will be small enough which can in turn be used to control the deviation between the iterates\u201d or even better \u201cthe deviation between the iterates in v_1 and r_2 depend on the value of sigma_2 (which is used to initialize the weights w_1 and which can be taken sufficiently small to control this distance)\u201d\n- I don\u2019t see why the fact that \u201cv_2\u201d has a uniform dynamics (i.e. the successive gradient steps are approximately constant) implies that delta_2 will be small. This implies delta_2 will be approximately constant but not necessarily small. I might be missing something here\n\t\n- I think the paragraph at the end of page 7 and beginning of page 8 should be rewritten or removed. You are trying to explain a proof which you can\u2019t expand and that makes the paper unclear. Honestly I think the best would be to remove this and replace your statement of lemma 4.2 with something very informal\n\n\nPage 8\n\n- on page 8, it is not clear to me why \\overline{w}_2 does not depend on \\delta_2\n- on page 8, I guess when you say \u201cthe length of Stage 1.1 and Stage 1.2 is proportional to \\delta_2\u201d what you mean is the total variation in the weights? if so it might be helpful for the reader to specify which weights\n- Generally speaking the part where you explain lemma 4.2. is not clear  \n- When you explain how you control the \n- In the last paragraph of page 8, perhaps you could recall the definition of \\tilde{f} ?\n\n===================================\n\nAdditional typos:\n\n===================================\n\n\n- page 2, end of paragraph 2: \u201chas also been in used\u201d \u2014> \u201chas not been used\u201d or \u201c\u201dhas not been in use\u201d (although the second is less correct)\n- page 7, \u201cStages 1.1 and 1.2 only requires\u201d \u2014> \u201conly require\u201d\n- page 8, beginning of the page : \u201cif \u03b42 remain relatively constant\u201d \u2014> \u201cif delta_2 remainS constant\u201d\n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_Lcb7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5071/Reviewer_Lcb7"
        ]
    }
]