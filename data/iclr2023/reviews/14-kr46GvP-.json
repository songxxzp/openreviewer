[
    {
        "id": "CHf3l9_4yTU",
        "original": null,
        "number": 1,
        "cdate": 1666585896938,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666585896938,
        "tmdate": 1668874225799,
        "tddate": null,
        "forum": "14-kr46GvP-",
        "replyto": "14-kr46GvP-",
        "invitation": "ICLR.cc/2023/Conference/Paper5916/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In the paper under review, it is shown that the TD for validation data (validation TD) correlates with the performance of the RL + regularization methods in high UTD (replay ratio) settings. \nBased on this finding, a method that selects the regularization method with the smallest validation TD from multiple regularization methods is proposed. \n",
            "strength_and_weaknesses": "# Strengths:  \n[Novelity] [Quality]:  \n* The comparison of several regularization methods in addition to methods such as Reset, DroQ, and Spectrum normalization, was performed on several benchmark tasks. This comprehensive comparison result would be a good experiment stack. \n\n* Through many experiments in several settings (eg., approx. offline learning based on the tandem learning framework), it is shown that validation TD correlates with actual performance. A unifying method (AVTD) in which the regularization method is selected in accordance with its validation TD score is proposed. It is shown that the performance of this unifying method is better than that of each regularization method. This kind of unification of RL methods is very important in practice, because, currently, the success of RL methods (+ hyperparameter values) is highly task-dependent.\n\n\n# Weaknesses: \n[Clarity] [Quality]:  \nThe paper is basically well-written, but there are typos and unclear points. \n\n> RL algorithm has been a important \n\n-> RL algorithm has been an important\n\n\n> Concretely, our method, AVTD, trains several off-policy RL agents on a shared replay buffer where each agent applies a different overfitting regularize.\n\nSince AVTD appears for the first time here, the name in its non-abbreviated form should be provided as well: our method, <non-abbreviated form> (AVTD)\n\n> In theory, these off-policy algorithms can be made very sample efficient by making sure the Q-network fits the current replay buffer well,\n\nneeds reference. \n\n> Our analysis analyzes a standard SAC agent in the high UTD regime. \n\n-> We analyze a standard SAC agent in the high UTD regime. \n\n> we plot the training and validation TD errors as well as the standardized TD gap (STD gap),\n\nWhy introduce a Standardized TD gap? (Why not just use validation TD?) Validation TD also correlates with performance, and the AVTD algorithm uses validation TD. So I don't see any motivation to introduce Standardized TD gap. \n\n> Figure 2\n\nWhy not merge it with Figure 1 and arrange the figures horizontally? In the current version, the legend and some captions are duplicated and consume space. \n\n\n> We would also highlight that the regularizers that achieve the lowest validation TD error offline are usually one of the top performing methods online (Figure 5)\n\nIt would help to understand the correlations more easily if there were figures summarizing the results shown in Figures 5 and 12. \ne.g., how many times did the method with the lowest TD error rank in 1st place in actual performance? and how many times did it rank in 2nd or lower places?\n\n\n> The solution of (Nikishin et al., 2022) which\n\n-> The solution of Nikishin et al. (2022) which\n\n> (Khadka et al., 2019) also\n\n-> Khadka et al. (2019) also\n\n\n> Algorithm 1. 10: After every nepisode episodes, collect a heldout trajectory and add to Dheldout with the same action selection strategy above for D.\n\nWhat is the size of the Heldout dataset \\mathcal{D}_{\\text{heldout}}? If the size is large, this impairs overall sample efficiency since we need to consume non-trivial numbers of samples for validation.\n\n\n\n> Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290, 2018a. URL http://arxiv.org/abs/1801.01290.\n\n> Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861\u20131870. PMLR, 2018b.\n\n> Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. The impact of non-stationarity on generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020a.\n\n> Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient non-stationarity and generalisation in deep reinforcement learning. arXiv preprint arXiv:2006.05826, 2020b.\n\n> Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Proceedings of the 36th International Conference on Machine Learning, 2019a.\n\n> Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning, pp. 2052\u20132062, 2019b.\n\n> Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. Advances in Neural Information Processing Systems, 32, 2019a.\n\n> Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In Advances in Neural Information Processing Systems, pp. 12498\u201312509, 2019b.\n\nDuplicate citations.\n\n> Spectral normalization (SN). For spectral normalization, we follow the implementation of\n\n-> Spectral normalization (SN). For spectral normalization, we follow the implementation of Gogianu et al. (2021)? Bjorck et al. (2021)?? or Miyato et al. (2018)???\n",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments on strengths and weaknesses.\n",
            "summary_of_the_review": "I am leaning toward acceptance.\nThe main strength of this paper is proposing a framework for unifying multiple RL methods (more precisely, regularization methods) and performing an empirical analysis of the framework.\nCurrently, the success or failure of RL methods (+ hyperparameter settings) is highly task-dependent. Thus, the framework to unify individual methods is very important. \n\nThe presentation of the paper seems to be not very much polished. \nMore careful modification of the presentation would further improve the paper's quality.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_zRMn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_zRMn"
        ]
    },
    {
        "id": "FRPGBtAPPyN",
        "original": null,
        "number": 2,
        "cdate": 1666614014016,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666614014016,
        "tmdate": 1666614014016,
        "tddate": null,
        "forum": "14-kr46GvP-",
        "replyto": "14-kr46GvP-",
        "invitation": "ICLR.cc/2023/Conference/Paper5916/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "[\"Long\" summary]\n\nThe paper deals with sample efficiency in deep reinforcement learning (DRL). In particular, there was a known problem in DRL with high update-to-data (UTD) ratio, addressed by several recent studies with different regularizers. The authors aim to understand why DRL usually fail with high UTD and how we can overcome this problem in general. Addressing this problem is a vital step toward data-efficient DRL. The paper first examined possible reasons that cause the failure, including data quality, non-stationarity and s distribution shift. The empirical results show that any reason alone cannot explain the failure. Therefore, the authors conjectured that the main cause is statistical overfitting.\n\nThen, to solve the statistical overfitting the paper proposed. An algorithm was proposed, in which multiple DRL models with various regularizers (each proposed by a previous study) are simultaneously trained while sharing one replay buffer. At each step, the model with lowest validation TD error is selected to compute the policy. Experiments were conducted to show the effectiveness of the algorithm. Furthermore, some additional experiments results are shown in Sec.5 to support the design choices.\n\n[In short]\n\nThe first contribution of the paper is the empirical results for better understanding the failure with high UTD, which are helpful and necessary for data-efficient DRL.\n\nThe second is the introduction of an algorithm to adaptively select the agent with proper regularizer to handle high UTD.",
            "strength_and_weaknesses": "[Pros]\n- The paper provides important insight about why DRL fail when trying to exploit the data by high UTD ratios. This is helpful and necessary toward data-efficient DRL.\n- The experimental evaluations are relatively comprehensive and well-defined.\n\n\n[Cons]\n- If I understand correctly, AVTD (Algorithm 1) needs to train all the off-policy agents, which means the computation cost is almost the sum of the individuals (K is the number of agents with different regularizers).\n- The overall performance of AVTD (Figure 9, aggregated normalized score) is almost the same as WD=0.01 and only marginally outperforms Reset. In particular, AVTD performed the worst in Acrobot swing-up. Practically saying, why should we use AVTD since it is less computationally efficient while not significantly better than baseline methods.\n\n\n[Questions]\n- What does \"AVTD\" stand for?\n- Page 13, SN, we follow the implementation of WHAT?\n- Is there any possibility to theoretical investigate the high UTD failure?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was written in good quality. The ideas are addressed clearly with support from rich empirical results. While no theoretical analysis is involved, the paper provides novel and significant empirical insights.",
            "summary_of_the_review": "The paper touches an interesting and essential problem in DRL: the failure with high UTD ratio using off-policy algorithms. I appreciate the paper because it provides insightful empirical results toward understanding the failure. Meanwhile, the proposed algorithm, AVTD, which aims to overcome the failure, does not demonstrate satisfying performance though it costs more computation. I think at least the first contribution of the paper is significant to DRL community. Hopefully the authors can figure out a better way to overcome the difficulty in the near future. Overall, I lean toward an acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_yYZT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_yYZT"
        ]
    },
    {
        "id": "GxlGCnC4R8",
        "original": null,
        "number": 3,
        "cdate": 1666641073348,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666641073348,
        "tmdate": 1666641073348,
        "tddate": null,
        "forum": "14-kr46GvP-",
        "replyto": "14-kr46GvP-",
        "invitation": "ICLR.cc/2023/Conference/Paper5916/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work systematically tests a number of possible reasons for off-policy DRL methods not performing as well as they potentially could.\nFrom this, the authors conclude that statistical overfitting is a key contributor to the poor performance.\nBased on this insight, they propose a method for keeping statistical overfitting low. This method is shown to successfully reduce overfitting and improve RL performance.",
            "strength_and_weaknesses": "This is a high-quality paper.\nIt begins with a clear starting question that is thoroughly addressed.\nThe experiments performed for this are clear and compelling.\nBased on the investigation, a concrete conclusion is drawn and a novel method is produced.\nThis method is novel and well-motivated.\n\nThe generated insights have the potential to be the foundation for further methods.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this work is a good example of all four of these qualities.\n\nIn addition to the three causes mentioned in the abstract, \"high variance\" is mentioned in the introduction. However, this is not addressed to the same degree within the paper.\n\nThe authors correctly note that the other possible reasons may still contribute but are not the primary culprit.\n\nThe experiment in Section 3.2 answers a core question and is a great inclusion.\n\nThe relationship to overfitting in the offline RL setting is glossed over.\n\nMinor Comments:\n- End of Section 1: \"[AVTD] attempts to counteract automatically select regularization schemes by hill-climbing\" is missing words.\n- Start of Section 2: The MDP is defined in terms of R, but then r is used later.\n- The authors could induce overfitting to varying degrees and compare the ranking of agents by overfitting vs ranking of agents by performance.\n- Section 3: \"has also been used previously [to] stabilize\" (missing word)\n- Section 3: \"bellman\" -> \"Bellman\"\n- Commas are generally missing\n\nQuestions:\n- How crucial is feature normalization to the results in Figure 3?",
            "summary_of_the_review": "This is a high-quality paper. It begins with a clear starting question that is thoroughly addressed.\nBased on the investigation, a concrete conclusion is drawn and a novel method is produced.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_rmvQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5916/Reviewer_rmvQ"
        ]
    }
]