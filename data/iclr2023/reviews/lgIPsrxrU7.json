[
    {
        "id": "OcvSnhSGM6",
        "original": null,
        "number": 1,
        "cdate": 1666154196697,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666154196697,
        "tmdate": 1666154196697,
        "tddate": null,
        "forum": "lgIPsrxrU7",
        "replyto": "lgIPsrxrU7",
        "invitation": "ICLR.cc/2023/Conference/Paper5443/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a new attack for categorical data by probabilistic distribution of non-adversarial samples, named Probabilistic Categorical Adversarial Attack (PCAA). Unlike conventionally expensive attack by greedy search, PCAA can convert the discrete optimization into continuous optimization and apply gradient based approach to generate stronger adversarial samples efficiently. Upon the attack, this work also provides a probabilistic adversarial training method against PCAA and other adversarial attacks.",
            "strength_and_weaknesses": "Strength:\n\n1. This work provides an attack/defense pair instead of attack alone to avoid potential risks on adversarial machine learning research.\n\n2. This work does not only provide PCAA theoretically in epsilon/delta language, but it also provides an efficient algorithm with experimental results.\n\nWeakness:\n\n1. Lack of theoretical explanation why PAdvT is also good to attacks other than PCAA.\n\n2. In algorithm 2, the adversarial training only depends on adversarial samples. What about also considering non-adversarial samples? I mean training with mixture of adversarial and non-adversarial samples.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: All definition, algorithm and writing in this work are clear.\n\nQuality: Both theoretical and applied aspects of attack and defense is good.\n\nNovelty: The attack is novel from probabilistic perspective.",
            "summary_of_the_review": "This work is a good work to cooperate theory and application together in the adversarial machine learning area. We should encourage more work like this.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_qsau"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_qsau"
        ]
    },
    {
        "id": "QWhekZ8qsdB",
        "original": null,
        "number": 2,
        "cdate": 1666449219850,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666449219850,
        "tmdate": 1666449219850,
        "tddate": null,
        "forum": "lgIPsrxrU7",
        "replyto": "lgIPsrxrU7",
        "invitation": "ICLR.cc/2023/Conference/Paper5443/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a probabilistic categorical adversarial training method, which conduct adversarial attack and adversarial defense on categorical data. The authors transform the discrete optimization problem on categorical data into a continuous problem, which is then solved using PGD. The proposed method can achieve a better balance between time complexity and performance, and the effectiveness is verified by comparisons on multiple datasets and multiple baselines.",
            "strength_and_weaknesses": "+Extends PGD based AT to categorical input data.\n\n+Achieve a better balance between time complexity and performance, including categorical adversarial attacks and robustness of adversarial training.\n\n-The technique novelty is somewhat limited, considering that it is only the expansion of PGD based AT on categorical input data.\n\n-The performance is not significant. The results show a better balance between time complexity and adversarial attack or defense robustness. However, compared with the existing technology, the proposed method neither achieves the least time consumption nor achieves the best adversarial attack or defense robustness.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality and reproducibility is ok. The nolvelty is somewhat limited, considering that it is an extension of PGD based AT, and there is no significant performance improvement.",
            "summary_of_the_review": "This paper extends PGD-AT well to categorical input data, and the main concern is the inability to achieve better performance than the existing technology.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_Vnth"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_Vnth"
        ]
    },
    {
        "id": "VR9JvZmbK2Z",
        "original": null,
        "number": 3,
        "cdate": 1666544337311,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666544337311,
        "tmdate": 1666544337311,
        "tddate": null,
        "forum": "lgIPsrxrU7",
        "replyto": "lgIPsrxrU7",
        "invitation": "ICLR.cc/2023/Conference/Paper5443/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper's objective is to efficiently generate adversarial examples for categorical data stronger than prior search-based methods. To tackle this problem, it proposes a Probabilistic Categorical Adversarial Attack (PCAA) that transfers the adversarial attack in the discrete space to an optimization problem in a continuous probabilistic space. Notably, it assumes that the features of adversarial examples follow a categorical distribution and aims to generate adversarial examples with the corresponding distribution that leads to high loss value and are closer to the clean adversarial examples.\n\nFurther, the paper proposes Probabilistic Adversarial Training that utilizes the proposed objective of PCAA for inner maximization in standard adversarial training. The experimental evaluation is conducted on three datasets, where the proposed method achieves a higher success rate with significantly lower running time for higher $\\epsilon$ values.\n",
            "strength_and_weaknesses": "## Strengths\n \n* The problem of generating adversarial examples and defending against them is essential, which has yet to gain the required attention in the community. The paper generated categorical adversarial examples by estimating their probabilistic distribution, which is simple and interesting.\n* The proposed method is well-written and easy to follow. The time complexity analysis between various attacks is well-described in the paper.\n* The attack experiments are conducted over three diverse datasets showing the efficiency of the proposed method.\n\n---\n\n## Weaknesses\n\n### Originality and quality\n* The practicality and usefulness of the proposed method need to be clarified. In Table 1, GS obtains the highest success rate for most cases; it is unclear why the adversary would prioritize the attack's efficiency at the cost of having a significantly weaker attack. Further, the reported time is in seconds, except for IPS, the computational cost does not seem too expensive to choose PCAA.\n* The paper highlights that PCAA is more efficient for higher budget attacks; however, the choice of higher $ \\epsilon $ needs to be better justified. I'd suggest including the comparison between the generated adversary examples for various attacks to motivate the requirement of robustness against higher epsilon for these datasets. \n* While the paper cites Bao et al. [1] in experiments, it lacks a discussion and comparison with this recent paper that analyses the classifier's vulnerability with categorical inputs via an information-theoretic adversarial risk analysis.\n* The paper needs to provide confidence intervals for any method for attack and defense evaluation; therefore, it is hard to conclude the improvement provided by PADVt in Figure 3, as PAdvT shows similar or worse performance than ASCC. \n\n\n### Clarity \n* The paper is not well-written and requires many changes to improve the readability for the readers:\n* The y-axis in Figure 3 and Figure 4 should be mentioned, which makes it difficult to read the plots. \n* The paper also swaps the X-axis and the multi-row in Figure 1 and Figure 2, making it difficult to read these plots.\n* For both figures, it would be easier to interpret them when represented as tables instead of statuettes.\n* For gradient search SR in LSTM, the SR for 0.32 is lower, is that an exception in that table or was it a typing mistake?\n* Instead of having one paragraph for ablation analysis and one subsection, I suggest combining them into one subsection with multiple paragraphs.\n\n### Reproducibility\nThe code was not released with the submission. I hope the authors can do that in the later version to promote the reproducibility of the attack and defense for future works.\n\n---\n[1] Bao et al. Towards Understanding the Robustness Against Evasion Attack on Categorical Data. ICLR 22\n",
            "clarity,_quality,_novelty_and_reproducibility": "The review above elaborates on all these points in detail.",
            "summary_of_the_review": "The paper tackles a significant problem; however, the usefulness of the proposed attack is unclear. Further, the proposed defense provides limited or no gains compared to prior works and are limited to NLP data, whereas the objective of the paper was to focus on more categorical tasks. The paper also needs improvement in the clarity of results and overall writing. Due to these reasons, I recommend a weak reject for the current version of the paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_j3K7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_j3K7"
        ]
    },
    {
        "id": "DLLAM9nACb",
        "original": null,
        "number": 4,
        "cdate": 1666711338029,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711338029,
        "tmdate": 1670994890095,
        "tddate": null,
        "forum": "lgIPsrxrU7",
        "replyto": "lgIPsrxrU7",
        "invitation": "ICLR.cc/2023/Conference/Paper5443/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "Adversarial attack on discrete (categorical) input is a challenging problem. This paper proposes PCAA, which converts the problem to a continuous optimization problem and solves it with gradient descent. It further shows that using PCAA for adversarial training (PAdvT) can improve the model robustness.\n",
            "strength_and_weaknesses": "Strength:\n\n- The idea of using cross-entropy loss and Lagrangian form is novel and interesting. \n\nWeaknesses:\n\n- This paper takes an L0 distance as the only constraint to adversarial examples, however it is insufficient for text classification tasks, and may also be flawed on other tasks. For example, replacing the word \u201cgood\u201d with \u201cbad\u201d will significantly change the meaning of a sentence by the L0 distance is just 1. \n- The adversarial probability distribution pi assumes that each feature is independent. However, it is not true in natural language and other types of input sequences. \n- Textual adversarial attack requires human evaluation to ensure that the adversarial sentences do not change the meaning (or the true label). That part is missing in this paper. \n- Attacking a text classifier seems to be an important use case for the proposed method. There are numerous works trying to improve the quality of the adversarial sentences. Those methods are not cited and compared in this paper. \n  - Jin et al., Is bert really robust? a strong baseline for natural language attack on text classification and entailment\n  - Li et al., Bert-attack: Adversarial attack against bert using bert\n  - Li et al., Contextualized perturbation for textual adversarial attack\n  - and many other papers. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The methodology is presented clearly. \n\nIt has some novelty in formulating the attack into an optimization problem. However, there are some issues with the formulation considering practical NLP or sequence classification tasks. There are issues with the experiments as well. \n\nThe source code was not submitted. \n",
            "summary_of_the_review": "The methodology has some novelty. However, there are still many issues with the problem formulation and experiments. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_H8X1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5443/Reviewer_H8X1"
        ]
    }
]