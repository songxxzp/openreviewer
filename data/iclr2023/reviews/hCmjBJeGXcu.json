[
    {
        "id": "TNYswXeuEC",
        "original": null,
        "number": 1,
        "cdate": 1666619626103,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666619626103,
        "tmdate": 1669397234936,
        "tddate": null,
        "forum": "hCmjBJeGXcu",
        "replyto": "hCmjBJeGXcu",
        "invitation": "ICLR.cc/2023/Conference/Paper5605/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work proposes SIMOL, a soft improvement based multi-objective optimization algorithm, for efficient meta learning. The proposed method first formulates meta learning as a multi-objective optimization problem, where each task is an objective to optimize. Then, to handle the huge number of tasks (e.g., can be up to 7 x 10^6) in meta-learning, it further leverages the soft improvement algorithm with mini-batch update as its scalable gradient-based optimizer for model training. Theoretical analyses and experimental studies have been conducted to show the good properties and practical performance of the proposed method.   \n",
            "strength_and_weaknesses": "**Strengths:**\n\n+ This paper is generally well-organized and easy to follow.\n\n+ To my understanding, this is the first work to consider meta learning as a multi-objective optimization problem that treats each task (the number could be huge) as a single objective to optimize. But I also have some concerns on this formulation (see weaknesses below). \n\n+ The proposed SIMOL method is simple and easy to implement. \n\n**Weaknesses:**\n\n**1. Novelty**\n\nThis work proposes to formulate meta learning as a multi-objective optimization problem. However, many similar works have already been done to leverage multi-objective optimization for multi-task learning [1,2,3]. Given the close relation between meta learning and multi-task learning [4], this formulation is straightforward.\n\nThe second contribution to dealing with the huge number of tasks is indeed interesting. However, the proposed scalable optimization method directly uses the improvement function [5] with the mini-batch update, which makes it like yet another multi-objective optimization method for MTL/meta learning. I also have some concerns on the proposed method's ability to deal with the huge number of objectives (see below).\n\n**2. Multi-Objective Optimization Formulation for Better Generalization**\n\nIt is unclear why the multi-objective optimization formulation could lead to a better generalization performance for meta learning.\n\n*i) Why Pareto solution on the training set is good for meta learning?* \n\nIn multi-task learning, the goal is to find a solution to maximize the performance for all tasks (objectives). Since the tasks could be conflicted with each other, it is reasonable to find Pareto solution(s) for the given problem. In meta-learning, however, the goal is to improve the fast adaption performance for new tasks that are not seen for the training (optimization) process. Why could a Pareto solution on the training set be good for better generalization performance for unseen tasks?\n\nIs it meaningful to formulate meta-learning as a multi-objective optimization problem with huge objectives, especially when the tasks we actually care about (e.g., new unseen tasks) are not in this huge set of objectives? A clear discussion on this motivation could be very helpful.\n\n*ii) Why could the simple linear scalarization approach be worse than other methods for meta learning?*\n\nTo my understanding, the simple linear scalarization approach can still find (stationary) Pareto solutions, although it is true that it cannot find any Pareto solution on the non-convex part of the Pareto front. Since the goal here is to find a single Pareto solution (but not to find the whole Pareto front), it is unclear why the solution found by linear scalarization will perform worse than the solutions found by other multi-objective optimization methods. Different Pareto solutions should be non-dominated with each other, and it is hard to tell which one should be better than the others, especailly for the performane of unseen tasks.\n\n*iii) Why the improvement function method could be better to handle meta learning with huge tasks?*\n\nThe improvement method used in this work is a non-scalarization multi-objective optimization algorithm, which aims to find a common descent direction for **all** objective functions. Many other multi-objective optimization algorithms also have the same goal but with different mechanisms. Given its min-max structure, the improvement method is closely relative to the Chebyshev method for multi-objective optimization [6] (now with equal preferences and adaptive reference point), which has been used for MTL [7,8]. Why could it have better performance for meta learning?\n\n*iv) Why a single Pareto solution could be good for all tasks?*\n\nClosely related to the previous points, since there is no single best solution for all objectives, most multi-objective optimization methods aim to find a good approximation (e.g., a set of representative solutions with different optimal trade-offs) for a given problem [6]. See the related approaches for multi-task learning [9, 10, 11] and also meta multi-objective reinforcement learning [12]. \n\nThe motivation for finding only one single Pareto solution to have the best overall performance for all objectives (and even the new objectives that the algorithm has never seen) is not solid. \n\n**3. Algorithm Performance**\n\nThe closely related work that takes meta-learning as multi-task learning [4] should be included as a baseline. Since this work is \"one step further\" from [4], why not directly develop the SIMOL method based on [4] which could be more efficient and straightforward than the bilevel formulation?\n\n**Much more importantly**, due to the opposite findings in some closely related work, a solid analysis could be needed to support the proposed algorithm's promising performance. The key motivation of this work is to use a multi-objective optimization algorithm to solve the meta-learning problem, and some promising performances have been reported for different problems. However, the current work on multi-task learning leads to an opposite finding. According to [13,14,15], a simple linear scalarization method can perform comparably or even better than all those multi-objective optimization methods for both supervised learning and reinforcement learning. The effect of fine-tuned hyper-parameters (rather than the multi-objective optimization formulation) could lead to significantly different algorithm performances. Due to the close relation between multi-task learning and meta-learning, strong and solid evidence and discussion should be provided to support the finding in this work. \n\n\n**4. Theory**\n\nThis work leverage the theoretical analysis from the improvement function method [5] to give the (Pareto stationary) convergence analysis, and Lin et al. [16] to give the convergence analysis for mini-batch update. However, the more important issues, such as the connection from the Pareto solution to better generalization performance for meta-learning, and the reason why the Pareto solution found by simple linear scalarization could have poorer generalization performance, are not discussed.\n\n**Other Comments**\n\n1. In Figure 1, if batch size =1, both mini-batch MGDA and SIMOL will only have one objective value (either f1 or f2) and its gradient at each step. There should be only a single term in (2) for SIMOL without any maximization comparison. Why did they perform quite differently in this case? \n\n2. It is also interesting to know the performance of the best-10% for different problems.\n\n3. It seems that a \")\" is missing in the definition of \\bar d^* two lines below (16).\n\n[1] Multi-task learning as multi-objective optimization. NeurIPS 2018.\n\n[2] Gradient surgery for multi-task learning. NeurIPS 2020.\n\n[3] Conflict-averse gradient descent for multi-task learning. NeurIPS 2021.\n\n[4] Bridging multi-task learning and meta-learning: Towards efficient training and effective adaptation. ICML 2021.\n\n[5] Interactive bundle-based method for nondifferentiable multiobjeective optimization: NIMBUS. Optimization 1995.\n\n[6] Nonlinear multiobjective optimization. Kluwer 1998.\n\n[7] Tchebycheff Procedure for Multi-task Text Classification. ACL 2020.\n\n[8] A Multi-objective / Multi-task Learning Framework Induced by Pareto Stationarity. ICML 2022.\n\n[9] Pareto multi-task learning. NeurIPS 2019.\n\n[10] Efficient Continuous Pareto Exploration in Multi-Task Learning. ICML 2020.\n\n[11] Learning the Pareto Front with Hypernetworks. ICLR 2021.\n\n[12] Meta-Learning for Multi-objective Reinforcement Learning. IROS 2019.\n\n[13] A closer look at loss weighting in multi-task learning. arXiv:2111.10603, 2021.\n\n[14] In Defense of the Unitary Scalarization for Deep Multi-Task Learning. arXiv:2201.04122, 2022.\n\n[15] Do Current Multi-Task Optimization Methods in Deep Learning Even Help? arXiv:2209.11379, 2022.\n\n[16] On gradient descent ascent for nonconvex-concave minimax problems. ICML 2020.\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:** The paper is generally well-written and well-organized.\n\n**Quality:** The quality of the overall paper is good. But the lack of discussion for some crucial concerns listed in the weaknesses makes the contribution unclear.\n\n**Novelty:** The multi-objective optimization formulation is a reasonable extension from closely related work, and the novel contribution on dealing with huge tasks for meta learning is not well supported by the current analysis. \n\n**Reproducibility:** Given the recently opposite findings on multi-objective optimization for MTL, there is a concern that the proposed method could not be robust for the meta learning problem.",
            "summary_of_the_review": "To my understanding, this is the first work to consider meta learning as a multi-objective optimization problem that treats each task (the number could be huge) as a single objective to optimize. However, due to the major concerns on the multi-objective optimization formulation, algorithm performance, and theoretical analysis, I cannot vote to accept the current manuscript.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_4MPs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_4MPs"
        ]
    },
    {
        "id": "U4AEoBrb6z",
        "original": null,
        "number": 2,
        "cdate": 1666817590068,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666817590068,
        "tmdate": 1666831863285,
        "tddate": null,
        "forum": "hCmjBJeGXcu",
        "replyto": "hCmjBJeGXcu",
        "invitation": "ICLR.cc/2023/Conference/Paper5605/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considers solving meta-learning (formulated as bilevel optimization with the inner level being tasks) using techniques from multi-objective optimization (MOO). Previous work along this line required computing gradients for each task at each iteration, which is expensive. This paper proposes an algorithm to find an $\\epsilon$-Pareto stationary point using only mini-batches of tasks, built on the concept of improvement functions from MOO. Experiments are done on linear regression, few-shot classification, and meta-reinforcement learning. The proposed algorithm SIMOL generally improves over MAML and Prototypical Networks without significantly increasing computational burden, unlike previous work.",
            "strength_and_weaknesses": "# Strengths\n- The worst task performance of SIMOL improves over those of vanilla MAML and Prototypical Networks without increasing the computational burden drastically. The overall performance with a PN backbone is also better.\n- The algorithm is well mathematically motivated, and theoretical analysis of convergence is provided along with experiments.\n- The paper is well written and the arguments are straightforward to understand. \n\n# Weaknesses\n- There are no ablation studies, e.g. on the architecture of the network\n- For the MAML backbone, the improvement of SIMOL is often not statistically significant\n- The convergence theory may not be tight; the rate is slower than indicated from experiments\n\n# Questions\n- Are gradient signs missing from line 5 of Alg. 1?\n- How are the hyperparameters chosen?\n- Why is PCGard not included as a baseline?\n",
            "clarity,_quality,_novelty_and_reproducibility": "# Clarity\nThe paper is written pretty clearly. The text is not difficult to understand, \n\n# Quality\nThe algorithm seems mathematically sound, although I did not closely check the proofs. The experiments are fairly complete in terms of evaluation metrics and diversity of environments, although no ablations are included. SIMOL provides improvements that are often statistically significant.\n\n# Novelty\nI am not completely up-to-date with the literature on multi-task optimization, but as far as I know the use of improvement functions for meta-learning is novel and presents a new class of algorithms.\n\n# Reproducibility\nThe authors do not provide code, but provide pseudocode and hyperparameters. I don't think reproducing the results would be an issue.",
            "summary_of_the_review": "This paper is a novel application of approaches from MOO to meta-learning, with the intuition that a Pareto optimality would also lead to faster adaptation at test time. The algorithm is mathematically motivated and analyzed, and shown to improve upon MAML with similar computational complexity. However, SIMOL does not always lead to statistically significant improvements and no ablation studies are carried out.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_f9rZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_f9rZ"
        ]
    },
    {
        "id": "ud-E4FhDTPc",
        "original": null,
        "number": 3,
        "cdate": 1667473750208,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667473750208,
        "tmdate": 1667473750208,
        "tddate": null,
        "forum": "hCmjBJeGXcu",
        "replyto": "hCmjBJeGXcu",
        "invitation": "ICLR.cc/2023/Conference/Paper5605/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes to formulate the meta-learning as a multi-objective optimization (MOO) problem instead of the traditional single objective problem where the sum of each task\u2019s loss is optimized. By doing do the compromising phenomenon can be avoided, but it also incurs a new problem \u2014 the current gradient-based MOO solvers cannot scale to a large number of objectives. Then the authors proposes a scalable gradient-based solver via the use of improvement functions. Besides, this algorithm is guaranteed to have theoretical convergence. Experimental results on few-shot regression, few-shot classification and reinforcement learning show that the proposed method, called SIMOL, outperforms several existing algorithms in terms of either efficiency or generalization. ",
            "strength_and_weaknesses": "Strength:\n\n1. This work is well motivated that reformulating the original problem to a MOO can avoid the compromising phenomenon.\n2. The overall logic of deriving the solution along the journey is quite clear: 1) The improvement function can provide a closed-form gradient for the original problem; 2) The closed-form requires  uniform sampling of the tasks, which can be parameterized as a neural network; 3) Optimizing the network requires another sampling of tasks to compute the gradient of the NN; 4) Theoretically, the gap between the full batch and mini-batch of tasks can be bounded, so we can safely adopt the SGD to optimize the NN and finally solve the MOO problem with pareto optimality.\n3. Empirical studies looks very promising that the proposed method works quite well on multiple tasks and is better than the competitors.\n\n\nWeaknesses:\n\n1. There are tons of math equations. The author may need to simplify or condense them a bit, and give timely interpretation for them. It is important to clearly extract and convey the message behind the equations to the readers.\n2. The fonts of Figure 1 and 2 are quite small that it is hard to read.\n3. Typos: e.g., \u201cthe converged solution may be comprised\u201d \u2014> \u201cthe converged solution may be compromised\u201d\n4. Immediately after Equation (7), it is logically not correct to say \u201cWe first rewrite the above as:\u201d, since Lemma 3.1 is to prove that (7) and (8) are equivalent, but before the proof, we don\u2019t know if that\u2019s true.\n5. Definition 2.1. and 2.2. may need to cite some classical literatures rather than those new ones, as these definitions should exist long time ago.\n6. In Table 3, SIMOL has much large variance than others in the MAML category. Would that be a concern?",
            "clarity,_quality,_novelty_and_reproducibility": "The overall idea is clearly conveyed, though the massive equations hurts the readability a bit.\n\nThe idea of converting the mete-learning problem as a multi-objective optimization sounds new, and the derived algorithm with theoretical guarantee is a solid contribution too.\n\nAlgorithm 1 seems easy to reimplement, so the results could be reproducible with some effort. ",
            "summary_of_the_review": "The paper seems to be the first to reformulate the meta-learning problem as a MOO to avoid the compromising problem. Then a gradient-based algorithm guaranteed to converge to pareto optimal solution is proposed. Overall, this is a solid contribution. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_sFDS"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5605/Reviewer_sFDS"
        ]
    }
]