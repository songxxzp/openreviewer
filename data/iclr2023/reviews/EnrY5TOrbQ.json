[
    {
        "id": "qgPYmAEels",
        "original": null,
        "number": 1,
        "cdate": 1666104102134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666104102134,
        "tmdate": 1669697086948,
        "tddate": null,
        "forum": "EnrY5TOrbQ",
        "replyto": "EnrY5TOrbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3194/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the behavior of a single ReLU neuron in the gradient descent training process. The main results include that first, a new class of distributions (called O(1)-regular marginals) is proposed to characterize the regularity of the input space distributions. This class includes multivariate Gaussian distributions, and does not require compact support. Second, under the proposed O(1)-regularity, and with constant step-size, Theorem 1.1 claims that in polynomial (of the input space dimension, and the error threshold) number of iterations, the expected loss converges to the optimal value subject to a small error threshold.\n\n======================\n\nupdate after the discussion session: we find that most of our concerns have been resolved. We are therefore happy to recommend acceptance of this paper.",
            "strength_and_weaknesses": "Strength:\n1. the introduction of O(1) regularity\n2. the better convergence results\n\nWeakness:\n1. For Lemma 4.3, please specify the limit process for the big-O notation.\n2. When we read Section 1.2, a natural question is whether the O(1) regular marginal conditions can be induced from a sufficient condition: the distribution Dx has a continuous and compactly supported density function, which takes a positive value at the origin? Whether, subject to scaling x|-> ax for a specific a>0 because of (iii), this density condition is also necessary? If not, would the O(1) regularity contain any meaningful application scenario that is excluded from the C0 density assumption?\n3. For the simplification right before Section 4, would the analysis be different if the optimizer \\tilde{v} appears at the boundary of H?\n4. Please provide the Appendix as cited in the paper. For the time being, we are using the arXiv version of this paper to review the Appendix, and the comments are made accordingly.\n5. Please specify the limit process for the notations \\Omega(1) and O(1), below Equation (9). The same problem is in (1), and at the beginning of Section 4.1.\n6. Since in the proof Lemma 4.2 is cited, please add the O(1) regularity assumption to Lemma 4.3\n7. The constant c2 plays no role in Lemma 4.2. I guess this constant can be removed.\n8. For the arXiv version (dated August 04, 2022, same as below). The last inequality of (11) of the arXiv version: the O(1) distribution is replaced by the Normal distribution. The same mistake is observed in Equation (20) of the arXiv version.\n10. The O(1) regularity assumption just assumes beta_4 is positive, which could be very small. So, in the last two paragraphs, from beta<q^(1/4)/beta_4 one can not derive |b_u|>1/2.\n11. The claim (page 10 of the arXiv version) \"Observe that when (b) holds Lemma 4.4 implies the loss is O(OPT)\" requires the specification of the definition of \"O(OPT)\", to be further checked. Also, why would this lead to Lemma 4.5?\n12. Lemma 4.6 (the arXiv version) is different from the cited source. In Lemma d.4 in [VYS21], the underlying distribution is assumed compactly supported, and the bound c of the distribution support is changed to the square root of d (space dimension) in Lemma 4.6. In this paper, the O(1) regularity does not guarantee compact support. I guess that such a difference may not affect the conclusion, but then the new lemma should be proved separately.\n13. The O(1) regularity should be assumed for Lemma 4.5, since in the proof Lemma 4.2 is employed. Also, the boundedness of w_t and v should either be proved or assumed.\n14. Line -3 on page 11 of the arXiv version: according to Assumption (i) in O(1) regularity, the bound should be beta_2, not 2.\n15. Line 9 on page 12 of the arXiv version: there should be a factor $\\sqrt{beta_2}$ on the right-hand side.\n16. Line -4 of page 12 of the arXiv version: if C_2 is larger than C_1 C_p^2, then C' is negative.\n17. Line 3 of page 15 of the arXiv version: C_G is not elsewhere claimed to be greater than sqrt(2).\n18. Line -5 of page 6 of the ICLR version: the existence of such w_t (in particular, w_0) should be assumed. In particular, if v=0, such existence is violated.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written in a clear style that is not difficult to follow. I doubt the quality of the paper because of the problems found above. There is a novelty since this paper claims better convergence analysis. Since this paper studies the theoretical behavior of algorithms, reproducibility is not applied.",
            "summary_of_the_review": "There are some mathematical mistakes. Appendix is claimed but missing.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "The paper studies machine learning theory, and there is no ethics concern.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_DGFB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_DGFB"
        ]
    },
    {
        "id": "jwvgNCODob",
        "original": null,
        "number": 2,
        "cdate": 1666645444530,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645444530,
        "tmdate": 1666645444530,
        "tddate": null,
        "forum": "EnrY5TOrbQ",
        "replyto": "EnrY5TOrbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies the problem of agnostically learning a ReLU function via gradient descent using samples drawn from Gaussian (or even a more general class of) distributions.  The main result states that under a suitable random initialization, if gradient descent is run for sufficiently many iterations on the population risk loss function, then it outputs a ReLU function the error for which is within a constant factor of the optimal error, i.e., the minimum value of the population risk (referred to as: OPT). This result does not necessarily need the bias of the ReLU to be zero and improves existing guarantees for gradient descent (Frei et al. 2020).",
            "strength_and_weaknesses": "Pros: The paper has the following strengths.\n\n1. The exposition is in general clear and easy to follow. The problem setup and related work are detailed nicely that makes it convenient for the reader.\n\n2. The analysis is an improvement over that of (Frei et al. 2020) since this latter work assumes zero-bias ReLU's and achieves an error bound of $O(\\sqrt{d \\ OPT})$ when the samples are drawn from a Gaussian distribution.\n\nCons: \n\n1. I believe that the related work is incomplete for gradient descent learning of ReLU functions. The work of Soltanolkotabi [1] considers this problem in the realizable setting (with zero-bias) with the inputs drawn from a Gaussian and derives error bounds for recovering the unknown parameter. It is not clear to me how easy it is to extend this analysis to the noisy teacher setting. Since [1] is well cited, it would be good to check other related works that cite [1] in order to be more complete in the literature review. \n\n2. I find it is a bit restrictive to analyze gradient descent on the population risk. While this is natural to do as a first step in the analysis and certainly non-trivial, I believe the main theoretical result should ultimately be the one for the finite sample setting and should ideally be presented in the main text. This analysis is currently relegated to Appendix D as Theorem D.1. I also have a question regarding Theorem D.1 later below.\n\n[1] M. Soltanolkotabi, Learning relus via gradient descent, NeurIPS 2017.\n\nFurther comments:\n\n1. In the statement of Theorem D.1, it is not clear to me how $|y| \\leq B_Y$ can be assumed since the input $\\tilde{x}$ is generated from a Gaussian. For instance, isn't this violated in the realizable setting where y is defined as a ReLU?\n\n2. I was trying to understand whether in the special case of the \"noisy teacher\" setting (or \"realizable\" setting) the analysis says something about the convergence of the gradient descent iterates to the ground truth parameter. However from Section 4.1, it seems to me that this is not the case since it is only mentioned that the iterates lie within a ball of radius $O(\\sqrt{OPT})$ around the ground truth parameter. But we know from [1] that iterates of projected gradient descent (on the empirical loss) converge linearly to the ground truth in the realizable setting.  Some more explanation in this regard would be helpful.\n\n3. In Section 1.1 just above Theorem 1.1, $\\tilde{v}$ is introduced but has not been defined till then. There is a small typo towards the bottom of page 3 in \"Vardi et al.\"\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written quite clearly in general and is easy to follow. The quality of the result is good as it improves upon the existing work of (Frei et al. 2020). There seem to be new ideas used in the analysis for gradient descent along with a novel initialization scheme which are to my knowledge novel. ",
            "summary_of_the_review": "I think the contributions of the paper are decent and improve upon the analysis of (Frei et al. 2020) for learning ReLUs. The paper is also written cleanly and is easy to follow. As mentioned under \"weaknesses\" and in \"further comments\", I do have some comments regarding some closely related work and other technical aspects which need clarification.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_na9i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_na9i"
        ]
    },
    {
        "id": "LtkdxOeQKa5",
        "original": null,
        "number": 3,
        "cdate": 1667101700725,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667101700725,
        "tmdate": 1667102305427,
        "tddate": null,
        "forum": "EnrY5TOrbQ",
        "replyto": "EnrY5TOrbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3194/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides a convergence analysis of gradient descent for learning a single ReLU function under Gaussian feature distributions. The paper focuses on the mean squared loss between the single ReLU function the label, but allows updating both the weight vector and the bias parameter of the ReLU function.\n\nThe main contribution is proving that the gradient descent algorithm starting from a random initialization converges to within a constant factor of the global minimum of the loss within polynomial (in input dimension) number of iterations.\n\nBesides the case of Gaussian feature distributions, the analysis can also be generalized to the O(1)-regular marginals.",
            "strength_and_weaknesses": "Strength:\n- The result of this paper improves upon a previous result for learning a single ReLU function by additionally incorporating the bias in the analysis. Further, the analysis shows that the convergent iterate is within a constant factor of the optimum, whereas prior result requires an additional factor of d (i.e., input dimension).\n\nWeakness:\n- The feature distribution is assumed to be drawn from a Gaussian distribution. While this can be relaxed to the case of (1)-regular marginals, generalizing further seems challenging (e.g., does the condition hold for mixture of Gaussians or anisotropic Gaussians?)\n- There are related works that consider the realizable setting of learning ReLU networks, which should perhaps also be discussed in the related work, e.g., here\u2019s one:\n\n\u201cLearning Distributions Generated by One-Layer ReLU Networks.\u201d Shanshan Wu, Alexandros G. Dimakis, Sujay Sanghavi. 2019.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and easy to follow for the reviewer. Though there\u2019re minor aspects that could be fixed, e.g., equation (14) and (16) are both over length.\n\nThe technical novelty could be better highlighted in the analysis. For example, how the analysis incorporate the bias and improve the approximation factor of the optimum value could be better emphasized during the proofs.",
            "summary_of_the_review": "Overall I think this is a solid paper that contributes to the learning of nonlinear ReLU functions. It expands prior result and clearly states the limitations too (i.e., limited to one node) for future work. As noted above there is some concerns related to missing several related works in the literature.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_4WPP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_4WPP"
        ]
    },
    {
        "id": "kfQ-uLz5l6Z",
        "original": null,
        "number": 4,
        "cdate": 1669598938232,
        "mdate": 1669598938232,
        "ddate": null,
        "tcdate": 1669598938232,
        "tmdate": 1669598938232,
        "tddate": null,
        "forum": "EnrY5TOrbQ",
        "replyto": "EnrY5TOrbQ",
        "invitation": "ICLR.cc/2023/Conference/Paper3194/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This work studies the convergence of gradient descent in the regime of learning ReLU activations agnostically for a class of distributions that contains the standard high dimensional normal distribution. In contrast with previous results, they consider the case where bias exists and apply gradient descent on the weight vector+bias. Their approach essentially connects the analysis of [1] and [2].\n\n[1]: Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient descent.\n[2]: Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient descent",
            "strength_and_weaknesses": "Pros:\n1. This is an important problem. Previous results have focused on the unbiased case, so this is somehow the first result for biased ReLU activations.\n2. The gradient descent happens in bias and the weight vector, which means that the naive idea of extending the dimension works\n\nCons:\n1.  The authors state in the abstract that this is the first algorithm based on gradient descent that achieves these guarantees when the bias is zero. But [3,4] apply gradient descent and achieve $O(OPT)$. The first work does gradient descent on a surrogate loss which differs from this work, where the gradient descent is applied to the standard objective. Meanwhile, the second work uses gradient descent on the same objective and achieves $O(OPT)$. So, I believe this sentence should be removed from the abstract. \n2.  This work considers the case where the ratio between $||w||_2$ and $b_w$ is bounded by a constant. The title and the abstract leave the impression that this work provides a theorem for any value of bias which is not the case. One of the main difficulties of learning ReLU activations is to sample enough points so that $\\sigma'(w^Tx+b)>0$  and $\\sigma(u^Tx+b')>>0$ ($u,b'$ are the best parameters for this instance) for a large portion of the points (see [5] theorem 4.2 for the intuition); with these assumptions, this is somehow easily satisfied (for standard Gaussian) with a good initialization (Section 5) because the mean and variance of the RV $u^Tx+b'$ is bounded by constants. \n\n\n\n\n[3]: Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R. Klivans, Mahdi Soltanolkotabi. Approximation Schemes for ReLU Regression\n[4]: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning a single neuron with adversarial label noise via gradient descent\n[5]: Gilad Yehudai, and Ohad Shamir. Learning a Single Neuron with Gradient Methods\n",
            "clarity,_quality,_novelty_and_reproducibility": "The work is sound and well-written.\n\np.9 has a broken reference.",
            "summary_of_the_review": "I think the contributions of this work are not sufficient for acceptance in ICRL. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_iUj1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3194/Reviewer_iUj1"
        ]
    }
]