[
    {
        "id": "kh_RkeyalQ6",
        "original": null,
        "number": 1,
        "cdate": 1666854541252,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666854541252,
        "tmdate": 1666854541252,
        "tddate": null,
        "forum": "dmWMfJeZMM",
        "replyto": "dmWMfJeZMM",
        "invitation": "ICLR.cc/2023/Conference/Paper3117/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper considered the problem of learning graph models in an unsupervised setting. The authors extended the state-of-the-art GLAD method to the unsupervised setting, and show its effectiveness in handling missing data by numerical experiments.\n",
            "strength_and_weaknesses": "Strength:\n\nThe authors extended the state-of-the-art GLAD method to the unsupervised setting. Numerical results demonstrated that the proposed method can robustly handle missing data.\n\nWeakness:\n\n1. The extension from GLAD to uGLAD is simple without the need to change the architecture. Therefore, the contributions of this paper are not significant. \n\n2. There are a large number of methods proposed for learning graph models in the literature. More compared methods are needed in experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow, but the contributions of the paper are not significant.",
            "summary_of_the_review": "The authors extended the state-of-the-art method GLAD to the unsupervised setting. However, such an extension is very simple to conduct, and it is not clear about its significance. The contributions and novelty of this paper are not significant, and experimental comparisons are not sufficient.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_32py"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_32py"
        ]
    },
    {
        "id": "sPTFyq4GVHu",
        "original": null,
        "number": 2,
        "cdate": 1666886216572,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666886216572,
        "tmdate": 1666886216572,
        "tddate": null,
        "forum": "dmWMfJeZMM",
        "replyto": "dmWMfJeZMM",
        "invitation": "ICLR.cc/2023/Conference/Paper3117/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends previous work (GLAD) and proposes a novel deep model, uGLAD, to recover sparse graphs from Gaussian data. The paper introduces the following extensions over the GLAD framework - (1) it extends the framework to recover graphs in unsupervised learning by replacing the loss function (2) it utilizes adaptive regularization parameters (3) by using uGALD in a multi-task setting, the paper describes a consensus strategy to handle missing data. The experiments in the paper show the uGLAD model outperforms traditional block coordinate descent (BCD) on graph recovery of both Gaussian and non-Gaussian (simulated) data.",
            "strength_and_weaknesses": "Strength:\n+ This work extends the GLAD model to unsupervised and multi-task settings. The adaptive parameter for each precision matrix element provides more thorough controls, and the inductive-biases-based model eases computational burdens. Overall, the method looks promising in theory, and the reasoning behind the extensions is valid and important for real-world applications\n\nWeakness: \n\n- The paper requires a more extensive and robust experimental setup to support its claims. \n\n- For example, in section (page 8), the authors state that the GLAD paper has shown that \u201cADMM and G-ISTA gave a similar performance as BCD.\u201d So the paper only compares uGLAD with BCD. But there are more sparse Gaussian graphical model estimators. For example, M-estimators [1] and thresholding [2,3] are other types of methods. Although some of these recently proposed methods induce the same regularization coefficients for all precision matrix elements, comparing uGLAD with them is worthwhile to validate adaptive parameter performs better. \n\n- Moreover, this paper tested uGLAD on multi-task data and data with missing values. However, uGLAD is also only compared with BCD. But there are more recent works for these two tasks. For instance, JGL[4] and FASJEM[5] estimate multi-task graphs. MissGLasso[6] and [7] estimate inverse covariance matrices from data with missing values. \n\n- The choice of the simulation is not well justified, and the experimental settings and the results need better descriptions to connect to the paper's contributions. \n\n- Real-world gene expression datasets now have features >25 (in the order of 100s and 1000s). How does the proposed method scale to the large feature size and more noise?\n\nMinor:\n\n- The paper has presented a paper directly from another work (with citation) - this practice is not common in the field and is often discouraged. It would be useful to modify the figure to highlight the novelty and contributions of this work. \n\n- References should have brackets around them when they are not being used as subjects of the sentence. \n\nReferences:\n[1] Yang, Eunho, Aur\u00e9lie C. Lozano, and Pradeep K. Ravikumar. \"Elementary estimators for graphical models.\" Advances in neural information processing systems 27 (2014).\n\n[2] Sojoudi, Somayeh. \"Equivalence of graphical lasso and thresholding for sparse graphs.\" The Journal of Machine Learning Research 17.1 (2016): 3943-3963.\n\n[3] Zhang, Richard, Salar Fattahi, and Somayeh Sojoudi. \"Large-scale sparse inverse covariance estimation via thresholding and max-det matrix completion.\" International Conference on Machine Learning. PMLR, 2018.\n\n[4] Danaher, Patrick, Pei Wang, and Daniela M. Witten. \"The joint graphical lasso for inverse covariance estimation across multiple classes.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76.2 (2014): 373-397.\n\n[5] Wang, Beilun, Ji Gao, and Yanjun Qi. \"A fast and scalable joint estimator for learning multiple related sparse gaussian graphical models.\" Artificial Intelligence and Statistics. PMLR, 2017.\n\n[6] St\u00e4dler, Nicolas, and Peter B\u00fchlmann. \"Missing values: sparse inverse covariance estimation and an extension to sparse regression.\" Statistics and Computing 22.1 (2012): 219-235.\n\n[7] Loh, Po-Ling, and Martin J. Wainwright. \"High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity.\" Advances in neural information processing systems 24 (2011).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper's logical flow is fluent, and the method is easy to understand.\n\nQuality: The experiment and result sections lack comparison with state-of-the-art methods and cannot support the paper's claims. \n\nNovelty: This work is an extension of the GLAD model. The methodology exhibits novelty but is not supported well by the results. \n\nReproducibility: The paper requires more details of the method and settings to run it to be more reproducible. ",
            "summary_of_the_review": "Overall, the ideas presented are interesting and useful. The paper, however, requires a more descriptive and robust experimental design and results to contribute significantly to the community. \n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_HUi2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_HUi2"
        ]
    },
    {
        "id": "Txw7U1ffHQA",
        "original": null,
        "number": 3,
        "cdate": 1666951920604,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666951920604,
        "tmdate": 1666951920604,
        "tddate": null,
        "forum": "dmWMfJeZMM",
        "replyto": "dmWMfJeZMM",
        "invitation": "ICLR.cc/2023/Conference/Paper3117/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes uGLAD, an unsupervised version of previously proposed GLAD paper, to learn the graph structure for undirected (Gaussian) graphical models. . Authors also added several functionalities on top, including the capability to handle multi-task and missing values. Empirical results show good performance. ",
            "strength_and_weaknesses": "Strength:\nEmpirical improvement seems significant. \n\nWeakness:\n- novelty: although the paper makes several contribution, they are mostly incremental or straightforward. New additions on unsupervised loss function, multi-task extension, and missing value imputation are standard and do not offer any new insight. \n- organization: related to novelty, it is not fully clear what problem authors are tackling after reading the paper. Different sections seem to solve different problems, and they seem to be put together just to fill the content of the paper. \n- Experiments are only done via one baseline method. Despite authors' claim on other baselines have similar performance, it was only done with specific datasets. Other baselines' results are important here.\n\nOther:\n1. citations are not done correctly.\n2. \"...we expect the linear convergence property holds...\" since you change the loss function, how can you ensure this is still the case?\n3. \n",
            "clarity,_quality,_novelty_and_reproducibility": "clarity: mostly clear\n\nquality: ok\n\noriginality: poor as  additions are standard.",
            "summary_of_the_review": "incremental improvement over GLAD. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_aant"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3117/Reviewer_aant"
        ]
    }
]