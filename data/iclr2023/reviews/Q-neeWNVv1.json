[
    {
        "id": "dDUR9s1lU9",
        "original": null,
        "number": 1,
        "cdate": 1666650961142,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666650961142,
        "tmdate": 1669497437026,
        "tddate": null,
        "forum": "Q-neeWNVv1",
        "replyto": "Q-neeWNVv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4765/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a sequential (agent by agent) policy gradient update algorithm (A2PO) in the MARL setting that guarantees monotonic policy improvement for each agent and also their joint policy. Theoretical guarantees are provided for monotonic improvement and experimental results also support the claims made in the paper. \n",
            "strength_and_weaknesses": "This paper addresses an important problem in MARL. In prior work, the non-stationarity due to simultaneous policy update of agents in the multi-agent setting gives rise to high variance in the gradients, requiring more samples for convergence of each agent. Instead, the proposed A2PO algorithm uses importance sampling to weight off-policy samples used in sequential update of agent policies for the different agents in the team. \n\nIn general, the paper is not easy to follow due to the use of several notations that are not properly defined before being referenced and therefore, forces the reader to go back and forth looking for the appropriate definitions. It would help to perhaps create a consolidated table for all the notations and symbols used in the paper. \n\nSpecific comments about notation: \n\n- Page 3, Section 3.2 - what does the superscript $\\mathcal{I}$ indicate in $\\mathcal{L}^{\\mathcal{I}}_{\\hat{\\pi}^{i-1}}$? \n\n- Page 6, first paragraph, second line after eq 6: $\\mathcal{C}(\\cdot,\\cdot)$ has been used without first defining it. The definition is given in Page 7. \n\nSec 4, Semi-greedy Agent Selection Rule: \u201cpurely greedy selection may lead to early convergence which harms the performance.\u201d - Could you please elaborate this point? \n\nSec 4, Adaptive Clipping Parameter: Could you please explain what is precisely meant by \u201cbalanced and sufficient\u201d clipping ranges? \n\nSec 5.2: The results in the ablation study are difficult to follow. The paragraph PreOPC compares to MAPPO and CoPPO with V-trace, also to HAPPO and RPISA-PPO : but these methods are not described in the main paper which makes it difficult to understand the results. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Although the proposed algorithm and the experimental results are interesting, the overall presentation of the paper is unsatisfactory. \n\n",
            "summary_of_the_review": "I would recommend rejecting the paper in its current form, primarily because the presentation of the material is difficult to follow and the paper does not seem to be polished overall due to several typos, even in the appendix, that make it cumbersome to read. \n\n===========\n\nAfter the rebuttal period, the authors' response to all the reviews has helped me understand the work more clearly. The authors have added a Table for consolidating all the notations used in the appendix. Although my initial reading of the paper gave me the impression that the paper is hard to follow and from a quick comparison of the latest version of the paper, it does not look like the authors have included any additional explanations in the text - I do not think the other reviewers had the same problem, so it might just be me. But I will agree that the authors have sufficiently addressed concerns regarding the main contributions of the paper in their response here to the reviewers and their experimental results are thorough and extensive. So, I am updating my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_N8aU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_N8aU"
        ]
    },
    {
        "id": "iWv6HaisElS",
        "original": null,
        "number": 2,
        "cdate": 1666656268779,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656268779,
        "tmdate": 1666656268779,
        "tddate": null,
        "forum": "Q-neeWNVv1",
        "replyto": "Q-neeWNVv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4765/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper briefly reviews previous works of sequential updating schemes in multiagent trust region algorithms and points out the limitations of these works, which are either sample inefficient or lack of monotonic improvement guarantee for each agent. To achieve both sample efficiency and monotonic improvement guarantee for each agent, it proposes a new algorithm Agent-by-Agent Policy Optimization (A2PO) and analyzes its monotonic improvement bound. ",
            "strength_and_weaknesses": "**STRENGTHS**\n\nThis paper does a sufficient literature survey. It is well-structured and easy to follow. Mathematical statements and proofs look sound. Experimental results show improvements over baselines. \n\nWEAKNESSES\n\n1) The motivation is a little confusing. Why should monotonic improvement for each agent be guaranteed? Is it possible that a better joint bound is obtained by giving up a monotonic improvement guarantee for each agent?\n\n2) The sample-inefficient claim for RPISA may not be right. It is not necessary to drop $(n-1)/n$ of the collected sample. For example, [1] is an implementation of RPISA, which is proven to be sample efficient. It will be helpful to have further discussions on these algorithms both theoretically and empirically.\n\n3) A2PO seems to take a lot of time in training. It is highly desired to elaborate on this issue.\n\n[1] Jianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards Global Optimality in Cooperative MARL with Sequential Transformation. ArXiv, abs/2207.11143, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "A sound work with a clear presentation. The method also looks novel.",
            "summary_of_the_review": "This work discusses the limitations of previous multi-agent trust-region algorithms and proposes an interesting method, which empirically shows outperformance. However, some further discussions are needed to justify the significance of this work. A final recommendation will depend on how questions in WEAKNESS are clarified.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_7L45"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_7L45"
        ]
    },
    {
        "id": "U6otwp7Ihd",
        "original": null,
        "number": 3,
        "cdate": 1667214938060,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667214938060,
        "tmdate": 1667548619939,
        "tddate": null,
        "forum": "Q-neeWNVv1",
        "replyto": "Q-neeWNVv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4765/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposed a new MARL training objective to improve training efficiency. In the experiment, the proposed method, A2PO, outperforms previous SOTA methods. ",
            "strength_and_weaknesses": "Strength:\n1. I admit this paper has good writing and analysis.\n\nWeakness:\n1. The wall time may be increased due to the multiple updates. The authors should compare wall time usage of different methods.\n\n2. The authors try to improve the sample efficiency problem of current MARL algorithms, but the performance improvements are not significant. Reasons below:\n\n- a. The benchmarks are somewhat toy. For instance, SMAC is a well-studied benchmark, many methods have shown performance not worse than A2PO (such as MAT[1]).\n\n- b. The author should consider MARL tasks that are really in the predicament of the sample efficiency, such as multi-agent competitive environments (such as Neural MMO[2]) and multi-agent visual navigation tasks.\n\n\n [1] Wen M, Kuba J G, Lin R, et al. Multi-Agent Reinforcement Learning is a Sequence Modeling Problem[J]. arXiv preprint arXiv:2205.14953, 2022.\n\n [2] Suarez, Joseph, et al. \"The neural mmo platform for massively multiagent research.\" arXiv preprint arXiv:2110.07594 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Quality: Good.\n- Clarity: Good.\n- Originality: Good.",
            "summary_of_the_review": "This paper proposed a new MARL method and evaluated on some MARL benchmarks. But the improvements are not that satisfactory, ",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_GdLP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_GdLP"
        ]
    },
    {
        "id": "MNI3WehF6q",
        "original": null,
        "number": 4,
        "cdate": 1667482322628,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667482322628,
        "tmdate": 1667482322628,
        "tddate": null,
        "forum": "Q-neeWNVv1",
        "replyto": "Q-neeWNVv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4765/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the sample efficiency and single-agent monotonic improvement guarantees in sequential (agent-by-agent) policy updates in cooperative multi-agent tasks. To retain the guarantees of monotonic improvement for single agent, the authors propose the PreOPC, which approximate the true advantage of an agent. With this single-agent monotonic bound, a tighter joint monotonic bound can be achieved. Then, by optimizing the surrogate objective with the approximated advantage, the authors propose the A2PO algorithm, which achieves better performance with two techniques: semi-greedy agent selection rule and adaptive parameter clipping. Experiments have shown the effectiveness of the proposed algorithm.",
            "strength_and_weaknesses": "Strength:\n1.\tThe paper is well written and well organized.\n\n2.\tThe paper provides the single-agent monotonic bound in the setting with single-rollout and sequential policy update.\n\n3.\tExperimental results are solid.\n\nWeaknesses:\nSome questions:\n1.\tIt seems that the theoretical results are established on the condition that all agents can observe the global state and take this global state as input. I am somewhat unsure whether the theoretical results are fully satisfied by the experiments because in the environments such as SMAC, agents typically have access to local observation.\n\n2.\tRegarding the sequential update scheme, one concern is scalability, though the experiments in this work show that the sequential update scheme has better performance. This is because the update of an agent's policy can be done only when all preceding agents have updated their policies. When facing the settings with a large number of agents, this could be slow compared to simultaneous update schemes such as MAPPO. So, in this sense, I would like to know how long (wall clock time) it takes for the A2PO and other simultaneous update schemes such as MAPPO and CoPPO.\n\n3.\tCan the authors provide more explanations about the assumption on $\\xi^i$ in Theorem 1 and 2? Compared to previous monotonic bounds, A2PO can achieve a tighter bound only given that $\\xi^i$ is small enough. How to ensure this in experiments (please correct me if I have misunderstood or missed something)?\n\n4.\tThough this paper focuses on the policy-based methods, I think some discussion on the value-based methods such as QMIX and its variants is necessary (maybe in the appendix).\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think this paper is good in clarity, quality, novelty, and reproducibility. The theoretical analysis and experimental results are solid and quite convincing.",
            "summary_of_the_review": "After reading the paper carefully, I think my comments and recommendation are accurate. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_GN3i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_GN3i"
        ]
    },
    {
        "id": "fMDpLv-NSWe",
        "original": null,
        "number": 5,
        "cdate": 1667584711975,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667584711975,
        "tmdate": 1667584711975,
        "tddate": null,
        "forum": "Q-neeWNVv1",
        "replyto": "Q-neeWNVv1",
        "invitation": "ICLR.cc/2023/Conference/Paper4765/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new surrogate objective in joint policy optimization for coordination tasks in MARL, following single roll-out and sequential update schemes. The new surrogate depends on a new off-policy correction method similar to the one in (Munos et al., 2016), and retains the monotonic improvement guarantees on individual agent and overall performance, while also maintaining high sample efficiency. Furthermore, they empirically show that the order of the agents' update and an adaptive clipping on the importance sampling probabilities has a substantial effect on the overall performance. Finally, they provide extensive experimental results on four MARL benchmarks, where they outperform previous trust-region MARL methods. \n",
            "strength_and_weaknesses": "#### $\\textbf{Strength}$\n\n1. The problem considered in the paper is well-motivated and the challenges are incrementally introduced, together with the relevant related work.\n2. The proposed off-policy correction method is theoretically guaranteed to satisfy monotonic single-agent improvement and, as a consequence, overall monotonic improvement. \n3. The joint policy monotonic improvement is tighter than previous state of the art.\n4. There are extensive experimental evaluations on well-known MARL benchmarks which clearly demonstrate the advantage of using A2PO. \n5. The authors further provide experimental justification for using semi-greedy agent selection and adaptive clipping in the estimated probabilities. \n\n#### $\\textbf{Weaknesses/Questions}$\n\n1. In the abstract it is claimed that A2PO improves sample efficiency, and in Table 1 it is again stated that the sample efficiency is high. What is meant by sample efficiency? Is it sample complexity? In any case, to my understanding, there is no subsequent result that shows an improvement of the sample efficiency in the paper. Is the statement based on a similar efficiency result imported from HAPPO since the rationale (at lease with respect to single roll-out usage) is similar? \n2. I would have liked to see some convergence guarantees of A2PO for single agent and/or multiple agents scenarios. Does A2PO converge to individual optimal policies for every agent (jointly), and if so, how fast compared to HAPPO or similar methods, in terms of agents number, episode length and epoch length? \n3. Can you elaborate on the importance of single-agent policy improvement in this setting? The reward function $r$ is the same for all agents, so there is no conflict of interest or notion of equilibrium, all agents are striving towards the same goal. If it wasn't for the improvement of joint monotonic bounds with respect to HAPPO, why should one care about single-agent policy improvement in this scenario? \n\n#### $\\textbf{Suggestions}$\n\n1. In some places such as first paragraph of Section 4 or Table 1, in-line equations may be improved to increase readability. \n2. In Section 3.1, either use the usual $\\mathbb{P}$ notation for probability, or you may want to define what $Pr$ means. \n3. I suggest you define the sample trajectory right after the definition of the value function, since that is where you first use it.\n4. In the sequential update scheme illustration (pipeline) in Section 3.2, you might want to define the surrogate objective $\\mathcal{L}$.",
            "clarity,_quality,_novelty_and_reproducibility": "#### $\\textbf{Clarity and Quality}$\n\nThe paper is well-written and not hard to follow. Although some definitions might be improved, this does not affect the overall clarity of the paper. Further, the problem is well-motivated and well-explained.  \n\n#### $\\textbf{Novelty}$\n\nThe novelty of the paper stands in the key insight of using the off-policy correction method into a sequential update scheme. This combination results in higher performance, both theoretically and empirically. Theoretical techniques are standard. \n\n#### $\\textbf{Reproducibility}$\n\nI did not check the code. ",
            "summary_of_the_review": "#### $\\textbf{Summary}$\n\nOverall, I find the contribution of the paper important to the MARL community. The authors provide an elegant solution to the single-agent policy improvement problem in trust region cooperative MARL using sequential update schemes. Furthermore, the extensive experimental results offer a complete picture of the benefits of A2PO, and also of the benefits of agent selection order. \n\n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_ZzqH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4765/Reviewer_ZzqH"
        ]
    }
]