[
    {
        "id": "R_HxAWlsJwe",
        "original": null,
        "number": 1,
        "cdate": 1666559582358,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559582358,
        "tmdate": 1666559582358,
        "tddate": null,
        "forum": "1uPo_IrEp8",
        "replyto": "1uPo_IrEp8",
        "invitation": "ICLR.cc/2023/Conference/Paper3796/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a sampling-based policy search for MDPs. The algorithm uses Thompson sampling over policies rather than over individual actions within policies. The algorithm is empirically evaluated and theoretically analyzed. ",
            "strength_and_weaknesses": "Strengths: the paper contains both theoretical analysis and empirical evaluation of the proposed algorithm. \n\nWeaknesses: the paper requires significant proofreading to be followed. There are many typos and inconsistencies, and settings, assumptions,  and algorithm descriptions are unclear. The empirical evaluation results are hard to interpret, and will be problematic to reproduce as there was no code or data provided. Citations are rather random.\n\nExamples: $\\Gamma$ appears on page 2, not counting the abstract, but is only defined on page 3. Citation for 'online reinforcement learning' on page 1, first line of introduction, points to an article on a specific application of online RL, not particularly relevant to what follows. 3.1, problem setting, defines finite MDP and then refers to  'episodes' and 'epochs', which are not defined, and are used in a confusing way. Why the total reward of an episode should be in [0, 1]? How is this related to the reward function r to be SxA -> [0, 1]? In Assumption 3.3, it is not clear what do $e_1$ and $e_2$ correspond to, and this is not defined. In Algorithm 1, \"game environment\" suddenly pops up, without having been defined anywhere earlier. And so on, so forth.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper requires significant proofreading. The results are very problematic to reproduce based on the high-level description only, without any code or data.",
            "summary_of_the_review": "The paper may contain interesting ideas, however significant work is required to make it ready for publication.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_craj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_craj"
        ]
    },
    {
        "id": "QFgKv6CmQ7_",
        "original": null,
        "number": 2,
        "cdate": 1666590334781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590334781,
        "tmdate": 1666590334781,
        "tddate": null,
        "forum": "1uPo_IrEp8",
        "replyto": "1uPo_IrEp8",
        "invitation": "ICLR.cc/2023/Conference/Paper3796/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies reinforcement learning theory in the average-reward MDP setting. The main result is a $\\tilde{O}(D\\sqrt{SAT})$ regret bound for a new algorithm that performs posterior sampling over policies. ",
            "strength_and_weaknesses": "While I acknowledge that both average-reward MDPs, as well as the posterior sampling type algorithms themselves are interesting objects in RL theory, I think overall I am highly concerned about the contribution of this paper, due to its lack of awareness of existing work, as well as the many clarity / consistency issues within the basic definitions and settings.\n\n* For high-level comparisons of existing work, first of all, I think the authors did not make clear that they are restricting to the average-reward MDP setting, which is strange\u2014There are at least two equally popular models, discounted MDPs, and finite-horizon MDPs, that are extremely well-studied in terms of online RL in the tabular case, but almost not mentioned at all in related work. With the exception of mentioning Jin et al. (2018) and Zhang et al. (2020), which the authors claim \u201chave not achieved the state-of-art regret bound\u201d. However, Jin et al.\u2019s result is only one H (horizon) factor from optimal regret, and this was closed by Zhang et al. (2020)\u2019s algorithm which achieves the minimax regret $\\sqrt{H^3SAT}$ in the finite-horizon setting. \n\n* For the regret bound, I think the current best result for average-reward MDPs is already $\\sqrt{DSAT}$, for example, achieved by the work of Zhang and Ji (2019, Corollary 1). Thus the claim of the established $D\\sqrt{SAT}$ result \u201cmatching the current best regret bound\u201d sounds questionable.  \n[Reference] Zhang, Zihan, and Xiangyang Ji. \"Regret minimization for reinforcement learning by evaluating the optimal bias function.\" Advances in Neural Information Processing Systems 32 (2019).\n\n* Many basic definitions seem non-standard and inconsistent throughout the paper. For example, the definition (2) of regret does not look like the standard definition for RL (it maximizes over a single action $a$ instead of policies). Eq (2) also disagrees with the later appearance of Regret in Eq (10). Neither agrees with the standard definition of regret, which measures the difference against the optimal average reward (e.g. Zhang and Ji, Page 3). \n\n* The authors define $\\theta_{t_k}$ as the \u201ctransition probability\u201d of the \u201cmodel-based agents\u201d within time-step $k$ and epoch $t$ (Section 3.1), whereas $\\theta_\\star$ is the true transition probability. What is the \u201ctransition probability of model-based agents\u201d? Do the authors mean some estimate maintained within the algorithm?\n\n* Assumption 3.1, why is \u201c$\\epsilon$-optimal policy\u201d measured on this (estimated) $\\theta_{t_k}$? I think the standard definition is to measure the deployed policy versus the optimal policy on the ground truth model $\\theta_\\star$?\n\nOverall, because of these concerns, I am quite concerned about the contributions even before getting into the actual algorithms / analyses.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "Please find my comments in the strength and weaknesses part. ",
            "summary_of_the_review": "This paper studies a topic that received wide attention in recent years (online RL in average-reward MDPs). However, the paper seems quite lacking in awareness of recent work, and has many issues in basic definitions and the clarity of the setups, which makes its contributions questionable in my opinion. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_RWGK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_RWGK"
        ]
    },
    {
        "id": "8B48h8gD5K",
        "original": null,
        "number": 3,
        "cdate": 1666711085461,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666711085461,
        "tmdate": 1666711085461,
        "tddate": null,
        "forum": "1uPo_IrEp8",
        "replyto": "1uPo_IrEp8",
        "invitation": "ICLR.cc/2023/Conference/Paper3796/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The authors consider the regret minimization problem for infinite-horizon average-reward Markov decision process (MDP). They propose the Reward-Weighted Posterior Sampling of Policy (RWPSP) a Thompson sampling inspired algorithm. Precisely  RWPSP maintains a posterior distribution over policies with is updated with the optimal policy of the estimated probability transitions. They prove a regret bound of order O(\\Gamma \\sqrt(T)/S^2) where \\Gamma /S^2\\leq D\\sqrt{SA} where T is the horizon, S the number of states, A the number of actions and D the diameter of the MDP. They experimentally show that  RWPSP compares favorably to other baselines.",
            "strength_and_weaknesses": "Weaknesses:\n \n- Presentation ",
            "clarity,_quality,_novelty_and_reproducibility": "In the current state, the submission is really hard to read, see specific comments. Many notations are inconsistent of absent.  Some sentences are not finished or make no sense. Therefore, I'm not able to provide any feedback on the core ideas of this submission.\n\n#Specific comments:\n- P1, Abstract: What do you mean by 'gain the globally sub-optimal rewards'?\n- P1, Section 1: I'm not sure that Kevton et al. 2020 is the only paper dealing with the exploration-exploitation dilemma in reinforcement learning.\n- P1, Section 1: The references Ding et al., 2021;Oh & Iyengar, 2019; Moradipari et al., 2019 concerns rather contextual bandits problems than reinforcement learning problem, maybe you could consider citing, e.g.  Agrawal and Jia (2017b),Osband and Van Roy, 2017, Russo 2019.. who analyses Thompson sampling like algorithm for reinforcement learning.\n- P1, Section 1: The statement 'The general optimistic algorithms require to solve all MDPs lying within the confident sets' is wrong. Take for example the UCBVI algorithm by Azar et al., 2017  that only needs to solve one MDP (with bonuses added to the rewards).\n- P1, Section 1: What do you mean by 'Thompson sampling [..] results in biased estimates of the transition matrix' exactly?\n- P1, Section 1: It is not clear for me how a 'Bayesian method to update transition probabilities' can have a regret bound?\n- P1, Section 1: D is not defined. For the bound to be relevant you should precise the setting you are considering, e.g. infinite-horizon average reward...\n- P1, Section 1: You need to define what you mean by  'samples posterior policy distributions' because for example in PSRL by Osband et al., 2013; algorithm by also a posterior policy is sampled (a policy from the posterior over optimal policy exactly which is obtained by sampling a posterior over the MDP and then computing the optimal policy of this particular MDP).\n- P2, Section 2: What do you mean by 'total regret bound' exactly?\n- P3, Section 3.1: 'probability could be defined The transition probability' sentence not finished. There is a repetition in the sentence after.\n- P3, Section 3.1: What is epoch k?\n- P3, Section 3.1: What is $\\pi_{\\theta_k} exactly? The globally optimal policy for which criteria? What is the 'local optimal transition probability'?\n- P3, Section 3.1: How are generated the action a_t' in (1)?\n- P3, Section 3.1: It is the maximum over what in the definition of \\Gamma?\n- P3, Section 3.1: How are generated the states s_t and action a_t in (2)?\n- P3, Section 3.1, (2): You change notation for the expectation.\n- P3, Section 3.1: It is a sum over what in the definition of r(s, \\pi)\n- P3, Section 3.1, definition of D: It is a maximum over what, it should not be \\pi instead of \\pi_{t_k}?\n- P4, Section 3.2: What do you mean by 'online settings'?\n- P4, Section 3.2: I do not understand the sentence 'As they often get stuck in locally optimal results' which results?\n- P4, Section 3.2, Assumption 3.1: The quantity J_\\pi(\\theta) is not defined you only introduced J(\\pi). It seems to be a definition. \n- P4, Section 3.2: You still not defined properly the transition time.\n- P4, Section 3.2,Assumption 3.2: The max are not well defined and does this assumption holds for all t_k?\n- P4, Section 3.2: What is the definition of N(\\pi) exactly?\n- P4, Section 3.2: The posterior probability under which Bayesian model?\n- P5, Section 4: Since you did not define the Bayesian model and \\mu in particular it is complicated to follow the presentation of Algorithm 1.",
            "summary_of_the_review": "See above",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_5tze"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_5tze"
        ]
    },
    {
        "id": "Pdkvn3F3Le",
        "original": null,
        "number": 4,
        "cdate": 1667521930323,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667521930323,
        "tmdate": 1667521930323,
        "tddate": null,
        "forum": "1uPo_IrEp8",
        "replyto": "1uPo_IrEp8",
        "invitation": "ICLR.cc/2023/Conference/Paper3796/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper introduces a new posterior sampling algorithm (RWPSP) for the horizon-free finite state tabular MDP setting in RL. The algorithm is part of the posterior sampling family of approaches (e.g. a la Thompson sampling), but differentiates itself from past results by:\n- not attempting to sample an estimate the transition model and optimize a policy out of that\n- but instead building directly a posterior on policies based on an estimate transition model, and then sample a policy out of that\n\nThis different approach results in a different regret bound of O(\\Gamma T^1/2 S^-2) which the authors show is always smaller or equal than the current best bounds of O(D(SAT)^1/2).\n\nFinally, preliminary results on toy MDPs are provided to highlight the improved performance in terms of regret of the proposed method.",
            "strength_and_weaknesses": "The main strength of the method are matching (and potentially improving) the current best rates for this setting, the novelty of the approach to the posterior sampling family, and the seemingly complete lack of propabilistic statements in the regret proof.\n\nThe weaknesses are:\n- The claim of optimality of O(D(SAT)^1/2) regret only holds for posterior sampling approaches (the paper \"Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function\" Zihan Zhang, Xiangyang Ji, NeurIPS2019 seems to report a O((DSAT)^1/2) bound under comparable assumptions). I am not sure if since 2019 no other posterior sampling approach also matched this bound.\n- Could the authors clarify the proof of lemma 5.8? From Lemma A.6 in the appendix we have equation (33) proving \\Gamma \\leq D by application of Assumption 3.2. At this point further bounding D with DS^2(SA)^1/2 seems pointless. A finite MDP cannot have less than 1 state, so a direct application of \\Gamma \\leq D would yield a O(DT^1/2/S^2) regret, much better than existing results.\n- Lemma A.6 seems to rely on Assumption 3.2 heavily. However I do not see why it would be natural to assume that the optimal policy would reduce the diameter between *any two states* in the MDP compared to any of the policies encountered during optimization. In particular the MDP reward (and therefore the optimal policy) does not usually care about arbitrary transition times, but only about transitioning to a good state as quickly as possible. On the other hand a sub-optimal policy might be well aligned (due to sheer randomness) with the explorative goal of tansversing the whole space as efficiently as possible.\n- Assumption 3.2 does not define what would the max be over.\n- I am not fully familiar with the setting, but the complete lack of any probabilistic statement in any of the results (i.e. none of the theorem hold only with some probability, and the regret bounds seem to rely only on the assumptions). The only statistical comment is a brief reference in section 5.2. This seems very unlikely to be rigorous considering posterior sampling is a randomized algorithm. If this was indeed the case it would be a significant aspect of the contribution and should be highlighted in the text.\n- The exposition of the whole paper is well structured, but clarity falls apart at a more careful inspection. Most of the maxes are missing the variable that are being maxed on. Section 3.1 contains broken phrases, including crucially the one that introduces \\theta. p_\\theta is never introduced. the span is mentioned once and never again. Lemma 5.3 uses a different notation in the main paper and in the appendix. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper seems well structured at a high level, but once I started looking into the details it is severly lacking. I made a fair attempt at understanding the proof, but due to incomplete notation and very barebones derivations in some places I would not be confident in its meaningfulness.\n\nThe quality of the paper is negatively impacted by the lack of clarity.\n\nSimilar, if there are novel contributions beyond the algorithmic one they should be appropriately highlighted in the proof. Which part of the derivation is made possible by sampling directly posterior policies rather than full transition models?\n\nThe reproducibility of the theoretical part is not very good due to lack of clarity. The experimental part is not reproducible, as the authors do not report any of the relevant hyperparameters to run RWPSP, nor include code.\n\n ",
            "summary_of_the_review": "Overall I feel the paper introduces a nice idea (side-stepping the problem of estimating and sampling the transition model), but I could not understand from the theoretical derivation how this change unlocks new solutions in the regret analysis. Due to this lack of clarity and confusion in the soundness of the result I would not reccomend acceptance for the paper in its current state.\n\nHowever, despite having a background in theoretical RL, I also recognize that the field is extremely specialized and I might be missing necessary context and conventions given for granted in the proof derivation, and remain open to re-evaluate my position if the authors can clarify their work.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_j9Gs"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3796/Reviewer_j9Gs"
        ]
    }
]