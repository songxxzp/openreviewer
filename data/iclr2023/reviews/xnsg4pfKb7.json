[
    {
        "id": "JQ6I1HOCsRQ",
        "original": null,
        "number": 1,
        "cdate": 1666342793247,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666342793247,
        "tmdate": 1669185547093,
        "tddate": null,
        "forum": "xnsg4pfKb7",
        "replyto": "xnsg4pfKb7",
        "invitation": "ICLR.cc/2023/Conference/Paper5036/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The idea is to learn inverariance to commutative groups. The groups are learned from data through the proposed block, which relies on the bispectrum idea. The proposed block (depicted in Fig. 2) resorts to a third-order polynomial that can be implemented on standard frameworks, as it requires an affine transformation of the input and a Hadamard product. In addition to this, an orbit separation loss is proposed. ",
            "strength_and_weaknesses": " * [+] Including symmetries in neural networks is an interesting research direction. \n\n * [+] The writing is mostly clear, while I like the summary of the group notation in the appendix. \n\n * [+] The proposed block for the bispectrum is simple. \n\n * [-] The experiments are weak at the moment. To be specific, I do not find much the application of this in real-world data in the experimental section. \n\n * [-] The claim on the adversarial robustness seems exaggerated (see below). \n",
            "clarity,_quality,_novelty_and_reproducibility": "In table 1, what are the FLOPs of each method? It would help to compare them in addition to the comparison of accuracy. \n\nThe paper mentions that it is not outperforming the compared methods but it has the benefit of learning the group from the data. However, it is not clear to me what is the real-world application of this; definitely I do not notice any related experiment. \n\nIn sec. 4.3 there is the claim that: \u201cThe goal of many adversarial attacks is to discover input points which are similar to a target sample in the model\u2019s representation space, yet perceptually dissimilar to the true sample in the input space.\u201d. Are there some citations for this? \n\nMany of the most popular adversarial attacks are FGSM, PGD, C&W; in all of these cases the idea is the input image to be perceptually similar to the adversarial image. Are there any results on those benchmarks to demonstrate how the proposed network is more robust to those common attacks? That is, since the manuscript mentions that the method is \u201cstrongly robust to adversarial attacks\u201d. \n\nIt\u2019s not clear to me what is the role of the proposed bispectral block or the orbit separation loss in practice. Could the authors elaborate on their relationship and the sensitivity to the hyper-paremeter $\\gamma$? \n\nTo continue on the previous line: are there any numerics on adversarial robustness when compared with other methods? At the moment only table 1 has numeric results in the main papers and most of the methods score above > 98%, which does not make it easy to appreciate the contributions of the proposed method. \n\nMinor: Instead of \u201cthe context of vision\u201d it should be the context of image/scene recognition in the introduction, since not all the vision tasks require the same symmetries. \n",
            "summary_of_the_review": "The proposed bispectral block is new to me, and the idea of including invariances to groups in the network is also an interesting research direction. I would say one of the weaknesses in my mind is the experimental validation that is currently weak, given the empirical nature of this work. \n\n_____________\nAfter the rebuttal: See the discussion below, but the core idea is that some of the concerns have been addressed below. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_QqWn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_QqWn"
        ]
    },
    {
        "id": "yO_v4zyz-E",
        "original": null,
        "number": 2,
        "cdate": 1666600574992,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600574992,
        "tmdate": 1669169548246,
        "tddate": null,
        "forum": "xnsg4pfKb7",
        "replyto": "xnsg4pfKb7",
        "invitation": "ICLR.cc/2023/Conference/Paper5036/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a neural network architecture, Bispectral Neural Networks(BNNs), which aims to learn the group-invariant representations of the actions of compact commutative groups. This simple architecture is composed of two layers: a single learnable linear layer, followed by a fixed collection of triple products computed from the output of the previous layer. Then, some comparison experiments are conducted to show the effectiveness of the proposed method. ",
            "strength_and_weaknesses": "strengths:\n1. This paper has comprehensive and rigorous mathematics proof. \n2. The network is simple yet effective, and the competitive results show the effectiveness. \n3. This paper has demonstrated that the completeness property endows these networks with strong adversarial robustness. ",
            "clarity,_quality,_novelty_and_reproducibility": "This paper presents a novel approach for learning groups from data in an efficient, interpretable, and fully feed-forward model that requires no prior knowledge of the group, computation of the exponential map, or bayesian inference. ",
            "summary_of_the_review": "From the manuscript, the authors propose BNNs, which could simultaneously learn groups, their irreducible representations, and corresponding complete invariant maps purely from symmetries implicit in data. I think it is good. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_pa8G"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_pa8G"
        ]
    },
    {
        "id": "O8JerSSGaE8",
        "original": null,
        "number": 3,
        "cdate": 1666797253130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797253130,
        "tmdate": 1666797253130,
        "tddate": null,
        "forum": "xnsg4pfKb7",
        "replyto": "xnsg4pfKb7",
        "invitation": "ICLR.cc/2023/Conference/Paper5036/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents the Bispectral Neural Network (BNN), a network primitive which learns to be invariant to transformations present in the data. The model is defined in terms of two main components. Firstly, the model treats the irreducible representations of a group of transformations as weights in the network, thus essentially learning a Fourier transform on the group, which is equivariant to the group action. The Fourier representation of the input is then mapped to the bispectrum, which is invariant to the transformation but retains the information needed to uniquely restore the image (up to the transformation), by the network. Secondly, a novel loss function encourages the bispectrum for inputs of the same class to be identical, effectively making the trained network invariant to the transformation. A crucial point here is that the group of transformations need not be defined a priori; the network discovers the transformations automatically.\n\nThe authors test the model in four different experiments, demonstrating that it can construct the transformation group from data alone, that the learnt irreducible representations are transferable to other datasets, and that the model is robust to adversarial attacks.",
            "strength_and_weaknesses": "### Strengths\nOverall, I find this paper very strong. While the bispectrum is extensively used in other fields, it does not seem to be the case in machine learning (as far as my searching abilities go). Simply introducing this incredibly useful concept to the machine learning community has value in itself, and then further parametrising it in terms of a neural network, which, given a novel loss function, learns the group of transformations directly from data, makes this paper very strong. The simplicity and elegance of the proposed method are intriguing, and I could see the ideas introduced here forming the basis of many future papers on invariant and equivariant models.\n\nIn summary, the paper\n- presents an incredibly useful concept (the bispectrum),\n- introduces a novel and interesting loss function, and\n- demonstrates how to learn irreducible representations of the transformation group using a neural network.\n\n\n### Weaknesses\nThe paper appears to be mostly a proof-of-concept. The datasets used in the experiments are quite simple, which the authors acknowledge themselves. It is also unclear to me if the method is usable in practice in terms of the computations that are required to compute the bispectrum. Since the complex weight matrix needs to be of size $n \\times n$, $n$ being the number of input dimensions, does the method scale to inputs of even moderately large dimensionality? A study of this, e.g., in terms of the training time and memory cost for increasing image sizes, would have been interesting. The authors say that they are currently working on a localised version of the model, such that it can be used in a convolutional neural network, which would address this issue. Still (and without knowing exactly how much work this entails), one might have expected such an extension to be included in the current paper.\n\nI see these weaknesses as minor, though, given the significance of the model and the ideas and concepts the paper introduces.\n\nIn summary, I see the main weaknesses as being that\n- the model is only tested on simple datasets, and that\n- it is unclear if the model scale to inputs of moderately large dimensionality.",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of the paper is exceptionally high. As someone relatively new to the field of equivariances in machine learning and entirely new to group representation theory, I found the paper extraordinarily pedagogical. Appendix A is very helpful and section 2 is well-written and easy to understand despite the abstract topic. The method itself appears to be novel (I wasn't able to find anything similar) and the ideas and concepts could have a significant impact on the fields of invariances and equivariances in machine learning. The authors provide the code to reproduce all experiments and figures, though I did not try to run this. In general, the paper should be of significant interest to the ICLR community. \n\n\n**Questions for the authors**\n\n1. For the loss, did you experiment with other norms? For images, in particular, the $L_2$ norm is not very informative, but I cannot intuitively see if this could cause problems.\n2. As mentioned under \"weaknesses\", it is unclear to me how well the method scales. I don't expect any new experiments, but do you have a feeling for how large inputs the method can handle?",
            "summary_of_the_review": "The paper presents a novel method for learning representations of data that are invariant to transformations of the data. Not only is the method novel, the ideas and concepts presented in the paper are intriguing and should be of significant interest to the ICLR community. The experiments are somewhat weak, though, and the paper seems to be mostly a proof-of-concept, yet the strengths far outweigh these weaknesses. I view the paper as a very significant and impactful contribution to the fields of invariance and equivariance in machine learning.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_1MdX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5036/Reviewer_1MdX"
        ]
    }
]