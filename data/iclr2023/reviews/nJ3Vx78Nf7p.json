[
    {
        "id": "VVZnilLcWn",
        "original": null,
        "number": 1,
        "cdate": 1666186496512,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666186496512,
        "tmdate": 1666186496512,
        "tddate": null,
        "forum": "nJ3Vx78Nf7p",
        "replyto": "nJ3Vx78Nf7p",
        "invitation": "ICLR.cc/2023/Conference/Paper5778/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new method called NBD to learn Bregman divergences between data points using a deep neural network. The proposed method is tested on some synthetic and small machine learning benchmark data sets. The results show that NBD outperforms several other metric learning approaches for some tasks.",
            "strength_and_weaknesses": "Strength:\n\nThe authors adopted a more generic method to parameterize the phi function, which seems better than several other existing Bregman divergence learning approaches in various settings.\n\n\nWeakness:\n\nI doubt the usefulness of non-Euclidean distance learning. The authors enumerate the use in retrieval, clustering, and ranking in the introduction. However, there is little evidence in the paper to show significant improvement in these applications compared to Euclidean approaches. Instead, the authors focused on various artificial settings of the pairwise distances. Winning in such artificial scenarios may not justify the new method.\n\nSection 5.1 seems to be a realistic application. But the MAP@10 values are low compared with the state-of-the-art. Suppose we just use an image classifier (e.g., ResNet or Vision Transformer) and calculate the MAP@10 using the softmax outputs and their Euclidean distances. What is the benefit of using the proposed method?\n\nIt is infeasible to show that NBD works well for arbitrary non-Bregman distance learning. In Section 5.4, the authors have to modify the distance function as a remedy, which makes the claims looser.\n\nThe experiment setting is not clear. How many supervised pairs were used in training (compared to the number of data points)? Which pairs did you sample?\n\nTable 4: most SVHN results are 96.9, which looks unnatural. Why?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is not very good.\n- The end of Section 1 mixes the contributions and paper outline. Consequently, the major highlight of the work is unclear.\n- The description of the main method is too short (Sections 2.2 and 2.3). The choice of ICNN is not well-motivated. Was it used for Bregman divergence learning before? Why is the ICNN parameterization better? A theoretical analysis is missing.\n",
            "summary_of_the_review": "The paper seems to have a step forward in non-Euclidean distance learning. However, the main contribution is unclear, and the impact of the proposed method is questionable. I tend to give borderline rejection.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_GJoF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_GJoF"
        ]
    },
    {
        "id": "DftbVs2XCJ",
        "original": null,
        "number": 2,
        "cdate": 1666358563001,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358563001,
        "tmdate": 1666358608688,
        "tddate": null,
        "forum": "nJ3Vx78Nf7p",
        "replyto": "nJ3Vx78Nf7p",
        "invitation": "ICLR.cc/2023/Conference/Paper5778/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a method for learning a Bregman divergence using neural networks. A Bregman divergence is the divergence defined using a convex function, and any convex function has its corresponding Bregman divergence. The proposed method learns the divergence by representing the convex function by an Input Convex Neural Network (ICNN) (Amos et al., 2017). The paper provides a series of experiments to confirm the superiority of the proposed method over the previous methods.",
            "strength_and_weaknesses": "# Strengths\n- The approach of directly learning the convex function by an ICNN seems interesting. Related papers on ICNNs support the representation power of the model.\n- The paper presents quite extensive experiments, and the proposed method shows excellent performance.\n\n# Weaknesses\n- I have a concern with Eq. (2). The paper says \"by construction, the resulting neural network satisfies convexity,\" but I don't think this is the case. If, additionally, $g$ were a non-decreasing function as assumed in Amos et al. (2017), I would understand the claim. I understand that the proposed method uses $g(x) = \\log(1 + \\exp(x))$ which is nondecreasing, so this may not be a big problem. However, I wonder if theories by Chen et al. (2019) and Pitis et al. (2020) apply to this activation function.\n- If I understand well, some experiments assume that divergence values are given as labels, which I believe is not very realistic in practice.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and easy to follow.\n- It cites and discuss relevant previous work properly, and the motivation of the work is clear.\n- Introducing ICNNs to learning Bregman divergences is novel as far as I know.\n- I think the paper and the supplementary material provide sufficient details for reproducing their experiments.\n- One thing that is not very clear to me is what labels are given in each experiment.\n\nQuestions:\n- In Eq. (3), is $\\tilde{y}$ fixed or varied with $\\theta$ when we take the gradient?\n- In Section 5.1, is the triplet loss the same as that for PBDL described in Section 2.1?\n- In Algorithm 1, line 8, what loss does it use?",
            "summary_of_the_review": "Overall, I like the paper and the authors make great contributions with many empirical results. I have a few concerns that I mentioned above, but I tend to accept the paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_rc5T"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_rc5T"
        ]
    },
    {
        "id": "Im6Ew_lmo--",
        "original": null,
        "number": 3,
        "cdate": 1666764347126,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666764347126,
        "tmdate": 1666839458910,
        "tddate": null,
        "forum": "nJ3Vx78Nf7p",
        "replyto": "nJ3Vx78Nf7p",
        "invitation": "ICLR.cc/2023/Conference/Paper5778/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper addresses the limitation of previous methods for Bregman divergence learning and proposes a solution named Neural Bregman Divergences (NBD) using Input Convex Neural Network (ICNN), which can be computed efficiently and gives finer resolution to the generating function. Experimental results verify the performance of the proposed method on many metric learning tasks.",
            "strength_and_weaknesses": "Strength:\n\nThis paper developed a new Neural Bregman Divergence, which jointly learns a  Bregman measure and a feature extracting neural network. It also demonstrates that the proposed method more faithfully learns divergences over a set of both new and previously studied tasks, including asymmetric regression, ranking, and clustering.\n\nWeakness:\n\n1. The novelty of this paper is limited. The proposed method basically follows the existing Bregman learning methods (Siahkamari et al., 2020; Cilingir et al.,2020), and only jointly learns a Bregman measure and a feature extracting neural network in the proposed method. \n\n2. The experimental results of non-Bregman learning do not outperform SOTA methods. Does it mean the proposed method NBD only achieves better result on specific tasks where the underlying ground truth is from Bregman divergence? Will it restrict the application of NBD?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing can significantly be improved. \nThe novelty of this paper is limited. ",
            "summary_of_the_review": "This paper developed a new Neural Bregman Divergence, which jointly learns a Bregman measure and a feature extracting neural network.  It also demonstrates that the proposed method more faithfully learns divergences over a set of both new and previously studied tasks, including asymmetric regression, ranking, and clustering. Some experimental results verify the effectiveness of the proposed method. However, the novelty of this paper is limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_DTa2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_DTa2"
        ]
    },
    {
        "id": "VX0udYXC_g",
        "original": null,
        "number": 4,
        "cdate": 1667757079888,
        "mdate": 1667757079888,
        "ddate": null,
        "tcdate": 1667757079888,
        "tmdate": 1667757079888,
        "tddate": null,
        "forum": "nJ3Vx78Nf7p",
        "replyto": "nJ3Vx78Nf7p",
        "invitation": "ICLR.cc/2023/Conference/Paper5778/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper provides a method for learning Bergman divergences using input convex neural networks as the convex generating function. This eliminates the shortcomings of previous work on the topic including non convexity of the neural network for deep divergence learning and high computational complexity of linear Bregman divergence learning. They further use CNN's as feature extractors and implement joint training. They experiment with various tasks such as regression, ranking and clustering and outperform the competing methods. Further they extend the Bregman divergence learning in two directions, one where the divergence satisfies both triangle inequality and symmetry and another where only triangle inequality is satisfied. ",
            "strength_and_weaknesses": "The paper has many contributions and experiments. The idea of using input convex neural networks for learning Bergman divergences is new. The extensions to satisfy the triangle inequality and symmetry are also new. They have done extensive experimentation with different tasks and have compared to a significant number of competing methods. Further they have defined interesting divergence regression tasks such as BregMNIST and BregCifar.",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the writing is good. There are many sections in the paper which looks misleading at first but has a good flow in general.",
            "summary_of_the_review": "I think the paper has done enough work and has good quality for publication in the conference. Accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_XV53"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5778/Reviewer_XV53"
        ]
    }
]