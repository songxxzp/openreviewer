[
    {
        "id": "jWM2ow2g08I",
        "original": null,
        "number": 1,
        "cdate": 1666548371786,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548371786,
        "tmdate": 1666548371786,
        "tddate": null,
        "forum": "-ng-FXFlzgK",
        "replyto": "-ng-FXFlzgK",
        "invitation": "ICLR.cc/2023/Conference/Paper5716/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose Neural Image-based Avatars (NIA), a method for synthesizing novel views and poses. Its main advantages are the implicit body NeRF representation and the image-based rendering. They claim to outperform the current state-of-the-art on the ZJU-MoCap and MonoCap datasets.",
            "strength_and_weaknesses": "Main Strengths:\n- The method gathers two complementary state-of-the-art approaches (i.e. neural radiance fields and neural image-based rendering).\n- The NIA is generalizable for different subjects.\n\nMain Weaknesses: \n- The approach relies on sparse view images and the authors do not refer to the LOLNeRF (Rebain et al., CVPR22), a related and recent method based on single views.\n- Most of the previous approaches rely only on PSNR and SSIM metrics. Despite that, it would be interesting to see additional metrics on the evaluation, or at least to read the authors' comments on why other metrics were not adopted.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and technically sound. The overall pipeline is new, but it strongly relies on Kwon et al. (2021) and Wang et al. (2021). The authors describe the implementation details in the supplementary material and claim that the code will be published upon publication.\n\nMinor suggestions:\n\n- section(s) -> Section(s); sec. -> Sec..\n- Section 4.1.1 could be on top of page 6.\n- Eval. -> Evaluation/Evaluated (Tab.1).\n- References could be included in Table 1, or at least in its caption.",
            "summary_of_the_review": "The paper introduces a new approach to generalized novel views and poses synthesis. Although it mainly put together existing methods (Kwon et al. (2021) and Wang et al. (2021)), it achieves state-of-the-art results. Therefore, I believe the contribution is worthy of publication.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_Ke1f"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_Ke1f"
        ]
    },
    {
        "id": "1XfBqsyl7w",
        "original": null,
        "number": 2,
        "cdate": 1666628339664,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666628339664,
        "tmdate": 1666628339664,
        "tddate": null,
        "forum": "-ng-FXFlzgK",
        "replyto": "-ng-FXFlzgK",
        "invitation": "ICLR.cc/2023/Conference/Paper5716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper aims to obtain Neural Image-based Avatars (NIA) for synthesizing novel views and poses of an unseen human from sparse input images. NIA is derived from a neural blending network to integrate 1) NeRF-based body representation conditioned on parametric SMPL human body model and 2) neural image-based rendering.",
            "strength_and_weaknesses": "[Strengths]\n+ The motivation is well-written and easy to follow. \n+ The citations are sufficient.\n+ The authors provide several visualizations for realizing their rendering results.\n\n[Weaknesses] \n- The proposed idea novelty is limited. The framework of NIA is similar to [Zhao et al. (2022)] since the two major steps of both methods are generalizable implicit body representation and sparse views blending. The core design of NIA is the neural appearance blending module (sec. 3.2), yet no ablation study to discuss and analyze why such a design helps.\n- Three assumptions exist in this work: 1) the calibration parameters of the multi-view input images are known; 2) the foreground human region masks are known; 3) the fitted SMPL models are available as prior. The first two assumptions are also seen in other methods; however, yet the third assumption is new in this work. The third assumption seems too strong since the SMPL model estimation is a vital factor affecting human NeRF rendering. Therefore, while assuming the fitted SMPL models are available as a prior, it is doubtful that the comparison experiments are fair enough compared with other methods. Notice that Table 4 (i) shows that the NIA w/o image-based rendering (with the third assumption) has already better than most other methods.\n- In sec 3.3, the description of \u201cthe 3D coordinates of the posed SMPL vertices in both reference and target spaces are known by the nature of motion-tracked SMPL model\u201d is another strong assumption that makes the NIA available to avoid the finetuning step. It is unclear why the such assumption is available while dealing with the unseen subject with unseen poses.\n- The comparison experiments are not convincing. It seems that some existing methods are reimplemented since the reported results of these methods (degraded) in this paper are not the same as their published ones. It is better to annotate which results are reimplemented and which are not.\n\n- Some implementation details are not clear:\n1) The limitations of the sparse input images. Are there any constraints for view directions?\n2) The feature representation of P_n is not defined;\n3) The design of two MLPs in equation (1) is different from NeRF [Mildenhall et al. (2020)]. Why is such a design reasonable, and what is its advantages?\n4) The architecture of F_B in equation (2). Does (2) means that it has one single model for a specific value of N?\n5) SE(3), x^can, k-th part sampling method, function R(), function t() in sec. 3.3;\n\nReference:\n[Zhao et al. (2022)] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In CVPR, 2022.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity is not sufficient.\n\nThe framework of NIA is similar to [Zhao et al. (2022)], weakening its novelty.\n\nReproducibility is limited due to its complex system.",
            "summary_of_the_review": "The primary concern of this paper is its limited novelty. The key idea is to learn to adaptively combine the parametric body model-based NeRF and image-based rendering techniques, which largely overlap the method proposed by Zhao et al. (2022). Besides, the assumptions of \"the fitted SMPL models are available as prior\" and \"the 3D coordinates of the posed SMPL vertices in both reference and target spaces are known\" are too strong to compare with the existing methods for the fairness concern.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_Pdgp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_Pdgp"
        ]
    },
    {
        "id": "1hNw9dPGbq",
        "original": null,
        "number": 3,
        "cdate": 1666629710537,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666629710537,
        "tmdate": 1666629710537,
        "tddate": null,
        "forum": "-ng-FXFlzgK",
        "replyto": "-ng-FXFlzgK",
        "invitation": "ICLR.cc/2023/Conference/Paper5716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper combines two rendering modules for synthesizing novel views and novel poses of human bodies from sparse multi-view images. \nThe first one is a body representation based on neural radiance fields and SMPL, extracting pixel-to-image features from each view, and attaching features to SMPL vertices. In this way, a coarse feature map for each view can be constructed, which is further converted into dense feature voxels through convolutions. Multi-view features are obtained through view-wise self-attention processing, and colors are finally obtained through MLPs.\nThe second one is a neural image-based rendering module, which can improve the details of the result by directly obtaining the pixel color from the source view image.\nResults of the two modules are finally combined together with the prediction weights.",
            "strength_and_weaknesses": "**Strength**    \n* This method can achieve good results on unseen views and unseen poses. Thanks to the image-based rendering method, it has good generalization performance across datasets.\n* The ablation experiments show that the fusion of implicit body representations and image-based rendering plays a complementary role.\n* Sufficient experiments have proved the effectiveness of the method.\n\n\n**Weaknesses**   \n* When the number of viewing angles is reduced to 1, the results are greatly degraded. Which part of this degradation comes from deserves further analysis, e.g. with regard to multi-view consistency.\n* The appearance blending module shows effectiveness in visualization and quantitative studies. However, why it works or the mechanism it works is not demonstrated clearly.\n* The method directly combines two rendering methods, which both exist in previous works. Any new insight about the pipeline/module design needs to be clarified.",
            "clarity,_quality,_novelty_and_reproducibility": "* **Clarity**: Most contents are clear and easy to follow.\n* **Quality**: The proposed method has been evaluated and studied on substantial benchmarks and settings. Especially, experiments are also performed on cross-dataset settings.\n* **Novelty**: The method directly combines two rendering methods, which both exist in previous works. This makes the novelty a little bit in discount.\n* **Reproducibility**: Most implementation details are provided. It is likely to reproduce the method. Code is not available.",
            "summary_of_the_review": "Through extensive experiments, this paper shows better performance than previous methods. And made a preliminary exploration on cross-dataset settings. Some questions/issues need to be fixed. The influence of the view number and the mechanism of appearance blending need to be clarified. Any new insight about the pipeline/module design needs to be further discussed to rebut the novelty concern.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_28Gy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_28Gy"
        ]
    },
    {
        "id": "CBsrgObwkuE",
        "original": null,
        "number": 4,
        "cdate": 1666690084000,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666690084000,
        "tmdate": 1666690084000,
        "tddate": null,
        "forum": "-ng-FXFlzgK",
        "replyto": "-ng-FXFlzgK",
        "invitation": "ICLR.cc/2023/Conference/Paper5716/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a human full-body avatar model that combines human NeRF and image-based rendering methods. The proposed model achieves significant improvement for unseen pose and unseen person synthesis. Extensive quantitative and qualitative evaluations have been conducted to measure the model's effectiveness.",
            "strength_and_weaknesses": "Paper strengths:\n- The proposed method is novel in combining human NeRF and image-based rendering.\n- The proposed method shows clearly superior performance both qualitatively and quantitatively.\n- Experimental evaluations are extensive and comprehensive.\n\nPaper weaknesses:\n- It seems unclear how much the current model's performance depends on accurate SMPL fitting. Would the proposed model be compatible with in-the-wild selfie videos followed by a single-view SMPL estimation?\n- I am a bit confused by the feature pooling in Equation (1) which currently does not consider visibility, should invisible views be ignored here?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly-written and shows high quality. I believe the work is original as far as I'm aware.",
            "summary_of_the_review": "Overall speaking I think this is a solid paper. The method is novel, the results are significantly better than previous state-of-the-art, and the evaluations are thorough. My concerns are minor and can be addressed by additional discussions. Therefore, my initial rating for the paper is accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_4ShL"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5716/Reviewer_4ShL"
        ]
    }
]