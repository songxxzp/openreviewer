[
    {
        "id": "L3fKobwqpo",
        "original": null,
        "number": 1,
        "cdate": 1666548484502,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666548484502,
        "tmdate": 1666548484502,
        "tddate": null,
        "forum": "mbxz9Cjehr",
        "replyto": "mbxz9Cjehr",
        "invitation": "ICLR.cc/2023/Conference/Paper3277/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of meta-safe reinforcement learning (Meta-SRL) through the CMDP-within-online framework to establish the first provable guarantees in this important setting. It obtains task-averaged regret bounds for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in a static environment, or task-relatedness in a dynamic environment.\n\nFurthermore, it enables the learning rates to be adapted for every task and extends our approach to settings with a competing dynamically changing oracle. Finally, experiments are conducted to demonstrate the effectiveness of the approach.\n",
            "strength_and_weaknesses": "Strength: Propose the a novel CMDP-within-online framework where the within-task is CMDP and the meta learner aims to learn the meta-initialization and learning rate. It shows that the task-averaged regrets for optimality gap (TAOG) and constraint violations (TACV) diminish with respect to both the number of steps for the within-task algorithm M and the number of tasks T.  It adapts the learning rates for each task\nto a dynamic environment.\n\nWeakness: \n\n1. the notation $\\nu^*_t$ in page 2 is unclear. What is $\\pi^*$? It makes harder to understand this paper.\n\n2. The primal approach explained in Section 2.1 is confusing. Why the sub-optimality gap in (2) holds by running CRPO? What is CRPO? Is CRPO an online or offline algorithm? There is no explanation about CRPO and no comments about the \"carefully chosen parameters\". It is not helpful for people who is not familiar with CRPO and makes the paper not self-contained. Those observations make the paper hard to read.\n\n3. Lemma 1 relies on $D^*$ is known and Theorem 3.1 relies on approximation error of DualDIce. The whole algorithm/analysis seems to be a composition of existing analysis (DualDice+CRPO), which might lack generality. It is hard to understand what is the key contribution of this paper.\n\n4. Theorem 3.3 replies on the assumptions of two regret upper bound $U^{init}_T$ and $U^{sim}_T$. There is no explanation regrading why those assumptions are reasonable. With those assumptions, the hardness of analysis is largely diminished.\n\n5. In the experiment, only frozen lake and acrobot are considered. How about the your algorithm for Mujoco environments?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper is somewhat unclear. ",
            "summary_of_the_review": "Please address the questions I mentioned above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_GvYx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_GvYx"
        ]
    },
    {
        "id": "WxMHV58puI",
        "original": null,
        "number": 2,
        "cdate": 1666645080089,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666645080089,
        "tmdate": 1669653318243,
        "tddate": null,
        "forum": "mbxz9Cjehr",
        "replyto": "mbxz9Cjehr",
        "invitation": "ICLR.cc/2023/Conference/Paper3277/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the meta-learning problem regarding various safe RL tasks that are modeled by constrained Markov decision processes (CMDPs).  The authors proposed a CMDP embedded online learning framework to generalize learnt safe policies to other unseen CMDP environments. In theory, the authors proved regret bounds in terms of within-task optimality gap and constraint violations in both static and dynamic regret metrics. Finally, some comparison experiments are provided to show the effectiveness of the proposed method.  ",
            "strength_and_weaknesses": "Strengths:\n\n- The studied meta learning problem is motivated by applications of learning a safe policy. This is a useful generalization of meta-learning to  constrained Markov decision processes, which allows to adapt the learnt policy to new constraints quickly. \n\n- The propsoed CMDP embedded online learning framework is practical in sense that it only relies on the inexact solution to sub-CMDP  tasks and data trajectories from previous tasks. \n\n- When the task similarity is bounded, the authors provide theoretical guarantees on both task-within optimality gap and constraint violation in static regret bound, under some assumptions on policy initializations. For practical use, the authors generalize this result to a dynamic regret bound, which is more useful since it does not assume boundedness of the task-similarity. \n\nWeaknesses:\n\n- The provided CMDP embedded online learning framework builds on a simplified version of the existing CRPO algorithm. Although this simplification permits convenience in analysis, it brings some restriction to the applicability of the proposed algorithm. \n\n- In the meta-learning problem with multiple tasks, the static regret seems to be not very useful since the bounded ask similarity is assumed and environments associated with different tasks are different. Otherwise, this becomes a single task problem which doesn't have to necessitate meta-learning.\n\n- The authors mentioned the hardness of bounding regrets in meta-learning by citing Kwon et al. (2021). This seems to be the key challenge to establish the theoretical guarantees. Although a series of assumptions are made, it is not very clear if they are necessary to address this challenge. In addition, the challenge from constraint seems to be unrevealed yet.\n\n- The proposed CMDP embedded framework relies on DualDICE to estimate state-action distributions. However, DualDICE is an existing offline RL algorithm for the standard MDPs. It is more natural to think of some off-line CMDP algorithms that respect the constraints more effectively. \n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the paper has a clear delivery except for some ambiguity. Since the paper builds on several existing algorithms, mixing notation from different sources make it hard to follow the main results. More efforts need to be made to explain the proposed algorithm well. Some assumptions or techniques are introduced in the middle of the paper, while it is less discussed how do they address the key challenge. In addition, many references are mixed in the content, which is a distraction to readers.   \n\nThe paper presents a new meta learning framework for safe RL, which is useful to generalize across different safe learning tasks. To achieve this, the proposed method combines several existing techniques in a clever way. However, the analysis and assumption mostly build on existing results, the novelty in which requires justification. The experiment utilizes a simple RL environment which might be too simple for meta-learning to generalize.  \n\nHere are some other questions for consideration: \n\n- It is useful to summarize an abstract meta algorithm at the beginning instead of delaying Algorithm 1 to the end of the paper.\n\n- Whey we can't use the original CRPO results? Assuming exact action-value function makes the proposed algorithm and theory impractical. \n\n- How does Assumption 1 address the hardness results from Kwon et al. (2021)? How are they related to your constrained learning problems?\n\n- Assumption 2 need to be clarified: 'o-minimal structure'.\n\n- Please check your citation formats. They are not standard. \n\n- It is useful to verify the effectiveness of the proposed method in other environments, e.g., humanoid? ",
            "summary_of_the_review": "All claims of the paper are supported by proofs or references. Most of them are well-supported, but some cited results are vague or modified in an ambiguous way. The proposed meta learning method combines existing RL algorithms for safe RL and theory builds on existing results and assumptions. The experiments are based on some basic RL environments. \n\n\n============================\n\nPOST-REBUTTAL. Thank you for your response. Since most of my concerns are addressed, the score has been updated. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "The paper concerns the meta-learning problem with multiple constrained learning tasks. It is important to discuss the pros/cons of the proposed framework when engineers apply the proposed method to problems with fairness constraints on different tasks. ",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_fmH2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_fmH2"
        ]
    },
    {
        "id": "wPX9aPNU_r",
        "original": null,
        "number": 3,
        "cdate": 1666903387718,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666903387718,
        "tmdate": 1666903387718,
        "tddate": null,
        "forum": "mbxz9Cjehr",
        "replyto": "mbxz9Cjehr",
        "invitation": "ICLR.cc/2023/Conference/Paper3277/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The proposed method concerns safe reinforcement learning as a constrained MDP problem. In particular, the authors consider safe meta RL, a method to deal with dynamically shifting environments. Technically, the method is realized using a so-called \u2018CMDP-within-online\u2019 framework, where a constrained learning task is performed \u2018within\u2019, and the resulting policies are input for the meta learner. Theoretical regret bounds and estimation errors (regarding sub-optimality) are given, and the expected theoretical learning rates are derived. \n",
            "strength_and_weaknesses": "++Important problem, clearly fitting into ICLR\n++Strong technical contribution\n\n\u2014Experimental evaluation is very sparse. \n",
            "clarity,_quality,_novelty_and_reproducibility": "I think this is a very good paper, rather clear and of high quality. I vote for acceptance, but would encourage the authors to expand on the experimental evaluation, for instance adding one environment, and making the distributional shift in environments clearer. I list a few questions to the authors below.\n\nPerhaps relevant to add to the literature is the recent paper listed below that deals with robust MDP learning against shifting environments\n\nSuilen et al. Robust anytime learning of Markov Decision Processes. NeurIPS 2022.\n\n\nQuestions\n\n\u2014 how is it justified that the \u2018meta-initialization\u2019 policy is \u2018fair\u2019 that is, has full support over the state/action space? I will necessarily not be safe, if I\u2019m not mistaken. Please discuss this requirement.\n\n\u2014 with long sequences of learning tasks, how does the approach perform in sparse reward settings which usually require a lot of data?\n\n\u2014 should the safety constraints also be adapted with shifting environments, or what is the motivation to have them static?\n\n\u2014 how do the individual tasks compare to a safe policy improvement problem (Thomas et al), where one aims to (safely, or better, reliably) find a policy that outperforms the behavior policy?\n",
            "summary_of_the_review": "Strong paper, experimental evaluation should slightly be improved.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_DCQm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3277/Reviewer_DCQm"
        ]
    }
]