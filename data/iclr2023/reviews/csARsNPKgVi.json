[
    {
        "id": "-OEyBHZvPq",
        "original": null,
        "number": 1,
        "cdate": 1666227947284,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666227947284,
        "tmdate": 1666227947284,
        "tddate": null,
        "forum": "csARsNPKgVi",
        "replyto": "csARsNPKgVi",
        "invitation": "ICLR.cc/2023/Conference/Paper252/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, authors proposed AutoSKDBERT, a new knowledge distillation paradigm for BERT compression, that stochastically samples a teacher from a predefined teacher team following a categorical distribution in each step, to transfer knowledge into student.\n\nWith extensive experiments on GLUE benchmark show that the proposed AutoSKDBERT achieves state-of-the-art score compared to previous compression approaches on several downstream tasks.",
            "strength_and_weaknesses": "Strength \nAuthors proposed a novel learning algorithm to learn from multiple teachers.\n\nWeaknesses\nThis paper only focused on bert. My understanding is the proposed method can be adapted to other network architecture. The impact would be larger if such method is independent of the network architecture of teacher/student. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nOverall the clarity is good.\n\nSome questions:\nIn the second paragraph in Introduction, \"the ensemble of multiple teachers are not always more effective than the single teacher for student distillation........ On the other hand, between the large-capability teacher and small-capability student, there is a capability gap which can be prone to unsatisfactory distillation performance\" I wonder why the capability gap of student and teacher can be used to explain why the *ensemble* of multiple teacher? Is it because the capacity is low in student, so adding more capacity (by ensemble) does not help?\n\nIn table 1, I am a bit confused why different data set uses so many different metric. It would be helpful to report 2-3 metrics for all dataset, to avoid potential cherry picking.\n\n\nQuality, Novelty\n\n\nHere are some questions:\nIt would be also interesting to see the training time for distilling of the proposed method compared to other existed method.\n\nGiveh there are multiple metrics, it is hard for readers to understand the magnitude of the improvement. It would be helpful to add information like standard error/confidence interval for each metric.\n\nFor the comparison benchmark, which of them are using multiple teachers? It is important to have a fair comparison given the improvement can come both from the proposed distilling as well as more teachers. I would recommend add some simple heuristic based distillation method with multi-teacher as benchmark.\n\n\nReproducibility\n\nOverall the experiment description is clear, and data used in experiments are open-sourced public data. However, it may still hard to reproduce without the experiment script.\n\nFor \"The hyper-parameters for student distillation.\", it would be helpful to determine why different dataset has different parameter. Are those hyper-parameters recommended by other research? \n\n*More important*, \"The hyper-parameters for categorical distribution optimization.\" how such parameter are selected? Is the algorithm sensitive to such parameters?\n\nAlso it may be helpful to add how multi-teacher is used? Is it just use all teachers in all examples? As there are so many distill methods, it would be helpful to add details for reproducibility.",
            "summary_of_the_review": "I choose the final decision 'marginally below' given my questions above as well as the proposed method for multi-teacher is only studied on BERT. If authors are able to clarify and extend the work, I would be happy to reconsider the decision.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "None",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper252/Reviewer_FHc1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper252/Reviewer_FHc1"
        ]
    },
    {
        "id": "b9Vtmuom_go",
        "original": null,
        "number": 2,
        "cdate": 1666247152101,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666247152101,
        "tmdate": 1666247152101,
        "tddate": null,
        "forum": "csARsNPKgVi",
        "replyto": "csARsNPKgVi",
        "invitation": "ICLR.cc/2023/Conference/Paper252/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Given a predefined teacher team, the paper aims at finding an optimal categorical distribution of these teacher models for student network distillation. Rather than simple ensemble learning (uniform weight), the paper proposes a two-stage selection and optimization strategy. In the first stage, ineffective teachers are identified and discarded by optimizing the categorical distribution. In the second stage, the categorical distribution is further tuned for effective teacher teams. The paper conducts experiments on the GLUE benchmark and make comparison with the most relevant works such as DistilBERT and TinyBERT.",
            "strength_and_weaknesses": "**Strength**:\n\n1. The formulation of the problem is clear and the paper is well-written.\n2. Most relevant works are discussed. The experiments show that AutoSKDBERT achieves good results.\n\n**Weaknesses**:\n1. To show that optimizing the categorical distribution is necessary, I wonder whether a single-teacher KD is enough. Table 1 shows some results. However, I would like to see the complete results from T1 to T14. Besides, does \"Best Single Teacher\" refers to the teacher that can achieve the best performance for student distillation or the teacher itself that achieves the best performance?\n2. The novelty of the proposed method is limited. ",
            "clarity,_quality,_novelty_and_reproducibility": "The formulation of the problem is clear and the paper is well-written. However, the novelty is quite limited. The paper presents experimental details for reproducibility.",
            "summary_of_the_review": "Although the proposed method achieves decent results, the technical novelty is quite limited. Optimizing the categorical choice while optimizing the network parameters is a widely studied bilevel optimization problem in neural architecture search and other areas. Another important concern is that a more simple baseline should be considered such as single best teacher and Gumbel softmax optimization to systematically study whether it is necessary to optimize the categorical distribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper252/Reviewer_SQHt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper252/Reviewer_SQHt"
        ]
    },
    {
        "id": "KiSI1cU-Km",
        "original": null,
        "number": 3,
        "cdate": 1667502954632,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667502954632,
        "tmdate": 1667502954632,
        "tddate": null,
        "forum": "csARsNPKgVi",
        "replyto": "csARsNPKgVi",
        "invitation": "ICLR.cc/2023/Conference/Paper252/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposed a new knowledge distillation paradigm for BERT, AutoSKDBERT, which stochastically sampled a teacher from the predefined teacher team by a categorical distribution. They proposed a two-phase optimization framework to better incorporate the categorical distribution and student model. To alleviate the gap between categorical distribution optimization and evaluation, they proposed an SSWO strategy for optimization. The experimental results show superiority compared with strong baselines.",
            "strength_and_weaknesses": "### Pros:\n1. The proposed method is simple yet effective, and it provided a more flexible way to give full play to the teacher team.\n2. The experimental results show obvious superiority compared to the SOTA baselines on the GLUE benchmark.\n\n### Cons:\n1. One critical problem is how to choose the ineffective teacher number, i.e., $m$. It is a very important parameter in the whole paradigm. However, as claimed in Sec 3.2.2, $m$ is a hyperparameter and it may differ a lot for different tasks.\n2. As claimed in Sec 3.3.2, AutoSKDBERT delivers 25 categorical distribution candidates and selects the best one. How to produce 25 candidates in detail?\n3. As stated in Sec 3.3.1 and Sec 4.1, are there 25 categorical distribution candidates in both phase-1 and phase-2? Or phase-1 will only produce 25 candidates in the ablation study experiments?\n4. In Sec 3.3.1, what does *trains them from scratch* mean? What is *them*? the student model?\n3. In figure 2, we can see $T_{14}$ plays a significant role, but $T_{13}$, which has the same number of parameters as $T_{14}$, is much weaker. The only difference is that $T_{14}$ adopts *whole word masking*. Since *whole word masking* is signally effective, why not adopt it on other teacher models? \n4. Because $T_{14}$ is a new type of BERT model with better performance, do the baselines use such a model as one of the teacher models? If not, what are the results without $T_{14}$? What if the baselines use the same teacher models?\n6. Since some teacher models, e.g., $T_{14}$, perform distinctly better, it will be good to compare with the best single-teacher KD.\n5. What's the number of teacher models of the baselines? Is it a fair comparison if the baselines use fewer teacher models than 14?\n8. In Sec 4.2, for the CR setting, the training involves all the teachers, why not use the same way in evaluation? Then it will be consistent between training and evaluation and can compare with SSWO more fairly.\n1. It seems unclear about the categorical distribution initialization module. Why is the initialized $\\theta_i$ like shown in Figure 1, i.e., why are 0.199, 0.201, 0.199, 0.200, and 0.201?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper provides a new paradigm to distill a student model from multiple teacher models. It's mainly about application and techniques. Some critical parameters need manual tuning.\n- Some clarification of details is not very clear. But the main idea and motivation are clear.\n- This work should be original.\n",
            "summary_of_the_review": "Overall, the proposed method is simple and easy to implement. And it could raise better performance than SOTA. But there are lots of tricks to boost the performance, like 1. generating 25 candidates, training 25 student models, and choosing the best one; 2. mutual-tuning $m$; 3. using a stronger teacher model (*whole word masking*). However, there lack of details and analysis of the tricks. Further, the clarification should be clearer.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper252/Reviewer_jsZh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper252/Reviewer_jsZh"
        ]
    },
    {
        "id": "c025OJo6q_",
        "original": null,
        "number": 4,
        "cdate": 1667581056341,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667581056341,
        "tmdate": 1667581056341,
        "tddate": null,
        "forum": "csARsNPKgVi",
        "replyto": "csARsNPKgVi",
        "invitation": "ICLR.cc/2023/Conference/Paper252/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a knowledge distillation approach with multiple teachers by introduce a weighting on teachers, and the weights are optimized in a bi-level manner. It achieves state-of-the-art results on several GLUE tasks compared with other model compression methods.",
            "strength_and_weaknesses": "Strength:\n* The authors propose a reasonable bi-level optimization for the weighting of teachers and achieve strong results.\n\nWeakness:\n* The paper writing is comparatively weak, e.g., The figure 1 is even harder to understand than the main text part.\n* Lack of meaningful analysis and discussion towards the result numbers. For example, overall the proposed approach performs on par with MoBERT. Why in some tasks it performs worse? Is the proposed approach orthogonal to the previous methods? Will the performance be better if multiple techniques are applied together?",
            "clarity,_quality,_novelty_and_reproducibility": "Some places of the paper writing is unclear. The proposed method is novel, and the experimental setting looks clear.",
            "summary_of_the_review": "This paper proposes a reasonable approach to weight multiple teacher in knowledge distillation, and gets good results on the GLUE benchmark, but the results are not discussed and analyzed thoroughly, and the paper writing is comparatively weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper252/Reviewer_n9fn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper252/Reviewer_n9fn"
        ]
    }
]