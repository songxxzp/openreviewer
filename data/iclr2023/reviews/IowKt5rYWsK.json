[
    {
        "id": "6mF-5ZONLBF",
        "original": null,
        "number": 1,
        "cdate": 1666371699099,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666371699099,
        "tmdate": 1666371699099,
        "tddate": null,
        "forum": "IowKt5rYWsK",
        "replyto": "IowKt5rYWsK",
        "invitation": "ICLR.cc/2023/Conference/Paper1136/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors present GPViT, a non-hierarchival transformer model for visual recognition. The authors argue that by not using a hierarchical approach, and by relying only on high resolution features they achieve better recognition when fine grained details are needed for recognition. The authors introduce the Group Propagation Block to exchange global information tokens which achieves runtime efficiencies under these scenarios. They evaluate their approach on image classification. semantic segmentation object detection and instance segmentation.",
            "strength_and_weaknesses": "Strengths:\n\nS1: tested on a variety of tasks\n\nS2: a good discussion on past transformer work, and the paper's place within this past work is discussed\n\nS3: thorough testing and a thorough ablation study is performed. The authors show that their model achieves good accuracy. For imagenet however it is not always clear to me that there is an improvement in terms of the number of parameters used. Discuss this in more detail in the results\n\n\n\nWeaknesses:\n\nW1: unless I missed it, will source code be provided. I did not notice a discussion on whether source code will be provided to make it easy for someone to replicate these results.\n\nW2: section 2: clarify what you mean by \"inductive biases\".\n\nW3: Eq 2: where is the concat operation shown in Figure 2?",
            "clarity,_quality,_novelty_and_reproducibility": "See my comments above. Overall the paper is well written. it has a few items that need clarification but i do not think these constitute ground for rejection. The model is described in detail. It is not clear to me whether source code will be provided. A clarification from the authors is needed on this. The results are good and I think the ICLR audience would be interested in the paper.",
            "summary_of_the_review": "As I indicated above, the results are good and I think the ICLR audience would be interested in the paper. The effort made to decrease the number of parameters should be appreciated by the ICLR audience.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_vWHX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_vWHX"
        ]
    },
    {
        "id": "o3ecdowd7NC",
        "original": null,
        "number": 2,
        "cdate": 1666673450227,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673450227,
        "tmdate": 1666673450227,
        "tddate": null,
        "forum": "IowKt5rYWsK",
        "replyto": "IowKt5rYWsK",
        "invitation": "ICLR.cc/2023/Conference/Paper1136/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a new transformer-based architecture that enables the extraction of high resolution feature maps from an images, while avoiding hierarchical downsampling in intermediate layers. To deal with the quadratic complexity of normal attention layers, a set of latent group representations is learned that collects information from the high resolution tokens. The latent group representations then mix their information based on an MLPMixer block and in turn the image patch tokens can cross attend to the update group features. Furthermore local attention is used for self-attention of the patch tokens. Using these principle, the complexity is linear in the number of groups and in the number of tokens, allowing a higher number of used input tokens. The resulting architecture is evaluated on several tasks (classification, detection, segmentation) and achieves good results compared to other methods with similar FLOP counts.",
            "strength_and_weaknesses": "Strengths:\n- While the idea to use such groups to reduce the complexity of attention is not really very novel, the application in this specific architecture is very interesting and show some promising results. Furthermore, a lot of the design choices seems rather general and similar systems could be applicable to other modalities such as point clouds, or potentially even multi-modal data.\n- The overall direction of not using intermediate downsampling is interesting and worth investigating.\n- The paper is easy to read and quite straightforward to follow.\n\nWeaknesses:\n- I think there is quite some rather related work that is not discussed. For example the Perceiver models by DeepMind have a different focus, but they are pretty similar in the underlying idea. Mask2Former also has a similar transformer part, albeit it works on top of a backbone to extract basic features and similar ideas where also explored in \"Generative Adversarial Transformers\". These are just a few I can recall off the top of my head. I wouldn't be surprised if there are a lot of other similar approaches. I think it is crucial that such related work is discussed with more care. Given that this is one main part of the contributions, the paper loses quite a bit of novelty in my mind. (And just because this uses an MLPMixer instead of vanilla attention does not make the model inherently different!)\n- I'm quite sad to see yet another paper that simply claims a method achieves state of the art results on some task, when it's obviously not true. Simply looking at other approaches, it becomes clear that the numbers are far from state of the art. I know the focus here is likely on models with similar FLOP counts, but in most of the sentences where claims about being state of the art are made, this fact is simply omitted. It would be very important to clarify this! And apart from that, it would also be interesting to see how well this type of architecture generalizes to bigger versions of a model with similar FLOP counts than the actual state of the art. This should clearly be clarified more!\n- Just looking at parameter counts and FLOP counts is not really giving a clear picture, but the actual throughput of a model is also important. (Have a look at the paper called \"The Efficiency Misnomer\".) Considering this architecture, I wouldn't be surprised if such a comparison would actually be favorable for the model, but I think it's sad that we don't see such numbers.\n- I think the idea of the approach is interesting and it would be great to see a bit more experiments with respect to the design choices that went into the different building blocks. E.g. how many groups should be used, or why is the MLPMixer used instead of normal self-attention? Indeed some of these things are discussed in the appendix, but I think they should be featured more prominently in the main paper in order to highlight the important aspects of this architecture design.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is pretty clear and easy to follow. Especially the overview figure of the architecture clearly explains the main building blocks. Even though I am not aware of approaches that follow the exact same approach, there are quite some very related existing methods that are not discussed here and I think this limits the novelty. Nevertheless a deeper view into this architecture could be interesting, but sadly the results here are more focused on showing \"state of the art\" performance. As such we also don't gain a lot of novel insights. Given the authors state that code will be released upon acceptance, I assume the results should be reproducible. Furthermore, the results are based on several MM toolkits, potentially further improving the reproducibility.",
            "summary_of_the_review": "To me the core approach is interesting, but I'm not very convinced by the FLOP constrained evaluations. Furthermore some important related work is missing. Together these things limit the novelty and contributions quite a bit in my opinion and as such I am leaning more towards rejecting the paper. But I'm interested to see the other reviews and the rebuttal and would be willing to change my mind here.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_oVeK"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_oVeK"
        ]
    },
    {
        "id": "z1H5SGMmNgI",
        "original": null,
        "number": 3,
        "cdate": 1667006114853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667006114853,
        "tmdate": 1667006114853,
        "tddate": null,
        "forum": "IowKt5rYWsK",
        "replyto": "IowKt5rYWsK",
        "invitation": "ICLR.cc/2023/Conference/Paper1136/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a non-hierarchical transformer model for visual recognition tasks (detection, segmentation). Unlike recently proposed hierarchical methods like Swin Transformer that use an hierarchical transformer architecture, exchanging global information between features is computationally expensive for non-hierarchical transformers. To deal with this challenge, the paper proposes an efficient  Group Propagation Block (GP Block) to exchange global information between high-resolution features. In a GP block, grouped features formed by learnable group tokens and then global information is exchanged between grouped features. Finally, global information in updated grouped features is returned to the image features through the transformer decoder. GPVIT is evaluated on  image classification, semantic segmentation, object detection, and instance segmentation and obtains state-of-the-art performance. Keeping parameters or FLOPs constant, GP-ViT shows improved performance over previous methods in all cases.  \n",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written. The method is explained well and carefully evaluated. \n2. The idea of group propagation is intelligent and is effective at reducing computational cost while keeping the architecture simple. \n3. The ablation studies and especially the explanation of how the architecture was built are informative and help explain the contribution of different components of the architecture.\n4. The method obtains SOTA performance across tasks, with impressive gains when compared to prior methods with similar number of parameters or FLOPs. \n\nQuestions:\nHow would you scale up GPViT beyond L3 to obtain better performance? Are there marginal gains to be had on current datasets by further scale up?\n",
            "clarity,_quality,_novelty_and_reproducibility": "See  above. ",
            "summary_of_the_review": "The paper proposes a novel, yet uncomplicated, architecture for non-hierarchical transformers for visual recognition tasks and obtains SOTA performance on these tasks. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "10: strong accept, should be highlighted at the conference"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_6Qyc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1136/Reviewer_6Qyc"
        ]
    }
]