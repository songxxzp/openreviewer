[
    {
        "id": "74qMK14njlz",
        "original": null,
        "number": 1,
        "cdate": 1666537824806,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537824806,
        "tmdate": 1668802586766,
        "tddate": null,
        "forum": "eSQh8rG8Oa",
        "replyto": "eSQh8rG8Oa",
        "invitation": "ICLR.cc/2023/Conference/Paper5399/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes the Greedy Actor-Critic approach, which uses two policies, and a new CCEM approach: a conditional (on states) varianta of CEM. The critic is trained using Sarsa updates (with a replay buffer). A *proposal policy* (one of the two policies) is used to sample a distribution of actions, which are then evaluated and ranked by the critic. The top N of these actions are used for training the policies. The main policy (the second of the two policies) is used to select actions to be executed in the environment. Both policies are updated such that they increase the likelihood of the top N actions mentioned previously, but the proposal policy additionally has an entropy regularization term (which the main policy doesn't).",
            "strength_and_weaknesses": "**Strengths**:\n- While the paper is rather technical and dense, in general it is possible to follow along with the main ideas.\n- Theoretical and algorithmic contributions appear to be sound, with good emprical results.\n\n**Weaknesses**:\n- In several parts of the more technical writing, it seems that there are slight mistakes and confusing notation (notation suddenly changes or is not explained at all), making some parts difficult to follow (see detailed comments below).\n- There are some statements where I'd appreciate a bit more intuition/explanations (see detailed comments below).\n\n---\n\n**After discussion**: the revision of the paper has fixed and clarified many of my concerns, and I have updated my score.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: at a high level good, but have some concerns for some details.\n\n**Quality**: sound as far as I can tell.\n\n**Novelty**: good.\n\n**Reproducibility**: only concern I have is due to clarity concerns for some technical details, as mentioned above.\n\n---\n\n**Detailed Comments**:\n- p.2: \"attempt to obtain (unbiased) estimates of this gradient\" --> what is \"this gradient\"? I think you mean \"the gradient of this objective\"?\n- Third paragraph of Section 2: suddenly a boldface $\\boldsymbol{w}$ is introduced as subscript for a policy $\\pi$. Earlier in the paper, $\\theta$ was used to denote the parameters of a policy $\\pi_{\\theta}$. Why did this suddenly change to $\\boldsymbol{w}$? Is it the same thing, or something else? Without explanation, I would assume it was just the same thing... but later in the paper I keep seeing unexplained mixes of $\\boldsymbol{w}$, $\\theta$, and even yet again a new $\\boldsymbol{w}'$. I can guess that that last one is used for the proposal policy, due to how it is used in the update that also involves entropy regularization... but I really shouldn't have to guess this, this should be clearly explained. In the later parts I think $\\theta$ is more commonly used only as subscript for $Q$, but (especially early in the paper) also for policies, so... I continue to be confused throughout all of the paper.\n- p.3: \"for $h < N$\" --> I think this should be $h \\leq N$?\n- p.3: \"narrows around higher valued $\\theta$\" --> technically this can be read as saying that it is simply trying to make the values of the parameters $\\theta$ high. I think you rather mean that it narrows around parameters $\\theta$ that lead to policies that achieve high returns.\n- \"CEM, however, finds the single-best set of optimal parameters\" --> does it really find the optimal parameters? Or does it just try to approximate them?\n- Figure 1 has equations saying that policies are equal to certain gradients. Are they really equal to the gradients? Or are their parameters just updated according to those gradients?\n- p.4: \"The actor $\\boldsymbol{w}_t$\" / \"The proposal $\\boldsymbol{w]_t'$\" --> do these symbols actually denote the full actor/proposal [policy]? Or just their parameters?\n- At the end of page 4, I do not understand why a proposal policy is no longer necessary for problems with discrete action spaces, and why it would no longer be useful to sample a distribution of actions. To me, exactly the same intuition would still apply. I think it would be useful to provide more explanation here.\n\n\n**Other Minor Comments**:\n- p.3: \"are sample from\" --> are sampled from\n- Caption Fig 1: \"more quickly that\" --> more quickly than\n- The legends for Figures 2 and 3 need to be slightly bigger to be readable.\n- p.9: SOTA --> I know what the acronym means, but it is never fully written out and should not be used without fully writing out at least once.",
            "summary_of_the_review": "Potentially a good paper, but currently having slightly too many issues with imprecise statements and confusing / unexplained notation.\n\n---\n\n**After discussion**: the revision of the paper has fixed and clarified many of my concerns, and I have updated my score.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_y7Rc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_y7Rc"
        ]
    },
    {
        "id": "qHPdFpQrGnD",
        "original": null,
        "number": 2,
        "cdate": 1666604683456,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666604683456,
        "tmdate": 1666604683456,
        "tddate": null,
        "forum": "eSQh8rG8Oa",
        "replyto": "eSQh8rG8Oa",
        "invitation": "ICLR.cc/2023/Conference/Paper5399/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes an Actor-Critic algorithm in which the actor learns to pursue (imitate) the greedy policy of the critic, using, with continuous actions, a method inspired from the Cross-Entropy method (pursuing the top-K actions from a sample of relatively good actions). With continuous actions, the core contributions of the paper are the suggestion of using CEM to approximate $\\argmax_a Q(s, a)$, and the introduction of an entropy-regularized proposal policy $\\pi_{w'}$, to choose which actions to sample for computing the top-K of $Q(s, a)$.",
            "strength_and_weaknesses": "Strengths:\n\n- The algorithms is simple, elegant and performs very well even on challenging tasks (Atari games, Mujoco tasks)\n- The paper is clear and well-written\n\nWeakness:\n\nMy use of \"pursue\" in \"Summary of the paper\" is on purpose. This paper actually presents an algorithm for continuous-action Conservative Policy Iteration [1], which can also be seen as continuous-action state-dependent Pursuit [2]. There is a large and exciting field of Reinforcement Learning that considers gradient-free optimization of the actors, using imitation learning to pursue the greedy policy of critics. Such work include:\n\n- The Actor-Mimic [3], in which a DQN policy is distilled into an actor\n- Dual Policy Iteration [4], the formalism behind AlphaZero, in which the actor pursues a proposal policy (in the case of MuZero, obtained by planning with MCTS)\n- Approximate Conservative Policy Iteration [1], in which the proposal policy is the greedy policy of an on-policy critic. This paper, with discrete actions (Algorithm 2, discrete), is Approximate Conservative Policy Iteration\n- Bootstrapped Dual Policy Iteration [5], close to Approximate Policy Iteration, but that shows that off-policy critics (learned with Q-Learning instead of SARSA) work, leading to extremely high sample-efficiency and hyper-parameter robustness (claims also made in this paper, and mostly due using Dual Policy Iteration instead of Policy Gradient)\n- Batch/Offline Reinforcement Learning also has extensive literature on proposal policies and how to sample actions. For instance, Batch-Constrained Q-Learning [6] learns the proposal policy using a conditional Variational Autoencoder. This work is however only distantly related to this paper.\n\nTo my knowledge, the proposal policy presented in this paper, and its use to compute approximate greedy policies with continuous actions, is novel. The fact that it works is however not surprising, as the literature mentioned above consistently, over the past years, showed that doing something else that Policy Gradient for actor-critic training is very good indeed.\n\nMy points above lead to two remarks:\n\n- Would it be possible for the authors to have a look at the world of Conservative Policy Iteration, Dual Policy Iteration and Policy Distillation, and briefly discuss that in the paper?\n- Why is the critic learned with SARSA in this paper? [6] shows that using SOTA critic-learning algorithms (the proposed algorithm combines twin-delayed critics (TD3) with Boostrapped DQN) leads to significant gains in sample-efficiency. The only challenge is that all these learning algorithms assume the existence of $\\argmax_a Q(s, a)$, so, discrete actions, but this paper happens to propose a way to compute that max and argmax using CEM and a proposition policy.\n\n[1]: Kakade, S., Langford, J.: Approximately optimal approximate reinforcement learning. In: International Conference on Machine Learning (ICML). pp. 267\u2013274 (2002)\n\n[2]: Agache, M., & Oommen, B. J. (2002). Generalized pursuit learning schemes: New families of continuous and discretized learning automata. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 32(6), 738-749.\n\n[3]: Parisotto, E., Ba, J. L., & Salakhutdinov, R. (2015). Actor-mimic: Deep multitask and transfer reinforcement learning. arXiv preprint arXiv:1511.06342.\n\n[4]: Sun, W., Gordon, G. J., Boots, B., & Bagnell, J. (2018). Dual policy iteration. Advances in Neural Information Processing Systems, 31.\n\n[5]: Steckelmacher, D., Plisnier, H., Roijers, D. M., & Now\u00e9, A. (2019, September). Sample-efficient model-free reinforcement learning with off-policy critics. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 19-34). Springer, Cham.\n\n[6]: Scott Fujimoto, David Meger, and Doina Precup. 2019. Off-policy deep reinforcement learning without exploration. In International Conference on Machine Learning. 2052\u20132062",
            "clarity,_quality,_novelty_and_reproducibility": "- Significance: very high. The results presented in this paper are very strong, and the approach solves a problem (continuous-action Conservative Policy Iteration)\n- Novelty: moderate. There is a whole field of RL in which the actor imitates some (greedy) function of the critic, and this work builds on/reinvents parts of that.\n- Calrity: high. The paper is well-written and easy to follow.",
            "summary_of_the_review": "Very good idea, but learning the critic with SARSA? It seems that it helps with the proof, but maybe not the empirical performance. Other algorithms that train the actor towards target actions (as opposed to using Policy Gradient) should be discussed in the paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_BG9i"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_BG9i"
        ]
    },
    {
        "id": "u4PVEfHxA23",
        "original": null,
        "number": 3,
        "cdate": 1667410690966,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667410690966,
        "tmdate": 1670869996220,
        "tddate": null,
        "forum": "eSQh8rG8Oa",
        "replyto": "eSQh8rG8Oa",
        "invitation": "ICLR.cc/2023/Conference/Paper5399/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new policy improvement scheme called conditional cross-entropy method (CCEM), which leverages the cross-entropy method in global optimization to keep track of the set of maximal actions. The proposed algorithm is a variant of the actor-critic methods and called GreedyAC, which (i) uses CCEM for updating the actor and (ii) applies entropy regularization on the loss function of the actor. The proposed method enjoys the advantages of (i) being non-sensitive to the entropy regularization parameter, (ii) concentrating on the greedy policy more slowly, (iii) having better empirical results in some environments than SAC, and (iv) enjoying policy improvement guarantee and convergence.",
            "strength_and_weaknesses": "**Strength**\n- This paper first introduces a new perspective of applying CEM to RL and thereby proposes the GreedyAC algorithm with theoretical convergence and improvement guarantee.\n- The proposed method is not sensitive to the entropy regularization parameter and appears to have some empirical advantage over the popular benchmark SAC.\n\n**Weaknesses**\n- While the theoretical results are interesting and could have their own merits, the algorithm analyzed in theory appear to deviate from the actual algorithm introduced in the paper (cf. Algorithms 2-3). \n- There are several concerns about the experimental results:\n    - SAC: The reported empirical performance of SAC looks quite different from the existing benchmarks. For example, from Figure 2, it is surprising that SAC fails to solve the simple Acrobot and MountainCar tasks. By contrast, in the existing benchmarks (e.g., Table 1 in https://arxiv.org/abs/1907.02057), SAC appears quite strong in these classic tasks of OpenAI gym. It is not clear what makes the difference, and further explanation is needed.\n    - Sensitivity: One of the claims is that GreedyAC empirically could be less sensitive to the entropy regularization parameter than SAC. However, it is not clear why this is true from the design or the convergence analysis. Further justification is needed for this claim, either from intuition, theoretical analysis or ablation studies.\n- Some components of GreedyAC look somewhat ad hoc and would require more explanation. For example, the proposal distribution plays an important role in the policy improvement of GreedyAC, and it relies on an entropy regularization term.\n- Several missing references on CEM for RL: There are several prior works on the combination of CEM and value-based methods, such as [Simmons-Edler et al. 2018] and [Shao et al. 2022]. These references are missing in this paper.\n\n[Simmons-Edler et al. 2019]  Riley Simmons-Edler, Ben Eisner, Eric Mitchell, Sebastian Seung, Daniel Lee (2019). \u201cQ-Learning for Continuous Actions with Cross-Entropy Guided Policies\u201d. In The Reinforcement Learning for Real Life (RL4RealLife) Workshop in the 36th International Conference on Machine Learning (ICML), 2019.\n\n[Shao et al., 2022] Lin Shao, Yifan You, Mengyuan Yan, Shenli Yuan, Qingyun Sun, Jeannette Bohg, \"GRAC: Self-Guided and Self-Regularized Actor-Critic.\" Conference on Robot Learning. PMLR, 2022.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**: \nThe writing is clear in most places. Here are some places that could be improved:\n- Regarding the convergence analysis (cf. Theorem A.1), only informal results are provided in the main text. While this arrangement could provide intuition, on my first read, it is a bit confusing to me what kind of convergence result has been established in the paper. I would suggest that the authors at least provide a formal statement of Theorem A.1 as this result appears to be one of the major components of the paper.\n- The caption of Figure 1 is somewhat confusing. It would be helpful to clarify the description of Figure 1.\n- It is not clear how to parametrize the percentile-greedy policy by some parameter $w$. \n\n**Novelty**\n\nPlease see the above comments about the weaknesses and the missing references.\n\n**Reproducibility**\n\nAs mentioned above, the performance of the baseline method are somewhat different from the existing public benchmarks.\n",
            "summary_of_the_review": "This paper presents a new perspective of applying CEM to policy improvement. While the theoretical result is interesting, there are some issues with whether the analysis actually captures the behavior of the proposed algorithm as well as with the empirical evaluation of the baselines and claim on sensitivity. Moreover, the novelty of this paper should be further clarified as there are some missing references that look relevant but not discussed in this paper.\n\n========== Post-rebuttal ==========\nI would like to thank the authors for providing a very detailed response. My main concerns, the difference between the algorithm and the analysis as well as the experimental results of the benchmark methods, have been addressed. As a result, I decided to raise my score from 5 to 6.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_zQ75"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5399/Reviewer_zQ75"
        ]
    }
]