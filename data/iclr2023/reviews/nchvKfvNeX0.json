[
    {
        "id": "6641F3EZFy",
        "original": null,
        "number": 1,
        "cdate": 1666361953696,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666361953696,
        "tmdate": 1666361953696,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper aims to show that for any learning problem formulated over target classes containing ReLU neural networks of a prescribed architecture, the number of training samples needed for an adaptive randomized algorithm to guarantee a given uniform accuracy scales exponentially both in the depth and the input dimension of the network architecture.",
            "strength_and_weaknesses": "### Strengths\n\n- The paper considers an unorthodox (but important and impactful) statistical learning problem: sample complexity for uniform accuracy (instead of expected accuracy). The motivation and impact of such an analysis are discussed carefully throughout the manuscript. The main ideas and techniques of the work were developed based on well-founded rationales and in general, the paper is well-written with a sufficient literature review.\n- One of the important (and maybe novel) insights of the work about neural networks in this learning setting is that while there exist functions that can be exactly represented by small neural networks, these representations cannot be inferred from samples because of the network\u2019s expressiveness.\n- The mathematical analyses of the work are rigorous, intuitive, and seem correct.\n- One main technical contribution of the work is to explicitly show that neural network models contain a large class of \u201cbump functions\u201d with estimable L^p norms and disjoint supports. Thus, with a \u201csmall\u201d number of observations, no algorithm would be able to distinguish those functions from the zero function, or one another. The sample complexity depends on p, and when p=\\infty (uniform accuracy), the dependency is exponential.\n- Beyond this direct application of the analysis, several components of its explicit computations would be of great interest to a broader audience. For example, the bump functions, which have support in B(y, 1/M) and are greater than 1/2 in B(y, 1/(2Ms)), are very similar in spirit to the concept of mollifiers and approximation of the Delta Dirac function. The fact that they can be represented by a standard feed-forward ReLU network could be a useful tool in other contexts.\n\n### Weaknesses\n\n- None noted.\n\n### Additional comments\n\n- As stated in the comments, I think the explicitness of Lemmas A.1 and A.2 are technically interesting, and some detailed descriptions of the insights of those constructions would be appreciated. For example, the steps (and the constructions of the layers) in the proof of Lemma A.2 have their own meaning (representing the hat functions, representing the bump functions, controlling the norms), and some verbal descriptions of what they do would make the parts more accessible. (I want to note that it\u2019s just an optional comment and will not affect my review decision.)",
            "clarity,_quality,_novelty_and_reproducibility": "(see Strengths and Weaknesses)",
            "summary_of_the_review": "The paper rigorously addresses a meaningful question in applied machine learning. The work provides novel insights into the learning problem, and its technical contributions might be of broad interest.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_zxQe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_zxQe"
        ]
    },
    {
        "id": "ePqDX740kyY",
        "original": null,
        "number": 2,
        "cdate": 1666640978516,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666640978516,
        "tmdate": 1666640978516,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors study the problem of learning classes of deep, feedfoward neural\nnetworks with ReLU activations through with guarantees in terms of $L^\\infty$\naccuracy, through the angle of sample complexity requirements. More precisely,\nthey consider a rather general model where a learner can adaptively query\npoints of the solid cube $[0, 1]^d$ for evaluations of an unknown deep ReLU\nnetwork $f$, and provide lower bounds on the number of queries necessary for\nany (possibly randomized) such algorithm to output an $\\varepsilon$\napproximation to $f$, for all possible choices of the network $f$ in a\nrestricted class (bounded number of neurons, bounded depth, and norm-bounded\nweights). The obtained lower bound grows exponentially in the dimension $d$ and\nthe network depth, demonstrating a strong barrier to learning in this setting.\nUpper bounds are derived in the same algorithmic setting (as before,\ncomputational complexity is not considered)\n\n",
            "strength_and_weaknesses": "## Strengths\n\n- The lower bound holds very generally and without conditions associated with\n  other SQ-type lower bounds that apply to more natural algorithms like GD/SGD.\n- The focus on $L^\\infty$ estimation has possible connections to\n  security-critical settings where one is interested in the ability to recover\n  the parameters of a neural network model.\n- The paper is very clearly written and well-referenced. It is possible to\n  understand the paper's main results at a technical level without having to\n  reference notation/etc. in the appendices because background information is\n  explained completely.\n\n## Weaknesses\n\nAlthough I have not checked the proofs, the authors' results seem reasonable to\nme. Therefore most of my comments here will not be weaknesses with the\n(well-written) paper per se, but rather questions about the motivation and\nimplications for practice of the theory.\n\n- The requirement $B \\leq 3d$ in Theorem 2.2 seems unnatural -- can this be\n  improved? Is there a \"functional\" description (e.g. from the proof) of what\n  this constraint implies?\n- The exponential dependence on dimension may be seen as not too\n  surprising: one needs a number of points exponential in dimension just to\n  cover the input space, and the class seems complex enough that it is really\n  necessary to have such a dense covering. What seems more interesting is\n  that it is also necessary to sample exponential in depth. However, one is\n  naturally curious about the extent to which this is simply because the\n  learning problem is extremely hard -- arbitrary depth-L networks can\n  parameterize very complex functions, with many linear regions (e.g.\n  following Mont\u00fafar's work) -- and whether the same exponential dependencies\n  would appear for learning e.g. a Sobolev or a Besov class. Another angle\n  on this question would be to consider how the bound would look for learning a\n  \"smoothed\" class of ReLU networks, e.g. those with a bounded number of\n  linear regions, or the like. The behavior that may be most interesting as a\n  theory for the practical successes of deep learning would be one that\n  captures the ability of deeper networks to *adaptively* represent complex\n  functions, when it is necessary, given sufficient samples -- and it seems\n  this kind of explanation is out of reach of the present theory, given that\n  the class being learned contains extremely complex functions implemented by\n  general depth-$L$ networks. (The authors mention this as a direction for\n  future work in the conclusion; it would be nice to hear some thoughts in this\n  context in the response.)\n- Similarly to the previous point, I am not completely convinced about the\n  connection the authors make between their result and adversarial robustness\n  -- although indeed in the setting of adversarial robustness one is interested\n  in $L^\\infty$ stability guarantees, these are generally not in the\n  teacher-student setting where the training labels are generated by some deep\n  neural network, but rather by some dataset with certain structures\n  (statistical, geometric, etc.). In this latter setting, the requirements to\n  be robust to worst-case corruptions may be quite different from the setting\n  that is considered here. I think this is especially pertinent with regards to\n  the $\\exp(L)$ dependence in the authors' lower bound -- how would the\n  corresponding lower bound look in the setting of a structured, labelled\n  dataset?\n\n\n## Minor / Questions / Comments\n- Is there a version of Theorem 1.1 that applies to networks of arbitrary\n  intermediate widths (like Theorem 1.4)?\n- In Theorem 1.1, when $q \\leq 2$ the result seems to be vacuous in high\n  dimensions unless $\\varepsilon$ goes to zero; on the other hand, it seems\n  that working through the corresponding bound in Theorem 2.2 implies a\n  nonvacuous result in high dimensions (it seems the dependence on width $B$\n  has been worst-cased to a dimension dependence in Theorem 1.1). Would it\n  make sense to state the result differently? Reading Theorem 1.1, I have the\n  sense that the bound is very loose with respect to Theorem 1.4 for \"mild\n  regularization\" (whereas with the actual dependence, the gap between the\n  bounds becomes much clearer).\n- Some comments on *what the class of functions implemented by the\n  architectures considered is* would be helpful here in assessing the degree\n  of difficulty of the learning problem, relative to some of the other\n  classes mentioned in Remark 1.3 -- e.g. in the proof idea section describing\n  the bump function, can we also know Sobolev estimates for these functions (to\n  better understand what the class being embedded is, effectively)? \n  - The proof sketch of the lower bound reminds of some of the description of\n  Bubeck and Sellke's work on robustness is there any connection here?\n  (Keywords like Baum's construction of memorizing networks and its\n  generalization by Bubeck et al., etc...)\n- Could the authors provide a proof sketch for the upper bound in Theorem 2.4?\n  Given that this upper bound applies to an adaptive, potentially randomized\n  algorithm, is it safe to assume that this upper bound applies to an algorithm\n  that is \"stronger\" than e.g. empirical risk minimization under a\n  distributional assumption, which as the authors mention in related work has\n  been considered in works with a similar flavor? I am just interested in how\n  to compare this upper bound to results I have seen in related work on $L^p$\n  $p \\ll +\\infty$ learning of ReLUs with certain algorithms.\n- It might be more correct to say that the \"minimal error over neural network\n  target functions\" is estimated/approximated, rather than \"computed\", on page 8.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper builds off various works on expressivity and learning of deep\nfeedfoward neural networks to establish a new result, with a motivation that\nseems to also be new. The work is clearly written and well-referenced.\nMathematical claims are very clearly written, without ambiguous notation.\n\n",
            "summary_of_the_review": "\nThe paper presents a solid contribution to the understanding of teacher-student\nlearning of neural networks to a high degree of uniform accuracy. I am unsure\nof the general implications of the theory given the student-teacher setting,\nbut I think the work may lead to many interesting follow-up investigations, as\nthe authors mention in the conclusion.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_eNk7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_eNk7"
        ]
    },
    {
        "id": "tJawMVu3Xo9",
        "original": null,
        "number": 3,
        "cdate": 1666990032899,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666990032899,
        "tmdate": 1669056025888,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Motivated by robustness of trained models, the paper studies the statistical complexity of learning the class of ReLU networks up to $\\ell_\\infty$-error (uniform error). In particular, they show that the sample complexity required for learning ReLU neural networks with $L $ layers each with width at most $3d$ (where $d$ is the input dimension) and bounded norm coefficients requires exponential in $d$ and $L $ samples. In contrast, learning to $\\ell_2$-error requires polynomially on $d$. They also give (almost) matching upper bounds. The results in the paper allow for the underlying algorithm to be adaptive in the selection of the data points, that is, it can query the points necessary to give the required guarantee. The key idea is to use the flatness of ReLU to show that the NN class contains functions of the following form: for any $y\\in \\mathbb{R}^d$ the function is non-zero only for a small ball around point $y$ in the space. Therefore, to learn the function from samples to $\\ell_\\infty$ accuracy, one needs to sample a point in every small ball, that is, cover the entire space which requires exponential in $d $ samples. The norm bounds on the weights governs the radius of these balls and the base of the exponent.",
            "strength_and_weaknesses": "Strengths: \n- The paper makes progress towards understanding the information theoretic limits of robust learning, which is an important problem in the light of adversarial vulnerabilities in current ML models. The lower bounds hold for all algorithms and not restricted to some computational models.\n- The paper is well-written and easy to follow. The authors do a good job of thoroughly discussing the theorem statements and their implications.\n\nWeaknesses:\n- I am not convinced by the technical depth of the main result and more generally the setting. It is worst-case in nature and for a very general class. This makes the lower bound weaker, since getting $\\ell_\\infty$ bounded error in such a setting seems naturally hard. The examples in Remark 1.3 where such a goal is tractable are restricted to polynomials/linear functions which have special structure in terms of interpolation.\n- Experiments are in very low dimensions. Trends for different errors also seem similar. Hence, it is unclear what the reader should take away from the experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "I have some questions that are not clear from the paper:\n- Why does the width need a dependence on $d$? What is the actual dependence on the width independent of $d$?\n- The construction of these bump functions uses only 2-layers to actually construct the \u201chat\u201d function and then the rest of the layers are pass through layers that affect only the scaling of the function (hence the Lipschitz constant). Similarly, the width shows up in its interplay with the norm constraint affecting the scaling. Assuming that the $\\ell_1$-norm of the weights is bounded by 1 at each layer would remove this blow-up. Is there any use of depth outside of this?\n- What are the differences with the constructions in Telgarsky 2015? I understand that the work deals with purely approximation and not sample complexity, but they do bound the total error in the interval.",
            "summary_of_the_review": "Overall, in my opinion, the worst-case nature of the result makes it not very interesting. The requirement of $\\ell_\\infty$-bounded error becomes extremely strong, and not very insightful for realistic models. As the authors point out in the limitations, studying this in a more reasonable sub-class of neural networks or understanding uniform error restricted to some manifold or under some perturbation model would be more useful and interesting.\n\nPost rebuttal: Increased the score from 3-> 5.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_Jti4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_Jti4"
        ]
    },
    {
        "id": "79ySqW3zS6Q",
        "original": null,
        "number": 4,
        "cdate": 1667249172503,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667249172503,
        "tmdate": 1667310244259,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides sample complexity upper bound and lower bound for learning high uniform accuracy ReLU networks. It shows that any learning algorithm recovers neural networks to achieve high uniform accuracy needs intractably many samples (exponentially depending on the input dimension, network width and network depth).\n",
            "strength_and_weaknesses": "Strength:  This paper study the sample complexity of learning high uniform accuracy neural networks. Uniform accuracy is not a common criterion that people analyze in statistical learning theory, but it can help to understand the stability of the network, which I believe is an important contribution to the deep learning community.\n\n\nWeakness:  While I\u2019m not familiar with uniform accuracy-related work, I feel some of the important lines of literature are missing. For example, I\u2019m wondering whether the author can first provide some detailed literature results regarding the difference between sample complexity of average accuracy vs uniform accuracy on simpler hypothesis classes, such as linear classifier, polynomial classifier, etc? Moreover, I believe how neural network approximate function is another line of related work related to this paper, which should be discussed and compared with the results.\n",
            "clarity,_quality,_novelty_and_reproducibility": "1. I suggest formally giving the definition of uniform accuracy instead of burying it in the text. \n2. A detailed comparison between the sample complexity of NN under uniform accuracy and under normal average accuracy proposed in the literature, is necessary.\n3. The proof sketch in the main paper can be clearer, i.e. first give the definition of bump functions. Some of the math notation in the proof sketch is not introduced.\n4. What are the x-axis and y-axis of figure 3, also the discussion is not very clear to me.\n5. Though the theorem suggests sample complexity exponentially depends on the dimension, I'm wondering whether the experiments can be more flexible. Only consider dimension=1 and 3 looks too simple and hardly gives any insight for real-world application.\n6. I'm not following the motivation that connects uniform accuracy with adversarial robustness, in the sense that this paper considers a teacher-student setting, so all the samples are guaranteed to generate from the same neural network, whereas for adversarial robustness, even considering the standard Lp perturbation attack, there's no guarantee that the adversarial examples and the clean samples all come from the same network.",
            "summary_of_the_review": "Overall, I believe this is an interesting paper with good contribution that considers the sample complexity of neural networks for uniform accuracy.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_81vF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_81vF"
        ]
    },
    {
        "id": "5v8AeFP4Ws",
        "original": null,
        "number": 5,
        "cdate": 1667392766052,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667392766052,
        "tmdate": 1667392766052,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a fundamental impossibility result associated with the neural network function class. It is proven that given a specific network architecture (e.g., depth and width) with ReLU activation function and norm-bounded parameters, there exists a neural network function u such that u cannot be determined *efficiently* from (possibly adaptive) input-output sample pairs, where the needed sample size to determine u can be lower bounded by a term that scales exponential in the input dimension d. This result is equivalent to the following one: given a specific network architecture with ReLU activation function and norm-bounded parameters, to learn a target function uniformly well with error $\\epsilon$ (where the error is measured to be the $L^\\infty$ distance between the learned function and the target), the sample complexity must scale like $\\Omega(\\epsilon^{-d})$.\n\nThe paper discussed a quite general setting with the error defined to be $L^p$ distance for any $p\\in [1,\\infty]$. The above exponential bound happens exactly when $p=\\infty$, which contrasts the common squared-error setting $p=2$, for which sample-efficient learning is possible.\n\nMoreover, the dependency of several problem-dependent parameters is made explicit in the bound, such as the network layer $L$, the hidden dimension $B$, and the scale of the parameters $c$ under $\\ell_q$-norm. The authors gave a fruitful discussion of their results and also presented an upper bound. They finally conducted experiments to verify the theoretical results.",
            "strength_and_weaknesses": "**Strength**:\n\nI feel the presented theoretical result is interesting. The result gives us a better understanding of the complicated nature of the neural network function class which is fundamentally different from classic function classes, such as linear models or polynomials. It shows that the representation of a neural network cannot be inferred from a tractable number of input-output samples even if the number of network parameters is small. I partially agree with the authors that the result may be useful for future works in understanding several important topics, e.g., why modern neural networks are vulnerable to adversarial examples. I also agree that learning functions to minimize the uniform error $L^\\infty$ instead of $L^2$ is *necessary* for some applications such as solving partial differential equations. So in this respect, I think the present theoretical result is significant.\n\nThis paper is well-written and clear. Although the theoretical bound involves many problem-dependent parameters in a complicated manner, it is well-explained using carefully presented remarks. Moreover, the simplified version (Theorem 1.1) gives readers an overall picture of the more advanced result, while being easy to understand. The authors also present a proof sketch that is helpful in gaining insights into why the result holds. The discussions in this paper are particularly comprehensive.\n\n**Weaknesses**:\n1. I think there is a mismatch between the setting in this paper and the commonly used settings in learning real-world function classes. In practice, it is unlikely that the target function is represented by a neural network similar to the counterexample in this paper. If I understand correctly, the reason why the sample complexity must scale like $\\Omega(\\epsilon^{-d})$ is that neural networks can represent ''bump functions'' which are highly local. Therefore, if there are no samples in the ``bump'' area, the target function clearly cannot be learned. However, such non-smoothness may hardly appear in practice. Therefore, the current theory may not shed light on practical settings. I think it is an important and natural assumption that the target function to be learned is ''smooth'' (rather than all functions that can be represented by neural networks).\\\n Although the authors said that ''this is satisfied in several applications of interest, e.g., model extraction attacks (Tram\u00e8r et al., 2016; He et al., 2022) and teacher-student settings (Mirzadeh et al., 2020; Xie et al., 2020)'', the target functions there are *well-trained* neural networks rather than any possible one in the neural network function class. Therefore, these cases may not be the same as the one studied in this paper.\n\n2. The authors also said that the result is related to the adversarial vulnerability of neural networks. While I partially agree with the potential implications of the result in this paper, I think the current setting is still quite different from the setting of adversarial examples. In particular, Theorem 1.1 may only show that neural networks can represent highly non-smooth functions with many spikes, for which adversarial examples exist. However, given finite samples, why are neural networks prone to learn bump/spike functions rather than smooth functions? It is because of the inductive bias or training algorithms? I think currently it is still not clear how the result in this paper relates to the adversarial vulnerability of neural networks.\n\n3. While I am generally satisfied with the case when $q=\\infty$ (i.e., the setting of bounded weight coefficients), I think the current bound for $q<2$ may be vacuous and unreasonable. In particular, in this case $\\Omega$ scales like $d^{-d}$, which means that when fixing $\\epsilon$, the bound $\\left(\\frac {\\Omega}{64d\\epsilon}\\right)^d$ vanishes as $d\\to \\infty$. Namely, the lower bound becomes trivial when $d$ is large. This is clearly impossible since the problem becomes harder when $d$ is larger. Therefore, the bound for the case of $q<2$ is very loose and may not really make sense. Note that there is a large gap between the lower bound and the upper bound for $q\\le 2$ (Theorem 2.4, which does not vanish when $d$ is large). In another way, the main theorem only applies when $\\epsilon=o(1/d)$. For the high dimensional setting which usually occurs in practice, the bound becomes vacuous.",
            "clarity,_quality,_novelty_and_reproducibility": "See the above section.\n\nBesides, I also have a series of questions:\n\n1. In Theorem 1.1, it is written ``$L$ layers with width up to $3d$''. Is the statement wrong (maybe the correct statement would be $L$ layers with width at least $3d$)? I think increasing the width should only make the network function more complicated.\n2. In the main theorem, when $q=2$, it seems that $\\Omega$ will have different values under the two different calculations in equation (3). Are the constants $\\frac 1 {4\\times 3^{2/q}}$ and $\\frac 1 {24}$ correct? What is the reason for this inconsistency?\n3. Theorem 2.2 requires the assumption that $L\\ge 3$. I wonder why it cannot be applied for two-layer networks. Are two-layer networks fundamentally different from networks with $L\\ge 3$ layers in proving this theorem? \n4. The authors said that Theorem 2.2 only applies for ReLU neural networks. I am quite interested in the assumption and would like to know more about it: what are the difficulties in generalizing the result to other activation functions? Do you think/conjecture that Theorem 2.2 can still apply in general settings?\n5. Regarding Figure 3: it is said that this paper focuses on ReLU networks. However, the blue curve in Figure 3 is smooth, which is strange to me. Can the authors provide an explanation?\n6. The authors use simple vector $\\ell_q$ norm to constrain the model's parameters. While it is certainly fine (not a weakness), I may wonder if it is possible to use bounded matrix norm per layer to constrain the model's parameters since it is finer than the coarse vector norm and takes into consideration the layered network structure.\n\nMiscellaneous minor issues:\nThe first line in page 8: what is equation (2.1) linked to?",
            "summary_of_the_review": "Overall, I think this paper is interesting. The result is well-presented, and the writing is clear. Nevertheless, there are several weaknesses raised above. I hope the authors can clarify them and I will reevaluate this paper based on authors' response.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_VFv9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_VFv9"
        ]
    },
    {
        "id": "BBmGP4rEvS-",
        "original": null,
        "number": 6,
        "cdate": 1667541632155,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667541632155,
        "tmdate": 1667541632155,
        "tddate": null,
        "forum": "nchvKfvNeX0",
        "replyto": "nchvKfvNeX0",
        "invitation": "ICLR.cc/2023/Conference/Paper4030/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper provides a theoretical study on the difficulty of learning an unknown ReLU network with a fixed number of layers by another ReLU network with the same number of layers. While previous attempts usually look at NP-hardness of the optimization process of this task, even allowing for known data distribution, the paper tackles the problem from a learnability/information theoretic point of view and studies bounds on sample complexity that any learning algorithm can achieve. The end result is a separation between L-infinity learnability and Lp learnability when p is finite. In particular, when requiring that the learned network is close in L-infinity norm, the sample complexity scales at least exponentially in depth and input dimension, even if one allows for arbitrary runtime in the learning algorithm.\n\n\n\n\n\n\n",
            "strength_and_weaknesses": "Strengths\n\n1. The results are quantitative where both the upper and lower bound are non-asymptotic. The proof is clean and constructive. \nThe paper is very clearly written. Special care was given to compare to existing work and to discuss specific results in order to avoid over-claiming.\n2. Assumptions made are natural and rather weak. The learning problem is also natural and relevant (teacher-student learning).\nRelevant empirical study to substantiate theoretical claims.\n\nWeaknesses\n\n1. The results are not very surprising as learning in L-infinity is known to be hard. \n2. Some discussions can be useful regarding the upper bound result (of Theorem 2.4). \n3. The results are only truly novel in the L-infinity setting, which is a restrictive setting (mainly for security, safety-critical applications), as learning is known to be tractable in L-p where p is finite. (This is acknowledged by the authors.) \n4. The constructed hard function for the lower bound proof makes use of sharp bumpy hat functions, which is not natural in real-world settings.\n5. I would really appreciate a more in depth comparison to SQ learnability literature since they share the common setting of studying sample complexity (as opposed to runtime complexity under fixed sample size). The authors, however, did a good job comparing their results in the PAC learnability setting.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Clarity:\nThe paper is written very clearly and is a treat to read. The authors went to great lengths to avoid misrepresentation of their results and perform comprehensive comparison with existing literature. \n2. Quality:\nI did not go through the proof in the appendix line-by-line due to lack of time but on quick inspection, and on studying the proof sketch provided in the main text, I believe the proof is correct and has high quality. The proof idea is also clear and organized. \n3. Novelty:\nIn the L-infinity setting, the paper claims to be the first to show quantitative lower-bound for learnability. \n4. Reproducibility:\nExperiments in the papers are described in detail with hyperparameters listed comprehensively in the appendix. I have not run the code myself due to lack of time but I believe that the results are reproducible, or otherwise can be quickly shown to be wrong. ",
            "summary_of_the_review": "Despite the limited setting under which the main results are proven (learning under L-infinity) and slightly unrealistic counterexample for the lower bound (which is rather common in the literature, in fact), the paper is very clearly written (even in the proof), gives concrete, quantitative, and comprehensive results under mild assumptions and is, in general, an interesting and well-conceived theoretical paper. Therefore, I recommend acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_8e6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4030/Reviewer_8e6q"
        ]
    }
]