[
    {
        "id": "E1TeMlJkfR",
        "original": null,
        "number": 1,
        "cdate": 1666606345619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666606345619,
        "tmdate": 1666606450256,
        "tddate": null,
        "forum": "VYaTFO2Myi5",
        "replyto": "VYaTFO2Myi5",
        "invitation": "ICLR.cc/2023/Conference/Paper1645/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper studies offline goal-conditioned reinforcement learning problems and proposes a novel offline goal-conditioned supervised learning method, Goal-Masked Transformers (GMT), which applies random goal-masking and predict goals as an auxiliary tasks. The learned policy could successfully switch the goals during a single episode (e.g. goal 0 --> goal 1) in both MuJoCo or Atari environments",
            "strength_and_weaknesses": "### Strength\n- The writing is easy to follow.\n- The learned transformer policy seems to smoothly switch the goal during a single episode, compared to multiple models or shot context (Figure 7).\n\n\n### Weaknesses\n- There are no baselines for comparison. Decision Transformer [1], Categorical Decision Transformer or Bi-directional Decision Transformer [2],  Prompt Decision Transformer [3], GoFAR [4], WGCSL [5], and LiPS [6] can be relevant methods to be compared (of course, not the all but more is better).\n- The goals in MuJoCo and goals in Atari are quite unclear. In MuJoCo, goals seem to stand for the discrete label that is clustered with k-means (based on x-velocity in the trajectory). In Atari, goals seem to stand for the returns in the trajectory. As far as I know, those abstract quantities are often not considered as a goal in the existing goal-based RL literature.\n- The goal switching is limited to once in the episode. Also, the number of goals is limited to 3 (0, 1, 2 in MuJoCo, 2, 5, 10 in Atari).\n\n\n[1] https://arxiv.org/abs/2106.01345\n\n[2] https://arxiv.org/abs/2111.10364\n\n[3] https://arxiv.org/abs/2206.13499\n\n[4] https://arxiv.org/abs/2206.03023\n\n[5] https://arxiv.org/abs/2202.04478\n\n[6] https://arxiv.org/abs/2012.03548\n",
            "clarity,_quality,_novelty_and_reproducibility": "### Quality\nThere are some points that should be improved in writing:\n- The reason why trajectory masking doesn't work well is not explained in Section 2.3 (recursively referenced in footnote 2).\n- How many random seeds did you use? I only found the description of  \"15 times\" rollout, for the evaluation.\n- Please use `\\citep` in the second paragraph of Section 4.\n\n### Clarity\nThere are some unclear points in the paper:\n- The definition of **goal** is missing, although \"goal\" variable of this paper seems different from typical goal-conditioned RL papers.\n- The is no architectural detail. I guess most parts are borrowed from Decision Transformer or Generalized Decision Transformer, but there are no descriptions or citations.\n- The definition of $\\mathcal{S_k},\\mathcal{A_k}, \\mathcal{G_k}$ in Section 3.1.1 is unclear.\n- The reason why step reward is an evaluation metric is unclear, while x-velocity in the trajectory seems to be related to goal variable.\n\n\n### Originality\nSince the architecture seems to be on top of Decision Transformer, and the problem formulation is similar to Generalized Decision Transformer, the originality of this paper can be limited (random goal-masking and auxiliary goal prediction loss).",
            "summary_of_the_review": "This paper lacks the definition of goals, proper comparison against relevant baselines, and novelty compared to Decision Transformer or Generalized Decision Transformer. Considering those aspects, I vote for rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_33J4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_33J4"
        ]
    },
    {
        "id": "OofJ1XylI_H",
        "original": null,
        "number": 2,
        "cdate": 1666673704134,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666673704134,
        "tmdate": 1666673704134,
        "tddate": null,
        "forum": "VYaTFO2Myi5",
        "replyto": "VYaTFO2Myi5",
        "invitation": "ICLR.cc/2023/Conference/Paper1645/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The goal of this paper is to try to learn a goal-conditioned policy which is robust to goal-switching, i.e. when the goal given to the policy changes in the middle of the episode. The proposed approach uses a causal transformer, and proposes masking the goal randomly to ensure that the model (GMT) is able to predict the goal from action and state information. The paper presents results on MuJoCo and Atari environments. ",
            "strength_and_weaknesses": "Strengths: \n\n- To my knowledge, the idea of goal masking in GCSL-style methods is novel\n- The method has interesting promise and good applications \n\nWeaknesses\n\nIn my opinion, a major weakness is the lack of detail around why certain design decisions are made about this approach. For example, I think more discussion is needed as to why goal switching is a difficult problem with the current framework. Wouldn't since we learn a policy pi(a_t | g_t, a_t) via hindsight relabelling, shouldn't the policy automatically react well to changing g_t. Another detail missing is how exactly the training data is selected, how the goals are chosen. Couldn't we think of goal-switching as two runs of shorter length? \n\nSecondly, I think more evaluations are needed. There are no comparisons to baseline approaches (Decision Transformer, standard GCSL, etc). It is important to show how these methods perform when the goal is switched in the middle of the episode. The metric used for the Mujoco tasks is the episode reward, but for goal-conditioned frameworks, the success rate is what is important. Can the policy reach the goal? It is unclear why clustering is needed, adds a strong assumption that we need to know the full state space, which will not be true when scaling to more difficult settings, thus I think this approach should be tried on random (not clustered goals). Ablations such as no-masking, are also missing. \n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "As mentioned in the strengths and weaknesses section, this paper is novel but there are a lot of details missing about 1) why the approach is important (why goal-switching is difficult), 2) how the approach works, and 3) lack of baselines and evaluations on the right metrics. \n\nIt seems like the approach is reproducible. ",
            "summary_of_the_review": "Overall, this paper presents an interesting direction and a novel approach that provides an initial result towards this direction, but design choices and explanations of the approach are unclear. Additionally, this approach needs a more thorough set of evaluations (see strengths and weaknesses sections). ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_HqN1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_HqN1"
        ]
    },
    {
        "id": "qKEZtYHADN",
        "original": null,
        "number": 3,
        "cdate": 1666838278555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666838278555,
        "tmdate": 1666838278555,
        "tddate": null,
        "forum": "VYaTFO2Myi5",
        "replyto": "VYaTFO2Myi5",
        "invitation": "ICLR.cc/2023/Conference/Paper1645/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on the problem of goal conditioned reinforcement learning, where the authors propose a method to train goal conditioned policies via supervised learning using Transformers. Specifically, the authors treat the transitions of state, action and goal in a trajectory as a sequence, and train a Transformer network with causal attention to predict the future action and goals in an autoregressive way. In addition, in order to make the model pay more attention to goals, the authors also introduce the strategy of goal masking during training, where the ground truth goals are replaced with a mask token with certain probability.\n\nThe authors evaluate the proposed method in Gym locomotion and Atari tasks, and the results show that the proposed method is able to perform the correct task condition on given goals.\n",
            "strength_and_weaknesses": "I think this paper presents an interesting idea of training goal conditioned policies. However, I do have some important concerns about the proposed method.\n\n\nI\u2019m not convinced about the novelty of the proposed method. The proposed method is essential decision transformers [1] applied to goal conditioned reinforcement learning, where the return-to-go is replaced by the goal in the transitions, where the goal can be considered a special case of rewards. Hence the only novelty for the proposed method is the application of masking in goals, which I do not believe is enough contribution.\n\nMy major concern about this paper is that the authors have not included any comparison of the proposed method to previously proposed baselines. Goal or return conditioned RL is a relatively mature subfield in RL with many prior works. Just to name a few, I believe the specific tasks solved in the paper can also be solved by prior methods such as Decision Transformers [1], Trajectory Transformers [2], HER [3], GCSL [4], RCP [5] and RvS [6]. Therefore, it is impossible to verify the empirical performance of the proposed method without comparing to some of these baselines.\n\n\n### References\n[1] Chen, Lili, et al. \"Decision transformer: Reinforcement learning via sequence modeling.\" Advances in neural information processing systems 34 (2021): 15084-15097.\n\n[2] Janner, Michael, Qiyang Li, and Sergey Levine. \"Reinforcement learning as one big sequence modeling problem.\" ICML 2021 Workshop on Unsupervised Reinforcement Learning. 2021.\n\n[3] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Advances in neural information processing systems 30 (2017).\n\n[4] Ghosh, Dibya, et al. \"Learning to reach goals via iterated supervised learning.\" arXiv preprint arXiv:1912.06088 (2019).\n\n[5] Kumar, Aviral, Xue Bin Peng, and Sergey Levine. \"Reward-conditioned policies.\" arXiv preprint arXiv:1912.13465 (2019).\n\n[6] Emmons, Scott, et al. \"RvS: What is Essential for Offline RL via Supervised Learning?.\" arXiv preprint arXiv:2112.10751 (2021).\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well written. The description of the goal switching problem can be improved, since the problem of goal switching is essentially the problem of achieving multiple goals in the term of goal conditioned RL.\n",
            "summary_of_the_review": "Given the concerns about novelty and empirical evaluations, I cannot recommend acceptance of the paper in its current state.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_S5ck"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1645/Reviewer_S5ck"
        ]
    }
]