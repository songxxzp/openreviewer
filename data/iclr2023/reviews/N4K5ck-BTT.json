[
    {
        "id": "8v1ISjCRkt",
        "original": null,
        "number": 1,
        "cdate": 1666663895554,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666663895554,
        "tmdate": 1666663895554,
        "tddate": null,
        "forum": "N4K5ck-BTT",
        "replyto": "N4K5ck-BTT",
        "invitation": "ICLR.cc/2023/Conference/Paper4072/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a method for improving knowledge distillation from a high capacity teacher model to a low capacity student model. In addition to training the student to match the teacher's outputs with cross-entropy, the paper proposes learning a 'guide' (or essentially a gating) function from the teacher's output that reduces the weight placed on difficult examples when training the student. Experiments reveal that this smooths the optimization landscape for the student, enabling it to find the best minimum even when it does not have the capacity to represent the ground truth function. In addition to results on toy environments, the experiments examine CIFAR-100 datasets with standard architectures, and show an improvement over vanilla KD and other KD techniques. ",
            "strength_and_weaknesses": "A strength of the paper is in the quality of the experiments. Starting with the toy experiments, the authors clearly demonstrate how their approach is valuable in helping a low capacity student find the best minimum point in the loss optimization landscape; Figure 1 convincingly shows how the gated teacher loss function reduces local minima. Beyond the toy experiments, the paper tests on real datasets with standard large resnet models, and shows consistent improvements over prior work. \n\nA weakness of the paper is its organization. The intro extends for almost 4 pages, and includes detailed experiments and proofs that are wholly inappropriate for the intro. The method has not been fully explained, and yet the paper is diving into experiments and theoretical arguments to show that it works. ",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity:**\n- As mentioned above, the paper should be restructured so that the intro gives the high level idea, then the method is explained, then the theoretical arguments and toy experiments are presented. Even having the \"if g(x) $\\approx$ 1, teacher discounts the input\" is a level of technical detail not appropriate for the intro. \n- Egregious abuse of negative vspace made it hard for me to parse page 2... I couldn't find the continuation of the previous page because it appeared to be part of the figure caption. \n- Equation 2 is nicely presented and clearly broken down into its components. \n- p. 5 \"contraints the helper g\" -> constrains\n- p. 6 \"the indices guide function helps are picked as\" - ??\n- Table 4 is awkwardly located in the related work section, after the results are already written up. \n- p. 7 \"where student is very less-capacited compared to teacher model\" -> both spelling and grammar errors. Where the student has much less capacity\n- incomplete sentence on p. 7 under the bolded heading \"Experiment results.\"\n- p. 8 \"as to remedy to\" \n\n**Quality:**\n- As mentioned above, both the toy experiments and the scaled up experiments are compelling evidence for the usefulness of the method.\n- The results could be presented more carefully so as not to be misleading. Why does Table 3 compute gain as the gain over CE rather than KD, since KD is the more relevant baseline? More importantly, the convention is to bold the highest scoring method. In Table 4, SimKD scores highest for Resnet8x4 (rows 1 and 5), so it should be bolded. \n- The relevance of the feature matching baselines in table 4 is not well explained. \n- The point about reaching near teacher accuracy with a model with many fewer parameters is a good one, and could be emphasized more. This could be impactful for a number of applications. \n\n**Originality:**\n- To the best of my knowledge, this work is original in proposing that the teacher not penalize the student for learning examples that are too difficult in order to preserve model capacity in the student. ",
            "summary_of_the_review": "Overall, the paper is poorly organized but the results are compelling. I think it will be of interest to the community. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_DdZ6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_DdZ6"
        ]
    },
    {
        "id": "T1nLRxeIMd",
        "original": null,
        "number": 2,
        "cdate": 1666665443160,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665443160,
        "tmdate": 1666669809031,
        "tddate": null,
        "forum": "N4K5ck-BTT",
        "replyto": "N4K5ck-BTT",
        "invitation": "ICLR.cc/2023/Conference/Paper4072/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces a novel knowledge distillation (KD) method aims at addressing the capacity mismatch between teacher and student models. The authors propose to additionally learn a weight function to mask hard to learn examples, and thus increase the performance of student models. ",
            "strength_and_weaknesses": "Strength\n- The paper is well motivated. I especially like the introduction section (despite being long) which clearly conveys the important ideas with two illustrating examples.\n- The methodology is well formulated. \n- Experiments are comprehensive with detailed discussions. \n\nWeaknesses\n- The selection of budget is an important factor in the whole algorithm, but there seems not to be discussions on how users should choose in practice. \n- The stability and efficiency of the proposed algorithms are not discussed. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written with clarity. And the idea proposed here is novel. \n\nMinor edits:\n- two \"bound\" in page 2 under section \"Good Generalization\". ",
            "summary_of_the_review": "This paper provides a novel approach for knowledge distillation by distinguish hard to learn samples for students, and proves to provide a student model with smoother landscape, fewer local minima, and thus better generalization errors. I think the idea proposed is novel, and the paper is well motivated with very intuitive examples to present the ideas. It is a good paper for ICLR. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_XDpR"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_XDpR"
        ]
    },
    {
        "id": "V_I-kWWW_bg",
        "original": null,
        "number": 3,
        "cdate": 1666740423765,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666740423765,
        "tmdate": 1666740423765,
        "tddate": null,
        "forum": "N4K5ck-BTT",
        "replyto": "N4K5ck-BTT",
        "invitation": "ICLR.cc/2023/Conference/Paper4072/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose an approach to distillation where the teacher provides both a predictive target and scaffolds the student's prediction by censoring hard-to-learn examples. In the case where the student has far fewer parameters than the teacher this scaffolding leads to a smoother loss landscape for the student and thus the student encounters fewer local minima. The authors call their approach DiSK (distilling selective knowledge). ",
            "strength_and_weaknesses": "The greatest strength of the work is the considerable gain in accuracy compared to cross-entropy or kullback-leibler. It is very impressive for such resource constrained models. Figure 1 and the example provided is excellent to provide an overview of what can be achieved in regards to simplifying the loss landscape. \n\nOne of the weakness in the paper is that it is unclear what the guide function model is. In one part of the paper it is described as \"obtained by using an MLP on softmax outputs of the teacher\" in another place it is described as taking the teacher's last layer features and the prediction probabilities as input.\n\nAnother weakness is that decisions are presented without an explanation. Why relax the guide function to be continuous? Why is the budge constraint necessary?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and provides a novel approach to performing distillation when the student model is significantly less resourced than the teacher model.",
            "summary_of_the_review": "The authors provide a novel approach to performing distillation in which scaffolding allows hard to learn examples to not impact the student. The authors note that an implementation would be provided in the final version, and as a reader I desperately wanted it. I believe it would clarify some questions i have in regards to the guide functions implementation.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_sPvE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4072/Reviewer_sPvE"
        ]
    }
]