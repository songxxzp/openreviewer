[
    {
        "id": "vbJpw9QZTR5",
        "original": null,
        "number": 1,
        "cdate": 1666473828689,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666473828689,
        "tmdate": 1666473828689,
        "tddate": null,
        "forum": "k60XE_b0Ix6",
        "replyto": "k60XE_b0Ix6",
        "invitation": "ICLR.cc/2023/Conference/Paper952/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper introduces Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encoding, by combining continuous label encodings space with regularizers. The proposed method is extensively demonstrated on 11 benchmarks. ",
            "strength_and_weaknesses": "strength:\n\n(1) The authors tackle the problem of deep regression through the lens of binary classification. It is also interesting and technically sound to extend the conventional hand-crafted encoding method to be differentiable. Overall, I like this simple yet useful idea. \n\n(2) The experiments are extensive, and the proposed end-to-end encoding method has competitive performance compared to hand-crafted. The ablation study also demonstrates the effectiveness of two proposed regularizers.\n\nWeakness: \n\n(1) In the experiments, manually designed BEL outperforms RLEL in many cases. I'm curious why an end-to-end training method seemingly achieves a suboptimal solution, compared to a hand-crafted method. Can the authors provide some intuition on it? Also, how different is it between the learned encodings and (better performed) hand-crafted encodings? \n\n(2) I encourage the authors to discuss the efficiency of the method. What is the time cost to incorporate the label encoding learning with regularizers, compared to other hand-crafted encodings? ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe manuscript is well-presented and the idea is simple and easy to follow.\n\nQuality:\n\nThe method is extensively demonstrated on 11 benchmarks with significant improvement upon baselines. The ablation study also demonstrates the effectiveness of the proposed regularizers.\n\nReproducibility:\n\nThe authors have their code attached, and I think the implementation details provided in the appendix are sufficient.  ",
            "summary_of_the_review": "The authors propose a simple yet effective end-to-end training to simultaneously learn the label encoding and features. The experiments are solid and extensively demonstrate the effectiveness of the method. Therefore, I recommend borderline acceptance. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper952/Reviewer_XTEa"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper952/Reviewer_XTEa"
        ]
    },
    {
        "id": "wCXM97Jo2y",
        "original": null,
        "number": 2,
        "cdate": 1666518446818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666518446818,
        "tmdate": 1669100290633,
        "tddate": null,
        "forum": "k60XE_b0Ix6",
        "replyto": "k60XE_b0Ix6",
        "invitation": "ICLR.cc/2023/Conference/Paper952/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper focuses on deep regression by using label encoding. Specifically, this paper proposes Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encodings. RLEL contains two regularization terms, designed to encourage encodings with certain properties. The first one encourages the rule that the L1 distance between label encodings should increase as the difference between two labels increases. The second one encourages the rule that the L1 distance between label encodings for adjacent target label values should be minimized. The effectiveness of the proposed method is demonstrated by a lot of experiments.",
            "strength_and_weaknesses": "Strengths:\n- This paper proposes a novel label encoding method to solve regression and seems can alleviate the disadvantage of the previous label encoding method, the space of label encodings for regression is large.\n- This paper introduces two simple yet meaningful rules for label encoding and designs corresponding regularization terms to encourage label encodings to satisfy the rules.\n- Comprehensive experimental results on various datasets clearly demonstrate the effectiveness of the proposed method.\n\nWeaknesses:\n- According to Eq. (5), the generation of soft target labels is affected by the batch size K. It seems impossible. So, I guess it should be the number of target labels N rather than K in the generation of soft target labels. \n- The authors should use unified representation. For example, the correlation vector is denoted by $\\hat{C}_i$ in most cases. However, $C_i$ is used in Eq. (5) and Fig. 3.\n- The authors assume $y_i \\in [1, N]$ for simplicity as the real-valued targets with any arbitrary numeric range can be scaled and shifted to this range. However, they do not provide details and discussions of N. Obviously, different N would cause different performance.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is written well and organized well. Although some details and discussions of the number of soft target labels N are missing, this paper has done well in other aspects.",
            "summary_of_the_review": "Overall, this paper proposes a novel label encoding method that is reasonable to solve the deep regression problem. There are also some shortcomings in this paper as mentioned above. The authors should carefully check the manuscript repeatedly and provide necessary details and discussions to further polish this paper.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper952/Reviewer_TMxF"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper952/Reviewer_TMxF"
        ]
    },
    {
        "id": "1wVnI721VaU",
        "original": null,
        "number": 3,
        "cdate": 1666525384555,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666525384555,
        "tmdate": 1669094766831,
        "tddate": null,
        "forum": "k60XE_b0Ix6",
        "replyto": "k60XE_b0Ix6",
        "invitation": "ICLR.cc/2023/Conference/Paper952/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an end-to-end automated approach to find label encodings for deep regression problem. In my understanding, these tasks are usually using images as input and are outputting a continuous value. Due to the nature of deep regression, a general approach to regress directly does not show good results compared to task-specific approaches. Their work is largely based on a prior work [Shah 2022], in which 1) binary-encoded labels were introduced to encode quantized regression values 2) three desirable properties of suitable encoding/decoding functions were identified. This paper extends [Shah 2022]'s work by not using manually identified binary codes, but learn the continuous label encodings from the data directly. Their empirical experiments show that their learned-encodings and the new model result in lower or comparable errors to [Shah 2022] work.\n\n[Shah 2022] Deval Shah, Zi Yu Xue & Tor M. Aamodt. LABEL ENCODING For REGRESSION NETWORKS. ICLR 2022.\n\n",
            "strength_and_weaknesses": "Strength: clarity and quality of writing. Extensive experiments.\n\nThe paper is well written with a pretty clear introduction of background and related work. I also like how the author motivates the underlying work by showing the challenges in the design space. By giving a literature review on the binary encoding's usage in regressor, I was able to understand why the properties of suitable encodings (identified by prior works) are important in such a work. By following this guidance to the properties, the author started to introduce the regularization functions to encourage such properties when using real-valued label encodings. The loss function is then introduced based on the regularizer design. The experiments use almost the same datasets (from Shah 2022) so that a fair comparison with its predecessor can be easily conducted. The author also compares with a few other label encoding design approaches and regression approach to show that their work is a generic regression approach and generalizes better with even a small dataset.\n\nWeakness: not strong originality.\n\nMost of the originality in this paper is to propose a learned continuous label encoding (compared to existing work on using manually designed binary encoding). From the regularizer design, the author identifies effective regularization functions to encourage the expected property within a continuous encoder. The proposed method seems effective but incremental in nature. \n\nQuestions to the author:\n1. How would you decide the number of quantization buckets ($M$ in the continuous predicted encoding $Z_i$)? Is there an optimal dimension size for various dataset?\n2. Computationally (during training or inference), is there any significant difference between RLEL and BEL?\n3. In equation (3), can you give more explanation that why you choose the magic number 2?\n4. It seems to me (by looking at Table2) BEL is reasonably performing well though it is manually designed. I kindly expect that adaptively learned encoder shall perform even better if more data is given during training. I am not familiar with the scale of the experimental dataset you have used. How many examples are there in each of them? Were you able to test your method in a significantly large-scaled problem (e.g., with many more training examples) to show the superiority of your model)?\n5. Does RLEL use uniform quantization (similar to BEL)? Is there something that can be done to adaptively select quantization level and non-uniform quantization?  ",
            "clarity,_quality,_novelty_and_reproducibility": "The quality of writing is excellent. The author provides extensive experiment setting and code in the appendix and supplemental materials. I believe it will be sufficient to reproduce the work. \n\nMinor comment:\nIn figure 1(a), the legend (the blue dot vs red dot is too small) is almost unrecognizable.\n",
            "summary_of_the_review": "Weak accept. The paper proposes an effective way to automatically learn continuous encoding to help improve deep regression model performance. If the author could give more theoretical analysis on why the proposed model encourages the expected encoder properties, I would be happy to increase my scores. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "n/a",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper952/Reviewer_ZxJ5"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper952/Reviewer_ZxJ5"
        ]
    },
    {
        "id": "rDCXvCnloLh",
        "original": null,
        "number": 4,
        "cdate": 1666687317848,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666687317848,
        "tmdate": 1669098212600,
        "tddate": null,
        "forum": "k60XE_b0Ix6",
        "replyto": "k60XE_b0Ix6",
        "invitation": "ICLR.cc/2023/Conference/Paper952/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the deep regression problem and proposes a new automated label encoding learning framework along with two regularizers. In specific, the authors relax the assumption of binarized label encodings of the BEL method and propose to search for continuous label embeddings. Moreover, two regularizers are introduced. The first restricts the learned label encoding to preserve the distance of the bits. The second is to enrich the bit transitions. Experiments show the proposed method obtains competitive results to manually designed label encodings.",
            "strength_and_weaknesses": "Pros:\n\n1. The handcrafted label encoding, though effective, increases the difficulty of the practical deployment of deep regression models. This work proposes an automated label encoding learning method that is much more practical. \n\n2. The experimental results are promising as the proposed method achieves competitive or better performance than manually designed label encodings. \n\n3. The paper is well-organized and mostly written clearly. \n\nCons:\n\n1. While the presentation is generally good, the methodology is hard to follow. Some notations are not clearly defined. For example, the authors used B, Z, E to denote the label encodings. What are their differences? Why E is the average of E? What is \\mathcal{E}(Q_i) on page 5?\n\n2. The 2.1 section (and Figure 2), while important and informative, isn't really clear to me. Why is the bit transition from 1->0 and 0->1 important for label encoding? Also, how does the bit position measures the binary classifier's decision boundary? Personally, I recommend the authors polish this section for a clear explanation of the important properties of label encodings.\n\n3. I noticed that the softmax operator and the cross-entropy loss are used. It suggests the regression network learns a multi-class classifier. While this paper frequently mentioned the binary classifier, I would like to see the difference between these two designs. In particular, it seems a multi-class classifier may reduce the complexity of the encoding space.\n\n4. The authors propose to regularize the matrix D instead of the original label encodings in Eq. (4). While empirically effective, there are no theoretical insights on why it works. \n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall well-written and the proposed method is original. But some notations and analyses are confusing. Source code submitted.",
            "summary_of_the_review": "See above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper952/Reviewer_vuKX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper952/Reviewer_vuKX"
        ]
    }
]