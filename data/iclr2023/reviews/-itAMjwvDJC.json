[
    {
        "id": "sALDlby5tC6",
        "original": null,
        "number": 1,
        "cdate": 1666831022758,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666831022758,
        "tmdate": 1668730719305,
        "tddate": null,
        "forum": "-itAMjwvDJC",
        "replyto": "-itAMjwvDJC",
        "invitation": "ICLR.cc/2023/Conference/Paper4542/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "I thank the authors for their thoughtful replies as well as the clarifications that they have made in the paper. I would increase my score to 7 if that were an option. In any case, I think that this is a valuable contribution and look forward to seeing the paper published =)\n\nThis paper extends prior work on measuring manifold capacity to the setting of sparse one-vs-all classification. This is well matched to biological data and provides a more optimistic bound on the number of neurons needed to compute the proposed metrics. They show that across artificial and biological processing stages manifold dimension decreases while ambient space size might increase \u2013 both factors that contribute to superior classification performance.",
            "strength_and_weaknesses": "Strength: The paper is clearly written and presents and interesting extension to prior work that is well matched to realistic settings. The experiments are straightforward and support the claims.\n\nWeaknesses: It would be great if the methods would rely less on prior work. Basically, it is impossible to properly understood the current paper fully without having read the prior work by Chung et al. This is unfortunate and might make the current work seem more incremental.",
            "clarity,_quality,_novelty_and_reproducibility": "Nits:\n\nThe abstract could be a bit more high level, without references or footnotes and more accessible for a wider audience.\n\nThroughout the paper references seem split, are they not in the same \\cite{} call in latex?\n\nFootnote 1: point missing.\n\nIntroduction, 3 paragraph, last sentence: this is unclear, please explain/rephrase\n\nDefine VC dimension when first using the term (also Cover)\n\nIntro, last paragraph before list: Does that mean manifold capacity\n\nSection 2: Please define \\tilde{s} this is important!\n\nSec 3: \u2018We extend this work by\u2019\u2026 -\u2018of\u2019 correlation+\u2019s\u2019\n\n3.1. First sentence manifold+\u2019s\u2019\n\n\u2018Furthermore, this bias\u2019 \u2026. Why? Please clarify\n\n3.1.2: what does \u2018null space of low correlations\u2019 mean? Clarify\n\nFig 2. How do you project back?\n\n4.1. What is theoretical vs simulated capacity and why is one of them ground truth??\n\nFig 3c: please change colors, these are too similar\n\nFig 4: really cool!\n\n4.5 first paragraph: this is a nice intuition!\n\nFig 5 column iv: this seems add odds between ANN and biological data. Is that significant, and if yes, can you speculate why?\n\nFig 6a: Great plot, but please improve the clarity of the caption\n\n",
            "summary_of_the_review": "Overall, I think that this is a cool extension for measuring manifold capacity in sparse settings. The results across processing hierarchies also make sense in artificial and biological systems (maybe a newer architecture like ResNet would have been more interesting than VGG). It would be great if the paper were more self-contained, currently it seems somewhat derivative/incremental on prior work. That being said, I think improving previous bounds to realistic sparse settings is really cool and valuable.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_4A5K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_4A5K"
        ]
    },
    {
        "id": "vuXrz3dhci",
        "original": null,
        "number": 2,
        "cdate": 1666867676244,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666867676244,
        "tmdate": 1666868759144,
        "tddate": null,
        "forum": "-itAMjwvDJC",
        "replyto": "-itAMjwvDJC",
        "invitation": "ICLR.cc/2023/Conference/Paper4542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors of the paper propose Sparse Replica Manifold analysis for estimating sparse manifold capacity that measures how many object manifolds can be separated under sparse classification tasks, which is pervasively used in both neuroscience (real neural data) and deep learning (artificial neural data) domains. Building upon Mean Field Theory Manifold Analysis (MFTMA), the authors derive and prove Theorem 1, which theoretically computes sparse manifold capacity under the assumption of different Gaussian Radii and Dimensions for each manifold. To account for the correlation between manifolds, authors leveraged CLLD to compute a space of low correlation. The authors then perform comprehensive experiments on both real and artificial neural data to demonstrate that their theoretical and empirical analyses are consistent. Overall, the paper is presented very well and has convincing results.",
            "strength_and_weaknesses": "Strengths\n- The paper is overall very complete in itself, especially the inclusion of MFTMA in the main manuscript is very helpful.\n- Thorough empirical study is performed.\n- Nice to see that sparse analyses are significantly faster compared o dense analyses.\n- Overall clarity and organization of the paper are great.\n\n\nWeaknesses\n- 2.0  (t^tilda, t_0) vectors are lacking variable definition. Also, why are these sampled from a Gaussian distribution?\n- Appendix section A.1 DERIVATION TO THEOREM 1 may be included in the main manuscript to improve completeness. I regard this derivation as one of the main technical novelties of this paper.\n- I understand that Eqs 4 and 5 are built upon the assumption that the manifolds are distributed Gaussian with a given radius and Dimension. What happens when they are not? Does this analysis break down?\n- 3.1.2 \"null space of low correlations\" what does that mean? Please give some background.\n- 4.1, \"Observe this means that a manifold needs to have enough feature dimension for the bisection search to operate appropriately.\" It is not only a hard sentence to understand, but why this should be the case for a 0.5 probability that a one-vs-rest dichotomy is separable is unclear. For completeness, the authors could elaborate on this.\n- Figure 3 indicates that manifold capacity is underestimated on artificial data, whereas it is almost exactly estimated in real neural data. A discussion on why this might be the case could be interesting.\n- It would be interesting to see the analysis on some other (possibly larger) datasets, such as ImageNet. Also, analysis with a more recent and widely used deep architecture would be more relevant, such as Residual CNNs and/or transformers.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well-written and organized. Empirical discoveries in this paper are highly novel and would be beneficial to both computational neuroscience and machine learning community. Although implementation details of the proposed approach are described in the Appendix, authors could anonymously release the code for increased reproducibility.",
            "summary_of_the_review": "This study on neural representations is very intriguing for both discriminatory analyses in statistical learning and to better study computational principles in neuroscience. Therefore, studies like this are (and will be) of interest to venues like ICLR. I suggest acceptance of this work. I believe that if the authors address/comment on the concerns mentioned in the Weaknesses section, the overall strength of the paper will be increased.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_TxMd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_TxMd"
        ]
    },
    {
        "id": "MJkAGe9kSt",
        "original": null,
        "number": 3,
        "cdate": 1667167357200,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667167357200,
        "tmdate": 1667167357200,
        "tddate": null,
        "forum": "-itAMjwvDJC",
        "replyto": "-itAMjwvDJC",
        "invitation": "ICLR.cc/2023/Conference/Paper4542/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper extends the sparse replica manifold analysis approach in Chung 2018 to incorporate across-manifold correlations and account for non-homogeneous manifolds. They demonstrate the application of sparse manifold capacity allows analysis of a wider class of neural data and show that their theory and algorithms provide a better match to real data.\n\n",
            "strength_and_weaknesses": "\nStrengths: \n\n- Moving to case where different manifolds can have different radii and varying correlations is better match to real data.\n\n- Simple algorithms and some theoretical justifications.\n\n- Analysis on two neural datasets show that their approach can better predict properties of neural responses than the homogeneous analysis.\n\nWeaknesses: \n\n- The main contribution of the work is extending previous theory to the heterogeneous manifold case, and providing a new algorithm for computing inter-manifold correlations. Thus, it would be useful to have some ablations to understand how adding different assumptions or constraints impacts the theory and match to practice.\n\nIn particular, \n(i) How much does the relaxation of the homogeneity constraint on the manifolds impact the match to data vs. the sparsity assumption? \n(ii) How does ovr match the assumption about how the brain is performing object recognition? It wasn\u2019t entirely clear how the ovr is important -- or if this analysis tells us something about how neural circuits are trying to classify different objects.\n(iii) How much is the low rank projection step in CLLD impacting the match to data? How do you select the dimension to project to?\n\n- Further synthetic experiments involving homogeneity and heterogeneity would be useful to understand the theory\n\n- The extension of the theory is relatively incremental and hasn\u2019t been dissected in a rigorous manner.\n\n- Instead of measuring across manifold correlations using manifold centers, they propose to use categorical local linear differences (CLLD). Are these metrics sensitive to the size of the dataset and how many neurons are in each? \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is relatively well motivated and mostly clear; however, the results appear somewhat incremental and the improvements in their analysis have not been thoroughly dissected.",
            "summary_of_the_review": "This paper provides new tools for analyzing the geometry of representations in artificial and biological neural networks. They extend theory from Chung 2018 to a more realistic setting where manifolds can have different underlying geometry and propose an algorithm for computing inter-manifold correlations. \n\nWhile their analysis and theory does indeed provide better matches to real data, the theoretical results appear to be relatively incremental and haven't been adequately dissected in the empirical studies. Is the better match due to using heterogeneous manifold estimates, or is the proposed estimation method key? The role of the dimension of the projection, the dimensionality of the manifold versus the number of neurons or ambient dimension hasn't been clearly dissected in the experiments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_EQzE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_EQzE"
        ]
    },
    {
        "id": "U9QzXeiYytT",
        "original": null,
        "number": 4,
        "cdate": 1667430070331,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667430070331,
        "tmdate": 1667430070331,
        "tddate": null,
        "forum": "-itAMjwvDJC",
        "replyto": "-itAMjwvDJC",
        "invitation": "ICLR.cc/2023/Conference/Paper4542/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The submission presents an extension to the representational analysis framework of Chung et al. (2018) in order to apply the framework to more standard computer vision tasks such such as multiway classification, with the goal of more precise comparisons between humans and neural networks.",
            "strength_and_weaknesses": "##### Strengths:\n\n1. The idea seems valid, although I do not have the domain knowledge to place its significance.\n\n##### Weaknesses:\n\n1. The explanation of and motivation for the method generally assume a lot of background; for example, what is meant by \"separability\", \"system capacity\", \"manifold\" and \"manifold capacity\", \"random dichomoties\". As such, it was not easy to understand all details of the method.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**. The paper was not an easy read, as it assumes a lot of prior knowledge in this subdomain.\n\n**Novelty**. The extension appears novel.\n\n** Reproducibility**.No explicit attempts at reproducibility are made in the paper, but a promise to release the method is made.",
            "summary_of_the_review": "The submission needs improvement in clarity to better convey its contributions to the neuroscience and computer vision communities.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_yhnc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_yhnc"
        ]
    },
    {
        "id": "TR8wziDPC7f",
        "original": null,
        "number": 5,
        "cdate": 1667477813124,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667477813124,
        "tmdate": 1667477813124,
        "tddate": null,
        "forum": "-itAMjwvDJC",
        "replyto": "-itAMjwvDJC",
        "invitation": "ICLR.cc/2023/Conference/Paper4542/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends Chung et al. (2018), which proposes Sparse Replica Manifold analysis to estimate manifold capacity. The authors show that the application of sparse manifold capacity requires a smaller number of features and is faster compared to dense labeling. The authors also illustrate the effects of ambient dimension and manifold intrinsic dimension on sparse separability.",
            "strength_and_weaknesses": "Strengths:\n\n1.The proposed Sparse Replica Manifold analysis overcomes the limitation of size of neuroscience dataset and is close to real tasks in cognitive science.\n\n2.The authors analyze the manifold capacity of both deep neural network manifolds and neural manifolds, making the conclusions more convincing.\n\nWeaknesses:\n\n1.The authors follow the work of Chung et al. (2018), but they don\u2019t clearly claim the contributions of previous work.\n\n2.The datasets this paper uses are relatively old and limited.\n\n3.More comprehensive and thorough experiments are necessary to prove the conclusions.\n\nQuestions for the Authors\n\n1.In chapter 3, the authors claims that sparse manifold capacity has been previously considered in Chung et al. (2018) and that they extend this work k by taking into account of heterogeneous geometries and correlation between manifolds. But in \u201cRESULTS\u201d, they don\u2019t clearly claim the advances of Sparse Replica Manifold compared with previous works and the contributions of taking different Gaussian Radii and Dimensions into account.\n\n2.The dataset this paper uses are from Majaj et al. (2015), Freeman et al. (2013) and VGG-16 trained by a CIFAR-100. More experiments on latest and large-scale datasets are needed.\n\n3.In Figure 5, the change ranges of Dg in both neural dataset and the tendency of Rg and Deff is not so evident. More experiments are necessary to prove the conclusion.\n\n4.The broader application of manifold analysis is meaningful; however, the scale of neuroscience dataset is larger recently. The total sequential computation time of Spares Replica Manifold Analysis has also no advantage. The influence of this method needs to prove again.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of this paper is limited and some description is not clear.",
            "summary_of_the_review": "This paper is the following work of Chung et al. (2018) and many conclusions have been proposed before.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_pKPQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4542/Reviewer_pKPQ"
        ]
    }
]