[
    {
        "id": "NvtGzUS9R8t",
        "original": null,
        "number": 1,
        "cdate": 1666584909494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666584909494,
        "tmdate": 1669612944635,
        "tddate": null,
        "forum": "ugA1HX69sf",
        "replyto": "ugA1HX69sf",
        "invitation": "ICLR.cc/2023/Conference/Paper3670/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a solution to \u201cembodied reference understanding\u201d, which aims to locate so-called \u201creferents\u201d which are objects referred to by text and gestures. The core observation of this work is that referents do not lie on the elbow-wrist line but rather on the eye-wrist line, which they call the \u201cvirtual touch line (VTL)\u201d. By learning to predict the VTL using a multi-modal hybrid transformer architecture and by encouraging the co-linearity of object center, eye, and fingertip, the proposed method is shown to perform well on the YouRefIt dataset.",
            "strength_and_weaknesses": "The introduction motivates the VTL well, and general approach is easy to understand, though the architecture section is somewhat brief.\n\nWhile it is written well, the \u201cRelated Work\u201d section is much too verbose and contains parts that are typically considered out-of-scope or tangential, in particular: \u201cGaze estimation\u201d, \u201cSaliency estimation\u201d, and \u201cHRI and collaboration\u201d. Less relevant work could be removed to make the section more compact and to-the-point.\n\nOverall, the results are quite impressive. It is clear that the VTL objective out-performs the use of EWL. The visual information provided by the imaged human is also useful (vs inpainting), and the proposed contribution overall out-performs recent works. It is also nice to see that the VTL-EWL discussion is further validated by seeing the cosine similarities defined in Eq. (1).\n\nHowever, one must wonder whether there are additional ways to demonstrate the value of the contributed method. That is, are there other object-referring datasets that could be used where body keypoints are not annotated, but are visible and thus permit a VTL? I am not a domain expert so cannot make any recommendations, but the method and evaluation here seems far too specific and lack possibilities to generalize.\n\nMiscellaneous:\n- It is never really explained what the \u201c(No Pose)\u201d experiments are.",
            "clarity,_quality,_novelty_and_reproducibility": "The work is introduced, explained, and validated well. It is of relatively high quality and clarity.\n\nI believe the concept of VTL and its incorporation into a multi-modal transformer architecture is rather novel and original.",
            "summary_of_the_review": "The paper was a good read, and the proposed solution is well packaged and delivered. However, the text could benefit from a mild revision, and the results on a single dataset might not necessarily warrant acceptance. Yet, I lean towards the positive side as the overall concept is simple, intuitive, and executed well.\n\n**Post rebuttal comment.**\nI thank the authors and the reviewers for their diligent comments. After having reviewed all available written materials, I feel that while the authors have made a good effort to respond to the various concerns, and their results do show meaningful improvements to the baseline of YouRefIt (Chen et al., 2021), it is difficult to enthusiastically recommend the paper's acceptance for the following reasons:\n- the task is incredibly niche and this is self-evident by the lack of datasets to run evaluations on (and therefore show within-research-domain generality)\n- no indications are made on how to extend the proposed concepts (hand-designed reference vector based on pre-trained keypoint network) to other areas of ML and CV (to show out-of-research-domain generality).\n\nGiven that the scope of NeurIPS is focused in the ML domain with allowance for competitive applications to CV, it is important for papers to provide knowledge that goes beyond a very niche CV topic. Yet, the paper is solid enough to be published at a top-tier CV+ML venue, so I retain my rating of borderline/weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_qAaB"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_qAaB"
        ]
    },
    {
        "id": "r_4Rr82Ijt_",
        "original": null,
        "number": 2,
        "cdate": 1666594457776,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594457776,
        "tmdate": 1666597668409,
        "tddate": null,
        "forum": "ugA1HX69sf",
        "replyto": "ugA1HX69sf",
        "invitation": "ICLR.cc/2023/Conference/Paper3670/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a method to automatically locate referents in scenes using a combination of embodied gesture signals and natural language descriptions. Their key contribution is the implementation of the \"virtual touch-line\", which is the extended line connecting the eye and the fingertip to localize objects in scenes. The authors propose a transformer-based network and a geometric consistency loss to combine the language and the gesture signals and predict the object bounding boxes. They demonstrate the benefits of their approach through experimental evaluations and ablation studies.",
            "strength_and_weaknesses": "**Strengths**\n\n1. The proposed design of the virtual touch-line is sound, intuitive, and well backed up by prior work.\n\n2. The geometric consistency loss is also sound.\n\n3. The ablation studies show the benefits of the individual components of the proposed end-to-end network.\n\n4. The discussion of the limitations helps to understand the scope of the work.\n\n\n**Weaknesses**\n\n1. Looking at Eqn. 1, the authors take the reference line to be the one connecting the eye and the object. Have the authors considered the alternate, the line connecting the fingertip and the object? Is there any specific reason to choose one over the other, e.g., the noises in eye and fingertip detection?\n\n2. I did not find an ablation study that uses only the gestural signals but no language signals. This could be an interesting ablation to observe, especially in the real-world context where the language input is noisy or corrupted.",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity**\n\nWhat does the \"no pose\" ablation mean in Table 1 and how is it different from \"inpainting\"?\n\n\n**Quality and Novelty**\n\nThe proposed method and implementation using the virtual touch-line are novel. The experimental results show the quality of the work.\n\n\n**Reproducibility**\n\nThe authors have clearly explained all the components of their method, making it reproducible. They also provide their source code.",
            "summary_of_the_review": "Overall, the paper solves a challenging problem using an intuitive and sound approach. The technical descriptions are clear and the experiments highlight the quality of the work. This method sets a new benchmark in embodied reference understanding and can be reused in associated applications.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_iwkJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_iwkJ"
        ]
    },
    {
        "id": "QA1nW91mcYm",
        "original": null,
        "number": 3,
        "cdate": 1666605334551,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605334551,
        "tmdate": 1666605334551,
        "tddate": null,
        "forum": "ugA1HX69sf",
        "replyto": "ugA1HX69sf",
        "invitation": "ICLR.cc/2023/Conference/Paper3670/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a transformer based technique for embodied reference understanding. In this problem, the task is to determine the object being referred to (by the human in the image and the text) in an image. The model takes in visual and text features (from a CNN and BERT respectively) and outputs 1) the bounding box of the referent (the object being referred to) 2) touch-line, the vector from the human (referrer) to the referent (referred object). The motivation for including the touch line is that this should help the model locate the referred object. The model is trained to predict both of these outputs. For the touch-line, the authors annotate the datasets themselves to provide this information. They try two different ways to define the touch-line. One is the vector from elbow to wrist (EWL), and the other is the one from the eye to fingertip (virtual touch line, VTL). Citing studies from psychology, the authors argue that VTL is a better choice and in fact show that predicting VTL leads to better referent detection than using EWL. The model uses an additional geometric loss to encourage the predicted touch line and the predicted referred object to lie on the same line. Overall, their model is able to reach SoTA performance on the YouRefIt embodied reference understanding dataset.",
            "strength_and_weaknesses": "Overall, I think this is an interesting paper that shows a few modifications to a multi-modal transformer based setup leads to a significant increase in performance in embodied reference understanding. Note that I'm not familiar with this literature very well (embodied reference understanding seems to be a problem recently introduced), so my assessment of novelty/originality might not be 100% accurate. As far as I understand, the main contributions here are the multi-modal transformer and the predicting virtual touch line alongside the bounding box of referent. I like the virtual touch line idea (and the argument/results for why you would prefer it to EWL) and this seems to increase performance. Overall, the results are pretty strong but it is not fully clear where this increase in performance comes from (more on this below). I'd say the writing is overall pretty clear but some model and evaluation details are missing, which makes it difficult to judge the full merit of the approach (again more on this below).\n\n- What exactly leads to better performance from this model? Or to put it another way, what makes this model work much better than the Chen et al.'s model? Looking at the results, even without any pose information or humans in the image, the method in this paper does much better than Chen et al's? Why is this the case? I think it'd be great to have an ablation analysis that shows where this increase in performance comes from.\n- The requirement of pose annotation (and VTL) limits the applicability of the approach slightly, but the model seems to do better than previous ones even without any pose information. So it'd be nice to know why.\n- An alternative could be to use a pre-trained pose estimation network and use the features from this model as additional inputs. Have the authors tried this?\n- What parts, if any, are pre-trained in this model? It looks like the model builds on a previous object detection model. Is this pre-trained? This has to be mentioned in the paper.\n- None of the results tables have standard deviations. Even though the performance gap between models is pretty large, it'd be nice to have these std devs.\n- In Table 2, when you train with EWL, do you still have the geometric consistency loss? If so, wouldn't this hurt performance since EWL usually doesn't lie on a line with the referent.\n- What happens if you don't have the geometric consistency loss (but still predict VTL)? How much does this hurt performance?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, this work has a few novel ideas (e.g., virtual touch line, using pose) and makes a significant improvement in performance on embodied reference understanding. The writing is overall clear and the results are strong. However, it is not fully clear where the performance increase comes from so the paper would benefit from a more in depth discussion and analysis of the model and results.",
            "summary_of_the_review": "Given the significant increase in performance on the embodied reference understanding task and the novel idea of coupling virtual touch line prediction with referent detection, I think this would be a good addition to the conference. However, I'd have liked to see a more detailed analysis of the performance of the model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_BH2V"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3670/Reviewer_BH2V"
        ]
    }
]