[
    {
        "id": "IzAu2fWH63",
        "original": null,
        "number": 1,
        "cdate": 1666655280231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655280231,
        "tmdate": 1666655280231,
        "tddate": null,
        "forum": "jCpTofV7iY_",
        "replyto": "jCpTofV7iY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3683/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors propose a new language model named non parametric prompting PLM for natural language understanding specially for zero-shot learning. It is an important topic because these days many word-class associations are being produced by end users and previous models heavily depend on unlabeled data and human effort.  The authors showed that the proposed method outperforms state-of-the-art in terms of text classification accuracy and GLUE benchmarks on four different datasets including AG news, DBPedia, IMDB. and Amazon.",
            "strength_and_weaknesses": "The authors put significant effort on proving effectiveness of their method in a variety of NLP tasks. However, I wanted to see significant test results to make sure that the improvements are not random. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper was well written and easy to follow. I would require authors to add the github link for the code. ",
            "summary_of_the_review": "Overall, zero shot learning is an interesting topic in natural language processing as so many new categories and topics are being produced on the web. The authors proposed a simple and easy to implement method for pre trained language models to minimize human effort in terms of labeling and building training data. Overall I am satisfied with the current draft of the paper and request to move forward with discussion. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_Gywf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_Gywf"
        ]
    },
    {
        "id": "f3K8F_mBpoG",
        "original": null,
        "number": 2,
        "cdate": 1666679471853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666679471853,
        "tmdate": 1666679471853,
        "tddate": null,
        "forum": "jCpTofV7iY_",
        "replyto": "jCpTofV7iY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3683/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how to effectively transfer pretrained language models to natural language understanding (NLU) tasks in a zero-shot manner. The proposed method, NPPrompt, does not require any labeled sample or rely on humans to construct prompt label words. NPPrompt generates the label words by searching the related words from the initial word embedding of the pre-trained language model. Then, it aggregates logits from the label words and predicts the category with the largest score. Experiments show that the proposed method is effective and outperforms strong baselines with a large margin.",
            "strength_and_weaknesses": "Strength\n1. Proposed method is simple, intuitive and effective.\n2. Conduct extensive experiments on a wide range of NLU tasks.\n\nWeaknesses\n1. The proposed method is not very novel. Generalizing LMs to NLU tasks by checking the vocabulary distribution over the masked token is a common practice. The method which leverages related words to label category does not look very novel to me.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-written. The work is original, but the proposed method is kind of similar to many existing works, which makes it less novel.",
            "summary_of_the_review": "This paper proposed NPPrompt, which aims to enable LMs' zero-shot ability to NLU tasks without requiring any labeled sample or relying on humans to construct prompt label words. Results show that NNPrompt outperforms strong baselines with a large margin. However, the overall novelty of this paper is limited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_LMgP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_LMgP"
        ]
    },
    {
        "id": "BzoQuofHjas",
        "original": null,
        "number": 3,
        "cdate": 1667520503347,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667520503347,
        "tmdate": 1667520503347,
        "tddate": null,
        "forum": "jCpTofV7iY_",
        "replyto": "jCpTofV7iY_",
        "invitation": "ICLR.cc/2023/Conference/Paper3683/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper presents a novel method for zero-shot inference with masked language models (MLMs) such as BERT and RoBERTa for classification problems. The key idea is to use the encoders of the MLMs themselves to find relevant words for a given label name. For example, if the target label is \"SPORTS\", then the proposed method will first find the words by searching k-nearest neighbors with embedding distance. The embeddings are induced by the MLM itself. Finally, they aggregate the logits of these k words (for filling the masked positions). The authors argue that this new method (named NPPrompt) is a fully zero-shot classification method because it does not need any other information about labels. The empirical comparisons show that the proposed method has a comparable performance with other zero-shot methods (that needs some additional info such as KB or the unlabeled data). \n\n",
            "strength_and_weaknesses": "## Strengths\n\n- The proposed NPPrompt method is simple and effective. It only requires the label strings for doing zero-shot classification. \n- The empirical results show that it is on the same level as other methods that need more information about the labels.\n\n## Weakness \n\n- The proposed method is pretty limited to the classification tasks, particularly for news classification, where the label names are informative and have many semantic relevant words in the vocabulary. It is also limited when the label names are multi-word expressions or phrases. It does not support the cases where the label names are not informative enough and additional label description is needed. \n\n- The title and narrative can be misleading and an overclaim. As mentioned before, it is limited to masked LMs and the advantage is mainly for tasks like news classification. \n\nSuggestion:\n\n- I suggest the authors extend the proposed method to *multi-label classification* settings where an example can have multiple acceptable news. \n- A major limitation is the multi-word expression and out-of-vocabluary label names.  How do you generalize the method to support that?\n- Also, I suggest the authors try some multiple-choice QA problems.",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is mostly clear and the novelty is incremental. I think the reproducibility should be okay since the method is pretty naive. \n\n- Can you show the kNN results for sentiment analysis tasks and NLI tasks? I don't find them in the paper but they are quite important for qualitative analysis. ",
            "summary_of_the_review": "Overall, I think the paper presents a novel zero-shot classification paper but the limitations are there and the performance is not that significant. I think if the authors can extend the paper with my suggestions in the above section, the paper will be more suitable to the ICLR community. Otherwise, it will have much less impact to the field. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_DKg9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3683/Reviewer_DKg9"
        ]
    }
]