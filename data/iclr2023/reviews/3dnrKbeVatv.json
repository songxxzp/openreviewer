[
    {
        "id": "4ALmgcwLCA",
        "original": null,
        "number": 1,
        "cdate": 1665749887639,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665749887639,
        "tmdate": 1669277240361,
        "tddate": null,
        "forum": "3dnrKbeVatv",
        "replyto": "3dnrKbeVatv",
        "invitation": "ICLR.cc/2023/Conference/Paper4466/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the setting of homogeneous domain generalization, and proposes a novel single-sample based adaptation scheme. In this scheme, the discriminative classifier remains constant but the respective test sample gets adapted in feature space based upon an energy-based objective. The main contribution of the paper is to adjust energy-based density modelling such that it can be employed at test-time for domain adaptation. For this, the energy-based modelling is extended to be conditioned on categorical information inferred from the test sample before adaptation such that test sample adaptation  does not loose categorical information.",
            "strength_and_weaknesses": "Strengths:\n * Using energy-based density modelling as basis for test-time adaptation and adapting the sample rather than the model are novel ideas (to my knowledge).\n * The presentation of background and method in Section 2 is thorough and complete.\n * The proposed method is well motivated and clearly derived from first principles.\n * Useful illustrations are provided that illustrate essential parts of the problem/method\n * The empirical evaluation is thorough and conducted on diverse benchmarks, including non-image data\n\nWeaknesses:\n* The main question for me is if conditioning on the categorical information $z$ in energy-based density modelling is reasonable. This design choice is motivated by \"In order to maintain the category information of the target samples during adaptation and promote better classification performance, we further introduce a categorical latent variable in our energy-based model.\" (Introduction). However, when used within a discriminative model, wouldn't it be more natural to predict classes directly upon this categorical latent variable rather than (i) first adapting a sample to become more likely under the energy with this categorical variable and then (ii) predict based upon this sample? Directly predicting upon the latent variable would be more computationally efficient and it remains unclear why the two step approach (i) + (ii) should result in better predictive performance? Specifically, when making $p_{\\phi_i} (y\\vert z^n, x)$ unconditional of $x$, that is: $p_{\\phi_i} (y\\vert z^n)$, how would this perform at inference time?\n* The paper somehow (probably unintentional) obfuscate that adaptation is conducted in feature space and not in input space. It is briefly mentioned at the end of Section 2, but is easily missed (I missed it did during the first read). Since this is clearly an essential part of the method's design, it should be presented earlier and clearer. This also applies to Algorithm 1 where both input and its feature representation are denoted by $\\mathbf{x}$.",
            "clarity,_quality,_novelty_and_reproducibility": "As discussed above, sufficient novelty is contained in the proposed method. Clarity and reproducibility lack sometimes, among others for the following reasons (and the one mentioned above under weaknesses):\n* Several important details are missing such as: dimensionality of z; how are heads for predicting z, y and the energy defined; what is N in equation 11? In the current form, the proposed method is hard to reproduce.\n* The textual discussion after Equation 7 is lengthy and not always correct. It would benefit from another iteration of shortening and making the discussion more precise. Example \"The fourth term minimizes the energy and maximizes the prediction log-likelihood of the samples from the model distribution.\" -> because of the stop-gradient, this term does not optimize energy/log-likelihood of a given sample, but rather increase the probability of such low-energy/high likelihood samples under $q$.\n* Algorithm 1 states that contrastive samples at training time come with probability 50% from some set $\\mathcal{B}$. This is not discussed in the main text and also not motivated\n* Initialization and update of $d_{x^j}$ remains unclear.\n\nMinor points:\n * In Figure 5, why is the energy without conditioning on z higher than when conditioning? It seems that in such an unconstrained setting, a lower energy should be feasible.",
            "summary_of_the_review": "In summary, the paper proposes an interesting approach to a relevant problem. Clarity and reproducibility could be improved with a minor revision. At this point, my main concern is whether energy-based sample adaptation as proposed is actually helpful if it is based upon inferred categorical information (which kind of requires solving the classification problem already _before_ the adaptation). This point requires clarification and some solid empirical support before warranting acceptance of this paper.\n\n### Updated after author feedback ###\nThe authors have addressed my main criticism and have provided preliminary evidence that energy-based test sample adaptation is preferable over standard adaptation in a small batch setting. I am increasing my overall score, assuming that the authors will extend and revise their manuscript accordingly.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_Y7Jd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_Y7Jd"
        ]
    },
    {
        "id": "YeViYYCAaGu",
        "original": null,
        "number": 2,
        "cdate": 1666161882445,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666161882445,
        "tmdate": 1668792009115,
        "tddate": null,
        "forum": "3dnrKbeVatv",
        "replyto": "3dnrKbeVatv",
        "invitation": "ICLR.cc/2023/Conference/Paper4466/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a domain generalization method with a new discriminative energy-base model. The proposed idea is novel and effective because they adapt the target samples to source distributions instead of generalizing the model to unseen target samples in most previous methods. Theoretically, the authors provide detailed mathematical derivations and proofs. Experimentally, the authors present extensive empirical results and conduct a thorough ablation study.",
            "strength_and_weaknesses": "Strength:\n\n1. The paper is well written, the motivations for choices in the method are clear and the method is simple yet effective, ripping the benefits of a discriminative energy-based model and Langevin dynamics.\n\n2. The background of the energy-based model is clear and kindful in Section 2.\n\n3. Extensive experiments show that this method performs quite well in comparison to existing DG methods on both image and text benchmarks. Thorough insights are conducted in experiments.\n\n4. Theoretical validation and qualitative results indicate that energy-based test sample adaptation is a good solution to reduce the domain gap at test time.\n\nWeaknesses:\n1. It's a bit unclear why the authors haven't included comparisons or discussions about how this method would perform in source-free DA settings. I expect to see whether this method would work in that context too.\n\n2. During the training phase, the proposed sample adaptation framework simultaneously trains the shared backbone network, the classification model and an extra energy function. In the test phase, the target samples are adapted by Langevin dynamics of the energy function. Does this compare fairly to existing DG methods to only use source data at the training step? Depending on the method but it would be a good discussion to have. As an aside, this concern arises because the proposed framework w/o adaptation has achieved comparable performance (e.g., Table 1 and Table 2).",
            "clarity,_quality,_novelty_and_reproducibility": "It is a well-written paper with thorough motivation for the choices made. However, reproducibility lacks sometimes.\n\nMinor questions:\n\nIn Sec. 2.1, The sentence \"The objective function in eq. (1) encourages\" is not finished.",
            "summary_of_the_review": "Interesting paper on domain generalization at test time that proposes a discriminative energy-based model to adapt target samples to source distributions, and works well on six image and text benchmarks. I have concerns about its viability as a source-free domain adaptation method but would be willing to reevaluate based on the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_aESp"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_aESp"
        ]
    },
    {
        "id": "gb86AmAwVt",
        "original": null,
        "number": 3,
        "cdate": 1666358296958,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666358296958,
        "tmdate": 1669034190041,
        "tddate": null,
        "forum": "3dnrKbeVatv",
        "replyto": "3dnrKbeVatv",
        "invitation": "ICLR.cc/2023/Conference/Paper4466/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper proposes a test-time sampling based domain generalization approach based on a discriminative energy-based model formulation. The model performs modification of test-time inputs by sampling towards the source data distribution via Langevin dynamics, while simultaneously preserving label information (by augmenting their model with a categorical latent variable) to maintain the discriminative components of the adapted sample. Experiments are performed on both image and text dataset benchmarks on multi-source domain generalization, demonstrating the effectiveness of all components of their model with several ablation studies.",
            "strength_and_weaknesses": "Strengths: 1) The paper is nicely written and the methodology is well described (considering also the clarifications in the Supplementary). 2) Proposed approach is novel and it shows compatibility to different data structures due to its capability to operate in a latent feature space.\n\nWeaknesses: 1) There are several fundamental assumptions implied in the methodology with regards to the extent of the source and target domains, which should have been clarified/discussed. 2) The proposed model is rather complex in its optimization objective since it involves many parametric components put together, and this may somewhat limit its applicability to a broad range of domain generalization problems (e.g., single-domain generalization). 3) Empirical evaluations show minimal performance gains with the proposed approach with respect to the state-of-the-art.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is written nicely in my opinion, however there are several parts of the Appendix that authors should incorporate in the main manuscript to make it easier for the reader to digest the approach: particularly the two Algorithms and Figure 6 with the caption in Suppl. Sec B. Quality & Novelty: The approach appears to be novel with its application to domain generalization. Results are however not pushing the state-of-the-art drastically in all respects. Reproducibility: Sufficient, since the authors provide their implementations. However without precisely the executed runs (or saved models), I believe it can be quite challenging to reproduce the exact results.",
            "summary_of_the_review": "Generally I find this paper to touch an interesting problem with a novel generative-discriminative modeling based approach. Yet, there are some unclear points that I would like to clarify and I summarize them below:\n- The model learns an unbounded energy function. It is not clarified though, at test time how does the method ensure a lower energy bound on this sampling process? To illustrate: a given test target domain sample can(?) in fact be evaluated to have a much lower energy value than observed already at the beginning of the sampling process. This would then \"adapt\" the sample (for a fixed number of SGLD steps) towards previously unexplored regions by the source domain data? How can the model correct for this and give confident decisions in such cases?\n- A somewhat implicit assumption of the model is that the categorical latent representation for the test sample will have high fidelity to the correct class. Can the authors discuss how reasonable is this? In case the target sample is represented strongly within an incorrect source domain class (through the only-source-trained backbone), wouldn't this divert the SGLD sampling process off-track, towards the wrong class?\n- Is there a deeper analysis of the ensemble classification approach? How robust does this make the model, in comparison to e.g. adapting the target sample only to the closest source domain? Similarly one could also compare the ensemble approach to decision making with respect to the most confident source domain mapping.\n- Experiments show that the proposed method can only work on learning from multiple source domains. Interesting observations are presented in the supplementary (Table 9) in that sense, where the model more or less fails on domain generalization from CIFAR-10 to CIFAR-10-C with hand-crafted sources. How is it on ImageNet-C, did the authors try? Also there many simple data augmentation methods that can achieve very high domain generalization of a vanilla classifier to CIFAR-10-C and ImageNet-C, which one should include in Table 9.\n- How sensitive are the final accuracies in Table 2 to the # sampling steps? Can the authors provide some additional rows to Table 2 with 10-steps and 50-steps for instance?\n- What was the exact feature dimensionality for the EBM modeling network per dataset? How did the authors define the feature extractor backbone? (e.g., is it a post-ReLU activation before the final classifier layer or so?)\n- Minor comment: The first sentence of the second paragraph in Sec 2.1 is unfinished.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_yzZP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_yzZP"
        ]
    },
    {
        "id": "7ybbh4fkaY",
        "original": null,
        "number": 4,
        "cdate": 1666664534770,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664534770,
        "tmdate": 1666665621446,
        "tddate": null,
        "forum": "3dnrKbeVatv",
        "replyto": "3dnrKbeVatv",
        "invitation": "ICLR.cc/2023/Conference/Paper4466/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors proposed an energy-based sampling adaptations method for domain generalization. In their method, they adapt the unseen target sample to source-domains at test time. A category latent variable is used to sample update. They show the effectiveness in several classification tasks.",
            "strength_and_weaknesses": "Strength:\n\n1. The energy-based approach of sample adaption for domain adaption is interesting. The background and proposed method (training and inference) are also clear.\n\n2. The proposed method show the effectiveness on several tasks. And the visualization of the adaptation process shows the effectiveness of the method.\n\n3. The authors also point  out the failure case or limitations of the proposed method with complex background. \n\nWeaknesses:\n\n1. Seems hard to work well in complex tasks. And the proposed rely on good feature representation (the shared backbone model). \n\n2. Somewhat unsure whether is fair during inference.  In this paper, the ensemble inference method is used. The adaption can lead the predication more confidently at each domain classifier.  The sum of log p for each domain domain seems reasonable with sample adaption. it is unclear whether without sample adaption is already good enough with voting method.  \n\n3. In Figure 5,  energy and accuracy are shown as the number of step increase. The energy is decreasing. However, there is still some space for energy. What is the sample accuracy with lowest energy?",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity is clear. \n\nOne question, in this paper, adaption is also used in training. However, it is unclear how useful for this part. Maybe sample adaptation is already  work well during inference. If this is true, the novelty is limit,  large part of paper content is about training. ",
            "summary_of_the_review": "Generally, this is a good paper with clear writing. The story is complete. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_E7hz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4466/Reviewer_E7hz"
        ]
    }
]