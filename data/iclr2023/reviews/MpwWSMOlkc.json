[
    {
        "id": "2O2ifRhxcb",
        "original": null,
        "number": 1,
        "cdate": 1666260295638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666260295638,
        "tmdate": 1666773515765,
        "tddate": null,
        "forum": "MpwWSMOlkc",
        "replyto": "MpwWSMOlkc",
        "invitation": "ICLR.cc/2023/Conference/Paper3788/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This work proposes a manifold learning strategy based on autoencoders (with a Euclidean distance reconstruction loss) and continuous k-nearest neighbor graphs (CkNN) (with a topological / local distance preservation loss between input and latent space). It implements a constrained optimization where topological loss is minimized such that the reconstruction loss is upper bounded.",
            "strength_and_weaknesses": "\nPros:\n- Paper is well written\n- Method is clear and simple\n- Experiments on a number of datasets\n\nCons:\n- Novelty is limited\n- Choices not motivated or ablated\n- Additional choices not explored\n- Experiments are weak\n\nThe paper is very well written and easy to follow, as a result the method and its evaluation are very clear. The method is simple and found to be effective on a number of datasets.\n\nThe novelty is limited, as the method is merely a combination of existing autoencoders, existing priors and an existing nearest neighbor graph definition.\n\nThe proposed strategy is presented as a fixed recipe, without motivating why the particular choices are made and without ablating either the algorithmic choices or the hyperparameters. The later must have been tuned but no results are given or discussed. This is important because it is unclear if hyperparameters have been also tuned for competitors. The choice of $k=9$ is given as an arbitrary choice, while for the length scale $\\delta$ not even a value is given.\n\nWhy is the topological distance minimized while the reconstruction loss posed as a constraint? Why not vice versa? Why not minimize a function of both?\n\nMore importantly, why use CkNN and why this way? Using CkNN is claimed as a main contribution (\"we are the first to adapt it to...\"). Fixing all other choices made, how would the behavior be affected by any other graph definition? For example:\n- CkNN uses the geometric mean of the kNN distances of x and y in defining an edge between x and y. Why not experiment with the generalized (power) mean, which subsumes the minimum/maximum, the RMS as well as the harmonic, geometric and arithmetic mean among others? Why not any other symmetric function?\n- If the graph definition is so important, why not a weighted graph, which would result in weighted loss terms in eq. (1)? Again, a number of functions could be candidate for the weights.\n\nExperiments are weak. In particular:\n- The datasets used are small (in fact, toy) and the metrics are near perfect if I am not mistaken (assuming perfect value 0 for two of the metrics and 1 for the other two). As a result, the differences between methods are tiny.\n- Tables use bold black and bold blue, which are not explained in the captions. Bold black must be the best but bold blue is unclear. This is misleading because bold blue is chosen for the proposed method.\n- In Table 5 (COIL), SNE is best under 2 of the 4 metrics and in Tables 1-4 (MOCAP) the difference is 0.1%.\n- In Fig. 9 (MOCAP), the latent space of UMAP looks excellent, while in most other cases UMAP is missing from the comparison with no explanation.\n- On Swiss Roll and CIFAR10, metrics are perfect and there is no benefit over VR.\n\nI would expect experiments on more realistic datasets used for unsupervised metric learning, for example CUB, Cars, SOP and InShop. These experiments should also use larger network architectures and include comparison with modern unsupervised metric learning methods.",
            "clarity,_quality,_novelty_and_reproducibility": "As detailed above, the writing is of high quality and the method is very clear. However, the originality is limited as the proposed method essentially combines existing ideas without justifying and supporting the choices made.",
            "summary_of_the_review": "This is a simple and interesting manifold learning strategy that makes sense but novelty is limited, choices appear to be arbitrary, additional choices are not explored and experimental evaluation is weak.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_1cQZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_1cQZ"
        ]
    },
    {
        "id": "NHoVjheLqE",
        "original": null,
        "number": 2,
        "cdate": 1666465341573,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666465341573,
        "tmdate": 1670320749205,
        "tddate": null,
        "forum": "MpwWSMOlkc",
        "replyto": "MpwWSMOlkc",
        "invitation": "ICLR.cc/2023/Conference/Paper3788/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an extension of auto-encoder via incorporating the local distance preserving. Specifically, the local distance preserving is implemented by: \n\n(1) constructing two graphs (via CKNN) on the raw features and the learned representations; \n(2) comparing the two graphs via the pairs contained in both two graphs. \n(3) further proposing to optimize a contrained optimization problem for the generative models (e.g., VAE). \n\nThe authors also discuss several cases with different priors.  \n\nSufficient experiments are also reported in both main paper and appendix. \n",
            "strength_and_weaknesses": "## Strength\n\n- The paper is easy to follow. \n- The paper seems technically sound and the authors discuss different situations with diverse prior distributions. \n- The experiments are sufficient for me. \n\n## Weakness\n- My main concern is that the novelty seems not sufficient for me. The constrained optimization for generative models seems well-studied and the idea of local preserving seems widely used. Although this paper may emphasize that the preserving mechanism is built on *distance*, it not a significant contribution for me. Moreover, another key is the utilization of continuous k-nearest neighbors, which may be not a remarkable contribution of this paper. \n- In problem (1), the loss only considers the overlapped links. Should the different links between two graphs be also important for training auto-encoder? Could the author provide some explanations? ",
            "clarity,_quality,_novelty_and_reproducibility": "\\The constrained optimization for generative models seems well-studied and the idea of local preserving seems widely used. \n\nAlthough this paper may emphasize that the preserving mechanism is built on *distance*, it not a significant contribution for me. \n\nThe constrained optimization for generative models is also well-studied. \n\nAnother key is the utilization of continuous k-nearest neighbors, which may be not a remarkable contribution of this paper. ",
            "summary_of_the_review": "The main concern is the novelty and some questions about some parts.\n\nI'd like to update my score after the discussion period. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_bRM4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_bRM4"
        ]
    },
    {
        "id": "v_80mBGZLT",
        "original": null,
        "number": 3,
        "cdate": 1666709517974,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666709517974,
        "tmdate": 1666709517974,
        "tddate": null,
        "forum": "MpwWSMOlkc",
        "replyto": "MpwWSMOlkc",
        "invitation": "ICLR.cc/2023/Conference/Paper3788/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduces several auto-encoder models that preserve local distances when mapping from the data space to the latent\nspace. The proposed models use a local distance-preserving loss that is based on the continuous k-nearest neighbours graph which is known to capture topological features at all scales simultaneously. In order to improve training performance, the models formulate learning\nas a constraint optimisation problem with local distance preservation as the main objective and reconstruction accuracy as a constraint. The proposed method provides state-ofthe-art or comparable performance across several standard datasets and evaluation metrics.",
            "strength_and_weaknesses": "Strength: The paper is well written and easy to follow.\nWeaknesses:\n1. The novelty of the proposed method is limited, the proposed method just integrates the previous  local distance-preserving loss into auto-encoders;\n2. In the experiments, the detailed parameter settings of auto-encoders are not clearly introduced.",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty is limited and some details are missed.",
            "summary_of_the_review": "Based on the limited novelty and unclear presentation, I am willing to reject this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_n97h"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3788/Reviewer_n97h"
        ]
    }
]