[
    {
        "id": "lw40dALZie4",
        "original": null,
        "number": 1,
        "cdate": 1665996147303,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665996147303,
        "tmdate": 1665996147303,
        "tddate": null,
        "forum": "v4ePDrH91D",
        "replyto": "v4ePDrH91D",
        "invitation": "ICLR.cc/2023/Conference/Paper2610/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper contributes a topology-based variant of the precision and recall measures, originally introduced by [Sajjadi et al.](https://proceedings.neurips.cc/paper/2018/file/f7696a9b362ac5a51c3dc8f098b73923-Paper.pdf). The new method makes use of a confidence band analysis, inspired by methods from topological data analysis, to obtain more robust estimates of the underlying support. The proposed measures are thus less affected in the presence of certain outliers and specific types of perturbations. A suite of experiments comparing the new measure with existing methods shows that the proposed method can be gainfully employed in practice.\n",
            "strength_and_weaknesses": "I see the main **strength** of the paper in making the existing measures more robust towards certain sources of noise and perturbation. Employing a method from topological data analysis is a useful way forward to better characterise the underlying support manifold. The recent years have shown that a better understanding of generative models necessitates also a better understanding of the respective spaces involved; the method at hand provides a useful step in this direction.\n\nWhile I appreciate the research direction and consider this to be a timely, relevant contribution, the current version of the paper needs to be improved in the following areas:\n\n1. **Clarity (and intuition)**. Even though the method is described as topology-driven, there are almost no explanations of the underlying concepts of topology in the main text. While I am familiar with the cited literature, in particular concerning the reliable estimation of topological features, the paper does not provide sufficient explanations for its approach:\n    - Concepts from persistent homology are being used throughout the text but not explained. This will make the paper inaccessible to readers without prior exposure to the theory. I see two ways around this, the first involving a more detailed discussing of topological concepts, the second aiming to rephrase statements in a \"less topological\" way. For instance, \"homological features\" could probably be rephrased to \"connected components\" in this context.\n    - It seems to me that only *connected components* are being used to assess the topology. This needs to be clarified, in particular since the persistence diagram shown in Figure 1 works for higher dimensions as well.\n    - Likewise, there is no exploitation of multi-resolution structure in the data as far as I can tell. A single threshold is defined for extracting the superlevel set; this is more or less like a \"noise suppression technique\"; the differences in scale between modes/connected components is not exploited in any way.\n    - Given that the main feature of the new method is its purported robustness, more details about this calculation need to be provided. Section 3.2 is too terse for this.\n    - Fidelity and diversity as concepts need to be introduced at least briefly.\n\n2. **Novelty and delineation to existing concepts.** Reading the definitions in Section 3.1, my impression is that these are minor modifications of the existing measures, but potentially being calculated in a somewhat simplified space, i.e. the superlevel set of the data with low-persistence (irrelevant) connected components being removed. It is therefore important to provide a better delineation to the existing terms. Moreover, it appears that the main contribution of this paper consists in applying the confidence band calculation method to derive a parameter for removing outliers form an underlying space. If this is the case, would it not be possible to apply this method *as-is* for the \"ordinary\" definition of precision and recall? In some sense, all the experiments crucially depend on the outlier removal process, so I wonder what would happen if the outlier removal and the calculation of `TopP` and `TopR` would be disentangled.\n",
            "clarity,_quality,_novelty_and_reproducibility": "## Clarity\n\nAs mentioned above, the current version of the paper is lacking in clarity, in particular when it concerns readers without prior exposure to TDA. Moreover, the original paper on precision and recall by Sajjadi et al. provides a gentler introduction to the required concepts; it might be useful to consider some simple examples presented in this work. Please find some additional comments here:\n\n- The second paragraph of the introduction read more like a separate motivation section for me. Consider rephrasing this.\n\n- I would suggest going into more details for Figure 1 and potentially remove the persistence diagram. To my understanding, the method for estimating confidence bands does *not* make use of topological features directly (of course, they are implicitly being used to create a bound with respect to the Hausdorff distance of functions, but this is a technical detail that can be ignored here); it is thus vital to expand more on their actual utility for this approach.\n\n- What is $p_h^*$ in Section 2.2? I assume this is lifted from one of the articles on persistence diagram confidence bands; please clarify and introduce this.\n\n- \"for $\\forall$ element\" in Algorithm 1 is redundant\n\n- What is $h_n^d$ denoting in Section 3.1? Is this supposed to read $n h^d$?\n\n- Section 5.1.3 is missing a reference to Figure 4, I believe\n\n- What is $\\psi$ in Section 5.2.1?\n\n## Quality\n\nThe quality of the paper could be introduced in places, in particular when it comes to the experiments:\n\n- Why are no standard deviations of scores being shown? Given a high degree of stochasticity, I would expect this to be the case.\n\n- Why are not other metrics being shown in Figure 8? I would find a comparison with existing metrics to be highly relevant here.\n\n- Adding to this: please disentangle the \"outlier removal\" step from the calculation of precision and recall. Would it not be possible to subject the measures by Sajjadi et al. to the same treatment?\n\n## Novelty\n\nAs mentioned above, I have some trouble assessing the novelty of the paper; a delineation to existing work would be most welcome. Here are some additional comments concerning this aspect:\n\n- When discussing the properties of the new estimator, it appears that existing results are being lifted from literature. This should be made more clear from the beginning; initially, I had some trouble understanding the theoretical contributions of the paper.\n\n## Reproducibility\n\nSince no code is provided, important implementation details are missing and I do not believe that the work is reproducible in its current form (this is also influenced by the clarity issues outlined above).\n\n## Minor suggestions\n\nThe paper employs some non-standard and less formal phrasing. Here are some suggestions for improving this:\n\n- I do not understand the phrase \"it is of question to to distinguish features [...]\". Please rephrase.\n\n- Samples are not \"well-curated\", but data sets can be well-curated.\n\n- What does \"wild\" mean for practical scenarios? There is the notion of a \"tame\" function in persistent homology and computational topology in general; is \"wild\" supposed to be a contrast to this?\n\n- \"tweak\" --> \"adjust\" / \"rewrite\" / ...\n\n- I would suggest to rephrase \"we let the kernel density estimator [...] as\" to \"we denote the kernel density estimator [...] as\". Similar reformulations apply throughout the paper.\n\n- I would rephrase \"can have noises\" to \"can suffer from noise\"\n\n- \"anagous\" --> \"analagous\"\n\n- \"proposedt opological\" --> \"proposed topological\"\n\n",
            "summary_of_the_review": "While I appreciate the direction and subject of this paper, I cannot endorse it for publication in its current form, mainly due to a lack of clarity and intuition, as well as a missing delineation to previous work, which in turns makes it hard to assess the novelty  of the approach. I do believe that filtering outliers using the proposed method is a potential step in the right direction, but in its present form, the paper does not make a compelling case for this. This might change if the topological considerations were to be expanded in order to better discuss their potential benefits.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_xs7v"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_xs7v"
        ]
    },
    {
        "id": "dXhoM1QY0q",
        "original": null,
        "number": 2,
        "cdate": 1666596799270,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596799270,
        "tmdate": 1670419649428,
        "tddate": null,
        "forum": "v4ePDrH91D",
        "replyto": "v4ePDrH91D",
        "invitation": "ICLR.cc/2023/Conference/Paper2610/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a pair of robust metric for the evaluation of generative models, dubbed Topological Precision and Recall (TopP&R). The proposed metric is proved to achieve consistency with robustness. The effectiveness is validated through experiments on both synthetic and real data. ",
            "strength_and_weaknesses": "Strength: \n\nS1:  A new pair of robust metric for the evaluation of generative models is proposed.\n\nS2: Consistency with robustness is proved for the new metric. \n\nWeakness:\n\nW1: This paper has some parts not clearly written. There are some examples:\n\n(W1.1): The persistence diagram is strange. The x-label and y-label seem incorrect.\n\n(W1.2):In defining precision$_P(\\mathcal{Y})$ in Section 3.1, is $Q(supp(Q))$ really needed? \n\n(W1.3): What is $\\mathcal{P}$ in \"lim inf$_{n\\rightarrow \\infty}\\mathbb{P}\\big( \\mathcal{P}\\in\\mathcal{B} \\big)$\" in Page 13?\n\n(W1.4): How to understand \"homological features whose (birth) \u2265 $c_X$ and (death) \u2264 $c_X$\" in Page 13?\n\n(W1.5): In Assumption A1, it says \"(3) $K$ is Lipschitz continuous and of second order..\". Is this sentence complete? \n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity score 3/10: This paper is in general not clearly written. See the Weakness for example. \n\nQuality score 5/10: The proof of consistency seems correct. I haven't checked all the details partly because this paper is hard to follow. \n\nNovelty score 6/10: The proposed metric is relatively new since the idea of using persistence diagram for outlier resistance is natural. \n\nReproductivity 5/10: There are so many details in the experiments and it seems not easily reproduced without shared codes.",
            "summary_of_the_review": "Due to the comments in \"Clarity, Quality, Novelty And Reproducibility\", I suggest \"marginally below the acceptance threshold\".\n\n\n-------------------------After rebuttal---------------------\nMany thanks for the authors rebuttal. I read the authors' feedback and other reviewers' comments. Although some of my concerns have been addressed, I am still sorry to suggest \"5 borderline reject\" due to the \"Clarity, Quality, Novelty And Reproducibility\".",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_sgKE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_sgKE"
        ]
    },
    {
        "id": "huljrDjxdhl",
        "original": null,
        "number": 3,
        "cdate": 1666931958807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666931958807,
        "tmdate": 1666931958807,
        "tddate": null,
        "forum": "v4ePDrH91D",
        "replyto": "v4ePDrH91D",
        "invitation": "ICLR.cc/2023/Conference/Paper2610/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper claims that the previous metrics for evaluating generative models (e.g., FID, IS, Precision, and Recall) may not be reliable as they estimate support manifolds based on sample features, and sample features may contain outlier or noisy features. Thus, it proposes a new method for robustly estimating support manifolds using Kernel Density Estimation under topological conditions. The theoretical results show that the proposed metrics are 1) robust to outliers and Non-IID noise and 2) consistent. Finally, the experiments demonstrate relative robustness to outliers and Non-IID perturbations of features for the metrics. ",
            "strength_and_weaknesses": "<Strengths> \n\n1) The paper addresses an important topic as several metrics have been proposed by the community for evaluating generative models, but there is no global consensus on which metric can better rank the models. \n\n2) The theoretical analysis shows the robustness of the proposed metrics in the presence of outliers / noisy samples. Also, it shows that the proposed metrics are consistent.\n\n3) The experiments demonstrate some degree of noise/outlier robustness in both synthetic and real scenarios.\n\n<Weaknesses>\n\nAlthough the theoretical insights and the goal of the paper to \u201cdevelop a metric robust to noise/outliers\u201d are interesting, I think they may have a limited application due to the following points:\nIn practice, there is neither ground truth about a data point being an outlier nor about the number of modes. One uses the training dataset (real data denoted by X in the paper) to train the generative model and then uses the metrics to measure different aspects of the model like fidelity, diversity, etc. The proposed metric does not provide a metric that can identify noisy/outlier samples before training. Rather, it attempts to filter noisy/outlier samples in (X) and generated images (Y) when quantifying the quality of the trained model.  Now I have the following questions in this regard:\nThe paper mentions in section 3.1 that: \u201cUsing the superlevel set at C_x allows to filter out noise whose KDE values are likely to be small.\u201d However, doing so may result in unwanted consequences. For example, in practical settings, datasets have a long-tailed and imbalanced distribution w.r.t different factors such as age, gender, race, etc. The minority sets in data are not necessarily outliers, but they are likely to have a low KDE value considering that real-world data usually lies on a low-dimensional manifold with disconnected parts in the high-dimensional space [1]. Thus, the proposed metrics may simply discard them and hide the potential problems of the generative model regarding fairness.\n\nI appreciate the experiment in Figs. 2 and 3 that shows precision and recall may not accurately reflect fidelity and diversity in the presence of outliers. However, as I mentioned above, there is no ground truth about a point in data being an outlier in practice, and the generative model gets trained on both inlier and outlier/noisy data. Thus, even if the proposed metric can filter noisy samples in X and Y, its predicted quantities may not reflect the true behavior of the model and its problems.\n\nBased on parts (a) and (b) above, it seems the theorems provided in the paper may also have limited usefulness in practice.\nAs far as I know, the truncation trick is proposed because the training dataset may not cover all of the latent space of the style GAN. Thus, randomly generating samples from some areas of the latent space of the trained model may result in low-quality/noisy samples. When increasing the diversity in Fig. 5, inevitably, the model will generate low-quality/noisy samples as well which should result in lower precision values. However, TopP remains constant, and the precision score decreases. Does this observation indicate that TopP may be inferior to the regular Precision score in some scenarios?\n\nThe proposed method needs several parameters that should be tuned while limited details and no code implementations have been provided. The paper only mentions references about how the values of \u2018h\u2019 are calculated for different experiments, but it does not state what those values have been. Also, different choices of \u2018alpha\u2019 in Alg. 1 will result in different C_x, which also determines the samples to keep/discard when calculating TopP/TopR. I believe that exploring how the proposed metrics change with different values of alpha/h and suggestions about how to choose them should be provided in the paper.\n\nI think the paper should also consider using self-supervised learned (SSL) representations beyond ImageNet pretrained classifiers to check the relationship between the metrics. It has been shown that SSL representations produce a more reasonable ranking than the latter [3].\n\n[1] Disconnected Manifold Learning for Generative Adversarial Networks, NeurIPS 2018.\n[2] On Self-Supervised Image Representations for GAN Evaluation, ICLR 2021.\n\nI would be happy to raise my score if the authors can address my concerns.\n",
            "clarity,_quality,_novelty_and_reproducibility": "<Clarity> I recommend that the authors improve the writing of the paper. For example, I cannot find which dataset is used to produce the figures in Fig. 4.\n\n<Quality and Novelty> I am not familiar with related works about topological data analysis, so I am not sure about how much the proposed method is novel, but the theoretical results seem to be novel.\n\n<Reproducibility> As 1) the code implementations and 2) important hyperparameters (See <Weaknesses, section 3 above> are not provided, I think reproducibility of the results is difficult.\n",
            "summary_of_the_review": "Based on the points above, I think the paper should be enhanced in several aspects:\nHow such a metric will be useful to develop generative models considering the concerns that I mentioned about the metric may not reveal the problems of the model regarding fairness or the data itself?\n\nSubstantial missing details regarding experimental implementations and hyperparameter values should be provided.\n\nThe paper should also compare the relationship between different metrics when using self-supervised learned representations as they have been shown to better rank the generative models.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "Please see <Weaknesses section 1. a> above.",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_YyJA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2610/Reviewer_YyJA"
        ]
    }
]