[
    {
        "id": "pgwDZTSFtDU",
        "original": null,
        "number": 1,
        "cdate": 1666446777164,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666446777164,
        "tmdate": 1666446797324,
        "tddate": null,
        "forum": "8JqINxA-2a",
        "replyto": "8JqINxA-2a",
        "invitation": "ICLR.cc/2023/Conference/Paper1852/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper models the joint distribution of texts and images using a discrete diffusion model. The basic idea is to firstly using a VQ model to convert an image to discrete tokens, and then model tokens of both texts and images. This paper designs a specific Markov transition matrix as well as a mutual attention mechanism for this joint modeling task. By joint modeling, this paper can do tasks such as joint sampling of texts and images, text to images, and image captioning.\n",
            "strength_and_weaknesses": "Strength:\n\nThe experiments are interesting for me. For example, it can mask parts of images and captions, and then generate the masked parts, which might be useful in practice.\n\nQuestions:\n\n1. Missing details of conditional generation. This paper models a joint distribution $p(x^{img}, x^{txt})$, where sampling from the joint distribution is obvious. However, it is not obvious for me to sample from the condition distribution, such as $p(x^{img} | x^{txt})$. How does this work implement it?\n\n2. Missing details of image impainting. The mask is drawn in the pixel space, instead of the token space. So the implementation of impainting is not so obvious. I suggest to add these details in Appendix. Besides, does this method support mask an arbitrary area in the image? How to ensure the consistency in the unmasked part between the original image and the generated image?\n\n3. In Eq.(10), why the conditional distribution is $p_\\theta(x_0^{img} |x_1, x_0^{txt} )$ instead of $p_\\theta(x_0^{img}|x_1)$? I think $p_\\theta(x_0|x_1) = p_\\theta(x_0^{img}|x_1) p_\\theta(x_0^{txt}|x_1)$, and therefore $L_0$ should be $-E_q [\\log p_\\theta(x_0^{img}|x_1) + \\log p_\\theta(x_0^{txt}|x_1)]$.\n\n4. In Eq.(12) and Eq.(13), the left part is unrelated to $x_0^{txt}$, but the right part is related to $x_0^{txt}$. Is this a typo?\n\n5. This work uses a large number of parameters (600M) for relatively small dataset (MSCOCO), but there is still a performance gap between it and the compared method, e.g., FID=25.11 v.s. FID=10.5 on MSCOCO. Perhaps there is still an improvement space on the architecture.\n\nSummary:\n\nOverall, I think this paper is interesting, but it can still be improved by adding more details and fixing some confusions.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clear. While the technique exists, the task is interesting.",
            "summary_of_the_review": "See Strength and Weaknesses.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_1mAH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_1mAH"
        ]
    },
    {
        "id": "50QJEuYil1",
        "original": null,
        "number": 2,
        "cdate": 1666557283045,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666557283045,
        "tmdate": 1666557283045,
        "tddate": null,
        "forum": "8JqINxA-2a",
        "replyto": "8JqINxA-2a",
        "invitation": "ICLR.cc/2023/Conference/Paper1852/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a method for image-text generation that can perform conditional and unconditional image and text synthesis from either blank inputs or by partially masking any of them. The method includes a two pathway for image and text encoding-decoding with a unified diffusion path and matrix, that works with the fusion of the visual and text tokens. The paper is accompanied with code. The experiments include text-to-image synthesis, image captioning, and image and text replacement, which can be accomplished by the very same model. The results are competitive w.r.t. state of the art works, setting a promising direction towards multimodal generation.",
            "strength_and_weaknesses": "The paper is conceptually easy to follow and understand, and has the merit to propose a unified model for both image to text, and text to image synthesis as well as completion. The paper is reproducible and the computational requirements to achieve good results are not excessive.\n\nThe quantitative results do not seem to surpass previous works on either of the modalities, which might at first set some concerns with the proposed work. However, it is worth noting that the metrics do not fully reflect which model is better, and the visual results are really on par with existing methods (For example the FID score is a metric that below some level is indeed indicating a dataset replication). It is also worth remarking the merit of the method to achieve competitive results in both tasks with the same model. In addition, the multi-modal completion feature is a desirable property of the method which to my knowledge has not been proposed yet in the literature. \n\nWhile the paper is technically sound and easy to follow, it reads quite weak; the writing and presentation need a good effort before the camera ready to improve the paper\u2019s readability. There are several typos and while the mathematical derivation is interesting, some other details might be more attractive to the audience that are left to the supplementary material, such as the experimental details. I would thus suggest the authors to perform a thorough proofreading. \n\nSome well renowned methods are left out of the comparisons, such as DALL-E [Ramesh et al. \u201cZero-Shot Text-to-Image Generation\u201d], for the task of image generation. While DALL-E is a very big model, showing how the model\u2019s capacity can affect performance would also boost the paper\u2019s contributions. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper needs some improvement in regards to clarity.\n\nThe paper is novel and the method is of broad interest.\n\nThe paper is accompanied by code.",
            "summary_of_the_review": "The paper has the merits to be published at ICLR, and it has the potential to draw the audience's eye. However, some extra efforts are needed towards improving the quality of the manuscript.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_tuqJ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_tuqJ"
        ]
    },
    {
        "id": "diVt9gzVNP",
        "original": null,
        "number": 3,
        "cdate": 1666594893276,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594893276,
        "tmdate": 1669650644483,
        "tddate": null,
        "forum": "8JqINxA-2a",
        "replyto": "8JqINxA-2a",
        "invitation": "ICLR.cc/2023/Conference/Paper1852/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a unified discrete diffusion model for simultaneous vision-language generation. The proposed model extends multinomial diffusion to the joint text-image tokens with a transition matrix to prevent transiting among modalities. A mutual attention module is proposed to better capture the inter-modal linkages.",
            "strength_and_weaknesses": "## strengths\n- The proposed method is simple and shows promising results.\n- The proposed unified framework supports various tasks including cross-modal, text-to-image, and image-to-text generation.\n## weakness\n- The paper is not clearly written. For example, I cannot fully understand the proposed mutual attention from Fig. 3. It would be good if the authors could provide e.g. mathematical definitions.\n- One of the main contributions: the unified transition matrix seems a trivial and intuitive extension of VQ-Diffusion or multinomial diffusion to the concatenated text-image tokens. The novelty is slightly limited.\n- Autoregressive models like DALLE also model the joint distribution of text and image. How does the proposed model compare to autoregressive models?\n- From the results shown in Tables 1 & 2, the proposed method does not perform the best in any of the entries. Could the author provide more discussion on the results? For example in Table 1, the T2I FID is much higher than VQ-Diffusion, which is a very close baseline. Any intuition why joint generative vision-language training is not helpful here?",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is not clearly written, e.g. the introduction of mutual attention.\n- Novelty is a little limited given that the unified transition matrix is a trivial and intuitive extension of multinomial diffusion to the joint text-image sequence.\n- Code is provided in Supp.",
            "summary_of_the_review": "My rating is mainly based on the novelty and clarity of the paper. I might amend my score if my major concerns are addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_bEZ6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_bEZ6"
        ]
    },
    {
        "id": "Cs7k4PdRMlG",
        "original": null,
        "number": 4,
        "cdate": 1666759688393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666759688393,
        "tmdate": 1670800989683,
        "tddate": null,
        "forum": "8JqINxA-2a",
        "replyto": "8JqINxA-2a",
        "invitation": "ICLR.cc/2023/Conference/Paper1852/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a multimodal diffusion model, that can perform text-conditioned, image-conditioned and text-and-image-conditioned generation. They utilize discrete diffusion, based on VQ-diffusion [Gu et al, 2022], with the images encoded using discrete VAE and the text encoded using byte-pair encoding. The  discrete diffusion process is designed in the form a unified text-image Markov transition matrix to estimate the joint distribution of language and image. To implement this in practice, they propose a mutual-attention transformer with a fused embedding, which enables the unified diffusion process. They perform experiments for image generation and text-image generation using the CUB and MSCOCO datasets and compare their method with baselines using FID and IS scores. The proposed method is worse than the baseline VQ-diffusion on both datasets, but obtains competitive FID scores. Image caption generation is also evaluated on MSCOCO, where the proposed approach obtains competitive (but not state-of-the-art) results. However, uniquely, the method is able to simultaneously generate both text-and-image and this capability is evaluated by comparing the CLIP similarity between generated image-caption pairs with CLIP scores of eval set pairs, where the proposed method shows slightly better results. ",
            "strength_and_weaknesses": "Strengths:\n1. The paper is clearly written and the method is described in detail. The writing, along with the code, makes the paper reproducible.\n2. The method is evaluated thoroughly for image generation, text generation and joint image-text generation using both qualitative and quantitative experiments and compared with baseline methods. \n\n\nComments:\n1. The proposed method is basically an extension of VQ-diffusion for the multimodal case, with a new mutual-attention transformer with a fused embedding proposed to enable multimodal generation. While this extension is not trivial, it is fairly intuitive.  Can the authors highlight the main novelty and technical contributions of the paper? \n2. The generation results for individual modalities are competitive, but worse than VQ-diffusion, indicating that the joint distribution estimation leads to degradation in performance for T2I generation.  Can the authors comment on this?\n3. The multimodal generation results are significantly worse than the autoregressive approach (OFA). While its a very different approach, it would be good for authors to comment on the comparison. \n",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "The paper proposes an extension of VQ-diffusion for multimodal discrete diffusion generative model. The results are reasonably good and the novelty of the approach is not somewhat limited. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_cr8c"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1852/Reviewer_cr8c"
        ]
    }
]