[
    {
        "id": "YR73n3sdW_",
        "original": null,
        "number": 1,
        "cdate": 1666559868932,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666559868932,
        "tmdate": 1666682686532,
        "tddate": null,
        "forum": "wNUgn1n6esQ",
        "replyto": "wNUgn1n6esQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5658/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the problem of reward-free reinforcement learning under (possibly mismatching) cost constraints in both the exploration phase and the planning phase. Crucially, the paper shows how to exploit the knowledge of a safe baseline policy to achieve zero constraints violations while matching the sample complexity of unconstrained methods in both tabular and low-rank MDPs. The proposed method, called SWEET, is based on three novel components: A notion of (eps,t)-greedy policy that allows for some exploration over actions without deviating too much from a reference policy, an approximate error function that accounts for the uncertainty on the estimated model, and an empirical safe policy set built upon the approximation error function.",
            "strength_and_weaknesses": "*Strengths*\n- (Relevance and value of the contribution) The main result of this paper, i.e., that reward-free RL under safe constraints is not statistically harder than the unconstrained version is really interesting;\n- (Originality of the method) The proposed method is based on several original components, and, to the best of my knowledge, is not an incremental extension of previous approaches;\n- (Thorough analysis) The paper provides sample complexity results that match the best-known rates for unconstrained reward-free RL in both tabular MDPs and low-rank MDPs.\n\n*Weaknesses*\n- (Computational complexity) The paper does not discuss the computational tractability of the Algorithm 1;\n- (Technical novelty) The technical novelty of the analysis is not clearly discussed in the main paper, although the originality of the algorithm suggests that uncommon techniques have been used;\n- (Strength of the assumptions) The paper assumes to have access to a safe baseline policy and the presence of a positive safety margin.\n\n*Questions*\n\n(Complexity) Can the authors discuss the computational complexity of Algorithm 1? Especially, the optimization problem in line 6 can be solved efficiently?\n\n(Technical novelty) Can the authors describe the main technical innovations of their analysis?\n\n(Assumptions) Although the assumptions of knowing a safe baseline policy and the positive safety gap are mostly reasonable, it would be great to have a discussion on whether they can be overcome and to what extent. E.g., do the authors believe that the baseline policy assumption can be avoided through a weaker notion of zero constraint violations (such as constraints violations decrease at a fast rate)?",
            "clarity,_quality,_novelty_and_reproducibility": "To the best of my knowledge, the paper is novel and the provided results are of great value. However, some aspects of the framework could have been explained with more details (see comments above). Since I did not carefully checked the analysis, it is hard for me to judge its reproducibility.",
            "summary_of_the_review": "This looks like a great work to me. Especially, it stands out for the relevance of the main take, i.e., that reward-free RL exploration under safety constraints is not harder than unconstrained reward-free RL (at least when we have access to a baseline policy), for the originality of the methodology and (supposedly) the analysis, and for the completeness of the results, which include both tabular and low-rank MDPs. Thus, I am currently providing a fully positive evaluation. However, I could not check the analysis with such a short reviewing window, and I reserve to update my evaluation when I will have the chance to look into the technical details.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_gkGM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_gkGM"
        ]
    },
    {
        "id": "jGKLSUdxcVW",
        "original": null,
        "number": 2,
        "cdate": 1666631396532,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666631396532,
        "tmdate": 1669056615142,
        "tddate": null,
        "forum": "wNUgn1n6esQ",
        "replyto": "wNUgn1n6esQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5658/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the reward-free reinforcement learning setting with additional safety constraint. Assuming the access to a safe baseline policy, the SWEET algorithm proposed in this paper meets the safety requirement throughout the learning process with high probability. After the reward-free exploration phase, the algorithm can output a near-optimal policy for any  reward function and (new) safety constraint. The SWEET algorithm achieves tight sample complexity guarantees on both tabular and linear MDPs.",
            "strength_and_weaknesses": "Strengths:\n\n- This paper proves tight sample complexity bounds for the safe RF-RL setting. This bound matches RF-RL without safety guarantees up to some constant factors. \n- The algorithmic design principle (Alg. 1) is applicable to both tabular and linear MDPs.\n\nWeaknesses:\n\n- I have concerns about the paper's soundness (details in below).\n- I don\u2019t think the autonomous driving example mentioned in the introduction is a good way to motivate this paper. In the exploration phase of the SWEET algorithm, the safe policy set is essentially a mixture between the safe baseline policy and some rather random policy, so that the average constraint is smaller than the threshold.  In the autonomous driving example, this policy could be crashing the car with some probability and using the policy baseline otherwise, which doesn\u2019t sound the right type of guarantees.\n",
            "clarity,_quality,_novelty_and_reproducibility": "I couldn\u2019t verify the correctness of Theorem 1. If I understand correctly, there are at least some unstated assumptions regarding the safety constraint $(c^*, \\tau^*)$:\n\n- In the paragraph after Eq. (16) (page 16), $\\gamma$ is defined by $(\\Delta_{c^*}-3\\mathfrak{U})/\\Delta_{c^*}$. But to make $\\pi^\\gamma$ well-defined, $\\gamma$ must belongs to $[0,1]$. It\u2019s unclear to me how to establish such a relationship. In particular, could the authors elaborate why $\\Delta_{c^*}>0$ and $\\Delta_{c^*}\\ge 3\\mathfrak{U}$? It seems to me that the choice of $c^\\star$ and $\\tau^\\star$ is arbitrary.\n- Fundamentally, I don\u2019t think Theorem 1 can be true without any assumptions on $(c^\\star,\\tau^\\star)$. Imagine that $\\tau^\\star=\\min_\\pi V^\\pi_{P^\\star,c^\\star}$. Then finding an $(c^\\star,\\tau^\\star)$-safe policy is equivalent to finding the exact optimal policy under the utility $c^\\star$, which is impossible under finite samples.\n\nSome informal statements in this paper are not well-supported:  \n\n- In related work section, this paper claims \u201cChen et al. (2022) studies RF-RL with more general function approximation, but their results cannot recover the upper bound we have for the low-rank MDPs\u201d with no further discussions. Could the author elaborate?\n- In the introduction, this paper claims \u201cRemarkably, the sample complexities under both algorithms match or even outperform the state of the art of their constraint-free counterparts up to some constant factors, proving that safety constraint incurs nearly no additional sample complexity for RF-RL.\u201d While the first part of this statement is supported by Theorem 1, I don\u2019t see how the second part is supported --- the safety constraint always introduces an extra multiplicative factor $1/\\Delta_{min}$ to the sample complexity, which is exactly the additional cost of having a safety constraint.\n\n",
            "summary_of_the_review": "My main concerns are the correctness/clarify of this paper. As a result, I recommend a rejection for now, but I will be happy to raise my score if my concerns are addressed.\n\n===== after rebuttal ====\n\nThe authors addressed my concerns regarding the correctness of the theorems. Therefore I'll raise my score accordingly.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_5h43"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_5h43"
        ]
    },
    {
        "id": "Bo6HZziyghC",
        "original": null,
        "number": 3,
        "cdate": 1666635651970,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666635651970,
        "tmdate": 1670026598991,
        "tddate": null,
        "forum": "wNUgn1n6esQ",
        "replyto": "wNUgn1n6esQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5658/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the reward-free RL in general MDP settings with safety constrain. The author then extend their analysis to tabular MDP and low-rank MDP for a detailed result.",
            "strength_and_weaknesses": "### Strength:\n\n- Defining the convexity in uncertainty quantification is interesting.\n\n### Weakness:\n\n- I'm not sure if Assumption 1 will hold for **any given constraint $(c, \\tau)$. For example, if $\\tau = \\min_{\\pi} V_{P^\\*, c}^\\pi$, then $\\Delta(c, \\tau) = 0$. \n- Following the previous question, does the author want to express that $\\tau$ is a selected parameter in the exploration phase, then it's not clear how to select the $\\tau$\n- In the stopping condition we need $T \\ge \\Delta(c, \\tau)$. Does that mean we need some prior knowledge of the magnitude of the safety margin?\n",
            "clarity,_quality,_novelty_and_reproducibility": "- This paper is well-written and easy to follow. studying safety in reward-free exploration is interesting and useful\n- There are some concerns I mentioned in the weakness where the author needs to further clarify\n- In Definition 6, I would suggest the author to includes $\\pi'$ in the notation $\\pi^\\gamma$, since $\\pi^\\gamma = \\gamma \\pi + (1 - \\gamma) \\pi'$, informally speaking.",
            "summary_of_the_review": "This paper provides some new thoughts on reward-free RL, especially on safety concerns. The methodology proposed by the authors is quite interesting. However, It has some issues regarding the Assumptions, parameter selection, and knowledge about the safety margin. Therefore I would suggest a marginal rejection but would like to change my score after the discussion if the authors well clarify my concerns.\n\n\n***\nI've carefully read the author's response and other reviews. The authors' response addressed some of my concerns thus I would raise my score to 6",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_3Lc4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_3Lc4"
        ]
    },
    {
        "id": "2aYH5WT1_L-",
        "original": null,
        "number": 4,
        "cdate": 1666642357337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642357337,
        "tmdate": 1670265226949,
        "tddate": null,
        "forum": "wNUgn1n6esQ",
        "replyto": "wNUgn1n6esQ",
        "invitation": "ICLR.cc/2023/Conference/Paper5658/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies reward-free RL with safe exploration. The authors consider the scenario where a\nsafe baseline policy is given beforehand, propose a unified algorithmic framework called SWEET, and instantiate the SWEET framework to the tabular and low-rank MDP settings. Their algorithms utilize the concavity and continuity of the truncated value functions to achieve zero constraint violation with high probability. The sample complexities of their algorithms match or outperform their constraint-free counterparts, which shows that safety constraint hardly increases the sample complexity for reward-free RL.\n",
            "strength_and_weaknesses": "**Strengths:**\n\n1 The proposed algorithmic framework SWEET is general, and can be applied to the tabular and the low-rank MDP settings to obtain start-of-the-art sample complexity guarantees. \n\n**Weaknesses:**\n\n1 The ideas of safe exploration based on known baseline policy, approximation error function and estimated safe policy set are not novel. There have been several works that study bandit/RL with safe/conservative exploration.  \n\nFor example,  \n[1] Kazerouni, Abbas, et al. \"Conservative contextual linear bandits.\" Advances in Neural Information Processing Systems 30 (2017).  \n[2] Yang, Yunchang, et al. \"A unified framework for conservative exploration.\" arXiv preprint arXiv:2106.11692 (2021).  \n[3] Amani, Sanae, Christos Thrampoulidis, and Lin Yang. \"Safe reinforcement learning with linear function approximation.\" International Conference on Machine Learning. PMLR, 2021.  \n\nIt is well known that in the regret minimization setting, the requirement of safe exploration will only incur an additional $O(1)$ regret (independent of the number of timesteps $T$ played in the RL game). Hence, it is also not surprising that safe exploration will not incur additional sample complexity for reward-free RL.\n\nThe technical novelties of the proposed techniques, e.g., approximation error function and estimated safe policy set, are unclear to me. In addition, the results of this paper reply on known baseline policy $\\pi_0$ and safety margin $\\kappa$. While a known baseline policy is widely used in the safe bandit/RL literature, the algorithm design and results in this paper heavily depend on the prior knowledge of safety margin $\\kappa$, and are expected.\n\n2 For the low-rank MDP setting, the authors obtain a better sample complexity result under the safe exploration constraint than all existing constraint-free algorithms (without proposing new techniques for low-rank MDP). This result is surprising. Can the authors discuss what techniques used in their algorithm design/analysis enable them to achieve a tighter sample complexity?\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is clearly written and well executed. In my opinion, the idea behind the safe exploration is not novel. The theoretical results in this paper are supported by complete proofs.",
            "summary_of_the_review": "This paper is well executed and gives results for both tabular and low-rank MDP settings. My concern mainly falls on the novelty of the used techniques, e.g., approximation error function and estimated safe policy set, and the reliance on the prior knowledge of safety margin $\\kappa$. In my opinion, the finding of \u201csafe exploration incurs nearly no additional sample complexity for reward-free RL\u201d is not surprising and similar conclusion has been obtained in the regret minimization bandit/RL. Due to the above reasons, I give borderline rejection.\n\nIf the authors can address my concerns on technical novelty (what techniques/findings are novel and unique for reward-free RL compared to existing literature of bandit/RL with safe exploration), I am ready to raise my score.\n\n====After reading the authors' rebuttal====\n\nThank the authors for their response.\n\nI appreciate that the authors provide a new method which does not need prior knowledge on the safety margin $\\kappa$, which relieves my concern on prior knowledge of $\\kappa$.\n\nFor the novelty, I am not sure if the reward-free-exploration safe RL brings significant unique challenges than existing safe/conservative RL. \nIn the rebuttal, the authors explained that, the reward-free-exploration safe RL needs to both ensure safety for a given constraint during exploration and find a near-optimal policy *for arbitrary reward and constraint* during planning. \nHowever, since the algorithm can first estimate a sufficiently accurate transition model during exploration and then do planning, I am not sure if finding a near-optimal policy *for arbitrary reward and constraint* will pose significant unique challenges (at least for the tabular setting). \nTherefore, I think the finding that \"safe exploration incurs nearly no additional sample complexity for reward-free RL\" is not surprising, since we already know that safe exploration incurs nearly no additional sample complexity for standard RL.\n\nFor the proposed technique, while the authors explained that the truncated value function is critical to their analysis, I did not get the significance and novelty of this technique very much. In my understanding, I think that SWEET does clipping on the value function (instead of the Q-value function) with the universal reward upper bound $1$, while RepUCB does not do clipping on value functions, and thus, SWEET improves the dependency of $H$. This technique does not look very significant and novel to me. The authors may consider to highlight more on how this truncated value function preserves concavity and continuity and brings large advantages in analysis compared to the conventional clipping technique in their revision.\n\n====After reading the authors' additional explanations and discussions=====\n\nThe authors' additional explanations relieve my concerns on the unique challenges of reward-free safe RL. In addition, I appreciate the authors' efforts in designing a new algorithmic strategy which does not require the prior knowledge of $\\kappa$. So I raised my score from 5 to 6.\n\nI suggest the authors to discuss the unique challenges of reward-free safe RL in a deeper analysis level (instead of just mentioning the differences of problem settings), and elaborate how the concavity of the truncated value function helps to handle these challenges more specifically in their revision. For the current version, it is not easy for readers (at least, to me) to quickly understand the challenges of reward-free safe RL and the novelty of the truncated value function compared to conventional clipping technique.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_kzdP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5658/Reviewer_kzdP"
        ]
    }
]