[
    {
        "id": "OyF7GEuo4Z1",
        "original": null,
        "number": 1,
        "cdate": 1666121062497,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666121062497,
        "tmdate": 1668802296631,
        "tddate": null,
        "forum": "4t9q35BxGr",
        "replyto": "4t9q35BxGr",
        "invitation": "ICLR.cc/2023/Conference/Paper3925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the negative impacts of sparse features as a result of Linf adversarial training.  The authors find that this sparsity causes the Linf robust models to be more vulnerable to small occlusions and smaller magnitudes of random Gaussian noise in comparison to standard trained models.",
            "strength_and_weaknesses": "Strengths:\nI think the experiments are interesting and highlight a weakness of Linf adversarial training well.\n\nWeaknesses:\nPaper clarity can be greatly improved, for instance the authors mention \"inequality phenomenon\" early in the paper, but it isn't clear what this means until after the authors describe using the Gini index. The authors should provide some short explanation of what they mean by \"inequality phenomenon\" early on in the text to improve clarity.  Some other parts I was confused about are \n- at the beginning of Section 2.3 what is meant by \"weak signals\"\n- in Section 3.1 \"In detail, we treat the input image...\" seems to be 2 separate sentences? or the second half of the sentence \"feature attribution methods aim to assign...\" should be deleted?\n- In section 3.1, in the definition of regional inequality, what exactly does a region mean?  Is it a set of pixels in the input space?  How are these regions chosen?\n- In 3.2.1, the text for the description of noise type 1 is confusing.\n\nWhat is the motivation behind using Gini index?  How much more informative is Gini index compared to just measuring sparsity?  Does sparsity not imply that Gini index will increase?\n\nDoes this inequality phenomenon arise due to more sparse features or is a result of something more specific to Linf adversarial training.  How successful are these occlusion and noise attacks on models trained with regularization for sparsity?\n\nAll visualizations are on ImageNet.  It would be nice to see some visualizations for CIFAR-10 as well.\n\nCould the authors provide clean and robust accuracies of models evaluated?\n\nTo confirm, the \"error rate\" is more of a measure of stability rather than error right?  If the model misclassifies the clean image and also misclassifies the perturbed image as the same class, then there is no error?  In that case, I think that the metric should be renamed to something involving \"stability\" rather than \"error\" to avoid confusion.  Another possible metric is to filter out the images that both the clean model and adversarially trained model are incorrect on and then measure error.  This would allow us to understand the frequency that a correctly classified image changes class which I think is a little more informative.",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, I think that the paper's clarity can be greatly improved (see Weaknesses).  I think that overall the experiments are well-designed and novel and results are significant, but I think that the use of Gini index can be motivated a little better (as opposed to just using sparsity as in previous works).  I also think that additional baselines outside of just standard trained models should be used: specifically models trained with sparsity regularization should be considered.  Additionally, comparisons to L2 adversarially trained models can be interesting as well.",
            "summary_of_the_review": "Overall, I think this is a very interesting work that investigates vulnerabilities that arise from Linf adversarial training.  The authors demonstrate these vulnerabilities by designing attacks against Linf adversarially trained models.  However, I think that there can be some improvements especially with regards to the clarity of the writing.  I also think that the authors should consider other baselines (specifically models trained with sparsity regularization) instead of just comparing to standard training.  This would allow us to better understand whether sparse features are the cause of these vulnerabilities or whether the observed vulnerabilities are unique to Linf adversarially trained models.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_1FTk"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_1FTk"
        ]
    },
    {
        "id": "rzQ16DTYBB",
        "original": null,
        "number": 2,
        "cdate": 1666577797667,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666577797667,
        "tmdate": 1666577797667,
        "tddate": null,
        "forum": "4t9q35BxGr",
        "replyto": "4t9q35BxGr",
        "invitation": "ICLR.cc/2023/Conference/Paper3925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "Overall, the paper provides a novel and intuitive understanding about adversarial trained models. The paper identifies a new phenomenon and points out its unrealized threats which will inspire new related research.\nCorresponding index (Gini coeffiencient) is designed to illustrate such phenomenon.\n",
            "strength_and_weaknesses": "Strength:\n1. The strength of this paper is in the formulation of \u201cinequality phenomenon\u201d occurring during l_{infty} adversarial training. The paper provides an intuitive understanding about adversarial trained model\u2019s behavior, and utilize a novel index to quantify such phenomenon. \n2. Using Gini index is an extra strength, the author also extends original Gini index to regional and global aspects which makes technical contribution. The proposed index is strongly correlate with the inequality phenomenon.\n3. Based on the observation of the phenomenon, the paper proposes several simple attack methods. The work identify unrealized threats brought by such inequality phenomena that $l_{\\infty}$-adversarially trained models are much more vulnerable than standard trained models under inductive occlusion or noise. It shows unrealized vulnerability of adversarial training, which can inspire new research in this area.\n \nWeakness:\n1. Current attack methods create perceptible occlusions on resultant images, it could be good to see more imperceptible occlusions for performing attacks.\n2. Writing can be improved at times. The paper could be further polished, and some typos exist.\n3. More visualization results are preffered.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper identified a novel phenomenon that do not exist in prior works. Description of the method is clear and easy to follow, the method can be reproduced.",
            "summary_of_the_review": "The paper undertakes an original approach to studying the inequality phenomenon of l_{infty} adversarial training and find new adversarial vulnerability. The phenomenon about adversarial training is novel, empirically insightful, and potentially will inspire further work. Overall, this new perspective is novel to me and I would tend to accept this paper. If the author can address my concerns. I would be more convinced.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_Mxrb"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_Mxrb"
        ]
    },
    {
        "id": "oWwvZSKQtD",
        "original": null,
        "number": 3,
        "cdate": 1666596785674,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666596785674,
        "tmdate": 1669624530989,
        "tddate": null,
        "forum": "4t9q35BxGr",
        "replyto": "4t9q35BxGr",
        "invitation": "ICLR.cc/2023/Conference/Paper3925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work highlights the limitations of L-inf adversarial training - models trained using Linf-AT (specifically at large perturbation bounds) tend to rely heavily on very few features, when compared to normally trained models. This makes them vulnerable to attacks that perturb very few pixels, or occlusion based attacks, which are indeed more common in the real world. The authors propose two attacks that replace important features with either noise or occlusions to show the vulnerability of Linf adversarially trained models. ",
            "strength_and_weaknesses": "Strengths -\n\n- The finding that Linf adversarial training makes models more vulnerable to noise and occlusion based attacks that perturb a few pixels, is novel and interesting. \n- The proposed attacks show the difference between Linf-AT models and standard trained models. \n\n\nWeaknesses -\n\n- There are several grammatical errors that severely impact readability.\n- Prior works have shown that gradients of adversarially trained models are perceptually aligned [1]. It is not clear why the feature attribution maps in Fig.1 are not perceptually aligned. \n- If the perturbation radius is controlled such that only the non-robust features are suppressed during adversarial training, even though the model would be more vulnerable to occlusion and perturbation attacks when compared to standard training, it would actually be more aligned to human perception. For example, in a waterbirds vs. landbirds classification, a normally trained model may correctly classify an image even if important features of the bird are completely occluded, because it considers background and texture for classification. Whereas, an adversarially trained model may rely more on features such as beak, eyes and shape of the bird, making them more vulnerable to occlusion attacks. Despite this increased vulnerability, Linf-AT model is more aligned to human perception and hence is more preferred.\n- There seems to be a typo in the paragraph above Eq.4. Sum of N least important features being similar to f(x) is considered as the criteria for inequality. This should have probably been the sum of N most important features I suppose?\n- Could the authors show how the proposed attacks generated from an adversarially trained model perform on standard models, and vice versa?\n- Could the authors confirm whether the standard trained models and adversarially trained models being compared in the paper use the same augmentations? This is important in light of the cutout augmentation results in A.3.\n- It is important to show what happens in case of other adversarial training regimes such as L2 norm adversarial training. \n\n\n\n[1] Tsipras et al., A. Robustness may be at odds with accuracy.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - Below average, due to grammatical errors \nQuality and novelty - The finding in the paper is novel, although it is not clear whether these are fundamental differences that conflict with the robustness requirements. \nReproducibility - Clear ",
            "summary_of_the_review": "Although the paper makes some novel and insightful observations, I believe it is not yet ready for publication in terms of clarity and completeness of experiments to draw conclusions - details are mentioned in the weaknesses section. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_AXew"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_AXew"
        ]
    },
    {
        "id": "vxvgBh8TKRh",
        "original": null,
        "number": 4,
        "cdate": 1666623687404,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623687404,
        "tmdate": 1666623687404,
        "tddate": null,
        "forum": "4t9q35BxGr",
        "replyto": "4t9q35BxGr",
        "invitation": "ICLR.cc/2023/Conference/Paper3925/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides some insights on the vulnerability of l_{infty} adversarial trained model. The paper identifies inequality phenomenon occurs during l_{infty} adversarial training which is quantified by Gini index. To show the later, the paper proposes two methods: inductive noise and occlusion to demonstrate the vulnerability of l_{infty} adversarial trained model caused by such phenomenon. This paper provides a novel perspective and sheds light on the practicality of l_{infty} adversarial trained model.",
            "strength_and_weaknesses": "Pros:\n- A very interesting observation, inequality phenomenon occurring during l_{infty} adversarial training. \n- Provides a new perspective on feature representation of l_{infty} adversarial trained model, which will motivate future research.\n- Proposes regional and global Gini to evaluate the inequality phenomenon quantitively that providing an intuitive explanation. \n- Experiments show l_{infty} adversarial trained model is even more fragile than standard trained model under some scenarios, the results are interesting for me.\n \nCons:\n- Though this is a very relevant and timely work related to reliability of l_{infty} adversarial training, it would help if the authors could provide some effective solution to release such phenomenon or other suggestion.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper introduces a novel phenomenon about l_{infty} adversarial training. This paper is well written and well organized, the method is easy to understand and reproduced.",
            "summary_of_the_review": "Interesting phenomenon, reasonable metric (Gini), well-motivated method that demonstrate the vulnerability of l_{infty} adversarial trained model.\nThis is a good paper, with some aspects of the presentation that should be improved.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_yDNt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3925/Reviewer_yDNt"
        ]
    }
]