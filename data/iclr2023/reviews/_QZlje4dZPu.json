[
    {
        "id": "mtbU7iKwpk7",
        "original": null,
        "number": 1,
        "cdate": 1666488593698,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666488593698,
        "tmdate": 1666488593698,
        "tddate": null,
        "forum": "_QZlje4dZPu",
        "replyto": "_QZlje4dZPu",
        "invitation": "ICLR.cc/2023/Conference/Paper534/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a new method for regression problems based on Supervised Contrastive Learning approach(SupCon) designed for classification tasks. The paper argues that existing regression-based approaches, don't learn embeddings suited for regression tasks, whereby the distance in embedding space is equivalent to the distance in the label space. They empirically show this claim by a UMAP plot for a regression task, whereby the embeddings learned by baseline L1 approaches do not capture the intrinsic ordered relationship between the samples. The paper argues that learning a regression-suited embedding space, will allow the models to be robust to corruption and also generalize to unseen targets.\nThus, the authors propose an approach similar to SupCon for regression tasks which they call SupCR(Supervised Contrastive Regression). In SupCon the positive samples for contrastive learning are not only the augmented version of the query but also those that belong to the same class as the query. In SupCR, the authors first consider the closest sample as the positive, and all other samples whose label distance is larger than the distance between the query and positive are treated as the negative. This is then iterated for the second closest, third closest samples and so on. Once the embeddings are learned in the contrastive way, the authors either fine-tune the entire network or only train a linear layer using either L1/MSE/Huber loss. \n\nThe authors show that their proposed approach achieves state-of-the-art results over several different datasets. Additionally, the proposed method shows robustness to corruption, better transfer learning abilities, and generalization to unseen targets.\nThe authors also provide a theoretical justification for how their proposed optimization will make the feature embeddings ordered according to order in the label space. ",
            "strength_and_weaknesses": "Strengths - \n1. The paper is well written and easy to follow along.\n2. The authors do an extensive study across different approaches and settings, such as which similarity function to use, whether to use projection head or now and others.\n3. Along with better task performance, Improved robustness to corruption and generalization to unseen targets shows the goodness of the approach.\n\nWeakness - \n\n1. For the contrastive learning part, the paper iteratively considers the first closest sample, then the second closest and so on for their positive samples. It is unclear if we really need to consider this iteration till 2N steps. What happens if we only consider the first closest of the first K closest samples as positive. This ablation study would be important to justify the proposed approach, but is missing from the paper.\n\n2. It's unclear how is SupCon used as a baseline in the paper, when the task is a regression task. How are the authors constructing positive samples for the SupCon approach?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written with extensive experiments. The originality of the work is limited as it is adapting SupCon approach for regression tasks.",
            "summary_of_the_review": "While the paper does extensive study and provides a good justification of the proposed approach via state-of-the-art results, the novelty of the work is a bit limited as it is mostly just adapting SupCon for regression tasks. I also have a few more concerns regarding the further justification of the proposed approach and the baselines. \nI am leaning towards weak accept but am willing to update my ratings if my concerns are addressed.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper534/Reviewer_7jug"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper534/Reviewer_7jug"
        ]
    },
    {
        "id": "SKASMo9sYf7",
        "original": null,
        "number": 2,
        "cdate": 1666657661638,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666657661638,
        "tmdate": 1670619103461,
        "tddate": null,
        "forum": "_QZlje4dZPu",
        "replyto": "_QZlje4dZPu",
        "invitation": "ICLR.cc/2023/Conference/Paper534/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper proposes a new variant performing supervised contrastive learning with continuous labels. Improvements on four image-based and one EEG regression tasks are demonstrated when using the learned feature space instead of training various baseline algorithms from scratch. The paper also shows improvements in terms of robustness and transfer learning performance, as well as ablations on the impact of a projection head on the regression performance.",
            "strength_and_weaknesses": "**Strengths:**\n\n- The method is simple to implement, well evaluated and the results are laid out clearly.\n- The authors demonstrate improvements of their method both in terms of accuracy and robustness to image perturbations on a range of different evaluation tasks.\n- The method is supported by theory. *Disclaimer: While the theoretical results are plausible, I did not check the proofs yet. I might update this line after verifying them.*\n\n**Major Weaknesses:**\n\n- The models used for the regression tasks are relatively small. Do the results still hold when a more powerful backbone architecture (e.g., a ResNet50, or even more recent architectures) are used? What was the motivation for using a ResNet18?\n- The evaluation against the baselines is thorough, but I am missing a comparison to conceptually different methods. Even if the proposed method does not outperform them, it is good to provide this as a reference. Can the authors comment on the SOTA models for each of the five benchmarks? E.g. for Table 3, I find a reference result of [around 3-5 angular error](https://paperswithcode.com/sota/gaze-estimation-on-mpii-gaze), while authors report >5. I would be interested in learning about the state of the art of each of the benchmarks (I am not experienced with these datasets, so maybe the tables already contain the applicable SOTA).\n- Since all results are for visual data and the method uses augmentations for training the embedding, I am missing a comparison to state of the art representation learning methods like DINO (Caron et al., 2021) or also older methods such as SimCLR (Chen et al., 2020). Depending on the results, these do not necessarily need to go into the main paper, but it would be good to confirm that the boost in performance can actually be attributed to the supervised contrastive learning scheme, rather than the augmentations. The comparison could be executed within the proposed evaluation protocol (train the model on the data using the respective augmentations, then evaluate - this has been partially done in Table 7 for one of the datasets) and beyond it using the \"extra\" ImageNet-pretraining data (use the pre-trained checkpoint, evaluate the feature space). I am happy to further discuss the specifics of this experiment before the authors execute it to ensure that the protocol makes sense.\n- The positioning in the literature is not accurate, and the paper title is too broad. The authors already cite two papers (and I propose three additional ones below) that perform contrastive learning on continuous labels for the purpose of regresson tasks. Please see below (\"Novelty\").\n- The paper lacks an ethics statement given the use of data involving human subjects, and the nature of the tasks (e.g., age prediction). More details on the used datasets should be added. I did not flag the paper for ethics review yet because I assume that the original datasets included such a statement.\n\n**Minor Weaknesses**\n\n- Tables 1-4 (and also others) are lacking standard deviations / confidence intervals. For readers less familar with the datasets, it is hard to judge whether the performance improvement is signficant in all cases. If the authors want to save compute, at least running the best results for 3-5 seeds would be great.\n- Details on the grid search are missing (search range, number of samples per hyperparameter, ...)\n\n**Questions:**\n\n- Are all datasets taken from an established benchmark, or is the particular experimental protocol used in the paper a novel contribution? What was the rationale for using these specific prediction tasks for evaluation?\n- How did you find the optimal temperature parameter?",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity/Quality:**\n\n- The paper is clearly written.\n- Experimental evaluation is sound, results are laid out clearly.\n\n**Novelty:**\n\n- The particular loss function proposed in the work is novel, to the best of my knowledge.\n- The idea of \"supervised contrastive regression\" and its effectiveness has been demonstrated before and is not novel (which is not a weakness in itself - just the presentation thereof is currently problematic). The related work and intro should be adapted/expanded in this regard. A few examples of papers I am aware of beyond the already cited ones, there might be more:\n    - [Dufumier et al. (November 2021)](https://arxiv.org/pdf/2111.05643.pdf) discuss a supervised contrastive loss by re-weighting the loss terms based on a kernel function, which they originally proposed in [Dufumier et al., (September 2021)](https://link.springer.com/chapter/10.1007/978-3-030-87196-3_6) [(pre-print, June 2021)](https://arxiv.org/pdf/2106.08808.pdf).\n    - [Schneider et al. (April 2022)](https://arxiv.org/abs/2204.00673) propose an alternative formulation of \"supervised contrastive regression\" by changing the *positive* distribution according to a similarity function applied to the data with continuous, discrete or both continuous/discrete labels (from comparing the Figure 1 in this paper and Figure 1 in Schneider et al., the resulting embeddings also seem to be qualitatively quite similar).\n- In the light of this and the other works cited under Related Work already, the paper title is too broad. I find it necessary to specialize the title into something like \"Supervised contrastive regression by ranking negative samples\" or similar (i.e., name the particular contribution in the title). It should be made clear that the technical contribution of the paper lies in the specific choice of loss function rather than \"Supervised Contrastive Regression\" in general. In the intro, paragraphs 3 and 4 should also be updated accordingly, which currently incorrectly claim that this paper introduces the notion of contrastive learning with continuous labels.\n\n**Reproducibility:**\n\n- The experimental setup is clear. It would be good to clarify whether the authors will full open source the code for the experimental setup/evaluation.",
            "summary_of_the_review": "The paper introduces a new loss function for supervised contrastive learning with continuous labels. The experimental setup is interesting and well evaluated, and the paper is well written. The main limitation is the insufficient discussion of prior work on contrastive learning with continuous labels and positioning in the literature, along with possible improvements in the experiments. I made specific suggestions on how to improve these weaknesses above, and am willing to adapt my scores.\n\n---\n\n## Post Rebuttal comments (for now)\n\nThe authors did a good job at addressing my concerns. I raised the *Correctness* from 3 to 4, especially because a lot of insightful baseline experiments have been added. I also appreciate that the authors now better position the work in the existing literature, and refined both the paper title and the discussion of prior work.\n\nI raised my overall score from 5 to 6 (pending discussion with other reviewers).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper534/Reviewer_dbgX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper534/Reviewer_dbgX"
        ]
    },
    {
        "id": "DQV7cV9tPXc",
        "original": null,
        "number": 3,
        "cdate": 1666760948611,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666760948611,
        "tmdate": 1666760948611,
        "tddate": null,
        "forum": "_QZlje4dZPu",
        "replyto": "_QZlje4dZPu",
        "invitation": "ICLR.cc/2023/Conference/Paper534/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper attempts to incorporate Contrastive Learning into regression problems. The format of contrastive loss is not changed compared with SupCon, except for the definition of positive/negative pairs within the dataset. Namely, an anchor is first defined as a landmark and \n the sample with the closest label to the anchor's label is then defined as the corresponding positive sample and other remaining samples in a mini-batch are negative samples. optimizing LSupCR will make the feature embedding ordered according to the order in the label space. This definition of positivity/negativity is used to make the feature embedding ordered according to the order in the label space. \n",
            "strength_and_weaknesses": "The paper is written clearly and organized well. Contrastive learning for regression is an interesting topic where deep learning does not well addressed fully. Experiments show noticeable improvement over the baseline in terms of accuracy/loss.\nWeakness: 1. Please clarify how to determine the anchors in a batch; or the authors just enumerate all samples and assign each sample as an anchor?\n2. The proof needs more explanation step by step in the Appendix. The theorem guarantees that the order of similarity is consistent with the order of the similarity of labels. But it is not necessary that the order can preserve the continuity and the specific distance between labels. I am curious if the authors can make some more investigation into this concern.",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity needs some improvement, which can be seen in the following Summary Of The Review in detail.\nThe quality and novelty of the work are high, as it proves the error bound where the embedding guarantees that the order of similarity is consistent with the order of the similarity of labels. \n\n",
            "summary_of_the_review": "Here are some concerns that might need clarification:\n1. How do authors choose the anchors in a batch? Is the anchor defined randomly, or every sample has to be enumerated?\n2. How to obtain the last inequality from the second last inequality of Eq. (2)? Namely, how to derive Eq (3)?\n3. How to obtain Eq. (4)? There are many long equations that lack detailed explanation, which makes it hard to parse, although the proof is placed in the Appendix. Please add the necessary explanation of the derivation step by step. \n4. In SupCon and other related works, the feature embeddings are often normalized of length 1. In this case, the maximum distance between two samples is 2, which seems to enforce the delta in Def 1 to be greater than 0.5. This restriction may affect the feasible range of epsilon. I am wondering if the authors thought about this.\n5. In the experiments, the performance suggests that SUPCR is more robust to unforeseen data corruption. I am curious if that's because the data augmentation is used as much contrastive learning did? Would the authors want to discuss this?\n6. To obtain the smoothness and continuity of Figure 1 c, do the authors have to choose many anchors so that the embeddings can be less discrete like how SupCon shows in Figure b? So how is the training efficiency of the model?",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "na.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper534/Reviewer_hEGc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper534/Reviewer_hEGc"
        ]
    },
    {
        "id": "9h6TQCIfw4S",
        "original": null,
        "number": 4,
        "cdate": 1667372822165,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667372822165,
        "tmdate": 1667373390125,
        "tddate": null,
        "forum": "_QZlje4dZPu",
        "replyto": "_QZlje4dZPu",
        "invitation": "ICLR.cc/2023/Conference/Paper534/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a supervised contrastive regression loss for regression tasks, different from the supervised contrastive loss used for classification. The loss term is the same as NT-Xent where in their proposed approach, positive samples of an augmented view must fall within a radius or distance in the (continuous) label space, and negatives fall outside of this distance. Subsequently, they iteratively apply this term for the first closest positive sample, second, third and so on until the N-1 farthest sample. They use this to pre-train networks, which are further fine-tuned by freezing the model and training a single linear head, as in self-supervised methods.",
            "strength_and_weaknesses": "The overall idea somewhat makes sense, but some parts are lacking.\n- Most datasets selected are image datasets, though often regression tasks are on time-series/tabular data or other data modalities where creating \"augmented\" views drastically distorts the input where the label is no longer a valid one. It needs to be clarified how this would work for such datasets, where the relationship between augmenting/transforming the input and its correspondence on the target.\n- on the TUAB dataset, what augmentations were used? How is random-crop/random flip related to age? Is the same age used for both samples? In that case, augmented views of the sample will always have label distance 0, even though the augmentation may completely distort the input. \n- What happens if the target is cyclical? This hierarchical setup for contrasting iteratively will fail for such cases unless I misunderstand parts of the algorithm.\n- What happens to the contrastive term on the furthest sample? Is this dropped? I'm assuming it is.\n- Alternate baselines that pre-train or use an SSL loss as a regularizer along with the supervised L1 loss term should be added. For example, why is this method is compared with CORN?",
            "clarity,_quality,_novelty_and_reproducibility": "The text is clear, experiment details are lacking, e,g,\n- how was the grid search setup for both baselines and current proposed approach?\n- Does L1 refer to the same base model but trained solely with L1 loss?\n- How does the loss behave through out training? I'd presume mode collapse could easily happen for many cases where the augmentation distorts the input but maintains the same target y.\n",
            "summary_of_the_review": "Overall, the applicability of this approach on other datasets is not clear and although the results do seem promising it's hard to assess whether the gain is simply coming from pre-training. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper534/Reviewer_ZitN"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper534/Reviewer_ZitN"
        ]
    }
]