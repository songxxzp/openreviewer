[
    {
        "id": "0q6lzEVLwNx",
        "original": null,
        "number": 1,
        "cdate": 1666665393761,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665393761,
        "tmdate": 1670322471081,
        "tddate": null,
        "forum": "98p5x51L5af",
        "replyto": "98p5x51L5af",
        "invitation": "ICLR.cc/2023/Conference/Paper2854/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the \"reliability\" of large language models (LLM) with prompt engineering or in-context learning.  Authors consider four \"facets\" of reliability: (1) robustness to domain shift, (2) neutrality w.r.t. social biases (3) uncertainty calibration and (4) ability to update the learned knowledge. For each facet, authors study how LLM prompt-tuning / in-context learning compares against traditional fine-tuning of models such as RoBERTa, T5, DeBERTa, etc. For some facets, authors compare several prompting strategies in terms of their reliability. Aside from this general line of thought, the paper also contains a plethora of other results in each of the four respective \"facets\".",
            "strength_and_weaknesses": "### Strengths\n\n1. The paper studies multiple highly impactful issues; As LLM prompting is gaining new applications, it is important to understand whether or not we can \"trust\" it - and how to make it more trustworthy.\n2. Authors report a metric ton of evaluations. If the paper's worth was measured by the \"total weight\" of all contributions, this one would be among the best.\n3. Authors make considerable effort to make the paper easier to follow. The main sections (\"facets\") follow a consistent structure. Each has a short summary at the end.\n4. The paper is well-written and reasonably \"tidy\" -- especially if we take into account the density of information.\n5. Authors clearly state the limitations of their work, both in the main paper and in the dedicated limitations section.\n\n### Weaknesses\n\nIn general, it feels like the paper attempts to squeeze too much information into a limited space. Each of the four \"facets\" is an established research area with its own complexity, caveats, and prior work. It feels like the paper tries to be both an overview of these facets *and* an independent analysis of each one. In each specific section (facet), the paper has area-specific flaws: inconsistent evaluation, incomplete ablation, missing some task-specific caveats\n\n\n1. missing comparisons and related work in individual facets, e.g. GPT-3 social / ethical issues [1,2,3], GPT-3 uncertainty[4], GPT-3 domain adaptation[5], knowledge editing[6], retrieval-augmented question answering [7, 8].\n   * for instance, in Section 5 (knowledge updating), it is unclear how the retrieval-based editing compares with [6]  -- and which of the results are novel compared to [7, 8], and how does the proposed calibration strategy compare with [4].\n   * please do not consider [1-8] as an exhaustive list of related work. I am not an expert in most of these facets. My point is: since a non-expert (me) can name multiple missing baselines, authors are likely to find more relevant work if they deliberately search for it.\n\n2. As an overview of reliability: the choice of four specific facets feels arbitrary: e.g. it is unclear why we focus on generalizability and not, say, robustness to adversarial [9,10,11], or prioritize mitigating social bias over avoiding toxicity[2,12].\n\n4. (Most) results are presented as-is, with no analysis. For instance, in Section 2, GPT-3 has a smaller generalization gap than a fine-tuned MLM model -- but what exactly causes this behavior?\n  * Example A: Is this because of prompting v.s. fine-tuning? -> What if we use prompting for the MLM model?\n  * Example B: Is this because of language model's training objective?  -> what if we compare similarly-sized CLM and MLM, e.g. T5-11B vs BLOOM/BLOOM-13B -- and try prompt tuning in the baseline model\n  * Example C: Is this because of the training data? -> What if we compare CLMs and MLMs trained on similar *public* datasets?\n  * Note: it is not a weakness that authors did not perform every possible analysis - but it would help the paper\n\n\n5. (Some) claims are overly broad, e.g. \"Language model probability and self-consistency frequency can produce better calibration than a supervised DPR-BERT model, specially on OOD test sets\" -- this claim requires testing multiple language models in different setups.\n\n6. It is not always clear how authors selected the baseline models for each task, e.g. S2 only has roberta, S3 adds deberta, S4 only bert, S5 is only T5\n\n7. In T7, authors evaluate uncertainty calibration through selective prediction (e.g. accuracy on top-10% most confident). However, it is difficult to assess: what portion of the selective accuracy gain is due to better uncertainty calibration, and what portion is simply a more accurate base model. Prior art typically uses additional uncertainty metrics to decouple these two effects.\n\n\n\n[1] https://aclanthology.org/2021.nuse-1.5/\n\n[2] https://cdn.openai.com/palms.pdf\n\n[3] https://aclanthology.org/2021.acl-long.416/\n\n[4] https://arxiv.org/abs/2205.14334\n\n[5] https://arxiv.org/abs/2112.08786\n\n[6] https://arxiv.org/abs/2110.11309\n\n[7] https://arxiv.org/abs/2005.11401\n\n[8] https://arxiv.org/abs/2206.06520\n\n[9] https://simonwillison.net/2022/Sep/12/prompt-injection/\n\n[10] https://arxiv.org/abs/1908.07125\n\n[11] https://arxiv.org/abs/2209.02128\n\n[12] https://arxiv.org/abs/2112.04359\n\n\n\n\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "\n\n**Clarity & quality:** the paper is well written. Few small typos (below) do not affect the overall quality. If I had to cram 4 research papers in 9 pages, I would probably do much worse.\n\n**Novelty:** the paper has a large number of assorted finding in the four facets of reliability. To the best of my knowledge, at least some of them are not covered by related work, at least in their specific formulation with LLM prompting. Unfortunately, I am not an expert in all four research directions present in the paper and may have missed something (hence my confidence score).\n\n\n### Minor stuff\n\n> P1:  and their evaluation methods may not feasible on the much larger GPT-3 model.\n\nMay not BE feasible. Also, if you decide to keep the current direction of the paper, please explain why they are not feasible (possibly later in the paper).\n\n> S3:   Random shuffling achieves the lowest bias gaps (...) Interestingly, putting either Pro-Bias or Anti-Bias examples at the end leads to much larger bias gaps. This contradicts the recency bias \u2026\n\n[mostly nitpicking] This contradiction was not clear to me. If you randomly order sentences, then, assuming recency bias, the most recent sentence is equally likely to be biased either way, and I thought it would average to zero. Perhaps it would be best to explain this contradiction in more detail.\n",
            "summary_of_the_review": "\n\nThis paper combines 4 related (but separate) short research papers and an overview. To the best of my knowledge, all four research directions could be upgraded to a standalone short/full paper, through the addition of baselines, missing related work, fixing some task-specific issues and providing further justification of the already claimed results. At least 2 of 4 papers (and likely all 4) would stand a good chance to be accepted here or on *ACL / EMNLP-type conferences. The overview part has its own merit: should authors modify it to add relevant prior work and focus on analyzing existing results, it would be a valuable resource to beginners.\n\nHowever, in the current form, each of the four research directions is only given a couple of pages, lacking background and task-specific nuances - and adding this information would be difficult because of the size limit, and deferring it to the appendix would make the paper difficult to follow. I would consider one of two strategies:\n\n* splitting the paper: giving each research case at least a short standalone paper -- where you will have space to better support your main claims.\n* reformatting as a meta-analysis: focus on summarizing existing work and pinpoint open problems in model reliability; possibly consider more facets, or provide a better justification for the four aspects currently in the paper.\n\nTo clarify: that is what I would have done. I will be glad if authors prove me wrong by finding a more elegant solution to these issues.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_d2Hh"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_d2Hh"
        ]
    },
    {
        "id": "l4bvDLJSsJw",
        "original": null,
        "number": 2,
        "cdate": 1666691096017,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666691096017,
        "tmdate": 1666691096017,
        "tddate": null,
        "forum": "98p5x51L5af",
        "replyto": "98p5x51L5af",
        "invitation": "ICLR.cc/2023/Conference/Paper2854/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper systematically studies the reliability of GPT-3 from four key facets: generalizability, fairness, calibration, and factuality. With proper prompting strategies, GPT-3 can outperform some supervised baselines like RoBERTa and BERT on these four facets. Reliability is one important aspect of deep learning models and experiment results are comprehensive for these facets. This work shows the capabilities of large language models and sheds new insights into the reliability of prompting large language models.\n",
            "strength_and_weaknesses": "Strength: \n(i) Reliability is an important perspective for the deep learning community. This paper investigates the reliability of the famous model GPT-3. Comprehensive experiments are performed from four different aspects: (1) generalizability to distribution shifts; (2) social biases and fairness; (3) uncertainty calibration; and (4) factual correctness.\n(ii) The experiment is comprehensive in general. For instance, for generalizability, the authors consider domain shift, perturbations and spurious correlation, adversarial attacks completely. These perspectives are common facts regarding the robustness of the model.\n(iii) The conclusion is interesting for large language models, which can inspire the community further and is helpful for model deployment. For example, prompts can update memorized knowledge, which is a meaningful characteristic.\n\nWeaknesses: \n(i) In figure 1, the authors claim that \"prompt generalizes out-of-distribution\" and \"prompt can update memorized knowledge\". However, I think other facts are influencing the final results like the scale of the model and the number of data. Can you perform an ablation study to verify that?\n(ii) Since the whole paper uses empirical methods to study the reliability of the model, the number of baselines is somehow limited. For example, in Table 1, the authors only compare GPT-3 with one supervised model RoBERTa. Generalizability is a hot topic in the deep learning area and can the authors show more supervised baselines here?\n(iii) I also doubt the fairness of the experiment. Since the training strategies and the number of parameters will also influence the final experiment results, it should be better to list the experiment details and the number of parameters in each experiment. \n(iv) There are mainly two differences between LLMs and other baselines. One is the scale of the model, and another is the training setting (number of data, training strategies). However, I think the architecture of different models is also important for reliability. For example, can you provide experimental results for other types of architectures like CNN?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The paper is well-organized and clear. \nQuality: The paper focuses on empirical results for reliability and the experimental results are extensive.\nNovelty: Since the paper performs practical results only, the novelty is somehow weak in my view.\nReproducibility: There is no URL (e.g., github page) for reproducibility.",
            "summary_of_the_review": "This paper studies the reliability of large language models - GPT-3 empirically and provides some interesting conclusions. Reliability is one important topic in the deep learning field and the experiment is comprehensive in this paper. Even though there exists some unfairness for further verification, their conclusions are meaningful to the deep learning community. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_S8uY"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_S8uY"
        ]
    },
    {
        "id": "MbUuNWmDF3i",
        "original": null,
        "number": 3,
        "cdate": 1666731637736,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666731637736,
        "tmdate": 1666731637736,
        "tddate": null,
        "forum": "98p5x51L5af",
        "replyto": "98p5x51L5af",
        "invitation": "ICLR.cc/2023/Conference/Paper2854/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors explore reliability in GPT3, specifically in four areas: generalizability, fairness/bias, uncertainty calibration, and knowledge updates. They develop prompting methods for improving accuracy in these areas.",
            "strength_and_weaknesses": "Strengths:\n- In general, it's great that these ares are getting more attention, especially in LLMs.\n- The deeper analysis on prompt engineering for these specific bias use cases seems very useful (e.g., prompting with balanced demographic groups leading to more fair outputs).\n- Section 5.2 (Memorization vs Updating) experimental setup was really interesting and well-designed. This felt like it could be expanded into a larger paper, with more exploration on why types of information the model memorizes (and what it has trouble forgetting)\n\nWeaknesses:\n- It feels like this paper over-claimed its novelty. In the abstract, they say that \"existing research focus on models\u2019 accuracy on standard benchmarks and largely ignore their reliability\", however the GPT-3 paper has multiple pages on bias analysis, as do the Gopher paper, PaLM paper, etc. \n- Similarly, there is a lot of very relevant related work in Appendix A which should be in the actual paper. For example, in Section 3 (Social Bias and Fairness) it should be made more clear that similar analyses (e.g., WinoGender) were indeed run in the original GPT3 paper, and that the main contribution in this paper is \"how these biases change under different prompting schemes in the few-shot setting\" (quoted from the appendix, but not clear in the paper itself, at least to me). \n- Typos:\n   - Page 7, \"the 2-shot accuracy is 5.8 points worse than the 64-shot is better calibrated\"\n   - Page 3, \"coreference resolution question answering\" (needs an \"and\"?)",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, and seems very reproducible. Like I mentioned, most of the novelty is in the prompting methods to make specific reliability methods work better.",
            "summary_of_the_review": "If this paper's contributions were phrased as \"Others have studied reliability in LLMs. We propose a range of prompting strategies to make this work even better\", I would be more supportive. As it is, it seems more like the authors are presenting the main analysis of reliability in LLMs, which feels like an overstatement.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_UbF3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_UbF3"
        ]
    },
    {
        "id": "CzptoP5gvI",
        "original": null,
        "number": 4,
        "cdate": 1666889402287,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666889402287,
        "tmdate": 1666889402287,
        "tddate": null,
        "forum": "98p5x51L5af",
        "replyto": "98p5x51L5af",
        "invitation": "ICLR.cc/2023/Conference/Paper2854/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the effects of GPT3 on generalization, bias, uncertainty, and knowledge discovery.",
            "strength_and_weaknesses": "Strengths\n- The four facets considered are important and timely given the popularity and impact of GPT3. \n- The paper is coherent and thorough.\n\nWeaknesses\n- It is hard to follow the conclusions. It would be a useful endeavor to have a summary of the main takeaways from each facet.\n- It would be nice to have a list of open questions that result from this work.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: High\nQuality: High\nNovelty: High\nReproducibility: Adequate, but it would be nice to know how best to reproduce the results here in.",
            "summary_of_the_review": "This is an interesting paper that looks into how reliable GPT3 is in various settings. The takeaways will be well-received by our community. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_r9LV"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2854/Reviewer_r9LV"
        ]
    }
]