[
    {
        "id": "jqOmYSZJhG4",
        "original": null,
        "number": 1,
        "cdate": 1666366707196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666366707196,
        "tmdate": 1666366707196,
        "tddate": null,
        "forum": "hO8qWILpJ3J",
        "replyto": "hO8qWILpJ3J",
        "invitation": "ICLR.cc/2023/Conference/Paper4894/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The submission proposes a Complete Latent Likelihood (CoLLike) objective, which is given as (3), as an alternative to commonly used log-likelihood objectives (which the authors call Marginalized Likelihood, MaL) for learning latent variable models. In essence, when the prior over latent variables are known or given, the proposed method generates i.i.d. latent variables from the prior, and find a permutation between the generated latent variables and observed samples. \n\nThe motivation comes from scenarios where computing a marginalized probability by integrating over latent variables can be intractable, typically required in EM style algorithms. Instead, by finding the optimal pairing of observed samples and (generated and fixed) latent variables, which can basically be reduced to a bipartite matching (or assignment) problem, authors could avoid the intractable integration step. \n\nExperiments are performed on datasets with relatively small number of latent classes, and compare the performance of CoLLike and MaL. ",
            "strength_and_weaknesses": "\nThere are a number of controversial heuristics unjustified in my view. \n\n1. One basic principle of the MLE is its (asymptotic) consistency (which is why in the first place people want to do ML estimation). What can be said about the proposed algorithm? Is it consistent at least? \n\n2. The proposed algorithm is not a strict generalization of K-means, since empirical distribution of true labels does not necessarily match the true prior. Impact of such sampling errors should be considered (e.g., when clusters are very well-separated). \n\n3. For AG News data, the huge gap in Accuracy is suspicious. More thorough and detailed discussion on this phenomenon (on why it happens) should follow. \n\n\n\nOther weaknesses\n\n- Discussions in Section 5 are reasonable, but they are sort of trivial and do not add much value to the paper in my opinion. \n\n- What can be done if the prior of latent variables are not known in advance? \n\n- In the experiments, it is not clear what NLL / Agg. KL stand for (and also Accuracy is not precisely defined).\n\n- It is unusual to use $-\\log$ as a distance measure for OT. So why this should be better than using a typical Euclidean distance measure?\n",
            "clarity,_quality,_novelty_and_reproducibility": "While the claim of the paper is clear, the proposed algorithm is a naive variant of MLE, and is not supported by convincing reasons. ",
            "summary_of_the_review": "I do not think the paper contains enough contribution or originality to be published. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_vRN3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_vRN3"
        ]
    },
    {
        "id": "Q7PER-SgQ4",
        "original": null,
        "number": 2,
        "cdate": 1666623757901,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666623757901,
        "tmdate": 1666631059682,
        "tddate": null,
        "forum": "hO8qWILpJ3J",
        "replyto": "hO8qWILpJ3J",
        "invitation": "ICLR.cc/2023/Conference/Paper4894/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Complete Latent Likelihood (CoLLike) which utilizes permutation for matching in the latent domain. The authors argues that permuting the latents and then matching to inputs leads to expressive models with exact likelihood and no posterior collapsing, compared to MaL. Also, the suggested CoLLike can link likelihood and optimal transport.\n",
            "strength_and_weaknesses": "The experimental results show the efficacy of the suggested CoLLike.\n\nHowever, it seems that the computational complexity of O(N^3) can be problematic, where N indicates the number of data instances.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The authors provided their code as supplementary material, and the quality and the novelty are fair.\n\nFor the clarity of the paper, here are some questions to the authors.\n\n- In the beginning of Section 2, the authors should mention that x is an input and y is a label to be predicted, even though it is convention. Or, am I wrong?\n\n- I wonder why the authors represent the latent variable z as \u201clabel\u201d. Does the latent z is limited to some attributes, for example? Can\u2019t it be in any shape? It would be better to provide a graphical notation of x, y, and z, together.\n\n- Sampling from the prior ensures the first requirement, \u2026 -> probably (x_N, y_{pi(N)}) ?\n\n- Figure 1 is very hard to follow. What are the circles indicates? Where is permutation pi in the figure?\n\n- The python-like pseudo-code should be written with more consistency.\n\n- The complexity of CoLLike seems to be problem. Even though the authors utilized the combinatorial linear sum assignment problem with complexity of O(N^3), the problem is that N is the number of data instances which can be a huge number. Could you compare the wall-clock runtime of CoLLike against the baselines? Also, I\u2019m curious about the effect of additional techniques: minibatch assignment, low-dimensional discrete latents, and factorized conditional. How much do they reduce the wall clock runtime in the experiments?\n\n- Also, how exact is the combinatorial linear sum assignment problem? Does it always solvable?\n\n- I suggest bring the objective of CoLLike and the combinatorial linear sum assignment problem to the main paper. Those should be treated in the main body.\n\n- It seems that there are no direct comparison between CoLLike and ELBO in the experiment section. Does it due to that the ELBO does not have tractable likelihood? Can't it be approximated by Monte Carlo estimation, for example, in [1,2]?\n\n- Could you provide more information about the experiment setting in Section 6.2? \n\n- In data generation perspective, how ones can generate data from the latent prior p(z)? Could you explain the sample generation process?\n\n- I suggest authors to have proof-reading from the natives.\n\n[1] Stick-Breaking Variational Autoencoders, https://openreview.net/pdf?id=S1jmAotxg \n\n[2] Dirichlet Variational Autoencoder, https://www.sciencedirect.com/science/article/pii/S0031320320303174\n\n",
            "summary_of_the_review": "The paper is fairly-written in general. However, the authors are missing a critical point, which is an analysis on the computation complexity. Following from the authors, the provided algorithm has O(N^3) complexity, where N indicates the number of data instances. My major concern for this paper is that the utility of the proposed method may vary with computational time. Also, the clarity of the paper needs be improved. Therefore, at this moment, I'm leaning to the negative side on this paper.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A\n",
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_wzh3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_wzh3"
        ]
    },
    {
        "id": "SQYaPhMZsZN",
        "original": null,
        "number": 3,
        "cdate": 1666624423196,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666624423196,
        "tmdate": 1666624423196,
        "tddate": null,
        "forum": "hO8qWILpJ3J",
        "replyto": "hO8qWILpJ3J",
        "invitation": "ICLR.cc/2023/Conference/Paper4894/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose a new learning objective for fitting latent variable models. The basic idea is to sample a set of latent variables from the prior and find a permutation of those that aligns well with the observed data. The method provides a point estimate of the latent variables, but the paper is written from a perspective where it is suggested as an alternative to marginalising over the latent variables rather than focusing on the effect of the constraint imposed over standard joint likelihood. The approach is demonstrated primarily in context of supervised classification problems, where it is shown to find latent variables that align well with known classes.",
            "strength_and_weaknesses": "Strengths:\n- The paper focuses on a fundamental learning problem and hence has potential for significant impact if the contribution is considered valuable by the community\n- The core idea of constraining the possible values for the latent variables based on a sample from the prior is interesting\n- The learning algorithm seems to work well, in the sense that we observe empirically the properties we should expect from the objective (higher MI, lower KL between the prior and the conditional etc)\n\nWeaknesses:\n- The objective is poorly motivated, using only high-level argumentation like posterior collapse in some specific latent-variable models as justification for limitations of marginal likelihood, and it relies on possibly questionable assumptions (sharp distribution for the latents). The main story focusing on its relationship with marginal likelihood is odd.\n- The presentation is on a too high level. For instance, the main method is described only in terms of pseudo-code and the experiments are near impossible to parse from the main paper, for instance to figure out how the proposed method was used in these.\n- The computational cost of operating with permutations is dismissed lightly; the authors mention it as a challenge and outline possible solutions on a high level, but do not transparently communicate what it means in practice. The experiments completely omit all discussion on the computational cost or even whether the proposed speedups were used.\n- The notation and terminology are confusing and non-standard, making the paper difficult to read. For example, the phrases \"unknown $z|x$ target conditional\" or \"target $z|x$ distribution\" used in Introduction do not mean anything at that stage. Similarly, Section 3 is written in rather confusing terminology and makes standard concepts sound unjustified choices; the choice of conducting learning in a manner that is faithful to the modelling assumptions is called \"keeping the model posterior unchanged\" etc.",
            "clarity,_quality,_novelty_and_reproducibility": "The authors approach a fundamental question of what learning means, but unfortunately the paper is written in quite a confusing manner from a narrow perspective. I would assume a paper proposing a replacement for standard learning objectives to analyse the problem from a proper statistical perspective with rigorous mathematical treatment, rather than talking about vague concepts like \"the target conditional\". It seems that the motivation for the work comes from specific problems (posterior collapse etc) in specific types of latent variable models and the proposed objective indeed may be a good heuristic for addressing these specific aspects, but the paper suggests it would be a general solution for learning any latent variable models. From this perspective it is highly questionable and difficult to read.\n\nAlready the starting point is odd. The paper proposes a specific kind of a point estimate for $z$ as an alternative for marginal likelihood (that integrates over $z$), rather than relating it to standard point estimates that would be much the natural baseline for this. This discrepancy remains throughout the motivation, model description and experiments, where (unsurprisingly) metrics that favor point estimates are shown to be better. The paper would be easier to understand and follow if it focused on explaining the effect of the specific constraint for $z$ (namely that the set of all $z$ must be compatible with a sample drawn from the prior), rather than attempting to describe the objective as alternative to marginalising over the latent variables.\n\nIt is good that the authors clearly mention the main assumptions, but I have trouble accepting either one of them in general case. First of all, why should the \"marginal of the target conditional in the latent domain\" (sic) correspond to the prior $p(z)$? The standard practice in latent variable models is to use priors that are broad to allow for the data define the actual posterior, and models with no posterior concentration would be considered poorly defined. I understand the need to avoid extreme posterior collapse in flexible deep models, but nevertheless we should typically expect $p(z|x)$ to differ quite notably from $p(z)$. You later show empirically that you indeed reach $p(z|x)=p(z)$ which nicely shows the algorithm works well and that you can satisfy the assumption, but I would consider that as negative result from the perspective of learning a useful model for the data.\n\nSimilarly, I do not understand the assumption of confidently assigning a single $z$ to each $x$. Quite contrary, I would expect $p(z|x)$ to be broad (assign clearly non-zero probability for non-diminishing set of $z$) for many common use-cases for latent variable models. For clustering, we need probabilistic models exactly when the clusters overlap severely, in recommender engines we use latent variable models because for new users we know we cannot determine $z$ accurate before seeing enough data etc. In brief, the whole assumption seems to contradict the most common use cases for the model family. Even if I somehow misunderstood the statement or we narrow down to some specific sub-family of latent variable models where this assumption makes sense, the justification 'as in any real-world dataset' is nowhere near adequate. Finally, I do not see how 'assignment of a single $z$ to each $x$' would assure the second assumption. It certainly assigns one $z$ for each $x$, but does not say anything about the actual $p(z|x)$ in the sense that we could confidently tell for future $x$ what the latent variable is. Then again, the proposed approach does not even seem to define $p(z|x)$ outside the training instances.\n\nThe presentation of the algorithm is poor; you only provide a pseudo-code rather than explaining it mathematically. As you anyway provide code that likely includes similar parts, it would be better to describe the method properly in the main paper. Furthermore, the discussion of possible speedups remains on such a high level that we do not even know whether you have derived the details (and used these in the experiments) or whether these are just ideas for future work.\n\nThe empirical experiments are similarly vague. It seems you use 'MaL' to refer to a solution of some approximation (which seems to vary from case to case) and explain the methods on a very high level (\"we use Glow-like normalising flow\"). I know the details are provided in the Supplement, but the presentation in the main paper is still too simplified. As a practical example, your experiments do not say anything about the computational cost of the approach. You clearly mention in technical sections that the method has high cost because of the permutation and propose a few techniques for addressing it, but you do not even say whether those were used and you do not say anything about the actual cost. Now the reader has no way of knowing whether the MaL and CoLLike are approximately as fast, or whether one of them is 1000 times slower. For a method with known computational issues you absolutely need to say something about the cost in the empirical section.\n\nThe empirical results are good in the sense that they confirm the learning algorithm, but do not tell much about the specific formulation. Replacing integration over $z$ with point estimates indeed increases MI between the estimate and the ground truth, due to reduced uncertainty, and this is what you see. You would most likely get a similar result when comparing the solution of k-means and the posterior of a probabilistic mixture, but it would not imply the former is a better objective. \n\nQuestions and minor remarks:\n- What is the exact relationship to EM? It sounds like the most closely related method, rather than MaL, yet it is only briefly mentioned. Should there perhaps be some comparison as well? Even EM is 'more advanced' than your method as it approximately integrates over $z$ rather than finding a point estimate, but still a better match for comparisons.\n- Does your learning algorithm propagate gradient information through LAP? Not easy to figure out from the pseudocode as it may depend on how LAP is implemented. \n- It sounds like your approach works in 'transductive' setting, in the sense that you assume all $x$ to be available at training time. Can you still estimate $z$ for new samples in an inductive manner? If yes, how? This should be discussed.\n- How would the model work in a case where the true $p(z|x)$ are very broad (e.g. a clustering case where the clusters are almost -- or perhaps completely -- overlapping)? Would it still find a good objective value? I strongly feel that illustrating the model in some simple examples, in addition to the main goal of flexible latent-variable models, would help clarifying the key concepts. For instance, a simple mixture model case would show the readers how the optimal solution is similar to what k-means would produce, in the sense that it is a hard clustering where the global optimum is one that assigns each sample to the closest cluster (unless I'm mistaken). An example like this would show what the effect of sampling the set of $z$ from the prior is in practice.\n- How would the method work if the prior is way off? For instance, if you assume Dirichlet prior in clustering with constant cluster sizes but the data strongly suggests some clusters have ten times more samples?\n- Constrained K-means does have a probabilistic interpretation; see e.g. Jitta and Klami \"On controlling the size of clusters in probabilistic clustering\" (AAAI, 2018).\n",
            "summary_of_the_review": "A bold attempt of re-defining the fundamental learning objective of latent variable models, but there are too many open questions regarding the theoretical foundations and also the empirical side. The biggest issue is that the approach is compared against marginal likelihood when it actually corresponds to a specific way of constraining a point estimate for the complete likelihood, and because of this mis-positioning the paper actually avoids discussing what the effect of sampling the set of possible latent variables from the prior is in practical cases or even in easy examples. From the perspective of general latent variable models the assumptions made for deriving the new objective are either shaky or even wrong, but it may be that for specific types of models they provide reasonable inductive biases. However, for that the paper would need to be re-written from a very different perspective.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_V5Wu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_V5Wu"
        ]
    },
    {
        "id": "gccyOWsfQ9",
        "original": null,
        "number": 4,
        "cdate": 1667353097588,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667353097588,
        "tmdate": 1667353097588,
        "tddate": null,
        "forum": "hO8qWILpJ3J",
        "replyto": "hO8qWILpJ3J",
        "invitation": "ICLR.cc/2023/Conference/Paper4894/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a complete latent likelihood (CoLLike), an alternative to the marginal likelihood (MaL) objective that is standard in ML algorithms (MaL) today. A systematic comparison between COLLike and Mal in the KL framework is compared. By discussing posterior collapse and high deviation of the aggregated posterior from the prior, authors show CoLLike can prevent these drawbacks. Empirical studies on image and NLL tasks show consistent improvement of CoLLIke over MaL. ",
            "strength_and_weaknesses": "Strength:\nThis paper address a fundamental problem in all machine learning training and it potentially has a big impact. \nMany connection and differences with existing objective are discussed - it was enlightening.\n\nWeakness:\nSome concern on the permutation estimation exist due to inefficiency. \n\n\nComments:\n- \" target conditional is actually specified and equals $p_{\\theta} (z|x)$ if we ask what distribution we want to mimic\":  the exact form of $p_{\\theta} (z|x)$ is still not specified, and it could be anything since it canceled out. If $p_{\\theta} (z|x)$ is changed to any other distribution,  MaL  remains the same. This seems to contradict the statement. \n- \" the CoLLike objective allows learning models with reverse factorization\": ELBO has been used in VAE like structure, which resembles the function form. In addition, it would be good to provide examples on advantage or necessity of reverse factorization.\n- some references are not cited correctly. e.g., \" InfoMax principle Huszar (2017))\".\n- Eq 3: does the permutation mean all possible label assignments for all samples? How is it a permutation if the number of each class within Z are different? \n- \"since the set of all possible \u03c0 values is countable and finite, optimization can be performed by an exhaustive search.\" it seems for large sample size N, this is not truly feasible with exhaustive search. Even with Hungarian complexity O(N^3), it is still quite a big number, in particular in large datasets popular nowadays. \n- Hence, it would be interesting to see the time comparison between different objectives in the experiments. \n- \" we emphasize our focus on learning useful z|x instead of simplifying the model with factorized x|z conditional\" can authors elaborate on this point? what if x|z is not factorized?\n- \"To reduce the high variance of CoLLike unsupervised classification\": authors should have discussed this point earlier of this work. ",
            "clarity,_quality,_novelty_and_reproducibility": "quality: fundamental contributions with sound justifications.\n\nclarity: presentation is clear \n\noriginality: novel ",
            "summary_of_the_review": "a fundamental and potentially impactful work to address the training objective in ML algorithms today. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_G4Xy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4894/Reviewer_G4Xy"
        ]
    }
]