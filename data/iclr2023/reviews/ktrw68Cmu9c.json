[
    {
        "id": "2f8kXn91w4J",
        "original": null,
        "number": 1,
        "cdate": 1666537606580,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666537606580,
        "tmdate": 1666537606580,
        "tddate": null,
        "forum": "ktrw68Cmu9c",
        "replyto": "ktrw68Cmu9c",
        "invitation": "ICLR.cc/2023/Conference/Paper1043/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "Large language models trained on source code can generate many plausible solutions to programming problems. This work proposes to simultaneously use these models to generate tests for the same prompts, which are then used to re-rank the generated solutions, using a consensus set based algorithm. The results show that this technique substantially improves the correctness of the highest ranked sample(s) across a range of language models, and additionally provide an analysis of the impact of test quality, which underscores that especially powerful models benefit most from this modification because they are also most likely to generate meaningful tests.",
            "strength_and_weaknesses": "This work introduces a relatively straightforward, yet highly effective technique, and explores its design decisions in great depth. I was left with nearly no questions after reading through the results in the appendix, and find the approach convincing. This makes it a strong contribution to the domain of code-generating models.\n\nIn terms of weaknesses, a lesser complaint is that this work makes very little effort to connect its findings to the theme of representation learning. Implicit in the experiments and results is the observation that LLMs must have some instrinsic notion of code execution, at least as syntactically represented by test cases, but this is neither explored per se, nor does the work draw connections with any related fields and applications. As such, it seems like it would be a better fit at a software engineering venue. Nevertheless, conferences such as this tend to draw papers from a very wide range of application domains, so this does not change my overall verdict.\n\nAnother relatively minor issue is the decision to remove input/output examples from the (HumanEval) prompts, or rather, the discussion around that experimental decision. While (the end of) Sec. 2.1 reasonably states that this helps avoid biasing the models, App. A makes a much stronger claim that removing these cases is required to \"fairly\" verify CodeT. This strikes me as a rather odd statement: program synthesis research has historically mostly operated under the assumption that input/output examples would be provided by a user, just like the HumanEval benchmark does. The CodeT approach also clearly both benefits from these tests _and_ generalizes beyond them, judging from the substantial performance gains over its own results without \"seed\" tests as reported in Tab. 4. While it is entirely reasonable to compare both conditions on the premise that not requiring examples involves less effort by the user, suggesting it is in some way more fair seems off. I would suggest reporting both settings side by side in the main text instead. Finally, the text in 2.1 also claims that removing the original tests increases the \"diversity and difficulty\" of the generated tests, which does not appear to be substantiated by experiments.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is overall very well written and comprehensive. I leave a few minor notes on the writing below.\n\n**Writing Comments:**\n- Figure 3: it would be helpful to expand the caption to describe the meaning of both the edges and boxes.\n- P4: \"this score equals to\" -> \"this score is equal to\"\n- P6: \"super hard\" -> consider using more formal terminology (e.g., \"especially hard\")\n- P9: \"in the future work\" -> remove \"the\"\n",
            "summary_of_the_review": "This paper is well-written, thorough, and yields strong results. Its proposed technique is not particularly complex, but is both effective and very well investigated. Its main limitation is a relatively narrow focus on a specific programming task approach purely from a large language model perspective, which leaves it both without strong connections to the role of self-evaluation in the broader ML community, and to make the occasional questionable claim relative to prior work in program synthesis.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_GQjW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_GQjW"
        ]
    },
    {
        "id": "zV7SIf8OCj",
        "original": null,
        "number": 2,
        "cdate": 1666646933317,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666646933317,
        "tmdate": 1670529791965,
        "tddate": null,
        "forum": "ktrw68Cmu9c",
        "replyto": "ktrw68Cmu9c",
        "invitation": "ICLR.cc/2023/Conference/Paper1043/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles the problem of generating a code solution to a programming problem. The key observation is that current models like Codex can generate solutions, but selecting the appropriate solution is challenging. This is reflected in the large gap between pass@100 and pass@1. If test cases were available, one could simply run the top 100 generated solutions on all the test cases to get the best one. But in several real world settings, test cases are not available.\n\nThis paper proposes to generate test cases and solutions simultaneously with the same model (no further training necessary). They engineer a prompt that makes the model generate test cases, and use a random sampling strategy to run selected solutions on selected test cases. From the results of these runs, they pick the best solution according to certain criteria.\n\nThey show that their approach achieves approximately a 10% improvement on the HumanEval and MBPP benchmarks, with minor improvements on the Apps and CodeContests benchmarks.\n",
            "strength_and_weaknesses": "**Edit :** After reading the review by reviewer ZjXf, I have several new concerns about the paper and evaluation.\n\n**Comparison with AlphaCode:**\n\nThe AlphaCode paper uses a separate trained model to generate test inputs, and uses the outputs on these test inputs to cluster its generated solutions (which can be up to 1e4). In comparison, CodeT re-uses the same model to generate test inputs *and* desired outputs. When presented in this light, this diminishes the novelty of CodeT somewhat. A detailed discussion should be included in the paper.\n\n**Computation of pass@k:**\n\nThe \"biased\" approach to compute pass@k is to just generate $k$ solutions and see if any one of them passes all test cases. Using the \"unbiased\" approach from Chen et al. (2021), one would generate $n$ solutions, and compute the number $c$ that pass all the hidden test cases. Then the pass@k is computed using a combinatorial formula.\n\nAs I understand it, the baselines have been evaluated with the \"unbiased\" formula, but CodeT has been evaluated with the \"biased\" formula (mentioned under \"Implementation Details\" in Section 3). This seems a little strange and not kosher. I think this evaluation needs a rethink.\n\n**pass@k vs n@k:**\n\nThe metric n@k as presented in the AlphaCode paper is as follows - generate $k$ solutions and use some filtering mechanism to narrow it down to $n$ solutions where $n \\le k$. Then if any one of the $n$ solutions passes all the hidden test cases, report success on that problem.\n\n*This seems like the setting that you should be evaluating CodeT on.* In particular, pass@k does not seem well suited to your problem setting, and results in an odd mismatch in evaluation procedures between CodeT and your baselines.\n\nI would reformulate the entire paper as a comparison of *different filtering approaches*, with the evaluation based on the n@k metric. The filtering approaches would be:\n1. Primitive filtering (baseline). Just take the top $k$ solutions out of $n$ generated solutions (\"biased\" pass@k).\n1. Random filtering - take a random sample of $k$ solutions out of $n$ generated solutions. This (in expectation) is the \"unbiased\" pass@k metric defined in Chen et al. (2021).\n1. The AlphaCode approach - train another model to generate test inputs, and cluster the $n$ solutions based on their outputs on these generated inputs.\n1. CodeT - reuse the same model to generate test inputs as well as desired outputs.\n\nThere needs to be a detailed comparison between the latter two approaches. I understand that it is difficult to recreate the AlphaCode setting with their trained input generation model, but at the very least there should be a *qualitative* comparison.\n\nMy old review comments (pre-edit) follow.\n\n---------------------------------------------------------------------------------------------------------------\n\n**Strengths :**\n\n1. Simple, elegant method that requires no retraining\n1. Convincing performance improvements with thorough experiments\n\n**Weaknesses**\n\n1. This approach could potentially increase the code generation time several-fold, especially if you have ~1e3 tests for 1e2 solutions => ~1e5 executions. The HumanEval benchmark has just 7.7 \"ground-truth\" tests on average per problem, so if one were to execute all top 100 solutions on the given tests then there would be ~770 executions. I did notice and appreciate the note at the bottom of the Q2 section and the experiments in Appendix G.3, but this should really be its own section. Execution time is, in my opinion, a significant factor in determining the real-world utility of this model. I would ideally like to see :\n\n    * How many \"ground-truth\" test cases does each benchmark provide, on average, per problem?\n\n    * How many executions (code running on test) does CodeT require, on average, per benchmark?\n\n    * The results in Appendix G.3 moved to the main text.\n\n2. (Minor) The choice of using the *product* of number of solutions and number of test cases seems slightly counter-intuitive to me : why would we (intuitively) care about how many other solutions matched the same test cases? It's all the more intriguing because you say that \"such cases are not rare\", and your ablation study in Appendix H shows that this is an important design choice. Some intuition here would be helpful.",
            "clarity,_quality,_novelty_and_reproducibility": "Edit - I have updated my evaluation of quality and originality after reading the review by Reviewer ZjXf.\n\n**Quality and Originality :**\n\nThe idea of reusing the same model with a carefully engineered prompt is innovative and yields surprisingly good results for such a (conceptually) simple change. However when presented in the light of the AlphaCode paper, this diminishes the novelty of the approach somewhat. Moreover, this comparison is missing in the paper.\n\n**Clarity :**\n\nI do feel that the presentation of the paper could be vastly improved. High-level comments :\n\n1. There is a lot of text in the paper with numbers embedded in it, which could possibly be better represented as a table/diagram. In particular, I would like one table to which I can refer to see the different datasets, number of problems, average number of \"ground truth\" tests per problem (see point 1 under \"Weaknesses\"), and sampling number. That brings me to my next comment...\n\n1. After several re-reads, I still was unable to understand what is meant by *\"we sample 100 times for each problem and extract the first five syntactically correct test cases from each sample\"*. I was of the impression that we can generate $n$ solutions and $k$ test cases independent of each other. But this sentence seems to suggest that for **each** solution, we generate $5$ test cases. Why would there be a correspondence between generated test cases and generated solutions?\n\n1. (Minor) I have a question about the following step - \"If we want to obtain k code solutions, we can select the top k consensus sets with the highest scores, and one code solution is picked up from each of the k consensus sets.\" Suppose you've generated 100 solutions grouped into 5 consensus sets of 20 solutions each. Then if you pick one from each set, you will have only 5 solutions. How do you obtain 10 solutions to compute pass@10?\n\n**Reproducibility :**\n\nI couldn't find provided code to reproduce the results, but the approach is simple enough that I think one could reproduce the results without unreasonable effort.",
            "summary_of_the_review": "**Edit** - I have updated my evaluation and score after reading the review by Reviewer ZjXf.\n\nThe idea that this paper presents is simple, elegant and innovative. However it is missing a comparison with the approach taken by AlphaCode, i.e., training a separate model to generate test inputs. Further, the evaluation is shaky and in my opinion, needs to be reformulated around the n@k metric and as a comparison of different filtering approaches. On the whole, I think this is an innovative idea, but the paper in its current form is not ready for publication. I would be happy to reconsider my score if the authors are able to address these concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_ziee"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_ziee"
        ]
    },
    {
        "id": "-aDQk0iHBM",
        "original": null,
        "number": 3,
        "cdate": 1666654975936,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666654975936,
        "tmdate": 1668760918892,
        "tddate": null,
        "forum": "ktrw68Cmu9c",
        "replyto": "ktrw68Cmu9c",
        "invitation": "ICLR.cc/2023/Conference/Paper1043/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper is part of the recent stream of papers which use large language models to generate code to solve coding contest type problems. Two popular ways of measuring the performance of such models is to (1) evaluate $pass@k$ -- $k$ solutions are generated for a problem and evaluated against the unit tests for that problem. If even 1 solution clears all the test cases for that problem, then the problem is considered solved. The metric is the fraction of problems so solved in the dataset (2) $n@k$ -- $k$ solutions are generated but *only* $n$ solutions are evaluated against the unit tests for the problem. If even 1 out of the $n$ (instead of $k$) solutions passes all the test cases, the problem is considered solved and the metric is the fraction of problems so solved. This metric was widely used in the AlphaCode paper [1]\n\nThe idea behind this paper is as follows. Given a prompt, they first generate $K$ solutions to the problem that the prompt describes. In their experiments $K=100$. They then use the language model to generate new test cases $T$ for the problem. While the validity of any of the test cases in $T$ is unknown, the agreement of the different generated solutions in terms of passing or not passing test cases in $T$ can be used to *cluster* the solutions.\n\nThey then use this clustering to select $n=1,2,10$ solutions from the $K=100$ candidate solutions they generate. They compare this against baseline models which are allowed to generate only $n=1,2,10$ solutions. They claim improvements for $n=1,2,10$ but not for $n=100$ when the computation budget is the same for the baseline as well as their model.\n\n[1] Competition-Level Code Generation with AlphaCode (https://arxiv.org/abs/2203.07814)",
            "strength_and_weaknesses": "While I think the paper is the start of an interesting direction, I think in its current form, it is lacking important comparisons as well as motivation.\n\nIn the paper, they claim to compare on $pass@k$ (Table 1) where they set $k=1,2,10$. However, $pass@k$ is relevant when *all* the models are allowed to generate only upto $k$ solutions. In this instance however, CodeT is allowed to generate upto 100 solutions (from which 1,2,10 are selected) but other models are only allowed to generate 1,2,10 solutions. Thus the CodeT model is allowed a far larger computation budget and has an unfair advantage under the $pass@k$ metric. \n\nFor such a scenario, it is the $n@k$ metric from the AlphaCode paper that makes the most sense and provides for a fair comparison -- where they also generate $k$ solutions but then select $n$ solutions out of those to evaluate against the ground truth unit tests. In the AlphaCode paper, they select the $n$ representative solutions by executing the $k$ solutions on a model trained to generate inputs and clustering based on the output.\n\nIn this paper, they also do a clustering -- however instead of clustering by executing on a model trained to generate inputs and clustering based on the output, the clustering is based on executing on the code generation language model based generated inputs and the binary evaluation of whether the test case passes or not.\n\nGiven the above, the main two issues that need to be worked on are as follows\n\n(1) How does this paper's approach compare with AlphaCode's approach for clustering and then evaluating the selected solutions?\n(2) Why might clustering based on the binary feedback of success or failure from generated test cases from a language model perform better than a more fine grained clustering based on the generated output on an input?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\n\nThe paper evaluates based on $pass@k$ when it really should be evaluating on $n@k$ for a fair comparison. This aspect is not made clear at all.\n\nOriginality:\n\nGenerating test cases using language models has been explored in several papers. Clustering generated solutions based on generated inputs was explored in the AlphaCode paper. Clustering generated solutions based on binary success/failure on generated *unit tests* (which include the expected output) has not been explored but there is no motivation given as to why this might work better than the previous AlphaCode work -- nor is any comparison done.",
            "summary_of_the_review": "The reasons for my rating are\n\n(1) $pass@k$ is an unfair metric for what they're trying to evaluate -- as they're selecting a subset of generated solutions which gives their model effectively a far larger computation budget than their baselines.\n(2) No comparison against AlphaCode style clustering techniques for solution subset selection.\n(3) Motivation for why their clustering may work better than AlphaCode's clustering is not given.\n\n**Edit**: After reading the author's response, I'm raising my score to a 5. My response (copy-pasted from below)\n\n\"I thank the authors for their detailed response as well as the additional experiments.\n\nI still disagree that raw pass@k is an appropriate metric as you're comparing a method with filtering to a method without. That would suffice if there were no extant filtering or ranking methods but that is not the case.\n\nI agree that ranked pass@k is appropriate. I also find the comparison with AlphaCode critical and that it shows an advantage for CodeT is encouraging and a point in CodeT's favor.\n\nOverall however, I agree with Reviewer ziee that this should have been presented as a filtering paper -- which would change the narrative a great deal as well as would require more thorough experiments -- for example there are several reranking methods that could be used to filter out top-k solutions and could serve as good baselines. \n\nI encourage the authors to modify the paper and add such baselines.\n\nI'll raise my score to a 5 but I'm not comfortable voting for an accept given that the paper should be framed as a filtering paper and should involve a much more thorough investigation against various alternative baselines for filtering/ranking \"",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_ZjXf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_ZjXf"
        ]
    },
    {
        "id": "OfwWWznLh1",
        "original": null,
        "number": 4,
        "cdate": 1666674545283,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674545283,
        "tmdate": 1666674545283,
        "tddate": null,
        "forum": "ktrw68Cmu9c",
        "replyto": "ktrw68Cmu9c",
        "invitation": "ICLR.cc/2023/Conference/Paper1043/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a method to improve pretrained language models' performance on executiion-based benchmarks by using the same model to generate test cases and perform dual execution agreement. The proposed method is simple and effective without any additional training. Experiment results with multiple pretrained models on multiple execution benchmarks show impressive improvement.",
            "strength_and_weaknesses": "Strength:\n- The paper is well-written. The method is well explained with examples. Experiment results are extensive with analysis.\n- The proposed method does not rely on any additional input (test cases in particular) besides the problem description/prompts, so it is a self-contained method without much assumption.\n\nWeakness:\n- The apparent weakness is obvious and also mentioned in the paper. CodeT method only works for programming problem datasets with clear test cases. It limits the applicability of CodeT, for example, it cannot be used in general code generation scenarios in code companion tools like Copilot.\n- Considerring the goal for the proposed method is to prompt or post-processing of 100 generated sample, it makes sense to compare the proposed method with language model generation post-reranking  method, which also intend to select/rank from a number of generations.",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well-writtern and is of high quality. The proposed method is novel. And I expect that the results should be easy to reproduce.",
            "summary_of_the_review": "Overall, I think this is a good paper with novel method proposed and well-performed experiments.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_tYm4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1043/Reviewer_tYm4"
        ]
    }
]