[
    {
        "id": "0ypnaTl922",
        "original": null,
        "number": 1,
        "cdate": 1666656123550,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666656123550,
        "tmdate": 1666656123550,
        "tddate": null,
        "forum": "dKkMnCWfVmm",
        "replyto": "dKkMnCWfVmm",
        "invitation": "ICLR.cc/2023/Conference/Paper6099/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper is about Multi-objective online learning. Compared with the original online learning setting, this paper considers the case that the online algorithm is required to solve multi-sequence input loss functions simultaneously rather than the single input loss function. To measure the performance of the algorithm, this paper involves the sequence-wise PSG (Pareto sub-optimality gap), which extends the regret for the single objective online learning. Meanwhile, this paper involves the min-regularized norm with Multiple Gradient Descent to make a trade-off to the multi-gradient of the loss sequences in the proposed algorithm. Finally, this paper gives a sub-linear \u201cregret\u201d bound to the problem. ",
            "strength_and_weaknesses": "This paper provides a new and efficient measurement of Multi-objective online learning. This measurement sequence-wise PSG is an extension of the original regret for the single loss function. Then, by proposing the algorithm, this paper involves the min-regularized norm to overcome the potential instability caused by the gradients for loss functions shown in a counterexample. This is a contribution.   \nHowever, there are some concerns about this paper.  \n1.\tIn the definition of sequence-wise PSG, the domain of the comparator of the algorithm is constrained to the Pareto optimality, not the total domain of the algorithm. Although the paper states the reason, it seems to be a very strong assumption of the problem. In one of the related works like \u201cOnline Minimax Multiobjective Optimization: Multicalibeating and Other Applications \u201d (D. Lee et al.), the comparator term is also constrained but the domain to the comparator term remains. This paper might be supposed to emphasize more in detail to support the advantages of this measurement.   \n2.\tIn this paper, the regret bound for the multiobjective loss is T^{1/2}. In the main part and the appendix, the reviewer can only follow the result of the case when m=2, according to Lemma 3. This is a significant issue for this paper since theorem 1 only implies the sub-linear bound when the min-regularized norm method can bound the last term with arbitrary m. Otherwise, the result is a little bit incremental. Another suggestion is that the authors may rewrite the proof to make them easier and more obvious to follow, especially for Corollary 1.   \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly in the main part, but the proof of the main result seems to be ambiguous to follow. With counterexamples, it provides the necessity to involve the sequence-wise PSG and min-regularized-norm. Under these two concepts, the paper gives a learnable algorithm for multi-objective online learning. The result now for m=2 is clear, but for the further result, it might not be convincing. The idea of this paper is acceptable and enlightening if the proof is correct.  ",
            "summary_of_the_review": "It is an interesting work for multi-objective online learning. In this paper, an effective measurement has been involved in online learning for multi-objective functions. Meanwhile, this paper offers an available multi-gradient descent technology in the online version. I would like to re-rate this paper if the presentation and analysis have been improved.\n\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_rnNt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_rnNt"
        ]
    },
    {
        "id": "N0CZAUk5FZ",
        "original": null,
        "number": 2,
        "cdate": 1666896970393,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666896970393,
        "tmdate": 1666896970393,
        "tddate": null,
        "forum": "dKkMnCWfVmm",
        "replyto": "dKkMnCWfVmm",
        "invitation": "ICLR.cc/2023/Conference/Paper6099/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the problem of multi-objective online learning. In the classic online convex optimization problem, there is a single objective function, and the goal is to find the best action that leads to the best value of the objective function in a sequential decision-making setting. This paper extends the problem to a case where there are multiple objective functions. In this setting, then, the notion of best action needs further elaboration to define formally. The reason is that it is possible to have different best actions for different terms of the objective function. In this paper, the notion of Pareto optimality is used to define the Pareto optimal action. In addition, regret in multi-objective settings needs to be carefully redefined. \n\nThen,  after problem formulation, the paper develops an algorithm based on Doubly Regularized Online Mirror Multiple Descent, and the regret of the algorithm is analyzed, and it is shown that it matches the regret in a single objective case.",
            "strength_and_weaknesses": "**Strengths**\n- The paper is very well written and well organized. \n- The results are solid, and the authors provide sufficient insights on the issues with alternative solutions.\n- There are numerical results that support theoretical results. \n\n\n\n**Weaknesses**\n- The paper sounds like a nice re-execution of the multi-objective bandits in the full information feedback and online convex optimization. That said, I believe the authors did a great job clarifying the differences and unique challenges in this paper. Hence, I still believe that the contribution of this paper is valuable. \n- Is it possible to derive a problem-specific regret lower bound that captures the unique structure of the multi-objective property of the problem? It seems that in the paper, the only remark on the optimality of regret shows that with a proper parameter setting, the result matches the single-objective case. However, it is unclear what is the definition of optimality of regret in the multi-objective setting. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear, and the quality of the results and presentation is great. The novelty might be somehow limited since it mainly borrows ideas from similar problems in prior literature. ",
            "summary_of_the_review": "Overall, I enjoyed reading this paper, and I think it studies a timely and practically relevant problem. The theoretical contribution is nontrivial and nicely executed. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_GCcZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_GCcZ"
        ]
    },
    {
        "id": "W9zVz4KoE9V",
        "original": null,
        "number": 3,
        "cdate": 1666912397517,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666912397517,
        "tmdate": 1669104189997,
        "tddate": null,
        "forum": "dKkMnCWfVmm",
        "replyto": "dKkMnCWfVmm",
        "invitation": "ICLR.cc/2023/Conference/Paper6099/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies multi-objective online learning, a framework in which the learner has to optimize jointly several conflicting objectives. In particular the notion of optimality and discrepancy need to be redefined. The authors propose a definition of the regret based on the sequence-wise Pareto Suboptimality Gap. Then, they propose an algorithm based on the regularized min-norm and prove it achieves a $\\sqrt{T}$ regret. Experiments complement the contributions.",
            "strength_and_weaknesses": "**Strengths**\n- the paper is globally clear and well written\n- the topic is of interest to the ICLR community\n\n**Weaknesses**\n- I am not fully convinced by the definition of the regret (which is one important contribution of the paper). Typically, in Proposition 1, isn't $\\lambda^*$ just selecting the objective with respect to which the regret is the smallest? If the previous regret proposed was indeed too hard, this one seems on the contrary a bit easy. I point out this paper [1], where the authors study multitask Bayesian optimization. In particular in Section 2 they define a notion of regret which is the expectation (over a distribution of directions) of the scalarized regret. This seems to me a sensible definition, as opposed to the best (or the worst) possible direction. Could the authors comment on this point?\n\n**Questions**\n- one could imagine a setting where feedback (i.e., the gradient) is received only for a subset of the objectives. How would the regret bound look like in that case?\n- does the DR-OMMD achieve improved regret bounds if the objectives satisfy additional regularity assumption (e.g., strong convexity, exp-concavity)? If not, what is the reason?\n- it seems to me that the $\\lambda$ can be interpreted as learning rates. They are many ways to optimally tune the learning rate and adapt to the comparator, see e.g. [2, Chapter 9]. Could it make sense to use them rather than the regularized min-max norm to derive a sequence of optimal $\\lambda_t$?\n\n[1] No-regret Algorithms for Multi-task Bayesian Optimization, Chowdhury and Gopalan 2020\n[2] A Modern Introduction to Online Learning, Orabona 2020",
            "clarity,_quality,_novelty_and_reproducibility": "Globally good",
            "summary_of_the_review": "Overall, I am not fully convinced by the definition of the regret proposed by the authors. I also proposed some directions to explore to complement the contribution.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_WSb3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_WSb3"
        ]
    },
    {
        "id": "V-Nkh5ZrgB",
        "original": null,
        "number": 4,
        "cdate": 1667191113273,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667191113273,
        "tmdate": 1667191113273,
        "tddate": null,
        "forum": "dKkMnCWfVmm",
        "replyto": "dKkMnCWfVmm",
        "invitation": "ICLR.cc/2023/Conference/Paper6099/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers a multi-dimensional version of the online convex optimization. Concretely, the problem studies a sequential interaction between a predictor and an adversarial environment -- at each time, the predictor picks a point in a convex set and the environment subsequently picks M convex functions from the domain to the reals. The goal of the predictor is to make sequential predictions so as to minimize regret with respect to the Pareto frontier of the sum of the M dimensional functions over the time horizon. \n\nThe paper contributes to a rigorous definition of regret, provides a variational formula for the same which then aids in analysis. The first observation the paper makes is that directly plugging in the one dimensional OMD to the standard iterative multi-objective MO descent algorithm leads to linear regret. The key observation made here was that the offline algorithm optimizes the mixing weights for the gradients of the M function to the current iterate. However, in the online case, since the environment is adversarial and the regret is with respect to hindsight, converting naively the offline iterative algorithm to an online one yields linear regret. The paper then shows that the simple idea of solving two regularized optimizations -- one to choose the mixing weights and a separate one to choose the next iterate -- yields low regret.",
            "strength_and_weaknesses": "Strengths -- Very well written paper. The style was nice -- presenting the problem, showing through detailed examples why simple approaches fail and then using those examples to build up to the final algorithm. \n\nWeakness -- The intuition and the effect of \\lambda_0 in the algorithm is not clear.  This is however a nit-pick and I believe can easily be fixed in the write-up. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity -- Very well written\n\nNovelty -- Original insights and crisp communication of those insights\n\nReproducibility -- Yes",
            "summary_of_the_review": "Overall this paper is very well written -- provides a simple algorithm and a new insight (double, separate regularization) for an important practical problem. The style of writing is also very pedagogical and aids the reader in quickly grasping the key concepts.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_GxU6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6099/Reviewer_GxU6"
        ]
    }
]