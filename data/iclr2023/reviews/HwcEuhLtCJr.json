[
    {
        "id": "8eA67H2F4K",
        "original": null,
        "number": 1,
        "cdate": 1666365549022,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666365549022,
        "tmdate": 1666365549022,
        "tddate": null,
        "forum": "HwcEuhLtCJr",
        "replyto": "HwcEuhLtCJr",
        "invitation": "ICLR.cc/2023/Conference/Paper4171/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "        This paper analyzes the cold posterior effect in Bayesian neural networks. This effect is simply that these models perform better at test time if the posterior is sharpened artificially. The authors argue that there are several works form the literature analyzing the causes of this, with mixed results. In this paper the authors use generalization PAC-Bayes bounds to study the cold posterior effect. They show that these bounds correlate well with the generalization error in terms of the parameter lambda, controlling the amount of sharpness. Then, they analyze the influence of these bounds on such parameter, in the case of linearized models. The results show that the bounds incorporates complex interactions, which may explain the mixed results observed in the literature.\n",
            "strength_and_weaknesses": "Strength:\n\n        - Nice theoretical results and experimental evaluation.\n\nWeaknesses:\n\n        - The results obtained may explain the wide variety of explanations observed in the literature for the cold posterior effect, but the paper does not provide insights about to address this problem or improve current models. This is a bit disappointing in this regard.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "        The paper is well written and in general clear.\n",
            "summary_of_the_review": "  I believe that this is a nice paper with nice theoretical results and good empirical observations. However, I am a bit disappointed in the sense that it does not provide any improvement over the current methods that are provided. This questions the practical utility of the theoretical results obtained and the observations made.\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_jFkt"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_jFkt"
        ]
    },
    {
        "id": "D8HAW4AO1R",
        "original": null,
        "number": 2,
        "cdate": 1666541158080,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666541158080,
        "tmdate": 1666541158080,
        "tddate": null,
        "forum": "HwcEuhLtCJr",
        "replyto": "HwcEuhLtCJr",
        "invitation": "ICLR.cc/2023/Conference/Paper4171/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies what happens in Bayesian deep learning if the likelihood is scaled down by a factor of $1/\\lambda$. This is related to the \"cold posterior effect\". The authors empirically show that two different PAC-Bayesian bounds, which also include a scaling of $1/\\lambda$, correlate well with this tempered posterior's performance. Finally, the authors derive a PAC-Bayesian bound for a simplified model neural network and show that it has a complex dependence on the temperature $\\lambda$.",
            "strength_and_weaknesses": "I have a few issues with the paper as-is; I've broken these down into categories below.\n\n**Discussion of previous work.** I think the discussion of previous work is flawed in a few places.\n1. The problem this paper studies is mathematically equivalent to tempering the likelihood in Bayesian inference. Maybe the literature has advanced and I've missed something, but I don't think this is the setting people talk about when they discuss the \"cold posterior effect,\" which is what the current paper says it is about. For example, the authors write \"As discussed in ... Wenzel et al. (2020) the relevant setting for the ELBO is the one we consider...\" I don't think Wenzel et al. (2020) says this. That paper notes that people have considered tempering just the likelihood in the context in variational inference, but they differentiate this likelihood-tempered setting from the cold posterior setting they consider in Appendix E.\n1. Citations of theorems from other papers (Theorems 1 and 2) should explicitly cite the theorem from that work. This is especially important for something like Catoni, 2007, which is 175 pages long; as a reader, there's no practical way for me to find the relevant theorem from that book. Also, have all the conditions for each theorem been recalled here? Theorem 1 looks like Theorem 4.1 from Alquier et al. 2016, which has an assumption on it that isn't recorded in the current paper.\n5. The paper is written as if it is about Bayesian inference. But in the experiments, the authors note \"We first use $Z_{train}$ [the training data] to find a prior mean $w_\\pi$.\" This means the experiments are about an *empirical Bayesian* method; Empirical Bayes is not the same thing as standard Bayesian inference.\n2. \"...the Blackwell-Dubins consistency theroem...\" I think Doob's theorem is a much more common reference for the contraction of Bayesian posteriors.  But it's important to note that posterior asymptotics are going to be different when using regular Bayesian inference than when using an Empirical Bayesian procedure (which is what this paper seems to be about), and results for one method may not be valid for the other.\n3. \"Germain et al. (2016) were the first to find connections between PAC-Bayes and Bayesian inference.\" I don't think this is quite true -- e.g. Grunwald (2012) cited here discusses PAC-Bayes. Is there some more specific context in which this is true?\n\n**Theoretical development (Section 4).** I'm not sure what the takeaways from the authors' theory is, given some issues with the assumptions going into it and the discussion of their results.\n1. Why is taking $\\sigma = 1$ a reasonable model? Is there anything lost by making this assumption?\n2. It is assumed that the gradients of a neural network come from a Gaussian mixture, and the fact that previous work has shown neural network gradients to be clusterable (Zancato et al. 2020) is cited as justification. I don't see why the results of Zancato et al. (2020) imply that the gradients should come from a normal distribution -- data can be clusterable and not come from a mixture of Gaussians. For example, a mixture of uniform distributions will be very different from a mixture of Gaussians.\n3. The proved PAC-Bayesian bound requires the temperature $\\lambda$ to be $O(1/n)$, where $n$ is the number of training datapoints. But \"cold posterior\" effects will show up for large $\\lambda$. So what interesting behavior can we extract from this bound? E.g. for $\\lambda \\in (0, 0.0001)$ (which this bound is restricted to for even moderate $n$), all of the plots in the paper show the same behavior.\n4. There isn't much discussion of the proved bound except that it displays \"complex interactions\" between the model and data. Some more discussion seems needed here. Can we learn anything from these complex interactions? What do the interactions tell us about the cold posterior effect? I think it's clear that the assumptions going into the bound are unrealistic. But if the bound for this toy model matches empirical results in interesting ways that previous bounds do not, this would still be a great contribution to the literature.\n\n**Empirical results.** I wasn't entirely clear on what the takeaway was supposed to be from the experiments. These seem to be evaluating existing PAC-Bayesian bounds; overall, what does each experiment tell us about understanding the cold posterior effect? It seems to me that most of the experiments just validate the cold posterior effect by showing larger $\\lambda$ gives better performance. Is there something that I'm missing here? Also a few more specific points:\n1. \"We might think these results are due to the poor (Isotropic) approximation to the posterior, however as we will see in the next section this behaviour carries over to other approximations to the posterior.\" This next section changes both the model (regression -> classification), the bound used (Theorem 1 -> Theorem 2), and the posterior approximation. Too much is different here to validate that changing the approximation isn't changing things; how do we know that the effect of changing the approximation isn't being canceled out by one of the other changes?\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "**Clarity.** There were a few places in which I had trouble with the notation / development.\n1. \"Observations $(X,Y)$ are assumed...\" Do you mean $(x_i, y_i)$ here?\n2. I don't think it was ever explicitly said where the distribution $p(y \\mid x, f)$ comes from (two sentences later normalizing model outputs is mentioned, but this should be made explicit).\n3. There's some switching between considering distributions over functions and distributions over weights of neural networks. It would be good to settle on one.\n4. Before section 2.1, $\\hat\\rho$ was \"the posterior\". But it seems like in Section 2.1 it is an approximation to the posterior. Can the authors clarify or standardize this?\n5. In the second equality of the equations displayed at the bottom of page 3, the authors write:\n$$\\hat\\rho(w) \\log \\frac{\\hat\\rho(w)}{p(w \\mid X, Y)} = \\hat\\rho(w) \\log \\frac{\\hat\\rho(w) p(Y \\mid X)}{\\pi(w)p(Y \\mid X, w)}.$$\nIt seems like $p(w \\mid X, Y)$ has been expanded using Bayes rule. But:\n$$ p(w \\mid X, Y) = \\frac{p(X, Y \\mid w) \\pi(w)}{p(X, Y)} = \\frac{p(X \\mid w) p(Y \\mid X, w) \\pi(w)}{p(Y \\mid X) p(X)}.$$\nSo it seems like the development in the paper is assuming $p(X \\mid w) = p(X)$. Can the authors correct what I've written here or explain why this assumption is true?\n6. \"samples from $\\hat\\rho$ are inputted to the full neural network.\" I thought that $\\hat\\rho$ was a distribution over neural networks. What does this sentence mean?\n7. In figure 3 and 4, ten blue lines are shown, one for each posterior. But only one red line (PAC-Bayesian bound) is shown. Why are there not ten red lines, which each one showing the PAC-Bayesian bound for each posterior? Is the displayed red line the mean of these ten lines?\n8. In Section 4, $\\phi_i$ is used, but I don't think this was ever defined.",
            "summary_of_the_review": "Overall, I think the paper needs more connection with the literature, and also a deeper explanation of the presented results. As-is, it's not clear to me what the new high-level takeaways are from this paper.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_qVvM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_qVvM"
        ]
    },
    {
        "id": "MGUJq3_7S-u",
        "original": null,
        "number": 3,
        "cdate": 1666560870147,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666560870147,
        "tmdate": 1666560870147,
        "tddate": null,
        "forum": "HwcEuhLtCJr",
        "replyto": "HwcEuhLtCJr",
        "invitation": "ICLR.cc/2023/Conference/Paper4171/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors consider the application of PAC-Bayes theory to study the \"cold posterior effect\" previously observed in Bayesian neural networks. They rely on the aesthetic relationship between cold posteriors and certain PAC-Bayes bounds on the negative log likelihood to carry-out their analysis. They consider the negative log-likelihood loss for regression and the $0-1$ loss for classification. The authors rely on data-dependent priors selected with a validation set, then apply a Laplace approximation to determine the posterior (in the PAC-Bayes sense). From the experiments, the authors conclude that cold posteriors often lead to better generalization bounds, and suggest this as a mechanism for explaining the cold posterior effect.",
            "strength_and_weaknesses": "## Strengths\n\n- The writing is clear; figures are high quality.\n- Experiments seem reasonably thorough.\n\n## Weaknesses\n\n- I found it hard to pin down exactly what insight about (Bayesian) deep learning is gained. The authors show that lower temperatures correspond correlate with better generalization and tighter generalization bounds. I would like the authors to clarify what this correlation teaches us about deep learning above and beyond what is known. The claim that a tighter generalization bound correlates with better generalization does not seem inherently interesting, and I think needs to be tied more clearly to concrete/actionable insight.",
            "clarity,_quality,_novelty_and_reproducibility": "## Quality and Reproducibility\n- Experiments appear to be thorough. The appendix includes details.\n- Monte Carlo estimation of the moment term seems problematic. In particular, the moment term roughly corresponds to a condition insisting that the tails of a distribution are light enough, and therefore could be very high variance or infinite. I am not convinced 100 MC samples would necessary be reasonable for estimating this, without some knowledge of the tail behavior (e.g. this would be quite different in the case of a bounded random variable, where a concentration inequality could be applied). The authors mention this briefly in the supplement, but in a way that suggests that since 100 samples suffice for estimating some statistics of the posterior, they would also be sufficient for estimating this term. However, since the term involves the exponetial of the random variable in question, this seems speculative.\n\n## Clarity\n- Aspects of the combination of Bayesian and frequentist epistemology in the introduction are confusing. For example, the authors argue (from the frequentist perspective) that a Bayesian approach maybe be suboptimal. Then state \"the posterior is our best guess at the true model parameters without having to resort to heuristics\". As far as I know, this is only true from a (subjective) Bayesian view, as a frequentist might prefer, for example, a minimax approach. The authors should be more clear about what assumptions are being made and consistent about the view that is taken or delineate clearly when the viewpoint changes.\n- A better summary of the experiments section would be helpful. There is a lot going on, which is great. But perhaps a table clarifying what parameters were selected using what method and what data (perhaps in the supplement) would improve readability.\n\n## Questions\n\n- Is it clear that the \"moment term\" is even finite for $\\lambda >0$ in the application considered? \n- It may be worth mentioning explicitly that the empirical version of minimizing the divergence in equation 2 is maximum likelihood.\n- Can you clarify how the MAP estimate can lead to the optimal bound? I don't see how this can be the case unless you have a prior variance that is also $0$, in which case the whole method must ignore the validation data and is equivalent to using a validation set-based bound. Otherwise, the Kullback-Leibler divergence should tend to infinity leading to a vacuous bound. \n- Is the PAC-Bays posterior selected based both on training and validation data (as is standard)? I suspect I have missed this in the appendix despite looking for it, and would appreciate a pointer.\n\n## typos \n- p1. ``tells states''\n",
            "summary_of_the_review": "I think this paper is slightly below the acceptance threshold. The writing is good, experiments are thorough and well-thought out. However, I think some details of the experiments (e.g. Monte Carlo estimation of the moment term) lead to technical issues; and it is somewhat problematic that the generalization bounds reported may not be reflective of the actual values due to sampling error. More importantly, I think it isn't totally clear what the take-away of the paper is, in terms of insight that sheds more light on deep learning/Bayesian deep learning. The writing should make this evident. The discussion section points to more things that could be done, but really I think this space would be better spent clarifying how what was done in this paper relates to the bigger picture. It is quite possible I have missed something regarding this, and I hope the authors can clarify this in their response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_oDKT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_oDKT"
        ]
    },
    {
        "id": "FOfT4uT9VU5",
        "original": null,
        "number": 4,
        "cdate": 1666715443619,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666715443619,
        "tmdate": 1666715443619,
        "tddate": null,
        "forum": "HwcEuhLtCJr",
        "replyto": "HwcEuhLtCJr",
        "invitation": "ICLR.cc/2023/Conference/Paper4171/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "In this paper authors do a PAC-Bayesian analysis for the cold-posterior effect, i.e. an observation that sharp posterior typically lead to better predictive performance. Main contribution of the paper is obtaining a PAC-Bayesian bound for Laplace approximation to the posterior. Since Laplace approximation relies on the Hessian, authors study versions of generalized Gaussian\u2014Newton approximations to the posterior. Under these approximations, authors experimentally show that \u201ccolder\u201d posterior yields better smaller PAC-Bayesian bound. Authors also derive PAC-Bayesian bound for stochastic linearlized network. ",
            "strength_and_weaknesses": "## Strengths\n\nThe paper is generally well organized.\n\nPaper tries to study Bayesian phenomenon of cold-posterior from a frequentist point of view of PAC-Bayesian bounds. deriving performance guarantees of an approximation of the Bayesian posterior. \n\n## Weaknesses\n\n- In my opinion, the problem of studying the cold-posterior on itself is not well motivated as Bayesian analysis is more useful for uncertainty quantification rather than pure prediction. Of course, one can improve prediction quality if the posteriors are sharper.\n- For the linearized network, authors merely suggest a connection to the cold-posterior without attempting to tease apart details.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is generally well written and all the results are presented and the results appearcorrect. \n\nIn terms of novelty, the authors study various previously studied approximation to the Hessian and apply those in the context of Laplace approximation of the posterior and thus I consider it to be incremental progress. \n\nIn terms of reproducibility, I see some of the PAC-Bayes bounds appear to be smaller than the test error obtained. Authors should comment on that because PAC-Bayesian bounds ought to be an upper bound.",
            "summary_of_the_review": "I recommend a weak reject for the paper. Main reasons for my recommendation is that the problem being studied is not very well motivated.  And the fact the technical developments are only incremental progress.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_Pea3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4171/Reviewer_Pea3"
        ]
    }
]