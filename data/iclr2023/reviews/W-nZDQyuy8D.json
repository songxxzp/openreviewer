[
    {
        "id": "OMYTFQ7P0e",
        "original": null,
        "number": 1,
        "cdate": 1666667179084,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666667179084,
        "tmdate": 1666667179084,
        "tddate": null,
        "forum": "W-nZDQyuy8D",
        "replyto": "W-nZDQyuy8D",
        "invitation": "ICLR.cc/2023/Conference/Paper1824/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper focuses on open-world class-agnostic object detection tasks. This paper proposes a pseudo-labeling approach to alleviate the issue that base object categories are limited. Different from conventional methods that acquire pseudo labels from RGB-based images, this paper acquires pseudo labels from depth or normal images. The method, named GOOD, achieves state-of-the-art results on several datasets.",
            "strength_and_weaknesses": "*Strength\n1. GOOD reveals that Depth and normal images are superior to edge and PA images for generating pseudo-labels in Table 2. Whereas, it would be better if RGB images are taken as a comparison.\n\n2. The idea to leverage the depth and normal images in the open-world class-agnostic object detection task is simple and innovative. Low-level information deserves more attention in open-world tasks.\n\n3. GOOD achieves state-of-the-art results in several datasets.\n\n*Weaknesses\n1. The experiments are insufficient. For example, in cross-dataset benchmarks in Table 1, GOOD only selects ADE2k as the target test set. However, most approaches [1, 2] mainly consider UVO [3] as the target dataset for the cross-dataset benchmark.\n\n2. Heuristically, pseudo-labeling methods work on the partially labeled training set, such as VOC categories in COCO. The effects need to be further validated if the training set is exhaustively (e.g. UVO dataset) or nearly exhaustively (e.g. COCO to Non-COCO in LVIS dataset) labeled. The authors need to add at least one experiment under such scenes.\n\n3. There are several mistakes in the paper, such as the legend in Fig. 7b.\n\n[1] Wang, W., Feiszli, M., Wang, H., Malik, J. & Tran, D. Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity. CVPR (2022).\n[2] Saito, K., Hu, P., Darrell, T. & Saenko, K. Learning to Detect Every Thing in an Open World. Arxiv (2021).\n[3] Wang, W., Feiszli, M., Wang, H. & Tran, D. Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation. ICCV (2021).",
            "clarity,_quality,_novelty_and_reproducibility": "This paper clarifies its contribution well and provides details to reproduce the experiments. The novelty is somewhat new.\n",
            "summary_of_the_review": "The paper proposes a novel and effective pseudo-labeling method for open-world class-agnostic object detection. The results acquired are superior as well. However, the authors need to conduct the necessary experiments as requested in the weaknesses for a comprehensive comparison. The reviewer will increase the rating if the above concerns are well addressed.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_e8x4"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_e8x4"
        ]
    },
    {
        "id": "DFo-a8Z1z0",
        "original": null,
        "number": 2,
        "cdate": 1666674932700,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666674932700,
        "tmdate": 1666674932700,
        "tddate": null,
        "forum": "W-nZDQyuy8D",
        "replyto": "W-nZDQyuy8D",
        "invitation": "ICLR.cc/2023/Conference/Paper1824/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors present a self-training approach for detecting novel objects in an open-world setting using off-the-shelf monocular depth and normal estimators. The presented method improves over prior work on open-world settings on COCO and ADE20K. ",
            "strength_and_weaknesses": "Strengths: \n- Intuitive, novel method that outperforms prior work on important benchmarks\n- Many reasonable ablations that compare against baselines that help show the value of geometric cues vs. self-training in general.\n- Good analysis of where the method improves, and where the RGB self-train method falls short or outperforms vs. the proposed method.\n\nWeaknesses: \n- Segmentation: One would expect that a key win from dense geometric cues would be improved segmentation of unseen objects. This work (and possibly the results) would be much stronger if evaluated on instance segmentation rather than bounding-box object detection. Admittedly, this is a more complex task, but would increase the impact of this paper.\n- As one might expect, the accuracy improvements drop off significantly as the # base classes increases. It might be nice to plot accuracy on novel classes as the # base classes increases (you already have two points for this plot in Table 1 (b) at n=20 and n=80). \n- Relatedly, it would have been nice to see experiments on the LVIS dataset, which covers many categories, and has a vocabulary that matches to COCO. This would have allowed authors to report AR_N for LVIS, unlike for ADE20k in Table 1 (b).\n- It would have been nice to see how well the model generalizes to objects which are not in the training sets of the depth/normal estimation models. This could even be qualitative, on a few images of objects known to be missing from the original training sets (e.g., a new object from a movie, or a recently introduced tech device).\n- Nits:\n    - I would highly recommend removing Fig 5, or at least replacing it with 5-10 randomly chosen images, instead of 1 (likely carefully selected) image. It\u2019s difficult to generalize from this one example, and can lead unsuspecting readers to lend excess credence to the conclusion drawn from that example.\n    - Fig 7b \u2013 (1) do you take more pseudo boxes from a single RGB model? Wouldn\u2019t a more fair comparison train K independent RGB models?   (2) When using multiple modalities, do you do any filtering? i.e., if the top box from depth and from edges is the same, do you use both as pseudo-groundtruth?\n    - Appendix A \u2013 any idea why k is higher for RGB than depth and normals?\n    - Table 3 \u2013 Given RGB helps improve for smaller classes, does it make sense to train a \u201cGood-All\u201d which uses RGB, depth and normals? Or to sample pseudo-groundtruth based on size (small boxes from RGB model, larger boxes from depth and normals)?  [No need to run this experiment, just curious about the authors\u2019 thoughts]\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- Clarity, quality, novelty: no concerns\n- Reproducibility: Code release would help significantly. In the absence of this, a table that explicitly states the hyperparams for the teacher (pseudo-labeling) model vs. the student model would be critical. Even if the hyperparams are the same, stating them explicitly in a table would simplify reproduction and address any confusion.\n",
            "summary_of_the_review": "Overall, this is a good paper that should be accepted. Given the nice idea and promising results, the work leaves me wanting more (results with instance segmentation, results on LVIS, further analysis, etc.), but the work passes the bar for acceptance as is. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_3ZGc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_3ZGc"
        ]
    },
    {
        "id": "RGi60DSbQa",
        "original": null,
        "number": 3,
        "cdate": 1666683620490,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666683620490,
        "tmdate": 1670545660049,
        "tddate": null,
        "forum": "W-nZDQyuy8D",
        "replyto": "W-nZDQyuy8D",
        "invitation": "ICLR.cc/2023/Conference/Paper1824/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studied to improve the generalization of open-world class-agnostic object detection with geometric cues. In practice, depth and normal maps predicted from pretrained Omnidata model were used to train an object proposal network for pseudo-labeling unannotated novel objects. The resulting Geometry-guided Open-world Object Detector (GOOD) significantly improves detection recall for novel object categories even with only a few training classes.",
            "strength_and_weaknesses": "[Strength]\n1. This paper shows that depth and normals help overcome the overfitting problem of the only RGB-based proposal network, thus generating more unlabeled pseudo boxes which finally improve the detection performance.\n\n[Weaknesses]\n1. The novelty of this paper is limited. The network architecture and training losses used in this paper are the same as OLN. It looks that the only difference is that this paper uses additional geometric cues to train the proposal network, while OLN only uses RGB images. However, to my knowledge, geometric cues have been widely used in many 2D tasks to improve performance. Thus, this paper does not give new messages and insights.\n2. In experiments, it is recommended to study how does the number of base classes influence the performance.",
            "clarity,_quality,_novelty_and_reproducibility": "1. This paper is well-written and easy to follow.\n2. The method proposed in this paper is clear and easy to reproduce.\n3. The main problem of this paper is the limited novelty.",
            "summary_of_the_review": "I tend to reject this paper because of the limited novelty in this paper. \n\nupdate: After discussion, I would like to raise my rating from 5 to 6 because of their great performance. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_FTvx"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_FTvx"
        ]
    },
    {
        "id": "E3GrG8dlr6Y",
        "original": null,
        "number": 4,
        "cdate": 1666983147304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666983147304,
        "tmdate": 1670545558496,
        "tddate": null,
        "forum": "W-nZDQyuy8D",
        "replyto": "W-nZDQyuy8D",
        "invitation": "ICLR.cc/2023/Conference/Paper1824/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper proposes an interesting and novel idea of using geometric cues like depth and normal maps to obtain object proposals for object detection. The authors propose a two phase approach for improving object proposals for unseen objects. In the first stage they train an object proposal generation method which uses geometric cues like depth and normal maps to provide pseudo-labeling. The generated pseudo-labels are added to the training set and the final proposal method is trained using the original boxes and these generated boxes. ",
            "strength_and_weaknesses": "The idea of explicitly using geometric cues for object proposal generation is novel and interesting. The authors have proposed a two phase training pipeline which first uses geometric cues to generate proposals for unlabeled objects. These pseudo-labels and the original ground-truths are used to fine-tune the proposal network in the second phase. The authors show that the proposed approach can achieve improved performance.\n\nThe main weaknesses of the paper are lack of details about the decisions, and an absence of discussion on how to extend the proposed method to more recent proposal-free object detection methods. In particular, the authors should address the following in a revision/rebuttal:\n\n1. How exactly is the proposal network trained in Phase 1? Is the same network trained for both depth and normal? How exactly? Are the depth and normal maps stacked? Or are they just used as separate/independent images? \n\n2. Connected with the first point above - Why not use RGB images as well in the first step, along with depth and normal? Does that lead to worse performance? Why/Why not? Further, what other geometric cues can be used in addition to depth and normal? \n\n3. I want the authors to consider and discuss, how can this proposed approach be extended to proposal-free object detectors? What is the utility of proposal-based object detectors if DETR and other similar object detectors give better performance? Further, I want the authors to consider if the presented approach can help proposal-based object detectors achieve a higher performance than more recent proposal-free object detectors. For example, can a Mask-RCNN containing an RPN trained with the proposed approach get better performance than a DETR model? \n\n4. The authors should demonstrate the effectiveness of the proposed approach on larger and more challenging datasets like LVIS. This will help in making their claims stronger. \n\n\n\nAfter discussion with the authors and consulting with the AC and other reviewers, I am increasing my rating. ",
            "clarity,_quality,_novelty_and_reproducibility": "Most of the paper is clearly written. However, as mentioned above, the authors should add more description/details about some of their decisions regarding the model design. The quality of the paper is good overall but can be significantly improved by incorporating the suggestions above. The proposed approach is novel and can benefit a large portion of the computer vision community. The idea seems simple enough to be reproducible, but I will appreciate it if the authors release their code. ",
            "summary_of_the_review": "I am currently recommending a \"weak-reject\" rating based on the weaknesses mentioned above. However, I would be happy to consider upgrading it to an \"accept\" if the authors are able to satisfactorily address the points mentioned above. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_SvRu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1824/Reviewer_SvRu"
        ]
    }
]