[
    {
        "id": "rQfkacL_-8",
        "original": null,
        "number": 1,
        "cdate": 1666616717798,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666616717798,
        "tmdate": 1666616717798,
        "tddate": null,
        "forum": "4LMIZY7gt7h",
        "replyto": "4LMIZY7gt7h",
        "invitation": "ICLR.cc/2023/Conference/Paper1259/-/Official_Review",
        "content": {
            "confidence": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers.",
            "summary_of_the_paper": "The paper studies robust fair clustering and motivates the issue of robustness under attack. The authors conduct numerical experiments to show that state-of-the-art models are highly susceptible to the attack they design. The authors then propose a Consensus Fair Clustering and numerically show its robustness.",
            "strength_and_weaknesses": "Strength: \n1. The issue of robustness under attack is well-motivated. I believe this could be an important property for any robust fair clustering. \n2. The designed attack is quite novel.\n\nMy major concern is the following:\nThe proposed Consensus Fair Clustering appears to respond well to the designed attack. Is there any guarantee that it remains robust under ANY attack that the adversary would choose?",
            "clarity,_quality,_novelty_and_reproducibility": "The issue of robustness under attack is well-motivated. The designed attack is quite novel.",
            "summary_of_the_review": "The issue of robustness under attack is well-motivated, and I believe this could be an important property for any robust fair clustering. \n2. The designed attack is quite novel. The designed attack is quite novel.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_EUnH"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_EUnH"
        ]
    },
    {
        "id": "5SNRDERMpA",
        "original": null,
        "number": 2,
        "cdate": 1666626399304,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626399304,
        "tmdate": 1666946825611,
        "tddate": null,
        "forum": "4LMIZY7gt7h",
        "replyto": "4LMIZY7gt7h",
        "invitation": "ICLR.cc/2023/Conference/Paper1259/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper tackles **unsupervised fair clustering**. The authors first propose **a black-box adversarial attack** on state-of-the-art fair clustering algorithms, effective on a toy dataset they propose as well as real datasets (MNIST, USPS, Office-31). In their threat model, an adversary can modify *the protected attribute* of a subset of the dataset. Its goal is to minimize a fairness utility function while trying not to degrade the performances. Success of the attack is evaluated on the remaining untouched datapoints.\nIn a second time, the authors propose **a defense against that attack**, an unsupervised fair clustering algorithm which is robust to the attack.",
            "strength_and_weaknesses": "### Strengths\n\n- Being *black-box* and exploring a wide range of ratio of poisoned data, the threat model seems highly relevant;\n- Brings neural network in the field of fair machine learning, which seems to still be understudied (as showed by the state-of-the-art algorithms used to compare to in the paper);\n- The paper is well written and easy to read.\n\n### Weaknesses\n\n- Some comparisons would have benefited being more thorough:\n    - Figure 2 and 4 would have benefited showing results on CFC;\n    - Table 2 and 4 would have benefited comparing with random attacks;\n    - Figure 6 experiments would have benefited from a confidence interval;\n    - Comparing different fair algorithms that could be used to generate the $J$ parameter in equation $(4)$.\n- Even though an in-depth exploration of CFC is proposed in appendix, an ablation study would have been relevant, especially on the different losses being used.\n- Studying the robustness of CFC against other known attacks on clustering (relaxing some of the problem constraints if necessary) would have been highly appreciated.\n- Is the ratio of poisoned samples reasonable? 15% seems like a lot, it would be interesting to see how CFC compare with a low number of adversarial samples (1%?)",
            "clarity,_quality,_novelty_and_reproducibility": "### Clarity\n\nThe paper is well written and quite clear, except for a few details:\n\n- Page 8, right before equation $(3)$, the \u201ccluster $c_k$\u201d are mentioned but I still did not understand where they came from.\n- Page 8, between equation $(3)$ and $(4)$, the meaning of $P(g)$ seems unclear to me. Why would $P$ take $g$ as an argument when the $P$ has allegedly been constructed with the probabilities $p_k^x$ which do not seem to depend on $g$.\n- Page 13, in Balance definition, the symbol $r_k$ seem confusing. Balance has been generalized to more than two groups in Suman K. Bera\u2019s *Fair Algorithms for Clustering* and a similar notation explicitly showing the group considered such as $r_k^g$ would have been preferred.\n\n### Quality\n\nOne of the claim in the paper seems confusing. At page 7, the authors write: \u201c*Since attacked samples are a tiny portion of the whole training data, the probability of these being selected into the subset is also small, which decreases their negative impact.*\u201d But since all of the samples are used in a basic partition (since $\\cup_{i=1}^r X_i = X$), the reason for the robustness of CFC must be attributed to another mechanism. This question is later investigated in appendix G but further work would be required (comparing several values of $r$?)\n\n### Novelty\n\nThis work is undoubtedly novel as the threat model and downstream task do not seem to appear in other works.\n\n### Reproducibility\n\nReproducilbility cannot be evaluated as such without the source code but the given implementation details are highly appreciated. Solutions exist to ensure confidentiality or uploaded in supplementary material.",
            "summary_of_the_review": "The novelty and relevance of this work make it interesting for the scientific community in both Robust Machine Learning and Fairness-related Machine Learning research. Although I pointed a few (alleged) weaknesses, the paper should have tackled, they nonetheless exhibited a relevant attack and experimentally demonstrated a defense to that attack. Further work on this topic will be of great value for the community.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "details_of_ethics_concerns": "**This paper exhibit an attack to which state-of-the-art algorithms are vulnerable but proposes a defense to that attack.** It is of high importance for users of the vulnerable algorithms to be aware of their flaws and the existing solutions. Works proposing both an attack and a defense are rare enough for me to emphasise on how necessary it is.",
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_HmWf"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_HmWf"
        ]
    },
    {
        "id": "D9UD_pOHodu",
        "original": null,
        "number": 3,
        "cdate": 1666661435368,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666661435368,
        "tmdate": 1670359288932,
        "tddate": null,
        "forum": "4LMIZY7gt7h",
        "replyto": "4LMIZY7gt7h",
        "invitation": "ICLR.cc/2023/Conference/Paper1259/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies adversarial attacks on fair clustering. The authors demonstrate simple adversarial attacks on existing fair clustering approaches and propose an algorithm that is robust to the attack.",
            "strength_and_weaknesses": "------------------------\nStrengths:\n------------------------\n-- The paper is mainly well written (except section 3.1) and easy to follow.\n\n-- The idea of adversarial attacks on fair clustering is natural and worth exploring. \n\n------------------------\nWeaknesses:\n------------------------\n-- The paper would be strengthened if the authors provide a clear motivating example.\n\n-- The size of $G_A$ in all the experiments is rather large. In all the definitions, it is assumed that this proportion is small but all of the experiments focus on 15% which is rather large. How do the results change for proportions much smaller than 15%?\n\n------------------------\nMinor Comments:\n------------------------\n-- What is the axis in Figure 2? It would be nice to have labels for graphs.\n\n-- Can you elaborate on why the fairness violation can decrease after accounting for attacks? This intuitively does not make sense as the approach which only optimizes for the fairness violation should achieve a smaller loss.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mainly clear (except for section 3.1). The ideas are somewhat novel as similar attacks have been studied for the clustering objective (with no fairness considerations). I do not see any link to the code but most of the experimental details are described somewhat clearly. ",
            "summary_of_the_review": "Overall, I think this is interesting work, though I am not an expert in the area.\n\n-------------------------\nPost Rebuttal:\n-------------------------\nI thank the authors for their response. After reading all the reviews and responses, I have increased my score from borderline accept to accept.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_XY3W"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_XY3W"
        ]
    },
    {
        "id": "CvuAcHXW2Ek",
        "original": null,
        "number": 4,
        "cdate": 1666684150979,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666684150979,
        "tmdate": 1666684150979,
        "tddate": null,
        "forum": "4LMIZY7gt7h",
        "replyto": "4LMIZY7gt7h",
        "invitation": "ICLR.cc/2023/Conference/Paper1259/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper studies the fairness attack and defense problem. The authors propose a black-box attack against fair clustering algorithms that works by perturbing a small percentage of samples\u2019 protected group memberships. They also proposed a defense algorithm, named Consensus Fair Clustering (CFC), that utilizes consensus clustering along with fairness constraints to output robust and fair clusters. Empirically, existing fair clustering algorithms are highly susceptible to adversarial influence, while the proposed CFC algorithm is highly effective and robust as it resists the proposed fairness attack well.",
            "strength_and_weaknesses": "Strength: \n- The proposed problem of robust fair clustering is interesting and novel.\n- The proposed black-box attack is natural in robust clustering literature.\n- The empirical results that existing fair clustering algorithms are highly susceptible to adversarial influence are convincing.\n\nWeaknesses:\n- The proposed approach CFC does not have any provable guarantees, both in fairness and in accuracy. I expect to see at least some performance analysis for CFC.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The writing is good and clear. The empirical results can verify the proposed conclusions. A comment is that the authors should provide an ethnic discussion since the topic is about fairness.\nQuality: The proposed black-box attack is interesting, but lacks some theoretical analysis. For instance, what is the worst influence of such a black-box attack on some known fair clustering algorithms? \nNovelty: The considered problem is novel.",
            "summary_of_the_review": "The paper studies the fairness attack and defense problem and proposes a novel black-box attack. The empirical results that existing fair clustering algorithms are highly susceptible to adversarial influence are convincing. The major concern is the lack of theoretical analysis on both the influence of black-box attacks and the proposed CFC approach.\n\nOverall, I tend to accept the paper. I think the paper can be further improved by providing some theoretical analysis. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "Yes, Discrimination / bias / fairness concerns"
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_UuYT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1259/Reviewer_UuYT"
        ]
    }
]