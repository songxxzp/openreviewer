[
    {
        "id": "ejV_QSiUWA",
        "original": null,
        "number": 1,
        "cdate": 1665755056911,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665755056911,
        "tmdate": 1665755056911,
        "tddate": null,
        "forum": "odI2OpMFq-D",
        "replyto": "odI2OpMFq-D",
        "invitation": "ICLR.cc/2023/Conference/Paper6227/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "The paper analyses the effect of the width of the data pipeline on the throughput of binary neural networks advocating for a decrease in the number of bits from 32 to 8. A series of adjustments are made to ensure, on the selected model that the performance doesn't drop when switching 32 to 8 bits (including a 2 staged training procedure). The paper also details an implementation technique for such binary convolutions on ARM CPUs. However, the proposed modifications are minor adjustments of existing techniques and are of limited novelty. Moreover, the architecture used are not representative of the current state of the art.",
            "strength_and_weaknesses": "Strengths:\n- Good latency improvement on Raspberry Pi 3 and 4.\n- Overall, the paper is easy to follow.\n\nWeaknesses:\n1. Limited novelty. It is widely accepted by the community that the main remaining bottleneck in the throughput of binary network is the presence of real valued operations and data. For example, [A] removes completly the BN, while [B] proposes a binary-friendly shift based implementation. While other works investigate quantizing to int8 the BNs and the first and last layer, with [C] investigating the impact of the data width both at train and test time.\nThe proposed modification are incremental at best and are of limited novelty.\n\n2. Insufficient evaluation. Many of the recent models such as [D,E] but not limited to, make various architectural changes to the activation functions, learning various scaling factors and exotic layers. Significant testing is required with such networks that represent the current state of the art. In certain cases the gap between the tested models and current state of the art is more than 10% on ImageNet.\n\n3. Insufficient related work. The related work section is incomplete and misses important works. Most of the methods (network and methodology wise) are before 2019.\n\n4. There are results on a per-layer basis, but no aggregated ones per model. What is the speedup in this case?\n\n5. Perhaps its just a strange coincidence, but why the results from Table 2 before and after re-training have exactly the same acc? I would expect the int8 quantization + 2 staged training to have perturbed the results in either directions.\n\n6. 2-staged approaches are common for training binary networks. Where the baselines from Table 2 also trained using a stage approach for fairness?\n\n\n[A] \"BNN - BN = ?\": Training Binary Neural Networks without Batch Normalization, Chen et al, CVPR-W'21\n[B] Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or \u22121, Courbariaux et al, 2016\n[C] Enabling Binary Neural Network Training on the Edge, Wang et al 2021\n[D] \"ReActNet: Towards Precise Binary NeuralNetwork with Generalized Activation Functions\", Liu et al, ECCV'20\n[E] High-Capacity Expert Binary Networks, Bulat et al, ICLR'21",
            "clarity,_quality,_novelty_and_reproducibility": "Overall the paper is easy to follow and most of the details are provided, although a code release would be recommended to allow for full reproducibility. \n\nOn somewhat of a technicality, Section 6 and algorithm 2 are not clearly delimitated as appendix.",
            "summary_of_the_review": "The main concerns with the paper are: insufficient/incremental novelty and insufficient empirical results, that do not reflect the current state of the art.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_yPN2"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_yPN2"
        ]
    },
    {
        "id": "F6A_JlzfvK",
        "original": null,
        "number": 2,
        "cdate": 1666664692730,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666664692730,
        "tmdate": 1666664692730,
        "tddate": null,
        "forum": "odI2OpMFq-D",
        "replyto": "odI2OpMFq-D",
        "invitation": "ICLR.cc/2023/Conference/Paper6227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes some modifications to the execution flow of BNNs for better performance on low-end hardware, specifically ARM CPUs. Firstly, the authors reduce 32-bit intermediate representations after the XNOR popcount in the BNN to 8-bit values to reduce overhead. Secondly, they replace BN with a comparison operation or use 8-bit quantized BN statistics to perform the computations in lower bitwidth arithmetic. The authors also use specific ARM instructions to implement the BNNs more efficiently.",
            "strength_and_weaknesses": "Strengths:\n- The paper is well-written and it is easy to understand the methodologies applied.\n\nWeaknesses:\n- The paper has very limited novelty. Reducing the intermediate bitwidths to 8 bits rather than the original full-precision values is a technique that has been performed for many years in academia and industry to reduce the overheads, and the particular gradual mapping of 32-bit values to 8-bits during training is also not particularly novel. Additionally, the conversion of full-precision BN operations to 8-bit ones is a natural extension of having 8-bit input/output operands and is not a new contribution on its own. \n- The experimental results only show speedups on specific example layers (see Figure 6). But the important question is what are the \"end-to-end\" latency measurement for a \"full network\" and how does that compare with prior art? ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper provides a set of engineering hacks to reduce the overhead of BNN execution on ARM CPUs, however, the proposed techniques are not particularly novel. There are no new technical methods introduced for reducing the overhead. Please see the weaknesses above.",
            "summary_of_the_review": "Please see the weaknesses above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_GiJD"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_GiJD"
        ]
    },
    {
        "id": "Pa_mmQW3ED",
        "original": null,
        "number": 3,
        "cdate": 1666797141652,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666797141652,
        "tmdate": 1666797141652,
        "tddate": null,
        "forum": "odI2OpMFq-D",
        "replyto": "odI2OpMFq-D",
        "invitation": "ICLR.cc/2023/Conference/Paper6227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper tries to accelerate the inference of BNNs by:\n\n1- Quantizing the BatchNorm layer into 8-bits\n2- Simplifying the deployment of BN layer non ResNet like networks\n3- And proposing an optimized assembly implementation of the binary convolution",
            "strength_and_weaknesses": "Weaknesses: The paper lacks novelty for a conference like ICLR. Quantization of BN into 8-bits has already been proposed in previous works and the simplification of BN is obvious. The impact of their implementation of BN layer on the acceleration of BNN is not explored. \n\nStrength: The proposed assembly implementation of the binary convolution. ",
            "clarity,_quality,_novelty_and_reproducibility": "\nThe provided methods and solutions are easy to understand.\n",
            "summary_of_the_review": "Due to the lack of novelty I recommend to reject the paper. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_mSLQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_mSLQ"
        ]
    },
    {
        "id": "eOLpJMH6HR",
        "original": null,
        "number": 4,
        "cdate": 1666808165365,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666808165365,
        "tmdate": 1666808165365,
        "tddate": null,
        "forum": "odI2OpMFq-D",
        "replyto": "odI2OpMFq-D",
        "invitation": "ICLR.cc/2023/Conference/Paper6227/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a training scheme to increase data pipeline of binary neural networks when implemented on ARM. To this end, a two-step clipping method and 8-bit batch normalization have been proposed. The clipping method insures that the values can fit into 8 bits so that batch normalization can be performed using the same bitwidth. The proposed method has no impact on accuracy performance and its implementation on Raspberry Pi shows up to 1.9x speedup compared to prior works.\n",
            "strength_and_weaknesses": "Strengths:\n-- The paper provides an insightful challenges about hardware implementation challenges of binary neural networks on ARM.\n-- It provides a hardware implementation on ARM and yields up to 1.9x speedup compared to prior works.\n-- The paper is well-written and easy to understand.\n\nWeaknesses:\n-- The two-step clipping method was introduced to accommodate for 8-bit batch normalization. However, it doesn't seem to be necessary for VGG-style block. According to Eq. (2), all we need to compute is sign of the batch norm's output, which can be achieved by comparing the input of batch norm with the threshold tau. Of course, tau can be represented using 8 bits. Then, the input of batch norm doesn't need to be represented using 32 bits anymore; its 8-bit representation suffices. I am also assuming the scaling factor of binarization is merged with BN, is it a right assumption? For the ResNet-style block, the batch norm is mixed-precision not 8-bit according to Fig. 3. Then, what's the point of representing the input using 8 bits?\n\n-- 8-bit batch norm has been proposed before in literature (e.g., https://arxiv.org/pdf/1805.11046.pdf), which undermines the novelty of this work.",
            "clarity,_quality,_novelty_and_reproducibility": "I believe the paper falls short in terms of novelty (see my comments listed as weaknesses). The contribution of this easy to understand but there are some aspects missing such as scaling factor of binarization.\n",
            "summary_of_the_review": "I believe this paper suffers from the lack of motivation and also novelty. The statements of this paper don't explain why two-step clipping is required to make the inputs of BN into 8 bits while BN is in mixed-precision (8-bit and 16-bit). Besides, 8-bit batch norm has been introduced before in different works.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_vs6q"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper6227/Reviewer_vs6q"
        ]
    }
]