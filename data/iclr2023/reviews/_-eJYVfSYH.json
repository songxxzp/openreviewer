[
    {
        "id": "7zdZ_X3l7S9",
        "original": null,
        "number": 1,
        "cdate": 1666458080866,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666458080866,
        "tmdate": 1670262517083,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper investigates the existing conclusion that decentralized stochastic gradient descent (D-SGD) degrades the generalizability, which conflicts with experimental results in large-batch settings that D-SGD generalizes better than centralized distributed SGD (C-SGD). Specifically, the authors present new theoretical analysis to reconciles the conflict. They show that D-SGD introduces an implicit regularization to enable the escaping of the sharpness of the learned minima. Additionally, they theoretically prove that the implicit regularization is amplified in large-batch settings when the linear scaling rule is used. In their work, they also show that D-SGD converges to super-quadratic flat minima eventually. To validate the analysis presented in the paper, the authors utilizes a benchmark dataset with a few models to show empirical results, compared to C-SGD.",
            "strength_and_weaknesses": "I think overall the investigated topic in this paper is quite interesting. Basically the implicit regularization in D-SGD has not sufficiently been investigated based on the empirical results obtained by using D-SGD and C-SGD. Most existing works have paid much attention to the convergence analysis, while ignoring some findings from the experimental results. The authors have showed some new analysis to explain the better generalizability in D-SGD and tried to explain mathematically why this happened. The paper is also easy to follow and well written. However, in the current form, there are quite a few confusions in the paper that require more works from the authors to make it technically solid and sound.\n\n1. It looks like the definition in this paper for C-SGD, namely the centralized SGD, still involves the communication, which is basically the star-network or Federated Learning setting. While this could somehow be confusing. As centralized SGD typically is referred to as a setting that has no local agents or workers, but just only one agent (could be the centralized server) WITHOUT any communication happening during training. Thus, the C-SGD in this work can be more appropriately replaced with \"distributed\" to avoid confusion. Or some more clarification needs to be detailed in the work.\n\n2. Eq. (1) shows the update for the centralized distributed SGD (C-SGD). It should be noted that from the network topology in Figure 2, communication happens between the server and local agents. The authors should detail this part as well. Distributed learning can involve either just data parallelism, model parallelism or FL. Based on what has been shown in the work, I think it is exactly the FL setting, in which communication would be there between server and agent. Also, there is a typo below Eq.(1) at the bottom of page 3, it should be C-SGD.\n\n3. After Theorem 1, the authors mentioned that \"Theorem 1 shows that the decentralization navigates..., in order to lower the regularization term...\" How? It is not obviously observed from the theorem statement. The authors should give more detail on this. Additionally, the authors claimed that this is the first work to show the equivalence between D-SGD and C-SGD on a regularized loss with implicit sharpness regularization. Again, the authors should give more technical detail to show instead of just saying that. Where is the regularized loss?\n\n4. How to arrive at Eq.(4)? Where is the gossip matrix \\mathbf{P}? It doesn't look like the update of the consensus model for D-SGD. I don't see communication in the formula.\n\n5. The authors mentioned in the paper that the quadratic approximation in the analysis of mini-batch SGD fails to capture how decentralization affects the training dynamics of D-SGD. I am confused here. Based on the statement from the author, then D-SGD performs poor for all scenarios where quadratic losses are there. Note that a lot of regression task is based on mean square error, which is a quadratic loss.\n\n6. The authors repeatedly claimed D-SGD to implicitly regularize the regularization term in the work. It is not obvious for readers, I believe. Can the authors clarify by presenting more detail?\n\n7. How to justify the assumption in Theorem 2? Is it generic in D-SGD? I don't think so.\n\n8. Eq.(6) is obtained by combining Eq.(3) and Eq.(4). How to get that? Substituting Eq.(4) into Eq.(3)?\n\n9. Why did the authors define a new notation in the paper, the so-called escaping efficiency? Such a measure has widely been used in evaluating various stochastic optimization algorithms. Any particular reason to call it so here?\n\n10. In Definition 3, please define all notations necessarily to make the statement clear.\n\n11. For experimental results, though we see the validation for the analysis, to me it is not that promising due to only one dataset. Can the authors add more datasets to validate? Also, I don't think the authors need to impose open problems in the main contents. Instead, they can add them as future directions in the last section. Without any preliminary discussion for these two open problem, that looks a bit weird. For different topologies, the authors have said in the paper that they observed the similar trends. Please add them into the paper to provide more evidence for the theory. \n\n12. Overall, this paper did really try to show some new and good analysis for D-SGD. However, it looks like the span in this work is way too big. Each subsection in Section 4 can be expanded into a standalone paper with comprehensive and thorough theoretical analysis and experimental results. I understand the authors may want to present their findings in time. While with the limit of space, this might hurt the soundness of the work.\n\n******************************Post-rebuttal*****************************\nThanks much for the authors' responses and revisions. I appreciate that. After carefully reviewing the responses from the authors and other reviewers' comments, I will keep my current score. Though additional changes have been made to the draft, it still didn't look technically sound and solid. Particularly, the empirical evidences to validate the theory are still weak, and some analysis in the work still requires more clarification. ",
            "clarity,_quality,_novelty_and_reproducibility": "Overall, the quality of the this paper is good. It is easy to follow, but requiring more work to make it technically solid and sound. The clarity of the work should be improved based on the comments provided above. The novelty is also good as the authors tried to address an issue reflected from experimental results and provide new analytical results.",
            "summary_of_the_review": "This paper investigates the existing conclusion that decentralized stochastic gradient descent (D-SGD) degrades the generalizability, which conflicts with experimental results in large-batch settings that D-SGD generalizes better than centralized distributed SGD (C-SGD). The authors provided theoretical analysis for the implicit regularization in D-SGD and showed that D-SGD can avoid the sharpness of the learned minima. They also presented some preliminary experimental result to support their findings. The paper looks good, but more work is required for clarity and novelty. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_hoUP"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_hoUP"
        ]
    },
    {
        "id": "wVpaPsdamOQ",
        "original": null,
        "number": 2,
        "cdate": 1666467879669,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666467879669,
        "tmdate": 1666467879669,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies whether decentralized stochastic gradient descent (D-SGD) can lead to better generalization compared to the centralized SGD. The authors showed that D-SGD implies an implicit regularization and used this result (Theorem 1) to argue that the implicit regularization promotes the generalization. The authors also study the escaping efficiency of D-SGD and showed that D-SGD favors the so-called super-quadratic flat minima.  ",
            "strength_and_weaknesses": "In my view, the paper studies a very important topic. Whether decentralization can help or hurt generalization is a very interesting research topic. The authors' idea to show that the decentralized SGD can introduce an implicit regularization, which might help generalization is a novel idea, and has some potential.\n\nHowever, I am not satisfied with the overall quality of the paper. \n\nFirst of all, there is little technical novelty or contribution here. The authors only looked at the average of iterates, instead of the individual iterates, and as a result, when you average out over the doubly stochastic matrix, it disappears from the analysis, and makes the analysis easy. However, it is well known that the average iterates are close to the individual iterates (see e.g. Yuan et al. \"On the convergence of decentralized gradient descent\" for the deterministic gradient case and Fallah et al. \"Robust distributed accelerated stochastic gradient methods for multi-agent networks\" for the stochastic gradient case). It would be nice if the authors can obtain some results for individual iterates instead of the average iterates. \n\nSecond, some conclusion and implication from the main result are not convincing to me. For example, in Theorem 1, the authors claim that D-SGD implicitly regularizes $\\lambda_{H(w_{a}(t)),1}\\cdot\\text{Tr}(\\Xi(t))$. However, if you look at the proof, this term is only an upper bound. It is not very convincing to me. Also, Theorem 1 looks like writing D-SGD as centralized SGD plus some additional term, and this idea is not new, see e.g. Fallah et al. \"Robust distributed accelerated stochastic gradient methods for multi-agent networks\". \n\nWhat's even more disappointing is that I am not 100 percent sure that the main result, i.e. Theorem 1 is correct. The proof is not rigorous. For example, in the proof of Theorem 1 in the Appendix, the authors did not explain how to control the remainder term R. What's even more troublesome to me is that in the proof, the authors wrote that \"Assuming for the sake of intuition, we expect each dimension of $(w_{j}(\\tau)-w_{a}(\\tau))$ to be uncorrelated.\" Why? Please explain. This is not clear to me at all. You cannot simply assume something that you do not know how to prove. Also in the proof, the authors wrote that \"if the topology is symmetric\". Please explain what you meant by saying \"if the topology is symmetric\". It seems the authors lack mathematical rigor and basic training in mathematics. In the statement Lemma C.4., the authors wrote that \"D-SGD can be viewed as the discretization of the following SDE...\" This is not a rigorous mathematical statement at all. It is okay to write this sentence as an informal discussion within a paragraph in the main body of the paper, but one cannot write a statement like that in a lemma. If you want to state it in a lemma, you should make the statement formal and rigorous. For example, see Li et al. \"Stochastic modified equations and adaptive stochastic gradient algorithms\", where they used weak convergence of certain order to describe SGD being approximated by an SDE. \n\nFinally, there is a disconnection of Theorem 1 and the other results in the paper, like escaping efficiency in Theorem 3. You should make the connections more clear.  ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is of poor quality, and as I mentioned previously, some results and proofs are non-rigorous. The topic is worth investigating and some ideas are novel, but the paper is poorly executed. I have some further comments as follows.\n\n(1) In Proposition C.5., should it be D-SGD instead of C-SGD?\n\n(2) In references, some letters need to be capitalized. For example, Entropy-sgd should be Entropy-SGD in Chaudhari et al., (2019) schur should be Schur in Davis (1962), and sgd should be SGD in Goyal et al. (2017).\n\n(3) In Theorem 1, because you consider the average iterates, instead of individual iterates, the doubly stochastic matrix does not seem to play a role? How does the network structure affect your main result?",
            "summary_of_the_review": "The paper studies an important and interesting topic. But I have doubt about the mathematical rigor and main results in the paper. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_m7pE"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_m7pE"
        ]
    },
    {
        "id": "8aF_xy3-rm",
        "original": null,
        "number": 3,
        "cdate": 1666918380915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666918380915,
        "tmdate": 1666918380915,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies decentralized stochastic gradient descent (D-SGD), in which different nodes have different subsets of the training data and communicate with their neighbors (through a graph structure) in order to minimize a loss function. The paper makes the case that D-SGD has an implicit regularization effect that causes it to minimize the sharpness (largest eigenvalue of the Hessian of the loss) and therefore prefer flatter minima.",
            "strength_and_weaknesses": "The strength of the paper is that it tackles a relevant problem (generalization in decentralized settings) and makes an interesting claim about the solutions that D-SGD finds.\n\nThe main weakness of the paper is that the argument presented therein is sketchy and not at all convincing. I have the following questions:\n- In eq. (3), are we considering the regime where the step size \u03b7 is large or small? If \u03b7 is small, then wouldn\u2019t the O(\u03b7^{\u00bd}) term dominate the main \u201cregularized loss\u201d term? In any case, the last term also seems to be of order O(\u03b7), so why is it not considered part of the \u201cregularized loss\u201d? It is completely unclear why we ignore certain parts of eq. (3) and focus on others, and **this point alone completely invalidates the reasoning of the paper**.\n- In step (3) at the end of section 4.1, how is the upper inequality established? Please provide a proof.\n- In the SDE after eq. (6), why does the noise involve \u03b7? When deriving an SDE limit, aren\u2019t we taking \u03b7 to zero?\n- The definition of escaping efficiency does not capture the notion of escaping at all. First of all, the definition of escaping efficiency only talks about the value of the loss, it does not talk about whether or not we \u201cescape\u201d (i.e., travel away from) w^*. Second, the escaping efficiency does not \u201ccharacterize\u201d a probability, because Markov\u2019s inequality is not guaranteed to be tight. Third, what is the interpretation of escaping efficiency? Is a large escaping efficiency supposed to mean that we are more likely to escape? If so, Prop. 4 seems to indicate that D-SGD escapes from super-quadratic minima, which is the opposite conclusion as asserted in the text.\n\nEssentially this paper makes a lot of claims based on extremely shoddy or missing reasoning, which makes this paper **not rigorous**.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is also poorly written and unclear: many notions are not defined clearly. For example, I cannot find the definitions of H and T in the main text, which is an egregious omission since it is not standard notation. Some other comments, typos, etc.:\n- Pg. 1, \u201cdecentralization nature\u201d -> \u201cdecentralized nature\u201d\n- Pg. 1, \u201cThis conflict signifies that the major characteristics\u2026\u201d What major characteristics?\n- Pg. 1, paragraph after the box is redundant.\n- Last bullet point in section 1, \u201ccontinuou-time SGD\u201d\n- Section 2: \u201cflat minimum varies slowly\u201d No, a flat minimum does not vary; the loss around a flat minimum varies.\n- Underneath eq. (1), what does it mean that \u03b6 is a random variable? From the equation it seems that \u03b6 is a summation index?\n- Top of pg. 4, \u201cequals to the\u201d -> \u201cequals the\u201d\n- Eq. (2), extra comma\n- Section 4, \u201cminimizes the sharpness of Hessian\u201d -> remove the words \u201cof Hessian\u201d\n- Section 4.1, rename section to \u201cD-SGD is equivalent to C-SGD on a regularized loss\u201d and edit the subsequent paragraph accordingly\n- Thm. 1, what does it mean that the probability is greater than 1 - O(\u03b7)? Where is the randomness coming over? On the LHS of the equality there is already an expectation\n- Pg. 4, inconsistent notation: is the subscript of the maximum Hessian eigenvalue 1 or max?\n- Pg. 6, missing * for the consensus distance\n- Defn. 3, U is not defined\n- Pg. 9, what is a \u201cspare\u201d topology?\n",
            "summary_of_the_review": "In summary I found the paper to be very unclear, and it based upon unclear reasoning. Hence I recommend rejection.",
            "correctness": "1: The main claims of the paper are incorrect or not at all supported by theory or empirical results.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "1: strong reject"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_baeM"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_baeM"
        ]
    },
    {
        "id": "UJUHxyPk2o",
        "original": null,
        "number": 4,
        "cdate": 1666994209006,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666994209006,
        "tmdate": 1666994209006,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors did theoretical analysis for the D-SGD algorithm and the results are claimed to show an implicit regularization during D-SGD optimization process that penalizes the learned minima\u2019s sharpness. The escaping efficiency of the D-SGD algorithm is also analyzed.",
            "strength_and_weaknesses": "The topic is very interesting and it has important implications for understanding the impact of decentralized algorithms on optimization problems. However the main result (Theorem 1) confuses me, since it says with high probability, the expectation of w(t+1) = w(t) - \\eta * gradient & regularization term + O(\\sqrt{\\eta}) + O(\\eta * weight diversity related term). This does not seem to make sense because when \\eta (which is the step length) becomes very small, the result seems to indicate w(t) will diverse and the gradient related term becomes minor compared to the other terms. This contradicts with our common intuition that a smaller step size leads to easier convergence.",
            "clarity,_quality,_novelty_and_reproducibility": "1. Should remove \"The code will be released publicly\" from the abstract. Instead, put a link to the code.",
            "summary_of_the_review": "See section \"Strength And Weaknesses\"",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_qFYy"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_qFYy"
        ]
    },
    {
        "id": "9r9SgoFTaJ",
        "original": null,
        "number": 5,
        "cdate": 1667033108781,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667033108781,
        "tmdate": 1667033108781,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the generalization performance of decentralized training compared with the centralized one, especially for the large batch setting. The authors show that there is an implicit regularization that penalizes the sharpness of the learned minima and the consensus violation, and this regularization is amplified by the large batch size. Also, the authors claim that decentralized SGD is more likely to find the super quadratic local minima. ",
            "strength_and_weaknesses": "The major strength of this work is studying the generalization performance of decentralized SGD compared with the centralized case by analyzing the eigenvalues of the hessian. The technical contributions include implicit regularization analysis, amplified regularization by large batches, escaping efficiency, and numerical evaluations.\n\nThe most concerns of this work are as follows:\n\n1) This is straightforward to see that the consensus violation would serve as the implicit regularization during the training phase, but how much this play on the generalization error is not analyzed. There is a gap between this observation and generalization performance.\n\n2) The large batch amplification is not clear. On page 20, it said that the regularization coefficient is $\\eta\\textrm{Tr}$. Please let me know when it was mentioned in the Thm.1. Note that the consensus error defined in this work is not dependent on the step size. Also, even it was true, $\\eta^3$ is overclaimed as at least the size of the gradient of the original loss is at least proportional to $\\eta^2$. Again, this issue originated from which metric the authors want to compare, weight, loss function, gradient, or generalization error.\n\n3) Even more problematic is that def.1 is rather weak. Is it from a theoretical analysis or just empirical observation? either one is fine, but here it serves as a definition. Assume this was a definition. It said a fixed learning rate to batch size ratio. From the equation shown in the proof of thm 2 (page 20. please use equation indices), there is no batch size. Def. 1 does not imply that here the learning rate can be replaced by the batch size. Also, the larger the $\\eta$ is, the less likely the thm 1. holds as claimed by the authors on page 6.\n\n4) It does not make sense to have a $w^*$ here as the authors consider the nonconvex problems. There is no clue that DSGD can converge to this point, even it is a local minimum. \n\n\n  ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: The definitions are clearly stated. Are these the standard ones? Also, the proofs presented in the appendix are not rigorous. Detailed explanations are encouraged to add. \n\nQuality: this paper only introduces the idea of recognizing the consensus violation as an implicit regularization but does not quantify that the generalization error has indeed a direct causal effect with this term.\n\nNovelty: to my knowledge, this way of looking at the decentralized generalization performance is new.\n\nReproducibility: no detailed hyperparameters are provided either in the main text or appendix. The code is not attached. There is no way to reproduce it.\n\n",
            "summary_of_the_review": "In summary, this paper considers a critical issue of addressing the generalization performance of decentralized SGD, however, the provided argument is not direct linking the consensus violation and generalization error. The theoretical argument is not rigorously justified even with good intuitions. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_Phej"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_Phej"
        ]
    },
    {
        "id": "1gjJPtXKHM",
        "original": null,
        "number": 6,
        "cdate": 1667413586617,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667413586617,
        "tmdate": 1667413586617,
        "tddate": null,
        "forum": "_-eJYVfSYH",
        "replyto": "_-eJYVfSYH",
        "invitation": "ICLR.cc/2023/Conference/Paper4492/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper provides theoretical and empirical justification of decentralized stochastic gradient descent achieves better validation accuracy than distributed centralized stochastic gradient descent. This paper first proves that D-SGD is equivalent to C-SGD with regularization In Theorem 1. In particular, this regularization term is penalizing both the global model curvature and the consensus distance between the global model and local models. The proof idea is by characterizing the gradient diversity between the global model and local models with second-order Taylor expansion. This paper studies the regularization effect under large batch setting in Theorem 2. In particular, when linear scaling rule is applied, the regularization coefficient of $\\lambda_{H,1}$ is cubicly depending on the batch size under the assumption that average local models gradient norm is not decreasing faster than exponent. This paper studies the escaping efficiency of D-SGD in Theorem 3 and Proposition 4. In particular, the grandsum is positive on super-quadratic minima and negative on sub-quadratic minima. ",
            "strength_and_weaknesses": "Strength:\n1. First work on the implicit sharpness regularization and escaping efficiency of D-SGD.\n\nWeaknesses:\n1. Theoretical justification. Rather than weakness, I would like to raise several questions on the theorem provided. Firstly, in Theorem 1, how does the communication part affect the model update? It seems that the consensus model is merely an average over local models why local models are communicated. I do not see how the later affects the reformulation of Eq. (4). Secondly, in Theorem 2, how does the largest eigenvalue of the Hessian matrix change according to the batch size? Since Theorem 1 suggests the implicit regularization depends on two terms multiplicatively, I think it is necessary to consider the effect of batch size on both of them rather than just one. If $\\lambda$ is not constant, it is hard to draw the conclusion that \"the sharpness regularization effect of D-SGD is amplified in large-batch settings\". \n2. Lack of necessary D-SGD examples. While there are at least 4 different topologies and doubly stochastic matrices for D-SGD, I am not sure which one is considered in theorems and experiments. Firstly, Theorem 2 is apparently depending on the spectral gap, which supposes to be different when the topology changes. Can the author provide some characteristics on the scale of $\\lambda$? Secondly, it seems that experimental results for sparse topologies can not be found in the current material, while this paper claims \"we also conduct experiments on grid-like and static exponential topologies\". \n3. The empirical evaluation in Figure 1 is not convincing enough. Firstly, the validation accuracy is way below the known accuracy based on the same network structures (e.g.  ResNet-18 can achieve 94% and at least 90%), while the curves seem saturated already. Especially this paper claims that \"C-SGD equals to the single-worker SGD with a larger batch size\". While I understand the purpose of this paper is not to pursuit SOTA accuracy, but the gap is huge makes me question about the training setup and tuning. Secondly, smaller batch (1024) significantly outperforms larger batch (8192) for both C-SGD and D-SGD. While this paper argues that \"these regularization effects are shown to be considerably amplified in large-batch settings\" for D-SGD at least and \"flat minimizers tend to generalize better than sharp minimizers\", I doubt the logic in this paper is complete. Overall, I do not see the empirical evaluation in this paper matches the results in (Zhang et al., 2021), while the later one serves as the motivation of this work. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity wise is great. The research question is well-motivated and literature review seem solid. I appreciate the author provides proof sketch for friendly explanation. \nQuality wise is fair. Please see my questions in Strength And Weaknesses.\nOriginality wise is good. Although this paper is based on existing technique and framework, the main analysis using Taylor expansion is non-trivial. ",
            "summary_of_the_review": "This paper studies an interesting research problem of D-SGD generalization based on recent advances. But before we can trust the insight of this paper, I believe some justification of the results needs to be done. Please see my questions in Strength And Weaknesses.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_ZZMZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4492/Reviewer_ZZMZ"
        ]
    }
]