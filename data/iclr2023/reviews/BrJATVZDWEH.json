[
    {
        "id": "qje8U0wYNd",
        "original": null,
        "number": 1,
        "cdate": 1666472550235,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666472550235,
        "tmdate": 1666472550235,
        "tddate": null,
        "forum": "BrJATVZDWEH",
        "replyto": "BrJATVZDWEH",
        "invitation": "ICLR.cc/2023/Conference/Paper1366/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper theoretically proves that sub-task decomposition can facilitate training of an elmon RNN by reducing the number of gradient updates to polynomial order while undecomposed tasks can't be learned with polynomial updates.",
            "strength_and_weaknesses": "Strengthes:\n1. Decomposing complexe reasoning tasks into subtasks is currently very popular in NLP, and some theoretically analysis of this paradigm can be helpful.\n2. The paper is overall well written and the proofs/theorems looks good to me.\n\nWeaknesses:\n1. The authors motivate their research problem by sub-task decomposition of LLMs. However, most current practice of using sub-task decomposition with LLMs are using in-context learning without gradient updates, which is very dissimilar with the settings analyzed in the paper.\n2. The model architecture analyzed in the paper is elmon RNN, which is also very different in nature compared with Transformer models (i.e., sequential computation in RNN v.s. parallel encoding with Transformers). The paper could be made more interesting if the proof can be generalized to Transformer architectures. ",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: Good\nQuality: Ok\nNovelty: Ok\nReproducibility: Good",
            "summary_of_the_review": "The paper provides some interesting theoretical analysis of the usefulness of sub-task decomposition for seq2seq tasks. However, the settings are somewhat oversimplified and deviate significantly from the main motivation of the paper (sub-task decomposition for NLP/reasoning tasks with LLMs.)",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_iwZz"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_iwZz"
        ]
    },
    {
        "id": "IiAW_UOLti0",
        "original": null,
        "number": 2,
        "cdate": 1666659282350,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666659282350,
        "tmdate": 1666659282350,
        "tddate": null,
        "forum": "BrJATVZDWEH",
        "replyto": "BrJATVZDWEH",
        "invitation": "ICLR.cc/2023/Conference/Paper1366/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper studies how intermediate supervision (i.e. breaking a task down into steps) can theoretically increase the power of learning complex tasks.\n\nThe authors give a real world motivating example of math problems i.e. \n\"John and I split 12 apples between us, and he gave 1 to his son. How many apples does he have now?\" (5)\n\nwhich can be broken down into:\n\n1. John and I split 12 apples between us. How many apples does he have now (6)\n2. John had 6 apples and he gave 1 to his son. How many apples does he have now? (5)\n\nThis has been empirically observed in the past and has gained recent prominence due to impressive results with large language models (i.e. chain of thought prompting)\n\nThe authors assume a particular model (a simple RNN) and theoretically prove the following:\n\nThere exists a binary classification problem of size d such that with intermediate supervision it is possible for seq2seq model to get an arbitrary low error with a polynomial number of gradient updates in d. However, without intermediate supervision, then for any polynomial time learning algorithm the error will be higher than 1/2 - epsilon. \n\nThey then explore the specific case of bit subset parity, showing learning curves for BERT that demonstrate that without intermediate supervision the number of training steps required grows very rapidly as the size of the problem increases, whereas with intermediate supervision the number of training steps grows at a much more favorable rate.\n",
            "strength_and_weaknesses": "Strengths\n-Well written and interesting. \n-Studies a phenomenon that is of great interest to the community right now. \n\nWeaknesses\n-The theory is interesting but does not lead to any new methods. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is written clearly, is high quality and is novel. ",
            "summary_of_the_review": "I support accepting the paper. It is well motivated, well-written and sheds theoretical insight onto an interesting and relevant problem. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_G5w1"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_G5w1"
        ]
    },
    {
        "id": "L5qVbLb-WT",
        "original": null,
        "number": 3,
        "cdate": 1666929431070,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666929431070,
        "tmdate": 1666931833251,
        "tddate": null,
        "forum": "BrJATVZDWEH",
        "replyto": "BrJATVZDWEH",
        "invitation": "ICLR.cc/2023/Conference/Paper1366/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper analyzes the learnability of sequence-to-sequence tasks, based on whether or not the answers to sub-tasks are provided as part of the input sequence. It proves a rather general result: That *any* function in the P time complexity class is efficiently learnable by a neural network when sub-task supervision is provided, but there exists functions in P which are not learnable (in polynomial time) without sub-task supervision.\n\nThe paper gives significant attention to the \u201cbit-subset parity\u201d problem: this is the problem of determining whether a bit-string of length d has an even or odd number of ones in an unknown subset of d/2 indices. It shows that this problem is efficiently learnable by an RNN when sub-task supervision is provided, but is not learnable (under standard cryptographic or computational hardness assumptions) in polynomial time without this supervision. In this case, the sub-task supervision is the parity of pairs of the unknown indices, recursively followed by the parity of these sub-task parity answers in a binary-tree like structure: $par(x_i1, x_i2), par(x_i3, x_i4), \u2026, par(par(x_i1, x_i2), par(x_i3, x_i4)), \u2026$ (see Figure 2, Section 5). The paper then uses the hardness results regarding this bit-subset parity problem from Shalev-Shwartz et al. (2017) to prove it is not efficiently learnable.\n\nAlthough the primarily contributions of this paper are theoretical, this paper also provides experiments showing that a powerful transformer network (BERT-base architecture) is unable to learn the bit-subset parity problem for $d \\geq 32$ (within ~2M gradient steps), but can easily learn it when the sub-task supervision is provided (e.g., can learn $d=128$ in  $< 100k$ steps).\n",
            "strength_and_weaknesses": "Strengths\n- I was impressed by the generality of their positive results: that any function in P is learnable when suitable sub-task supervision is provided. Their use of boolean circuits to prove this result was very clever.\n- Their use of the bit-subset parity problem throughout the paper was effective at making their theoretical result more concrete and easier to digest.\n- Although the primary results are theoretical, I do feel that these results have important empirical implications regarding the limits of sequence-to-sequence models in solving complicated tasks that require multiple intermediate reasoning steps, when intermediate sub-task supervision is not provided. It also serves as encouragement for empirical work that provides this intermediate supervision to large language models.\n\nWeaknesses\n- It is not immediately clear how the theoretical constructions presented in this paper can be applied to solve real problems in NLP/ML. Perhaps the authors could comment further on how they believe intermediate supervision can be provided in these practical settings.\n- NIT: I felt that the clarity could be improved in a few places. For example, I felt that the hypothesis class defined by equation (6) could have been more carefully explained (for example, what is e_z, and what is w?).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "In general the paper is quite clear, of high-quality, novel, and reproducible (full proofs provided, source-code for experiments is provided). ",
            "summary_of_the_review": "This paper provides important theoretical results regarding the learnability (or unlearnability) of sequence-to-sequence tasks by neural networks, depending on whether or not sub-task supervision is provided. These results are quite general, and provide important insights for future empirical research in sequence to sequence learning.  As a result, I definitely recommend acceptance for this paper (although my review is only medium confidence, given that I am not super familiar with the related work).",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "4: The contributions are significant, and do not exist in prior works.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_T2bv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_T2bv"
        ]
    },
    {
        "id": "vZ2iOlZn9M",
        "original": null,
        "number": 4,
        "cdate": 1667048528839,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667048528839,
        "tmdate": 1667308963873,
        "tddate": null,
        "forum": "BrJATVZDWEH",
        "replyto": "BrJATVZDWEH",
        "invitation": "ICLR.cc/2023/Conference/Paper1366/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper aims to address the compounded natural language problems required by introducing sub-task decomposition and concatenating intermediate supervision to the input and training the sequence-to-sequence model on this modified input. \nThe contribution of this paper can be concluded as:\n* proving a positive theoretical result on decomposing compounded tasks with neural networks in an end-to-end manner\n* The above theoretical results can apply to a broad family of tasks, and currently including SOTA  language models on complex multi-hop tasks in NLP\n* generalizing the above results, and demonstrating that sequence-to-sequence tasks with sufficient intermediate supervision  allow learning any function in the P time complexity class\n\n",
            "strength_and_weaknesses": "#Strength\n* The paper address a valuable problem: the compounded natural language problems requiring introducing sub-task decomposition and concatenating intermediate supervision\n* Sufficient proofs are provided in the appendix\n\n#Weaknesses\n* The theoretical analysis of sequence-to-sequence models seem not strong enough, and many of first-mentioned symbols in equations have inadequate explanation. Maybe it is because you have moved many details to appendix. Analogously, the experiments face the same problem.\n* The inspiration from this paper is limited, and it seems that the theoretical and experimental results are apparent\n* The structure of the paper can be improved. BTW, I think it would be more legible to organize the sequence (1,2,3,4) in a vertical way in Figure 1.\n",
            "clarity,_quality,_novelty_and_reproducibility": "* Clarity: somewhat clear. The paper is easy to follow, but some important details are missing or unclear.\n* Quality: somewhat technically sound. \n* Novelty: somewhat good. The inspriation is limited. \n* Reproducibility: key resources (e.g., proofs) are available and it would be better if sufficient details (e.g., experimental setup) are described such that an expert should be able to reproduce the main results.\n",
            "summary_of_the_review": "This paper has addressed a valuable problem: sub-task decomposition in sequence-to-sequence learning, and provided theoretical analysis, applied it to tasks and analyzed the time complexity. The paper is easy to follow but the inspiration is imited. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_f3DQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_f3DQ"
        ]
    },
    {
        "id": "AXK6454gar",
        "original": null,
        "number": 5,
        "cdate": 1667076903259,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667076903259,
        "tmdate": 1667076903259,
        "tddate": null,
        "forum": "BrJATVZDWEH",
        "replyto": "BrJATVZDWEH",
        "invitation": "ICLR.cc/2023/Conference/Paper1366/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents theoretical and empirical results supporting an exponential gap between a sequence-to-sequence task and its decomposition into many smaller subtasks. In this study, the authors consider a bit parity task where the objective is to determine if the number of ones in a subset of a 0/1 array of length d is odd or even. This is formulated as a binary classification task. \n\nWith this setup, the authors show the following main result:\n\n1) When a task is decomposed into subtasks, a seq2seq model can get a get arbitrarily low zero-one loss (\\epsilon) with a number of gradient updates that is polynomial in d.\n2) When no supervision over subtasks is provided, for any polynomial time learning algorithm, the zero-one loss will be > \u00bd - \\epsilon.",
            "strength_and_weaknesses": "Strengths:\n\n1. The paper proves a useful result shedding light on the effectiveness of breaking down sequence-to-sequence problems into supervised subtasks.\n2. The paper is well structured - it studies a well-defined problem, proves theoretical results, and verifies them empirically (albeit in a slightly different setting).\n\nWeaknesses\n\n1. As the authors point out, all of their results hold only when the number of subtasks is O(d) rather than O(1) and in most practical cases, O(1) subtasks are more likely.\n2. The theoretical and empirical results use different model architectures.\n\nQuestions:\n\n1. Since most of your theoretical claims are for recurrent networks, did you attempt to empirically validate them with RNNs? How do your conclusions hold for RNNs with activations besides ReLU? Is this assumption mostly a result of the setting used in Wang et al. 2021? ReLU RNNs.\n2/ Why did you leave the input embeddings and initial hidden states untrained?",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity - The paper is well-structured and motivated. The theorems are summarized clearly in the main body of the paper.\n\nQuality - The paper is of sufficiently high quality.\n\nNovelty - The main novelty of this paper is in proving the exponential gap in the learnability of sequence-to-sequence tasks with and without intermediate supervision. The paper builds on Wang et al 2021, Shalev-Schwartz et al. 2017 and Shalev-Schwartz & Shashua 2016.",
            "summary_of_the_review": "The main novelty of this paper is in proving the exponential gap in the learnability of sequence-to-sequence tasks with and without intermediate supervision. While there are some limitations on the number of sub-tasks with supervision required, it is still a good starting point for future work that seeks to understand why ideas like \u201cchain-of-though\u201d prompting of language models are so effective.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_Qt7K"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1366/Reviewer_Qt7K"
        ]
    }
]