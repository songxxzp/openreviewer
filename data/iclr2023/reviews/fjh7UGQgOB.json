[
    {
        "id": "w1EYChgpbg",
        "original": null,
        "number": 1,
        "cdate": 1666257438236,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666257438236,
        "tmdate": 1670942985944,
        "tddate": null,
        "forum": "fjh7UGQgOB",
        "replyto": "fjh7UGQgOB",
        "invitation": "ICLR.cc/2023/Conference/Paper1424/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper extends UGS in order to improve its performance when the graph sparsity is high. The authors observe that the performance of UGS drops when the graph sparsity goes beyond a certain extent. The authors explain that this is due to the fact that only a small portion of the elements in the adjacency matrix are involved in the loss calculation. To address this problem, the authors introduce a new loss term to take into account all the elements of the adjacency matrix. They also improve the robustness of lottery tickets with sparse graphs by defining graph pruning as an adversarial graph perturbation and formulating it as a min-max optimization problem.",
            "strength_and_weaknesses": "Strengths:\n- the paper is well written and easy to follow\n- the proposed method effectively solves one of the main weaknesses of UGS\n- the proposed method significantly improves the performance of UGS \nWeaknesses:\n- the experiments that assess the performance of the proposed sparsification method are conducted on three datasets: Cora, Citeseer and Pubmed. Even though these datasets are quite popular in this field, it is well known that they are not very reliable (see, e.g., the discussion in [*]). For this reason, it is better to evaluate the performance on other datasets such as OGB datasets or [*] in order to fairly assess the performance of the proposed method.\n- the analysis on the transferability of graph lottery tickets seems a bit disconnected from the rest of the paper. If I understood the paper correctly, the main goal of the paper is to propose a method for improving the performance of UGS on sparse graphs. The focus of this part is thus on the sparsity of the graph. Instead in Sec. 4 the authors analyze the transferability of the sparsified subnetwork to others graphs. Then, the focus in this case is on the sparsity of the network. For this reason, I cannot see the connection between this section and the rest of the paper. In addition, it doesn't seem to me that the proposed method can provide some advantages with respect to the transferability of the lottery ticket.\n\n[*] Dwivedi et al., Benchmarking Graph Neural Networks",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and the experiments are well described. The authors propose a significant improvement of a state-of-the-art method.",
            "summary_of_the_review": "I think that this paper could be a solid work if the experimental evaluation is improved by adding some experiments on more reliable datasets (see comments above).",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_75Eg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_75Eg"
        ]
    },
    {
        "id": "WfcYgMNyMo2",
        "original": null,
        "number": 2,
        "cdate": 1666643879231,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666643879231,
        "tmdate": 1670836435256,
        "tddate": null,
        "forum": "fjh7UGQgOB",
        "replyto": "fjh7UGQgOB",
        "invitation": "ICLR.cc/2023/Conference/Paper1424/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the pruning problem of graph neural networks (together with the adjacency matrix). It claims that the GNN performance drops sharply when the graph sparsity is beyond a certain extent in UGS. To address this issue, this work adopts two approaches: (1) adding a new auxiliary loss based on WD to better guide the edge pruning by involving the entire adjacency matrix. (2) regarding unfavorable graph sparsification as an adversarial data perturbation and formulating the pruning process as a min-max optimization problem. Further, this paper investigates the obtained winning tickets under a transfer learning setting. ",
            "strength_and_weaknesses": "Strength:\n1. This paper studies a practical and challenging topic in GNNs. As the scale of graph data becomes larger, it will indeed arouse our attention to graph pruning. \n2. This work formulates the pruning process as a min-max optimization problem to gain the robustness of lottery tickets when the graph sparsity is high, which is novel to me.  \n3. This paper is easy to follow, with clear motivation and questions to address.\n\nWeaknesses:\n1. Firstly, I am extremely confused about the relationship between the problem caused by the excessive graph sparseness and the GNN transfer learning problem studied in this paper. The latter one (graph transfer learning) seems an independent part. Moreover, regarding transferring winning tickets, it seems that [1] has already done it. \n2. The loss function in UGS is $L_{UGS} = L({m_g\u2299A,X},m_\u03b8\u2299\u0398)+r_1 ||m_g ||_1+r_2 ||m_\u03b8 ||_1$. Note that there are regularization terms in the loss function to control model sparsity and graph sparsity, but this paper only has the cross-entropy loss. It seems inconsistent and unfair to compare your model with UGS.\n3. The paper uses too much vspace, resulting in an unfriendly layout for the audience. a) The formula of the \u201cgraph sparsity\u201d and \u201cweight sparsity\u201d take up too much space. b) The layout of Figure 1 and 2 can also be adjusted accordingly (e.g., combining them). c) The whole algorithm process is a bit laborious for me to read, especially how to perform the network training process. An algorithm table may help. \n4. The paper separates the potential nodes of class c and the other nodes by Eq. (3), is the node number of $c$ identical to $\\bar{c}$? Can the Wasserstein distance measure the distance between two different node number distributions?\n5. The authors only evaluate the proposed method on 2-layer GNNs, which is not universal and convincing. For some deep GNNs, do we obtain similar observations to Figure 1? In my view, a very deep GNN can pass gradients to most of the edges.\n6. The improvement of the proposed method over UGS is not significant as shown in the experiments. I believe this work still has space to improve.\n\nI may have some misunderstanding above, please feel free to correct it in the rebuttal phase. Thank you.\n\n====after rebuttal====\nW1-W4 have been addressed by the authors. I still have concerns with W5 and W6.\n\n\n\n\n[1] Morcos A, Yu H, Paganini M, et al. One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers[J]. Advances in neural information processing systems, 2019, 32.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is mostly well-written and easy to follow. The novelty of adversarial perturbation is novel in the context of LTH and UGS. No code to support reproducibility.",
            "summary_of_the_review": "This paper addresses a challenging and practical problem. Although this paper has some merits, I would like to recommend \"borderline rejection\" before the rebuttal phase given the weaknesses listed above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "NA",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_DAbQ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_DAbQ"
        ]
    },
    {
        "id": "XUPjMezYet",
        "original": null,
        "number": 3,
        "cdate": 1666851614941,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666851614941,
        "tmdate": 1670517984464,
        "tddate": null,
        "forum": "fjh7UGQgOB",
        "replyto": "fjh7UGQgOB",
        "invitation": "ICLR.cc/2023/Conference/Paper1424/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper focuses on improving lottery tickets on GNN. Although existing work (UGS) prunes graph and weight simultaneously,  the performance degrades significantly when graph sparsity is high.\nThe paper found out the problem in UGS lies in the loss formulation for pruning adjacency matrix, as it does not involve all edges for back propagation.To solve this, they add a new auxiliary loss head that measures the inter-class separateness of nodes with Wasserstein distance to better guide the edge pruning.\nThe paper also proposed an adversarial perturbation technique, which formulates the pruning process as a min-max optimization problem.   This helps improve the robustness of their graph lottery tickets against the graph sparsity.\nAdditionally, the paper explored graph transfer learning by running node classification tasks on two citation networks.\nResults on three GNN models with multiple datasets show improvement over the existing LTH method.",
            "strength_and_weaknesses": "Strengths:\n1. The paper is easy to follow. The paper describes the disadvantage lies in existing work and then introduced there methods for improving. \n2.  Experimental on multiple GNN models (GCN , GAT, and GIN) and datasets (Cora, Citeseer and PubMed, Arxiv and MAG) seems to be sufficient.\n3. I like the ablation on the  auxiliary loss weights and the prune ratio combination of weight and graph.\n\nWeekness:\n1. The paper target the work UGS ( by Chen et al. ) directly, by improving the performance at a high graph sparsity. However, looking at Figure 5-7. I\u2019m not sure if the improvement is significant enough, especially since some seems quite similar.\n2. Less comparison with other existing work. \n\nQuestions:\n1. How to get equation 10,11 from equation 9?\n2. In Figure 4, the trend of Randomly initialized GLT and GNN is very similar.  What sparsity is the GLT? Does sparsity effect the converging speed?",
            "clarity,_quality,_novelty_and_reproducibility": "The paper has decent clarity and quality\nThe paper has some novelty, mainly applying two mathematical technique to improve the loss function and optimization of existing LTH.",
            "summary_of_the_review": "Generally speaking,  the paper has decent quality. \nMy concern lies in the significance of novelty and performance gain over existing works.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_FtWv"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1424/Reviewer_FtWv"
        ]
    }
]