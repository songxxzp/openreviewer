[
    {
        "id": "y9pqofNy8j",
        "original": null,
        "number": 1,
        "cdate": 1666682217853,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682217853,
        "tmdate": 1670470380609,
        "tddate": null,
        "forum": "XXTyv1zD9zD",
        "replyto": "XXTyv1zD9zD",
        "invitation": "ICLR.cc/2023/Conference/Paper1362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a simple method for the neural network ensemble by integrating the ensemble into one neural network with the group convolution operation. Empirical evaluations show a strong performance and space efficiency of the proposed method.",
            "strength_and_weaknesses": "Strength:\n1. The proposed group convolution based ensemble has comparable or better performance with a full ensemble, using less parameters and computation.\n2. The method is easy to implement with standard high-level neural network programming library.\n3. The results on ImageNet is persuasive. Although not shown in the comparisons, it outperforms the Rank-1 BNN which uses the BatchEnsemble as backbone, in terms of accuracy and other important metrics.\n\nWeaknesses:\n1. There are some missing results on ImageNet about efficiency. Why is that?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written and easy to read. ",
            "summary_of_the_review": "In general, this paper proposes a simple and useful ensemble technique. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_x31y"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_x31y"
        ]
    },
    {
        "id": "Jq0MX9MPSk",
        "original": null,
        "number": 2,
        "cdate": 1666806284712,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666806284712,
        "tmdate": 1666806284712,
        "tddate": null,
        "forum": "XXTyv1zD9zD",
        "replyto": "XXTyv1zD9zD",
        "invitation": "ICLR.cc/2023/Conference/Paper1362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors introduce PackedEnsembles (PE) to improve the efficiency of deep ensembles. Although deep ensembles achieve SOTA results on a variety of benchmarks, this comes at significant inference-time and memory costs, since the same model architecture is repeated $M$ times for ensembles of size $M$. In this sense, this paper belongs to the same line of research as MIMO and BatchEnsembles (and extensions thereof), as well as mixtures of experts (MoEs) to a lesser degree.\n\nPackedEnsembles rely fundamentally upon the notion of grouped convolutions, first introduced in (Krizhevsky et al., 2012) for hardware constraint reasons, and used in more modern architectures such as ResNext. Grouped convolutions essentially partition the filters at a given convolution layer into $M$ groups, and each output channel is obtained by the filters in of such groups (as opposed to all filters).\n\nAs this reduces significantly the capacity of each individual \"ensemble member\", the authors introduce a parameter $\\alpha$, which multiplies the width of the original model (resulting in ensemble members of size rescaled roughly by $\\alpha / M$). Furthermore, each individual ensemble member may itself make use of grouped convolutions, parameterized by the number of groups $\\gamma$.",
            "strength_and_weaknesses": "# Strengths\n- This paper proposes a simple technique to improve the inference and memory cost of deep ensembles. \n- Experimental results confirm the value of this method compared to commonly used \"efficient ensemble\" baselines such as MIMO and BatchEnsemble on Cifar datasets, across a variety of different metrics *and* architectures: the proposed PE model comes close, and sometime outperforms, the performance of deep ensembles on accuracy and uncertainty metrics, while drastically cutting down on the number of parameters and required operations.\n- The authors provide an extensive amount of ablation experiments to gauge the importance of the $\\alpha$ and $\\gamma$ parameters.\n\n# Weaknesses\n- It would be interesting to evaluate the other baselines on ImageNet tasks. I realize this can be an investment, but doing so would significantly improve the paper. Some results (for $M=4$ presumably, given reported parameter counts) are publicly available at the link referenced in footnote 2.\n- Although this is not necessary given the scope of this work, the authors may be interested in looking into the Mixtures of Experts literature, as decisions in data routing and partitioning may be also relevant.\n\n**Questions**\n- I'm curious about the position of MIMO in Figure 1: it looks like MIMO (4) has the same number of parameters as a single model, but much slower inference. Could you explain what causes this?\n\n**Nitpicks**\n- Equation (1) doesn't make use of the activation function $\\phi$.",
            "clarity,_quality,_novelty_and_reproducibility": "Quality: This paper is of good quality. The results are relevant to the community, the method well motivated and experimentally validated.\nClarity: the paper is clearly exposed, its position within the greater literature is well discussed.\nOriginality: This reuses many key ideas from previous work, with a novel perspective. ",
            "summary_of_the_review": "This paper proposes a straightforward, well-motivated, and experimentally validated technique to approach the performance of deep ensembles while significantly improving upon the complexity of deep ensembles. The results are validated across several ResNet-based architectures and standard image benchmarks. This paper could be improved by including more baselines for the ImageNet experiments, as the Cifar datasets are significantly easier; however, achieving this can be costly.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_FMDg"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_FMDg"
        ]
    },
    {
        "id": "oIevNIYVqbf",
        "original": null,
        "number": 3,
        "cdate": 1667644724441,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667644724441,
        "tmdate": 1667644724441,
        "tddate": null,
        "forum": "XXTyv1zD9zD",
        "replyto": "XXTyv1zD9zD",
        "invitation": "ICLR.cc/2023/Conference/Paper1362/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes an improved version of Deep Ensembled, i.e., Packed-Ensembles (PE), which cleverly leverages grouped convolutions to parallelize the ensemble into a single backbone to speed up the inference and training (eliminating the need to train multiple nns). The authors also extensively demonstrated the advantages of PE over the original DE. ",
            "strength_and_weaknesses": "Strength: \n\nSimple and effective implementation with extensive comparisons and discussions. Overall a well written paper. \n\nWeakness: \n\nThe introduction of group convolution into DE might be too incremental for ICLR. Also, it seems like this idea has been entertained before https://arxiv.org/abs/2007.00649.\n\nI am not sure that I totally get the conclusions in Table 7. All the performance discrepancies are very marginal (maybe some hypothesis testing is needed? But 5 is too small a sample size to draw conclusions), and I don't think better ECE, NLL etc. leads to better diversity. In fact, the fuction space diversity can be easily checked by the correlations among predictions of different ensemble members. Maybe the authors wanted to add experiment here, similar to what the MIMO work did.  \n\nOne downside of group conv is the scalability, i.e., with more ensemble members, the performance of group convolutions might drop. DE does not suffer from the same issue, and it would be interesting to test the performance of PE vs DE with an increasing number of ensemble members. ",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity of this paper is good. As previously mentioned, the novelty and correspondingly the quality, is limited. Reproducibility is high. ",
            "summary_of_the_review": "In summary, I tend to accept this paper as it is a simple and effective approach to improve the performance of DE, but I would like to see a more thorough experiment section to examine the pro and con of PE. Thus, my recommendation is marginal accept. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_GYMm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1362/Reviewer_GYMm"
        ]
    }
]