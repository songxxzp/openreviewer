[
    {
        "id": "ci-XZuZ2PqH",
        "original": null,
        "number": 1,
        "cdate": 1666315743960,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666315743960,
        "tmdate": 1666315743960,
        "tddate": null,
        "forum": "m3QhpKNXU6-",
        "replyto": "m3QhpKNXU6-",
        "invitation": "ICLR.cc/2023/Conference/Paper1829/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies how the generalization error decreases in multilayer CNNs. Using NTK, the authors theoretically show that the decay rate of the error is determined by the effective dimension of the data instead of the actual dimension. Here the effective dimension relates to how each hidden unit _sparsely_ depends on the output of the previous layer. The experiments also support the derived decay rate.",
            "strength_and_weaknesses": "Strengths\n1. The theoretical results are interesting. They clarify why CNNs are efficient in terms of learning theory. \n1. The experiments provide additional evidence that the decay rate is correct. \n\nWeaknesses\n1. The assumption of Eq 1 looks unrealistic. It poses that the input space should be the product of spheres, such as a torus. Images, a favorite data of CNNs, are not in that kind of space --- they are usually represented in a cube [0, 1]^{height x width x channel}. When an input is in a spherical space, it should be steerable and the same input should be obtained by steering 360 degrees. This property doesn't hold in the raw image space. I wonder how this assumption is critical in your analysis. Can we obtain the same result without the assumption? Also, does the assumption hold in the experiments?\n",
            "clarity,_quality,_novelty_and_reproducibility": "I cannot fully cover but the theoretical derivation looks correct. \n\nThe quality of the paper is high. Significant theoretical results are provided. Small but sufficient experiments are also provided. \n\nI'm not working on this area and I cannot judge the novelty. \n\nThe reproducibility is OK. Although the code is not provided, fine details of experiments (learning rate, etc) are shown in Appendix. ",
            "summary_of_the_review": "The paper brings a neat analysis of CNN in terms of generalization error. My concern is that the assumption used in the analysis do not seem to match real problems. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_gyaw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_gyaw"
        ]
    },
    {
        "id": "li5QlHlqgb",
        "original": null,
        "number": 2,
        "cdate": 1666652666615,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666652666615,
        "tmdate": 1666660419509,
        "tddate": null,
        "forum": "m3QhpKNXU6-",
        "replyto": "m3QhpKNXU6-",
        "invitation": "ICLR.cc/2023/Conference/Paper1829/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies convolutional neural networks in the kernel regime. In particular, they show that the corresponding kernel's spectrum inherits the network's hierarchical structure.  Therefore, deep CNNs can beat the curse of dimensionality if the target depends only on local groups of variables.",
            "strength_and_weaknesses": "Strength: \n1. The idea is novel.\n2. This paper is comprehensive, with the solid support of proof and experiment. \n\nWeakness: \n\n1. This paper is hard to follow since the claims are scattered in the paper, which is hard to find. \n\n2. This paper only cares about the NN kernel solution. So it doesn't have any optimization result or discussion of the width. \n\n3. This paper requires patch non-overlapping. It is unclear whether this paper can be extended to a more general setting. \n\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is well written. However, the organization may need to be improved since I find it quite hard to follow. ",
            "summary_of_the_review": "Overall, the results proved in this paper may be significant for people who study NTK. This paper feels complete, but I still have concerns about clarity. \n\nIn the abstract, the author gives the following claims. \"the rate of decay of the error is controlled by the effective dimensionality of these subsets,\" \"if the teacher function depends on the full set of input variables, then the error rate is inversely proportional to the input dimension,\" \"despite their hierarchical structure, the functions generated by deep CNNs are too rich to be efficiently learnable in high dimension.\"  Where can I find the support theorems of these claims? I tried my best to understand them, but I could only find some relevant explanations rather than solid theorems. \n\n",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_UzrZ"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_UzrZ"
        ]
    },
    {
        "id": "OaFr_JaDf7",
        "original": null,
        "number": 3,
        "cdate": 1666762581789,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666762581789,
        "tmdate": 1666762581789,
        "tddate": null,
        "forum": "m3QhpKNXU6-",
        "replyto": "m3QhpKNXU6-",
        "invitation": "ICLR.cc/2023/Conference/Paper1829/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors provide some generalization analysis for deep CNNs. Based on some spectral analysis, they prove that deep CNNs adapt to the spatial scale of the target function. They also show how the error rate depends on the input dimension. ",
            "strength_and_weaknesses": "Deep CNNs have been studied theoretically recently and generalization error analysis has been carried out for CNNs of various types. The spectral and convergence analysis provided in the paper is interesting. But the network structure in Equation (1) uses a special linear transformation of a simply average over h', instead of a general linear combination, which is very special. This restriction makes the mathematical analysis easier. More discussion on networks with a general linear transformation would increase the scientific value of the paper. ",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well written in general. But the network structure based on a simply average is too special and prevents applications of the derived analysis to network designs. ",
            "summary_of_the_review": "The spectral and convergence analysis provided in the paper for deep CNNs is interesting. But the network structure in Equation (1) uses a special linear transformation. This restriction makes the mathematical analysis easier. More discussion on general networks would enhance the paper. ",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_xmg3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1829/Reviewer_xmg3"
        ]
    }
]