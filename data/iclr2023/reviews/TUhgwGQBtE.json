[
    {
        "id": "d2j10jb8ppU",
        "original": null,
        "number": 1,
        "cdate": 1666266565117,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666266565117,
        "tmdate": 1666331807947,
        "tddate": null,
        "forum": "TUhgwGQBtE",
        "replyto": "TUhgwGQBtE",
        "invitation": "ICLR.cc/2023/Conference/Paper557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "In this paper, the authors first provide theoretical proof that the untrained GNN models are nearly optimal,\nsecond, propose a novel, based on sparse coding, NAS for GNNs: NAC - neural architecture coding.\nUnlike other NAS methods, NAC, based on the assumption that untrained GNN models are nearly optimal, does not update GNN weights, \nwhat leads to computational cost reduction and optimization stability improvement.",
            "strength_and_weaknesses": "*Strength*\nThe paper is interesting, and the proposed idea of using sparse coding for NAS in GNNs seems novel.\nThe proposed method seems to outperform previous art in terms of accuracy, taking less time. \n\n*Weaknesses*\nThe paper presents the results only on transductive small datasets and does not provide a sota for comparison.\nAs a result, it is hard to understand the impact of the proposed method NAC.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is well constructed and clear.\nThe idea of using sparse coding for NAS in GNNs seems novel.",
            "summary_of_the_review": "In general very interesting paper proposes new insights hoe to design NAS for GNNs in linear time. In addition, the paper is well constructed. \nEven though there are a few weak points, which are summarized in the following questions:\n\n1) Can you please elaborate on your statement that \" GNN models behave almost linearly\"?  I expect to see the theoretical or empirical justification for it. \nThe statement \"Technical motivation for NAC stems from the observation that an untrained GNN performs well, and training it might not provide extra benefit\"\nfills like very strong and should be theoretically or empirically justified, preferable for both transductive and inductive datasets.\n\n2) The proof result is \"the output layer alone can ensure the optimality of the network even when all previous layers have no updates\" \nis strongly relied on skipping all nonlinear activation functions. \nFrom my understanding, skipping all non-linear activations can dramatically reduce neural network feature extraction expressive power. \nWe can simply shrink all layers into a single layer, as a result, do not need depth.\nAgain, can authors provide empirical or theoretical justification for that?\n\n\n3) From my understanding, the objection of NAS algorithms is to provide architectures that can achieve better accuracy than handcrafted architectures. I expect the authors also to add SOTA results of the evaluated datasets to justify the practical impact of the proposed method.\n\n4) The experiments were only done on transductive datasets. It is known that inductive datasets are harder.\nCan authors provide experimental results also for inductive datasets?\n\nPre-rebuttal score: I give 5 (BR), and looking forward to receiving the author's responses to my concerns.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "I haven't found any ethical issues.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper557/Reviewer_WVaT"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper557/Reviewer_WVaT"
        ]
    },
    {
        "id": "oVF3k2drLKh",
        "original": null,
        "number": 2,
        "cdate": 1666287424264,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666287424264,
        "tmdate": 1666287526102,
        "tddate": null,
        "forum": "TUhgwGQBtE",
        "replyto": "TUhgwGQBtE",
        "invitation": "ICLR.cc/2023/Conference/Paper557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper first proposed that on some graph datasets a model with untrained graph layers can perform equally well as trained graph layers, and then proposed a NAS method that exploits this trait to accelerate NAS training.",
            "strength_and_weaknesses": "Strength:\n1. If the data assumption is correct, the proposed method does seem to achieve both better search results in much shorter time.\n2. Some novelty in proposing the sparse coding reformulation of NAS for GNNs.\n\nWeaknesses:\n1. In theorem 3.2, the authors states the assumption: \"data is linearly separable\". I doubt this assumption holds true for all the graph datasets. Is there any proof that this is actually true?\n2. Following the 1st point, GNN layers performing equally well without activation layers are usually observed on simpler GNN datasets using simple features such as Bag-of-words. I have some doubts that this will be the case when GNN datasets gets more complex and the interactions between nodes are more complex, e.g. in particle simulation or protein interaction datasets. In this case I am not sure if the proposed method can still work. Can authors perform experiment on more complex datasets?\n\nMissing citation:\nZhao Y, Wang D, Gao X, et al. Probabilistic dual network architecture search on graphs[J]. arXiv preprint arXiv:2003.09676, 2020.",
            "clarity,_quality,_novelty_and_reproducibility": "The writing is clear. Evaluation is lacking a bit as only the simplest graph datasets are used. ",
            "summary_of_the_review": "The proposed method works well but under a strong assumption and only on limited datasets. I can only recommend acceptance if authors can clarify my doubts above.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper557/Reviewer_GxHc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper557/Reviewer_GxHc"
        ]
    },
    {
        "id": "96XT5mMEs5i",
        "original": null,
        "number": 3,
        "cdate": 1666626677112,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666626677112,
        "tmdate": 1669991875681,
        "tddate": null,
        "forum": "TUhgwGQBtE",
        "replyto": "TUhgwGQBtE",
        "invitation": "ICLR.cc/2023/Conference/Paper557/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposed one method NAC to automatically design GNNs. With randomly initialized model weights, NAC designs the GNNs by learning the sparse encodings on top of the search space. The empirical and theoretical results demonstrate its effectiveness.",
            "strength_and_weaknesses": "### Strength:\n\nThis paper shows the feasibility of no-update GNNs in graph representation learning, and then applied them into NAS-GNN, which can transfer the bi-level optimization problem to single-level. It is interesting and useful for NAS-GNN.\n\n### Weakness:\n\n1. The optimization gap still exists due to the shared output layer.\nIn Theorem 3.2, one untrained GNN (which is denoted as A^LXO and omits the output layer) can make the final output as good as the well-trained GNNs based on the optimized weights $\\hat{W}^*_o$. It seems that this weight still needs to be trained with the first term of the optimization objective in Eq.(7). Then, the optimization gap cannot be addressed since different architectures also share these untrained weights (W_l) in each layer and the output layer(W_o). When we obtained the optimized weight $\\hat{W}^*_o$ with Eq.(7), this weight is sub-optimal for each candidate architecture in the search space. Therefore, the optimization gap still exists.\n\n2. The constraints on the dictionary D in sparse encoding. The second term in Eq.(3) is ignored in Eq.(7).\n\n3. Some experiments are ignored.\na) This paper only considered the NAS baselines in GNNs. Compared with SANE(Zhao et al. 2021b), the baseline performance is pretty lower.\n\nb) Since this paper remove all the non-linear operations in GNNs. The untrained SGC baseline should be considered. It seems that it is one competitive baseline with Theorem 3.1-3.3.\n\nc) The evaluations on \u201ccomplex\u201d datasets. As mentioned in Page 8, \u201cthe optimal condition can get compromised due to the high complexity of the data.\u201d It seems that the proposed method has difficulty when applied to a complex dataset. However, how to measure the \u201ccomplexity\u201d of the dataset? Besides, the proposed method aims to learn the sparse encoding for each node, which is equivalent to designing the node-wise GNNs. It would be better to evaluate the proposed procedures, e.g., large-scale datasets ogbn-arxiv.\n",
            "clarity,_quality,_novelty_and_reproducibility": "It is interesting and novel to utilize the no-update GNNs in NAS-GNN, while some aspects are unclear.",
            "summary_of_the_review": "This paper proposed one method NAC to automatically design GNNs based on the no-update GNNs. The theoretically analysis and some experiments can be further improved.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper557/Reviewer_oy8p"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper557/Reviewer_oy8p"
        ]
    },
    {
        "id": "ogXmmU0PPA",
        "original": null,
        "number": 4,
        "cdate": 1666665376470,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666665376470,
        "tmdate": 1666677016362,
        "tddate": null,
        "forum": "TUhgwGQBtE",
        "replyto": "TUhgwGQBtE",
        "invitation": "ICLR.cc/2023/Conference/Paper557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This work firstly proved that GNNs (without activation function) fixed with random weights can make final outputs as good as well-trained GNNs under mild conditions. Based on this foundation, the authors proposed a novel graph neural architecture search method neural architecture coding (NAC). Specifically, NAC holds a no-update scheme on the parameters of GNNs and concentrates on the training of architecture parameters. And the NAC is 200x faster than existed GNN-NAS methods and can find better GNN architecture.",
            "strength_and_weaknesses": "Strength:\n1. The proposed NAC is 200x faster than existing GNN-NAS methods. \n2. Besides, the NAC can find better GNN architectures than existing GNN-NAS methods.\n3. This work has a good theoretical contribution.\n\nWeaknesses:\n1. The assumption is strong and the theory holds for GNNs without activation functions.\n2. There is a lack of experiments on large-scale graph data.\n3. The contrast of SOTA GNN is missing in the experiment.\n4. The GNN architecture searched by the proposed method is not shown.",
            "clarity,_quality,_novelty_and_reproducibility": "The motivation of this work is quite novel. And the presentation of this paper is well organized. Besides, the authors add their code to supplementary material.",
            "summary_of_the_review": "    I recommend acceptance. This work proposed a linear complexity GNN-NAS method NAC. And the proposed NAC faster than existed GNN-NAS methods and can find GNNs with better performance than existing GNN-NAS methods. What\u2019s more, the work has a good theoretical foundation. Here are my major concerns:\n\n    1. The conditions of Theorem 3.1 and Theorem 3.3 are very strong. They only apply to some GNNs without activation functions.\n    2. There are errors in servel statement of this paper.  \u201cGraph neural networks behave almost linearly, so they can be simplified as linear networks while maintaining superior performance (Wu et al.,2019b).\u201d They only claim GCN can be simplified as linear networks.\n    3. Considering that the search space is different from the previous methods, it is suggested to add ablation experiments to verify the impact of the search space. \n    4. It is suggested to add training-free NAS methods to related works, such as [1][2].\n    5.  It is suggested to add SOTA GNN methods to related works.\n\n[1] Neural architecture search on imagenet in four gpu hours: A theoretically inspired perspective\n[2] NASI: Label-and Data-agnostic Neural Architecture Search at Initialization\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper557/Reviewer_9TQX"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper557/Reviewer_9TQX"
        ]
    },
    {
        "id": "JmRlKgveygu",
        "original": null,
        "number": 5,
        "cdate": 1667571385818,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667571385818,
        "tmdate": 1667585668548,
        "tddate": null,
        "forum": "TUhgwGQBtE",
        "replyto": "TUhgwGQBtE",
        "invitation": "ICLR.cc/2023/Conference/Paper557/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper considers the neural architecture search (NAS) problem specifically for graph neural networks (GNNs). By omitting the effect of non-linearity of GNNs (in terms of the NAS ranking of architectures), i.e., we can linearize the GNNs, and the NAS for GNN problem can be greatly simplified and formulated as a sparse encoding problem, which this paper called neural architecture coding (NAC). This paper first proves the linearized GNN can still achieve optimality under mild conditions (Theorem 3.1-3.3), and then proposes to search/optimize the architecture parameters via the sparse coding method. Experiments show that the proposed NAC method is much faster and accurate than some previous NAS for GNNs baselines.",
            "strength_and_weaknesses": "#### Strengths\n1. I like the idea of linearizing the GNNs for the NAS problem and formulating the search of architecture parameters as a sparse coding problem. To my best knowledge, this should be novel in the line of research of NAS for GNNs.\n2. The empirical results look promising compared to the previous methods in terms of performance and efficiency.\n\n#### Weaknesses\n1. One major concern of mine is regarding the clarity of the theory & algorithm parts. I think the major contribution of this work is the NAC algorithm which is described in section 3.2. However, I think the clarity (and length) of section 3.2 could be much improved. In terms of length, I think section 3.1 takes too much space, and I suggest moving the detailed proofs to the appendices. In terms of clarity, I suggest the authors define and describe the concepts/notations with more details. For example, how do we encode/what is the dimensionality of the GNN operator $\\alpha$; is $o^l$ a set function (aggregator) or a normal vector function (as the last line of Eq.(7))? The Eq. (7) is the central contribution and should be explained and analyzed with much greater details. Moreover, the notations and logic in sections 3.1 and 3.2 are somehow disconnected, and it is also hard to understand how is the theorems in section 3.1 useful to the NAC algorithm.\n2. The proposed NAC method outputs the optimal architecture by an argmax in each layer in Algorithm 1, so I think this method should be better understood as a differentiable NAS or Auto-GNN method. Does the author ensure the search space is exactly the same among all baselines, including random search (RS), Bayesian optimization (BO), and GraphNAS in Table 1? Also, is it possible to also consider Auto-GNN (and maybe some newer SOTA methods in that line of research) as a fair baseline?\n3. The evaluation is mostly done on very small graphs, and this is not sufficient to justify the (consistent) advantage of the proposed method on large-scale real-world graphs. Moreover, this paper also claims the efficiency advantages. Thus it would be necessary to evaluate on at least a few larger graphs.\n\n\n#### Minor Issues\n1. The format of the uploaded PDF is not directly generated from LaTeX, i.e., it seems that each page is an image thus, I cannot search/select text, or use the hyperlinks.",
            "clarity,_quality,_novelty_and_reproducibility": "1. I think the clarity of the center contribution, the NAC algorithm, needs improvement and some rewrites. And the paper structure needs some adjustment.\n2. I agree with the novelty claimed.\n3. I think the quality (accuracy and efficiency) is justified on small graphs but results on some larger graphs is necessary, especially for the efficiency claims.\n4. The reproducibility depends on whether the authors will release the code (conditioned on the acceptance) and cannot be judged now.\n",
            "summary_of_the_review": "Overall for the current manuscript, I recommend weak rejection. I do like the idea of NAC and agree with its novelty. But I think some rewrites are needed to improve the clarity, especially about the description of the NAC algorithm and related analysis, which could be extended with more details. Moreover, the experiments may also need a bit more recent baselines from the Auto-GNN line of research and, more importantly, evaluation on at least a few larger graphs to support the efficiency claims.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper557/Reviewer_P1Y6"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper557/Reviewer_P1Y6"
        ]
    }
]