[
    {
        "id": "1o5xNH1AidW",
        "original": null,
        "number": 1,
        "cdate": 1665897978707,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665897978707,
        "tmdate": 1666321249980,
        "tddate": null,
        "forum": "TtxOsdYU92d",
        "replyto": "TtxOsdYU92d",
        "invitation": "ICLR.cc/2023/Conference/Paper1756/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper proposes a learnable and differentiable pooling operation to replace the global average pooling (GAP) for metric learning.   \n\nSpecifically, the pooling operation is formulated as the solution to an optimal transport problem with additional residual $\\rho$ and mass ration $\\mu$ parameters. $\\rho$ represents the per-element residual, and $\\mu$ stands for the total assignment mass.   \nThe authors derived an iterative solution for forward computation and an inversion-free computation of gradient for backward computation. The closed-form backward gradient formulation is efficient for network training.  \n\nMoreover, to improve the transferring property, cross-batch zero-shot regularization is proposed by predicting class embedding vectors from the prototype assignment vectors ($\\pi$). The sub-problem (P3) has a closed-form solution. Since real class embedding vectors are not given, a meta-learning scheme utilized a zero-shot prediction loss to tackle this. \n\nIn the experiments, toy experiments are designed to investigate the mechanism of the proposed GSP. In metric learning experiments, various pooling alternatives are compared, and GSP achieves the best performance. In addition, GSP is evaluated under different architectures and methods, verifying its effectiveness. Regarding the absolute improvements, the general boost is around or below 1% in most cases, but the improvements are consistent across all settings. \n\n  \n",
            "strength_and_weaknesses": "## Strength\n- The proposed OT optimization problem in (P1) is novel and reasonable as an alternative to GAP. The introduction of $\\rho$ and $\\mu$ are the keys to model nuisance features and dynamic re-weighting. \n- Both forward and backward solutions are given for an entropy-regularized version of (P1), i.e., (P2). Moreover, the closed-form and inversion-free formulation of the partial derivatives enables differentiable and efficient backward computation. \n- A zero-shot regularization auxiliary task is proposed to improve the transferring ability of metric learning. A simple formulation with a closed-form solution is devised, and a meta-learning procedure is proposed to enable training without ground truth label embeddings. \n- The toy experiments corroborate the design intuitions of GSP and zero-shot regularization, providing useful insights. The comparison experiments are conducted on various datasets, with 13 pooling alternatives and with various backbone/methods, verifying the general effectiveness. \n\n## Weakness\n- One major concern is the extra parameters and computation complexities induced by the proposed method. For example, extra parameters involve $\\{ \\omega, \\Upsilon \\}$ and extra computations involve Proposition 4.1 ($k=100$ forward iteration) and Proposition 4.2 (backward computation). \n- For the deriving of (4.2), important details are missing, e.g., $\\frac{\\partial \\mathcal{L}}{\\partial \\rho^{(\\epsilon)}}$. In $\\frac{\\partial \\mathcal{L}}{\\partial c}$, which kind of $\\mathcal{L}$ is used? In Proposition 4.2, the first $\\mathcal{1}_n$ should be $\\mathcal{1}_m$, and $\\gamma$ is not defined. All these make 4.2 incomplete. \n- I can infer the procedure of meta-learning in zero-shot regularization from Figure 2. However, Section 4.3 is a bit confusing, especially (4.4). It will be much better if an algorithm box is provided. \n- The author mentioned several times the similarity with the top-$k$ selection process. However, no discussion or formulation analysis is provided to show the relationship explicitly. \n- In Sec 4.2, \"this function is not smooth\" why? Is it because of the max operation in normalization? \n- There are several typos, especially in the appendix regarding Preposition 4.1. \n \n",
            "clarity,_quality,_novelty_and_reproducibility": "The proposed Generalized Sum Pooling (GSP) operation and zero-shot regularization are novel, and a feasible solution is given to the optimization problem. \n\nExperiments support the effectiveness of the proposed GSP and zero-shot regularization methods. \n\nCode is provided for reproduction. \n\nOverall, the quality of this is good, except for some concerns listed in the above Weaknesses. ",
            "summary_of_the_review": "The overall quality and novelty of this paper are good, except for several concerns listed above. The experiments can verify the effectiveness of the proposed method.   \nI would like to see this paper accepted if the concerns and issues are solved by the authors. \n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N/A",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_MQSC"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_MQSC"
        ]
    },
    {
        "id": "Ou2ntBdIh_",
        "original": null,
        "number": 2,
        "cdate": 1666320863193,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666320863193,
        "tmdate": 1666320863193,
        "tddate": null,
        "forum": "TtxOsdYU92d",
        "replyto": "TtxOsdYU92d",
        "invitation": "ICLR.cc/2023/Conference/Paper1756/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "An extension of the global average pooling (GAP), called generalized sum pooling (GSP), is presented in this paper: while GAP is a convex combination with \"flat weights,\" GSP instead learns the weights by solving an optimal transportation problem. GSP allows for the selection of features to be pooled by introducing a \"background prototype\" (rho) into the formulation. Experiments compared GSP with several existing metric learning and pooling methods. The results showed that GSP gives small improvements in several cases.",
            "strength_and_weaknesses": "------------------- Strengths -------------------\n\n1. Introducing background prototype in optimal transportation problem is interesting.\n\n\n2. Thorough experiments are reported (in Supplementary Material).\n\n\n3. The paper is mostly well-written.\n\n\n4. Code is provided.\n\n------------------ Weaknesses ------------------\n\n1. Novelty\n\nAlgorithmic novelty may not be sufficient. As discussed in the paper, the idea of using optimal transportation formulation for feature pooling was proposed in Mialon et al. (2021), and the proposed problem (P1) is equivalent to a standard optimal transportation problem (by considering rho as a new pi_i, we can immediately get to the standard form). Given these, using entropy regularization, which is an efficient approach to solving the optimal transportation problem, is straightforward. (P2) is also equivalent to the standard form of the optimal transportation problem. The same is applied to the algorithm.\n\n\n2. Motivation\n\nWhile this paper focuses on metric learning, the application of pooling in general should not be limited to this. So far, I could not find a good reason for this limitation. I would expect an explanation if any.\n\n\n3. Evaluation of Rho\n\nThe point of GSP is to use rho, in other words, changing the range of i in (P1) 1<=i<=m to 1<=i<=m+1. So a before and after comparison between these two cases would be necessary.\n\n\n4. Overall Performance\n\nThe results of comparisons with various pooling methods reported in Supplementary Material (e.g., Table 2) show that the stand-alone performance of GSP is outperformed by several existing methods.\n\n\n5. Hyperparameter Setting\n\nGSP, together with zero-shot regularization, has many hyperparameters to be tuned (m, mu, epsilon, k, lambda). Since these were likely tuned for each dataset and task, it would not have been very difficult to achieve equal or better accuracy than other methods.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: This paper is mostly well-written. \n\n\nQuality: Experiments do not successfully show the effectiveness of GSP. The standalone performance of GSP is not better than several existing pooling methods.\n\n\nNovelty: Algorithmic novelty is lacking. The idea of learning pooling layer with optimal transportation has already been proposed. The problems (P1, P2) are equivalent to the basic optimal transportation formulations, so the solution is also not novel.\n\n\nReproducibility: Code is provided.\n",
            "summary_of_the_review": "The idea of using background prototypes is interesting and thorough experiments have been reported. \nThe major problems with this paper are the lack of novelty in the algorithm and the limited effectiveness of the proposed method. Also, the proposed method has many hyperparameters, which may be a drawback for its use in practical scenarios. Given that both novelty and effectiveness are important, I am leaning toward rejection for now.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_twxc"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_twxc"
        ]
    },
    {
        "id": "p-ig_sPYSv1",
        "original": null,
        "number": 3,
        "cdate": 1666696979337,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666696979337,
        "tmdate": 1666696979337,
        "tddate": null,
        "forum": "TtxOsdYU92d",
        "replyto": "TtxOsdYU92d",
        "invitation": "ICLR.cc/2023/Conference/Paper1756/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes a new pooling technique named generalized sum pooling. It is claimed as a strict generalization of global average pooling. The experiments are conducted on four different datasets and achieve a SOTA performance.",
            "strength_and_weaknesses": "I'm not an expert in this area, please correct me if I get it wrong. \n\nStrength:\n1. The proposed method seems to be sound, but I'm not fully checking it. The experimental results are outstanding \n2. The motivation seems to be reasonable.\n\nWeakness:\n1. While the motivation is reasonable, it still confuses me why we should redefine the learnable generalization of GAP as an optimization problem. What is the optimization objective?\n2. It seems like there are three categories of related works that are mentioned in this paper,  when selecting the baselines in your experiments, do you cover all of those three categories?\n3. Given the complicated structure of the GSP (i.e., Fig 2), how to can apply it to current existing metric learning or CV-related works?\n4. It is not clear to me, why we should bring the OT into this paper?",
            "clarity,_quality,_novelty_and_reproducibility": "Code is not available at this stage, - a password is required.\n\nI'm not an expert on this area, hence I would it seems to be novel.\n\n",
            "summary_of_the_review": "Based on my above point, I think this paper is novel and sound, but I do not fully check it through. \nWhile it still misses some key points which makes me confuse.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "N.A.",
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_ms1e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1756/Reviewer_ms1e"
        ]
    }
]