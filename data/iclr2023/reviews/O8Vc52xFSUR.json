[
    {
        "id": "oOskRmLWGK",
        "original": null,
        "number": 1,
        "cdate": 1666383502728,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666383502728,
        "tmdate": 1670886498063,
        "tddate": null,
        "forum": "O8Vc52xFSUR",
        "replyto": "O8Vc52xFSUR",
        "invitation": "ICLR.cc/2023/Conference/Paper2926/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper addresses learning in continuous action-spaces where circumspection of \"optimal\" behavior is necessary, specifically focused on healthcare. The paper proposes a method by which to constrain the policy search space to only those areas that are \"quasi-optimal\". Detailed theoretical justification and analysis of the proposed quasi-optimal Bellman operator is provided as well as preliminary application of the approach to a diabetes management dataset. ",
            "strength_and_weaknesses": "I'll first list the **strengths** of this paper:\n - The problem setting (constraining continuous action spaces to remain safe) is very important for the potential use of contemporary RL algorithms in real-world scenarios. \n- The authors have appropriately motivated the design of the approach from a theoretical foundation. \n- I found the theoretical justification and design of the quasi-optimal Bellman operator (aided by using q-Gaussian distributions) to be really interesting.\n- For the most part the derivation of the methodology is clear and easy to follow (specific examples of where this is not clear will be listed below in the weakness section). The authors have taken great care to list out the failings or limitations of each component and then how the next concepts address this limitation. This made following the line of reasoning and development much easier.\n\nNow for the **weaknesses**:\n- The use of $\\mu$ is not entirely clear. It's not clearly defined in the paper and shows up as an identifier for the quasi-optimal Bellman operator (first appears in Section 3). It would be helpful if the usage of this parameter (is it a scalar? a stand-in for a sampling policy? etc..) was more precisely discussed when it is first introduced. For example, it's never discussed how $\\mu$ is chosen and what is used for that selection process. Since this construction is critical for defining the disjoint regions of admissible actions, this should be better handled throughout the paper.\n- Figure 1 is not very informative. There is not enough detail in the main body of the paper to round out the insights that the Figure is trying to reaffirm. For instance, it's unclear what actions are admissible since the volume under the curves primarily correspond to the $<2\\mu$ region. I was led to believe (based on the definition of the screening set) that the admissible actions--eg. the support set-- would satisfy the $\\leq2\\mu$ condition. The term \"screening intensity\" is undefined\". \n- The choice of policy class to be concavely quadratic seems to be a pretty strong assumption/constraint? It's unclear whether the component $\\alpha_*(s)$ functions are explicitly learned or not. Being more straightforward about this would be helpful, particularly if there's a difference in how the policies (and underlying value functions) are pragmatically used. There's clear benefit theoretically but...\n- The assumption that equation 10 can be \"solved\" with off-policy data also assumes that the data is self-contained and consistent, right? I mean, that the data provides full coverage and there is no stochasticity/confounding? Otherwise, a batch of data would need to be infinite in order to have the value function and corresponding policy fully converge, correct?\n- The use of the Kernel representation is not very well motivated or explained. This was perhaps the weakest point of the theoretical derivation of the full method in my eyes. I can understand it's utility and why the choice was made. But the follow through and justification (and then actual implementation) is missing from the paper.\n- The experiments are not adequately described nor introduced. I cannot ascribe any significance to the presented results without understanding how the specific experiments were designed to test the proposed method and how the chosen baselines are fairly compared to. I'm not dismissing the results in total but I am not overly enthused by them since it is clear that the DeepRL baselines aren't set up to be real great comparisons (due to dataset limitations as well as the overall design of the algorithms). None of these baseline approaches claim to work in the small data regime so it's no surprise that the observed variance of the results is as high as it is.\n- Continuing with the complaints I have about the experiments, it's unclear what the experimental procedure is. Were the algorithms trained in a fully offline, off-policy fashion? If not, why?\n- The biggest weakness of this paper in my mind and which causes hesitation for me to consider accepting this work is that it does not adequately frame its contributions among the decades of work in safe RL as well as work applying Reinforcement Learning to healthcare challenges. These omissions overlook relevant contributions made in the community recently that address similar technical considerations raised in this paper. I will describe two such papers that the authors should consider as well as point to a really through survey of the field (as of 2020). Beyond this there are several more papers that should be considered by the authors for framing and adequately justifying the development of algorithms and strategies motivated by and for use in healthcare.\n  - First, I'll point to Tang, et al (ICML 2019) \"Clinician-in-the-loop decision making\". Which constructs set-valued policies of nearly optimal actions, learning to ignore those regions of the action space that are not relevant. While this paper doesn't address continuous action-spaces the theoretical foundations and questions about \"quasi-optimal\" learning are very similar.\n   - Second, I'll point to Fatemi, et al (NeurIPS 2021) \"Medical Dead-ends\" which introduces a way to re-think learning value functions in data-limited safety-critical environments by assessing regions of risk and recommending that these actions be removed from the list of options an expert can consider. \n   - For the survey, I strongly recommend Yu, et al (2021) \"Reinforcement Learning in Healthcare: a survey\".",
            "clarity,_quality,_novelty_and_reproducibility": "##Clarity\nFor the most part the paper is well constructed and clearly written. Some of the language connecting the ideas to healthcare could be tightened up to not be so speculative (e.g. \"Clinicians are *pretty* interested...\", etc). One way to do this is by including actual citations of clinical work to justify the statements and concepts. If the authors are speculating, it's probably isn't appropriate for them to be making as broad of claims as they are.\n\n## Quality\nAside from my concerns about the experiments and the lack of clarity around them (shared in the \"Weaknesses\" section above), this paper is of sufficiently high quality. The conceptual and theoretical justification for the proposed method are very well set out.\n\n## Novelty\nAs a core RL method, constraining learning in continuous action-spaces, this paper presents novel methodology. As a RL method proposed for healthcare, there is some cause for concern as some of the concepts and motivations are clearly covered in prior literature. ",
            "summary_of_the_review": "I found this paper to be very well motivated and throughly justified from a theoretical perspective. It's clear that the quasi-optimal learning strategy has some bearing for appropriately constraining policies in continuous action settings. I am less convinced about it's overall use in important safety-critical environments such as healthcare. Conceptually I am aligned with the proposed direction but I cannot currently advocate for the paper's publication at this stage. I would need to see more thorough experimental analysis of how the proposed method actually avoids high-risk or dangerous action decisions in comparison to prior methods to demonstrate that the proposed method is working as intended. The cumulative \"score\" in Table 1 does not communicate that to me. It's nice to see that the proposed method learns \"better\" policies but I'm not sure that it actually is fulfilling the design of the approach. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "details_of_ethics_concerns": "No outstanding ethics concern but I disagree with the authors' estimation that there is no risk in their ethics statement. There is abundant risk of someone implementing and using this algorithm without it being wholly validated in a controlled setting.",
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_xW9e"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_xW9e"
        ]
    },
    {
        "id": "0jW3Hezgic",
        "original": null,
        "number": 2,
        "cdate": 1666621517682,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666621517682,
        "tmdate": 1669079913452,
        "tddate": null,
        "forum": "O8Vc52xFSUR",
        "replyto": "O8Vc52xFSUR",
        "invitation": "ICLR.cc/2023/Conference/Paper2926/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper considers policy learning in continuous treatment setting, particularly in optimal dose finding. The contributions of the paper includes (1) the development of the quasi-optimal Bellman operator to address the non-smoothness issue; (2) the use of q-Gaussian policy distribution to avoid off-policy support; (3) the development of a PAC learnable algorithm with appealing theoretical properties; (4) comprehensive numerical analysis in comparison to several baseline algorithms. ",
            "strength_and_weaknesses": "The strengths and advances of the proposal includes:\n\n1. The development of the introduction of the quasi-optimal Bellman operator for policy learning in continuous action space;\n2. The use of q-Gaussian policy distribution to realize bounded support;\n3. A practical algorithm for quasi-optimal policy learning;\n4. Nice theoretical properties of the proposed algorithm;\n5. The use of real datasets to justify the finite-sample performance of the algorithm.\n\nSome suggestions, questions and comments:\n\n1. The paper considers estimating the optimal policy from an offline dataset. It would benefit from a discussion about existing offline RL algorithms and whether/how these algorithms are related to your proposal. \n2. Page 2 (minor) The notation paragraph. For the two sequences $(\\Psi(m))_m$ and $(\\Upsilon(m))_m$, I guess you require their elements to be strictly positive. \n3. Page 3 (minor) $\\Delta_{\\textit{convex}}(\\mathcal{A})$ is not defined. \n4. I found Section 3.1 a bit difficult to follow. For instance, does Equation (2) holds for any $\\rho$? When $\\rho=0$, it seems the right-hand-side equals the optimal value function. So why does the penalty term $\\rho$ exist on the right-hand-side? In addition, it seems the right-hand-side does not involve the state transition. So why is it referred as the Bellman operator? Equation (3), you may want to discuss the choice of $\\mu$. Is the proximal term used to smooth the policy class? Is $\\mu$ the degree of smoothness? A related question is, I was wondering if the Fenchel representation on the Bellman operator has been employed in some other papers. \n5. It seems Theorem 3.2 implicitly requires the action space to be one-dimensional. Do you have similar results for multi-dimensional action space? \n6. Section 3.3. It was mentioned that Equation (10) can be directly solved using the transition samples, so it overcomes the distributional shift issue in offline data. However, the distributional shift issue usually refers to the difference in the marginal state-action distribution instead of the state transitions. \n7. In additional to the kernel representation, can we use other functional approximators to solve the minimax optimization? Would you please discuss?\n8. Page 6, minimax optimization. It is not clear to me how the method based on average Bellman error solve the double sampling issue. Is it related to the couple estimation method (Antos, Szepesvari and Munos, 2008a; Farahmand et al., 2016)?\n9. Page 8, numerical experiments. The action space in the four environments are one-dimensional. Can we compare the proposed with method against existing policy learning algorithms with certain discretization? This could better support the claim in the introduction section. When the action space is $[0,1]$, you may want to equally define the interval into e.g., 5 or 10 subintervals. When the action space is unbounded, we could first apply certain transformation (e.g., based on the normal cdf) and then divide the interval. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and in good quality. The proposed algorithm as well as the theoretical results are novel. GitHub links are included to reproduce the numerical results. ",
            "summary_of_the_review": "The paper considers policy learning in continuous treatment setting, particularly in optimal dose finding. The proposed algorithm is novel and statistically sound. It is also empirically verified based on simulation and real data applications. However, I have some questions and comments regarding the presentation and the proposed methodology. I hope the author(s) can help address my comments.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_Y35L"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_Y35L"
        ]
    },
    {
        "id": "TfHZBWOhVp",
        "original": null,
        "number": 3,
        "cdate": 1666632182525,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666632182525,
        "tmdate": 1669404066211,
        "tddate": null,
        "forum": "O8Vc52xFSUR",
        "replyto": "O8Vc52xFSUR",
        "invitation": "ICLR.cc/2023/Conference/Paper2926/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors address the problem of an unreliable policy caused by using a Gaussian distribution to represent it in a continuous control setting. They create a new quasi-optimal learning algorithm that uses the q-Gaussian distribution and show that it has provable convergence in off-policy settings. They also analyze other key theoretical aspects of the algorithm, such as sample complexity and consistency.\n\nIn a synthetic environment where there are bounded action regions, they show their method performs better than competitors. They additionally show that it performs better than others in an existing simulator for diabetes.",
            "strength_and_weaknesses": "Strengths:\n1. The authors' arguments are comprehensively supported by theory.\n2. The empirical performance against other algorithms is strong, even deterministic ones that do not suffer from off-support bias.\n\nWeaknesses:\n1. The paper is closely related to offline RL, but there is not a clear discussion on off-policy evaluation or avoiding unwanted behaviors. The experiments appear to use only offline data, but use algorithms that were not developed for offline or safe RL (see #2). The method appears to also be applicable online\u2014it is not clear whether there is also an advantage in online settings.\n2. There is no comparison in experiments with other safe RL approaches. Indeed, there is little discussion of safe RL research in the paper\u2014it is unclear to me why this is. The motivation of the authors seems to lie at the intersection of offline and safe RL.\n3. The appendix gives the impression that the hyperparameters were tuned for the authors' method but not for the others.\n\nPost-response:\nThe authors have done substantial updates to the paper during the review period, including running new experiments. I am mainly satisfied with the paper as it stands and have updated my score.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity: All proofs appear in the appendix. There is not a lot of intuition given for why the authors take the approach that they do.\n\nQuality: It is unclear how significant the work is (see weaknesses).\n\nNovelty: The intuition behind wanting a q-Gaussian distribution is clear and this work is first to do it.\n\nReproducibility: code and access to data is provided.",
            "summary_of_the_review": "The authors present a thorough and mathematical analysis of a new algorithm that claims to address issues that arise in continuous action RL, but it is hard to tell how practical or useful the algorithm is because of limited experimental comparisons.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_RoFn"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper2926/Reviewer_RoFn"
        ]
    }
]