[
    {
        "id": "HcLTDq35gzl",
        "original": null,
        "number": 1,
        "cdate": 1665604170119,
        "mdate": null,
        "ddate": null,
        "tcdate": 1665604170119,
        "tmdate": 1665604170119,
        "tddate": null,
        "forum": "0cm8HroIxJV",
        "replyto": "0cm8HroIxJV",
        "invitation": "ICLR.cc/2023/Conference/Paper1318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper claims to prove representation bottlenecks of ConvNets considering the capacity of representing different frequency components of an input sample. It claims to introduce the rule of the forward propagation of intermediate-layer spectrum maps, as equivalent to the forward propagation of feature maps through a convolutional layer. It focus on layers representing convolutions, and zero-padding operations.\n",
            "strength_and_weaknesses": "The formulation seams to claim as novel existent well known concepts, at the same time that oversimplify the analytical model of ConvNets.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The novelty of the paper is not clear. The formulated model for ConvNets has oversimplified assumptions (removing non-linearities) and some the presented claims seams to be presented in well known theory (Fourier support of Convolutions) or previous papers on spectral bias. ",
            "summary_of_the_review": "I dont see the novelty of this paper. \n\nIt presents as new, a theorem that conducting the convolution operation on an input feature F is essentially equivalent to\nconducting matrix multiplication on spectrums of F (Theo 2). That is a well known results for Fourier analysis.\n\nNext, it reformulates the cascade of convolutional layers, without activation functions. It is also well known that a stack of convolutional layers can be rewritten as a single convolutional layer if no non-linearities are used. But such a simplification is much less expressive than deep models adopting non-linearities. \n\nOn the experimental section, the paper claims to 'Verifying that a neural network usually learned low-frequent components first.' This result and analysis was previously presented in Rahaman et al.(2018) On the Spectral Bias of Neural Networks.\n\nThe paper also investigate the effect of zero padding, and upsampler layers, but for this last one, the formulation adopted seams to be the introduction of zeros on new subpixel positions (similar to those modeled by a dilated convolutions), other than investigating what happen with upsampling filters. ",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_fHYG"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_fHYG"
        ]
    },
    {
        "id": "w8PgKxbl0Dx",
        "original": null,
        "number": 2,
        "cdate": 1666450501899,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666450501899,
        "tmdate": 1666450544088,
        "tddate": null,
        "forum": "0cm8HroIxJV",
        "replyto": "0cm8HroIxJV",
        "invitation": "ICLR.cc/2023/Conference/Paper1318/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper studies the forward pass of a convolutional decoder in the frequency domain. It finds that the CNN layer forward propagates each frequency component in the\nspectrum map independently to other frequency components.\nIt also finds that the CNN operations make a convolutional decoder network more likely to weaken high-frequency components.\n",
            "strength_and_weaknesses": "Strengths:\n\n\u2014-------------------\n\nThe paper has many strengths and is tackling an important problem. For the sake of time  I am only writing about weaknesses in this review, since those are the ones that should be actioned upon.\n\n\nWeaknesses (W)\n\n\u2014-------------------\n\nW: As far as I understand the analysis for the deep network is done on an idealized frequency domain network that does not correspond to a CNN with ReLUs and finite size filters. If this is true, this is the biggest drawback of the current paper. Furthermore, in real world networks, very small filters (e.g., 3x3 or even 2x2) usually provide the best results.\n\nW: Keeping in mind the previous comment, the paper\u2019s language might be too strong. It talks about proving various aspects of the forward propagation of the CNN, but maybe the proofs only apply to the idealized network?\n\nW: Regarding zero-padding, there are previous works that try to compensate for the resulting border effects, e.g., by learning different filters to the borders or using circular or other types of padding and convolutions. Could the authors discuss how zero padding vs. these methods affect the end-results of the CNN decoder and whether it would be beneficial not to zero-pad.\n\nW: The paper makes the claim that decoder CNN is biased towards learning low-frequency content. What is missing is the analysis on when this is actually harmful and whether it could be beneficial in some settings. Also, in cases where this is harmful, it would be interesting if the authors could provide a suggestion for improvement.\n\nW: The analysis is done for randomly initialized networks. While this is an interesting point of analysis, it is lacking the effect of training altogether. I would suggest the authors to extend their work towards the training as well.\n\nW: Based on Figure 3, ReLU seems to have a big effect when comparing the freq domain version of the network with the original one. A big downside of the analysis is the lack of analysing the effect of ReLU, since, of course, the activation function is a key component in enabling nonlinearity in the neural networks.\n\nW: The authors should bring the related work section from the appendix to the main paper and also build a connection with related works on freq domain neural networks, for example. Pan, H., Chen, Y., Niu, X., Zhou, W., and Li, D., \u201cLearning Convolutional Neural Networks in the Frequency Domain\u201d, arXiv e-prints, 2022. https://arxiv.org/abs/2204.06718 , and many others.\n\nW: The authors note that their work is different from the \u201cF-Principle\u201d e.g., https://arxiv.org/abs/1807.01251 because the current paper focuses on a \u201cfully different type of frequency\u201d, i.e., the frequency w.r.t. the DFT on an input image or a feature map. To me this distinction is not so clear and both works seem to be related. I would suggest the authors to build a better bridge between the works, since even the authors seem to be doing similar experiments, e.g., in \u201cC.1 VERIFYING THAT A NEURAL NETWORK USUALLY LEARNED LOW-FREQUENT COMPONENTS FIRST.\u201d\n\n\n\nMinor issues and spelling mistakes:\n\nB, page 26: \u201cDNN first qucikly\u201d -> DNN first quickly\n\nPage 27: \u201cDNNs tipically\u201d -> DNNs typically\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The writing clarity and quality is ok. There are many minor grammar mistakes though. There seems to be novelty in the works, and it should be reproducible.\n",
            "summary_of_the_review": "Decent work with some issues that would warrant a new revision before acceptance.\n",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_V1PA"
        ]
    },
    {
        "id": "2zOjK6NqYS",
        "original": null,
        "number": 3,
        "cdate": 1666760575716,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666760575716,
        "tmdate": 1666760575716,
        "tddate": null,
        "forum": "0cm8HroIxJV",
        "replyto": "0cm8HroIxJV",
        "invitation": "ICLR.cc/2023/Conference/Paper1318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper provides the bottleneck of representations in linear convolutional networks. This is based on the frequency analysis of the input and how it propagates into the network. They characterize and analyze the network in detail. Here are some of their statements as contributions: Each frequency propagates independently of the other through network mapping. The convolution and other operations related to it (e.g., zero padding) damp high-frequency components. The last finding (in their own wording) is that upsampling results in strong signals repetitively appearing at certain frequencies.",
            "strength_and_weaknesses": "The paper is written well and explained very clearly. The authors make it clear how their approach to frequency encoding differs from prior works. I very much enjoyed reading the paper. However, I have mixed feelings about this work. The paper gives lots of insights into linear convolutional neural networks. For someone with a signal processing background, however, I find the work to explain the known basics of discrete-time signal processing [1] to the deep learning community. Below I explain in detail.\n\nI found the work to miss acknowledging the digital signal processing literature and their terminology in this paper. The paper misses the literature on discrete-time signal processing. All the properties mentioned in this paper are known for convolution operators. Moreover, the authors lack certain terminologies known in the literature. For example, the \"repeats strong frequency components of the input\" in Figure 1 and (Conclusion 4) is known as \"aliasing\". Below are a few examples.\n- Their first theorem and the second are basic principles of convolutions and discrete Fourier transform. Could the authors explain what is new about this result?\n- The independence of DFT points in a convolution operator is known in the literature. What is new here?\n- Upsampling and aliasing are known in the signal processing literature. It is not clear what is new in their results on upsampling.\n- It is very clear from the perspective of signal processing that if the kernels are initialized with Gaussian distributed data, then the spectrum allows follows a Gaussian shape, and depending on the variance, etc. it acts as smoothing. Is this something new?\n\nThere are certain simplifications that are extreme.\n- On page 3, the authors mention that the diffraction process is small and can be ignored, but perhaps the main reason the cosine similarity in Figure 3 is not exactly 1 or very very close to 1 is due to the diffraction process term. Assuming the theorem is correct, Figure 3 indicates that the diffraction process should not be ignored.\n- Non-linear activations are building blocks of neural networks. This paper considers a linear network. This is extreme simplification and reduces the setting into a case that is known and studied already in the literature (Convolution operator or cascade of convolution operators are known for how they perform in the signal processing literature). Please state clearly in the abstract that you are considering a linear network.\n- They provide analysis of the network when it is NOT trained (or very early stage which is still assumed to behave like an untrained network) with independent variables and the assumption that the parameters are drawn from Gaussian distribution.\n\nMisleading statements:\n- \"zero-padding operation boosts magnitudes of low-frequency components of feature spectrums of the feature map\". Zero-padding in the spatial-image domain, from the perspective of DFT, increases the resolution in the frequency domain. It does not boost low-freq or filter high.\n- The author's statement on the weakening of high-freq encoding is not generally true. This depends on the frequencies that each convolutional kernel passes. In their framework, they assume that the kernels pass low frequencies implicitly by using Gaussian distributed data. Moreover, this depends on the data the network will be trained on. For example, as pointed out by the authors in Remark 5, natural images dominate low-frequencies, hence, CNNs trained on natural images tend to filter high-frequencies.\n\nSome word usage is not precise: \"Random noises\" in Section 3? I guess you mean drawn randomly from a particular (Gaussian) distribution. Please clarify \"irrelevant\" in Section 3? Do you mean \"independent\"? By \"upsampling\" do you mean \"expansion\"?\n\nPerhaps, if the work can be generalized to non-linear convolutional neural networks or networks that are trained and their dynamics, the findings would be new.\n\nMinor comments:\n\n1. I suggest not using a bold font in sentences.\n2. I found the first paragraph very disconnected from the main focus of the paper. It's the authors' choice, but I recommend making a better connection or starting with the second paragraph. For example, does the paper directly explain why DNN fails in any of the problems in the first paragraph? No, but you may say indirectly they can provide insights.\n3. Section 3, line 7, \"kenel\" --> \"kernel\".\n4. There is an extra \")\" in (11).\n\n[1] Discrete-time signal processing by Oppenheim and Schafer.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clear and nicely written. Almost all the findings of this paper are known basic principles of convolutions in signal processing. Hence, the work has minimal new findings.",
            "summary_of_the_review": "I very much enjoyed reading the paper. The paper gives lots of insights into linear convolutional neural networks and how frequency information passes through the network. However, all those insights are already known in the literature. I do not recommend the acceptance of this work for the following three reasons:\n\n1. Almost all the analysis and intuitions given by the paper are known for convolution operators (convolutions of convolutions is also a convolution). When the authors reduce their neural network into a convolution, then all results are immediate. Hence, results on the independence of frequency propagation, how down zero-padding works, and effect upsampling are already known for convolutions. \n2. Assumptions and simplifications are extreme.\n3. The paper does not acknowledge signal processing literature and its wording implies their work is new.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_wmw7"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_wmw7"
        ]
    },
    {
        "id": "LFi_3ay6Ax",
        "original": null,
        "number": 4,
        "cdate": 1667229921197,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667229921197,
        "tmdate": 1667229921197,
        "tddate": null,
        "forum": "0cm8HroIxJV",
        "replyto": "0cm8HroIxJV",
        "invitation": "ICLR.cc/2023/Conference/Paper1318/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper studies the effect of cascaded convolutional decoder network on the spectrum domain, and presents several findings summarized in the experiment section.",
            "strength_and_weaknesses": "Strengths:\n\n1. The authors performed a detailed study on the effect of different components in a decoder network on the spectrum domain by discrete Fourier transform.\n2. Extensive experiments are conducted to support the claims of the authors.\n\nWeakness:\n\n1. The findings of the paper are not surprising, and using discrete Fourier transform to study the learning of neural networks on the frequency domain is not novel. Previous works have proved that neural networks tend to learn low-frequency components [1] with the help of discrete Fourier transforms [2,3]. More discussions need to be included on the significance of this paper compared to previous works. \n\n2. The findings are limited to decoder networks. The theoretical proof does not consider common activation layers in decoder networks such as ReLU.\n\n3. The paper's organization within Section 2 and Section 3 is messy.\n\nBesides the above weakness, the other issue is about the (mathematical) novelty in many theoretical results. For example, Theorem 1, Corollary 1, and Theorem 2 are just another format of the very famous convolution theorem: Fourier transform of the convolution of two inputs signals is the elementwise product of the Fourier transform of the two input signals. \n\n[1] Kiessling, Jonas, and Filip Thor. \"A Computable Definition of the Spectral Bias.\" AAAI 2022.\n\n[2] Rahaman, Nasim, et al. \"On the spectral bias of neural networks.\" ICML 2019.\n\n[3] Xu, Zhi-Qin John, et al. \"Frequency principle: Fourier analysis sheds light on deep neural networks.\" arXiv preprint arXiv:1901.06523 (2019).\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "The organization of this paper is not very clear, with many theoretical findings intervened with bold-font bullets. The novelty of the theoretical results is limited. One should have no difficulty implementing the convolutional decoder network to verify the claims of this paper based on the description in the paper.",
            "summary_of_the_review": "While this paper presents various effects on the spectrum domain of different components in a cascaded convolutional decoder network, the paper lacks mathematical novelty in its theoretical results and discussion/comparison with closely related works. The presentation of this paper should also be improved significantly. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_oTym"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper1318/Reviewer_oTym"
        ]
    }
]