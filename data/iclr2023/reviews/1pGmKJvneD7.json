[
    {
        "id": "oOF242Pltb",
        "original": null,
        "number": 1,
        "cdate": 1666590512662,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666590512662,
        "tmdate": 1666590597358,
        "tddate": null,
        "forum": "1pGmKJvneD7",
        "replyto": "1pGmKJvneD7",
        "invitation": "ICLR.cc/2023/Conference/Paper3340/-/Official_Review",
        "content": {
            "confidence": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
            "summary_of_the_paper": "This paper introduces Lattice Vector Quantization (LVQ) to hyperprior-based learned image compression. The LVQ is applied to co-located feature samples along the channel dimension. Moreover, it applies QMC, a traditional technique, to arrive at differentiable rate estimates. The LVQ is incorporated into Cheng\u2019s learned codec (CVPR2020) to validate its coding performance. ",
            "strength_and_weaknesses": "Strengths:\n\n(1) The idea of introducing Lattice Vector Quantization to learned image compression is interesting and novel. \n\n(2) The RD performance of the proposed method looks impressive, showing up to 18% BD-rate saving over VTM. \n\nWeaknesses:\n\n(1) A similar work in CVPR2022 [1] that addresses Vector Quantization for learned image compression should be included for comparison and mentioned in related work. The novelty over [1] should be highlighted too. \n[1] Zhu, Xiaosu, et al. \"Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n(2) More ablation experiments should be conducted to analyze the complexity-performance trade-off among different choices of the lattice generating matrix. The complexity should include the model size, MAC, peak memory requirements, and encoding/decoding runtimes. It is unfortunate that these experiments are currently missing. \n\n(3) In Eq. (8), the LVQ is modeled by additive uniform noise during training. This additive noise model does not appear to consider the generating matrix chosen for LVQ.\n\n(4) The choice of the generating matrix is empirical and not well justified. \n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is easy to follow and is a serious work. The novelty is moderate. The paper provides enough details to reproduce the results. ",
            "summary_of_the_review": "The idea of introducing Lattice Vector Quantization (LVQ) to learned image compression is interesting and novel. Several well known LVQ techniques, including the design of the lattice generating matrix and the rate estimation, are extended to address learned compression. While it is intuitively agreeable that the coding performance may be improved further by introducing LVQ, the increased complexity is another issue that must be addressed. It is unfortunate that this aspect is currently missing. Moreover, one key reference is missing too.  ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_9ZNU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_9ZNU"
        ]
    },
    {
        "id": "hJMHawZmA8",
        "original": null,
        "number": 2,
        "cdate": 1666594894130,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666594894130,
        "tmdate": 1666594894130,
        "tddate": null,
        "forum": "1pGmKJvneD7",
        "replyto": "1pGmKJvneD7",
        "invitation": "ICLR.cc/2023/Conference/Paper3340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The authors propose to incorporate lattice vector quantization into recent hyperprior-based image compression framework. Due to the nature of lattice VQ, the proposed method achieves better R-D performance without optimizing and storing a large VQ codebook. Furthermore, for end-to-end optimization, the codeword probabilities are approximated by the Monte Carlo integration of multivariate normal distribution. On Kodak dataset, the proposed method achieves 18% BD-rate saving over the latest video coding standard H.266/VVC.",
            "strength_and_weaknesses": "Strengths: \n1.\tThe proposed entropy model is interesting and inspiring, i.e., approximating the discrete codeword probabilities by using the integration of continuous probability density.\n2.\tThe overall performance on Kodak dataset is significant.\n\nWeaknesses:\n1.\tComplexity and practicability. First of all, the authors didn\u2019t provide any complexity evaluation, such as model size and encoding/decoding time. Despite the authors claimed that the use of lattice VQ solves the problem of codebook size bloating and provides fast quantization process, there is no experimental result to support this claim. More importantly, for lattice VQ, the complexity of practical entropy coding still exponentially increases with VQ dimensions. As mentioned in Section 3.2.2, the model calculates the cumulative probability table of a huge amount of sorted codewords, where the complexity is even larger than that of conventional vector quantization process. The entropy model needs to first produce and store the density integration of each codeword in the lattice VQ codebook, and it then obtains the cumulative probability table at both the encoder and decoder sides. In this paper, the VQ dimensions are set to 24, which is quite large for practical entropy coding. It is important to provide a detailed complexity evaluation for each part of the proposed method. Besides, are the claimed R-D results in Figure 4 & Figure 5 the practical coding results or the estimated results (by using -log(P))?\n2.\tMore results on more datasets. The proposed method is only evaluated on Kodak with PSNR metric. What about the performance on high-resolution datasets such as CLIC-2021 and Tecnick? And it would be better to provide the results in terms of MS-SSIM.\n\nMinor concerns:\n1.\tA related work [1] for VQ-based image compression is missing. Please provide the performance comparison and analysis.\n[1] Zhu, Xiaosu, et al. \"Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression. CVPR2022\n",
            "clarity,_quality,_novelty_and_reproducibility": "The clarity, quality and novelty of the work are ok, despite the reproducibility need to be improved and verified.",
            "summary_of_the_review": "Even though I think the idea is interesting and deserving of discussion (especially the entropy modeling), I still have concerns about the complexity and practicability of the proposed method, which play a crucial role for the task of image compression. And the experimental results are not enough to support the authors\u2019 claims. I will reconsider based on the authors' response.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_fFaA"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_fFaA"
        ]
    },
    {
        "id": "B8pltmesup",
        "original": null,
        "number": 3,
        "cdate": 1666600159551,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666600159551,
        "tmdate": 1666600927875,
        "tddate": null,
        "forum": "1pGmKJvneD7",
        "replyto": "1pGmKJvneD7",
        "invitation": "ICLR.cc/2023/Conference/Paper3340/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper introduced Lattice Vector Quantization into VAE-based image compression framework. Besides, a multivariate normal distribution including covariance matrix parameters are proposed. Experiments demonstratedd the effectiveness of these two features with a 18% BD-rate save compared with VTM15.0.\n",
            "strength_and_weaknesses": "Experiments including the ablation study are convincing. The performance is also good.\nA more detailed introduction about some aspects of lattice vector quantization is preferred. For example, how the vector quantization matrix generation is performed during training.\nFor the experiment part, the effect of this quantization method on the computational complexity and memory consumption during training and inference is also expected. It will also be better if more experiments on different anchors (except Cheng) and different test datasets are performed.\n",
            "clarity,_quality,_novelty_and_reproducibility": "This paper is overall of good quality and some novelty. The layout of figures and tables can be improved. Some figs and tables are not well aligned. (For example: (Fig4 and Table 1), (Fig5 and Table2)). A related work [1] should be included for discussion and comparison.\n[1] Zhu, Xiaosu, et al. \"Unified Multivariate Gaussian Mixture for Efficient Neural Image Compression. CVPR2022\n\n",
            "summary_of_the_review": "This paper demonstrated that Lattice Vector Quantization can further improve the coding performance of VAE-based image compression framework. Experiments show the effectiveness but the conclusion can be stronger if more experiments can be performed. If the authors can provide more information about the implementation details of lvq, we can give a more solid recommendation.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_nBnd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_nBnd"
        ]
    },
    {
        "id": "klAhycf8G0H",
        "original": null,
        "number": 4,
        "cdate": 1666642281286,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666642281286,
        "tmdate": 1666642281286,
        "tddate": null,
        "forum": "1pGmKJvneD7",
        "replyto": "1pGmKJvneD7",
        "invitation": "ICLR.cc/2023/Conference/Paper3340/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper presents a neural image compression models based on lattice vector quantization (LVQ).\n\nThe authors note two deficiencies in existing work: (1) most models use scalar quantization (SQ), which is typically less powerful than vector quantization (VQ) if a large enough codebook is used, and (2) models that do use VQ do not use a hyperprior or end-to-end optimization. The authors address these problems by using LVQ, which has a large, though constrained, codebook, and they show how to jointly optimize a distribution over the lattice (the entropy model) with a hyperprior and spatially autoregressive model.\n\nTwo other important contributions are the evaluation of predicting a full covariance matrix for each LVQ group, rather than a diagonal covariance matrix, and using Monte Carlo integration to more quickly calculate (an approximation of) the likelihood of each feature vector.\n\nThe result is, as far as I know, state of the art rate-distortion (RD) performance for lossy image compression. The evaluation shows an 18% rate savings over VTM, one of the best standard (hand-engineered) image codecs, as well as gains over other learning-based (neural) codecs (Fig 5 and Table 2). The paper also compares different lattice dimensionalities and the effect of predicting a full covariance matrix (Fig 4 and Table 1).\n",
            "strength_and_weaknesses": "The primary strength of this paper is the achieving SOTA rate-distortion performance. As far as I know, now other papers or standard codecs achieve as good a compression rate when using PSNR as the quality metric.\n\nThe primary weakness of the paper is a lack of runtime data. I suspect this method is extremely slow since spatially AR context models are known to be slow due to the difficulty of parallelizing the decoding process, and the prediction of a full covariance matrix leads to a large number of output parameters (if I understand the setup correctly, 192 channels using \\Lambda_24 leads to 8 groups of 24D vectors and thus 8 * (24^2 + 24) = 4800 parameters per spatial location in the latent image representation). I suspect this leads to decode times of multiple seconds, even on fast hardware, when a deployable model needs to decode kodak-size images in a handful of milliseconds. To the best of my knowledge, no neural methods achieve decode speeds on par with standard codecs like VTM (or BPG or JPEG, etc.), but other models are probably two orders of magnitude faster than the method in this paper while only losing 5-10% in terms of rate.\n\nThe other weakness is a lack of direct comparison against stronger neural methods. Balle 2018, Minnen 2018, and Cheng 2020 are all relatively old methods. In particular, part of the motivation of this paper is the lack of modeling the dependencies between channels (SQ doesn't do this, VQ does). But existing papers adopt other methods for (partially) modeling these dependencies, for example:\n\nChannel-wise Autoregressive Entropy Models for Learned Image Compression\nDavid Minnen, Saurabh Singh\n(ICIP 2020) https://arxiv.org/abs/2007.08739\n\nELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding\nDailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, Yan Wang\n(CVPR 2022) https://arxiv.org/abs/2203.10886\n\nTransformer-based Transform Coding\nYinhao Zhu, Yang Yang, Taco Cohen\n(ICLR 2022) https://openreview.net/forum?id=IDwN6xjHnK8\n\nThe channel-wise AR approach used by these papers should be mentioned and compared to the LVQ approach adopted here since they both address the same underlying deficiency. I believe the RD performance of the LVQ approach is better than what is reported in these papers. However, the runtime is vastly different (ELIC in particular is focused on balancing RD performance with runtime), and the preferable trade-off probably falls to ELIC over the proposed LVQ method, i.e. I wouldn't accept 10-100x slower decoding to save 5-10% on file sizes.\n\nFinally, I'd like to better understand how much of the RD performance is due to the LVQ vs. the full covariance matrix. Fig 4 and Table 1 partially answer the question by showing that for several lattice dimensionalities, predicting a full covariance matrix helps (as expected). Another interesting comparison would be to use a trivial lattice (hyper-cubes) but still group the channels into 24D vectors and predict a full covariance matrix. I expect \\Lambda_24 is better (the underlying theory predicts this for mse) but the size of the gap would make it clear whether or not the added complexity of \\Lambda_24 is warranted.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The overall quality of the paper is quite good.\n\nNovelty is medium. While the results are SOTA and I agree with the authors' claim that previous work has not used LVQ with a hyperprior and end-to-end optimization, all of the components do already exist.\n\nReproducibility seems fairly difficult so releasing code to set up the lattice and approximate the likelihoods would help significantly (the other components already exist in opensource projects like TensorFlow-Compression and CompressAI).\n",
            "summary_of_the_review": "As far as I know, this paper presents state-of-the-art rate-distortion results for lossy image compression. That's a significant achievement given the existing work on this problem.\n\nThat said, the improvement in rate-savings is only moderate compared to the best existing methods (see references above), and the other methods appear to support much faster decode speeds, which is crucial for deployment.\n\nSo my recommendation is \"5: marginally below the acceptance threshold\" since it's not entirely clear how much of the performance gain comes from LVQ vs. predicting a full covariance matrix within each VQ group vs. the spatial autoregressive model.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper3340/Reviewer_SVp9"
        ]
    }
]