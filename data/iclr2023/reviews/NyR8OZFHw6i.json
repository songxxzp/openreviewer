[
    {
        "id": "j1TsuBeL5eS",
        "original": null,
        "number": 1,
        "cdate": 1666653595446,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666653595446,
        "tmdate": 1666653595446,
        "tddate": null,
        "forum": "NyR8OZFHw6i",
        "replyto": "NyR8OZFHw6i",
        "invitation": "ICLR.cc/2023/Conference/Paper5189/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "FIGARO is a model trained on a seq2seq task, where the input is either a human-interpretable high-level description of the music or a learned sequence of VQ-VAE tokens. Output is a symbolic music composition in the proposed REMI+ format, an extension to REMI that adds multi-instrument and multi-time signature capabilities. The authors show via several experiments and metrics that the produced music reflects the inputs given to the model.",
            "strength_and_weaknesses": "**Strengths**\n\n- The experiments and metrics are convincing that the model does utilize the inputs when constructing the output music.\n- The listening study indicates that the output music is of an overall high quality (or at least as high as the training dataset).\n- Code and weights are available for download.\n\n**Weaknesses**\n\nMy main concern is that the motivation for the model is somewhat unclear. The authors say that providing the expert description is easier than actually writing the music, but it seems to be only marginally easier. You still have to annotate every bar and include attributes that aren't necessarily musically intuitive, like note density and mean pitch. I could perhaps see constructing an UI that makes these attributes more intuitive. The authors do mention some of these issues in the Discussion section, but it would have been nice to see a more clear path toward direct usability.\n\nI also don't understand the motivation for the learned representation at all. It doesn't seem to be suitable for human input, so I guess another model would be necessary to translate between another representation and the learned tokens?",
            "clarity,_quality,_novelty_and_reproducibility": "**Novelty**\n\nThis work is most closely related to MMM (Ens & Pasquier, 2020) in that it allows for multi-track, multi-instrument symbolic generation. This paper expands on those ideas by adding more conditioning controls.\n\n**Quality**\n\nExtensive experiments are done to ensure model output is based on input and that overall quality is close to the training dataset.\n\n**Reproducibility**\n\nCode and weights are available for download. A colab demo provides the ability to interact with the model.\n\n**Clarity**\n\nOverall the paper is well written, but I had a few clarifying questions:\n\n- Section 4, Description-to-Sequence Model: How is training done so that the model supports any combination of expert or learned annotations? Is a combination selected randomly for each input?\n\n- Section 6: I thought calling unconditional generation \"random guessing\" was a little misleading. Perhaps you could say something about sampling a sequence based on the training distribution prior.\n\n- Section 6.1: \"both $F_{expert}$ and especially $F_{learned}$ require time and domain knowledge to create\" How would someone go about creating $F_{learned}$, even with domain knowledge?\n\n- Section 6.1: There's a mention of $F_{latent}$ that I think was supposed to be $F_{learned}$\n\n- Section 6.2: This doesn't sound like style transfer to me, this is more of a reconstruction metric. Style transfer would usually imply that some semantic quality of the music remains constant while underlying details change, but here you're just looking for as close of a reconstruction as possible. The \"Description mixing\" test sounds closer to style transfer, but from listening to the outputs, it's not clear to me that it's successful as a style transfer process.\n\n- Links throughout the paper are not clear. It was only by accident that I realized I could click on the Colab link. I would suggest highlighting the links or just including the URL to make it more obvious.",
            "summary_of_the_review": "FIGARO provides a way to condition symbolic music generation on several per-bar controls, and experiments show that these controls are successful. However, the use case is not completely clear.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_g33b"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_g33b"
        ]
    },
    {
        "id": "j5WtXw8Gs2",
        "original": null,
        "number": 2,
        "cdate": 1666682343494,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666682343494,
        "tmdate": 1666825758604,
        "tddate": null,
        "forum": "NyR8OZFHw6i",
        "replyto": "NyR8OZFHw6i",
        "invitation": "ICLR.cc/2023/Conference/Paper5189/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "The paper describes a system for training a multi-instrument symbolic music generation model conditioned on two types of features: human-interpretable attributes that can be extracted programmatically, and latent VQ-VAE encodings that are learned.  Both feature types can be used to exert fine-grained control over a piece of generated music, as demonstrated in several experiments.",
            "strength_and_weaknesses": "Strengths\n-----------\n1) The paper does exactly what it claims: proposes a system for controlling multi-instrument music generation with human-interpretable and learned latent features, and evaluates the technical aspects of the system.\n\n2) The samples on SoundCloud sound quite reasonable.\n\nWeaknesses\n--------------\n1) Far and away the biggest thing missing here, even though the authors may consider it out of scope for ICLR, is some kind of user study demonstrating that this method of controlling a music generation system is helpful to musicians or composers, expert or novice.  The experiments in the paper demonstrate that the system *obeys* the controls and produces plausible music, but not that it is usable.  And based on the baseline models, it isn't surprising that the model would obey the conditioning.  Much of the effectiveness of such a system seems like it would depend on exactly what attributes are used for expert conditioning, and this question isn't explored beyond measuring the impact of dropping out individual attributes, nor is there discussion of why these attributes were chosen.\n\n2) Two major weaknesses of the system that I suspect limit its real-world utility are its lack of support for a) infilling and b) missing controls.  Even though the conditioning signal is temporally fine-grained, a change to the conditioning at time T1 may have effects at time T2 distant from T1, which is likely not what users want.  And having to provide two types of conditioning values (one with multiple attributes) at all positions in a piece of music is likely to be very cumbersome for users.\n\n3) Minor issues:\n    * The paper mentions that an online demo is available; however no link is provided.\n    * \"GitHub\" is misspelled in the footnote on page 2.",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is clearly written, original if somewhat incremental, seemingly experimentally sound, and I believe I could reproduce it with some effort.",
            "summary_of_the_review": "The paper seems correct and is a valid research contribution to the area of music generation.  I sincerely hope the authors (or someone else) writes another paper that explores the HCI questions relevant to the system presented in this paper.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_GHu3"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_GHu3"
        ]
    },
    {
        "id": "Pqueo_fQu_",
        "original": null,
        "number": 3,
        "cdate": 1666685830790,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666685830790,
        "tmdate": 1666685830790,
        "tddate": null,
        "forum": "NyR8OZFHw6i",
        "replyto": "NyR8OZFHw6i",
        "invitation": "ICLR.cc/2023/Conference/Paper5189/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper describes a music generation approach which aims to extend the type of conditioning variables typically used in this area of research. The authors describe this as so-called description-to-sequence learning. Inspired by recent progress in text-conditioned image generation, the authors propose to derive descriptions of music in terms of musical attributes which can be easily extracted from the data itself. These descriptions are termed as expert descriptions. In addition, to fill in the gaps in the expert descriptions per bar, they also propose to learn descriptions using a VQ-VAE. Combining the two: expert and learned descriptions, the authors learn a sequence to sequence model transforming said descriptions to musical sequences. The proposed method is compared against competing methods using both objective and subjective evaluations. In all cases, FIGARO outperforms the baseline approaches. The authors also carry out various ablation studies showing the benefit of different types of expert descriptions.",
            "strength_and_weaknesses": "Strengths:\n- The proposed method is elegant in combining domain knowledge of music to extract attributes along with a VQ-VAE to learn latent descriptors of measures which are fed to a transformer encoder-decoder architecture to generate musical sequences.\n- The approach extends the fine-grained control of previous approaches to more musical dimensions thus affording more control in the output.\n- The results, both qualitative and quantitative look and sound good.\n\nWeakness:\n- The contributions seem a bit incremental. While it's true that this has not been done before in symbolic music, the setup is pretty similar to Jukebox with the addition of some extra musical attributes as conditioning minus the prior to generate conditioning latents/descriptions. In fact, the absence of the prior feels like a big weakness, but this is left as future work by the authors.\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper quality is good and writing is very clear. The work is also original, although I feel it lacks novelty a bit.\nThe authors have shared the code in the supplementary material and it seems like they plan to share it online as well.",
            "summary_of_the_review": "My overall impression of this paper is that it presents a simple yet effective method for controllable music generation. My only issue is that it feels a little incomplete and does not extend existing literature to the extent I would have hoped. Despite that, the results sound convincing and the user study shows that this method outperforms some competing approaches. Thus I am rating it as a weak accept.",
            "correctness": "4: All of the claims and statements are well-supported and correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_W9Fj"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_W9Fj"
        ]
    },
    {
        "id": "4O3F9pHRXEU",
        "original": null,
        "number": 4,
        "cdate": 1667349475245,
        "mdate": null,
        "ddate": null,
        "tcdate": 1667349475245,
        "tmdate": 1667523017637,
        "tddate": null,
        "forum": "NyR8OZFHw6i",
        "replyto": "NyR8OZFHw6i",
        "invitation": "ICLR.cc/2023/Conference/Paper5189/-/Official_Review",
        "content": {
            "confidence": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper presents a transformer-based conditional model to generate multitrack symbolic music. The model utilizes a \"description to sequence\" learning scheme. The authors show that the model produces more desirable musical sequences when human-interpretable features are incorporated during training.\n\n",
            "strength_and_weaknesses": "Strengths:\n\n- Novel training objective\n- The model can handle multi-track, multi-signature symbolic music\n- Comparative experiments between strong baselines in different settings with respect to numerous evaluation measures\n\nThere are a few cosmetic issues, which are explained below.\n\n",
            "clarity,_quality,_novelty_and_reproducibility": "- The paper is well-written and very easy to follow. The approach, experiments and results are clearly presented, and the appendices are very well prepared. \n- The literature review is comprehensive.\n- The model is built on past work, yet it's sufficiently novel\n- The code and model weights are shared. The implementation details (Appendix A) and extended data representing (REMI+) provide the readers with enough detail to reproduce the model.\n\nMinor issues:\n- At the start of page six: \"macro overlapping area (MAO)\" -> should have been MOA\n- Hyperlinks to websites (e.g. SoundCloud, Google Colab) should be more explicit, e.g. as shortened URL\n- Please cite a work describing \"mode collapse\"",
            "summary_of_the_review": "The work advances the state-of-the-art. The usage of learned and expert descriptions is sound. The experiments are comprehensive, and the dataset is large and representative enough (for music information research). The authors also extend the existing data representation in existing datasets. Given these strengths, I would like to recommend the paper for acceptance.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_itPu"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper5189/Reviewer_itPu"
        ]
    }
]