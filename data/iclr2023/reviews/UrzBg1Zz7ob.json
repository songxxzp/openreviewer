[
    {
        "id": "e9QoeGw5QN",
        "original": null,
        "number": 1,
        "cdate": 1666558254934,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666558254934,
        "tmdate": 1666558254934,
        "tddate": null,
        "forum": "UrzBg1Zz7ob",
        "replyto": "UrzBg1Zz7ob",
        "invitation": "ICLR.cc/2023/Conference/Paper4951/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper puts forth a methodology to understand the underlying factors (in the model and/or data) that affect blindspot detection methods. They rely on synthetic data wherein the number and nature of blindspots can be controlled. They also propose a new blindspot detection approach, PlaneSpot.",
            "strength_and_weaknesses": "Blindspot detection is an important problem and understanding how to quantify the effectiveness of different approaches and identify the factors that influence them in real datasets could be valuable. \n\nHowever, there seems to be a huge gap between the simplistic settings the authors study in this paper and real-world datasets. That would still be ok if the authors showed how their analysis could help improve blindspot detection in more realistic settings (eg, on ImageNet). As of now, the paper just reads as an analysis of a simple setting that may or may not be reflective of practice. As the authors themselves mention in the paper, it is not clear whether model/BDM trends on artificially induced blindspots are predictive of trends on naturally occurring ones.\n\nMore specific comments/questions:\n\n- In general, in the results in Figures 3, 4 and 6, one thing that is not clear is how sensitive the model is actually to these blindspots. In particular, an important quantity to visualize is model performance on blindspot datasets as the number of blindspots/specificity, etc. is varied. After all, just because the dataset has a blindspot doesn\u2019t mean the model learns it. It is entirely possible that as the number of blindspots increase, the model tend to not learn all of them. This is a key confounder in the results that must be ablated.\n- What exactly are the features in the synthetic dataset?",
            "clarity,_quality,_novelty_and_reproducibility": "The writing of the paper could be made clearer, especially details on the synthetic ECs. My concerns with novelty/usefulness of findings are discussed above.",
            "summary_of_the_review": "Overall, the analysis in the paper and its connections with blindspots in practical models on large-scale datasets is not yet sufficiently compelling. Therefore, I recommend rejection.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_X8MU"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_X8MU"
        ]
    },
    {
        "id": "ICoR5D11p7",
        "original": null,
        "number": 2,
        "cdate": 1666655306030,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666655306030,
        "tmdate": 1666655306030,
        "tddate": null,
        "forum": "UrzBg1Zz7ob",
        "replyto": "UrzBg1Zz7ob",
        "invitation": "ICLR.cc/2023/Conference/Paper4951/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": " In this work the authors give a framework for evaluating blindspot detection methods (BDMs). The authors define BDMs as methods taking labeled examples and a classifier, and outputting \"semantically meaningful\" groups of points. The authors introduce a new BDM, and a toy evaluation setting in which they perform image classification under a known distribution of images and a known set of \"blind spots.\" The introduced method, PlaneSpot, outperforms the other compared-with methods in both this toy setting and in a more realistic setting.",
            "strength_and_weaknesses": "The paper tackles an import problem: providing a taxonomy and evaluation framework for these \"blind spot detection\" methods. However, the work suffers from a number of issues around the completeness of the real world experiments and clarity. \n\nReal world experiments: All of the experiments are synthetic. Even on the real world experiments, the authors only evaluated on synthetic blindspots that are not representative of conditions in the real world. While this is an interesting experiment, the authors must go beyond these synthetic experiments to identify naturally arising blindspots in models --- otherwise it is not clear whether the presented method is actually useful. The approaches to doing so are relatively straightforward: the authors could\n* evaluate if identified blindspots match known blindspots of examples (for example, on ImageNet humans can lead to fish), or\n* investigate identified blindspots (test whether or not these blindspots actually matter with counterfactuals or another.\n\nClarity:\n* Missing definition of semantic meaningfulness: what makes a blindspot semantically meaningful? This is a critical question that defines the conceptual objects investigated in this paper. What is to stop one from outputting all the incorrect examples as a subpopulation? After all, incorrect examples are semantically meaningful as a unit in that they are all incorrect.\n* Notation: The paper uses a large amount of notation. There are many symbols, acronyms, and named methods that clutter the messages that the paper tries to get across. For example, Table 2.\n* Confidence intervals: Not defined; are they bootstrapped?\n* Figure 7: What does the x axis mean here? This specificity should be a number value.",
            "clarity,_quality,_novelty_and_reproducibility": "See above",
            "summary_of_the_review": "The paper studies an important problem, but does not have experimentally convincing results.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "3: reject, not good enough"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_E6Ce"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_E6Ce"
        ]
    },
    {
        "id": "yDyR704Fykx",
        "original": null,
        "number": 3,
        "cdate": 1666752468949,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666752468949,
        "tmdate": 1666752468949,
        "tddate": null,
        "forum": "UrzBg1Zz7ob",
        "replyto": "UrzBg1Zz7ob",
        "invitation": "ICLR.cc/2023/Conference/Paper4951/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors first propose a formalization of the Blindspot Discovery problem, which they claim was not previously done in a clear manner, and then proceed to propose a synthetic image benchmark for evaluating blindspot discovery methods, that they call SpotCheck. Furthermore they propose a novel blindspot discovery method, called PlaneSpot which outperforms existing methods. Finally, the authors evaluate existing methods as well as their proposed method on real-data sources and find that the results of their proposed synthetic benchmark SpotCheck, seem to correlate with the results of the benchmarks sourced from real datasets. They therefore conclude that their benchmark can be a good complementary and thorough way of evaluating BDMs. ",
            "strength_and_weaknesses": "Strengths: \n\nClear and concise writing.\nThe work attempts to provide a more clear formalization of Blindspot Discovery problem and they do so well. \nThe authors motivate the need for a more \u2018controllable\u2019 benchmark and provide one in a satisfactory way. \nGood empirical evaluation of the new proposed model.\n\nWeaknesses:\n\nWhile the proposed benchmark is a good step forward, I do feel that the paper suffers from some \u2018overclaiming\u2019, in the sense that, even their proposed benchmark may have \u2018naturally\u2019 occurring blindspots. For example, the model itself may have blindspots when the data presents more than two objects, or perhaps when it presents two objects that are on the right hand side of the square, or on the top left corner of the square. These are blindspots that do not exist within the authors\u2019 predefined schema for what label 0 and 1 belong to, but the model may none-the-less develop sensitivities to them. Frankly I think, trying to create a \u2018perfect\u2019 synthetic dataset for blindspot discovery can be a good way to create a blindspot for a research project. In the sense that perhaps a better way to frame the work would be an attempt to create a synthetic dataset with more specific and controllable attributes. However, and this leads to the second weakness.\nIt\u2019s not clear what the difference between existing benchmarks on real world data, that have previously discovered or defined blindspots, and the proposed synthetic dataset are, since even the synthetic dataset can\u2019t be said to not have \u2018naturally\u2019 occurring blindspots that the authors can\u2019t predict. A better way to quantify or justify this is necessary I believe. \nIt\u2019s not clear what the exact usefulness of the new benchmark is. Is it cheaper to evaluate on? Or is it the fact that it has more \u2018controllable\u2019 blindspots? If so, the above concerns must be addressed. Also, given the correlation between the real world datasets and the results from your synthetic dataset, wouldn\u2019t you say that perhaps one could omit using SpotCheck and simply use the real world datasets?\n",
            "clarity,_quality,_novelty_and_reproducibility": "The paper is very clear overall, but some further clarifications related to the claims is required as explained in the previous section. The quality of the work is high. The work is also relatively novel in that it tries to formalize an existing but previously unformalized research area, and also provides a new benchmark and model for it. The work seems quite reproducible from what is written in the paper, except for Spotcheck that is only defined in terms of the high level concepts and not the specifics of the dataset itself. More specific details are necessary in the main paper for proper reproducibility.",
            "summary_of_the_review": "While I am expecting the authors to provide me with more information wrt to the weaknesses I outlined above, I also believe that the work represents a good step forward for BDMs, both in terms of better formalization and benchmarks as well as models. ",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_w8rw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_w8rw"
        ]
    },
    {
        "id": "qNmdV5kGD2",
        "original": null,
        "number": 4,
        "cdate": 1666796693475,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666796693475,
        "tmdate": 1666796693475,
        "tddate": null,
        "forum": "UrzBg1Zz7ob",
        "replyto": "UrzBg1Zz7ob",
        "invitation": "ICLR.cc/2023/Conference/Paper4951/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper addresses the issue of Blindspot Discovery Methods (BDM), which is the task of finding semantically meaningful subsets of data that significantly degrade the performance of image classifiers. This paper proposes SpotCheck as an evaluation framework using synthetic data and PlaneSpot as a new method. Experiments are also conducted on real data sets.",
            "strength_and_weaknesses": "Strengths\n1. The definition of BDM and SpotCheck is clear.\n1. A simple BDM, PlaneSpot, is proposed using dimensionality reduction and Gaussian Mixture Model.\n1. Experimental results on both synthesized and real datasets.\n\nWeaknesses\n1. Motivation to use scvis (Ding et al., 2018) as a dimensionality reduction method is unclear. There are many methods, such as PCA, LLE, t-SNE, and UMAP. The reviewer would like to know why scvis is selected.\n1. It is also unclear how \u03a8 is generated using PlaneSpot. It seems clusters found by Gaussian Mixture Model are used for \u03a8, but its particular definition is not described.\n1. While the reviewer can agree that PlainSpot has the best accuracy on synthetic data, Domino sometimes outperforms PlainSpot on real data. This result is not merely a slight concern to say that PlainSpot is the best method among the other methods but also suggests that the evaluation of BDM by synthetic data proposed as SpotCheck may not correlate very strongly with the result using real data. In fact, there is also a discrepancy about the superiority of the methods, including Barlow and Spotlight, between the artificial data and the actual data.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity and Quality\n- The weaknesses above are all about clarity. Thus, there is room for improvement.\n\nNovelty\n- The proposed framework or BDM is simple and has no major technical novelties. However, the reviewer thinks significant technical novelty is optional for such a benchmark and baseline proposal.\n\nReproducibility\n- There is also a detailed description in the appendix, and the code is provided. Reproducibility is sufficient.",
            "summary_of_the_review": "Overall, the reviewer leans toward rejecting this paper. A response from the authors regarding the above weaknesses could improve the score.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "5: marginally below the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_pnqW"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper4951/Reviewer_pnqW"
        ]
    }
]