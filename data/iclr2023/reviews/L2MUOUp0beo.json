[
    {
        "id": "M_L0KNK8bCl",
        "original": null,
        "number": 1,
        "cdate": 1666425048807,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666425048807,
        "tmdate": 1666425048807,
        "tddate": null,
        "forum": "L2MUOUp0beo",
        "replyto": "L2MUOUp0beo",
        "invitation": "ICLR.cc/2023/Conference/Paper720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "This paper proposes Constrastive Real-time Explanation (CoRTX) framework to avoid encountering efficiency issues in practical scenarios. Specifically, to avoid the requirement of large amounts of explanation labels, this paper designs a synthetic strategy to select positive and negative samples for contrastive explanation representation learning. With experiments on three datasets, and theoretical analysis on both the positive sample selection method and contrastive explanation error bound, the proposed method CoRTX demonstrates efficiency and efficacy.",
            "strength_and_weaknesses": "- This paper is well-written, and the structure is clear. The authors extend from the problem in explainable machine learning of the huge requirement of labels to why unsupervised contrastive learning can help explanation with clear theoretical analysis.\n- Settings and evaluation metrics in experiments are good. The formulated three questions do help understand the motivation and effectiveness of this method, including the efficiency and efficacy of CoRTX, and whether the extracted contrastive explanation representation is still effective when fine-tuning, also the ablations on synthetic positive augmentations are good.\n- The generality of the proposed approach is well demonstrated by considering experimental scenarios with various modalities, i.e., experiments on images and tabular data.\n\nCons:\n\n- With the mention of \"real-time\" and \"in practice\" in the Intro, this paper focus on the real-time scenario, which will easily remind the readers of industrial-level real-time explanations, e.g., plug-and-play model or tool. However, if one involves a training phase, so-called \"real-time\" is not achievable, even though only the explanation head is trained in CoRTX. Although the proposed components of light-label manner and synthetic positive augmentation do reduce the computational effort and prevent the instances from being too hard in contrastive learning, it is still not the same as what I perceive as real-time for real-world scenarios. Besides, the evaluated datasets are not \"in practice\". To me, this is a good paper on explainable machine learning, but not a paper on \"real-time\". I would recommend that the authors change this paper's stance and contribution in another way rather than \"real-time\".\n- Sections 2.2 and 2.3 depict the motivation of the whole method, i.e., combining the advantages of two conventional approaches and reducing the dependency on labels. Although the motivation is intuitive, the content is not convincing enough. I would recommend the authors could write in a better way to couple unsupervised contrastive learning with explainable ML. For example, labels may bring strong supervised information into the training phase, affecting the inductive bias of the model, making the model favor more readily available label information over the information that may help in explanation, where the contrastive learning has already proved that models can learn features that are beneficial for downstream tasks without relying on labels.\n- The experiments and models are well designed, but there may be a lot of room for tuning in terms of parameters and design, such as the choice of the explanation head. The choice of the number of fully connected layers may have an impact, as also the dimensions of these layers, e.g., multiple projection heads of SimCLR.\n",
            "clarity,_quality,_novelty_and_reproducibility": "Have met the standards.",
            "summary_of_the_review": "Overall, I think the studied problem is essential, and the proposed method is novel and reasonable. The motivations are good, and the experiments are clear, comprehensive, and convincing. The theoretical analysis also helps much in understanding how contrastive learning help in explanation. Though the claims in motivations are not entirely convincing, I believe this is a good paper.\n",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "8: accept, good paper"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper720/Reviewer_ZCbd"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper720/Reviewer_ZCbd"
        ]
    },
    {
        "id": "HMRTFRvQiS",
        "original": null,
        "number": 2,
        "cdate": 1666605801280,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666605801280,
        "tmdate": 1669191615414,
        "tddate": null,
        "forum": "L2MUOUp0beo",
        "replyto": "L2MUOUp0beo",
        "invitation": "ICLR.cc/2023/Conference/Paper720/-/Official_Review",
        "content": {
            "confidence": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
            "summary_of_the_paper": "This paper presents a new real-time explainer framework called COntrastive Real-Time eXplanation (CoRTX).\nThe main goal of the algorithm is to limit the dependence of real-time explainers on predefined labels and to improve the throughput (number of samples/time) of the algorithm.\nCoRTX designs a positive and negative synthetic sampling strategy to learn an explanation representation in the form of feature importance.\nA positive sample is a masking of the original data to explain where some features are replaced by zero values.\nThis explanation representation is shown to be an approximation of Shapley values with a set of theorems.\nThe architecture of the method is very similar to that of an autoencoder, but with only the encoder and a loss of similarity.\nLearning is performed encouraging representations of positive samples to be close, while those of negative samples to be far away.\nAfter learning the representations, a fine-tuning process with a limited amount of ground truth labels is carried out to refine the explanations.",
            "strength_and_weaknesses": "Strengths:\n+ The results are very good and CoRTX outperforms the tested methods. The evaluations were performed correctly, including the evaluation of errors.\n+ Quantitative evaluation of the feature importance map is well conducted according to current standards.\n+ Good number of data sets tested to validate the claims.\n\n- The fine tuning part is done according to the tested metric. This is done only for CoRTX and is not fair to compare against other methods tested.\n- Two types of approaches were described in the introduction: methods that learn an explainer based on labels and supervised methods. CoRTX is a semi-supervised method, but only the first type of methods is tested. I would like to see a comparison with methods such as Dabkowski & Gal 2017, Chen et al. 2018 and Kanehira & Harada 2019, presented in the introduction and never mentioned again. CoRTX uses a similar framework as Chen et al. but the comparison was not made.\n- The saliency maps produced (Figure 5) are very different from those compared. I suggest a comparison with masking methods such as RISE (https://es.sonicurlprotection-fra.com/click?PV=2&MSGID=202210211531580778554&URLID=35&ESV=10.0.18.7423&IV=C019EBF33BCE13D9800D210395403F1C&TT=1666366319343&ESN=UynqeUYHEpb%2Ba0WBCa0%2FCeGBiM%2Fl9mITsoFXluKYkDY%3D&KV=1536961729280&B64_ENCODED_URL=aHR0cHM6Ly9naXRodWIuY29tL2VjbGlxdWUvUklTRQ&HK=7F3046409B02770A56F86E7E471588356C1EC6136067365613FD327EB358B71D) or XRAI (https://es.sonicurlprotection-fra.com/click?PV=2&MSGID=202210211531580778554&URLID=34&ESV=10.0.18.7423&IV=CC9762D41E2E67C8D730D7F2EE59A2AE&TT=1666366319343&ESN=kNkzfYKleQHNaDAOrggEOdhqRAXo1UQxQGUwd7mAT7g%3D&KV=1536961729280&B64_ENCODED_URL=aHR0cHM6Ly9naXRodWIuY29tL2h1bW1hdC9zYWxpZW5jeQ&HK=225CD5F9DF0A3A0AAC78077D46BE0581FB3486DED2BEC28FDA05130432137D5B) that produce saliency maps very similar to those of CoRTX.\n- The qualitative assessment of saliency maps has not been adequately evaluated. In particular, Section 4.4.1 states that \"CoRTX provides better explanation results and highlights particular regions that are essential for model predictions.\" However, shifting salience toward the subject is not always a symptom of better explanations. There is no certainty that the subject is important for prediction. In particular, there is an article entitled \"Sanity check for saliency maps\" by Adebayo et al. that shows this detrimental behavior of methods such as Smoothgrad (tested in this article).\n- The fine-tuning part appears to be optional from the text, but instead is required to produce a saliency map of size M. The representations output by the encoder have size d and cannot be used as saliency maps because d << M.",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity:\nThe paper is not very clear.\nSection 3 is too brief in explaining the architecture of the framework, especially when discussing the positive examples.\nx+ is never clearly stated how it is produced, and the reader must infer this from Figure 2.\nThe parameters of the equation are sometimes omitted, such as M and r in equation 2.\nFinally, the figures are not very clear.\nIn Figure 1, it would be nice to see the execution time to understand whether supervised approaches are really limited in a real-time context.\nIn Figure 3 it is not clear what a vertical line means, does it have infinite throughput? It is not clear.\nQuality: the quality of the paper could be improved.\nNovelty:\nThe work is not too new as it combines existing approaches to improve the throughput of their algorithm.\nReproducibility:\nThe code and all the information to reproduce the results is provided.",
            "summary_of_the_review": "The paper is not very clear and needs a thorough rewrite; the experiments part needs further experiments to fully support the claims.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper720/Reviewer_bBUe"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper720/Reviewer_bBUe"
        ]
    },
    {
        "id": "qHiPzOM13ZM",
        "original": null,
        "number": 3,
        "cdate": 1666736560915,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666736560915,
        "tmdate": 1670977680199,
        "tddate": null,
        "forum": "L2MUOUp0beo",
        "replyto": "L2MUOUp0beo",
        "invitation": "ICLR.cc/2023/Conference/Paper720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The authors proposed a method to obtain a good and fast explanation at the testing time. In particular, the authors focused on the approach that learns an explainer model to mimic the \u201caccurate explanation\u201d at test time (e.g., predict the Shapley value).\nThe authors pointed out that a major issue of existing methods is the requirement of many ground-truth explanation labels (i.e., fully accurate Shapley value for each sample), which can be computationally expensive.\nThe authors proposed a contrastive learning method to learn useful explanation-oriented representations so that they can learn the explainer effectively with only a few explanation labels. The main contribution is the synthetic strategy to select positive and negative instances.\nThe authors conducted various experiments to verify the efficiency of the proposed method.",
            "strength_and_weaknesses": "Strength\nThe paper focused on an important problem.\nThe synthetic strategy to select positive instances for contrastive learning is reasonable and well backed up, theoretically and experimentally.\nThe experiments are well established to show the effectiveness of the proposed method.\n\nWeakness\nThe related works are not sufficient compared to the scope of the title and abstract. In particular, the authors mainly focused on such Shapley-value and aimed to predict these values in test time. However, many other approaches (e.g., gradient-based saliency methods) can also provide fast real-time explanations. Some discussions are needed.\nI would prefer more intuition/explanation of how contrastive learning, especially the instances selection strategy, is useful in learning effective representation for the explainer.\nIn the abstract, the authors claimed that \u201caccurate explanation labels are hard to obtain due to limited human efforts.\u201d Is this true? Since the explanation is of the black-box model, I do not see how humans take part in producing the explanation labels.\nIn Figure 5, it would be useful to include the result of SHAP so that the readers can evaluate how the proposed method\u2019s explanation compares to the ground-truth explanation.\nIn the Feature Importance Ranking Task (Section 4.2), the equation to obtain hat{r}j is not clear (although I can guess what the authors are going to do).",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity. The paper is well organized.\nQuality. The paper appears technically sound, but I have not carefully checked the maths/details.\nNovelty. The paper contributes some new ideas.\nReproducibility. Good: key resources (e.g., proofs, code, data) are available, and key details (e.g., proofs, experimental setup) are sufficiently well-described for competent researchers to reproduce the main results confidently.",
            "summary_of_the_review": "The authors focused on an important problem of acquiring fast explanations at test time, although the scope is limited to Shapley-liked explanations. The proposed method is yet simple but effective and well backed up theoretically and experimentally. The weaknesses seem fixable, and under the condition that these points improve, I will recommend acceptance.",
            "correctness": "3: Some of the paper\u2019s claims have minor issues. A few statements are not well-supported, or require small changes to be made correct.",
            "technical_novelty_and_significance": "3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.",
            "empirical_novelty_and_significance": "Not applicable",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper720/Reviewer_A6qm"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper720/Reviewer_A6qm"
        ]
    },
    {
        "id": "LYjc_Poefb",
        "original": null,
        "number": 4,
        "cdate": 1666811622640,
        "mdate": null,
        "ddate": null,
        "tcdate": 1666811622640,
        "tmdate": 1669165694849,
        "tddate": null,
        "forum": "L2MUOUp0beo",
        "replyto": "L2MUOUp0beo",
        "invitation": "ICLR.cc/2023/Conference/Paper720/-/Official_Review",
        "content": {
            "confidence": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.",
            "summary_of_the_paper": "The paper proposes a contrastive real-time explanation framework CoRTX, it is a framework based on contrastive learning to learn model explanations using contrastive learning and fine-tuning a corresponding explanation head using a small amount of ground truth explanation labels. Here the paper used Shapley value as the ground truth.\nThe contrastive framework consists of the following given:\n(a) Train an explanation encoder to produce an explanation representation \n - Given a sample $x$, a set of synthetic positive instances  $X$ is generated via perturbations on x given by equation 2. \n - From set $X$ select $\\tilde{x}^+$ that minimizes equation 4. So $\\tilde{x}^+$ is the synthetic sample that gives the closet function output to $x$ when given to the target model.\n-  Select a set of samples from the training dataset negative samples (any samples other than the original one x).\n- Train the encoder by minimizing the typical contrastive loss in equation 5.\n(b) When the encoder is trained, an explanation head is added which is a model that takes as an input the output of the encoder and produces the Shapley values as an output. This entire architecture is then finetuned with Shapley value as ground truth labels.\n\nTwo versions of CoRTX were introduced CoRTX-MSE where the model was trained to minimize the L2 distance and CoRTX-CE where the model is trained to minimize the cross entropy of the ranking.\n\nThe paper performed experiments on two tabular datasets Census and Bankruptcy, and one image dataset CIFAR-10. For ground truth explanations the paper used Shapley values for Census dataset, Antithetical Permutation Sampling for Bankruptcy and kernalShap for  CIFAR-10 as an approximation of Shapley values.",
            "strength_and_weaknesses": "Strength:\n--\n- The paper is well-written and easy to follow.\n- The need for a fast reliable explanation method is crucial.\n\nWeakness:\n--\n- I am not convinced this framework is telling us anything about how a model is making a prediction, the only time the model in question is actually used to find the positive samples in the contrastive loss. Why are the produced representations \"Explanation representation\"  unless I am missing something the produced representations are simply just different representations. And until they are fine-tuned with ground truth explanations they have nothing to do with model explanations.  The FASTShap paper was sold as a faster way to calculate an estimate of the Shapley values of a model. This might be the case here but claiming that the representation produced by the encoder is an explanation representation is not well supported.\n- I strongly disagree with the notation of the ground truth explanations, especially when using an approximation of Shapley values as ground truth as done in two of the datasets in the experimental section.\n- The \"Supervised RTX\" or \"SOTA RTX\" as explained in the appendix as \"A supervised RTX-based MLP model trains with raw features of data instances and ground-truth explanation labels from scratch\" Where was this introduced? Which paper I could not find a reference for?\n- Why was there inconsistency when choosing ground truth I understand that Shapley values are expensive to calculate but why APS on one dataset and KS on another?\n\n- Experiments:\n    -  For \"EXPLANATION EFFICACY AND EFFICIENCY\": I find it very strange to be comparing with methods (PS/KS) that the paper has considered as ground-truth explanations. Is the rank accuracy and error reported against the actual Shapley values?\n    - For \"CONTRIBUTIONS ON EXPLANATION REPRESENTATION\" The paper is mainly comparing with Supervised RTX which again I am not sure which paper proposes this method (if this method has not been previously proposed by another paper then basically, this is a made-up baseline, and not realistic).\n    -  For \"QUANTITATIVE EVALUATION\" how is the masking and accuracy drop measured do you check the drop on the existing method or create an evaluation model similar to (Jethani et al., 2021)?\n",
            "clarity,_quality,_novelty_and_reproducibility": "Clarity---Good:\n\nThe paper is well written\n\n\nQuality--Fair:\n\nSome of the assumptions and experiments are not well explained. Please see the weakness section above.\n\nNovelty--Medium:\n\nThe idea of estimating Shapley values is not novel it has been done before. The idea of using contrastive learning and a subset of labels for the estimation is novel.\n\n\nReproducibility --Good:\n\nThe code was provided and experimental details were clearly stated in the appendix which can enable the reproduction of the results.",
            "summary_of_the_review": "Overall, the idea of estimating Shapley values is important.\nMy main concerns in the paper are as follows:\n\n- Why is representation produced by the encoder an \"explanation representation\"?\n- Using an approximation of Shapley values as ground truth if Shapley values are too expensive to compute that even practically computing 5% of a large training dataset like imagenet is unfeasible then the overall idea of this paper is not valid but using an estimation of Shapley values to estimate the Shapley values seems incorrect.\n- The paper mainly compares this idea of supervised RTX which is a model trained on all Shapley values and produces an explanation, however, this model does not really exist (please correct me if I am wrong and cite the paper accordingly) it is a made-up baseline introduced by the paper. Because as mentioned in the paper the ground-truth explanations are too expensive to generate in general.\n- Some of the experimental section are unclear please see the weakness section.",
            "correctness": "2: Several of the paper\u2019s claims are incorrect or not well-supported.",
            "technical_novelty_and_significance": "2: The contributions are only marginally significant or novel.",
            "empirical_novelty_and_significance": "1: The contributions are neither significant nor novel.",
            "flag_for_ethics_review": [
                "NO."
            ],
            "recommendation": "6: marginally above the acceptance threshold"
        },
        "signatures": [
            "ICLR.cc/2023/Conference/Paper720/Reviewer_MLHw"
        ],
        "readers": [
            "everyone"
        ],
        "nonreaders": [],
        "writers": [
            "ICLR.cc/2023/Conference",
            "ICLR.cc/2023/Conference/Paper720/Reviewer_MLHw"
        ]
    }
]