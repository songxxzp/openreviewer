[
    {
        "id": "D8OYsSYson",
        "forum": "TkdMRKvZDJ",
        "replyto": "TkdMRKvZDJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_G99H"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a phrase grounding-based style transfer (PGST) approach for single-domain generalized object detection. The authors leverage the grounded language-image pre-training model (GLIP) to learn object-level, language-aware, and semantic-rich visual representations. They define textual prompts for each target domain and use them to train the PGST module, which performs style transfer from the source domain to the target domain. The authors evaluate their approach on five different weather driving benchmarks and achieve significant improvements over existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses an important and challenging problem of single-domain generalized object detection.\n- The proposed PGST approach is novel and leverages the strengths of the GLIP model.\n- The evaluation results show significant improvements over existing methods on diverse weather driving benchmarks."
            },
            "weaknesses": {
                "value": "- The experimental evaluation could benefit from more detailed analysis and discussion of the results.\n- The paper could provide more insights into the reasons behind the observed improvements"
            },
            "questions": {
                "value": "1. Can the authors provide more insights into the limitations of the proposed approach and potential directions for future research?\n2. How sensitive is the performance of the proposed approach to the choice of textual prompts? Have the authors experimented with different prompt designs and evaluated their impact on the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763275553,
        "cdate": 1698763275553,
        "tmdate": 1699636481378,
        "mdate": 1699636481378,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CcxK931ecN",
        "forum": "TkdMRKvZDJ",
        "replyto": "TkdMRKvZDJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Phrase Grounding-based Style Transfer (PGST) for single-domain generalized object detection. PGST aligns image regions with textual prompts, enabling the model to perform well in multiple unseen domains. It outperforms existing methods and achieves large improvement across various benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is the first work to apply GLIP model to single-domain generalized object detection. In terms of novelty and performance improvement, it is a success. However, I'm a little concerned about the fair comparison, please see Weaknesses."
            },
            "weaknesses": {
                "value": "- When comparing with other SOTA methods, the comparison is not fair. The proposed method is based on GLIP, while previous methods are based on Faster R-CNN. Apparently, GLIP has much stronger capacity than Faster R-CNN. It's hard to say how much improvement comes from the proposed design, instead of GLIP network architecture or pre-trained data.\n- Will fine-tuning GLIP with PGST degenerate the GLIP's original performance, like its performance on COCO, Flicker30k entities?"
            },
            "questions": {
                "value": "I assume the following questions are open problems and not considered as the weaknesses of this paper:\n- Since GLIP has been pre-trained on so many data, there might be data leakage of target domain data. If so, does this really follow the problem setting of \"single-domain generalization\" ? \n- For the source domain augmentation with prompt, this paper uses a full-model tuning strategy. Have the authors tried prompt/linear probing and what the performance is ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4948/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_Ci7j",
                    "ICLR.cc/2024/Conference/Submission4948/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816477509,
        "cdate": 1698816477509,
        "tmdate": 1700708086465,
        "mdate": 1700708086465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5I2lgb3OX1",
        "forum": "TkdMRKvZDJ",
        "replyto": "TkdMRKvZDJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles single-domain generalization tasks for object detection. In this work, authors leverage the GLIP model to estimate different unseen target domains via their style transfer module, PGST, and text prompts which describe the object categories in the new domains. Once the different styles are learned both image and text encoders are finetuned to achieve the best performance. The state-of-the-art results are shown for the standard benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The manuscript is well-written and easy to follow\n2. Experiments are shown on standard domain generalization and adaptation benchmarks.\n3. Though the method takes inspiration from C-Gap(2023) in terms of using text prompts for domain generalization, using GLIP instead of CLIP seems to be a more reasonable direction for object detection tasks. The strong improvement over C-Gap and other baselines goes to show that."
            },
            "weaknesses": {
                "value": "1. The novelty of this work is using their PGST and GLIP for domain generalization tasks. However, a previous work PODA[1,2], which is not cited in this paper, implements a module similar to PGST using text prompts. This reduces the novelty of the current work. The authors should discuss this in the paper and propose what makes their PGST different from PODA. \n\n2. This work's performance is still much better than in PODA, so there is some merit. But if we remove PGST from the contribution (because of similarity w.r.t PODA ), is the contribution just integrating GLIP for domain generalization?\n\n3. All prompts used in this work directly correspond to the test domains. Why not have a general set of prompts showing all possible weather descriptions? How does that affect the performance? For example: a quick ChatGPT prompts for different weather scenarios and time of the day.\n```\n1. \"an image taken on a rainy day during the morning.\"\n2. \"an image taken on a cloudy day during the evening.\"\n3. \"an image taken on a snowy day during the night.\"\n4. \"an image taken on a sunny day during the early morning.\"\n5. \"an image taken on a foggy day during the late afternoon.\"\n6. \"an image taken on a stormy day during the twilight.\"\n7. \"an image taken on a clear day during the midnight.\"\n8. \"an image taken on a windy day during the golden hour.\"\n9. \"an image taken on a partly cloudy day during the dusk.\"\n10. \"an image taken on a misty day during the early evening.\"\n```\n\n4. Also, what if the prompts are unrelated to the weather , does it degrade the performance? These studies will be useful in judging sensitivity to the prompt's choice and design.\n\n5. It is not clear how the best model is chosen. Please refer to Gulrajani et Lopez-Paz , In search of lost domain generalization , ICLR'21 to indicate what strategy was used. This is crucial for the reproducibility of the method.\n\n[1] PODA: Prompt-driven Zero-shot Domain Adaptation, Fahes et. al. ICCV'23\n\n[2] P\u00d8DA: Prompt-driven Zero-shot Domain Adaptation, Fahes et. al. arxiv, 2022"
            },
            "questions": {
                "value": "Please have a look at the weakness for my major concerns. Based on the answers, I am willing to change my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4948/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4948/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4948/Reviewer_JhCL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4948/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835320413,
        "cdate": 1698835320413,
        "tmdate": 1700749917334,
        "mdate": 1700749917334,
        "license": "CC BY 4.0",
        "version": 2
    }
]