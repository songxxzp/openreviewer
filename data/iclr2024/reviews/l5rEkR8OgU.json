[
    {
        "id": "w1DigYUnj6",
        "forum": "l5rEkR8OgU",
        "replyto": "l5rEkR8OgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_vLEV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_vLEV"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors explore the effect of explicit intermediate supervision on learning complex functions. Specifically, they find that training on a mixture of multiple tasks yields better results than training only on one task. Some theoretical and empirical results show the effects of a combination of two tasks. Their findings could imply better training schemes for language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This is an interesting work on the investigation of learning effects with a mix of tasks. Some theoretical and empirical evidence is shown for the learning effect. The results on a mixture of the Parity/Sum task are interesting \n\n2. They also empirically show that it is easier and faster for the learner if the signals from easily inferred labels to learn target are provided. Experiments on LEGO and code interpretation task are done.\n\n3. Their findings on learning complex tasks contribute to the understanding of large language model learning and provide valuable insights for future related work on efficient training."
            },
            "weaknesses": {
                "value": "1. The theoretical results on the Parity/Sum task reply to some strong assumptions: bilinear parameterization, some initialization (for example, v = 0). Under these assumptions, the gradient over the parity distribution samples is zero. The assumption looks a little strong.\n\n2. It would be to show more experiment results for some settings, for example, performance on Sum task when training with a mixture distribution, or more Sum task samples and fewer Parity task samples ( p range from 0.5 to 1. In Figure 1, results with p ranges from 0.1 to 0.5 are shown."
            },
            "questions": {
                "value": "Typo errors:\n\nIn section 2.2, the parameter W size should be R*{k x (n + 1)}, not R*{k x n + 1}."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644269205,
        "cdate": 1698644269205,
        "tmdate": 1699636558168,
        "mdate": 1699636558168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jxAzrXSLVi",
        "forum": "l5rEkR8OgU",
        "replyto": "l5rEkR8OgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_rg2R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_rg2R"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new supervision named implicit intermediate supervision for complex functions. Particularly the authors provide theoretical and empirical evidence to support the effectiveness of implicit supervision."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It is novel that the paper pays attention implicit intermediate supervision instead of explicit supervision to solve intricate tasks in language modeling, and it provides detailed proof of the notion.\n\n2. The paper could also contribute to the understanding of how large language models learn complex tasks and may facilitate research on more efficient and effective training methodologies."
            },
            "weaknesses": {
                "value": "1. There is something wrong with the structure of this paper, for example, section 1.2 is named as Related Work, which is usually a separate chapter. Moreover, this paper losses the Conclusion part.\n\n2. This paper does not contain an example to show the advantages of implicit supervision over explicit supervision, or an example that demonstrates how the implicit supervision work. So can the authors provide a figure in Section 1 that can make readers get your innovation quickly?\n\n3. The authors point out that explicit intermediate step-by-step supervision is time consuming compared to the implicit supervision in abstract, but they do not provide experimental results to verify this view."
            },
            "questions": {
                "value": "Assuming that there are now n tasks, task 1, task 2, \u2026, task n. Will the following scenario occur: training task 1 with data from task 1 to task i yields excellent results, but when training task 1 with data from task 1 to task i+1, the performance significantly deteriorates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674273884,
        "cdate": 1698674273884,
        "tmdate": 1699636558071,
        "mdate": 1699636558071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ECdIR3b0RI",
        "forum": "l5rEkR8OgU",
        "replyto": "l5rEkR8OgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_JC1K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_JC1K"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on intermediate supervision. Popular approaches mainly use explicit step-by-step supervision, whereas the authors investigate implicit step-by-step supervision. The motivation is that models can implicitly explore the structure of tasks, and easy tasks can benefit the understanding of hard tasks. The paper then proposes two settings, multi-tasks and multi-labels, and uses a synthetic parity learning problem and feed-forward network structure to theoretically justify the motivation. During experiments, authors conduct experiments on transformer architectures and other datasets to show the generality of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper explains implicit intermediate supervision, which may help understand the large language model's capability of solving complex problems.\n2. The paper provides both theoretical understanding and empirical justification. The experiments show the observation also applies to the Transformer architecture."
            },
            "weaknesses": {
                "value": "1. The theory is a bit limited to the specific synthetic task. Are there possibilities to extend the idea to support a more general case?\n2. The experiments are based on synthetic datasets without realistic datasets. I think datasets that can be used in curriculum learning can be used here too, to justify the applicability of the proposed approach. \n\nI would currently put my score to 5 but may change my score based on the authors' rebuttal."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Reviewer_JC1K"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780654493,
        "cdate": 1698780654493,
        "tmdate": 1699636557951,
        "mdate": 1699636557951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g1VMaetcGb",
        "forum": "l5rEkR8OgU",
        "replyto": "l5rEkR8OgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_JwBs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5472/Reviewer_JwBs"
        ],
        "content": {
            "summary": {
                "value": "Intermediate supervision in the form of chain of thought reasoning traces has been a huge success in improving the reasoning abilities of large language models. However chain of thought requires using more compute at inference time, since the model has to generate many tokens to produce the chain of thought. This work explores an alternative approach, which involves training models in either the multi-task or multi-label setting. The idea being that training the model to complete simpler tasks can help it to learn to solve more complex tasks. They explore this on a simple parity task, which has been shown to be challenging to neural networks to learn, and on a math reasoning task and a code interpretation task. In each setting they observe that mixing simpler tasks into the dataset or defining a multi-label problem, helps the model to solve the more complex task. Without the additional supervision, the model either fails to learn at all or learns much more slowly. They also provide some theoretical results on their parity task. The results pain an interesting picture about why language model pretraining is so effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* They include both interesting theoretical and convincing empirical results\n* Many of their tasks show a dramatic difference between with/without additional supervision, potentially making these tasks a good source for future work to further study\n* Their results seem to hold across several tasks and on full transformer language models\n* The paper is overall well presented"
            },
            "weaknesses": {
                "value": "* The tasks they study are a little bit toy, which makes them easy to study, but it is a little bit unclear if these findings transfer cleanly to the LM pretraining setting.\n* They frame this as a replacement for chain of thought, and state that it saves on collecting full reasoning chains for supervision. However chain of thought prompting (the predominate way to get LM to output intermediate reasoning) only requires a handful of examples that can usually be written by a single person in a few minutes. Whereas their method would require collecting or synthesizing large datasets. I feel that a potentially more interesting framing could be around understanding how large scale pretraining (multitask) can enable LMs to learn tasks which are typically challenging for neural networks to learn on their own."
            },
            "questions": {
                "value": "* Hoes does model scale impact the results? Will larger models be able to learn more effectively from fewer examples of the simpler tasks?\n* Are your models initialized from scratch or fine-tuned from pretrained weights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5472/Reviewer_JwBs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812464686,
        "cdate": 1698812464686,
        "tmdate": 1699636557850,
        "mdate": 1699636557850,
        "license": "CC BY 4.0",
        "version": 2
    }
]