[
    {
        "id": "7Fv7T3Yb4Q",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_Zpi4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_Zpi4"
        ],
        "forum": "tAmfM1sORP",
        "replyto": "tAmfM1sORP",
        "content": {
            "summary": {
                "value": "This paper introduces the \"Hypotheses-to-Theories\" (HtT) framework, which is designed to equip LLMs with a rule library for conducting reasoning. HtT comprises two distinct stages: an induction stage and a deduction stage. In the induction stage, the LLM is initially tasked with generating and validating rules based on a set of training examples. Rules that frequently appear and lead to correct answers are aggregated to create a rule library. In the deduction stage, the LLM is then prompted to leverage this acquired rule library to engage in reasoning and respond to test questions. The authors have evaluated their approach on a numerical reasoning benchmark and a relational reasoning benchmark and argue that their approach can significantly enhance the performance of existing few-shot prompting methods."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The study and development of reasoning capabilities in LLMs is a very interesting and topical research area. LLMs have already demonstrated emerging capabilities across a wide range of reasoning tasks, primarily due to the evolution of sophisticated prompting methodologies. This paper provides further insights in this direction. Furthermore, the results of the experimental study seem to support the effectiveness of the proposed approach on existing benchmark datasets."
            },
            "weaknesses": {
                "value": "In spite of the pertinence of the addressed problem and the promising results from the experiments, I feel that paper also comes with \nsignificant weaknesses, which I will detail next. Specifically, I feel that the proposed Hypotheses-to-Theories (HtT) framework \nlacks comprehensive development within this submission. The authors describe general ideas but do not provide sufficient technical details \n  describing how their methods advance the state-of-the-art. The core technical contribution of the paper\nis succinctly described in just one and a half pages (pages 3 and 4), while the majority of the paper \nis devoted to the description of the experiments and the obtained results.  Unfortunately, the description of the approach\n appears somewhat lacking in depth and places an undue emphasis on \"tricks\" like XML tagging, detracting attention from fundamental principles and methodologies that can be adopted and further developed by other researchers.\n\nAs an example of the paper lacking important detail, I feel that the concept of a \"rule\" remains ambiguously defined in the paper. Specifically, the paper lacks an explicit description of the nature of \"rules\" that can be incorporated into the library and subsequently used in the deduction phase. The examples presented in Figure 1 proved to be somewhat confusing; for instance, the rule library in Figure 1 includes statements like \"3 + 4 = 7,\" which, in my view, represent concrete facts rather than rules. A rule typically constitutes a formalised, general statement that applies to a potentially infinite collection of objects (e.g., \"the successor of an even natural number is an odd number\" or \"all men are mortal\"). In this context, the assertion that the proposed approach can induce \"rules\" appears unjustified and potentially misleading.\n\nSimilarly, it remains unclear how these \"rules\" are to be applied. Rule application, in its essence, involves the process of using a general statement to derive new facts from existing information (e.g., given that 4 is an even natural number and the established rule that the successor of an even number is odd, we can deduce the new fact that 5 is an odd number). It remains unclear how a \"rule\" such as \"3 + 4 = 7\" would be employed in a deductive context to generate new insights in the aforementioned sense."
            },
            "questions": {
                "value": "I do not have specific questions. I believe that the paper should be substantially rewritten before it can be published at a top venue. In particular, the description of the core approach should be substantially expanded, the contributions to science should be emphasised and the new techniques developed should be made explicit so that they can be adopted and further developed by other researchers in the field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4966/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697194853791,
        "cdate": 1697194853791,
        "tmdate": 1699636483643,
        "mdate": 1699636483643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uxxzdN6qCy",
        "forum": "tAmfM1sORP",
        "replyto": "tAmfM1sORP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_r3qq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_r3qq"
        ],
        "content": {
            "summary": {
                "value": "This paper tries to improve LLM's reasoning ability, by inducing rules and applying the induced rules in deductive problems. Experimental results show that explicitly learning some rules and inject them into prompts can significantly benefit strong LLM models such as GPT-4, but not weaker ones like GPT-3.5."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Extensive experiments to verify the effectiveness of the proposed method.\n2. The presentation of this paper is articulate, and easy to read.\n3. The experimental results verified the effectiveness of this work."
            },
            "weaknesses": {
                "value": "1. Many details are missing, e.g., how does the induction stage work? How to use ground truth answers to verify the induced rules, does it require human annotators? What is the confidence of rules, how are they evaluated, and does LLM output a confidence score associated with the rules?\n2. This work is basically a technical report, some claims lack supportive facts or resources. For example, the authors say \"hallucination in LLMs resembles hypothesis generation in scientific discovery\", which is incorrect. In scientific discovery, hypotheses are generated by logical induction and abduction ([reference](https://plato.stanford.edu/entries/scientific-discovery/)), while the logic behind hallucination remains unknown.\n3. Rules in the form of natural language weaken the generalisation ability and usually cause ambiguity and may confuse people. This is exactly the reason why Gottfried Leibniz calls for mathematical logic. However, most of the time, humans, like LLMs, are using natural language for reasoning, so I don't think the method proposed by this paper is bad. It would be interesting to make a more comprehensive test for the learned rules using formal methods, for example, ask LLMs to abstract those numerical rules in Appendix C into higher-order forms, such as Peano axioms and see if LLMs can make use of those more advanced rules."
            },
            "questions": {
                "value": "Please see my above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4966/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698652829118,
        "cdate": 1698652829118,
        "tmdate": 1699636483564,
        "mdate": 1699636483564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "54LgCBp9dN",
        "forum": "tAmfM1sORP",
        "replyto": "tAmfM1sORP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_eHJ8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_eHJ8"
        ],
        "content": {
            "summary": {
                "value": "The key idea of the paper is  that LLMs can learn rules from examples, and then these rules can be used to deduce answers for other queries to the LLMs. The paper proposes an induction and deduction step. In the induction step, the model infers rules from exemplars, some of which are thrown-out, based on coverage (how often is the rule used) and accuracy (how often is the rule correct) of the rules. In the deduction step, the induced rules are given as a knowledge base, and the model is expected to use rules to infer new answers. \n\nThe authors test there idea on simple synthetic datasets, by extending the Chain of Thoughts (CoT) and Least to Most (LtM) prompting methodology, showing consistent advantage of using the proposed method. The datasets consist of simple task of arithmetic in different bases and learning (simple) kinship relationship rules."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Easy and intuitive prompting method\n- Shows that LLMs can learn simple rules \n- Can be useful when rules that need to be learnt are simple true/false propositions"
            },
            "weaknesses": {
                "value": "The general idea of the paper is nice, but the developed setup is too simplistic, and has not ben tested in any realistic setting. Specifically, I have the following concerns:\n \n- The setting is too simple, the learnt rules are just true/false propositions of the form \"A is B\"\n- The examples as shown in appendix are not very impressive, at least from a skim through, it seems the rules already exist explicitly in the prompt text. At this point how is this different from just knowledge retrieval as done in [1]. In fact the tasks presented in [1] seems much more nuanced than the one presented here. \n- The gains without XML tagging (an existing method in the prompting technical know-how) are marginal. Furthermore, (it seems to me) that the authors have not tested CoT and LtM with XML tagging, making it unclear how much of their gains are from tagging, and how much is from the extracted knowledge.\n\n[1] Trivedi et al. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. https://arxiv.org/pdf/2212.10509.pdf"
            },
            "questions": {
                "value": "- On page 4, you explain that you add XML tags to the prompts, Do you add the XML tags to the prompts of the methods compared?\n- The examples that you show in appendix seem very simple rules, are you able to extract complex rules? --- beyond \"A is B\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Reviewer_eHJ8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4966/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669508458,
        "cdate": 1698669508458,
        "tmdate": 1699636483443,
        "mdate": 1699636483443,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tzGnP7ndG1",
        "forum": "tAmfM1sORP",
        "replyto": "tAmfM1sORP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_CJEP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4966/Reviewer_CJEP"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenges of large language models (LLMs) in complex reasoning tasks, addressing their tendencies for generating plausible but inaccurate outputs and decreased performance in unconventional knowledge scenarios. Then it proposes the Hypotheses-to-Theories (HtT) framework as a solution, inspired by the scientific method, incorporating rule induction and deduction stages to reduce hallucinations and improve reasoning accuracy. Empirical tests with GPT on numerical and relational reasoning datasets demonstrated significant performance improvements over baseline methods, showcasing the potential of HtT to enhance LLMs\u2019 reasoning capabilities while mitigating existing challenges."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe idea that uses LLM to act as a rule learner is novel, distinguishing itself from previous methodologies that typically employ alternative strategies to mitigate hallucination or rely on symbolic methods for rule acquisition.\n2.\tSeveral innovative tricks pertaining to prompts have been introduced, effectively addressing intricate implementation details and enhancing the method\u2019s practicality.\n3.\tThe method's effectiveness has been validated through experiments."
            },
            "weaknesses": {
                "value": "1.\tAlthough the method presented in the article exhibits a certain degree of innovation, its articulation fails to meet the standards of ICLR, leaving many details unaddressed within the paper. This omission results in confusion among readers trying to grasp the intricacies of the proposed approach. \n\nFor instance, in the \"Induction from Deduction\" section, it is not specified how the rules are extracted \u2014 is it through regular expressions? \n\nIt is also unclear how the occurrence k and accuracy p are calculated based on the paper. These concepts are borrowed from the field of rule learning, yet the author does not elucidate how they are applied in the domain of natural language. This transition from rule learning to natural language processing necessitates a clear explanation, as the methodologies and challenges inherent to these domains can be vastly different. \n\nIn the appendix, considering Prompt 2, it is evident that a substantial number of ground rules are already present within the prompt. This raises a question: If we rely solely on the rules from the prompt, what level of performance can be achieved? \n\n2.\tThe citation is inconsistency. For instance, the first referenced NeurIPS paper does not include page numbers, while the second one does. The third citation is missing its source of publication, and the fourth one includes the conference name\u2019s abbreviation, unlike the others. \n\nXinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. Compositional generalization via neural-symbolic stack machines. In Advances in Neural Information Processing Systems, 2020.\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. 2023.\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5418\u20135426, 2020.\n\n3.\tThe paper falls short of providing a comprehensive description regarding the spectrum of problems that the introduced approach is adept at solving. This leaves a ambiguity as to whether the proposed method can effectively tackle all varieties of hallucination issues, a matter that necessitates further elucidation for a complete understanding of the method\u2019s capabilities and limitations.\n\n4.\tFurthermore, is the method capable of learning complex rules, such as first-order logic rules? If unification is required during reasoning, can LLMs still utilize these rules for inference?"
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4966/Reviewer_CJEP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4966/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807760760,
        "cdate": 1698807760760,
        "tmdate": 1699636483324,
        "mdate": 1699636483324,
        "license": "CC BY 4.0",
        "version": 2
    }
]