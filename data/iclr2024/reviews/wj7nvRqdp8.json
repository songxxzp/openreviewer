[
    {
        "id": "q4al3jX3sT",
        "forum": "wj7nvRqdp8",
        "replyto": "wj7nvRqdp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_xNxo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_xNxo"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a point-based image editing technique to pre-trained text-to-image diffusion models. The proposed method consists of a three-stage pipeline: LoRA finetune, point-track based Latent Optimization, and Latent-MasaCtrl denoise process. With the help of an image preserving mask, the proposed method enables reasonable point-based image editing for StableDiffusion models. The authors also introduce a benchmark dataset for drag evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Point-based image editing is an efficient interactive way for practical image editing applications. This work is one of the first to introduce such editing to diffusion models.\n\n- The proposed method borrows recent advanced techniques to build a reasonable three-stage pipeline.\n\n- Sufficient quantitative results and ablations are constructed to evaluate the proposed components.\n\n- The introduced benchmark dataset can benefit future works."
            },
            "weaknesses": {
                "value": "- The pipeline design is straightforward, and the design choice analysis would be more useful for future work. For example, there are also other techniques for identity preservation such as id-encoder, null-text inversion, etc.\n\n- The visual comparisons with other baselines (e.g., DragGAN, FreeDrag) are not sufficient. Comparisons on different categories of DragBench should be reported in the appendix. Besides, the comparison with DragGAN in the teaser is not fair since the preserving region is not applied to DragGAN.\n\n- The drag ability introduced in this work seems still very limited: most cases are about local deform or small 3D rotation. Such limitations and failure cases should be discussed in the paper to show the boundary of drag ability.\n\n- The time cost is still very expensive. It is claimed that for a 512x512 image, it may take more than 1 minute on an A100 GPU.\n\n- It would be good to add benchmark data examples in the appendix."
            },
            "questions": {
                "value": "More analysis of design choices, visual comparisons and limitations are suggested."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697959237205,
        "cdate": 1697959237205,
        "tmdate": 1699636605507,
        "mdate": 1699636605507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KBC057blpx",
        "forum": "wj7nvRqdp8",
        "replyto": "wj7nvRqdp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_U8vP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_U8vP"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new point-based image editing technique with high capability and effectiveness, DragDiffusion. The method can be applied to edit a variety of images for point-based editing, including real and synthesized images, as well as challenging cases such as images with multiple objects, surpassing the previous GAN-based editing method DragGAN both quantitatively and qualitatively. A new benchmark for point-based editing evaluation, DragBench, is introduced. It is a beneficial contribution to future research on image editing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- DragDiffusion is of high capability and can effectively accomplish point-based image editing tasks. In some cases, it can produce obviously better and more reasonable results than the DragGAN approach. \n- The method is well-designed. Starting from Diffusion UNet LoRA fine-tuning, the method further leverages 1) the latent optimization technique to align latents with user editing, and 2) the latent-MasaCtrl strategy to ensure enough identity between the edited image and the input. \n- DragBench is introduced for evaluating point-based image editing. \n- Extensive experiments with abundant visual evaluations for illustrating the effectiveness of DragDiffusion. Ablation studies are conducted on the number of inversion steps and the number of LORA fine-tuning steps for revealing the influence of key parameters on the method."
            },
            "weaknesses": {
                "value": "- The significance of the technical contribution is a little bit limited. Though with insights into the relationship between diffusion latent and the image content, the method relies on existing techniques to acquire good performance, including the latent optimization (from DragGAN) and the latent-MasaCtrl design based on the existing MasaCtrl strategy. Therefore, despite the effectiveness of the method and the smart usage of the insights on diffusion latents, it is hard to tell what new messages or general principles which are potentially useful for other works it can convey. \n- The technique is proposed for point-based image editing. It would be better if the method (or with some small modification) could solve other image editing problems. The claim on its strong power, generality, and versatility would be more solid if it could be broadly applied to other editing problems. \n- Showing more qualitative comparisons between DragDiffusion and previous or concurrent approaches such as DragGAN would be beneficial to evaluating its superiority in point-based image editing. \n- Ablation studies are not particularly sufficient. \n\nDragDiffusion is effective and superior to previous works. However, the main concerns lie in the significance of its technical contribution and the insufficient qualitative comparisons. Thus I vote the acceptance but limited to score 6."
            },
            "questions": {
                "value": "More visual comparisons?\n\nOther ablation studies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568607116,
        "cdate": 1698568607116,
        "tmdate": 1699636605401,
        "mdate": 1699636605401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L3yprFRG9I",
        "forum": "wj7nvRqdp8",
        "replyto": "wj7nvRqdp8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_DRgP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5767/Reviewer_DRgP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to enable point-based editing from pretrained Diffusion models like Stable Diffusion. It is inspired from DragGAN. The proposed method extends the point-tracking objectives from DragGAN while performing the sampling from the Diffusion Model. It also employs tricks like LoRA and MasaCtrl to retain the identity of the input image. The results are impressive. Additionally they also create a new benchmark dataset for this task, called DragBench."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Drag based editing in diffusion models with promising results on diverse set of images.  \n- A dataset to benchmark the drag based editing methods."
            },
            "weaknesses": {
                "value": "- The approach builds on the existing well established work DragGAN on text-to-image diffusion models. While the results are interesting the method just replicates the same loss function with minor changes (LoRA and MasaCtrl) which are already known in the literature. Hence I believe the technical contribution is limited for the community.  \n- Discussion on UNet features is not provided. It would be interesting to see an ablation over the intermediate UNet features used for the optimization and how does it affect the edit in the final image.  \n- Missing experiment - Ablation of using more than one time-step for the drag based optimization. Does it improves the performance of the editing with a better control?  \n- Details for the DragBench are missing from the main paper and the supplementary. How many images are there in the dataset? How are the labels gathered for the point based control?"
            },
            "questions": {
                "value": "Please refer to the weakness section for questions related to each point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5767/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5767/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5767/Reviewer_DRgP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835333769,
        "cdate": 1698835333769,
        "tmdate": 1699636605314,
        "mdate": 1699636605314,
        "license": "CC BY 4.0",
        "version": 2
    }
]