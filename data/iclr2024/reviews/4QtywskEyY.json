[
    {
        "id": "bWXPlEEtSN",
        "forum": "4QtywskEyY",
        "replyto": "4QtywskEyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_zgd9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_zgd9"
        ],
        "content": {
            "summary": {
                "value": "This article proposes a method for relation-based knowledge distillation using contrastive learning. Its main contribution lies in utilizing multi-stage feature outputs and decoupling angle and distance information in the distillation process."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The article discusses the shortcomings found in previous works and introduces some innovative ideas.\n2. Decouple the imformation of angle and length, which seems to be effective.\n3. Using multiple layers of features for effective knowledge distillation in classification tasks is meaningful. While multi-layer features have shown significant improvements in detection tasks, in classification tasks, it is common to use only the last layer features. Exploring the rational use of multiple layers of features is worthy of investigation."
            },
            "weaknesses": {
                "value": "1. The effectiveness of the proposed method is evident on CIFAR-100, but there is limited comparison on ImageNet 1K, and the improvements seem insufficient. Please provide additional experiments on ImageNet.\n2. Please include comparisons with latest methods\uff0csuch as MGD, DIST and NKD.\n\n   DIST: Knowledge distillation from a stronger teacher. NeurIPS.\n \n   MGD: Masked Generative Distillation. ECCV\n\n   NKD: From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels. ICCV\n3. Please provide further explanations and performance comparisons regarding the ADSS module.\n4. Some parts of the article employ unnecessarily complex language, which hinders readability."
            },
            "questions": {
                "value": "above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2985/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634023763,
        "cdate": 1698634023763,
        "tmdate": 1699636243141,
        "mdate": 1699636243141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nhgv7EOHGm",
        "forum": "4QtywskEyY",
        "replyto": "4QtywskEyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_G2bH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_G2bH"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on knowledge distillation (KD) where a (usually more compact) student model has to be trained using a (usually heavier) teacher model. They extend the ideas from relational knowledge distillation (RKD) in order to better utilize information from multiple intermediate stages of the models. They find that adding distance-wise relational information to contrastive learning based methods deteriorates the distillation performance. To rectify this, they propose Multi-stage Decoupled Relational KD (MDR) to decouple the distance-wise information into length-wise and angle-wise information. Intuitively, it seems to be easier to optimize the length and angle separately instead of implicitly through a single distance measure. They extensively evaluate the proposed method on KD benchmarks like CIFAR100 and ImageNet, as well as on few-shot learning, object detection, and transfer learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The idea of decoupling the distance information into length and angle information is novel, interesting, and intuitive. I appreciate that it is a simple yet effective idea.\n\n* The paper is fairly well-written and easy to follow.\n\n* The analysis and ablation experiments are quite extensive and confirm that the proposed components are useful."
            },
            "weaknesses": {
                "value": "* Page 5: \u201cRKD directly used distance information but ended up with unstable and degraded distillation outcomes.\u201d Based on Table 1, there is only a 2% difference between RKD and the proposed method. This claim of unstable and degraded outcomes seems exaggerated without proper evidence.\n\n* Page 6: \u201cTo preserve length-wise information while maintaining the representational ability of contrastive learning, we place a classifier behind each SM and directly use cross-entropy (CE) loss.\u201d There are not many details given about this and it is not illustrated in Fig. 2.\n    * If only CE loss is used, then how does it get contrastive representational ability?\n    * The first contribution mentions that this work improves existing contrastive learning based KD frameworks, but the contrastive loss is removed here. This seems contradictory.\n    * Also, there is no loss equation mentioned for SM training and no information on the loss weighting hyperparameters involved in SM training.\n    * The ablation study for SM training (Fig. 3c) only compares the SMP accuracy but not the downstream distillation performance. Please also show the distillation performance since we need to have a proper comparison to understand the benefits of changes to SM training.\n\n* Fig. 3b: It is unclear what ${\\mathcal{L}}\\_{len}$ represents because the text mentions it is distance-wise loss but it was defined as length-wise loss. Also it is unclear what it means by adding RDM. Because RDM was defined as having separate losses $\\mathcal{L}\\_{len}$ and $\\mathcal{L}\\_{ang}$ which is the second bar in the figure.\n\n* Fig. 3b: Another baseline is using the two decoupled losses and a third loss being the distance-wise loss. This needs to be checked in case having all three losses may further improve the optimization.\n\n* There are different hyperparameters for different datasets (as per Page 13) without any information on how they are chosen. Due to the large number of hyperparameters, the method may be difficult to tune.\n    * Also, there are no sensitivity analyses for each of these hyperparameters.\n    * As mentioned before, there is no information on the hyperparameters involved in the CE losses for the self-supervised modules.\n\n* Fig. 5: Please also add the visualization of distance-wise relational matrix, so we can see if that loss is also lower for the proposed MDR. It would also support the idea that decoupled optimization improves the solution to the overall goal optimization problem."
            },
            "questions": {
                "value": "* Please see the weaknesses section.\n\n* Minor comments\n    * Fig. 2: Please mention what the red cross indicates in the legend.\n    * Eq. 5: use \\exp instead of exp.\n    * Table 1: add a column for \u201caverage\u201d to make it easier for readers to understand the average improvements across all architectures.\n    * Fig. 3: x-axes of (a) and (b) have typos: \u201cshffule\u201d \u2192 \u201cshuffle\u201d. \n    * Fig. 3c: Indicate what 1st, 2nd, 3rd means in the text (I believe it is the layer number or stage number of the SM module). Also it is unclear if the plot is for the teacher SM or the student SM.\n    * Sec. A.4: typo in the title \u201cAblition\u201d \u2192 \u201cAblation\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2985/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2985/Reviewer_G2bH",
                    "ICLR.cc/2024/Conference/Submission2985/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2985/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807577892,
        "cdate": 1698807577892,
        "tmdate": 1700708067444,
        "mdate": 1700708067444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H3RWvHDnhw",
        "forum": "4QtywskEyY",
        "replyto": "4QtywskEyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_qoFL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_qoFL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Multi-stage Decoupled Relational (MDR) knowledge distillation framework, which selects the most suitable stages for each sample based on the relational representation capability of both angle-wise and distance-wise relational information. It decouples angle-wise and distance-wise information to enhance the  transferring efficiency and distillation quality. The experiments show that the proposed method achieves SOTA performance on the public datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The organization of this manuscript is satisfying and the proposed MDR achieves SOTA performance."
            },
            "weaknesses": {
                "value": "1. The relational decouple module (RDM) is not very novel, as the SSKD has already introduced contrastive prediction, the RKD also employs Angle-wise distillation loss for distillation.\n2. The authors should compare the MDR with the most recent advanced methods, e.g. CAT-KD [1] and ML-LD [2].\n[1] Ziyao Guo, Haonan Yan, Hui Li, Xiaodong Lin: Class Attention Transfer Based Knowledge Distillation. CVPR 2023: 11868-11877\n[2] Ying Jin, Jiaqi Wang, Dahua Lin: Multi-Level Logit Distillation. CVPR 2023: 24276-24285"
            },
            "questions": {
                "value": "1. What\u2019s major insight of the RDM modules? the decoupling technique is not very novel in knowledge distillation task.\n2. In table 1, in some cases the student network achieves higher performance than the teacher network, can you explain such phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2985/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839435930,
        "cdate": 1698839435930,
        "tmdate": 1699636242959,
        "mdate": 1699636242959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A5SF968Ats",
        "forum": "4QtywskEyY",
        "replyto": "4QtywskEyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_WUNU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2985/Reviewer_WUNU"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to improve knowledge distillation based on contrastive learning. Specifically, the authors introduce two aspects of improvements. The first is applying contrastive feature distillation over intermediate layers facilitated with a so called adaptive layer-stage selection. The second is using two functions, namely cosine similarity and L2-normed distance, measure relational feature distance between student and teacher models. Experimental validation is conducted on image classification (with CIFAR-100 and ImageNet datasets) and object detection (with PASCAL VOC dataset) tasks. Besides traditional supervised image classification, transfer learning and few-shot learning are also considered in experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well written in most parts. \n\n+ The proposed method is simple and straightforward. \n\n+ Comparative experiments are conducted on image classification (with CIFAR-100 and ImageNet datasets) and object detection (with PASCAL VOC dataset) tasks.\n\n+ The proposed method shows competitive performance."
            },
            "weaknesses": {
                "value": "- The method and presentation.\n\nThis paper improves contrastive knowledge distillation from the perspective of leveraging relation features at multiple intermediate layers/stages instead of one single layer (typically at penultimate layer). The proposed method includes two improvements, both of which are straightforward. First, applying contrastive knowledge distillation to the teacher-student layer pair which has the highest similarity among a set of candidate layer pairs. Second, separately using cosine similarity and L2-normed distance to invoke two types of contrastive knowledge distillation. Although the authors call these two simple improvements as Adaptive Stage Selection and Relation Decouple Module, they are hand-crafted but not \"adaptive\" and \"decoupled\". Moreover, I have not seen any theoretical insights for the design/formulation of them. Also, why and how they work are not clear to some degree.\n\nBesides, multi-stage designs are widely used in many existing knowledge distillation methods. Discussion of this line research is missing. \n\n- The limitations.\n\nThe authors did not discuss the limitations of the proposed method.\n\n- The experiments.\n\nTo comparative experiments on CIAFR-100 (Table 1), did the authors run the reference methods by themselves? Why the results for these methods are usually lower than the formal results reported in the original papers, e.g., SSKD which is closely related to the proposed method.\n\nTo comparative experiments in Table 1, Table 2 and Table 9 (in the Appendix), it seems that the results of the proposed method are obtained by using logits based KD at the head. I would like to see a more fair comparison, either using logits based KD to all methods or removing it to all methods.\n\nFurthermore, the proposed method adopts mixup in training. In this context, the baseline with mixup should be reported and used as more proper baseline. And with mixup, the performance of the reference methods would be also improved.\n\nFrom Table 6, the proposed method performs best with one single stage setting. This is weird to a large degree. Why?"
            },
            "questions": {
                "value": "Please refer to my detailed comments in \"Weaknesses\" for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2985/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839822032,
        "cdate": 1698839822032,
        "tmdate": 1699636242895,
        "mdate": 1699636242895,
        "license": "CC BY 4.0",
        "version": 2
    }
]