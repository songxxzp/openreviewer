[
    {
        "id": "FWVwGBYv5S",
        "forum": "uznKlCpWjV",
        "replyto": "uznKlCpWjV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_v9Uz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_v9Uz"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a theoretical analysis of the convergence of a variant of the popular PPO algorithm where the objective function is clipped."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides formal guarantees for the convergence of the clipping variant of PPO that has been used widely by reinforcement learning practitioners. This probably adds some reassurance that that this is a sound RL method to use."
            },
            "weaknesses": {
                "value": "The paper appears to be a good contribution to the filed of RL, but has little, if anything, to do with representation learning. ICLR'24 might not be the best venue for this paper.\n\nSome minor typos:\nLast paragraph on page 1: \"this analysis rely\" -> \"this analysis relies\"\nSame place: \"no longer involve\" -> \"no longer involves\""
            },
            "questions": {
                "value": "Can you think of a possible impact your result can have on the field of representation learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697984994130,
        "cdate": 1697984994130,
        "tmdate": 1699636132465,
        "mdate": 1699636132465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HrSfVW8mB1",
        "forum": "uznKlCpWjV",
        "replyto": "uznKlCpWjV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_wVXF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_wVXF"
        ],
        "content": {
            "summary": {
                "value": "This work takes a closer look at the theories behind the clipped surrogate objective of PPO (Proximal Policy Optimization). The authors provide a comprehensive analysis that proves the stationary point convergence of PPO-Clip and demonstrates the convergence rate."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work is novel in that it investigates the theoretical convergence of PPO, whereas the field has overwhelmingly focused on the empirical performance and application of PPO (e.g., Dota 2 with PPO and ChatGPT, RLFH with PPO). This is partially because the clip operation is inherently non-smooth, thus posing challenges to empirical analysis. The authors' analysis seems sound and comprehensive; they also clearly listed out the necessary list of assumptions. The authors' Theorem 3.2. indicates \"PPO-Clip has the same convergence property as the best current results available for policy gradient\", which seems significant."
            },
            "weaknesses": {
                "value": "Please take this with a grain of salt, as I have primarily been using PPO under empirical settings. I struggle to understand how this work connects with the wider research community. What is the implication of this work? This work demonstrates PPO has stationary point convergence \u2014 can this property be used in some ways?"
            },
            "questions": {
                "value": "> the unbounded score function makes the ratio of two policies arbitrarily large, even in the late stages of the optimization process.\nDo the authors mean the ratio **could** become arbitrarily large?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2010/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2010/Reviewer_wVXF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608251876,
        "cdate": 1698608251876,
        "tmdate": 1699636132390,
        "mdate": 1699636132390,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kGLC1umaoj",
        "forum": "uznKlCpWjV",
        "replyto": "uznKlCpWjV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_dSqC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2010/Reviewer_dSqC"
        ],
        "content": {
            "summary": {
                "value": "This paper presents theoretical analysis of convergence of PPO algorithm using clipping and version of two-time scale method, i.e., updating policy parameter with particular period. Analysis of PPO-clip is difficult due to non-smoothness of clipping operator and involves ratio of policies. Theorem 3.3 provides best iterate and averaged iterate convergence in terms of $||\\nabla V(\\theta_{n,1})||\\to 0$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles important question regrading theoretical analysis of practical implementation of PPO algorithm. The aim of the paper is clear and simple. \n\n- The analysis seems to be novel. The authors use two-time scale method to overcome the difficulty to deal with ratio of the policy and constructing particular set of events enables to derive recursive inequalities to bound the norm of gradient of the clipped loss function."
            },
            "weaknesses": {
                "value": "- Even though the aim of this paper is to tackle theoretical analysis of practical implementation of PPO, still there is some gap. Using a inner and outer loop style update seems to be far from practical implementation.\n-  The estimate for advantage function, $\\phi_n$ can be estimated with parameterized value network (e.g. neural network). However, I believe extending the proof to Actor-Critic setting is non-trivial. Hence, I believe the proof is setting restricted to Monte-Carlo setting. \n- Assumption 3.4. restricts the generality of $T$ and the learning rate. Providing simple examples on the learning rate, and $T$ would be helpful to understand the conditions about Assumption 3.4. For example, can we use $\\frac{1}{k}$ or $\\frac{1}{\\sqrt{k}}$ as the step-size?"
            },
            "questions": {
                "value": "- Above the paragraph of Step 1, what does it mean to have similar recursive inequality like policy gradient? Please provide more details.\n\n- In Step 1, how does the estimated clipped PPO loss have gradient? Should it be sub-gradient due to the non-smoothness of the clipping operator? Please provide more clarifications.\n\n- What is the intuitive meaning of $X_{n,k}$ and $Y_{n,k}$? The motivation of decomposition of error term  into $X_{n,k}$ and $Y_{n,k}$ is not really clear\n\n- The introduction of $C_{n,k}$ in Step 2 is quite abrupt. Please provide more details.\n\n- In Step 2, what is the meaning of bound of $\\mathbb{E}[||X_k|| \\mathcal{F} ]$? Depending on $\\frac{1}{\\sqrt{\\pi_{\\theta_{n-1,1}}(s,a)}}$ seems to be problematic. How is this term compensated? In deriving (14) where did $\\frac{1}{\\sqrt{\\pi_{\\theta_{n-1,1}(a\\mid s)}}}$ go?\n\n- I think there is typo in (13). $\\nabla V(\\theta_{n-1})$ should be $\\nabla V(\\theta_{n-1,1})$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2010/Reviewer_dSqC",
                    "ICLR.cc/2024/Conference/Submission2010/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834630415,
        "cdate": 1698834630415,
        "tmdate": 1700190951783,
        "mdate": 1700190951783,
        "license": "CC BY 4.0",
        "version": 2
    }
]