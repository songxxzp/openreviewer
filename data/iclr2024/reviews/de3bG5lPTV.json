[
    {
        "id": "kb23PGjKJJ",
        "forum": "de3bG5lPTV",
        "replyto": "de3bG5lPTV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_Vgbg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_Vgbg"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses two key issues in existing MARL methods: miscoordination and relative overgeneralization. To tackle these problems, the paper introduces local Q-functions as the logsumexp of global Q-functions with an energy-based policy. The authors then derive a multi-agent optimistic soft value iteration algorithm to learn nearly optimal policies and extend it to soft Q-learning using neural networks. Experimental results confirm the effectiveness of their proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using soft value functions to approximate optimality is interesting and novel.\nThe paper provides sufficient theoretical derivations and proofs."
            },
            "weaknesses": {
                "value": "Some notations require further clarification.\nExperimental evaluations appear somewhat simplistic.\nIt is unclear how the proposed method's optimality relates to the original MARL problem. \nSpecific questions have been raised, as detailed below."
            },
            "questions": {
                "value": "1. The paper states that policy gradient methods always fail to converge to the globally optimal solution. Please clarify which policy gradient algorithm in MARL this statement refers to and provide supporting evidence for the claim. The original explaination is unclear.\n\n2. The paper mentions that there is no theoretical guarantee that QTRAN and QPLEX can resolve relative overgeneralization, despite these methods being considered sufficient and necessary in IGM. Please provide further explanation if the authors think there is a gap between the optimality guarantee in this paper and their guarantees under IGM.\n\n3. Where is $\\hat{Q}^n$ defined? Is it $\\hat{Q}^n_\\zeta$?\n\n4. The objective defined in Lemma 1 appears different from the original expected return in RL or an entropy-regularized one. Please provide further explanation regarding what it means to achieve optimality in the context of this paper's defined objective function.\n\n5. The paper mentions that the proposed definition is equivalent to the \"original\" one when \\alpha and \\beta approach infinity. Please clarify what \"original\" means, along with the \"original\" problem mentioned in Theorem 2.\n\n6. What does $\\widetilde{Q}^*_{(\\infty, \\infty)}$ mean? While the fixed point of the original Bellman equation represents the optimal value function, please explain the significance of the fixed point of the Bellman operator defined in this paper.\n\n7. Where is $a^*_{(\\alpha,\\beta,\\gamma)}$ defined?\n\n8. The paper mentions that assigning different $\\zeta(\u00b7|s)$ on symmetric actions can break the symmetry. Given that miscoordination is one of the problems the paper aims to solve, please provide detailed explanations on how this mechanism addresses the issue, and how it works practically.\n\n9. What is the behavior policy in Algorithm 2 and 3? Is it $\\hat{\\zeta}$?\n\n10. The $L_{extra}$ term needs further explanation. Is it necessary for the experimental results? What does it signify when $\\hat{Q}^n(s,a^{\\*n}) > \\widetilde{Q}(s,a^*)$?\n\n11. Why not compare the proposed method with other value-based methods like QTRAN and QPLEX in experiments, considering that the proposed method is also value-based?\n\n12. The experiments appear overly simplistic. It would be beneficial to demonstrate the performance of the proposed method in more complex domains such as SMAC and GRF.\n\n13. The LBF experiments do not effectively demonstrate the proposed method's superiority. MAOSDQN does not outperform value-based QMIX or other policy-based methods. Additionally, please explain why $\\epsilon$ is set to be 0.5, which differs from QMIX in LBF.\n\n14. The proposed method seems highly sensitive to the parameters $\\alpha$ and $\\beta$. It is recommended that the authors conduct ablation studies to illustrate how these parameters affect optimality and convergence.\n\n15. While the paper proves that a large $\\beta$ can help in finding optima, one might suggest that a reward shaping method, e.g., $r' = e^{\\beta r}$, with a large $\\beta$, could yield similar results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698149669953,
        "cdate": 1698149669953,
        "tmdate": 1699636764916,
        "mdate": 1699636764916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1L4sbyzSWr",
        "forum": "de3bG5lPTV",
        "replyto": "de3bG5lPTV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_8w13"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_8w13"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to gain new theoretical understandings of multi-agent reinforcement learning algorithms in the realm of centralized training with decentralized execution. The authors found that current algorithms struggle with some basic tasks because of miscoordination and relative overgeneralization. To solve this, they introduce a new method called \"multi-agent optimistic soft q-learning\", which is implemented with an optimistic local Q-function and a softmax local policy. Here are the key points about this new solution:\n\n1.\tIt is a solution with a provable global convergence guarantee. \n2.\tIts objective gives near-optimal policies with a tractable error bound. \n3.\tIt can be easily modified for deep reinforcement learning scenarios. \n\nTo test this model, they tried it in simple learning environments as well as more complex ones, like the level-based foraging environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The subject of this research is quite interesting. It focuses on addressing two major problems, which are miscoordination and relative overgeneralization, all within a single approach.\n\n2. I really appreciate how the authors included clear examples in Section 2.3. These examples make it easier to grasp the concepts of these two issues.\n\n3. The authors have put in a lot of work to back up their solution with solid theoretical evidence."
            },
            "weaknesses": {
                "value": "1. It's not entirely clear if the problems of miscoordination and relative overgeneralization are really significant in practical uses of multi-agent reinforcement learning. This uncertainty makes it difficult to assess the true impact of this research.\n\n2. The experiments conducted seem quite basic, which doesn't provide enough evidence to fully support the authors' claims. For instance, in Sections 4.1 and 4.2, it would be helpful if the authors offered deeper insights into their experimental results, explaining why other methods didn't work and what makes their proposed solution effective in these scenarios.\n\n3. The choice of baseline models for comparison seems improper. There are several existing studies that address the issues mentioned. For example, regarding the problem of relative overgeneralization, the authors could have referred to works like \"Lenient Multi-Agent Deep Reinforcement Learning\" by Gregory Palmer et al., \"Actor-Attention-Critic for Multi-Agent Reinforcement Learning\" by Shariq Iqbal and Fei Sha, and \"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning\" by Ying Wen et al. These studies would have been more appropriate for comparison.\n\nSome small points:\n\n1. When talking about \u201coptimal adaptive learning (OAL) (Wang & Sandholm (2002)) is the first algorithm with provable convergence to the optimal policy\u201d, it might be useful to also mention the earlier work \"Nash convergence of gradient dynamics in general-sum games\" by Satinder Singh, Michael Kearns, and Yishay Mansour, which also discusses the convergence to Nash equilibrium.\n\n2. The text size in Figures 1 to 3 could be made bigger for easier reading."
            },
            "questions": {
                "value": "1. How serious are the problems of miscoordination and relative overgeneralization in real-life situations?\n\n2. Can you offer more explanation about why earlier studies couldn't tackle these two issues effectively?\n\n3. In the section on related work, you mentioned that previous research mainly concentrated on achieving Nash equilibrium in general-sum games and didn't focus much on cooperative settings. Could you elaborate on the differences between cooperative games and general-sum games? Are solutions designed for general-sum games still applicable in cooperative scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6676/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6676/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6676/Reviewer_8w13"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675062045,
        "cdate": 1698675062045,
        "tmdate": 1699636764804,
        "mdate": 1699636764804,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o0eZ3Q9Ta6",
        "forum": "de3bG5lPTV",
        "replyto": "de3bG5lPTV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_njtx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_njtx"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel multi-agent RL algorithm, which involves centralized training and distributed execution. Specifically, the authors design algorithms including MAOSQL, MAOSVI, and MAOSDQN. These algorithms are different variants and share the similar underlying convergence behavior. The authors theoretically analyze the global convergence and justify their claims. To validate the proposed algorithms, the authors use three benchmark models and a few baselines to show the superiority."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The investigated topic in this paper is quite interesting and critical and relevant to the community. The paper is well motivated and seems theoretically strong."
            },
            "weaknesses": {
                "value": "1. The authors mentioned two critical issues: miscoordination and overgeneralization. However, in the convergence guarantee, it is unclear to me how the authors have addressed these two issues.\n\n2. For these three algorithms, the selection of hyper-parameters are extremely important. Are there any specific rules for them to choose for different scenarios? Or they follow the similar values in various cases? In Theorem 2, the error bounds are related to these parameters. What are the values of them such that the bounds are tight?\n\n3. The epsilon-decentralized-softmax policy requires the $\\epsilon$, how to determine this value in practical scenarios?\n\n4. The experimental results are not promising. All games need variances in the plot or tables. Also, the information in Tables 4 and 5 is confusing. Additionally, the results in Table 6 did not show the superiority of the proposed MAOSDQN as other algorithms outperform in various tasks. \n\n5. Please clarify the notations in all theoretical statements to make them more readable."
            },
            "questions": {
                "value": "Please see the questions in the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805690004,
        "cdate": 1698805690004,
        "tmdate": 1699636764666,
        "mdate": 1699636764666,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tj8OMJvuVV",
        "forum": "de3bG5lPTV",
        "replyto": "de3bG5lPTV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_aokn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6676/Reviewer_aokn"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a co-MARL algorithm that solves miscoordination and relative overgeneration problems. The algorithm defines a local value function and a relation between the local value function and local policy of each agent. The proposed approach was also extended to neural network function approximators."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and the empirical results shows that the proposed algorithm with neural network approximators, MAOSDQN, performs better than the baselines."
            },
            "weaknesses": {
                "value": "In the empirical results, I am not convinced by the discussion for why the algorithms (including MAOSDQN) behave like that. More importantly, it seems that QMIX does not learn in the single-state Matrix game. I would appreciate if the authors can expand upon the discussion in section 4 and explain why we see a significant performance improvement between MAOSDQN and the baselines."
            },
            "questions": {
                "value": "I don\u2019t have questions at this revision cycle."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6676/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699509986319,
        "cdate": 1699509986319,
        "tmdate": 1699636764547,
        "mdate": 1699636764547,
        "license": "CC BY 4.0",
        "version": 2
    }
]