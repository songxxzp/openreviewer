[
    {
        "id": "a4Ebb970U8",
        "forum": "5EtSvYUU0v",
        "replyto": "5EtSvYUU0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_Bpa7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_Bpa7"
        ],
        "content": {
            "summary": {
                "value": "The submission introduces a generalised kernel - Neural Dynamic Kernel (NDK)  for stochastic gradient for very wide (infinite) neural networks. The Neural Tangent Kernel (NTK) and Neural Network Gaussian Process Kernel (NNGP) are two different timescale limits of the NDK - the NTK on the early training as deterministic gradient driven and the later driven by diffusion. \n\nThe NDK kernel different from the earlier Kernels as it is explicitly time dependent, and allows, in the short time scale and long time scale limits two different,  NTK (deterministic)  and NNGP (diffusive), like behaviours - the main contribution of the paper. \n\nThe argumentation in the appendixes is thorough, and the trial cases support the conclusions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Clear representation, with excellent narrative in the appendixes. These support the main text that concentrates nicely on the main line of thought. Some results are surprising like the different qualitative behaviours of the kernels in the one layer and two layer cases."
            },
            "weaknesses": {
                "value": "The meaning of  time dependence of the NDK kernel would need discussion.  Already the learning rate of the stochastic gradient is tuned and this is essential for the minimising algorithm to work. Now, two different domains arise when the gradient is big and one is far for the optimal point - it is good to move with big steps down the slope. At the bottom, the finite time step size, in gradient decent, will lead to diffusive Brownian motion around the minimum until the learning rate shrinks to zero. This behaviour is more a property of the stochastic gradient than a property of the neural networks themselves.\n\nAs discussed in the manuscript, the practical trials of the Kernel have to be performed (like the Langevin equation) with discretised time. The discretisation errors will lead to extra diffusive contribution and to the above description - unless a hybrid Monte Carlo is used where the evolution of differential equation is proven to be ergodic and reversible. Hence, we cannot easily distinguish the root cause of the phenomenon. There is no clear indication if the behaviours of the Kernels are coming from the structure of the model, or the structure of how it is solved."
            },
            "questions": {
                "value": "How would a given learning rate profiles influence your conclusions? I think a nonlinear time variable would do the trick. Can it change the qualitative behaviour as well, as the time dependent NDK?\n\nThe large N limit, with data size fixed to finite value leads automatically to overfitting, as the weights of the connection to the a neutron are a template of the input. With random weights of infinite extent, there will always be a fitting template for a particular data item. Generalisation suffers, as it would require an efficient compression that finds common templates for multiple data items. In the infinite N limit, this does not happen. Is there a way to look at the large N limit in such a way that one also includes the interesting large data limit?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Reviewer_Bpa7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698396676990,
        "cdate": 1698396676990,
        "tmdate": 1699969437145,
        "mdate": 1699969437145,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AmnuasolrN",
        "forum": "5EtSvYUU0v",
        "replyto": "5EtSvYUU0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_2yMn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_2yMn"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a generalized framework, that includes the Neural Tangent Kernel and the NNGP as special cases. This is facilitated by adding a noise term to the training dynamics (Langevin equations), and then expressing the predictor moments in the infinite-width limit as path integrals over the Langevin trajetories. Theory and experiments investigate training dynamics, finding a gradient-driven phase (NTK) and a diffusion-driven phase (NNGP in long-term equilibrium) in time. Generalization experiments in a classification task then show how noise-related hyperparameters related to initialization and regularization allow to trade the time scales of the two distinct phases against each other. The role of depth and non-linearity is briefly discussed. Lastly, the findings are related to a hypothesis of read-out weight realignment, which was proposed for explaining representational drift observed in neuroscience. For this, the read-out weights are frozen after sufficient training, while the hidden weights are allowed to further diffuse. Experiments show that, depending on whether the kernel is meaningful for the task, the outputs of training and test data decorrelate over diffusion time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1) A generalized framework is proposed, incorporating both NTK and NNGP. This is highly relevant, as both frameworks have led to major insights into NN theory. Such a generalized framework is of utmost importance and may lead to further novel deep insights.\n(2) The approach through treating the necessary noise-injection via Langevin dynamics and path integral formulations is novel and innovative, and may pave the way for a new tool to investigate learning systems via kernel representations. The trick with the markov proximal learning together with the replica trick may even be applied to much more general learning systems.\n(3) Interesting original experiments are shown"
            },
            "weaknesses": {
                "value": "(1) Clarity / presentation seems a bit unfocused or maybe even unripe. E.g. the paper's main theoretical result is Eq. 5, only which then allows to elucidate the relations to NNGP and NTK. This Eq. 5 is formulated in terms of random variables \\vec{v} and \\vec{u}. While \\vec{v} is briefly described in prosaic manner, \\vec{u} is not explained at all. It is burried deep in the supplementary. Another example is the statement in some regime \"the integral equation can be transformed into a linear ODE\". This should be supported/elaborated, and if done so in the supplementary, it needs a precise pointer where at exactly. Another example is that the Appendix A is never mentioned anywhere, albeit a major aspect of the theory outlined in Appendix B, as otherwise the path integrals cannot be easily factorized. The paper's originality and meaning is difficult to understand without studying the supplementary in detail."
            },
            "questions": {
                "value": "(1) Figure 3 is not clear, can you kindly add some more explanations esp. wrt. early stopping and what you mean with it precisely\n(2) In the main body (introduction), it is stated that the infinite-width limit is pursued but without requiring the amount of data to scale as the width of the NN. I.e. with finite data. This is the typical NTK or NNGP regime. However, for the main result eq. 5 the replica method is used. To my understanding, the replica method typically relies on a scaling of the data with the NN width. However, the necessity of an infinite-data simultaneous limit is never discussed in the main paper, but is also clearly departing from NTK and NNGP. This seems to indicate a contradiction or at least creates confusion. Please clarify.\n(3) In SI eq. 27, the exchange of integral and product of d\\theta_\\tau is done one line too early."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6006/Reviewer_2yMn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698417665255,
        "cdate": 1698417665255,
        "tmdate": 1699636643799,
        "mdate": 1699636643799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XzTcxy79ab",
        "forum": "5EtSvYUU0v",
        "replyto": "5EtSvYUU0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_88ey"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_88ey"
        ],
        "content": {
            "summary": {
                "value": "The authors study the learning dynamics of (regularized) infinite-width neural networks under Langevin dynamics (continuous stochastic gradient descent). They derive a new kernel termed the neural dynamic kernel (NDK) which exhibits both NTK and NNGP behavior in the limits. NDK can be thought of as the two-time extension of NTK under Langevin dynamics. Consistent with empirical results it explains two phases of dynamics, a deterministic gradient-driven phase (akin to NTK) followed by a diffusive phase exploring the solution space and approaching the posterior distribution of an infinite-width NN with Normal prior over the weights (akin to NNGP). Empirical results are in line with theoretical arguments and explore the two phases of learning dynamics as well as the effect of depth, temperature, initialization variance, and equilibrium variance. Furthermore, the authors use the theoretical framework and make connections to the representational drift phenomenon observed in neural systems which corresponds to the reorganization of synaptic weights while maintaining the behavioral performance in experimental tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "There has been a large interest in the behavior of overparameterized neural networks in the ML community. NTK and NNGP were introduced as two seemingly separate tools for studying the limiting behavior of infinite neural networks in the deterministic and Bayesian settings. While previous literature has pointed to connections between the two frameworks a clear unified framework for understanding the two didn't exist. This paper tackles this problem by introducing NDK and studying the limiting behavior.\n\nThe empirical results make the theoretical arguments very clear. Depicting the two phases of dynamics along with the limiting behavior of the models provides compelling evidence for the theoretical framework.\n\nThe connections to the neuroscience literature on representational drift are insightful and interesting (although this paper is not the first to make this connection and other papers in the literature on overparameterized neural nets already made this connection before) [1,2].\n\n[1] https://arxiv.org/abs/2110.06914\n\n[2] https://arxiv.org/abs/1904.09080"
            },
            "weaknesses": {
                "value": "My main comment is about the introduction part of the paper which can be made more comprehensive. The literature on learning dynamics in overparameterized neural nets has progressed quite extensively and learning dynamics, and placing the work in the existing literature and recognizing existing contributions fairly will make the paper stronger.\n\nAlthough I think the two-time extension of NTK is a novel contribution, I\u2019ve seen the one-time extension of NTK before (see e.g. Lemma 8.1.1 of [1]). In fact, the development of the NTK is the consequence of the limiting behavior of this one-time extension.\n\nThe authors cite a paper that discusses the diffusive properties of Langevin dynamics in the long-time limit. Another paper that discusses this diffusive behavior under learning using SGD is [2]. It appears to me that this is not a novel finding as it was reported before and it's a phenomenon that's already justified using theoretical arguments. Although this paper provides a unique perspective connecting NTK and NNGP using NDK I would frame the diffusive behavior of the SGD as a confirmation of the existing literature rather than a novel finding.\n\n[1] https://www.cs.princeton.edu/courses/archive/fall19/cos597B/lecnotes/bookdraft.pdf\n\n[2] https://arxiv.org/abs/2110.06914"
            },
            "questions": {
                "value": "- Can the authors discuss the extensions to multivariate outputs?\n- Can the framework be extended to different forms of regularization? Specifically, do we expect to observe the diffusive behavior and the limiting posterior distribution of a Bayesian model with a different prior under a different regularization (e.g. l1 regularization)?\n- The authors argue the relevance of SGD in the context of batch learning, does this framework have the potential to explain other tricks used in the learning of neural nets such as dropout, batch-norm, etc.?\n- Can we derive bounds w.r.t. the number of iterations, depth, number of training samples, etc. on when to expect to observe the transition from phase 1 to phase 2 and use that as the early stopping criterion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791114695,
        "cdate": 1698791114695,
        "tmdate": 1699636643692,
        "mdate": 1699636643692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jSSSucNWYK",
        "forum": "5EtSvYUU0v",
        "replyto": "5EtSvYUU0v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_2CVF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6006/Reviewer_2CVF"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the connection between deep neural networks and kernel learning. Specifically, the authors take a representation of the dynamics of evolution of the hyper-parameters in what is presumably a neural network and study the evolution of the kernel corresponding to the feature space. The authors show that the resulting kernel evolves from the NTK to the NNGP kernel (or at least that is my interpretation)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While there is obviously a significant body of work on convergence to the NTK, the connection from NTK to NNGP under stochastic noise is an interesting hypothesis. The authors explore the implications from several perspectives -- including testing the proposed NDK for different regimes and some interesting outcomes such as early-stopping and representation drift."
            },
            "weaknesses": {
                "value": "My major concerns with the paper, however, are not on the idea itself, however, but rather that the fundamental results are not rigorously established before proceeding to interpretation. This makes it hard to quantify or verify the claims. Specifically, the paper has no proofs or definitions which can be verified. There are many claims being made and several equations used to back up those claims. However, without rigorous definitions and statements to prove, it is difficult to verify these claims. In addition, the notation is inconsistent and meaning is unclear at times. \n\nMy rating for this paper is based on the anticipated magnitude of required restructuring and revision."
            },
            "questions": {
                "value": "Some detailed notes and questions supporting the summary are as given below.\n\n1) The authors make constant use of the first person plural in a way which does not encompass the reader. This makes it seem as if the paper were about the authors and not about the result.\n\n2) Section 2.2 has a lack of definitions. For example, the variable $\\mathbf{u}(t)$ --  what are the particular properties of $u$ for which  Equation $5$ holds?\n\n3) Some motivation for NDK kernel is presented in Equation 5, but Equation 5 has not been properly introduced.  Authors should include at least basic information describing equation 5 in Section 2.2 and show why this statistic $S$ is important. \n    \n4) There is quite a bit of inconsistent notation. For example, Equation 6 has a function $\\nabla_\\theta f(x, t)$ and then Equation 9 uses n $f(x, \\theta)$. In another example, $K$ has variously four arguments and then three and then two.  \n\n5) The authors often use $\\dot K(t,t',x,x')$. But what is the derivative with respect to?\n \n6) Equation 16 should be described in more detail. If Equation 16 is just a classical NNGP result, then the connection between NDK and NNGP is unclear. If Equation 16 is the limit of Equation 13, then this result should be presented in the paper as well, since Equation 13 depends on $\\mathbf{f}_{\\text{train}}(t')$.\n\n7) If we consider $t = t'$ for NDK, how is the dynamic different from NTK? \n\n8) In Fig 2, it would be nice to add NNGP equilibrium for comparison.\n\n9) How do authors define the optimal early stopping point and  optimum generalization error? \n\n10) The font of labels, legends and titles of all figures should be increased. \n    \n11) ``All the kernel functions above including the two-time NNGP kernel, the\nderivative kernel, and the NDK, have closed-form expressions for specific activation functions such\nas linear, ReLU and error function (see SI Sec.C, Cho & Saul (2009)).'' -- which equations are the authors referring to?\n\n12) Supplementary information such as Sec. B provides little to no explanation or context.\n\n13) ``predictor converges fast to the desired''\n\n14) ``Our theoretical framework for the Langevin dynamics provides a model of representational drift'' Which framework? The model? This is unclear.\n\n15) Margin violation on Eq 88"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6006/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829830248,
        "cdate": 1698829830248,
        "tmdate": 1699636643587,
        "mdate": 1699636643587,
        "license": "CC BY 4.0",
        "version": 2
    }
]