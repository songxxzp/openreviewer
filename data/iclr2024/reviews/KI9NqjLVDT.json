[
    {
        "id": "vR4UfGFsFT",
        "forum": "KI9NqjLVDT",
        "replyto": "KI9NqjLVDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an imputation method, ReMasker, that leverages a remasking autoencoder with a Transformer-based architecture. A significant amount of experimentation is included to validate the proposed procedure."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method provides a simpler heuristic than competing SOTA methods such as HyperImpute, while displaying similar or slightly improved accuracy. \n\nThe experiments are relatively thorough and show strong results for the MCAR setting in particular."
            },
            "weaknesses": {
                "value": "The methodology is not novel, as masking is a common practice for imputation algorithms and masked autoencoders with Transformers are used for other tasks. However, the authors do not claim novelty and I agree this is likely the first work to consider a masked autoencoder with Transformers for imputation specifically. \n\nThe claims that ReMasker generalizes to MNAR settings is not well justified:\n\n* It\u2019s not clear how we could properly learn a missingness-invariant representation in the MNAR setting. Under MNAR, the missing data values fundamentally depend on the missingness mechanism, so recovering a missingness-invariant representation would require external information.\n\n* The experimental condition for MNAR (masking via a Bernoulli random variable with fixed mean) is not representative of many MNAR settings. For example, cases where values across possibly many variables are masked due to censoring of one or more variables (e.g. $X_2$, $X_5$, $X_6$ are missing when $X_5 < 0$). The missing values often follow a completely different distribution than the observed values, which cannot be recovered using the observed data alone.\n\nIt is not clear that the results are practically different from HyperImpute, and perhaps other methods. The included bar plots are hard to read, so any assessment is difficult. The relative simplicity of ReMasker compared to HyperImpute is mentioned as one advantage, although the much simpler ICE appears competitive in almost all scenarios.\n\nWhile the theoretical justification is appropriately included in the Discussion section, the justification is a conjecture with minimal validation. In practice, the missingness mechanism is different for $m^+$ and $m^-$, so it is not clear that representation will be invariant in general. Further experimentation is needed."
            },
            "questions": {
                "value": "Is there a better way to represent the results visually? It is very difficult to read the plots (e.g. Figure 2), and in particular to the confidence intervals.\n\nWhile it is clearly stated that imputation for downstream tasks is not part of the scope of the paper, can any more discussion be provided? Considering imputation is almost always followed by downstream tasks, I believe more justification for this decision is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_YQFA",
                    "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698256407542,
        "cdate": 1698256407542,
        "tmdate": 1700491475564,
        "mdate": 1700491475564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s89cvPTsU7",
        "forum": "KI9NqjLVDT",
        "replyto": "KI9NqjLVDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c"
        ],
        "content": {
            "summary": {
                "value": "The authors propose ReMasker, a method for imputing missing values in tabular data based on masked auto encoders (MAE). Specifically, given a dataset, they mask some of the features and train a model to impute these features. The approach does not require all features to be present in training, so a dataset with missing values can be used for training. The authors provide extensive empirical study and a theoretical justification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think the authors propose an elegant solution to a problem many practitioners encounter. The paper is well written and the empirical results are convincing. \nIn general, for an approach such as the one presented, it is difficult to provide a theoretical analysis. I appreciate the theoretical justification in Section 5, but I would have liked it to have a more prominent position in the paper."
            },
            "weaknesses": {
                "value": "The authors fail to mention and discuss multiple previous works. Specifically, transformers have been previously been applied to tabular data (Somepalli et al. 2021, Ar\u0131k et al. 2020,  Huang et al 2020). Specific to Huang et al, the authors cite this paper for supporting their model choice, but fail to mention it in the related works. The MAE work by Majmundar et al. (2022) is also not discussed.\n\n\nSomepalli et al. 2021: SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training\nAr\u0131k et al. 2020: TabNet: Attentive Interpretable Tabular Learning\nHuang et al 2020: TabTransformer: Tabular Data Modeling Using Contextual Embeddings\nMajmundar et al. 2022: MET: Masked Encoding for Tabular Data"
            },
            "questions": {
                "value": "I have one minor question:\n- On page 1, you state that discriminative imputers are hindered by the requirement of specifying a functional form. It seems to me that your architecture is also a choice of \"functional form\". Am I wrong?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_Fz3c",
                    "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698660479522,
        "cdate": 1698660479522,
        "tmdate": 1700474781671,
        "mdate": 1700474781671,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0t3yfhKnWD",
        "forum": "KI9NqjLVDT",
        "replyto": "KI9NqjLVDT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
        ],
        "content": {
            "summary": {
                "value": "The paper Remasker: Imputing Tabular Data with Masked Autoencoding presents a novel algorithm for missing data imputation in tabular datasets using the masked autoencoding framework. The algorithm operates in two stages: a first stage where extra variables are masked and the autoencoder is optimized to reconstruct the re-masked variables, and a second stage where the trained model is used to predict the missing values. Authors evaluate the method on a wide range of state-of-the-art methods in several tabular datasets, showing competitive performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and grammatically correct with no evident typos. Every section is very well outlined with clear paragraphs describing the content of the manuscript. \n\nThe proposed method, namely the Remasker method, is easy to understand for experts and non-experts in the field. The idea is intuitive yet effective, showing promising results on how we can leverage the promises of transformer-based architecture for missing data imputation. \n\nThe evaluation of the method is quite thorough, where standard tabular datasets are tested under different state-of-the-art methods. This reveals that the authors are aware of the most recent advances in missing data imputation. Besides, different metrics and ablation studies are performed to motivate the performance of the method further. Another precious point is that the authors do comment on the limitations of their proposed method, which is something that is sometimes overlooked."
            },
            "weaknesses": {
                "value": "I have a few concerns and questions that I'd like the authors to address:\n\n1. While I understand the primary motivation behind using a transformer-based architecture in this work, I would appreciate a more in-depth discussion of why the authors chose to use masked autoencoders over other approaches, like deep generative models. It's crucial to outline the advantages and disadvantages of this choice. Additionally, it would be beneficial to address questions like the computational cost of training the Remasker method and its ability to generate new data, which is a common concern in the missing data imputation field.\n2. The argument presented in Section 4.1 about why Remasker generalizes different missing data assumptions could be strengthened. While transformer-based architectures have proven their superior performance in various tasks, a more comprehensive analysis of why transformers excel in missing data imputation would enhance the paper's quality and its contribution to both the missing data imputation and transformer-based research fields.\n3. The manuscript appears to be densely packed in terms of space. Figures and tables are positioned very closely to the text, which can make the paper seem cluttered and condensed. Consider the possibility of making some tables smaller or relocating them to the Appendix (e.g., Table 1), and possibly reducing the size of certain figures (e.g., Figure 2 with fewer datasets). This issue can be addressed in the camera-ready version if the paper is accepted."
            },
            "questions": {
                "value": "Some questions I have are the following:\n\n1. I'd like to understand the rationale behind using a deep encoder and a shallow decoder when describing the decoder. While the Ablation Study in Section 4.3 provides some insight, could you provide a more abstract explanation for this choice? Is there a specific reason why you believe the first fitting stage of the Remasker benefits from a more complex encoder?\n2. Some figures display results for different baselines (e.g., Figure 3 RMSE), and while they may appear distinct, the values on the y-axis are actually quite close. Could you elaborate on how these slight differences influence the final results? For instance, in Figure 3, Remasker outperforms other methods by only a narrow margin in terms of RMSE and AUROC. How does this marginal improvement make Remasker a more beneficial choice compared to other methods that utilize deep generative models to learn the data distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4077/Reviewer_w2Hd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4077/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755596477,
        "cdate": 1698755596477,
        "tmdate": 1699636371918,
        "mdate": 1699636371918,
        "license": "CC BY 4.0",
        "version": 2
    }
]