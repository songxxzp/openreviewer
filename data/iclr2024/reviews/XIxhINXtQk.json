[
    {
        "id": "qFi7C7D3YM",
        "forum": "XIxhINXtQk",
        "replyto": "XIxhINXtQk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that generates a 3D-edited NeRF from a single portrait image. The style is defined by an instructing prompt. Two streams of inputs of a real face and a 2D-edited face are passed through an encoder to generate identity conditions. The identity condition, together with text condition, is sent to a diffusion model to generate tri-plane features for NeRF rendering. Experimental results show that the proposed method outperforms compared baseline approaches under the authors' settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- It is the first (to my knowledge) paper that allows \"instructed\" 3D portrait editing from single images.\n\n- The experiments show that the proposed method outperforms compared baselines under the authors' settings."
            },
            "weaknesses": {
                "value": "- The results shown in the paper lack race diversity. There are almost no Asian or black people. I'm worried whether the proposed method does not perform well on those cases.\n\n- The identity may change after applying the proposed method. For example, in Fig. 1 first example, the eye shape changed after the beard was removed. In Fig. 3 middle example, the girl seems to look more Asian and the nose shape changed after editing. These are not analyzed in the limitation section.\n\n- The proposed method adopts two streams of inputs (real and edited images). However, the ablation study does not show the necessity of  \n them. Will only one stream work?"
            },
            "questions": {
                "value": "I would like to see the authors address my concerns mentioned in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698536517234,
        "cdate": 1698536517234,
        "tmdate": 1700668086226,
        "mdate": 1700668086226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y3I41VMxtI",
        "forum": "XIxhINXtQk",
        "replyto": "XIxhINXtQk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method that enables the creation of 3D portraits that have been edited based on text prompts. Leveraging the latent space of EG3D to impose 3D consistency, the proposed method finds a latent vector in the W+ space that matches the edits specified by the prompt and the identity in the input image. A diffusion model conditioned on the input 2D image and the editing prompt is used to predict this latent vector. Additionally, the paper proposes the following 1) Token position randomization to improve the quality of multi-instruction editing 2) An identity consistency module to improve identity preservation during edits."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) While each individual component of the method isn\u2019t novel, the whole method itself is\n\n2) Qualitative results in both the paper and appendix demonstrate plausible editing, though some identity loss remains\n\n3) Quantitative results demonstrate that the method better preserves the identity across edits. The user study additionally bolsters the main contribution of the paper."
            },
            "weaknesses": {
                "value": "1) The methods section could be written better, with a clear exposition of losses during training and the forward pass during inference. To that end, Fig 2 should be expanded to include both training and inference settings. \n\n2) While the identity consistency is better preserved that prior work, the still remains and identity drift during editing."
            },
            "questions": {
                "value": "1) Instead of an Encoder, if direct optimization of the W+ vector was used (assuming much larger compute), would it preserve the identity better? What if this is only done during inference and not training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728452454,
        "cdate": 1698728452454,
        "tmdate": 1700668761932,
        "mdate": 1700668761932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ehuGGxG3iO",
        "forum": "XIxhINXtQk",
        "replyto": "XIxhINXtQk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
        ],
        "content": {
            "summary": {
                "value": "The proposed approach, InstructPix2NeRF, is an end-to-end model designed for 3D-aware human head editing using a single image and an instructive prompt as inputs. To achieve this results, firstly the authors construct a multimodal 2D human head dataset by leveraging pretrained diffusion models such as e4e and InstructPix2Pix. Secondly, they propose a token position randomization strategy to enhance the model's ability to edit multiple attributes simultaneously. Last, an identity consistency module is incorporated to extract facial identity signals from the input image and guide the editing process. Experimental results demonstrate the effectiveness and superiority of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of the proposed paper can be summarized as:\n1. The authors propose a token randomization strategy that can increase the model's capability for editing multiple attributes simultaneously.\n2. An identity-preserving module is proposed to guide the editing process and present the original identity in the final outcomes.\n3. The proposed method is reported to be time-friendly, producing the results in few seconds."
            },
            "weaknesses": {
                "value": "The weaknesses of the proposed method can be summarized as:\n1. Through the visualization in Figure 1, I find that the original identity and RGB image attributes are not well preserved. Large differences can still be observed in the areas that are not supposed to be edited.\n2. Qualitative comparisons. (1) The proposed method seems to struggle with expression editing, e.g., it fails to make the head smiling; The instruct-pix2pix model doesn't encounter this problem; (2) Regarding the \"bangs\" example, I would prefer the instruct-pix2pix as it contains real bangs; (3) There is no comparisons with Instruct-NeRF2NeRF, AvatarStudio, and HeadSculpt, considering they are more similar works than the compared Talk-to-Edit and img2img; (4) More examples and more scenarios will largely improve the validation. Currently, there are only three types presented.\n3. Quantitative comparisons. (1) The evaluations are not comprehensive. Still, only three examples are presented; (2) More quantitative evaluations, e.g., user studies, would be beneficial."
            },
            "questions": {
                "value": "Besides the weaknesses above, I may have some questions that hope the authors can answer:\n1. There lack the reason for generating and using 20-30 instruction prompts for one single paired image. Will the number of instructions affect the training?\n2. How will the model perform when it deals with novel characters as in the movie, long hair examples, black men/women, and human of different ages?\n3. Will the background affect the edited results? It would be interesting to see the outcomes obtained when editing the same subject against various backgrounds."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770973757,
        "cdate": 1698770973757,
        "tmdate": 1699636188642,
        "mdate": 1699636188642,
        "license": "CC BY 4.0",
        "version": 2
    }
]