[
    {
        "id": "pWBUzJdqaJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_eksP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_eksP"
        ],
        "forum": "vex1yNHNFL",
        "replyto": "vex1yNHNFL",
        "content": {
            "summary": {
                "value": "The authors try to improve the training efficiency of VideoMAE, which adapts the idea of Masked autoencoders (MAE) to video recognition. Specifically, this paper proposes Temporal Progressive Training (TPT), which decomposes the intricate task of long-clip reconstruction into a series of step-by-step sub-tasks, progressively transitioning from short video clips to long video clips. Validation results demonstrate that TPT can improve training efficiency with similar performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Strong performance. Overall, the performance of TPT is good, it can effectively reduce the training costs of VideoMAE by factors of 2 or even 3 without any accuracy drop, which demonstrates the effectiveness of TPT.\n2. Empirical validations are extensive. The authors have evaluated TPT on two large-scale video recognition datasets and multiple backbones, and the results on large benchmarks are convincing."
            },
            "weaknesses": {
                "value": "1. The scope of this paper is not broad enough. The authors only study the training efficiency problem of VideoMAE, which has limited applications in the research community. Although there are papers utilizing the VideoMAE framework, this paper can only benefit those several works in the training speed and cannot generalize to most approaches. Furthermore, a more practical question is whether the training costs really matter for researchers who can afford VideoMAE training.\n2. Contribution is limited. The overall idea of this paper, which gradually increases the sampled frames during training, is the same as curriculum learning and there is no other technical contribution in this work.\n3. Missing comparison with baseline methods. There is a 'Multi-grid cycle' in Fig. 2 and the reviewer wonders whether this is the re-implementation of Multi-grid training [1]. Apart from that, there are efficient training methods in object recognition [2][3] which should also be re-implemented for comparisons since these methods can be directly adapted to video recognition.\n\n[1] Wu C Y, Girshick R, He K, et al. A multigrid method for efficiently training video models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 153-162.  \n[2] Wang Y, Yue Y, Lu R, et al. Efficienttrain: Exploring generalized curriculum learning for training visual backbones[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 5852-5864.  \n[3] Ni Z, Wang Y, Yu J, et al. Deep incubation: Training large models by divide-and-conquering[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 17335-17345."
            },
            "questions": {
                "value": "1. The reviewer wonders whether the idea of gradually increasing the frames during training could work in vanilla supervised learning in video recognition. If this simple idea works, it can be generalized to all the methods, which should be more exciting than reducing the training time of VideoMAE.\n2. There are some typos in this paper: (1) the second paragraph of introduction:  to further its training speed; (2) Sec 3.2.1, paragraph 'Type of sub-task.', last row: \u201dtemporal progressive video reconstruction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Reviewer_eksP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697419663616,
        "cdate": 1697419663616,
        "tmdate": 1699636728582,
        "mdate": 1699636728582,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RQcGGmbQC1",
        "forum": "vex1yNHNFL",
        "replyto": "vex1yNHNFL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_G1mA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_G1mA"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a simple method, Temporal Progressive Training (TPT), to improve the training efficiency of VideoMAE pretraining according to a schedule. TPT reduce the computational cost of VideoMAE by gradually introducing longer video clips during training, which divides the whole training period into several sub-tasks. To identify the optimal training configuration, this work conducts a search for the critical parameters within the schedule, shown in the ablation study. The results of experiments demonstrate that TPT significantly reduces training costs while maintaining the performance of VideoMAE."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper propose an efficient method (TPT) for training VideoMAE by compressing input video data in the temporal dimension. \n2. This paper tries to explore key parameters within the training schedule through extensive experiments, aiming to find the optimal solution.\n3. The proposed method significantly reduces the computational cost while achieving comparable performance with VideoMAE pretraining."
            },
            "weaknesses": {
                "value": "1. The algorithm and experimental design in this paper is similar to Multigrid[1]. However, this paper primarily focuses on the reduction of temporal length. In Multigrid, reducing spatial size is also an efficient means of reducing computational cost. This paper does not include any experimental analysis of such a design to show the advantages of reducing video temporal length compared to spatial dimension reduction in video pretraining.\n2. In Figure 2 of this paper, a comparison is made with the multigrid cycle. However, a clear definition of the multigrid cycle is missing.\n3. In 3.2.1, is the calculation of training resources beta reasonable? In the Video Transformer, the computational complexity of the Multi-Head Self-Attention (MHSA) module is O(n^2), where n is the number of input patches. However, in this paper, the calculation resources increase linearly with the length of the video.\n4. The writing of the method section is not so clear. It lacks the precise definition of variables and formulas to clarify the algorithm proposed in this paper.\n5. In Table 1, the Top-1 accuracy of VideoMAE on K400 is not consistent with its paper reported. The Top-1 accuracy of VideoMAE on K400 is 85.2% rather than 84.7%. Additionally, the results of VideoMAE with ViT-H (86.6% top-1 acc. on K400) is not compared in this paper, which is higher than the results of TPT-MAE with ViT-H. If TPT-MAE is built based on the VideoMAE code repo, the updated baseline results should be compared for fair.\n6. For the training efficiency comparison with baselines, This paper only includes FLOPs comparison, without a comparison of Wall-clock GPU hours.\n7. In the introduction, it is said that the calibration of optimal configurations can be achieved on a relatively small-scale dataset with a limited training budget, and the configuration can be generalized to other datasets and models. However, it seems that this conclusion is not supported by any experimental results.\n\n[1] Wu, Chao-Yuan, et al. \"A multigrid method for efficiently training video models.\" CVPR 2020."
            },
            "questions": {
                "value": "I have listed my major concerns in the weaknesses. I hope the authors can provide more rigorous descriptions of some details in the method and reliable experimental results according to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Reviewer_G1mA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699117727727,
        "cdate": 1699117727727,
        "tmdate": 1699636728478,
        "mdate": 1699636728478,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9BwQSCaDl3",
        "forum": "vex1yNHNFL",
        "replyto": "vex1yNHNFL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_RKG5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_RKG5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Temporal Progressive Training (TPT), which accelerates MAE pre-training for videos by optimizing a mix of frame sampling strategies throughout the training process. Compared to VideoMAE, which uses the same architecture and pretraining target as TPT but uses a single sampling strategy, TPT sets new pareto frontier (i.e., higher accuracy at same training budget or same accuracy at reduced training budget) on two popular benchmarks Kinetics-400 and Something-Something-v2. Moreover, the searched plan yields uniformly superior results compared to VideoMAE across different total budgets / datasets / models, avoiding the time-consuming search process for each new task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The motivation sounds reasonable (it is long known that  and the results look promising on two competitive benchmarks, especially that the searched optimal plan works for both the appearance-focused Kinetics-400 and the motion-focused Something-Something-v2.\n\n* It is interesting that the multi-grid [1] schedule, which is conceptually close to TPT but optimized for supervised training, does not work very well for unsupervised pretraining, potentially making the discovery of this work more valuable.\n\n[1] Chao-Yuan Wu et al., A multigrid method for efficiently training video models, CVPR 2020."
            },
            "weaknesses": {
                "value": "* **Missing important details about the search method.** Most of the details about the search algorithm are missing. As a result, it is very hard to assess the novelty of the method or the quality of the search result given the current status of the paper. The only clues I can find are vague statements such as '*... we apply heuristics to prune many configurations, reducing the search effort ...*' (In section 3.2.1, what exactly is the heuristics?), '*We graually assign these portions to each stage ...*' (In section 3.2.1, how is the exact move in each step decided?). As the *efficient search* is claimed in the conclusion section, it is expected that either some clear reference to previous works or unambiguous description (e.g., pseudo-code) is provided. The paper also does not provide any intermediate or the final search result for reference.\n\n* **Clarity issues.** Other missing information / self-contradiction / errors are listed as follows, with the more concerning points come first.\n  * In Introduction, it is claimed that '*this calibration can be achieved on a relatively small-scale dataset*' which is not well supported by experiments, given that the two datasets SSv2 and K400 are close in size, and it is not mentioned that any subset of SSv2 is used for search.\n  * Some confusing statements about experiments: '*using the 1600-epoch training scheduler, TPT-MAE reduces VideoMAE training epochs by 2x and FLOPs by 54%*' (section 5.1, does the 1600-epoch means 1600 passes through all videos, or 1600 passes through number of frames equivalent to an epoch with max numbers per frames? Is this comparing to VideoMAE-3200ep?), '*TPT-MAE consistently reduces training epochs by 2x with reduced training budgets while maintaining up to 7.5% reduction in training FLOPs*' (section 5.1, how is 2x reduction in epochs equivalent to only 7.5% reduction in FLOPs?), '*assigning more budget to the early stages of training ... led to a reduction in training FLOPs and faster training speed*' (section 5.3 the allocating computation part, according to the caption of Figure 3 the bars are actually showing **the same** budget as VideoMAE-400ep, instead of a reduction in training budget), '*came at the cost of a slight drop in performance. Conversely ... at higher training FLOPs*' (section 5.3 the allocating computation part, the conclusion from Figure 3 seems to be the opposite).\n  * In Abstract and Introduction, it is claimed that 3x reduction of training cost is achieved for Something-Something-v2, but I cannot find this result in the Experiments section. Instead, figure 3(a) clearly shows '**2x** fewer budgets' on SSv2.\n  * Confusing symbols. In section 3.2 paragraph 2, symbols $f_i, p_i, bz_i, lr_i$ come without definition. Symbol $p$ is used as patch size previously in section 3.1.\n  * Related Works paragraph 2: It should be *More recently, VideoMAE (Tong et al., 2022) and ST-MAE (**Feichtenhofer et al., 2022**) generalize the ImageMAE ...*"
            },
            "questions": {
                "value": "* In section 3.2.1, what makes it a good approximaton that computational cost is estimated by number of frames, given that the cost of attentions grow quadratically to the number of tokens? Are the training FLOPS numbers in the Experiments section calculated precisely or with this approximation?\n\n* Since the work focuses on the training cost, it would be helpful if the training cost (e.g., wall time / training FLOPS) for the baselines in Table 1 & 2 can also be included for comparison.\n\n* I'm a bit surprised that the mask ratio, which affects the encoder time as much as number of frames, is not included in the search space. Could some explanations be provided about why it is not searched by design?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6496/Reviewer_RKG5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699307979660,
        "cdate": 1699307979660,
        "tmdate": 1699636728364,
        "mdate": 1699636728364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LtjfQC3vVS",
        "forum": "vex1yNHNFL",
        "replyto": "vex1yNHNFL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_hJSy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6496/Reviewer_hJSy"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a temporally progressive training scheme, i.e.  progressively transitioning from short video clips to long video over the course of MAE pretraining, resulting in lowering of training costs while retaining performance on K-400 and SS-V2. Results are demonstrated across different model scales and architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work revives progressive training for the masked pertaining paradigm and demonstrates it's effectiveness. This a useful contribution as a baseline and also from a practical efficiency perspective and is also simple.\n\n2. Results are demonstrated across different model scales and architectures."
            },
            "weaknesses": {
                "value": "1. A minor weakness is that technical novelty is somewhat limited with much works on progressive training approaches in the past, though not using masked pretraining paradigm.\n\n2. Details of evaluation are not clear, e.g. how many spatial and temporal crops were used during testing? Implementation details could also be improved, there are not enough details for reproducibility.\n\n3. There is no real exploration of the benefit of TPT from being able to train on longer temporal context. K-400 and SS-V2 are both datasets that are easily solved fairly well with short temporal context. The impact of TPT on tasks beyond classification is also not explored.\n\n4. References could be expanded with discussion on other related work that tries to improves training efficiency in orthogonal ways, e.g. VideoMAEv2, Wang et al, 2023."
            },
            "questions": {
                "value": "It would be great if the authors could address the discussed weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6496/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699590015999,
        "cdate": 1699590015999,
        "tmdate": 1699636728267,
        "mdate": 1699636728267,
        "license": "CC BY 4.0",
        "version": 2
    }
]