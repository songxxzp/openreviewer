[
    {
        "id": "6Zmh9wv7Bu",
        "forum": "y1AJUwqsla",
        "replyto": "y1AJUwqsla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_NZzK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_NZzK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces CASTRL, a method to generate representations from high dimensional data, useful for downstream reinforment learning applications. The authors propose to replace traditionally used vision encoder with a modified Video-Swin Transformer that learns from sequences of observations in the past. They then employ an autoregressive model to capture structure in a sequence of observations. Experimental results in reward-free settings under some computational contraints and in limited-data cases show improvements across a variety of Atari environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Notably a lot of current deep reinforcement learning techniques lack some structure in the latent space. This has also motivated other recent work to factorize the state space, for example by further disentangling interaction-relevant from irrelevant factor. In a similar spirit, this paper proposes a (series of) approaches on how unsupervised pretraining can help along this direction.\n\nThe authors propose a new causal masked Video Transformer and a Context GPT that learn representations based on the context of the previous observations. They propose different ways to overcome challenges along the way, as:\n1. how to compensate the limited inductive bias of the Transformer under a low-data regime\n2. what kind of objectives to use in order to train the state representation learning \n3. what objective to use when training in a context aware way\n\nIn the end, the authors show this can be used to train a model that successfully generalizes to different environments, even in a low data  regime. The authors compare against other reward-free methods."
            },
            "weaknesses": {
                "value": "In general, I find that a lot of the proposals in the paper are not sufficiently motivated. I think it will be important for the authors to clarify these, also in the paper. \n\n1. I find the motivation of the causal masking in the Video-Swin Transformer insufficient. It is not clear to me how much of a difference this makes, or even why this is detrimental. People have always used sequences of frames to encode the current state [1]. As far as I can tell your convolutional encoder (top part of FIgure 2) also does not use a causal mask anyway (it is not clear what \"ResNet34 with minor adjustments\" exactly means, unless I missed the explanation somewhere).\n2. The idea of training in an unsupervised way for the state representation learning is quite nice and seems to be working. There is however a long history of related work that is related and is largely ignored in this context. Can the authors comments on what are the differences compared to [2] for example (or other object centric techniques, e.g. [3])?\n3. The authors propose ContextGPT as a way to predict based on a sequence of states. The objectives proposed in Section 4.3 are interesting. I find that the results do little to motivate why the selected objective worked better in the end. \n\n[1] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013).\n\n[2] Micheli, Vincent, Eloi Alonso, and Fran\u00e7ois Fleuret. \"Transformers are sample efficient world models.\" arXiv preprint arXiv:2209.00588 (2022).\n\n[3] Stani\u0107, Aleksandar, et al. \"Learning to generalize with object-centric agents in the open world survival game crafter.\" IEEE Transactions on Games (2023)."
            },
            "questions": {
                "value": "1. In the abstract you mention \"significantly improves\". It is not clear to me to what results exactly this is referring to.\n2. Is a DQN a suitable baseline for Figure 1?\n3. You use a CNN to compensate for the inductive bias of Transformers under limited data. However, it is known that with enough data Transformers become better [1]. How do you expect this tradeoff to evolve with more data?\n4. Section 3.1 is interesting but is quite preliminary and disconnected from the rest of the paper. Does your method somehow guarantee better compactness, sufficiency and generalizability?\n5. The y-axis in Figure 3 is not really informative. Can you perhaps change that to human normalized scores? I also do not think that you can deduce that CSW-MSA is better based on Figure 3.\n6. Is there a difference between limiting the number of batch samples per epoch instead of decreasing the number of epochs? It does not make sense to me.\n7. Can you comment on the model sizes that you are using? Models seems to be quite small.\n8. How do you match the GPT model scale to ensure a fair comparison between CastRL and the baselines?\n9. It would be interesting to provide more insights on Figure 4. For instance, why is the \"Without actions history\" better?\n10. Is it possible to use Atari 100k Dataset for better comparison?\n\n[1] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697802112815,
        "cdate": 1697802112815,
        "tmdate": 1699636422386,
        "mdate": 1699636422386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yxsw978Wlh",
        "forum": "y1AJUwqsla",
        "replyto": "y1AJUwqsla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_JKQw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_JKQw"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces context-aware state representation learning for reinforcement learning. The authors define 'context' as a summary of the state representation and propose a joint learning approach throughout the training process. They also present a variant of the video swin transformer that employs a causal attention mask to facilitate auto-regressive modeling. The proposed method has been tested on a broad spectrum of Atari games, where it significantly enhances downstream learning efficiency."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The exploration of high-level state representation is an intriguing direction, and I believe it holds potential value. However, the current version of the paper requires substantial improvements before it can be deemed acceptable."
            },
            "weaknesses": {
                "value": "1. The empirical results presented are unsatisfactory.\n  + The performance appears to lag significantly behind the state-of-the-art. DQN-based RL methods were able to achieve a return of 20 in Pong as early as 2013 (as per 'Playing Atari with Deep Reinforcement Learning'), yet the authors' plot in Figure 6 only shows a return of ~0.\n  + Figure 3, which compares models trained with and without causal attention, suggests that there is no discernible difference between the proposed method and the baseline in the games Seaquest and Pong. Furthermore, the return in these games shows negligible improvement throughout the training process, which raises questions about the effectiveness of the proposed method.\n2. The paper lacks clarity and completeness, and the writing needs significant improvement.\n+ Figure 2, which is a screenshot rather than a vectorized figure, is blurry and hard to interpret. The caption 'Illustration of the pre-training pipeline' does not provide sufficient clarity. The authors should make the figure more comprehensible.\n+ The main focus of the paper, 'context-aware' state representation learning, is introduced quite late (on page 6), which disrupts the flow of the paper. Moreover, the paragraph describing the method lacks clarity. For instance, the term $Y'$ is not defined anywhere in the paper.\n+ The pseudo code provided is confusing. For example, a 'squeezer network' is mentioned in the code block, but there is no reference to it in the main body of the paper.\n+ The bar plots in Figures 4 and 5 are more suited to presentation slides than a research paper. The authors should present the raw return for each game in a tabular format for better clarity and comprehension."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639319066,
        "cdate": 1698639319066,
        "tmdate": 1699636422323,
        "mdate": 1699636422323,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vSF26t0oFV",
        "forum": "y1AJUwqsla",
        "replyto": "y1AJUwqsla",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_6x1S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4468/Reviewer_6x1S"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a Context-aware State Representation Learning (CaStRL) framework where state representation is encouraged to incorporate context information for better downstream control tasks, together with a tailored attention mechanism for the Video-Swin Transformer for sequence learning. It is shown that the proposed approach with the pretraining + fine-tuning strategy can leverage limited demonstration data to improve imitation learning for the Atari games in a cross-task transfer setup."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The proposed approach is simple yet effective on the shown cross-task transfer setup on 2D game environments\n+ The authors have done extensive ablation studies on the pertaining strategies of their proposed method"
            },
            "weaknesses": {
                "value": "- The evaluation is a bit limited. It would be better to show results with different training and held-out task splits. My current understanding is that the evaluation is performed with a fixed set of 5 held-out tasks. It will provide additional insights into how this approach benefits cross-task transfer when the task similarities are higher or lower.\n- It would make the experimental evidence stronger if the authors could provide results on some other environments newer than the Atari ones (e.g., Procgen).\n- Lack of comparison with other pertaining strategies for the control tasks in the experiment results (or at least in terms of discussion)."
            },
            "questions": {
                "value": "- Aside from the points in \"weaknesses\", please clarify if the DT as the baseline is trained with the same budget as in the fine-tuning stage only of the proposed method. While I understand that it is hard to compare the baseline (w\\o pertaining) fairly with the proposed method (pretraining + fine-tuning), it might be necessary to show the results of the baseline with larger fine-tuning computation budgets (e.g., train longer) to see how large the benefits are from pretraining with the proposed method.\n- Missing some reference to related work. For instance, [1] also adopts the idea of context-aware representation learning for Transformer-based sequence learning in control tasks (using additional prompt tokens with a slightly different setup, though).\n\n[1] Chain-of-thought Predictive Control"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4468/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4468/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4468/Reviewer_6x1S"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699029750348,
        "cdate": 1699029750348,
        "tmdate": 1699636422257,
        "mdate": 1699636422257,
        "license": "CC BY 4.0",
        "version": 2
    }
]