[
    {
        "id": "RJwJ0RbbfF",
        "forum": "eMNN0wIyVw",
        "replyto": "eMNN0wIyVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
        ],
        "content": {
            "summary": {
                "value": "The paper is proposed for the Off-policy evaluation tasks in the RL domain. The proposed method OAT intends to solve the scarce and underrepresented offline trajectory challenges by data augmentation. Specifically, a sub-trajectory mining method and fuse process is designed to find potential exploration space and integrate into existing state-action trajectories seamlessly."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper first proposes to augment the HIS trajectory data for offline RL Off-policy evaluation tasks, which is important for the real-world policy evaluation challenges.\n\n2. The process of detecting the potential sub-sequence is explicitly explained and demonstrated, easy to follow and understand, such as from the discrete representations to determine the support value, and eventually identify the PSTs.\n\n3. Authors applied the VAE-MDP process to generate the potential sub-trajectory, and empirical study shows the overall framework OAT is achieving promising results."
            },
            "weaknesses": {
                "value": "1. The main paper omitted some significant parts and placed them in the appendix, however should be explained in the main context, such as how the latent latent prior is constructed. And how the C different clusters are initially divided when identifying the PST is not clear.\n\n2. Even though the experiment could show that the OPE method performance is improved, the paper is suggested to make a fair analysis of how reliable the reward is from the algorithm augmentation, which, however, is significant for the result value of off-policy methods."
            },
            "questions": {
                "value": "1. Could the authors discuss if it's only applicable for the behavior data to be augmented? Or can we also augment the target policies? \n2. When identifying the PST, at the first step, How are these C different clusters divided? And how is the number of c determined?\n3. As shown in Fig3, It is suggested to introduce how the latent prior is constructed, since it is the key step in constructing the new augmented PST.\n4. What is the complexity of the training VAE-MDP? Is there any analysis conducted to show the relations between training time and TDSS length/Potential trajectory scale?\n5. In Fig 4, it is obvious that the trajectories coverages are different on the left hand and right hand corner on the top of the maze, however if these two corners are not the potential area selected to augment, should they maintain the original distribution to the maximum extent? Apart from this illustration, It is suggested that the paper provides quantitative evaluation of the difference between augmented trajectory and ground truth trajectory."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Healthcare and e-learning data is used in the paper, it is suggested to make sure it follows the privacy policies."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_pod6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698552040696,
        "cdate": 1698552040696,
        "tmdate": 1700502300266,
        "mdate": 1700502300266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ykBVo14DUf",
        "forum": "eMNN0wIyVw",
        "replyto": "eMNN0wIyVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use a specialized form of data-augmentation, specifically to improve the accuracy of off-policy evaluation algorithms. It bases its algorithm on looking at promising sub-trajectories and augmenting the dataset with new samples to improve coverage of the state-action space."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper tackles an important problem of OPE, which receives relatively far less attention than control-based papers.\n2. Adapting data augmentation techniques for the RL setting, and particularly the OPE setting, is a very interesting direction.\n3. The paper applies the proposed idea to more real-life datasets, which is great since lot of OPE work just evaluates ideas on mujoco."
            },
            "weaknesses": {
                "value": "1. It feels like the paper was somewhat rushed. There are some confusing parts/writing errors such as: what is \"human-involved RLs\" (third line in intro), \"while human may behave\" (5th line last para of intro), Figure 2 I believe should be referencing Sections 2.1/2.2/2.3 (not 3.X), \"intrinsic nature that human may follow\" (2nd line Section 2.1).\n2. The notation in Section 2.1 is very difficult to parse. I suspect there is an easier way to explain this. Also Figure 3 is very confusing, and is not explained in the text (there are some references in the appendix, but I think there should be much more explanation of it given the complexity of the diagram).\n3. I think drawing connections to how humans behave (such as in Figure 2 caption) is too strong. It is appropriate for intuition purposes but making general statements on how humans behave seems a bit strong.\n4. Biggest concern. I dont think the paper gives a good intuition for why this works. While coverage is important, it seems like that is insufficient. Another challenge is distribution shift. Prior work [1] has discussed that under extreme distribution shift, OPE methods can diverge. However, this work does not discuss this. Moreover, it seems possible to me that this data augmentation technique could introduce samples that worsen the distribution shift, which could worsen accuracy of OPE methods.\n\n[1] Instabilities of Offline RL with Pre-Trained Neural Representation. Wang et al. 2021."
            },
            "questions": {
                "value": "1. Could one generate better samples by somehow tying in the evaluation policy behavior into the data generation process? It seems like if the goal is to evaluate some policy $\\pi_e$, we should account for what $\\pi_e$ does/samples actions?\n2. How is step 1 in the algorithm actually done? That is, under what basis are states grouped into clusters. Some  representation learning work does this based on behavior similarity [1] etc. How the states are grouped is important for all the remaining steps.\n3. The \"grounding\" process of Eqn 2 is unclear to me. How do you avoid the model from hallucinating and generating samples that cannot occur in the MDP since interaction with the MDP does not happen in the OPE setting?\n\n[1] MICo: Improved representations via sampling-based state similarity for Markov decision processes. Castro et al. 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_3xn9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684070268,
        "cdate": 1698684070268,
        "tmdate": 1700283257690,
        "mdate": 1700283257690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VxxVBzFGoS",
        "forum": "eMNN0wIyVw",
        "replyto": "eMNN0wIyVw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel approach to augment offline datasets for off-policy evaluation. This is achieved by introducing a three-step process (i) select relevant sub-trajectories in the dataset, (ii) use a VAE-based architecture to generate new trajectories, and (iii) add these trajectories back to the dataset.\n\nEmpirically, the authors show that the proposed method outperforms other data-augmentation methods on a diverse set of problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I am not an expert in the field of data-augmentation for RL, but I enjoyed the thought process behind the development of the framework: (i) select a criterion for what makes a specific region of the state-action space interesting for data augmentation, and (ii) use temporal generative models to sample new sub-trajectories. \n\nFigures are generally informative and overall the paper is well written."
            },
            "weaknesses": {
                "value": "In my opinion, the main weaknesses of this work lie in (i) the unsupported justifications of the results, (ii) the lack of ablations to validate the proposed innovations, and (iii) the relatively narrow-scoped experiments."
            },
            "questions": {
                "value": "(i) The unsupported justifications of the results\nIn more than one occasion, the authors (rightly) discuss very specific reasons that could confirm/justify the observed results. However, I feel like in most cases in this work, the justifications are not supported by data. I feel this is better explained through an example. The authors say: \"In contrast, the historical trajectories induced from simulations tend to result in better coverage over the state-action space in general, and the augmentation methods that do not consider the Markovian setting may generate trajectories that could be less meaningful to the OPE methods, making them less effective.\" What do the authors mean by less meaningful? And do they believe that this (i.e., non-Markovian augmentation are worse in scenarios with better state coverage) can be confirmed more generally?\n\nIn my opinion, the authors use phrases like \"we conjecture\", \"a possible reason\" without strictly backing up the claims with evidence (which would in turn greatly improve the quality of the paper)\n\n(ii) The lack of ablations to validate the proposed innovations:\nThe authors compare against a wide set of benchmarks, although, as I'm not an expert in OPE, it is unclear to me whether these are explicitly tailored for the OPE problem or not. Moreover, since the proposed framework is a composed of multiple smaller contributions (discretization, VAE architecture, etc.), the authors should make sure to isolate each of these contributions individually and support the claims with evidence and experiments.\n\n(iii) The relatively narrow-scoped experiments:\nDid the authors consider using this approach within an RL context? How would this perform?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5773/Reviewer_WbKh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818142687,
        "cdate": 1698818142687,
        "tmdate": 1700501931798,
        "mdate": 1700501931798,
        "license": "CC BY 4.0",
        "version": 2
    }
]