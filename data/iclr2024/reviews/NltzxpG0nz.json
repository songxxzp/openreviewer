[
    {
        "id": "a5dk2oQNEP",
        "forum": "NltzxpG0nz",
        "replyto": "NltzxpG0nz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new end-to-end architecture for multimodal agent interacting with a simulated environment. Specifically, it proposes to fuse a visual encoder with a pre-trained large language model for visual and textual generation. The model is trained on a curated dataset of Minecraft data, and is able to achieve convincing results. Overall, the work attempts to enhance interaction the capability of text-only LLM interaction for embodied agents and the results are promising for future work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed architecture is intuitively sound. It seems to be the natural way to merge a visual encoder, an LLM, and an image generator in an end-to-end pipeline.\n\nThe paper contains some smart ways to curate a large dataset.\n\nThe author suggests to publish codes and projects for reproducibility, which is nice."
            },
            "weaknesses": {
                "value": "The tasks are still fairly constraint in that everything is image-text or text-text pairs for evaluation. The task does not really test on the agent\u2019s ability to navigate or perform tasks in Minecraft environment directly.\n\n(FK-QA) Evaluation is still focusing on in-domain knowledge about Minecraft-related questions. There seems to have no procedures or details specifying the methods to ensure data leakage. Especially, given that the LLM is selected as a LLAMA2, and directly evaluate on data procured from the internet. There could be an additional benchmark on using LLAMA to fine-tune on the text-only datasets and to evaluate on the same sets of text-only evaluation set.\n\nTable-3 has Llama 2-7b model outperforming Llama 2-13b model. What would be a good explanation for this behaviour?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698382962674,
        "cdate": 1698382962674,
        "tmdate": 1699636761867,
        "mdate": 1699636761867,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MOEW0IjEJ5",
        "forum": "NltzxpG0nz",
        "replyto": "NltzxpG0nz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed Steve-eye, a multi-modal LLM for visual-based embodied game playing in the simulated environment. Especially, \nas previous game agent mainly obtain environment informations directly from game engine, Steve-eye want to intergrate perception system into game playing agent. To achieve this objective, this paper construct a multi-modal instruction dataset for instruction-tuning. And, the paper construct some benchmark to quantitatively evaluate the different capability of the Steve-eye."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper dedicate to a interesting and valuable problem that make game agent more human-like (use visual to sense the world).\n2. Steve-eye divided the visual perception for several ends: generation, QA, caption, skill planning, which suit the minecraft game well.\n3. Generate a large multi-modal instruction dataset for this task."
            },
            "weaknesses": {
                "value": "1. The most important question for Steve-eye is the evaluation. Steve-eye is not evaluated on some long-horizon objectives like Voyager and GITM. With generated multi-modal instruction dataset, it is so intuitive that the model can perform well on the ENV-VC, FK-QA, SPP benchmarks as the instructions are generated for these tasks. The skill plan, visual caption or QA are actually studied on real-world images by some VLLM such as LLaVA. There is no reason to go back and evaluate VLLM on simulation environment just for such tasks.\nI think the main reason for the community to carry on research on game engine is to evaluate much more complex tasks that are not hard to attempt in real world. In this regard, I think the Steve-eye should have some evaluation manners as Voyager and GITM, to show what happens when we replace the environment information of game engine to visual perception.\n\n2. Is the model trained on such instruction dataset generalizable to new game? Such as GTAV?"
            },
            "questions": {
                "value": "No anymore questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736286382,
        "cdate": 1698736286382,
        "tmdate": 1699636761758,
        "mdate": 1699636761758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NAXEaZubQZ",
        "forum": "NltzxpG0nz",
        "replyto": "NltzxpG0nz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
        ],
        "content": {
            "summary": {
                "value": "The \"Steve-Eye\" paper introduces embodied game-playing agents with visual perception capabilities and the potential to interact intelligently in open-world environments. By integrating large language models with a visual encoder, the paper enables agents to represent their surroundings and comprehend critical knowledge in the environment in a more intuitive manner. Utilizing Minecraft as a primary evaluation platform, the model demonstrates its effectiveness in a complex and dynamic setting, showcasing its versatility and potential as a general-purpose assistant."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**1. Strong Motivation:** The paper identifies and addresses a critical challenge in the field of embodied agents, highlighting the limitations of text-only interactions and demonstrating the necessity of incorporating visual perception for a more intuitive and effective user experience.\n\n**2. Extensive Task Definitions:** The authors have meticulously defined a variety of tasks to evaluate the agent\u2019s performance, spanning across environmental visual captioning, foundational knowledge question answering, and skill prediction and planning. This comprehensive approach ensures a thorough assessment of the agent\u2019s capabilities, showcasing its versatility and adeptness in handling different aspects of open-world interaction.\n\n**3. Large and Rich Dataset:** The paper introduces an extensive open-world instruction dataset, specifically curated to train the Steve-Eye model. This dataset can be a good contribution to the field for future instruction tuning of large multimodal models."
            },
            "weaknesses": {
                "value": "**1. In-Depth Analysis on Skill Planning Required:** The paper presents Steve-Eye, an innovative integration of visual perception with Large Language Models (LLMs) for embodied agents. However, there\u2019s a discernible lack of comprehensive discussion and analytical depth in the aspect of skill planning, particularly when compared to other game-playing agents such as \u201cMineAgent.\u201d While Steve-Eye showcases commendable performance, a more exhaustive exploration of its strategic capabilities, especially in comparison to MineAgent, is crucial. This detailed analysis could unveil the underlying reasons behind Steve-Eye\u2019s superior performance, providing readers with substantial insights and a clearer understanding of its capabilities in the embodied task of planning.\n\nOn a related note, the paper discusses two other tasks: Environmental Visual Captioning (ENV-VC) and Foundational Knowledge Question Answering (FK-QA). While these tasks are undoubtedly interesting and add value to the paper, their direct connection to the agent\u2019s in-game execution and decision-making processes is not explicitly clear. Strengthening this connection and elaborating on how these tasks intricately weave into the agent\u2019s planning and action sequences would significantly enhance the paper\u2019s overall contribution to the field.\n\n**2. Baseline Selection Could Be Improved:** The inclusion of \u201cgpt-assistant\u201d as a baseline in the performance evaluation, particularly noted in Table 5, brings about certain ambiguities. The choice of baselines is critical, and in this context, one might wonder if \u201cVoyager\u201d or other models specifically tailored for the Minecraft environment would serve as more apt comparisons. By opting for baselines that are more closely aligned with the Minecraft gaming milieu, the paper could present a more robust and convincing argument for Steve-Eye\u2019s effectiveness.\n\n**3. Clarification on Image Output Utilization Needed:** As depicted in Figure 4 and discussed in various sections of the paper, Steve-Eye has the capability to generate image outputs. However, the practical application and utility of these image outputs in the context of the tasks at hand appear somewhat nebulous. Providing readers with a clearer exposition on how these image outputs can be leveraged for specific tasks would enhance the paper\u2019s clarity and comprehensibility.\n\nMinor:\n**4. Figure 1 Requires Better Visual Clarity:** My initial interaction with Figure 1 was marked by confusion, necessitating several revisits to fully grasp its intended message. A visual element such as a horizontal separator line could significantly improve the figure\u2019s readability, distinctly delineating the unfavorable responses from the preferable ones. Currently, the figure could be misinterpreted as a continuous chat, leading to potential misunderstanding. Implementing this small yet impactful change would streamline the reader\u2019s experience, fostering a quicker and clearer comprehension of the depicted scenarios.\n\nMinor: \n5. Figure 7/8/12 in the appendix are not in the correct order."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838303805,
        "cdate": 1698838303805,
        "tmdate": 1699636761641,
        "mdate": 1699636761641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y4PQZ4t41f",
        "forum": "NltzxpG0nz",
        "replyto": "NltzxpG0nz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
        ],
        "content": {
            "summary": {
                "value": "The proposed method seeks to overcome \"text-only\" problems by combining an LLM with a visual encoder, which processes both visual and textual inputs and produces multimodal responses. This model is trained on a new dataset proposed by the authors, which contains multimodal perception, a foundational knowledge base, and skill prediction & planning. The trained model can perform multimodal I/O, benefitting from the two-stage training and fine-tuning.\n\nThe effectiveness of Steve-Eye is demonstrated through three proposed new benchmarks: environmental visual captioning (ENV-VC), foundational knowledge question answering (FK-QA), and skill prediction and planning (SPP).  The results show that finetuned Steve-Eye surpasses existing LLM in Minecraft scenarios, especially in multimodal Q&A."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Included in the summary and questions section."
            },
            "weaknesses": {
                "value": "Included in the questions section."
            },
            "questions": {
                "value": "I'm still confused about the Multimodal Perception Instructions section. Are the responses for the instructions drawn from those processed dense captions? How exactly is the extraction done? Also, it would be clearer for readers if example instructions, responses, and snapshots were provided together, as the current format requires flipping back and forth between the text and the appendix to understand these benchmarks.\n\nFor foundational knowledge question answering, evaluation relies on ChatGPT, which may be hard to reproduce and could incur high costs. Moreover, I doubt about the evaluation performance. Given that Steve-Eye-13b is fine-tuned on a vast amount of domain data, it should theoretically perform much better than Llama. Could the limited performance boost be due to the evaluation method or insufficient training on Wikipedia data?\n\nRegarding skill prediction, it seems that rule-based analysis is necessary since simply combining two random frames likely won't give enough information for skill assessment, even with human expertise. Yet, this kind of pretraining appears to somewhat improve performance, according to results from the authors. However, I believe a different approach, like training the model to predict the next move from videos and instructions, might be more beneficial, which requires a stronger capability to process sequences of frames or videos.\n\nIn summary, the proposed Steve model shows promise in multimodal Q&A for Minecraft, but it's not yet a fully embodied agent. Future work could improve its in-context Q&A and control capabilities, which would allow for understanding temporal context and environmental feedback-based control.\n\nThough there's potential to refine the evaluations and finetuning tasks, the authors do contribute massive datasets and pretrained multimodal large models to the community, which they plan to make publicly available. Hence, I am giving a positive review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841576943,
        "cdate": 1698841576943,
        "tmdate": 1699636761521,
        "mdate": 1699636761521,
        "license": "CC BY 4.0",
        "version": 2
    }
]