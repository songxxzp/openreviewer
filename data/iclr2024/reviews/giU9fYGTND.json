[
    {
        "id": "nLMiwLd4ie",
        "forum": "giU9fYGTND",
        "replyto": "giU9fYGTND",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_LhTp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_LhTp"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FedImpro to mitigate the client drift problem in FL. It provides a new way to correct gradients and updates. The method mainly has 2 parts. The first is the study of the generalization contribution of each client, then proposes to leverage the same or similar conditional distributions for local training. The second is to propose decoupling a deep neural network into a low-level model (features extraction) and a high-level model (classifier network)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Even though decoupling a neural network into a feature extractor network and a classifier network is not novel, the paper proposed to combine the decoupling method with the features distribution estimation method with privacy protection is quite novel for mitigating client drift. Also, the sampling part with the synthetic features to increase the distribution similarity is quite interesting too."
            },
            "weaknesses": {
                "value": "- All the distributions in the theoretical analysis considered conditional distribution conditioned on the label y, and then the paper said that ' it is straightforward to make all client models trained on similar distributions to obtain higher generalization performance'. But, for a dataset such as CIFAR10, when we partition it among clients, the non-IID is introduced by the label imbalanced across clients, which means that the conditional distribution on the label is the same. However, we would still experience client drift in this case. I think more explanation/analysis is required on this aspect. Also, since the analysis is on the conditional distribution, the experiments should reflect that, by using a dataset with the natural partition, such as FEMNIST. \n\n- since the feature distribution depends on the client selected in each round. Unlike the standard FedAvg training, the number of clients per round only impacts the convergence rate. Here for the FedImprov method, it will also impact the distribution estimation. Therefore, an ablation study analyzing the number of clients on the feature distribution estimation is lacking.\n\nMinor:\n- a small typo in Sec 5.1, M=10 should be cross-silo and M=100 should be cross-device."
            },
            "questions": {
                "value": "- do you use h_hat to update the low-level model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Reviewer_LhTp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772675042,
        "cdate": 1698772675042,
        "tmdate": 1699636446122,
        "mdate": 1699636446122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KK9LD4PAuK",
        "forum": "giU9fYGTND",
        "replyto": "giU9fYGTND",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_9583"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_9583"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FedImpro to construct similar features across clients for better federated training. The paper theoretically shows that the \"generalization contribution\" of local training is bounded by the conditional Wasserstein distances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of generalization contribution in FL sounds novel. \n2. Experimental performances of FedImpro look superior."
            },
            "weaknesses": {
                "value": "The idea of having a lower-level and a higher-level neural network in FL is not new, i.e. the feature extraction network idea. I don't see many comparisons to these previous work in the experimental section."
            },
            "questions": {
                "value": "I don't really follow how \"generalization contribution is bounded by conditional Wasserstein distance\" leads to the proposed FedImpro algorithm. I think the connection and logic flow can be made more clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Reviewer_9583"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806279765,
        "cdate": 1698806279765,
        "tmdate": 1700533901055,
        "mdate": 1700533901055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mZaVJChRqf",
        "forum": "giU9fYGTND",
        "replyto": "giU9fYGTND",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_AtvR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_AtvR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FedImpro, a framework which constructs similar conditional distributions for local training. And FedImpro decouples the model into high-level and low-level parts and trains the high-level part on reconstructed feature distributions, causing promoted generalization contribution and alleviated gradient dissimilarity of FL."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper has a good level of writing and it is easy to follow. The idea is easy to follow and understand.\n2. This paper combine split training with feature sharing to improve the generalization of the model."
            },
            "weaknesses": {
                "value": "1. I notice that the author ignore a very related and state-of-art baesline FedDyn [1], could the author conduct comparion experiments with FedDyn?\n\n2. The timecomsuming for training the model increases for FedImpro. Could the author list the cpu-time cost comparion experiments to reach the target accuracy?\n\n\n[1] Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" *arXiv preprint arXiv:2111.04263* (2021)."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Reviewer_AtvR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823808054,
        "cdate": 1698823808054,
        "tmdate": 1700534821497,
        "mdate": 1700534821497,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0kHHzMiTe3",
        "forum": "giU9fYGTND",
        "replyto": "giU9fYGTND",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_Kzyf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4656/Reviewer_Kzyf"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the issue of client drift arising in heterogeneous federated settings. As shown by recent literature, local models drifting away from the convergence points of the global model lead to slower and unstable convergence. This work first shows how the generalization contribution is bounded by the conditional Wasserstein distance between clients\u2019 local data distributions. To reduce the client drift, FedImpro decouples local training into two stages: the lower part of the model is trained on the local data, while the higher part is also trained on shared reconstructed features. Empirical results on FL benchmark datasets show the efficacy of FedImpro in terms of final performance, convergence speed, and reduced weights divergence."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses a very relevant issue for the FL community, i.e. limiting the negative effects of the client drift in heterogeneous settings.\n- The paper is well written and easy to follow\n- Very detailed discussion of related works\n- Theoretical claims supported by proofs\n- Extensive empirical analysis. FedImpro is compared with some state-of-the-art approaches in terms of final performance, convergence speed, weight divergence. Interesting ablation study on the depth of gradient decoupling. Results compared under different levels of client participation, local training epochs, data heterogeneity, and settings (cross-device vs cross-silo FL).\n- Limitations regarding communication and privacy concerns are explicitly addressed. It is shown how FedImpro does not lead to privacy leakage in the gradient inversion attack.\n- All details for reproducing the experiments are provided."
            },
            "weaknesses": {
                "value": "- My main concern regards the feasibility of deploying FedImpro in real-world contexts. FedImpro notably increases both the number of communications between clients and server, and the message size. The paper points out how the global distribution can be estimated using methods which impact the communication network less, but that does not eliminate the need for additional communication. \n- Some relevant related works are not discussed: ETF [1], SphereFed [2], FedSpeed [3]. \n- FedImpro is compared with relatively old baselines (published 3 years ago). More recent and relevant baselines are for instance FedDyn [4], FedSpeed, ETF. Also, I believe CCVR and SphereFed are highly correlated with FedImpro and it is relevant to understand how they behave w.r.t. FedImpro in terms of client drift reduction, final performance, communication costs, even if they are only applied at the end of training.\n- From Fig. 2, the actual impact of FedImpro on reducing the weight divergence appears very limited.\n\n**Minor weaknesses and typos:**\n- Lack of NLP datasets, e.g. Shakespeare and StackOverflow, or large scale datasets, e.g. Landmarks-User-160k.\n- Typo (?) page 7: I believe the explanation of cross-device or cross-silo setting should be switched, i.e. M=100 for cross-device FL and M=10 for cross-silo FL.\n\n**References**\n[1] Li, Zexi, et al. \"No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier.\" ICCV (2023).\n[2] Dong, Xin, et al. \"Spherefed: Hyperspherical federated learning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n[3] Sun, Yan, et al. \"Fedspeed: Larger local interval, less communication round, and higher generalization accuracy.\" ICLR (2023).\n[4] Acar, Durmus Alp Emre, et al. \"Federated learning based on dynamic regularization.\" ICLR (2021)."
            },
            "questions": {
                "value": "1. How does FedImpro compare with more recent FL methods addressing data heterogeneity? Please refer to Weaknesses for examples on related works.\n2. How much bigger is FedImpro's impact on the communication both in terms of number of communications between clients and server, and message size w.r.t. FedAvg and the other introduced FL baselines?\n3. How does FedImpro compare with CCVR and SphereFed w.r.t. centralized performances?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4656/Reviewer_Kzyf",
                    "ICLR.cc/2024/Conference/Submission4656/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4656/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834134119,
        "cdate": 1698834134119,
        "tmdate": 1700515660929,
        "mdate": 1700515660929,
        "license": "CC BY 4.0",
        "version": 2
    }
]