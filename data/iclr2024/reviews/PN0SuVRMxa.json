[
    {
        "id": "rzyoADUAUn",
        "forum": "PN0SuVRMxa",
        "replyto": "PN0SuVRMxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel method called Structured Packing for Long Context (SPLICE) aimed at improving the context utilization in long-context Large Language Models (LCLMs). The authors identify that the lack of long-range semantic dependencies in typical training data hinders the effective utilization of context in LCLMs. To address this, they propose incorporating related documents more frequently into training inputs. By using BM25 to collate the most mutually relevant documents into a single training context, the authors demonstrate that SPLICE can enhance model performance across various tasks and can be used to train large models to better utilize long contexts. The method was validated by training a large 3B model and showed improvements in perplexity and better long-context performance on a benchmark key-value retrieval task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces an innovative method to improve the context utilization of LCLMs. The SPLICE approach, which involves structuring training data using BM25, is interesting and it can be applied to any textual data, making it more generally applicable.\nThe paper demonstrates that the application of SPLICE results in improvements in perplexity across various tasks."
            },
            "weaknesses": {
                "value": "1.  Using Lexical matching methods to concatenate the documents into a longer one is a very engineering technique and it is a straightforward solution to construct longer samples. \n\n2. The experimental results are almost based on PPL, lacking experiments on real-world tasks. More experiments on benchmarks such as zeroScrolls[1] or L-Eval[2] to validate their models are needed. More extensive testing across a broader range of tasks and datasets would provide a more comprehensive evaluation of the method.\n\n3. Presently, the prevalent strategies for training long context models involve the use of extensive conversations and literary works. A comparative analysis of SPLICE with these existing methodologies is thus a necessary step.\n\n[1] ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding, 2023\n[2] L-Eval: Instituting Standardized Evaluation for Long Context Language Models, 2023"
            },
            "questions": {
                "value": "1. How does the choice of the BM25 method for document retrieval affect the model's performance? Would other document retrieval methods yield similar results?\n\n2. How can we SPLICE  on very large pertaining corpus which usually has more than 400B tokens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753204892,
        "cdate": 1698753204892,
        "tmdate": 1699636622952,
        "mdate": 1699636622952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r2nwC2olxZ",
        "forum": "PN0SuVRMxa",
        "replyto": "PN0SuVRMxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_wDkv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_wDkv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes structured packing for long context (SPLiCE) that constructs long context training examples by retrieving relevant documents using BM25. After experiments on a small language model with different datasets and configurations, SPLiCE is applied to large-scale language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main idea of constructing better training examples makes sense. SPLiCE is not too complicated and does not require expensive overhead or external models by relying on BM25."
            },
            "weaknesses": {
                "value": "Considering the additionally introduced complexity (though the SPLiCE algorithm is simple), the performance improvement looks very marginal, especially for large-scale models. \nOnly the part of the training is replaced with SPLiCE from the random baseline. That might be one of the reasons for marginal improvement, but it also implies that SpLiCE is not a standalone solution that can completely replace the existing training algorithm. \n\nLanguage modeling perplexity is the main evaluation metric. Comparing performance on other NLP downstream tasks that require long context modeling might be better to evaluate the effectiveness of the proposed method."
            },
            "questions": {
                "value": "I raised several concerns about why SPLiCE is not sufficient (or at least not fully validated) as it is. Could you address them?\n\nI guess the number of neighbors for each document is skewed, meaning that there exists hub documents. In that case, although a root document is randomly sampled from the document corpus, the retrieved documents are not uniformly distributed in terms of their likelihood and order. Couldn't this be a problem that may result in an imbalance in training? \n\nPacked documents are unnatural and different from contiguous documents. Is there any way to alleviate this issue?\n\nAs expected, using related documents in a long context is better than the random baseline. However, any design choices (top-k, order, or even REPO vs. SPLiCE) give clear differences.\nIn particular, top-1 is the best, and in that case, BFS is the same as DFS. \n\nWhy is Table 1 required? Table 2 fully covers Table 1.\n\nThe structure of the paper can be improved. For example, it is awkward that Section 4 also includes experiments while the title of Section 3 is experiments. Also, multiple Figures and Tables can be merged to spare some space for more extensive experiments or discussion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698901806798,
        "cdate": 1698901806798,
        "tmdate": 1699636622861,
        "mdate": 1699636622861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FeVHkpST3N",
        "forum": "PN0SuVRMxa",
        "replyto": "PN0SuVRMxa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes SPLICE, a similarity-based approach of grouping documents into pretraining examples to training better long context language models. For each example, the method starts with a single document and uses a BM25 retriever to include more relevant documents in the example in a BFS fashion. \n\nWhen applied to training a 270M model, the method outperforms the random baseline on both text and code perplexity. The model is also on par with the REPO method which relies on knowledge of the corpus structure. When used to train a 3B model, the method also outperforms the baseline on both perplexity and the key-value retrieval task. Ablation studies are included to analyze the impact of hyperparameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method is simple yet effective and can be easily applied to different scenarios.\n- Reproducibility: The authors attach the source code, which is great. Please also release the code if the paper is accepted.\n- Clarity: the paper is well written and easy to understand.\n- Significance: the significance of the paper is okay."
            },
            "weaknesses": {
                "value": "- The effectiveness of the method is only validated on language modeling and the key-value retrieval task. This does not guarantee the resultant model is stronger on realistic use cases. To test the usefulness of SPLICE, I would highly recommend comparing the models on more challenging and realistic long-context downstream tasks such as Quality and Squality.\n- It would be great if the method is tested on more settings: Use a neural retriever in addition to BM25, go beyond 3B and 32K, etc.\n- Novelty: The main idea is quite similar to many existing methods like the ones discussed in the paper (e.g. retro). However, I don't think the paper should be rejected only because of this."
            },
            "questions": {
                "value": "- 3.4 \u201cOn the code evaluations, the improvements are small.\u201d - Why say so? The average improvement on code datasets is 0.0625 and the improvement on arxiv is 0.07. The improvements seem to be similar.\n- 4.2: \u201cThe perplexity difference is larger for tokens further in the document\u201d - I might misunderstand something, but it seems the improvement is also large at the start? (Figure 3)\n- Typo: 3.2 \u201cMoreover, even thorough this method uses only the code data\u201d\n- In the abstract \u201cOur results indicate that SPLICE enhances model performance across various task\u201d - The context of this sentence is the 270M model. It is in fact only tested on one task: language modeling (though there are different datasets). You might want to rephrase to reduce confusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699093257321,
        "cdate": 1699093257321,
        "tmdate": 1699636622767,
        "mdate": 1699636622767,
        "license": "CC BY 4.0",
        "version": 2
    }
]