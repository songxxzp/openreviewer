[
    {
        "id": "jxczUdGaEQ",
        "forum": "FPpLTTvzR0",
        "replyto": "FPpLTTvzR0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_P55P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_P55P"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method to disentangle causal and non causal features in order to protect a gnn from adversarial attacks"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Good presentation \n- Great experimentation\n- The addition of the ablation study is very welcome \n- Interesting premise and motivation \n- Interesting approach"
            },
            "weaknesses": {
                "value": "- Minor language issues throughout the paper \n- It is not clear why the end result of the encoder H would be a causal feature. I can see how the learned representation could be a feature set that is invariant to the features the attack but making a causal claim is not completely justified. \n- it is unclear how the method generalises to multiple unseen attacks"
            },
            "questions": {
                "value": "- why is the features learned causal and not just invariant to the attack ? \n- how does the method generalise to types of attacks not seen in training ?\n\n\nEDIT AFTER REBUTTAL\n\nupdated score from 5 to 6"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Reviewer_P55P"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5591/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698228593587,
        "cdate": 1698228593587,
        "tmdate": 1700218139236,
        "mdate": 1700218139236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7phOQBxX5A",
        "forum": "FPpLTTvzR0",
        "replyto": "FPpLTTvzR0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_jN54"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_jN54"
        ],
        "content": {
            "summary": {
                "value": "This paper uses a new causal defense perspective to resist adversarial attacks by learning powerful and invariant predictable causal features, and proposes the Invariant causal defense method against adversarial attacks (IDEA). Experiments have proven that this method is effective against various attack methods and has excellent performance and strong generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The article contains a more complete proof process and theoretical basis.\n\n2. The article learns powerful and immutable causal features to deal with adversarial attacks from a relatively novel causal defense perspective.\n\n3. The experiment proves the effectiveness and generalization of the method proposed in the paper."
            },
            "weaknesses": {
                "value": "1. Judging from the results shown in Table 1 and Table 2, compared with other denoising methods, the performance of this method has indeed been greatly improved. However, the table only shows the excellence of this method when facing one of poisoning attacks or evasion attacks, and the results for other attack methods are not shown.\n\n2. There are many symbols listed in the article, and it seems unclear when mixed together.\n\n3.The drawing of the overall block diagram of the method is relatively rough.\n\n\n============================================================\nAfter rebuttal\n\nThe authors solve most of my concerns. Thus, I am willing to increase the rating score from 6 to 8."
            },
            "questions": {
                "value": "1. Does this method still have such obvious advantages in the face of the other attack methods mentioned in the article?\n\n2. There are also some purification methods that seem to be able to be extended to graph purification. Whether they will also encounter the limitations mentioned by the author, I hope the author can discuss or compare this. Such as the following methods:\n\n[1] Shi C, Holtz C, Mishne G. Online adversarial purification based on self-supervision[J]. arXiv preprint arXiv:2101.09387, 2021.\n\n[2] Liao F, Liang M, Dong Y, et al. Defense against adversarial attacks using high-level representation guided denoiser[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 1778-1787.\n\n[3] Zhou D, Wang N, Peng C, et al. Removing adversarial noise in class activation feature space[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 7878-7887.\n\n[4] Naseer M, Khan S, Hayat M, et al. A self-supervised approach for adversarial robustness[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 262-271."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Reviewer_jN54"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5591/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698679418226,
        "cdate": 1698679418226,
        "tmdate": 1700229107141,
        "mdate": 1700229107141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8t6mRCTupB",
        "forum": "FPpLTTvzR0",
        "replyto": "FPpLTTvzR0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_6toz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_6toz"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes a new framework for adversarial robustness in GNNs. The primary subject of interest is the learning of causal features defending against evasion and poisoning attacks. The empirical results are further supported by theoretical analyses with provable defense guarantees."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall the experimental methodology is sound with complete theoretical derivations."
            },
            "weaknesses": {
                "value": "The link is expired (https://anonymous.4open.science/r/IDEA_repo-666B), which made further investigation on code artifact and validating empirical results hard. Therefore, the claims made in the paper cannot be carefully checked. Additionally, the claim that causality directly contributes to improved defense performance is weak, as opposed to algorithmic superiority."
            },
            "questions": {
                "value": "What's the significance of Figure 1 (b)? Aren't there inherent learning capability differences between different GNN architectures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5591/Reviewer_6toz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5591/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804228469,
        "cdate": 1698804228469,
        "tmdate": 1700552099786,
        "mdate": 1700552099786,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rU3u2FqZbK",
        "forum": "FPpLTTvzR0",
        "replyto": "FPpLTTvzR0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_knCN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5591/Reviewer_knCN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a casual defense to improve the graph adversarial robustness. Specifically, it first defines a causal graph that some casual feature would have strong predictability for the label and maintains invariant predictability across attack domains so that perturbing the features adversarially won't induce a successful attack. It then defines a objective by defining different mutual information to learn the casual features. The experiments show the proposed method could achieve a significant improvement over current defenses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a new perspective on the casual inference to defend against graph adversarial attacks.\n2. The proposed method shows significant improvement on both evasion and poisoning settings."
            },
            "weaknesses": {
                "value": "1. Since I am not expert in causal inference, it is unclear to me how the initial casual graph is defined. And I am not clear whether the graph is based on author's assumption or derived automatically. If it is the former case, the truthfulness of the provided causal graph is debatable. \n2. The threat model is actually unclear. The proposed method actually built a detector neural network in the defense. The attacks tested seems have no knowledge about the detector network. Therefore, the improvement might be brought by the attacker's incapability acquiring enough model information. Also,   it is unfair to compare with other proposed methods since they are only modifying the provided model or aggression rule. A adaptive attack  or white box attack should assume the attacker has already known the added detection neural networks.\n3. There are some notations and figures problems that causes the paper not easy to follow. \nNode j in Figure 2 should be Node k.  Z in Section 3.2.1 is not defined.  (.)_\\cN is never defined. The overall framework only shows in Figure 3 without any introduction. Empty graph in Figure 5."
            },
            "questions": {
                "value": "1. Why does causal feature would only connect with label and input data? Is it defined or derived just based on some assumption the paper made or is there anyway to automatic define the graph?\n2. If the attacker knew the added neural network, would the proposed method still achieve a similar improvement in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5591/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699370225744,
        "cdate": 1699370225744,
        "tmdate": 1699636575867,
        "mdate": 1699636575867,
        "license": "CC BY 4.0",
        "version": 2
    }
]