[
    {
        "id": "SFM4lBhT4b",
        "forum": "kklwv4c4dI",
        "replyto": "kklwv4c4dI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes FeDualEx a federated primal-dual algorithm for solving distributed composite saddle point problems. The authors consider a homogeneous setting and provide convergence guarantees achieved by FeDualEx when the objective function is convex-concave. The authors also evaluate the proposed algorithm experimentally on synthetic and real datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall. the paper is well written with the ideas clearly explained. The proposed algorithm is well-motivated and backed by strong theoretical guarantees. The experiments show the effectiveness of the proposed approach."
            },
            "weaknesses": {
                "value": "- The authors have missed an important reference [R1] which considers a nonconvex composite optimization and develops Douglas-Rachford Splitting Algorithms for solving the problem. \n\n- The authors should also discuss [R2] and [R3] which consider a non-convex composite problem but in a decentralized setting and without local updates. Also, in contrast to the duality-based approach taken by the authors, the works [R2] and [R3] propose primal algorithms that directly update the parameters using proximal stochastic gradient descent. The dual approach proposed by the authors is justified by the \"curse of primal averaging\". A question I have is why the algorithms [R2] and [R3] seem to work even though they are primal algorithms.\n\n- Why are the guarantees presented in the paper independent of the number of clients? The effect of the number of clients should be discussed after the main results. Importantly, does the proposed algorithm achieve linear speed-up with the number of clients in the network?\n\n- In the initial part of the paper the authors refer to the distance-generating function to be strictly convex but later it is assumed to be strongly convex. It is advisable to call it strongly convex from the beginning. \n\n- Define $h_1$, $h_2$ in Definition 3. \n\n- After Definition 3, the authors mention that the previous approaches that add the composite term to the Bregman\ndivergence may not work for dual extrapolation as certain parts of the analysis break down. Can the authors be more specific about what they mean here?\n\n[R1] Dinh et al., FedDR \u2013 Randomized Douglas-Rachford Splitting Algorithms for Nonconvex Federated Composite Optimization, 2021(https://arxiv.org/pdf/2103.03452.pdf)\n\n[R2] Yan et al., Compressed Decentralized Proximal Stochastic Gradient Method for Nonconvex Composite Problems with Heterogeneous Data, 2023 (https://arxiv.org/pdf/2302.14252.pdf)\n\n[R3] Xiao et al., A One-Sample Decentralized Proximal Algorithm for Non-Convex Stochastic Composite Optimization, 2023 (https://arxiv.org/pdf/2302.09766.pdf)"
            },
            "questions": {
                "value": "See the weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_geTM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818436021,
        "cdate": 1698818436021,
        "tmdate": 1699636195724,
        "mdate": 1699636195724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X5gtResnBe",
        "forum": "kklwv4c4dI",
        "replyto": "kklwv4c4dI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_rzX1"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors study composite saddle-point problems in a Federated learning setup. They propose distributed gradient methods with local updates, which they call Federated Dual Extrapolation. They provide convergence analysis and communication complexity in the homogeneous case."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors propose a new method, for which they provide convergence analysis. This method has their own interest."
            },
            "weaknesses": {
                "value": "Table 1 presents the previous and current results strangely:\n1) First of all, to compare the obtained complexity for the proposed method with the previous result in a strongly-convex concave case, it should be used standard regularization trick.\n2) From my point of you, when complexity contains several terms, each of them should be added. \n\nAbout Table 2, The authors claim that \"The sequential version of FeDualEx leads to the stochastic dual extrapolation for CO and yields, to our knowledge, the first convergence rate for the stochastic optimization of composite SPP in non-Euclidean settings .\" It is not true, there is a wide field related to operator splitting in deterministic and stochastic cases. Look at this paper please https://epubs.siam.org/doi/epdf/10.1137/20M1381678. \n\nAlso, compared to the previous works, the authors use bounded stochastic gradient assumption and homogeneity of data. In many federated learning papers, those assumptions are avoided. Despite that the authors write \"Assumption e is a standard assumption\", it would be better to provide analysis without it to have more generality. \n\nIn Theorem 1, and Theorem 2, the final result contains mistakes in complexity, because some of them were done in the proof. \nThe first mistake is made in theorem 3 and repeats in the main theorem. Please look at the last inequality on page 40:\nTo make $3\\eta^2\\beta^2 -1 \\leq 0$, the stepsize should be chosen in the following way: $\\eta \\leq \\frac{1}{\\sqrt{3}\\beta}$. This will change the complexity of the methods. The same was done in the proof of Theorems 1, and 2. Please see Lemma 3, 17.\n\nThe second mistake is made in the proof of Lemma 13, in the last two inequalities, where should be $\\dots\\sqrt{2V^l_z(\\cdot)} \\leq \\dots \\sqrt{B}$. This thing also will change the final complexity.\n\nThe appendix is hard to read in terms of the order of Lemmas. I think it would be better if the numeration of Lemmas had a strict order (for example, after Lemma 5 lemma 6 follows.)\n\nOther things dealing with weaknesses, please, see in questions."
            },
            "questions": {
                "value": "1. In section 4, it is unclear how you define $\\ell_{r,k}$. Could you add an exact expression for it from the appendix to the main part? \n\nSmall typos:\n1. on the bottom of page 5 in the second argmin the bracket is missed. \n2. In definition 4, $t\\eta$ is missed in the formula for subgradient."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827526904,
        "cdate": 1698827526904,
        "tmdate": 1699636195649,
        "mdate": 1699636195649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HkI88EUg2U",
        "forum": "kklwv4c4dI",
        "replyto": "kklwv4c4dI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an Algorithm FeDualEx for solving composite saddle point problems under distributed settings. The proposed algorithm is inspired from the dual extrapolation algorithm while using a proximal operator which they define using the generalized Bregman divergence defined for saddle functions. They analyze this algorithm under homogeneous settings and derive its convergence rate for the duality gap. They also study the special cases when the number of clients equals 1, where the convergence rate of FeDualEx matches the existing rates known in the literature. The study also demonstrates that solving using the dual extrapolation has advantages of learning better sparse solutions than solving the primal."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper studies federated learning of composite saddle point problems, for which there does not seem to be much existing work. The proposed convergence rates. \n\n(Novelty) The paper proposes a new bregman divergence for saddle functions and its associated proximal operator, which are used in the dual extrapolation steps. \n\n(Clarity) The main results are presented well and contrasted to the related ones. The experimental results illustrate the benefit of solving using the federated dual extrapolation over methods such as Federated Mirror Prox. The comparison to the sequential algorithms also help to position the contributions in relation to the existing work."
            },
            "weaknesses": {
                "value": "The algorithm is similar that of Federated Dual Averaging (Yuan et.al.) while incorporating the dual extrapolation strategy over the newly defined Bregmen divergence and the proximal operators. The challenges associated with adapting the above strategy over FeDualAvg doesn't seem to be conveyed well in the paper. \n\nFrom the motivations perspective, some examples of practical setups which required distributed learning of saddle point formulations would be useful in appreciating the contributions better."
            },
            "questions": {
                "value": "Questions / Comments\nIs it possible to have a similar algorithm only using the generalized bregman divergence on x ? \nSome discussion on the relation between the variables z=(x,y) and \u03c2 would help since the former already includes a primal and dual pair. \n\nTo improve the clarity, one may include convexity assumptions of the functions involved while the main problem is defined in (1)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2585/Reviewer_jzbD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2585/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700986110530,
        "cdate": 1700986110530,
        "tmdate": 1700986110530,
        "mdate": 1700986110530,
        "license": "CC BY 4.0",
        "version": 2
    }
]