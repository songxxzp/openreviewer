[
    {
        "id": "ybVEZ7aWZe",
        "forum": "ZVi81SH1Ob",
        "replyto": "ZVi81SH1Ob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_dsyA"
        ],
        "content": {
            "summary": {
                "value": "Recent study by De et al. (2022) reports that large-scale representation learning via pre-training on a gigantic dataset significantly enhances differentially private learning in downstream tasks. While the exact behaviors of NoisySGD on these problems remain intractable to analyze\ntheoretically, the authors consider an idealized setting of a layer-peeled model for representation learning by neural collapse.\n\nThe writing is good and the results seem interesting, which attracts me to check their proof. The proofs are very simple. $M_k,k=1,\\cdots,K$ form an ETF frame, which separate categories very well, the zero initialization makes $f_0(M_k)=0$ for all $k=1,\\cdots,K$, which is very weird. The one-step NoisyGD seems not useful at all.\n\n I am not sure your results are for NoisySGD or NoisyGD. In introduction section, your statements are all about NoisySGD, but the other parts for NoisyGD. Moreover, there is no definition about NoisySGD at all in the whole paper."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The presentation is good."
            },
            "weaknesses": {
                "value": "The results are not meaningful."
            },
            "questions": {
                "value": "How about $f_W(x)=Wx+b$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734522527,
        "cdate": 1698734522527,
        "tmdate": 1699637065017,
        "mdate": 1699637065017,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hjuwIiHE3K",
        "forum": "ZVi81SH1Ob",
        "replyto": "ZVi81SH1Ob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_KJYE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_KJYE"
        ],
        "content": {
            "summary": {
                "value": "This paper studies theoretical analysis for differentially private fine-tuning under neural collapse. Specifically, this paper shows that if the neural collapse is assumed, and we only fine-tune last layer, the accuracy bound is indepedent of dimension and only related to privacy parameter. If the neural collapse is not perfect on private dataset, this paper also shows that perturbation on the features, class imbalance would require the accuracy to be depedent on the dimension. This paper also propose data normalization and PCA to mitigate this non-robustness issue."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides first theoretical understanding of DP fine-tuning reduces the error rate on down-streaming task. The setting of neural collapse is interesting and may be enlightening for potential future research."
            },
            "weaknesses": {
                "value": "1. Typos: In theorem 2, is it $\\gamma$ accuracy or $1-\\gamma$ accuracy?\n2. All of the proofs only analyze one-step Noisy-GD algorithm under a very strong neural collapse assumption. This setting is too simple and might not be reflecting what is happening in De at al (2022). If perfect neural collapse holds, then there is no need for further training. For example,  in theorem 2, you can set the clipping threshold G to be very small (near zero) to get near zero error rate. This suggests that you don't have to train on the private data if the neural collapse is assumed. This bound might not be very useful.\n3. The proposed tricks are not demonstrated empirically on real datasets.\n4. The proof is simple and the technical contribution is limited."
            },
            "questions": {
                "value": "1. Is there any empirical improvement by using the proposed data normalization and PCA tricks? I am curious because DP-PCA would also needs privacy-utility trade-off that needs to be accounted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699585265163,
        "cdate": 1699585265163,
        "tmdate": 1699637064881,
        "mdate": 1699637064881,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7jPah5ha11",
        "forum": "ZVi81SH1Ob",
        "replyto": "ZVi81SH1Ob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_MQQr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8521/Reviewer_MQQr"
        ],
        "content": {
            "summary": {
                "value": "The authors use neural collapse theory to analyze the behavior of last-layer fine-tuning with DP. They show that dimension independence emerges in a certain sense under perfect neural collapse and that small perturbations in the train and test data can disturb this independence. They show that data normalization and dimensions reduction can recover the dimension independence in the face of such perturbations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The general phenomenon explored in this paper (i.e. the empirical success of DP deep fine-tuning in high dimensions) is very interesting and timely.\n2. The results are presented with a high degree of technical precision and fluency.\n3. The result of Theorem 2 seems surprising and interesting to me, although I don't yet have a strong intuitive understanding of the proof."
            },
            "weaknesses": {
                "value": "1. **It may be difficult for some readers to understand:** Given the topic, I imagine many readers will be familiar with differential privacy but less familiar with neural collapse. As a result, the third paragraph of the introduction and the corresponding figure 1 will be meaningless to them without more explanation. Some of the introductory material is present in section 2.2, but it is a bit technical and not well-suited to newcomers. I would recommend giving a high level explanation of neural collapse in the introduction to help readers."
            },
            "questions": {
                "value": "1. Introduction, paragraph 1, last line: do you mean \"no-privacy utility tradeoff\"\n2. Bottom of page 7, Sigma is missing a backslash.\n3. The dimension independence of Theorem 5 requires $\\beta_0$ to scale with $p$, but the non-robustness results in 3.2 and 3.3 would also become dimension independent if $\\beta$ chosen to vary with $p$ in a similar way. Because of this, it's not clear what we are gaining from dimension reduction in section 4.2.\n2. It's not clear to me how this analysis of neural collapse applies to full fine tuning. The success of DP full fine tuning is most surprising because of the many total parameters of the networks (not just in the last layer). Neural collapse may explain some of the dynamics of last layer fine-tuning as presented in this paper, but clearly something interesting must be happening at intermediate layers in the full fine-tuning setting. I think it would be very helpful to mention whether there is any way that these results might shed light on the dynamics of intermediate layers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8521/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699604527884,
        "cdate": 1699604527884,
        "tmdate": 1699637064784,
        "mdate": 1699637064784,
        "license": "CC BY 4.0",
        "version": 2
    }
]