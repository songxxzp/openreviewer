[
    {
        "id": "pKsEW8Yja4",
        "forum": "yacRhge4zQ",
        "replyto": "yacRhge4zQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_wL1U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_wL1U"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the trade-off in trustworth ML, specifically that between fairness, privacy and model utility, by formulating it as a multi-agent game (called SpecGame) among two regulators and a model builder. The authors design a method called ParetoPlay to search for an equilibrium on the Pareto frontier. Experiments show that the designed method can be instantiated to two existing trustworth ML algorithms and demonstrate the trade-off between fairness vs. privacy vs. utility achievable by the designed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem of trade-off among different important criteria in trustworth ML is important.\n\n- The formulation of the interaction as a multi-agent game is intuitive.\n\n- The theoretical analysis and empirical results demonstrate the interaction among the agents in the game, which can be useful in understanding the (possible fundamental) limitations when designing new trustworth ML algorithms."
            },
            "weaknesses": {
                "value": "- A key assumption is the common-knowledge of a pre-calculated PF among the \nconsidered critera, specifically privacy, fairness and model utility. This can be difficult to satisfy in practice.\n\n- There are several (simplifying) assumptions made (which can take away the pratical feasibility of the work). For two examples,\n    - > We assume that regulators are able to give penalties for violations of their respective objective which they formulate as a utility (or value) function.\n\n    - > We assume the regulators hold necessary information about the task at hand in the form of a Pareto Frontier (PF) which they use to choose fairness and privacy requirements that taken together with the resulting accuracy loss are Pareto efficient:\n\n- The writing can be improved (for details, see the questions below)."
            },
            "questions": {
                "value": "1. In Section 1\n    > This is because nowadays ML models are trained, maintained, and audited by separate entities\u2014each of which may pursue their own objectives.\n\n    Are there references or real-world use-cases where this is true or implemented?\n\n2. In Section 1,\n    > ... that assumes shared knowledge of a pre-calculated PF between agent objectives.\n\n    Can this PF be realized in practice? i.e., how to accurately obtain it? and if only a somewhat inaccurate one can be obtained, what are its implications?\n    \n    \n3. What is $i\\in I$ in Definition 1?\n\n4. In Section 3,\n\n    > In this work, we do not consider a competition between regulatory bodies since both are assumed to be governmental agencies.\n\n    Even though the regulatory bodies are not set out to compete with each other, the inherent tension between the objectives can lead to competitive strategy profiles and actions, right?\n\n    In that case, what is the significant distinction of \"not considering a competition between the regulatory bodies\"?\n\n\n5. What is $\\\\{c_{i}^{(t)} \\\\}^t$ in the overall discounted loss in Section 3.2 ?\n\n6. In Section 4,\n    > However, ...the highly non-convex nature of agent losses in $\\texttt{SpecGame}$\n\n    How is the non-convexity addressed?\n\n7. In Section 4,\n\n    > for $t>1$, we assume a mapping from the penalty values in S to trustworthy parameters values $s_{reg} = (\\gamma, \\epsilon)$\n\n    (How) can this assumption be satisfied in practice ?\n\n8. What do the colors represent in Figure 4a?\n\n9. Are the experimental results averaged over multiple trials? If so, is there an analysis of the variation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Reviewer_wL1U"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698394752557,
        "cdate": 1698394752557,
        "tmdate": 1700547015848,
        "mdate": 1700547015848,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RAgT6VcsEb",
        "forum": "yacRhge4zQ",
        "replyto": "yacRhge4zQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_kbV4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_kbV4"
        ],
        "content": {
            "summary": {
                "value": "The paper is concerned with trustworthy ML. The main contribution is to model the setting as a game (SpecGame) between the model builder and the regulator who is interested in fairness and privacy. An algorithm ParetorPlay is introduced and it is shown that the agents remain on the Pareto frontier."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "-The idea seems interesting and novel and one can think of it as modeling a wide set of problems.\n\n-The paper is also concerned with an important problem (trustworthy ML)."
            },
            "weaknesses": {
                "value": "1-I think the main contribution of the paper is to model the dynamic interaction between the model builder and the regulator. Accordingly, it is more reasonable to think of only fairness or privacy or to possibly even abstract/lump both issues into one. I don't see how having these two considerations has added to the model. One can also consider the safety of the model or its robustness to adversarial manipulations as part of the regulator's concern for example. \n\n2-Why is the paper searching for Pareto Optimality instead of a Nash Equilibrium? Both agents (builder or regulator) are interested in their utility and as a result would deviate to increase it which is what would be captured by a Nash Equilibrium\n\n3-The section \u201cMaking a uniform strategy space.\u201d on page 6 is very unclear. What does \"consistent\" mean? It seems to suggest that the strategies are fixed. Further, it seems that the horizon is n. If that is the case the why does the utility in section 3.2 sum to infinity? \n\n4-Why is a correlated equilibrium and correlation device well-motivated in this setting? Also the paper mentions that \u201c This is known as a correlation device. If playing according to the signal is a best response for every player, we can recover a correlated equilibrium (CE). We leave the theoretical proof of this conjecture to future work.\u201d But doesn\u2019t Theorem 1 prove that you have a correlated equilibrium so is it a conjecture?"
            },
            "questions": {
                "value": "Please see Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849647586,
        "cdate": 1698849647586,
        "tmdate": 1699637187872,
        "mdate": 1699637187872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5Yq0QwTe4B",
        "forum": "yacRhge4zQ",
        "replyto": "yacRhge4zQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_SMAr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9431/Reviewer_SMAr"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the problem of multi-agent and multi-objective machine learning (ML) regulation games where there exist separate regulators enforcing privacy and fairness constraints on the learning model. Prior work in trustworthy ML often implicitly assumes that a single entity is in charge of implementing these different objectives, which is not realized in practice. The authors instead proposed SpecGame, a general framework for ML regulation games between three agents: model builder, fairness regulator, and privacy regulator. Since the agents' privacy loss is difficult to estimate in post-processing, the authors also proposed ParetoPlay, i.e. using a pre-calculated Pareto frontier as common knowledge among all agents, to simulate the interactions between agents in a SpecGame and recover equilibria points. Finally, the authors provided experimental results to show the suboptimality of the single-agent model in trustworthy ML and answer questions on how the regulators can achieve the desired equilibrium by changing their incentives after the game has converged."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths: \n- The proposed model of multi-agent multi-objective is novel and interesting to study. \n\n- The proposed scenario of SpecGame is well-defined and makes sense."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- The assumption of a pre-calculated Pareto frontier as common knowledge does not have theoretical proofs and the discussion around the empirical evaluation in Appendix J is lacking. \n\n- Throughout the main body, the authors sometimes refer to notations that were not defined previously. For example, in Section 3.2, notation c_i^{(t)} and L_i^{(t)} are the first time the \"(t)\" superscript is used. In Equation 4, the notation \\nabla_s is used without a definition. \n\n- The discussion of the experiments in Section 5 is confusing. At the end of Section 1, the author claimed that the experiments would highlight the suboptimality of studying trustworthy ML in a single-agent framework. However, in Section 5, the first research question shows that multi-agent setup leads to sub-optimalities. \n\n- Figure 1 does not have a legend. Overall, most figures are hard to read and the captions do not provide sufficient description of the experiments. \n\n- Minor typos: \\ell_build(w) instead of \\ell_b(w) on page 5 under Equation 3."
            },
            "questions": {
                "value": "- Can the authors clarify the assumption of a pre-calculated Pareto frontier as common knowledge?\n- Can the authors clarify the discrepancy between RQ1 and the contribution claimed at the end of Section 1? \n- Can the authors provide a more detailed description of the experiments in Section 5, as well as the reasoning for choosing the hyperparameters described in Appendix I? Particularly, the step size discount factor and the loss function weightings \\lambda_fair and \\lambda_priv?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9431/Reviewer_SMAr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9431/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699307582520,
        "cdate": 1699307582520,
        "tmdate": 1699637187770,
        "mdate": 1699637187770,
        "license": "CC BY 4.0",
        "version": 2
    }
]