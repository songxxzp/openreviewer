[
    {
        "id": "9LEYZysj0C",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_Jf74"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_Jf74"
        ],
        "forum": "9XdLlbxZCC",
        "replyto": "9XdLlbxZCC",
        "content": {
            "summary": {
                "value": "This paper proposes to learn to estimate the optical flow between two images in addition to the existing VICReg SSL for content learning. The authors argue that learning low-level pixel information from motion estimation can benefit downstream tasks such as semantic segmentation. Results show an improvement in downstream tasks when the encoder is jointly trained on SS and optical flow estimation task (MC-JEPA)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Results show a clear improvement in optical flow estimation (M-JEPA to MC-JEPA); this shows that to estimate optical flow correctly, content understanding is also important. The opposite is also true when comparing VICReg to MC-JEPA.\n- The method is easy to train and does not require both datasets to be similar or come from the same distribution."
            },
            "weaknesses": {
                "value": "- The biggest weaknesses are in the experimental setup and missing comparisons with SoTA. \n- It is very hard to validate the performance of the proposed model when using a different backbone from the comparison methods. The only valid comparison is VICReg Vs. MC-JEPA because they both use CNX-T.\n- Many of the recent SSL models use ViT-S or ViT-B for performance evaluation. Results with ViT backbone would make the proposed model comparable to many other methods. At least, include ViT-S in Table 4 to compare with DINO and iBOT.\n- Some methods, such as iBOT (ICLR22), are not reported. iBOT\u2019s performance on ADE20k with ViT-S is 45.4, which outperforms MC-JEPA. Also if we compare against ViT-B of DINO or IBOT, MC-JEPA is outperformed on many benchmarks. It makes the paper stronger more relevant backbones and methods are included.\n- Qualitative results of optical flow only shown against one relatively weaker method (as shown in Table 1). Results from SMURF and UPFlow could also be shown to visualize fail cases and limitations of this model.\n- This paper does not show any results of scaling up the model or training data; all backbones reported in table 4 are lightweight. I imagine that training two tasks like this would require a bigger backbone to handle multitasking. A study showing how performance scales with the model size would be useful."
            },
            "questions": {
                "value": "- The authors chose to do training on both tasks on very different datasets. I wonder if training this model on the same video dataset for both tasks would result in features robust enough for video downstream tasks, such as action recognition.\n- In section 4.3 (backbone), the authors mention that PWC is not adapted to learn good content features, which explains the low performance in Table 4. However, the method MCRW (in Table 1) uses a PWC backbone and performs much higher than the results of MC-JEPA with a PWC backbone in Table 4. Any reasons behind this significant difference in performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3341/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697321189682,
        "cdate": 1697321189682,
        "tmdate": 1699636283497,
        "mdate": 1699636283497,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nsNvSyzedn",
        "forum": "9XdLlbxZCC",
        "replyto": "9XdLlbxZCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_arP2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_arP2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn a joint embedding for the self-supervision of both motion estimation and image content. They evaluate the proposed model's performance on metrics for optical flow estimation and image/video segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written.\n\n2. The overall comparison results on two tasks demonstrate the model's superiority especially on segmentation."
            },
            "weaknesses": {
                "value": "Currently, the provided analysis is not sufficient to demonstrate their contribution by integrating the self-supervised learning for optical flow and content understanding. \n\n1. The proposed method performs on par or slightly worse against other flow estimation methods on Sintel benchmark and Kitti, which can not demonstrate the benefit of the proposed ''Joint-Embedding Predictive Architecture''.  The authors don't give convincing analysis for this issue.\n\n2. Unfair comparison. The proposed method brings more training data compared with the other content methods. It can be seen from Table 2 that the performance in segmentation is likely to degradation as the decrease of motion datasets.\n\n3. Missing analysis. From Table 4, we can see that the model's performance is quite sensitive to the used backbone (more than 10% between Rsenet50 and ConvNext. However, the authors didn't give explanation. Besides, the proposed model uses six loss terms in total for both flow estimation and content learning. I am wondering how to decide the trade-offs and if they would affect the final results."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3341/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698472738544,
        "cdate": 1698472738544,
        "tmdate": 1699636283426,
        "mdate": 1699636283426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G2cZlXiHgq",
        "forum": "9XdLlbxZCC",
        "replyto": "9XdLlbxZCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_RRax"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_RRax"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a self-supervised approach, MC-JEPA, which uses a shared encoder to learn optical flow and content features. The proposed approach achieves good performance on optical flow estimation and images and videos segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a valuable and novel insight: self-supervised learning of optical flow estimation and content features can be effectively unified within a single architecture under a multi-task setting. MC-JEPA not only learns motion features from multiple video datasets but also learns content features from large-scale image datasets. This dual-focus approach shows excellent performance across multiple evaluation benchmarks, demonstrating strong generalization capabilities that can be applied to a variety of downstream tasks, from motion prediction to content understanding."
            },
            "weaknesses": {
                "value": "The writing of the paper needs to further improve. The captions of the figures and tables are too detailed. It is better to condence the captions."
            },
            "questions": {
                "value": "1. In multi-task learning, how are the weights for different loss functions chosen and adjusted? Will adjusting coefficients for the six different loss functions for each task introduce a significant tuning cost during training? Would it be possible to draw comparisons between this tuning approach and other methodologies in the field of multi-task learning?\n\n2.Although some experiments have already been conducted to demonstrate improvements in Optical Flow Estimation, Image Segmentation, and Video Segmentation, additional experiments could be included to further validate the effectiveness of the proposed methods. For example, more experiments on video object segmentation (YoutubeVOS), video semantic segmentation, video panoptic segmentation(Cityscapes-VPS ,VIPSeg ).\n\n3. What's the motivation of the multi-task learning ? Learning optical flow seems to be a low level visual understanding, why it benefits semantic segmentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3341/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3341/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3341/Reviewer_RRax"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3341/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771057289,
        "cdate": 1698771057289,
        "tmdate": 1699636283311,
        "mdate": 1699636283311,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KYunPYI9Xz",
        "forum": "9XdLlbxZCC",
        "replyto": "9XdLlbxZCC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_q59r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3341/Reviewer_q59r"
        ],
        "content": {
            "summary": {
                "value": "The current focus in self-supervised learning of visual representations has been on capturing content features, which do not include object motion or location information. On the other hand, optical flow estimation is a task that does not require understanding the content of the images. In this work, they introduce MC-JEPA, a joint-embedding predictive architecture, and self-supervised learning approach that combines both objectives to learn the optical flow and content features together. This paper shows that these two objectives benefit from each other, resulting in content features that incorporate motion information. The proposed approach achieves comparable performance with existing unsupervised optical flow benchmarks and common self-supervised learning methods on downstream tasks like semantic segmentation of images and videos."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed MC-JEPA method combines self-supervised optical flow estimation and content feature learning in a multi-task setup with a shared encoder. This approach offers several advantages:\n\n1. Joint learning of motion and content features: By integrating optical flow estimation with self-supervised learning, MC-JEPA enables the simultaneous learning of motion information and content features within a single encoder. This allows for the incorporation of motion information into content representations.\n\n2. Improved optical flow estimation: The MC-JEPA method enhances the estimated optical flow by combining it with the self-supervised learning objective. By jointly optimizing these two objectives, the quality of the estimated flow is improved, leading to more accurate motion representations.\n\n3. Transferability to downstream tasks: The content features learned by MC-JEPA transfer well to various downstream tasks, such as optical flow benchmarks and image/video segmentation. This demonstrates the effectiveness of the joint learning approach in producing features that are useful for a wide range of visual tasks.\n\n4. Multi-task learning and joint-embedding architecture: MC-JEPA leverages the benefits of multi-task learning and joint-embedding architectures. By learning multiple tasks simultaneously and using a shared encoder, the method provides a more reliable and generalizable approach to building visual representations.\n\nIn summary, MC-JEPA combines the advantages of self-supervised learning, optical flow estimation, multi-task learning, and joint-embedding architecture, resulting in improved motion features and content representations that benefit various visual tasks."
            },
            "weaknesses": {
                "value": "While the proposed method incorporates some novel self-supervised approaches for image and video learning, the overall architectural novelty is not clearly explained. It is important to clarify the specific innovation of the proposed method in integrating existing self-supervised techniques.\n\nAdditionally, as a multitask method, it is crucial to explain how the different tasks are adjusted and why some task coefficients are the same. Providing a clear explanation of the task adjustment strategy and the rationale behind the equal coefficients for certain tasks is necessary.\n\n\nRegarding the architectural novelty of the proposed method, it is essential to clarify how it integrates existing self-supervised techniques in a unique way. While the specific details of the architecture are not mentioned in the given text, it is important to provide a clear description of how the joint-embedding predictive architecture (MC-JEPA) differs from existing architectures. This could include details about the specific network components, the fusion mechanism for combining optical flow estimation and content feature learning, or any other architectural innovations that distinguish MC-JEPA from previous approaches.\n\nRegarding the multitasking aspect of the method, it is crucial to explain how the different tasks are adjusted and why some task coefficients are the same. This could involve discussing the overall objective function used for multitask learning and how the weights or coefficients for individual tasks are determined. Additionally, providing a rationale for why certain tasks have equal coefficients could be based on their relative importance or the desired balance between different objectives. It is important to clearly explain these aspects to provide a comprehensive understanding of the method.\n\nIn summary, to enhance the clarity and completeness of the proposed method, it is necessary to provide a more detailed explanation of the architectural novelty and the rationale behind the task adjustment strategy, including the equal coefficients for certain tasks."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3341/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803321946,
        "cdate": 1698803321946,
        "tmdate": 1699636283226,
        "mdate": 1699636283226,
        "license": "CC BY 4.0",
        "version": 2
    }
]