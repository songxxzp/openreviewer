[
    {
        "id": "sdbFghGBqS",
        "forum": "KFjCFxiGk4",
        "replyto": "KFjCFxiGk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a way to utilize a theorem prover with a large language model to produce answers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The high level idea seems good (but the details I'm no so clear about). The results are very good."
            },
            "weaknesses": {
                "value": "The main problem with this that the details of the architecture isn't clear. Here is what I understand: The LLM gets language (The \"Context\" in the figures). The LLM generates a \"formalized context\" that can be used as the input to Peano. Peano implements a guide function, and outputs a set of valid one-step conclusions. This is input back into the LLM by biasing the logits (whatever that means), then presumably the LLM does sometime else to generate the next formalized contexts to do the next steps and so on. At some stage this halts and one of them produces an answer. (Does the LLM also outputs natural language?)\n[Alternatively: Using figure 2 as an example, The LLM takes the contact and produces the formalized context and the formalized goal. Peano takes these and outputs a proof (Is this the \"reasoning\" in that figure?). That would seem to make the most sense. But that can't be correct as the external tool only answers \"what inferences can be made next?\".]"
            },
            "questions": {
                "value": "What is the interface between the LLM and Peano? (What is the input of each and what is the output? Does Peano have any knowledge built-in (e.g., axioms for deontic logic)?\n\nWhat is an example application beyond artificial logic puzzles? (The legal reasoning is a good example, but it only used the theorem prover for bootstrapping.)\n\nWhat does \"bias the logits\" mean? How is it done? How does the theorem prover determine how to bias them?\n\n(My rating assumes there is a satisfactory answer to these questions. I will downgrade my rating if I still cannot understand the interface after the rebuttal period.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_GDuS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697671495265,
        "cdate": 1697671495265,
        "tmdate": 1700857310272,
        "mdate": 1700857310272,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gAcUfM7Fcm",
        "forum": "KFjCFxiGk4",
        "replyto": "KFjCFxiGk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_cFZp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_cFZp"
        ],
        "content": {
            "summary": {
                "value": "This paper studies logical reasoning in natural language with LLMs. Whereas a number of existing approaches may arrive at the correct answer with a wrong reasoning chain, this work proposes an approach to guide the LLM generations using a logic solver that constrains the space of possible generations to those that are logically valid. With this approach, while there can still be errors in the translation stage (i.e. the stage where the LLM translates from natural language to logical form), the logical conclusions made on those translations are valid. Experimental results are shown on multiple datasets including ProofWriter, PrOntoQA, Syllogism Validity, LegalBench, and ReClor."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Logical reasoning (or more generally multi-hop reaosning) in natural language with LLMs is an important area of research.\n- Showing results both for prompting and finetuning.\n- The writing was mostly clear and easy to follow.\n- The reported improvements for ReClor could be quite encouraging."
            },
            "weaknesses": {
                "value": "- Most of the experiments are done on the ProofWriter and the PrOntoQA datasets. Both these datasets have been constructed by turning logical theories into natural language using very simple templates. This is especially true for the PrOntoQA dataset where each sentence is of the format \"X is Y\" which is simply equivalent to (X, is, Y) in the triple notation. For this reason, while these datasets are appropriate benchmarks for measuring the general reasoning capacity of off-the-shelve LLMs, I do not think they are good benchmarks for the model proposed in this paper (translating these datasets back into their logical form is just too easy for nowadays LLMs). For this reason, while those results could be good sanity checks, I don't think they truly represent the merit of the proposed approach. They highly overestimate the performance we can expect on real tasks but highly underestimating how difficult it is to translate an actual natural language passage into logical form. \n- The failure example highlighted in Page 6 (translating to (sees A B) in one place and (see A B) in another) makes me worry about the applicability of the proposed approach to reasoning problems beyond synthetic tasks such as ProofWriter and PrOntoQA. It also makes me  think that BoardgameQA might have been a slightly better dataset to use. While it has also been generated synthetically by converting logical theories into textual format, the missing knowledge piece of it makes it better resemble real-world problems, and makes for a good test to see the extent of the \"see\" vs\" sees\" problem in the proposed approach. \n- While the results on the ReClor dataset are quite encouraging, I find them quite surprising as well for multiple reasons. 1- Given that the model is finetuned only on 120 samples, and considering the size of the models used, I would expect that the models should just overfit to those examples without any task transfer. 2- If I understand correctly, the finetuning is not on a mixture of the original data and the 120 data points, so I would expect that the model's general task solving ability should go down. 3- The ProofWriter and PrOntoQA datasets only require deductively applying the modus ponens rule, whereas the ReClor dataset requires more complicated rules and reasoning. For these reasons, I found the improvements a bit surprising and the provided explanation does not give much insights."
            },
            "questions": {
                "value": "- On which categories from Table 2 of the BoardgameQA paper do you expect your approach to fail/succeed? And why?\n- Given that the results in Table 6 are tested in a zero-shot setting, how do you extract the final answer? Is it possible that after finetuning on the 120 examples, the model mainly just learns to produce outputs in the specified format making it easier to extract the final answer (and hence higher predictive accuracy)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700157106,
        "cdate": 1698700157106,
        "tmdate": 1699636395101,
        "mdate": 1699636395101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aHWXWTMbEK",
        "forum": "KFjCFxiGk4",
        "replyto": "KFjCFxiGk4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel tool, termed \"guide,\" designed to ensure that language models engage in sound step-by-step reasoning. As a primary illustration, LogicGuide utilizes general logical reasoning systems to guide models towards producing logically consistent explanations. Experimental results indicate that LogicGuide enhances the performance of language models, in terms of reasoning accuracy, reducing content effects, self-learning and generalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper introduces a novel logical guidance framework designed to aid LLMs in performing logical inference. The method employs the most general form of deductive reasoning, making it versatile across a range of reasoning scenarios.\n2. Experiments across multiple datasets validate that LogicGuide enhances the performance of language models. The paper also provides specific examples demonstrating its efficacy in mitigating the impact of unwarranted prior assumptions and performing self-learning."
            },
            "weaknesses": {
                "value": "1. The proposed method necessitates a reliance on a complex formalization process during training and inference.\n\n2. The scenarios considered in the paper seem a bit limited. Despite experimenting on diverse datasets, the nature of problems within them appear quite similar. In more generalized contexts, it might be challenging to formalize and identify corresponding actions, such as `objects`, `relations`, etc.\n\n3. The paper's primary contribution, namely, how to harness logic to ensure output consistency, seems to overlap with prior work on the Peano theorem and the constrained Semantic Decoding algorithm, which weakens the novelty of the current research.\n\n4. It seems the proposed idea is similar to the idea in Logic-LM. The authors did not discuss their differences.\n\n   Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning https://arxiv.org/abs/2305.12295"
            },
            "questions": {
                "value": "1. How likely that encountering a formalization failure may happen, and are there strategies in place to minimize formalization errors?\n2. To what extent does using constrained generation reduce the reasoning space, so as to mitigate the issue of \"logical inferences made next can have a potentially large set of answers\"? Is it possible that still there may be a considerably large set of answers, if so, how does your method decide on the the most appropriate content to generate next?\n3. Discussions on generalization involve models bootstrapped from other formalizable tasks. In scenarios challenging to formalize, what amount of preparatory work, such as the number of samples of formalizable tasks, is essential to ensure the model with strengthened generalization inference capabilities? If in the absence of abundant corresponding simpler tasks, how to generalize \"guide\" in broader scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4276/Reviewer_HHEy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4276/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824043986,
        "cdate": 1698824043986,
        "tmdate": 1699636395028,
        "mdate": 1699636395028,
        "license": "CC BY 4.0",
        "version": 2
    }
]