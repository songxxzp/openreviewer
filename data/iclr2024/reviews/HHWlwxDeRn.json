[
    {
        "id": "zJEZ4hapaN",
        "forum": "HHWlwxDeRn",
        "replyto": "HHWlwxDeRn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an innovative method to obtain view-consistent 3D DFFs from sparse RGBD data, enabling one-shot learning of complex manipulations that can be adapted to unfamiliar settings. The key contribution of SparseDFF comprises a lightweight feature refinement network, optimized using a contrastive loss applied to pairs of views after projecting image features onto the 3D point cloud. Furthermore, by establishing consistent feature fields in both the source and target scenes, they design an energy function that simplifies the process of minimizing feature differences with respect to the end-effector parameters between the demonstration and the target manipulation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a captivating approach to 3D feature learning, involving the creation of a point-cloud-based 3D representation and the utilization of the DINO feature extractor. This 3D representation based on point clouds can enable one-shot dexterous manipulation. As demonstrated by the experimental results, the proposed method exhibits robust performance across various settings."
            },
            "weaknesses": {
                "value": "In light of the experimental findings presented in this paper, it is respectfully suggested that the method described may not be particularly captivating. For a more comprehensive critique, kindly refer to the Questions section."
            },
            "questions": {
                "value": "### Question 1:\nIn the contrast learning process, a distance of 1cm is set as the threshold to distinguish between similar and dissimilar parts. Can the authors provide clarification on how they precisely define this distance?\n### Question 2:\nIs distance truly an effective criterion for distinguishing between similarity and dissimilarity?\n### Question 3:\nIs there a typographical error in Equation (3)? Why does it contain both an equation symbol and an inequality symbol?\n### Question 4:\nAdditionally, is the pruning process defined in Equation (3) considered reasonable?\n### Question 5:\nCould this pruning process potentially lead to the removal of critical edge information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3409/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_hwb9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698576555175,
        "cdate": 1698576555175,
        "tmdate": 1699636292143,
        "mdate": 1699636292143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IqHMrHnH7g",
        "forum": "HHWlwxDeRn",
        "replyto": "HHWlwxDeRn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_sPkV"
        ],
        "content": {
            "summary": {
                "value": "This work applies 3D feature fields for manipulation. The key contribution lies in introducing a sparse view setting, where unlike previous works that use dense RGB views, this work uses sparse RGB-D views. A point-based sparse 3D feature field construction method is introduced to improve the 3D information aggregation quality and the grasping task performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The introduction of sparse RGBD camera setting.\nThe method of sparse DFF to reconstruct 3D feature fields from sparse RGBD inputs.\nReasonable experiment design and analysis."
            },
            "weaknesses": {
                "value": "There are existing sparse-view NeRF methods (e.g., [1,2]) that applies similar ideas as the sparse DFF, some of which are not extremely hard to apply to the normal DFF (e.g,, [1]). It is fairer to allow baselines to also utilize the depth information introduced in this work (e.g., introducing depth supervision similar as [1] in DFF). \n\n[1] Depth-supervised NeRF: Fewer Views and Faster Training for Free\n[2] MVSNeRF: Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo"
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604942916,
        "cdate": 1698604942916,
        "tmdate": 1699636292058,
        "mdate": 1699636292058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1Il4S22OY5",
        "forum": "HHWlwxDeRn",
        "replyto": "HHWlwxDeRn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_dYVL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_dYVL"
        ],
        "content": {
            "summary": {
                "value": "To mitigate the dense view requirement in the distilled feature field (DFF) for the application of one-shot dexterous manipulation, the authors introduce sparseDFF, which utilizes a sparse collection of RGB-D scans of a scene. Feature points are reprojected using depth, followed by a feature refinement process on these reprojections. Contrastive loss and point pruning are employed to enhance feature consistency within each local neighborhood, and an energy function is formulated to aid in reducing feature discrepancies. Performance on grasping benchmarks demonstrate the proposed methods surpasses DFF while significantly outperforms UniDexGrasp++."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The necessity of as few as 4 views for transferring manipulation skills is noteworthy, as this method can be readily generalized to novel scenes.\n- Point feature refinement, the minimization of feature discrepancies using an energy function, and point pruning are specifically designed for applications with RGB-D scans as input.\n- Utilizing DINO feature distillation for diverse downstream grasping tasks markedly outperforms the previous baseline (DFF) and UniDexGrasp++."
            },
            "weaknesses": {
                "value": "**[Clearance]** \n- The authors are encouraged to establish connections regarding why distilling DINO features is advantageous and elucidate its applications in downstream tasks.\n- The definition of **one-shot** should be explained in the manuscript (abstract or introduction).\n- The input should accurately be described as \"multi-view RGB-D scans\" rather than \"Given a 3D point cloud X\", in the method section. And the dimension of the variable should be added.\n- Regarding the motivation of using DINO, while the authors have highlighted, \"This field offers semantic understandings for inter-scene correspondences that transcend geometric descriptors\", how does it contribute specifically to image matching deep models like LOFTR?\n\n**[Method]** \nThe authors propose \"discard the 20% of points that accumulate the fewest votes.\".How was this hyper-parameter for the pruning ratio determined? Were multiple-stage or iterative pruning strategies considered?\n\n**[Experiments]** \n- How did the inference performance compare with baseline methods?\n- Was depth information also utilized by DFF?"
            },
            "questions": {
                "value": "See the raised questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820727744,
        "cdate": 1698820727744,
        "tmdate": 1699636291990,
        "mdate": 1699636291990,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q0OTpATc9T",
        "forum": "HHWlwxDeRn",
        "replyto": "HHWlwxDeRn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk"
        ],
        "content": {
            "summary": {
                "value": "The broader goal of the paper is to enable the transfer of robotic dexterous grasps from one object to a similar object (which can happen with understanding the inherent similarities of the 3D shape instances despite the variations in appearances, poses, or categories). More precisely, given a source scene, source hand-object grasp and a target scene, what could be the target grasps.\nTo do so, they leverage 2D vision models (namely DINO) to learn features in 2D space and then distill or back-project those features in 3D. The similarly of features in 3D space allows transfer of grasps from one object to another.\nCompared to prior works, which leverage dense multi-view images to infer dense 2D features for each three 3D point and simply average the multi-view features, the paper operates in sparse view setting. Rather than simply out the point features from multiple view, the paper paper learns a feature refinement network on each point features and defines a contrastive learning approach to bring points closer to each in 3D other more closer in feature space and points farther from each other in 3D space more farther in feature space.\nFinally, leveraging the projected and refined 3D features from sparse views, the paper performs the task mentioned in first bullet, mapping source grasp (on sourc e scene) to target scene."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Writing is good, and the paper is easy to follow.\nThe motivation of creating a generalized robotic manipulator and leveraging 3D priors for that, seems exciting and promising."
            },
            "weaknesses": {
                "value": "[Novelty Issue] Lack of any exciting factor or in some sense novelty: The difference from prior work in DFF (distill feature field) is that the paper operates in sparse view setting, where simple fusion of features from multiple views doesn\u2019t perform best. To overcome this loss of views from prior work, they refine the point features baed on the insight that points close in 3D, should have similar features. This seems a very natural and obvious technique of pruning or refining points which have incorrect feature consistency w.r.t 3D consistency.\n\n[Assumptions on GT depth and camera pose] Secondly, following up on the novelty part, the paper makes the assumption of having GT depth maps and also GT camera poses. Errors in either of them will make the above refinement step tricky.\n\n[Additional experimental comparison] Experimental Comparison against baselines like (Neural descriptor fields and follow-up) where the goal is similar but rather than using the large vision model, an object category specific feature descriptor is learned.\n\n[Less Relevant] Newer papers like Lseg, Conceptfusion have projected LLM/ vision-LLM features on the 3D scenes, comments on using them as feature backbones will be appreciated.\n\n[Less Relevant] Papers learning joint hand object poses, leaning visual affordances from images, also seem to be relevant related works, comparison against them in related work would be appreciated (Affordance Diffusion: Synthesizing Hand-Object Interactions, papers on hand-object interaction: HOI etc)."
            },
            "questions": {
                "value": "I would like authors to address the points raised in weaknesses section. \n1. What happens when there is noise in pose and/or depth? How will that impact refinement module?\n2. How does the method compare with baselines like neural descriptor fields and follow up works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3409/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3409/Reviewer_Txdk",
                    "ICLR.cc/2024/Conference/Submission3409/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3409/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698893923492,
        "cdate": 1698893923492,
        "tmdate": 1700602651596,
        "mdate": 1700602651596,
        "license": "CC BY 4.0",
        "version": 2
    }
]