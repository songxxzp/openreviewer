[
    {
        "id": "AfRdscQiaW",
        "forum": "U17KoLrXE8",
        "replyto": "U17KoLrXE8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ObjectNet Captions, a dataset created to mitigate the exploitation of spurious correlations by machine learning models in image captioning tasks. Along with this dataset, the authors present HUMANr, a novel captioning metric aimed at providing a robust and consistent measure of performance that can be easily replicated and crowdsourced. HUMANr is intended to be an absolute performance metric that provides a clear target for model improvement and the ability to recognize when human-level captioning has been achieved, addressing the overestimation of machine performance by current metrics.\n\n\n# Post-rebuttal\nI appreciate the efforts made by the author. Their responses partially address my concern about the comparison and the scale of dataset. Therefore, I raise my rating. I encourage the author to make their proposed dataset and metric easily to use, such as can easily download and running via `pip`, to let people use them in practical ways."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There are several strengths for this paper:\n\n- Introduction of a new dataset that targets a key issue, specifically the reliance on spurious correlations by captioning models. \n- Development of HUMANr, and it can be easily implemented and crowdsourced.\n- Potential to recalibrate the understanding of machine captioning performance, as HUMANr contrasts with existing metrics by showing the superiority of human captions.\n- The paper provides tools for automatic computation of HUMANr (in supplementary), facilitating its adoption by the research community.\n- It examined several learning-based Captioning models and metrics."
            },
            "weaknesses": {
                "value": "I feel there are two major flaw points:\n\n- The authors currently did not use GPT-related captioning models, such as BLIP2. According to my usage, BLIP2 outperforms the compared methods used in this paper.\n\n-  The proposed dataset only contains 17,674 images which are quite small-scale to evaluate a captioning model comprehensively."
            },
            "questions": {
                "value": "Please address the concerns mentioned above. \n\nCould the author please also provide random sampled image-captions pairs. The current appendix only contains a few examples which cannot be assessed comprehensively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Reviewer_DRVD"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734471116,
        "cdate": 1698734471116,
        "tmdate": 1700730232801,
        "mdate": 1700730232801,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wd6GC3gkaF",
        "forum": "U17KoLrXE8",
        "replyto": "U17KoLrXE8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_zcyp"
        ],
        "content": {
            "summary": {
                "value": "To evaluate the captions generated by machines, this paper collected a dataset and proposed a new human study protocol. The machine-generated captions are compared with human-generated captions and humans are involved in the evaluation loop. The human study is performed on three datasets, i.e., COCO, Nocaps, and ObjectNet Captions. Three models are evaluated in this experiment, i.e., GIT, ClipCap, ExpNet."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper focuses on an important problem for the image captioning community, i.e., how big is the difference between machine generate captions and human-generated captions.\nThe conclusion that the machine-generated captions still underperform human-generated captions on unusual datasets and fail to generate long sentences is insightful for the community."
            },
            "weaknesses": {
                "value": "However, there are several unclear questions need clarification.\n\n1. Apart from revealing how big is the difference between machine-generated captions and human-generated captions, it would be meaningful to reveal what is the difference between machine-generated captions and human-generated captions. Though the authors have revealed some differences, such as spurious objects and caption lengths, the root cause seems still unclear.\n\n2. Some experiment details are missing. For instance, how to compute the HUMANr score? \n\n3. Asking human participants to rate between 1-9 seems subjective. If two new image captioning models are evaluated with two different groups of people, will the results be comparable? It would be interesting to show the deviation of two different groups of people rating the same model in Figure 4.\n\n4. The ObjectNet cannot be regarded as a contribution as the authors only select some images with longer captions."
            },
            "questions": {
                "value": "In Section 4.3, paragraph 2, what does ``we eliminated all images where GITL failed any of the seven checks above\u2014human failures were not considered\u2019\u2019 mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928969980,
        "cdate": 1698928969980,
        "tmdate": 1699636452055,
        "mdate": 1699636452055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fBdkJrgLM1",
        "forum": "U17KoLrXE8",
        "replyto": "U17KoLrXE8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_X1XD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_X1XD"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the task of image captioning and proposes a new dataset and a new metric. There are some findings, for example, there is a large gap betten human and models on the task of image captioning. The proposed dataset if challenging compared with existing ones, which contains much more unique tokens and n-grams and should be useful for the community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. a new dataset is proposed. The dataset is more challenging and contains more unique tokens and n-grams.\n2. a new metric is proposed.\n3. analysing existing models vs. human using a wide range of metrics."
            },
            "weaknesses": {
                "value": "1. the scale of the dataset is small. \n2. the auther only considers traditional image captioning models. Some LLM-based models like LLaVA should be considered and the comparison among these models should be more interesting.\n3. the findings that there is a large gap betten human and models is a common sense, so I do not think it is a significant contribution. But if the author can show that the most advanced models like GPT-4v is inferior to humans and the proposed metric is able to measure the gap, it should be more interesting."
            },
            "questions": {
                "value": "Some important references related to image captioning metrics are missing.\n1. Learning to evaluate image captioning. CVPR 2018.\n2. Describing like humans: on diversity in image captioning, CVPR 2018.\n3. On diversity in image captioning: metrics and methods, TPAMI, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4705/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699068284607,
        "cdate": 1699068284607,
        "tmdate": 1699636451974,
        "mdate": 1699636451974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P4YLtYObEZ",
        "forum": "U17KoLrXE8",
        "replyto": "U17KoLrXE8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \"ObjectNet Captions,\" a challenging dataset for image captioning, and presents HUMANr, a new metric for evaluating caption quality. It highlights a significant performance gap between human and model-generated captions, emphasizing the limitations of current models in generating detailed, accurate captions. The study's findings challenge the notion that advanced models like GPT-4 surpass human capabilities in this domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Introduction of a challenging dataset and HUMANr metric.\n2. In-depth comparison of existing models with human performance using various metrics.\n3. The paper effectively showcases the limitations of current models in handling diverse and complex captioning scenarios."
            },
            "weaknesses": {
                "value": "1. The dataset's focus on home environments and its relatively small size (17,674 images) may limit its generalizability.\n2. Not including state-of-the-art models like BLIP2 or LLM-based models in the analysis.\n3. The human-centric approach, while insightful, may introduce new biases and subjectivities.\n4. The cost and scalability of HUMANr in large-scale applications are not addressed.\n5. The revelation of a performance gap between humans and models is not a novel insight and lacks depth without comparing the most advanced models.\n6. The paper omits crucial experimental details, like the computation of HUMANr and handling discrepancies in human evaluations."
            },
            "questions": {
                "value": "1. How can the ObjectNet Captions dataset be expanded to cover a broader range of environments and scenarios?\n2. What steps can be taken to include state-of-the-art models like BLIP2 in future evaluations?\n3. How does HUMANr address the subjectivity and potential bias in human judgment?\n4. Are there plans to adapt the dataset and HUMANr for non-English languages or diverse cultural contexts?\n5. How can the scalability and cost-effectiveness of HUMANr be improved for widespread adoption?\n6. Can the authors provide more details on the methodology, especially regarding the computation of HUMANr and the management of subjective human ratings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4705/Reviewer_V5WD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4705/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699856785260,
        "cdate": 1699856785260,
        "tmdate": 1699856785260,
        "mdate": 1699856785260,
        "license": "CC BY 4.0",
        "version": 2
    }
]