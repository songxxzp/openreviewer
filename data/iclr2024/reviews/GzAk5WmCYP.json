[
    {
        "id": "c3OwefQJia",
        "forum": "GzAk5WmCYP",
        "replyto": "GzAk5WmCYP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_wxrg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_wxrg"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the use of evolutionary algorithms to finetune hyperparameters on the fly in a federated learning setting. The resulting algorithm, FedPop, consists of two nested evolutionary algorithms: one local, optimizing the hyperparameters used for local updates at the clients, and another global, concerning the hyperparameters of the server update. This nested optimization strategy paired with the mutations introduced by the evolutionary algorithm substantially increases the number of hyperparameters FedPoP explores during training at virtually no extra computational cost. That translates into significant improvements in performance, as demonstrated in experiments on a number of common benchmarks for Federated Learning."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Hyperparameter optimization in federated learning is a relevant and challenging problem, and the application of evolutionary algorithms in this context is indeed novel, to the best of my knowledge. The main ideas of the paper are well motivated and supported by strong and extensive empirical results in a number of relevant benchmarks. Moreover, the paper is very well written and easy to read."
            },
            "weaknesses": {
                "value": "While the main idea of the paper is novel, I do not think it is particularly innovative, since this seems to be a straightforward application of evolutionary algorithms to hyperparameter optimization. The experimental evaluation, albeit extensive and well designed, is not entirely clear in a few points, as I pointed out in the questions below.\n\nMinor points:\n- In \u201cAfterwards, we randomly sample addition K HP-vectors\u201d, I believe the authors meant to say \u201cadditional\u201d.\n- In Section 3.4.4., the sentence \u201cwhich saves the communication and computation costs\u201d reads as if FedPop incurred none of these costs, which is probably not the intended meaning. Maybe \u201csaves in communication [\u2026]\u201d or \u201creduces the communication [\u2026]\u201d would read better."
            },
            "questions": {
                "value": "1. In the related work section, the following sentence is not clear \u201cWang et al. (2019) tunes only the local optimization epochs based on the client\u2019s resources\u201d. What exactly is being optimized here and how?\n2. In Table 1, the distinction between global and finetuned models is not entirely clear from the text. Do I understand it correctly that the finetuned version includes one last update over the global model using local data?\n3. According to Algorithm 1, FedPop returns $N_c$ models. Do the authors report the average over these $N_c$ models, or the best model among them?\n4. In the experiments, it is not always clear what the baselines are. \n    - Since RS or SHA are used as a starting point for FedEX and FedProp, are the results for SHA in Tables 2 and 3 obtained with twice the tuning budget? In contrast, what does \u201cNone\u201d mean in Table 1?\n    - In Figure 5, are FedPoP and RS started from the same population?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698328813558,
        "cdate": 1698328813558,
        "tmdate": 1699636285182,
        "mdate": 1699636285182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Kr1c0ORxR7",
        "forum": "GzAk5WmCYP",
        "replyto": "GzAk5WmCYP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_KeXM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_KeXM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), as an online \u201dtuning-while-training\u201d framework, offering computational efficiency and enabling the exploration of a broader HP search space. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both the client and server sides. Specifically, FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both the client and server sides."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The tuning-while-training framework is important for FL due to its high efficiency.\n2. Experiment result are done for real-world cross-silo FL applications."
            },
            "weaknesses": {
                "value": "1. No explanation about why RS initializes $N_c = R_t / R_c$.\n2. Obviously, the average validation loss of all active clients, is dynamic with the convergence for each HP-configuration. Decoupling the effect between convergence and the HP-configuration is important to implement tuning-while-training. Smaller validation loss may come from convergence instead of good HP-configuration. However, the paper does not consider this problem. Thus, it is hard to believe the proposed method can actually tune HP well.\n3. The baseline is weak, RS and SHA seem to be simple methods in tuning hyper-parameters. Maybe a greedy search algorithm can be better than RS.\n4. In table 1, the accuracies of IID and NonIID cases are too small. For CIFAR-10, the centralized training can obtain > 93% accuracy. And the dir=1.0 and 0.5 are not very strong data heterogeneity. Many works can obtain higher accuracy of this. Could authors explain more about this?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698571882099,
        "cdate": 1698571882099,
        "tmdate": 1699636285112,
        "mdate": 1699636285112,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aHGzU5q7sx",
        "forum": "GzAk5WmCYP",
        "replyto": "GzAk5WmCYP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_T6YN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_T6YN"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the hyperparameter optimization or HPO problem in the federated learning (FL) setting. Building upon the FL+HPO setting of Khodak et al. (2021), where the weights from models corresponding to different hyperparameters (HPs) can be aggregated (the \"weight-sharing\" assumption from Khodak, et al. (2021)), the paper proposes the Fedpop algorithm that optimizes both the server and client HPs. For both sets of HPs, Fedpop utilizes a population-based HPO scheme. The algorithm simultaneously initiates multiple HPO processes from random configurations. Within each process, the client HPs are initialized within a local region around the initial configuration, and then evolved via a population based training scheme with the set of clients forming the population, where weak client HP candidates are replaced by perturbations of strong HP candidates, effectively performing a local search around the initial point. Periodically, both the server and client HPs are synchronized, and HPs for HPO processes with weak performance are replaced with perturbed version of HPs for HPO processes with strong performance. Empirical evaluation of Fedpop highlights  gains over the existing Fedex scheme in IID and non-IID per-client data distribution setting and also in the cross-silo setting. The paper also presents the performance of the proposed Fedpop under various FL setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed Fedpop algorithm is able to handle both server and client side HPs, and does not require a final federated model training, further extending what we can do with HPO in the FL setting without relatively high communication overhead.\n\nThe empirical evaluation utilizes various problem setups such as IID and non-IID per-client distributions, a large-scale cross-silo setup and a large-scale ImageNet-1k training. The performance of the proposed Fedpop is extremely favorable compared to the baselines.\n\nThis paper proposes an interesting combination of local searches for client HPs and global search for all HPs, leveraging population based training at different granularities. This is probably the key to the strong empirical performance."
            },
            "weaknesses": {
                "value": "To the best of my understanding, Fedpop is only applicable to optimizer related hyperparameters (learning rate, momentum, etc) and not to architectural hyperparameters (such as number of layers or activation choices) much like Fedex as it relies on the \"weight-sharing\" assumption, and thus is limited in scope. Section 2, page 2 (end of paragraph on \"Hyperparameter Tuning for FL System\") mentions \"it does not impose any restriction on [...] model architecture\". But it seems that the architecture is pre-selected and not-tuned, and only optimization related HPs are tuned. The Fed-Opt setup depicted in Figure 1 and discussed in Section 3.1 assumes that the weights $\\mathbf{w}^k$ corresponding to different client HPs $\\alpha_i, \\beta_i^k$ can be aggregated. This limits the scope of the HPO problem considered. With Fedex, one can conceptually utilize the outer SHA loop to optimize more general HPs (server and client side), but that is not possible with Fedpop.\n\n\nOther specific points:\n- There appear to be some notational issues in equation (3) and the ensuing discussion. It is possible that I am misunderstanding the presentation, but there might not be a need for $H_j$, and authors can directly use $h_j \\sim U(\\cdots)$ everywhere. Alternately, the authors can present the equation is a more unambiguous manner.\n- It is not clear to me if Fedpop requires each client to simultaneously maintain multiple model weights for the $N_c$ HPO processes, and if the aggregator ever needs to simultaneously maintain $N_c$ (or even $N_c \\times R_c$) weights. If that is the case, it would seem that the per-client and aggregator memory overhead is significantly high (especially if we are trying to tune large models)."
            },
            "questions": {
                "value": "- If I understand Flora correctly, it only requires a single FL training, which is done once the hyperparameters have been selected. There is no federated \"retraining\" in Flora -- just a single federated training in the whole process. Also it is not clear what is meant by \"client's local HPs\". This is true for Fedex, but Flora handles global hyperparameters (such as architectures and such). What am I missing here? Can this be corrected or clarified?\n\n- What is $N_c$ and how is it related to $R_c$ and $R_t$? In Section 3.1, $N_c$ is defined as the number of clients, while in Section 3.3 $N_c$ is the number of initial HP configurations, and thus, the number of HPT processes. $K$ is used as the number of active clients, but also as the number of perturbed client HPs within the $\\Delta$-ball around $\\beta_i^0$ of the $i$-th tuning process. Can this be clarified?\n\n- It is not clear what kind of parity is maintained between Fedex and Fedpop. Are all algorithms allowed a total of $R_t$ number of communication rounds? There is no $R_t$ in Fedpop, and the number of communication rounds in fedpop naively seems to be $2 N_c R_c$ (the $R_c$ loop, and the $N_c$ loop, and one round of communication for Fedpop-L and one round for Agg). So is $N_c = R_t / (2 R_c)$? How is the corresponding $N_c$ (the number of initial configurations) in SHA for Fedex selected? It would seem that this would significantly affect Fedex performance.\n\n- Fedex does not really modify the server side hyperparameters $\\alpha$ (but starts with multiple to then have the outer SHA loop). How much of the improvement of Fedpop over Fedex attributable to the fact that the $\\alpha \\setminus \\beta$ hyperparameters are being tuned in Fedpop and Fedex is only given 5 configurations of $\\alpha$?  There is a  version of Fedex which utilizes an outer SHA (as used in the empirical evaluation) which allows it to try way more server+client HPs. So it seems a bit odd to see that Fedex only tries 5 $\\alpha$ values (in the table in Figure 2), which means SHA is seeded with only 5 configurations, which is a bit odd. One would assume that SHA would start with a larger number of configurations.\n\n- How is SHA used for Fedpop? SHA usually utilizes RS to initialize the search, and progressively eliminates candidates. In Fedpop, the elimination (in Fedpop-L and Fedpop-G) are handled internally. So what is SHA doing for Fedpop?\n\n- \"Tuning-while-training\" is the basis of various popular schemes such as Fedex, as well as, more traditional HPO schemes such as Successive Halving and Hyperband. \"Tuning-while-training\" is also the basis of the (well-studied) bilevel hyperparameter optimization problem. However, \"tuning-while-training\" heavily relies on the \"weight-sharing\" assumption (that is, weights from models with different HPs can be aggregated). Is Fedpop enabling \"tuning-while-training\" in a new setup?\n\n- How does FedPop address C1 and C2? Retraining (C1) is only avoidable with the \"weight-sharing\" assumption. How does Fedpop handle/mitigate the mentioned evaluation bias with non-IID data? The discussion in 3.4.4 appears to be without any evidence. Zhou et al. (2021) performs a single FL training on a single HP configuration so, in the notation of this paper, its communication overhead is at most $R_c$. It is not clear why gradient-based HPO from Khodak et al. (2021) is more expensive (in terms of computation or communication). It is also not clear why conducting Fedpop-G every $T_g$ rounds & performing local searches \"mitigates the noise\" introduced by the non-IID-ness of the problem. These appear to be strong claims without sufficient evidence. Can the author provide evidence for these claims?\n\n- When the local and global HPs are changing throughout the optimization, and $\\alpha^\\star, \\beta^\\star$ are the final best HPs selected by the Fedpop algorithm, what is the guarantee that any of the per-HPO-process weights $\\lbrace \\mathbf w_{i} \\rbrace_{i=1}^{N_c}$  (or per-client depending on what $N_c$ denotes) returned by the Algorithm 1 is the same as if a single FL training was initiated from scratch with fixed HPs $\\alpha^\\star, \\beta^\\star$ and a max communication budget of $R_c$? With RS or SHA, it is ensured that the model corresponding to the selected HPs is trained for $R_c$ (the max allocated communication budget per HP configuration).  With a bilevel formulation (considered in various bilevel federated HPO such as in FedNest, SimFBO [A] or FedBiO [B]), this is guaranteed if convergence is guaranteed. Retraining from scratch also ensures this but is computationally expensive. Without any such guarantee, it is possible to claim that there is no need for a \"retraining\" but the final selected model is only trained partially, making the no-retraining claim somewhat limited in scope.\n\n> - [A] Yang, Yifan, Peiyao Xiao, and Kaiyi Ji. \"SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.\" NeurIPS (2023).\n> - [B] Li, Junyi, Feihu Huang, and Heng Huang. \"Communication-Efficient Federated Bilevel Optimization with Local and Global Lower Level Problems.\" NeurIPS (2023).\n\n-  The **Evo** update (as presented) appears to be not related to its input $\\mathbf{h}$? It seems like we randomly sample a quantity, and then perturbing it irrespective of the input $\\mathbf{h}$. Is that the correct understanding?\n\n- (super minor) HPO seems to be a common acronym for hyperparameter optimization/tuning. Is there any particular reason authors decided to utilize the HPT acronym instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721129589,
        "cdate": 1698721129589,
        "tmdate": 1699636285037,
        "mdate": 1699636285037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c7SO3YXR4y",
        "forum": "GzAk5WmCYP",
        "replyto": "GzAk5WmCYP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_RYh9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_RYh9"
        ],
        "content": {
            "summary": {
                "value": "The authors propose FedPOP, which in spirit is the extension of Jaderberg et. al., 2017 to the federated setting. The authors show how to apply population based HP tuning to the typical server and client-side hyperparameters that appear in federated learning. They show the benefit of their method in a range of empirical studies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well-motivated and strikes me as a well-executed extension of Jaderberg et al 2017 to the federated setting. The problem of federated hyperparameter tuning is highly relevant - especially as in cross-device FL we cannot assume for the training-data and setup to be repeatable across different runs. Making maximal use of parallel tuning processes through population based evolution is a good approach. I especially enjoy the consideration of iid vs. non-iid and discussion surrounding evaluation - as well as the proposed solution through validation score decay. This is a unique problem that arises in FL and I appreciate the authors addressing it. \nThe number of different dataset considered is highly appreciated."
            },
            "weaknesses": {
                "value": "The paper describes an algorithm with a lot of hyper-parameters, training & evaluation settings with a lot of specified details across different dataset. As a reader, it is hard to keep an overview of the exact settings, assumptions and baselines as well as the choice of those hyperparameters (e.g. number of clients, dirichlet sampling prob, R_c, R_t, N_c, search-space, annealing rates and much more). I would highly appreciate a detailed table of required parameters to reproduce experiments and a clear definition of how baselines are compared (e.g. is R_t equalized) in the Appendix. Some of my below questions stem from a lack of overview.\n\nIt took my quite some time to understand the definition of components and the relationship $R_t=N_c*R_c$ from reading the text. The code-base conveniently uses the same help-string \"Training epochs for server\" for all three elements ;). A short and concise definition of these at the beginning of 3.3 would have been helpful. You seem to be equating \"training rounds\", \"communication budget\" and \"maximum resources\" in your exposition, although in any other FL-context, these terms can have very different meaning.\n\nSome statements and results seem to be contradicting (e.g. are weights initialized from the optimal RS/SHA setting vs. learning-curves, see below). At this point, the paper requires some additional care to clarify how empirical studies have been done.\n\nI believe this paper to have very high potential and I will raise my score should the authors alleviate my concerns about the existing empirical evaluations and clean up their exposition. Should the authors further expand their experiments with a discussion about the evolution on hyper-parameters and an empirical discussion around the power-law method for validation scores, I will consider raising my score further. \n\nThanks for putting this paper together, I enjoyed digging into it."
            },
            "questions": {
                "value": "- Algorithm 1 considers $K$ active clients per round. However these K same clients seem to be utilized by all tuning processes. What is the assumption here? Do we assume that the same K clients participate in $N_c$ different \"parallel\" training runs, executing one local training configuration after the other - or do we assume that at any point in time, there are $N_c*K$ clients available to participate in a specific training run? \n- RS or SHA as initial population construction. I understand that you pick the best configuration $\\alpha, \\beta, w$ from RS or SHA as starting point $\\alpha_i, \\beta_i, w_i$ for all tuning processes $i$. Consequently, for example Table 1 \"None\" rows stand for the starting-point of the subsequently applied Tuning Algorithms. FedPop (and FedEx) therefore enjoyed an additional budget of $R_t$ communication rounds, beyond what the \"naive\" tuning algorithms have been granted. What is therefore missing is a baseline where RS and SHA alone have been granted the additional budget to find better alternatives, i.e. through longer convergence times or additional random configurations. While I believe that you show that population-based training does provide additional benefits compared to naive RS, Table 1 (and the other results) is not equalized by resources-used, which is what you claim to base your experiments on. \n- In the context of the previous question, I would have expected the learning curves of Figure 5 and Appendix Figure 4 to look different: Why is FedPop not starting from the best-configuration of RS in terms of evaluation accuracy? These curves seem to suggest that you start from random $w$, which is contrary to your exposition in 3.4. \n- If my understanding is correct, then the communication and computation for the population construction with RS or SHA is not accounted for in the stated budget for FedPop. Could you please provide the configuration for creating these initial starting points? \n- One of the most interesting insights in Jaderberg et. al 2017 was to see the \"history\" of hyperparameters that lead to the final model. For example, it would 'discover' learning rate warmup and annealing. I would expect it to be highly insightful to see the same analysis for these federated experiments. Could you provide that for some of your runs?\n- The problem of federated evaluation is generally quite challenging, since not all clients are assumed to be available for evaluation all the time. I would love to understand the consequence of this for your experiments. Concretely: Assuming we do have access to the entire validation set at the server (which corresponds to standard FL evaluation for C10 and related \"toy\" datasets), how much standard-deviation do we observe by randomly sampling $K$ active clients for evaluation around the \"true\" global dataset-evaluation? How much does the choice between a) single-sample estimate $s_i$ per tuning process b) power-law decayed estimate $s_i$ and c) the oracle whole-federation $s_i$ for the computation of $FedPop-G$ influence the selection of hyper-parameters as well as the final result? This ties in to my previous question: Do you use the same $K$ clients' validation sets for computation of $s_i$ across all tuning processes $i$ or do you additional stochasticity comparing across different $K$ validation sets. While I intuitively understand why you propose the power-law method, I see a lack of empirical evidence to measure its benefits.\n- Hyperparameters are missing. E.g. how many clients do you consider for the individual experiments? I see some settings in the code, but the paper should be complete for reproducibility. Unless you are reproducing more hyperparameters from previous work (4.1.1) than is apparent from the text, in which case please be more explicit. \n- I have assumed, but realize now you haven't stated explicitly: Did you equalize $R_t$ between FedEx and FedPop? Beyond that, comparing Table 1 SHA FedEx with Table 4 FedPop-G, I believe we can see what happens if you use FedPop to only optimize the server-side parameters, which would put it onto equal-footing with FedEx in terms of what the methods can optimize. If the resource budget is equalized between those two, it would be valuable to point out that FedPop outperforms FedEx even when constraining FedPop to optimizing server-side h-params only, assuming we fix the local parameters to the optima found through SHA.\n- I do not understand Algorithm 1 in the appendix. My understanding of \"Using RS or SHA as population constructor\" follows 3.4, meaning we find the initial $\\alpha, \\beta, w$ through either RS or SHA. Algorithm 1 seems to perform SHA interleaved with FedPop. Please clarify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778973444,
        "cdate": 1698778973444,
        "tmdate": 1699636284951,
        "mdate": 1699636284951,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YyfmFwPT7p",
        "forum": "GzAk5WmCYP",
        "replyto": "GzAk5WmCYP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_5toX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3352/Reviewer_5toX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a hyperparameter optimization (HPO) algorithm for federated learning (FL). Compared to existing works, the proposed is more efficient since it employs an online \"tuning-while-training\" framework. The evolutionary algorithm is used to search for the optimized hyperparameters during the FL training process. Empirical results show that the proposed FedPop algorithm outperforms random search (RS) and successive halving (SHA) baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem tackled in this paper is well-motivated. Reducing the computational efficiency is an important and difficult problem for the HPO of FL since each run of the FL process is expensive.  \n\n2. The organization of this paper is good. The proposed method is clearly described and easy to follow."
            },
            "weaknesses": {
                "value": "1. RS and SHA are two very simple baselines that are not enough to demonstrate the significance of the proposed method. Some other SOTA methods such as Hyperband and BOHB should be compared. In Section 4.4, the convergence analysis is only done over RS and FedPop. Why are the learning curves of other baselines in Table 1 not shown in Figure 5?\n\n2. Some claims of this paper are not well supported. For example,\n\n- The authors highlighted several times that FedPop can be conducted in a parallel and asynchronous manner. However, it's not clear which steps can be asynchronously executed and how it could affect the efficiency of the HPO process.    \n\n- In Section 3.3, it is mentioned that \"the elimination might also discard HP-configurations which lead to promising results but perform poorly at early stages\". However, this issue seems to also exist in the proposed FedPop due to the replacing steps in FedPop-L and FedPop-G. Can this issue be eliminated or alleviated in the proposed method?\n\n3. The lack of theoretical analysis reduces the significance of the proposed method. Due to the \"tuning-while-training\" strategy, the hyperparameters used in one FL run keep changing. Whether the FL process can converge under such dynamic hyperparameters becomes an essential question. Even though the learning curves of two experiments are shown in Figure 5, it's not enough to verify the convergence property of FedPop in other general cases. The authors have also noticed that \"too distinct HP-vectors for the active clients would lead to unstable performance\", which implies that the change of hyperparameters could affect the convergence of FL in some scenarios. Some theoretical analysis would help to address this concern."
            },
            "questions": {
                "value": "1. In Algorithm 1, each $w_i^k$ is trained using different hyper-parameters (e.g., different batch_size). Then, can they be fused directly?\n\n2. The evaluation bias (C2) is considered in FedPop-G but not in the FedPop-L algorithm. However, for the non-IID case, the good hyperparameters for one client may not be suitable for other clients. Can you provide some analysis on the effectiveness of FedPop-L in the non-IID case?\n\n3. What aggregation algorithm did you use in the experiments? Does the proposed method work well for various FL aggregation strategies (e.g., FedAvg, FedProx, etc.)?\n\n4. In Table 1, why is the performance of some fine-tuned models worse than that of the global model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3352/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3352/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3352/Reviewer_5toX"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3352/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699009196678,
        "cdate": 1699009196678,
        "tmdate": 1699636284877,
        "mdate": 1699636284877,
        "license": "CC BY 4.0",
        "version": 2
    }
]