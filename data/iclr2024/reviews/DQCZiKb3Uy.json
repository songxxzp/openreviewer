[
    {
        "id": "YCh26JGSxU",
        "forum": "DQCZiKb3Uy",
        "replyto": "DQCZiKb3Uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a general task-related visual representation as input to the policy, aiming to improve the efficiency of reinforcement learning. By inputting task-related information (including auxiliary information) into VLM as prompts, efficient fusion with the current visual state is achieved. This paper verifies the effectiveness of this approach through experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "[1] The method proposed in this paper, PR2L, is very straightforward, and the story sounds reasonable. The organization of the paper is very clear, and it is polished well. \n\n[2] The experimental analysis is quite comprehensive, and ablation experiments have demonstrated the effectiveness of each part of the proposed method. Conducting experiments in a challenging environment like Minecraft is persuasive."
            },
            "weaknesses": {
                "value": "[1] Concerns about computational cost. Obtaining promptable representation requires running a complex VLM on every image returned from the environment, along with the prompt and answer. This cost is prohibitively high and not practical.\n\n[2] Concerns about the performance of the learned policy. The visually rich representation obtained at such a high computational cost should greatly improve the performance of the policy. However, the authors did not provide videos of rollouts on relevant tasks, making it difficult to judge the effectiveness of the promptable representation in a real Minecraft environment (the three tasks used in the paper are not very complex). \n\n[3] Insufficient literature review. Since the authors conducted experiments in Minecraft, they should have provided a more comprehensive discussion of articles that control and plan within Minecraft. However, the authors left out the following important literature. \n\n1. Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction. \n2. Video pretraining (vpt): Learning to act by watching unlabeled online videos. \n3. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. \n4. CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft. \n5. GROOT: Learning to Follow Instructions by Watching Gameplay Videos. \n6. Learning from Visual Observation via Offline Pretrained State-to-Go Transformer."
            },
            "questions": {
                "value": "My main concerns are presented in the \"Weaknesses\" box, please refer to it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_AR8Q"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698143988892,
        "cdate": 1698143988892,
        "tmdate": 1699636755872,
        "mdate": 1699636755872,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xI0hli5jGN",
        "forum": "DQCZiKb3Uy",
        "replyto": "DQCZiKb3Uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_Cm8x"
        ],
        "content": {
            "summary": {
                "value": "Paper presents an approach to provide task-relevant visual representations for RL, especially in an open-world environment.The main idea is to take a pre-trained VLM (vision-language model), feed it with the current visual observation, and a meticulously picked prompt about the current task, and then use the resulting embeddings produced by the last two layers of the VLM transformer as the representation. The authors also propose some additional techniques that could be helpful: 1) the VLM has to generate some text out of the prompt and use the embedding to correspond to both the prompt and the produced text, not just the embedding of the prompt only; 2) prompt engineering; 3) an encoder-decoder transformer is used as the policy to distill the representations into a summary embedding. Experiments and ablations on three Minecraft tasks show some promises."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+The study is relevant and could be of interest to many audiences with a background in large models, reinforcement learning, and representation learning.\n\n+The method is well-motivated and technically sound. Pretrained VLM indeed provides open vocabulary and even knowledge-aided representations for multimodal input, which can be quite beneficial to open-world environments. Plus, it is plausible to tweak the representation further via prompting. This is a very neat idea.\n\n+The results look impressive. Although the method is only evaluated on limited (3) tasks in a single environment (Minecraft), the advantages over the baselines and ablative approaches are significant. I do think the authors did a good job of comparing it against several interesting baselines, including no generation, no prompt, etc."
            },
            "weaknesses": {
                "value": "Having said those above, I do have some major concerns about the evaluation part of this paper. I also would like to point out some minor issues as well.\n\n-Albeit the promises shown by the results on 3 tasks on Minecraft, I don't think the approach is thoroughly evaluated, especially given their claim on \"leverage contextual prior information\" and \"visually-complex RL tasks\" (see abstract). I have the following suggestions:\n\n1) There are some other approaches that are designed to tackle similar issues, especially in Minecraft, ex. [1,2,3]. Although I agree some settings could be different (RL vs. IL, etc), they all deliver some backbone design or objective functions that could facilitate better representations. Comparisons against these missing baselines would help the reader with a better understanding of the significance of the proposed method.\n\n2) Minecraft is indeed a challenging domain in terms of open-world and complex visual observations. However, the tasks being evaluated here (spider, cow, sheep) do not seem to be challenging enough to justify the effectiveness of the proposed method, especially on the claimed \"leverage contextual prior information\". These mobs are indeed very common and the tasks themselves do not seem to involve complex visual stimuli. My suggestion is to try some long-term and open-ended tasks like surviving, collecting items, etc. [7] offers a few of them and worth taking a look at.\n\nMinor: some references on open-world representation learning and Minecraft agents should be cited: [1-6].\n\n\n[1] open-world control: https://arxiv.org/abs/2301.10034\n\n[2] VPT: https://arxiv.org/abs/2206.11795\n\n[3] STG transformer: https://arxiv.org/abs/2306.12860\n\n[4] DEPS: https://arxiv.org/abs/2302.01560\n\n[5] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[6] GITM: https://arxiv.org/abs/2305.17144\n\n[7] MCU: https://arxiv.org/abs/2310.08367"
            },
            "questions": {
                "value": "-In Figure 2 and 3, why do some curves not have shadows?\n\n-Some prompts shown in Table 1 require hand-crafted domain knowledge, ex. \"Spiders in Minecraft are black\". Is is possible to avoid this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698482236372,
        "cdate": 1698482236372,
        "tmdate": 1699636755676,
        "mdate": 1699636755676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nZPUgdAv07",
        "forum": "DQCZiKb3Uy",
        "replyto": "DQCZiKb3Uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces promptable representations for reinforcement learning (PR2L), which uses the semantic features of pre-trained VLM as state representation for reinforcement learning; the main advantage of PR2L to other pre-trained representations is that PR2L allows extract task-specific features from a generic pre-trained models by injecting task knowledge via prompting. PR2L outperforms both domain-specific representations and instruction-following methods on several tasks in MineCraft domain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "PR2L presents an interesting and creative way of utilizing pre-trained VLMs as representations for visual policy learning. It is unlike prior pre-trained representations for control work in which the features are generic (i.e., directly encoding the image observation); it is also different from recent Vision-Language-Action work (Brohan et al., 2023) in that it does not require fine-tuning a pre-trained VLM and enable high-frequency policies that are disentangled from the VLM backbone. \n\nAs VLMs are increasingly adopted in decision-making pipelines, PR2L is a timely work that presents a lightweight and simple alternative to the existing literature. \n\nThe paper itself is generally well-written and free of grammatical errors."
            },
            "weaknesses": {
                "value": "This paper's weaknesses mainly lie in its experimental methodologies. \n\n1. The only form of prompt that PR2L uses essentially amounts to object detection in the scene. This introduces a confounding factor of whether PR2L outperforms baselines because it is able to recognize objects better in the scene. \n\n2. The paper claims that the prompts are different from instruction following; however, the prompts are still manually constructed and task-specific. It's unclear the advantage of doing so as instructions, by construction, should exist as it is a direct form of task specification.\n\n3. The improvements of PR2L over its various ablations appear only moderate. Furthermore, the best prompt format for the tasks are not consistent; for Spider, it is helpful to include contextual information of what a spider looks like in MineCraft, whereas for the other two tasks, it is more helpful to disregard such information. Therefore, applying PR2L to a new task may require substantial prompt engineering for the best performance.\n\n4. PR2L is only evaluated on 3 tasks; these tasks are also the simplest in the MineDojo benchmark. The paper would be strengthened if more tasks and domains are evaluated. Currently, it is not convincing that PR2L can be generally applied to other visuomotor control domains. Relatedly, PR2L does not outperform MineCLIP on most tasks; given that MineCLIP exists and is open-sourced, PR2L's stated advantages can be better demonstrated via a new domain in which foundation pre-trained representations do not already exist. \n\n5. BLIP-2's vision encoder may not be the strongest baseline for pre-trained vision encoders. Several prior works such as VC-1, R3M, MVP, VIP are trained for decision-making and robotics tasks and may constitute stronger baselines in that category."
            },
            "questions": {
                "value": "1. Could a baseline that somehow incorporates oracle object detection information be included? This will test whether PR2L does better because it detects the object of task interest in the scene.\n\n2. Could more tasks and qualitatively different prompts be tested in the paper? Ideally, some tasks in MineCraft requires more than just object detection as auxillary information that may be implicitly captured by a VLM.\n\n3. Could additional pre-trained vision encoder baselines be included? \n\nI am willing to improve my assessment of the paper if these questions as well as the points in the Weakness section can be adequately addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_qGuw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698589937830,
        "cdate": 1698589937830,
        "tmdate": 1700671052844,
        "mdate": 1700671052844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BZDCpA1kdy",
        "forum": "DQCZiKb3Uy",
        "replyto": "DQCZiKb3Uy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a simple but effective approach of improving visual embeddings from pre-trained VLMs by providing VLMs with useful prompts. For example, to detect spider in Minecraft, instead of directly encoding the scene using a vision encoder, the authors extract more meaningful representations from the VLM by giving auxiliary information like \u201cSpiders in Minecraft are black. Is there a spider in the image?\u201d. This can help the VLM to focus on task-specific/domain-specific information and produce more meaningful embeddings that are useful for training a RL policy. The authors show that the proposed approach outperforms policies trained using generic image features from a vision encoder ( on 3/3 tasks), as well as domain specific image features (on 2/3 tasks)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The main contribution of the paper \u201cPrompting VLM via auxiliary information and task context\u201d allows extracting more meaningful representation from a VLM is quite interesting and easily applicable to a range of tasks. Instead of fine-tuning VLM for specific domains, it\u2019s easier to plug-and-play existing VLM and extract meaningful representations via prompting.\n- Overall, the paper is well written and is systematic in its evaluation. I also appreciate the authors willingness to address concerns preemptively (lack of visual tokens as input to the policy MLP, not fine-tuning VLM similar to RT-1)."
            },
            "weaknesses": {
                "value": "- Given that the approach is using a VLM, it\u2019d be nice to test the model for \u201cunseen\u201d tasks, containing objects and instructions not seen during training. For instance, does the policy generalise form \u201cCombat Spider\u201d to \u201cCombat Zombie\u201d?\n- I also recommend a stronger evaluation on Minecraft benchmark consistent with the evaluations done in MineDOJO. Currently, the paper shows result on only three tasks. For a more exhaustive evaluation, MineDOJO recommends evaluation on a collection of starter tasks (32 programmatic and 32 creative tasks).\n- While I understand that the authors didn\u2019t have the resources to train a RT-1 style baseline, would it still be possible to train an action decoder on top of the VLM to produce actions. I think having a strong RT-1 style baseline is very important to properly evaluate the question (2) mentioned in the paper \u2014 \u201cHow does PR2L compare to approaches that directly \u201cask\u201d the VLM to generate the best possible actions for a task specified in the prompt?\u201d\n\nOverall, I liked the main contribution of the paper. But I believe, in its current form, the evaluation in the paper is a bit weak and could be made more exhaustive (unseen tasks, more exhaustive MineCraft evaluation).\n\n\n**Update**: After reading the rebuttal, most of my questions are adequately answered. I still believe that directly training an action decoder (while keeping VLM frozen is a good baseline) and should be included in the paper. I also feel that the paper will be stronger if they include more exhaustive minecraft experiments (to check for generalisation on unseen tasks / objects) and more environments which are visually more complex like Habitat / AI2 Thor. Based on authors response, I am increasing my score."
            },
            "questions": {
                "value": "Apart from questions asked in the weaknesses section, I have additional questions: \n\n- The proposed architecture compresses the task-relevant features from the VLM into a single CLS token which can severely restrict the information available to the policy. While this may work for simpler environments like Minecraft which doesn\u2019t have a lot of clutter, it might not work for other environments / tasks (rearrangement tasks in indoor environments). Did the authors try an approach like Perceiver IO (Jaegle et al, 2021) which encodes N tokens to K (1\u2264K\u2264N) tokens?\n- I think the first question \u2014 \u201cCan promptable representations obtained via task-specific prompts enable more efficient learning than those of pretrained image encoders?\u201d is not really answered. It\u2019s unclear what efficiency mean (faster to train in FLOPS? faster to train measured by amount of training steps?). I think the paper can be made stronger by comparing training efficiency when using the proposed approach vs using VLM image-representations directly.\n- I didn\u2019t fully understand the various ablations done. Specifically, did the authors try just giving task context (and no auxiliary information)? Similarly, did the authors try giving just auxiliary information without giving task context.\n- While I don\u2019t expect this experiment to be performed for rebuttal, I really wish the authors evaluated their approach on tasks that are visually more complex (or the environments are more cluttered). For [e.g](http://e.gm)., using the approach to perform rearrangement tasks in indoor environments like Habitat / AI2 Thor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6621/Reviewer_sWjV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6621/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698871874923,
        "cdate": 1698871874923,
        "tmdate": 1700714952526,
        "mdate": 1700714952526,
        "license": "CC BY 4.0",
        "version": 2
    }
]