[
    {
        "id": "8BcsHa7SVM",
        "forum": "TyjVB2VIq3",
        "replyto": "TyjVB2VIq3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_T6bZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_T6bZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose to use a notion of system monotonicity to improve  reinforcement learning performance, in terms of  enhanced performance and reduced sample complexity.\n\nThe authors propose aPartial Order Representation (POR) framework, with the aim to improve the ability of reinforcement learning to capture the systems\u2019 monotonicity, resulting in valuable signals during training that can enhance performance and reduce sample complexity."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper has an interesting task, and proposes a particular environment, SoftGym, to study this task. There is clear novelty. The stated goals (at a high level) are clear."
            },
            "weaknesses": {
                "value": "I am concerned by the lack of technical depth to the notion of monotone systems, and in the claims of systems being monotone. It is simple to define counter-examples to the definition of monotonicity of systems, and hence everything else in the article falls apart. It is difficult to see how the experiments can be definitive if the basic definitions and the claims onf monotone behaviours of the systems for experiments are not clearly following the desired properties.\n\n \n \nThere is significant work on monotone systems---please provide the normal references, e.g., \n\nHirsch, M. W., & Smith, H. (2006). Monotone dynamical systems. Handbook of differential equations: ordinary differential equations, 2, 239-357.\n\nD. Angeli and E. D. Sontag. Monotone control systems. IEEE Trans. Autom. Control, 48(10):1684\u20131698, October 2003.\n\nThe definition of monotone system contradicts the standard definition.\n\"Monotonic Physical System. A discrete-time dynamical system is a monotonic physical system if there exists a function g : Rn \u2192 Rm, n > m, such that for any s_t \u2208 S, there exists an action at \u2208 A\nleads to g(st) >= g(st+1).\"\n\nIn the definition it notes \"for any s_t \u2208 S\": do you mean \"for *every* state\"?\nBy the given definition there may be only *one* state that is montone, and all others are non-monotone. This does not make sense.\n\nThe definition of monotone control system is much more precise:\n\nfor control space U, state space X, and parameter \u03a9, it must hold that:\n\nu1 \u2ab0 u2,\u03c91 \u2ab0 \u03c92 \u21d2 \u03d5(t, x1,\u03c91,u1) \u2ab0 \u03d5(t, x2,\u03c92,u2), for u1,u2 \u2208 U, \u03c91,\u03c92 \u2208 \u03a9, x1, x2 \u2208 X.\n\nThere is no proof or precise technical demonstration that the systems studied are in fact monotone: it is only a hand-waving argument.\n\n\"Through examination of these physical properties, it is clear that this system adheres to our definition of a monotonic physical system, as g(s) increases.\"\n\n---this is only a hand-waving argument---it is NOT clear to this reviewer. it is not even precise what is monotonic!\n\nIn sec. 4.3, I cannot understand the material following \"Non-monotonic Objective: Here, non-monotonic objective refers\"---very confusing!\n\nThe supplementary material is insufficient to provide a means to check the validity of the experiments. I need to see the underlying code to check if what is implemented is correct.\n\nThe results on sample efficiency are insufficient to justify the claims. For example, it is stated: \"For StraightenRope and FoldCloth tasks, POR has 1.04x and 1.02x improvement.\" This is really not sigificant. There is too little data on multiple experiments to make the strong claims noted."
            },
            "questions": {
                "value": "1. Can you compare your definition of monotonicity to that of the literature, and show that yours is correct?\n2. Can you provide precise specifications of the experiemental domains to show that they adhere to a correct definitoin of monotonicity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5891/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5891/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5891/Reviewer_T6bZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698425080012,
        "cdate": 1698425080012,
        "tmdate": 1699636624655,
        "mdate": 1699636624655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JsRgMOytel",
        "forum": "TyjVB2VIq3",
        "replyto": "TyjVB2VIq3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_Beac"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_Beac"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes `Partial Order Representation (POR)` framework, building around capturing monotonicity. The core idea is that many physics-based systems inherently display specific physical characteristics that are advantageous for learning algorithms. One such characteristic is the monotonicity of the environment. Monotonicity is defined as the consistent and gradual increase or decrease of a function or system's state variable based on the input. The task example of this characteristic considered here is when pouring water into a cup from a teapot, as the water is poured, the teapot's tilt angle consistently reduces, while the water volume in the cup correspondingly rises. Similarly, when straightening a rope by pulling its two ends, the distance between the ends consistently enlarges. \n\n### Methodology\nThe aim is to teach a system to understand the natural order of events or states in the environment, in a sense to know if one state comes before or after another. \n\n**Encoders**:\n   - **Online Encoder**: Processes the current observation/state.\n   - **Momentum Encoder**: Processes the next observation/state. It is called 'momentum' because instead of updating through regular backpropagation, it uses an exponential moving average (EMA) based on prior updates.\n\n**Feature Extractors**:\n   - After passing the observations through the encoders, we need to extract relevant features from the encoded states to further represent the monotonicity.\n   - The feature extractor, represented by the mapping function, does this by converting the latent representation (from the encoders) into some specific physical quantities that demonstrate monotonic behavior.\n\n**Data Generator and Classifier**:\n   - Once the features are extracted, they are processed by the Data Generator, which prepares them for the classifier. It creates pairs of data points from two consecutive time steps.\n   - The pairs are labeled either 0 or 1. The idea is to determine the partial order relationship between two states. \n      - A label of `0` might mean that the first state $\\( x_t \\)$ comes before the second state $\\( x_{t+k} \\)$.\n      - A label of `1` could imply the opposite, i.e., the state $\\( x_{t+k} \\)$ comes before $\\( x_t \\)$.\n\n**Optimization**\n- The model combines two loss functions the `POR` and `RL` loss.\n\n### Experiments\nExperiments include : `Softgym benchmark Pouring water from a teapot into a cup` & `Straightening a rope`. Baselines are `DrQ`, `SPR` and `CURL`."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The idea of learning the monotonicity property based on partial order of states is interesting and novel.\n- By using the classifier to predict these labels, the system learns the order or sequence in which states/observations follow each other in the environment. This is crucial for understanding monotonic systems, where some quantities consistently increase or decrease.\n- In essence, the system is being trained to recognize if one observation (or the physical quantity derived from it) naturally comes before or after another. This could be analogous to teaching a system to recognize that dawn comes before morning or that a glass being filled with water will have a higher water level as time progresses.\n- Ablation study of the monotonic objective and the correlation with the reward task seems convincing to me."
            },
            "weaknesses": {
                "value": "- All the baselines comparisons in this paper are from 2020 - There has been a ton of development in representation learning for RL, I\u2019m proposing a set of options just based on the recent SOT citations you can choose from to compare where appropriate (some are model-based, some are model-free). Please see references below.\n\n- Overall I like the core idea presented in this paper, but I think comparing against more recent baselines, can strengthen this paper. The paper as it stands is not ready for publication so I highly recommend the authors to consider comparing against SOTs. Happy to re-consider my score if this can be incorporated during rebuttal.\n\n[1] Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning - https://arxiv.org/abs/2107.09645\n\n[2] Masked World Models for Visual Control - https://proceedings.mlr.press/v205/seo23a/seo23a.pdf\n\n[3] Bigger, Better, Faster: Human-level Atari with human-level efficiency - https://proceedings.mlr.press/v202/schwarzer23a/schwarzer23a.pdf\n\n[4] Mastering Diverse Domains through World Models - https://arxiv.org/pdf/2301.04104.pdf\n\n[5] Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation - https://arxiv.org/pdf/2107.00644.pdf\n\n[6] Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation - https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.pdf\n\n[7] Visual Reinforcement Learning with Self-Supervised 3D Representations - https://arxiv.org/pdf/2210.07241.pdf\n\n[8] REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation - https://arxiv.org/pdf/2309.03322.pdf"
            },
            "questions": {
                "value": "- Figure 4 `DropFoldCloth` seems to be failing completely  ? I'm not sure what information I can gather from this clustered performance of all baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689437460,
        "cdate": 1698689437460,
        "tmdate": 1699636624548,
        "mdate": 1699636624548,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "npikOuCzdc",
        "forum": "TyjVB2VIq3",
        "replyto": "TyjVB2VIq3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_nyme"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_nyme"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new algorithm that aims to exploit any partial ordering properties that might be present in a task/environment (e.g. objects consistently fall due to gravity). The proposed algorithm aims to exploit such regularities in the dynamics and does so by introducing a new loss function to model-free RL via the use of a partial ordering classifier in a learned feature space. The resulting method shows consistent albeit modest improvements on several RL environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper motivates the core idea of the paper well. That said, I have some concerns about how broadly applicable the ideas are as mentioned in the weaknesses section.\n- The method shoes consistent, albeit modest, performance gains across several environments."
            },
            "weaknesses": {
                "value": "- It is unclear to me if the monotonic property is a property of a physical system (e.g. an object) or that of a specific task. For example, the authors provide the example of \"straightening a rope\" in Figure 1. If the goal is to tie a knot on the same rope, it is unclear if any monotonic property is preserved. Based on this, I find the concept of a \"monotonic physical system\" to be somewhat misleading, since it also depends on the specifics of the task. \n- In Eq (1), it seems odd to designate infinite probability to the deterministic transition. Perhaps it is better to define it as a delta distribution.\n- In Section 3.3, there is significant overloading of notation between \"notional\" Markovian state in a POMDP and the learned feature representation. For example, $s_t = f_o(o_t)$ would not be possible in a true POMDP, since to recover the state the entire history may be necessary. Furthermore, the state representation in which a partial ordering is preserved (e.g. the ends of a rope in the rope straightening example) may not be the representation a network actually learns. It is important to consistently use $z_t = f_o(o_t)$ to avoid confounding between $z_t$ (learned features) and $s_t$ (the actual Markovian state).\n- Why no comparisons to model-based algorithms (e.g. Dreamer, TD-MPC)? All the experimental comparisons are with model-free methods. It can be argued that POR is a pseudo-model-based method since it is trained to have a partial ordering between $x_t$ and $x_{t+1}$. It is well known that model-based methods are typically more sample efficient than model-free methods. Could it be that POR learns faster simply due to some pseudo-model-based elements in the algorithm? A concrete comparison to model-based methods would alleviate this concern."
            },
            "questions": {
                "value": "See weaknesses section. Please address all of them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699152521338,
        "cdate": 1699152521338,
        "tmdate": 1699636624435,
        "mdate": 1699636624435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0MgHHu8EE2",
        "forum": "TyjVB2VIq3",
        "replyto": "TyjVB2VIq3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_TCFd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5891/Reviewer_TCFd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework for improving the efficiency and performance of reinforcement learning (RL) algorithms by leveraging the deterministic and monotonic nature of many physical systems. The core concept is to exploit the inherent order in such systems, enabling more structured and informative state representations for RL tasks.\n\nCentral to the paper is the definition of monotonic physical systems, where state transitions are predictable and exhibit a clear, ordered relationship as a result of actions taken within the environment. The authors propose the Partial Order Representation (POR) framework which consists of:\n\nAn Online and Momentum Encoder that processes observations with the goal of reducing overfitting through an Exponential Moving Average (EMA) method.\nA Feature Extractor that identifies and extracts physical quantities with monotonic properties from the environment's states.\nA Data Generator that constructs training instances reflecting the monotonic relationships identified by the feature extractor.\nA Partial Order Classifier employs binary classification to determine the relative order of the extracted features, providing a basis for the RL algorithm to understand the consequences of actions in a structured manner.\nThe paper integrates these components within an RL setting, employing two distinct loss functions: one that guides the partial order classification (POR loss) and another that optimizes the RL policy (RL loss). The combination of these losses is tuned by a weighting factor, which balances the contribution of each to the model's learning process.\n\nThe framework's efficacy is demonstrated through experiments on the Softgym benchmark, chosen for its complex, yet monotonically physical, tasks. The results suggest that the POR framework significantly outperforms traditional RL methods in both performance and sample efficiency, highlighting the benefits of incorporating structured, physical knowledge into learning algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-motivated, identifying the need to exploit the structure within physical systems to improve RL outcomes.\n\nIt introduces a novel approach that integrates feature extraction, data generation, and partial order classification into the RL domain.\n\nThe empirical results suggest that the POR framework can lead to significant improvements in RL efficiency and performance."
            },
            "weaknesses": {
                "value": "The paper focuses on a very niche problem. I would advise the authors to give more examples of monotonic systems and show results on that. Perhaps a benchmark of similar tasks and not just the tasks from soft-gym environment. \n\nPerhaps a better way to tackle these tasks is imitation learning with techniques like Diffusion Policy (https://arxiv.org/abs/2303.04137). \n\nNo real world results is another issue.\n\nThe dependence on the accurate definition and extraction of monotonic features may limit the framework's generalizability."
            },
            "questions": {
                "value": "It would be beneficial to understand the sensitivity of the method to the choice of the weighting factor \u03bb in the combined loss function."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5891/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699375709354,
        "cdate": 1699375709354,
        "tmdate": 1699636624331,
        "mdate": 1699636624331,
        "license": "CC BY 4.0",
        "version": 2
    }
]