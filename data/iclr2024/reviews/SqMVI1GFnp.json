[
    {
        "id": "bJuZA5RG4v",
        "forum": "SqMVI1GFnp",
        "replyto": "SqMVI1GFnp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_mGDP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_mGDP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Lie neurons, an equivariant neural architecture that accepts semisimple Lie group elements as inputs. Lie neurons can be seen as an extension of vector neurons, which consider SO(3) as data transformation. The authors handle mathematical tools such as Lie algebra, adjoint representation, etc to ensure the equivariance on more general class of groups. The proposed network is evaluated on three synthetic tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The generalization of vector neurons from SO(3) to a semisimple Lie group is a novel contribution.\n\nThe paper is self-contained. It gently introduces the basics of Lie algebra necessary to understand the methodology."
            },
            "weaknesses": {
                "value": "Evaluation is the weakest point of this paper. There are three tasks: invariant regression, equivariant regression, and classification of polyhedrons. The first two tasks use synthetic data. Those are useful to know that the model is working as intended, but we cannot know the performance of real problems. The third task is more on the application side, but still synthetic and too clean (e.g. noise free). \n\nAnother concern is the range of applications. The problem setting --- the transformation itself is a data point --- is interesting, but it is not clear what kind of real problems can be formulated in that way. In other words, it's not straightforward to recognize the benefits of this framework from the practitioner's view."
            },
            "questions": {
                "value": "I don't wholly understand the setting of platonic solid classification.  \n1. The homography matrix H seems to be an input, but how is it computed from an image? \n1. Also, the number of input points (the possible face pairs) varies depending on the platonic solids. How can you deal with the difference?\n1. What is the network architecture used in this experiment?\n\nHow can we find $Adm$? For a given G (or its algebra g), is there any systematic way to calculate it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698232532232,
        "cdate": 1698232532232,
        "tmdate": 1699637006727,
        "mdate": 1699637006727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QBlsMkRECa",
        "forum": "SqMVI1GFnp",
        "replyto": "SqMVI1GFnp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_Wrrb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_Wrrb"
        ],
        "content": {
            "summary": {
                "value": "Inspired by the work on vector neurons, this paper proposes neural network layers which capture equivariance to the adjoint action of a Lie group on its Lie algebra, that is, action by conjugation. Such a network would take elements of the Lie algebra as input. The novel layers include a linear layer, two different nonlinearities, and a pooling layer. The nonlinearities draw on the notion of the Killing form, requiring it to be negative definite and thus restricting these particular layers to semisimple Lie algebras. The paper then runs three different synthetic experiments, one learning an adjoint action-invariant function, one learning an adjoint action-equivariant function, and one taking as input transformations between faces of 2-dimensional projections of 3-dimensional solids and trying to predict the solid. The paper compares networks with various combinations of their proposed layers with vanilla MLPs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Interesting ideas and interesting math:** The ideas contained in this paper are interesting, particularly envisioning neural networks as functions on Lie algebras. The reviewer appreciated central elements (such as the Killing form) from Lie theory cleverly integrated into neural network design. Most of the mathematical constructions seem relatively well-thought out (though this reviewer did not check the equivariance or invariance on each). Further, the work appears to be a nice generalization of a prior approach.\n\n**Good pacing and background:** Papers on equivariance in deep learning can be difficult to read since they draw on two different technical domains and the conversion of constructions and notation from representation theory to deep learning is fraught with peril. While the reviewer believes that some aspects of the writing deserve further work, the pacing (not too fast) and amount of background (enough for someone with sufficient mathematical maturity to understand) stood out as strengths. Because of this, the work was enjoyable to read."
            },
            "weaknesses": {
                "value": "**Motivation:** From a purely mathematical perspective, the problem is interesting. However, this reviewer\u2019s estimation is that to have a substantial impact, equivariant research needs to have some application to real data. This paper does not really have any experiments on real data, nor did this reviewer see a clear path towards an application. The Platonic solids experiment seemed to be aimed at real applications, but it was a little unclear to this reviewer what these would be. It would be helpful to outline how this type of problem arises in the real world even if no experiments are run with real data (in particular, if homography modeling is a real problem that one can imagine this network architecture being applied to, perhaps more details could be given).\n\n**Limited experiments:** This reviewer has two concerns about the experimental section. The first was the lack of analysis and the second was the lack of strong baselines and experimental breadth. As this is a novel architecture, it would be useful to see analysis of model performance beyond a single metric. For example, training curves and convergence, confusion matrices, etc. This would allow the reader to better understand what the model is and is not actually good at. For instance, empirically, equivariant models are often more data efficient, is that the case here? It might also make sense to measure the extent to which the proposed model is actually invariant to the action that it is claimed to be invariant to. Of course, no paper can run all possible experiments, but some additional experiments would be useful. This reviewer felt that they did not have a strong feel for the method, despite reading through three different experiments.\n\nIt would also improve the work if stronger baselines were compared against. For instance, what happens if we try to use some type of adjoint-action augmentation when training the vanilla MLP? It would also be interesting to see how the model performs with different Lie algebras, are some Lie algebras harder to learn on than overs? How does performance scale with Lie algebra size?\n\n**Writing and figures:** While the reviewer certain aspects of the writing, there are other ways that the the paper could be made more clear.\n- **Moving between elements of the Lie algebra and corresponding vectors for processing by the model:** One of the aspects of the paper most likely to cause the reader confusion is the process of moving between the Lie algebra $\\mathfrak{g}$ itself and the corresponding vector space $\\mathbb{R}^m$. As is pointed out in the Nitpicks section, one problem is that $E_i$ is never defined other than saying it is the image of $e_i$ under the map $\\wedge$. But then the definition of $\\wedge$ uses $E_i$. More broadly, it might be worth investigating sweeping the distinction between the Lie algebra and its underlying vector space under the rug to cut down on extra notation.  \n- **Figure 1:** This reviewer found Figure 1 hard to understand. It might be helpful to somehow include the model in the schematic so it is more clear what is the input/output, group action on the input vs group action on the output, etc.\n\n### Nitpicks\n\n- $E_i$ is never defined beyond $E_i = (e_i)^\\vee \\in \\mathfrak{g}$. It would be helpful to say something more explicit.\n- $\\log$ isn\u2019t actually used in the paper? Perhaps (2) could be removed.\n- For equalities in (13), it might make sense to state the reason all this works (the matrix $Adm_a$ and $W$ are being multiplied from different sides?\n- The experimental results should have confidence intervals attached to them."
            },
            "questions": {
                "value": "The reviewer asked a number of questions in the prior sections. These include:\n- Thoughts on how the proposed architecture would scale, both to higher dimensional Lie algebras and larger volumes of data.\n- More details on convergence, data efficiency, and other auxiliary metrics that can give a better picture of these models.\n- Discussion of what types of real data this architecture might be applied to."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729388292,
        "cdate": 1698729388292,
        "tmdate": 1699637006565,
        "mdate": 1699637006565,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d6rc0GHrRn",
        "forum": "SqMVI1GFnp",
        "replyto": "SqMVI1GFnp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_272k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_272k"
        ],
        "content": {
            "summary": {
                "value": "The authors build an architecture which takes as input elements in the Lie algebra vector space of a Lie group, and is equivariant to the adjoint representation of the Lie algebra. As building blocks, they use nonlinearities based on the Killing form and the Lie bracket, as well as ordinary linear layers. Their architecture generalizes vector neurons beyond the Lie algebra of SO(3). They obtain higher test accuracies than ordinary MLPs on three synthetic tasks, including a homography transform task on Platonic solids."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is the first to prescribe Lie algebra equivariance for a neural network. It applies to all semi simple Lie groups\u2019 algebras, which is general, and the resultant architecture is exactly equivariant. The use of the Killing form in a neural architecture seems to be novel, as is the explicit use of the Lie bracket. The experimental results are better than the baseline MLPs."
            },
            "weaknesses": {
                "value": "1. The problem of Lie algebra equivariance does not seem at all well-motivated by applications. This is evidenced by the datasets used in experiments: the first two are quite contrived, and the third one, although geometric in nature, remains synthetic. It is not clear in practice when one genuinely has input data that lives in a Lie algebra, rather than in the vector space associated with an ordinary group representation.\n2. There is no end-to-end description of the architecture. Also, the architecture is not motivated in a particularly natural or pedagogical way, e.g. with a discussion of universality, a comparison to other approaches one might take for linear equivariance, etc. Generally speaking, the presentation could be improved \u2014 to include more details, to better motivate the equivariance problem via applications, and conditioned on this setting, to better motivate this particular architecture.\n3. The only baseline included in experiments is an MLP. It would make sense to include an architecture equivariant/invariant to all linear transformations, for example, or an MLP augmented with Lie algebra transformations. On its own, the MLP seems like a weak baseline.\n\nAs a minor comment, I found the analogy made to vector neurons more confusing than enlightening. It would be helpful to include a self-contained description of vector neurons, to make this comparison more clear. But overall, vector neurons are just one example of an equivariant architecture whose input representations transform according to rotations. Another important feature of vector neurons is that one representation is maintained for each point in the input point cloud, but this does not appear to have an analogue here (please correct me if I am misunderstanding)."
            },
            "questions": {
                "value": "1. What is the wedge notation ^ used in equation 4? Is this defined anywhere?\n2. I don\u2019t understand equation 10, defining the Killing form. If I understand correctly, $ad_X$ is a function from the Lie algebra to itself, sending an input M to [X, M] = XM - MX. $ad_X \\circ ad_Y$ is then also a function from the Lie algebra to itself. What is the trace of this function? Should we imagine setting a basis for the finite-dimensional Lie algebra, constructing the matrix that represents $ad_X \\circ ad_Y$ based on this basis, and computing the trace (which I imagine would actually be invariant to the choice of basis)?\n3. Is the proposed architecture universal over continuous Lie algebra equivariant functions?\n4. The authors refer to the input data as \u201ctransformations\u201d, but also as elements of the Lie algebra. Why is it helpful to conceptualize elements of the Lie algebra as transformations? Does this interpretation still make sense for Lie groups which are not matrix Lie groups?\n5. How many parameters are in the baseline MLP, compared to the LN-LR and LN-LB architectures in the experiments?\n6. As far as I understand, the adjoint operation is a particular subset of general linear transformations of the inputs. However, the linear layer is equivariant to any linear transformation, not just the adjoint operation, correct? So to clarify, are the Killing form and the Lie bracket operations special nonlinearities, in that they are only equivariant to the adjoint operation (but not to more general linear transformations)? It looks to me like the Lie bracket nonlinearity in equation (16) would be invariant general unitary transformations, but I do not understand the wedge notation, so this may not be the case.\n7. Why is the adjoint operation meaningful to study? Does it arise often in applications? And can it not be expressed as a subgroup of GL(d), where d is the dimension of the Lie algebra?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818268237,
        "cdate": 1698818268237,
        "tmdate": 1699637006439,
        "mdate": 1699637006439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OV2tHs1ETi",
        "forum": "SqMVI1GFnp",
        "replyto": "SqMVI1GFnp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_fTEN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8125/Reviewer_fTEN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an equivariant architecture that operates on Lie algebras corresponding to transformations between vector spaces. The main contribution is the introduction of two kinds of nonlinearities: one using the Killing form of the Lie algebra and the other using the commutator. The approach is tested on two types of synthetic benchmarks as well as a classification task involving 3D geometries derived from 2D projections. The model is shown to outperform non-equivariant baselines on these tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of treating elements of Lie algebras (beyond $\\mathfrak{so}(3)$) as input seems novel and original, to my knowledge.\n- The approach taken to construct the nonlinearities is straightforward and sensible.\n- Empirical experiments appear to confirm the usefulness of such architectures."
            },
            "weaknesses": {
                "value": "- The motivation for the work is somewhat unclear. While there is one concrete example involving camera views, I would like to see additional examples to appreciate the broad applicability of this approach, even if only as citations in the introduction.\n\n- In many useful applications, the mapping between vector spaces is not defined on the canonical basis but through representations of the group. Finding bases for these representations is a challenging task, and I think a discussion on this topic is missing.\n\n- The fact that the Killing form for non-compact groups is not an inner product makes it ill-suited for measuring distances in the nonlinear activation. I am concerned about the potential numerical stability issues this could cause.\n\n- The only baselines presented in the paper are standard MLPs. Although I understand that this is primarily a theoretical work, I would expect to see at least one strong baseline in one of the tasks."
            },
            "questions": {
                "value": "- I am surprised by the invariant error in Table 1 for the LN-LR model. This seems to be an order of magnitude beyond floating-point precision. Do you have any idea where this discrepancy originates?\n\n- What is the insight behind LN-BRACKET, other than that it is an easily definable equivariant operation? It feels somewhat arbitrary, and no information is provided about the universality of a model containing this nonlinearity.\n\n- Related to the weaknesses, can you identify a practically useful application for this approach? Such an application should have interesting baselines for comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8125/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8125/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8125/Reviewer_fTEN"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8125/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833280900,
        "cdate": 1698833280900,
        "tmdate": 1699637006298,
        "mdate": 1699637006298,
        "license": "CC BY 4.0",
        "version": 2
    }
]