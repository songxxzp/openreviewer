[
    {
        "id": "lqNVvE074o",
        "forum": "g4I3Wzv3fw",
        "replyto": "g4I3Wzv3fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_ZA2E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_ZA2E"
        ],
        "content": {
            "summary": {
                "value": "Finding the optimal robust policy in Reinforcement Learning (RL) is often formulated as a two-player game and is related to the dynamic model of transition function uncertainty. In this model, the environment dynamics are allowed to change at every time step. In this paper, the authors consider the static model where one needs to provide robustness to a span of static transition models. While state-of-the-art robust RL algorithms build upon the dynamic uncertainty model, in this paper the authors show that the static uncertainty model can be solved by solving a set of non-robust problems. They have proposed a generic meta-algorithm IWOCS to identify the worst-case transition model in an incremental fashion. A deep RL version of the IWOCS is also proposed along with a comparison with existing robust RL algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors show that the static uncertainty model can be solved by solving a set of non-robust problems. They have proposed a generic meta-algorithm IWOCS to identify the worst-case transition model in an incremental fashion. A deep RL version of the IWOCS is also proposed along with a comparison with existing robust RL algorithms. The analysis looks fine. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "Overall, although the authors have proposed a new framework to tackle robust MDP problems under static uncertainty, the motivation is not clear. Also, the proposed methodology does not seem to provide any significant advantage compared to the robust methods under the dynamic uncertainty model."
            },
            "questions": {
                "value": "My concerns are as follows:\n\n1.\tWhy is the dynamic model not a generalization of the static model? In other words, if we assume a dynamic model while proposing an RL scheme and in reality, the environment behaves like a static model, why would the proposed RL scheme perform badly? In fact, (Lemma 3.3 Iyengar 2005) echoes that for stationary policies, the performances should be identical.\n\n2.\tIt seems, in an infinite-horizon problem as considered by the authors, the dynamic uncertainty model makes more sense than the static uncertainty model as the transition function can be piecewise static over the entire horizon. The rationale behind the static model being a better choice than the dynamic model from a practical viewpoint, needs more clarity.\n\n3.\tIn this paper, the authors have considered stationary policies. In this case, the robust value iteration framework for the dynamic model will work. What are the additional advantages that the framework proposed by the authors provides?\n\n4.\tThe authors claim that the proposed framework avoids the complexity of the dynamic model. However, it is established that the additional complexity due to robustness in robust value iteration is just $O(\\log(1/\\epsilon)$ for accuracy in the order of $\\epsilon$. On the other hand, solving a sequence of classical MDP problems as considered by the authors could be really slow to converge depending on scenarios.\n\n5.\tHow to construct the set $\\mathcal{T}_i$? This is not clear. It may be harder to construct than constructing the rectangular $(s,a)$ uncertainty set.\n\n6.\tFor different $(s,a)$ pair, the $argmin$ operation for $Q_i(s,a)$ may give us different $j$. How to tackle this problem? Can we still write the Bellman equation for $Q_i(s,a)$?\n\n7.\tAuthors are requested to explain how they compute $l(\\hat{T_j}(s,a);s\u2019)$ according to the formula given. It is not clear to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Reviewer_ZA2E"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729697713,
        "cdate": 1698729697713,
        "tmdate": 1699636587382,
        "mdate": 1699636587382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FvbKgUXoHW",
        "forum": "g4I3Wzv3fw",
        "replyto": "g4I3Wzv3fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_cHJH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_cHJH"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the problem of robust reinforcement learning (RL), where the transition dynamics of the test and training environments can differ. Specifically, it formulates this problem as a two-player zero-sum game between an adversary that chooses the worst-case transition functions and the learner that selects the robust policy. Rather than assuming the adversary can change the transition function at each timestep (the dynamic model), the authors adopt the static model and propose IWOCS. This incremental search algorithm constructs a discrete set of worst-case transitions to solve the zero-sum game. They implement IWOCS for deep RL settings and demonstrate its effectiveness across various standard robust RL benchmarks, showing it can outperform other state-of-the-art methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's extensive empirical evaluations are a major strength, demonstrating the proposed IWOCS algorithm's superior performance compared to state-of-the-art methods like M2TD3 on MuJoCo benchmarks. I find the following points as the strengths of the paper:\n\n* Evaluation of both worst-case and average-case performance, which highlights IWOCS's ability to balance robustness and generalization.\n* Handling of partial state space coverage via predictive coding, which enables switching to randomization in cases where the worst-case Q-values are not valid.\n* Use of separate regularized and unregularized Q-networks.\n* The empirical analysis of the convergence of IWOCS.\n* I also appreciate reporting of total clock time, which is essential in the assessment of efficiency.\n\nIn summary, I think the proposed incremental construction of worst-case transition sets is novel and, given the strong empirical results, makes a potential contribution to robust RL. However, I have major concerns about the soundness of their method and clarity of the writing, which I will elaborate on in subsequent sections."
            },
            "weaknesses": {
                "value": "Before discussing my concerns about the clarity and quality of the writing, a major factor that makes a fair evaluation of the paper hard, let me discuss my understanding of the incremental worst-case search and why I think some of the claims made in the paper might be incorrect. To make the writing simple and concrete, I will re-write the IWOCS algorithm for a general minimax problem of\n\\begin{align}\n\\min_{x \\in \\mathcal{X}} \\max_{y \\in \\mathcal{Y}} f(x, y)\n\\end{align}\nfor some arbitrary function $f$ and sets $\\mathcal{X}$ and $\\mathcal{Y}$. For this problem, IWOCS becomes the following algorithm:\n\n*input*: $x_0$, max number of iterations M, tolerance $\\epsilon$\\\n*for* i = 0 to M:\n1. Find $f^*(x_i) = \\max_{y \\in \\mathcal{Y}} f(x_i, y)$\n2. Define $\\mathcal{X}_i = \\\\{x\\_j\\\\}\\_{j \\leq i}$\n3. Define $x^\\star_i = \\arg\\min\\_{x \\in \\mathcal{X}_i} f^*(x)$\n4. Define $y_i = \\arg\\max\\_{y \\in \\mathcal{Y}} f(x^\\star_i, y)$\n5. Find $x_{i+1} = \\arg\\min\\_{x \\in \\mathcal{X}} f(x, y_i)$\n6. *if* $|f(x_{i+1}, y_i) - f(x^\\star_i, y_i)| \\leq \\epsilon$ *then*: *return* $y_i$, $x_{i+1}$.\n\nOne claim that's made multiple times throughout the text (e.g., in the *incrementally solving the static model* paragraph of section 2 and *exploring the variance and convergence of IWOCS* paragraph of section 5) is that the values of $f^*\\_{x^\\star_i}$ decreases monotonically and the algorithm is guaranteed to converge. Consider the following two cases:\n1. $x\\_{i + 1} \\in \\mathcal{X}\\_i$: In this case, $x^\\star_{i+1} = x^\\star_{i}$ as $\\mathcal{X}\\_i = \\mathcal{X}\\_{i+1}$. Therefore, $f^*\\_{x^\\star_i} = f^*\\_{x^\\star_{i+1}}$ and the algorithm is stuck.\n2. $x\\_{i+1} \\not\\in \\mathcal{X}\\_i$ but $x^\\star\\_{i+1} \\in \\mathcal{X}\\_i$: This case also results in a similar behavior of the algorithm. \n\nThe paper needs to address such scenarios more concretely (both in theory and practice) and provide evidence on why this will not happen. In particular, the idea of constructing incremental sets of worst-case models can also be applied to any two-player zero-sum game (similar to the formulation I wrote above). If this is already done in the literature, I would question the novelty of the method. If not, what are the advantages of this approach to the more classic methods, such as alternating optimization? A more thorough analysis is needed.\n\nBesides the soundness of the algorithm, I have some concerns regarding the writing, which I mention as follows:\n1. In the abstract, the authors claim to suggest an analysis of why solving the static model under mild hypotheses is a reasonable endeavor. However, they only provide shallow references to previous works (e.g., Iyengar, 2005 or Wiesemann et al., 2013) for this analysis. The paper is not self-contained, and the claims about the no duality gap or the equivalence of optimal policy under the static and dynamic cases are not fully supported. I expect a concrete and thorough overview of the conditions needed and the previous results (at least in the appendix section).\n2. The contributions of this paper are not clearly distinguished from previous work. Is this the first paper considering solving the static model instead of the dynamic case? Is the incremental construction of the worst-case models the main contribution? Or has the idea been previously used for minimax games? If not, I think the contribution not only applies to robust RL but any minimax problem, and the authors should clarify this.\n3. Most of the writing is unclear in terms of the introduction of the notations and concepts in the main text. In particular, section 4 can benefit greatly from re-writing, as in the current version, the notations are defined as part of the text and not in a separate place (e.g., the definition of $\\mathcal{T}\\_i$, $Q_i$), and it requires the reader to pass through the text multiple times to understand the algorithm and notations. The authors also use multiple similar notations such as $J$, $V$, $\\pi_{T_i}$, $\\pi_{i}$ without clarifying.\n4. Furthermore, not much discussion is provided for solving the minimization problem (finding the worst-case transition functions). This is especially important as finding the worst-case static model is more difficult than the worst-case dynamic one. The authors avoid discussing this problem in detail and refer only to evolutionary strategies such as CMA-ES to solve it.\n5. Finally, in the empirical evaluation section, the idea of training all the algorithms for the same number of steps does not necessarily provide a fair comparison. The results would be more compelling if the authors plotted all methods until their convergence."
            },
            "questions": {
                "value": "Besides the question I asked in the previous section:\n1. What is the concrete definition of $\\mathcal{T}$ in the experiments? \n2. As far as I know, meta-algorithm usually refers to algorithms that output another algorithm. Why do you name your algorithm meta-algorithm?\n3. In Table 3, can you show that the values of $4.0$ and $7.0$ are the true worst-case parameters in the Halfcheetah 2 environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Reviewer_cHJH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863022486,
        "cdate": 1698863022486,
        "tmdate": 1699636587237,
        "mdate": 1699636587237,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EHBaaURvqz",
        "forum": "g4I3Wzv3fw",
        "replyto": "g4I3Wzv3fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_SYvP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_SYvP"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of creating robust control policies in reinforcement learning (RL) that ensure consistent performance across different environments. It introduces the notion of a two-player game, rooted in the dynamic model of transition function uncertainty, but emphasizes the practical need for robustness in the face of static transition models. Despite the static model's increased difficulty, the paper proposes a reconsideration and introduces a meta-algorithm called IWOCS. This algorithm incrementally identifies worst-case transition models to guide the search for a robust policy. The discussion around IWOCS suggests a new way to separate policy optimization from adversarial transition functions, presenting fresh analytical perspectives. The passage concludes by mentioning a deep RL version of IWOCS, demonstrating its competitiveness with current algorithms on standard benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of using static model to solve robust MDPs is new and interesting.\n\n2. The paper demonstrates the competitive performance of IWOCS  through comprehensive experiments."
            },
            "weaknesses": {
                "value": "1. Although the paper has gained its intuition from the theory, it is better to formulate a simple theoretical analysis (convergence or sample complexity bound) of the algorithm. \n\n2. The theoretical concepts in this paper are not very readable."
            },
            "questions": {
                "value": "1. The paper's intuition of the algorithm comes from combining the static and dynamic models equivalence\nand the no-duality gap condition, does these assumptions hold in the experiments the paper proposed?\n\n\n2. Since the paper shows the competitive performance with respect to traditional methods, can authors give some reasons or conjectures about the advantages of IWOCS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Reviewer_SYvP"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5649/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699563902729,
        "cdate": 1699563902729,
        "tmdate": 1699636587139,
        "mdate": 1699636587139,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XAM7Pe0lxm",
        "forum": "g4I3Wzv3fw",
        "replyto": "g4I3Wzv3fw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_KrKt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5649/Reviewer_KrKt"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the *static* uncertainty model for robust RL and proposes a novel robust RL algorithm with empirical evaluation. It highlights the necessity to mitigate the conservativeness of policies yielded by solving *dynamic* uncertainty models, and suggests to revisit the *static* uncertainty model despite its intractability for exact solutions. Then it designs an incremental worst-case search algorithm, namely IWOCS, that approximates the solution by alternatingly construct a incremental discrete ambiguity subset $\\mathcal{T}_i$ and updating the optimal robust policy with respect to $\\mathcal{T}_i$. The algorithm is then implemented via deep neural networks and evaluated against state-of-the-art baseline algorithms on a few popular benchmarks, showing a considerable performance improvement in certain environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provides an interesting new perspective to understand the solution concepts for robust MDPs. It is arguable that *static* uncertainty models are more realistic and useful once they can be efficiently computed.\n2. The algorithm is novel, simple and intuitive. It is actually surprising to see that a straightforward approximation is able to yield satisfactory performance in some cases.\n3. The empirical evaluation results display sufficient workload. Detailed results are honestly included without cherry-picking. The discussion provides some intuition into understanding its performance, as compared to the baselines.\n4. Overall the presentation of the paper is fine, with no obvious obstruction to first-time readers."
            },
            "weaknesses": {
                "value": "1. The soundness of the meta-algorithm needs more justification.\n    * The current way it appears in the paper leaves readers the impression that the algorithm (esp. the idea of incremental ambiguity subset) comes solely from a simple heuristics, without *any* theoretical guarantee or *adequate* empirical evidence.\n    * For theoretical guarantee, it is advisable to include some convergence guarantees that *read like* \"$\\mathcal{T}_i \\to \\mathcal{T}$\". Though convergence of subset sequence is not reasonable in any sense for this setting, one can at least show, for example, that each time a new model is selected from $\\mathcal{T} \\setminus \\mathcal{T}_i$, and maybe further, that the robust value with respect to $\\mathcal{T}_i$ converges to that of $\\mathcal{T}$ (at a quantified rate).\n    * For adequate empirical evidence, one basically needs to show the above properties with experiments on a range of different environments. Currently the motivating experiment is not motivating at all, and fails to show any sign of convergence.\n2. There are more to do for the empirical evaluations.\n    * The deep RL algorithm is not described in details, making the paper not self-contained. It is especially unclear how the evolution algorithm is implemented without referring to the original paper. Further, the use of evolution algorithm needs more justification, since it barely has any optimality guarantees.\n    * The experiment setup is suspicious. Usually people train the networks to convergence or to a certain average reward level. Cutting off experiments with different algorithms at the same number of time steps doesn't seem right.\n    * It is evident that the proposed algorithm, on top of SAC, would be very time-consuming due to the incremental nature. Although efficiency (in terms of wall-clock time) is briefly touched upon in Appendix F, it is advisable to show results like time-to-performance.\n    * The analysis can be enriched by providing more discussion on the reason why IWOCS performs significantly worse than other non-robustness-oriented algorithms, and perhaps further, what kind of environments are the most compatible with IWOCS.\n    * It is advisable to include ablation studies (or at least comparison among different options) for design components like predictive coding and evolution algorithms.\n3. There are a few typos that need to be fixed. Besides, the important definitions and formula shall be highlighted in display mode. Writing everything in a single paragraph makes it hard for readers to grasp the key points at first glance and to review the important sentences."
            },
            "questions": {
                "value": "See the weakness part above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5649/Reviewer_KrKt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5649/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699744833119,
        "cdate": 1699744833119,
        "tmdate": 1699744833119,
        "mdate": 1699744833119,
        "license": "CC BY 4.0",
        "version": 2
    }
]