[
    {
        "id": "J2wCQQGlkj",
        "forum": "gS0XOu0JKs",
        "replyto": "gS0XOu0JKs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_9t2n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_9t2n"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an innovative uncertainty-aware in-context learning framework to enable LLMs to enhance or reject their output in response to uncertainty. Unlike human-defined methods that struggle with setting precise correctness thresholds, this framework introduces uncertainty information as an intermediary variable to influence the model's behavior. The approach involves fine-tuning the LLM using a calibration dataset and aims to improve responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. The evaluation confirms that the logit output values of the LLM partially reflect inherent uncertainty, and the model autonomously recognizes uncertainty, leading to improved responses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The studied problem of reliability of LLMs is important and holds significant importance in applying LLM to real-life scenarios.\n\n2. The authors validate the effectiveness of the uncertainty-aware in-context learning framework."
            },
            "weaknesses": {
                "value": "1. The experiments are only conducted on one dataset, limiting the generalization of the proposed approach.\n\n2. It is unclear how the 'calibration dataset' is created in section 2.3.1. The way to determine the range of uncertainty (ranging from 1 to 10, 1 to 100, and 1 to 1000) is arbitrary without sufficient empirical or theoretical support.\n\n3. Some experiment details are not enough. The authors only compare the model with vanilla fine-tuning. Can some of the methods mentioned in the paper [1] be used as baselines? \n\n4. There are some typos in the paper. For example, in section 2.1, 'We will elaborated this part' should be 'We will elaborate this part'. \n\n5. Missing references. Many papers discussing on LLM uncertainty are missing such as [2,3,4]. \n\nReferences:\n\n[1] Xiong, Miao, et al. \"Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs.\" arXiv preprint arXiv:2306.13063 (2023).\n\n[2] Lin, Stephanie, Jacob Hilton, and Owain Evans. \"Teaching models to express their uncertainty in words.\" arXiv preprint arXiv:2205.14334 (2022).\n\n[3] Kuhn, Lorenz, Yarin Gal, and Sebastian Farquhar. \"Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation.\" arXiv preprint arXiv:2302.09664 (2023).\n\n[4] Quach, Victor, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. \"Conformal Language Modeling.\" arXiv preprint arXiv:2306.10193 (2023)."
            },
            "questions": {
                "value": "1. In section 3.1, why do you set the temperature to 0.001?\n\n2. Could you provide additional insights into the reasons behind the procedure for constructing the calibration dataset?\n\n3. Would it be possible to expand the scope of datasets used in your experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732753577,
        "cdate": 1698732753577,
        "tmdate": 1699636276253,
        "mdate": 1699636276253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "29CpDnL0ql",
        "forum": "gS0XOu0JKs",
        "replyto": "gS0XOu0JKs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_JPa6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_JPa6"
        ],
        "content": {
            "summary": {
                "value": "The paper describes an uncertainty aware in-context learning (ICL) framework to help fine-tune LLMs. The ICL framework together with different uncertainty estimation methods are used to assess correctness of the LLM's responses and empirically show improvements in overall performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea presented is simple and interesting. Incorporating uncertainty does show significant improvements when compared to the base LLM model or its fine-tuned variant."
            },
            "weaknesses": {
                "value": "1) The paper is difficult to read and understand. It would benefit from a few more rounds of writing revisions. \n\n2) The paper lacks methods to compare against. While the proposed method does better than the base model and its fine-tuned version, it would good to include how close or better the proposed approach is with respect to SOTA methods on the target dataset.\n\n3) The authors do not discuss about granularity of the uncertainty scores and its impact on performance which they said they did in the experiments section on page 4.\n\n4) Please correct typos and grammatical mistakes. Certain abbreviations are referenced before they are defined and some aren't defined at all, e.g., SFT in Figure 1 on page 2."
            },
            "questions": {
                "value": "Please see above comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820857172,
        "cdate": 1698820857172,
        "tmdate": 1699636276179,
        "mdate": 1699636276179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cYgBLHh7LK",
        "forum": "gS0XOu0JKs",
        "replyto": "gS0XOu0JKs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_4TCY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3277/Reviewer_4TCY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for improving the reliability of large language models (LLMs) by leveraging uncertainty-aware in-context learning. It proposes a supervised fine-tuning procedure that enables the LLM to adaptively adjust its responses based on the uncertainty scores, either by refining incorrect answers or by refusing to answer when the model lacks relevant knowledge. Extensive experiments on the SciQ dataset demonstrate the effectiveness of the proposed framework in enhancing the accuracy and reliability of LLM responses. The paper also analyzes the relationship between uncertainty and correctness, and shows that the logit output values of the LLM partially reflect inherent uncertainty."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper proposes a remarkably simple method and is easy to understand. The technical details are clearly written and easy to follow.\n- The paper presents evidence to two conclusions: 1) the logarithmic output values of the LLM partially reflect inherent uncertainty. 2) finetuning on calibration resulting in improved response accuracy."
            },
            "weaknesses": {
                "value": "- Evaluation lacks comparison with other uncertainty estimation methods [1,2]. It only compares different variants of its own framework.\n- The claim \"multi-inference-based uncertainty estimation methods need to get the answer set before calculating the uncertainty.\" is wrong. multi-inference-based uncertainty estimation methods (e.g. [3]) do not need correct answer. In fact, any uncertainty estimation method that requires the gold answer set is useless.\n- It is not clear to me how uncertainty-aware correction works in SFT setting. It seems to me that the model never sees the uncertainty score during finetuning but is required to utilize the scores during inference. The discrepancy between finetuning and inference is not intuitive and not supported by empirical results.\n\n[1] Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs\n[2] Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback.\n[3] Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698990822258,
        "cdate": 1698990822258,
        "tmdate": 1699636276095,
        "mdate": 1699636276095,
        "license": "CC BY 4.0",
        "version": 2
    }
]