[
    {
        "id": "HcFRW9Ucwi",
        "forum": "9grjdFDiAj",
        "replyto": "9grjdFDiAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_PopN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_PopN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to study the stability of SGD from a probabilistic viewpoint. The argument is that the existing stability analysis based on the convergence as measured by the moment is not sufficient to explain the dynamic behavior of SGD. The main results show that the probabilistic stability of SGD in high-dimension is equivalent to a condition on the sign of the Lyapunov exponent of the SGD dynamics. The derived results provide a new perspective to understand why SGD selects solutions with good generalization from an enormous number of possible solutions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a different perspective to understand the dynamic behavior of SGD, which differs from the existing explanation that SGD prefers flatter solutions. The paper proposes probabilistic stability as a weaker condition than the moment stability. Furthermore, the connection of the probabilistic stability to the Lynapunov exponents is interesting."
            },
            "weaknesses": {
                "value": "It seems the analysis is not quite rigorous, and I found several gaps in the theoretical analysis.\n\nThe Lyapunov exponent seems to be a conservative parameter as it needs to take the maximum over all initializations. It seems that this quantity cannot fully illustrate the behavior of SGD since the initialization also has a large impact on the behavior of the algorithm, which cannot be explained by the Lyapunov exponent."
            },
            "questions": {
                "value": "Above Eq (16), the paper shows that algorithmic stability holds if $\\lambda=1/x_i^2$. Then, Eq (16) says that the largest stable learning rate is $1/x_{min}^2$. Should the largest stable learning rate be $1/x_{max}^2$ since we need the algorithm to be stable for any chosen example, and therefore should choose the smallest step size?\n\nAbove Eq (37), the paper shows that $m=tE_x[\\log|1-\\lambda h_t|]$. However, this identity only holds if $h_1=h_2=\\ldots=h_t$. In this case, the matrix $\\hat{H}(x)$ should remain the same over the optimization process. This seems to be a strong requirement. \n\nI cannot see how the argument below Eq (39) holds. That is, how to get the convergence by the law of large numbers?\n\nIn Eq (46), the paper uses the identity $E[\\hat{H}_t\\theta_t]=E[\\hat{H}_t]E[\\theta_t]$. This identity holds if $\\hat{H}_t$ and $\\theta_t$ are independent. This also seems to be a restrictive condition.\n\nTheorem 3 requires $X_i$ to be independent random matrices. The paper applies Theorem 3 to get Eq (54), which requires $\\hat{H}_i$ to be independent. However, it seems that these matrices are not independent, and therefore the Theorem 3 cannot be applied?\n\nI cannot see how Eq (52) holds. In particular, how this identity holds with a small O notation.\n\nTypos:\nEq (6): $\\lambda^3$ should be $O(\\lambda^3)$\nBelow Eq (10): \", This\" should be \", this\"\nProposition 2: \"but $L_p$ stable\" should be \"\"but not $L_p$ stable\"\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698114517768,
        "cdate": 1698114517768,
        "tmdate": 1699636083219,
        "mdate": 1699636083219,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k8gEEKm1sl",
        "forum": "9grjdFDiAj",
        "replyto": "9grjdFDiAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_My7s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_My7s"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces probabilistic stability as a new notion for analyzing the dynamics of SGD around critical points.  Specifically, the proposed notion is used to characterize different learning phases of SGD such as correct learning, incorrect learning, convergence to low-rank saddles and unstable learning phase. In particular, the authors provide many insights into the convergence to saddle points based on their probabilistic stability notion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Previous approaches may be insufficient to characterize the dynamics of SGD near saddle points while the proposed probabilistic stability in this paper overcomes this limitation."
            },
            "weaknesses": {
                "value": "The analysis in this work is restricted to the assumption that initialization point is near a given stationary point. While I acknowledge that such a limitation is not unique to this paper, it deviates from real-world scenarios. Numerous observations have indicated that the early phase of learning substantially impacts the ultimate generalization performance of deep neural networks. Consequently, centering on the dynamics around stationary points may fall short in elucidating the success of deep learning.\n\nIn addition, this paper is poorly written and it needs substantial improvement."
            },
            "questions": {
                "value": "Major concerns:\n\n1. A recent study [1] shows that neural networks trained by gradient-based methods may not necessarily converge to stationary points. In other words, the gradients (norm) may not even vanish when these networks exhibit satisfactory performance. This observation raises questions about the applicability of probabilistic stability in understanding deep learning.\n\n[1] Jingzhao Zhang, et al. \"Neural network weights do not converge to stationary points: An invariant measure perspective.\" ICML 2022.\n\n2. Is the notion of Linear stability as defined in Wu et al. (2018) equivalent to Definition 2 in this paper? Or, is one notion weaker than the other?\n\n3. On Page 3, in the text beneath the caption of Figure 1, the statement \"This means that Eq. (1) can be seen as an effective description for only a small subset of all parameters ...\" lacks clarity in its flow. Could you provide further elaboration on this?\n\n4. On Page 6, in the fifth-to-last line, you mention, \"The theory also shows that if we fix the learning rate and noise level, increasing the batch size makes it more and more difficult to converge to the low-rank solution...\". However, in Proposition 3, it's not  clear to me how the batch size explicitly factors into this observation. Could you elaborate more on the role of batch size in this context?\n\n5. Second line on Page 4: \"If $\\mathbb{E}[h]>0$, the condition is always violated\". When $\\mathbb{E}[h(x)]>0$, the RHS of Eq.(6) $<0$, doesn't it make the condition in Eq.(5) be valid? Why the condition is violated? Is it a typo or do I misunderstand anything?\n\nMinor comments:\n\n1. Notations are inconsistent in this paper. For example, $n$ serves as the representation of a norm type in the first paragraph on Page 2, where it is a scalar. Later in Section 3.1, $n$ is used as a fixed unit vector. In addition, there is a disparity in the definition of the loss function between Eq.(2) and Eq.(12), as they take different inputs.\n\n2. The paragraph after Definition 1: \"A sequence $z_t$ converges\" $\\Longrightarrow$ \"A sequence $\\theta_t$ converges\"\n\n3. The sentence after Eq.(2): \"The dynamics of $w$\", and the sentence before Eq.(10): \"the following condition implies that $w$...\", what is $w$ here? It is not defined.\n\n4. In the capitation of Figure 1: \"Phase **I** corresponds to\" $\\Longrightarrow$ \"Phase **Ia** corresponds to\"\n\n5. Paragraph after Eq.(7): \"The denominator is the Hessian ... signal in the gradient. The denominator is the strength ...\", do you mean the numerator for the first \"denominator\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698537294723,
        "cdate": 1698537294723,
        "tmdate": 1699636083133,
        "mdate": 1699636083133,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "awMmcjadZk",
        "forum": "9grjdFDiAj",
        "replyto": "9grjdFDiAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_bHzM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_bHzM"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate the behavior of SGD through the lens of probabilistic stability: convergence in probability to certain critical points. Dynamics are considered for quadratic losses (justified through local approximation about a critical point), with particular attention given to the effect of saddle points. Necessary and sufficient conditions are provided for probabilistic stability, first for rank-1 quadratic forms, and then for the general case. Probabilistic stability is compared with norm-based stability to highlight the inadequacy of norm-based approaches. Two synthetic examples are considered, and phase diagrams are presented to illustrate different behaviors according to the scale of the noise, and the step size. The phases are delineated as:\n\n- Ia. Probabilistic stability to correct solution\n- Ib. Stability in norm to correct solution\n- II. Probabilistic stability to incorrect solution\n- III. Convergence to low-rank saddle point\n- IV. Completely unstable\n\nUniversal behavior is implied when experiments on ResNet18 with CIFAR10 also display similar phases. Finally, selection of solutions is considered for SGD with a two-layer network w/ Swish activation, highlighting once again that norm-based convergence is an ineffective criterion."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Linearised dynamics are effective for investigating the late stages of training; well-suited for studying stability. \n- Convergence in probability is a much better condition to study than norm-based convergence; this is shown theoretically and empirically.\n- The examples are relatively simple, but are effective at demonstrating the phase transitions.\n- Universality of the phase diagrams even with real examples is fascinating"
            },
            "weaknesses": {
                "value": "- Studying dynamics of SGD using Lyapunov-type criteria is hardly new, so the theoretical contributions here are particularly limited in their novelty. These conditions for ergodicity of random linear recurrence relations (which immediately imply probabilistic stability as stated here) are already well known [1]. Such conditions have been considered for stochastic optimizers in the ML literature as well [2,3].\n- The presentation of the phase diagrams is less than ideal, especially since this is perhaps the key contribution of the paper. Figures are confusing as presented: at the very least, they should be closer to where they are described in text, and empirical results should be compared more directly with theoretical findings. Terms are introduced here that do not appear to be explained. \n- Minor: a few typos throughout, e.g. eqn 6 missing an O in front of the cubic term and under eqn 10. \n\n[1] Diaconis, P., & Freedman, D. (1999). Iterated random functions. SIAM Review, 41(1), 45-76.\n\n[2] Gurbuzbalaban, M., Simsekli, U., & Zhu, L. (2021). The heavy-tail phenomenon in SGD. In International Conference on Machine Learning (pp. 3964-3975). PMLR.\n\n[3] Hodgkinson, L., & Mahoney, M. (2021). Multiplicative noise and heavy tails in stochastic optimization. In International Conference on Machine Learning (pp. 4262-4274). PMLR."
            },
            "questions": {
                "value": "- Is sparsity/density the fraction of zero/non-zero elements in the matrix? This is my best guess, but it is surprising that there would be so many \"zero\" elements here; is there some larger cutoff which determines whether an element is \"zero\", or is something else considered here?\n- Is Figure 1 comparable to any other empirical examples? If not, what is the purpose of this figure?\n- Am I to interpret Figures 4 (right) and 5 (right) as displaying the same behavior as Figure 2 (right)? Can the theoretical prediction be overlaid here too?\n- Why is there an arrow from A to B in Figure 6? The text suggests that initialized at B, SGD jumps to C. \n- Is Figure 7 converging to Figure 2 as $N \\to \\infty$?\n- I assume convergence to the low-rank saddle should correlate with poor performance?\n- Can you outline the phases in the main text? These need to be displayed front and center. It is very confusing to have to refer to a figure legend towards the top of the paper for this. \n- Can you put the SGD solution selection part (including Figure 6) near Section 3.2? Otherwise, the purpose of this section is lost on first read."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760713499,
        "cdate": 1698760713499,
        "tmdate": 1699636083054,
        "mdate": 1699636083054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y90dQI5oWf",
        "forum": "9grjdFDiAj",
        "replyto": "9grjdFDiAj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_Mfzw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1548/Reviewer_Mfzw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new stability notion, i.e., probabilistic stability, to study the stability of the SGD learning algorithm. The goal of proposing the new stability notion is to explain why deep learning models trained with SGD generalize well. The paper also revisits some variance-based stability notions and illustrates that those stability notions cannot explain the convergence of SGD."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem studied in this paper, i.e., explaining deep learning phenomena using a new stability notion, is valuable and interesting.\n2. The paper tackles this problem from a different angle, such as characterizing SGD dynamics from control theory."
            },
            "weaknesses": {
                "value": "1. Literature on the variance-based stability notion is not adequately discussed in this paper. The paper presents the definition of the variance-based stability notion in Definition 2, but it is unclear what the current results are regarding this type of stability. Additionally, the reference for this stability notion cannot be found in this paper. It would be beneficial to include more discussions regarding the related work.\n2. The clarity of the paper can be improved. It is difficult to follow and extract the key points of each section that the paper wants to deliver. It is also hard to connect each section. For example, Section 3.1 shows that rank-1 dynamics are solvable, but how this connects with probabilistic stability is unclear. Section 3.2 jumps to the point that variance-based stability is insufficient, and it is hard to connect these two sections.\n3. The technical soundness and significance can be improved. Section 3 show that the linearized dynamics of SGD converges with probability under certain conditions, but it is difficult to establish a connection between this result and how it explains the generalization of SGD in deep learning. Section 4 discusses different phases of SGD learning, but it is unclear how these phases relate to the stability notions proposed in the paper and how they explain the generalization of SGD."
            },
            "questions": {
                "value": "1. Can we understand that the probabilistic stability defined in this paper is more like a convergence guarantee?\n2. What is the literature on variance-based stability?\n3. In Definition 1 and 2, it would be great to clarify whether \u03b8* is fixed or a random variable. After Definition 1, there is a typo in the convergence in probability, \"< \u03b5\" should be revised to \"> \u03b5.\"\n4. Some typos:\n    1. After equation (2): \"The dynamics of w thus obeys Eq. (1).\" Here, w is not defined.\n    2. Before equation (10): \"Then, the following condition implies that w \u2192 p 0:\". Also, w is not defined.\n\n\n5. Figure 1: It would be great to explain the x-axis and y-axis in the caption (the same in Figure 2). Also, what are w_t and u_t? How are the phases (different colors) calculated based on values on the x-axis and y-axis?\n6. Figure 3: It would be great to explain how the color map is calculated and how convergence is calculated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797917270,
        "cdate": 1698797917270,
        "tmdate": 1699636082974,
        "mdate": 1699636082974,
        "license": "CC BY 4.0",
        "version": 2
    }
]