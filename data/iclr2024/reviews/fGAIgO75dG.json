[
    {
        "id": "190cR9FVC3",
        "forum": "fGAIgO75dG",
        "replyto": "fGAIgO75dG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_VEfR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_VEfR"
        ],
        "content": {
            "summary": {
                "value": "The work proposes a new differentiable structure learning method for learning linear acyclic model that eliminates the assumption of equal error variances needed by several existing differentiable methods based on least squares. Building upon existing idea on smoothed concomitant lasso, the proposed method develops a regression-based score function that includes concomitant estimation of scale and decouples the sparsity parameter from the exogenous noise levels. Experiments with simulated and real-world datasets are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem considered is highly relevant because it is important to relax the assumption of equal error variances to handle heteroscedastic noises."
            },
            "weaknesses": {
                "value": "The formulation (5) in the heteroscedastic setting lacks identification guarantee. It is unclear which specific settings it is theoretically correct for. For the linear Gaussian setting, one should use Gaussian likelihood, e.g., in GOLEM, while for linear non-Gaussian setting, one should use non-Gaussian likelihood, e.g., in NOTEARS-ICA.\n\nThere are some possible issues with the experiments, elaborated in the next section."
            },
            "questions": {
                "value": "- Does the method work after data standardization (see the study by Reisach et al. (2021)? Since the method is specifically for heteroscedastic setting, this experiment should be included to support the claim.\n- For heteroscedastic Gaussian noise, the paper should compare the recovery results of Markov equivalence classes instead of DAGs, since the true DAG cannot be identified in theory.\n- Regarding performance of DAGMA and GOLEM:\n    - For DAGMA, did the authors try using the log-likelihood in the heteroscedastic setting? The authors of DAGMA paper consider such log-likelihood for nonlinear setting, but could be straightforwardly done for linear setting.\n    -  For GOLEM, did the authors use the EV version to initialize the NV version, as suggested by their paper? Also, Section 5.1 says that GOLEM is based on profile-log-likelihood--I think a more straightforward comparison with Eq. (5) is their version without profiling.\n    - Did the paper try to tune the hyperparameters for these two methods, since the settings considered here are quite different from their papers?\n- What does \"decouples the sparsity parameter from the exogenous noise levels\" mean? I did not manage to find any elaboration or explanation of it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715226298,
        "cdate": 1698715226298,
        "tmdate": 1699636910784,
        "mdate": 1699636910784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fhxIb4GEri",
        "forum": "fGAIgO75dG",
        "replyto": "fGAIgO75dG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_wJd3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_wJd3"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new continuous optimization problem for DAG learning. It leverages results from the concomitant scale estimation literature to learn a weighted adjacency matrix while estimating the scale of the exogenous noise variables. The experiments clearly show that optimizing the new objective (by inexact block coordinate descent) instead of the original l1-regularized objective of DAGMA results in better estimation of the graph across various settings."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper tackles an important problem of interest to the general ICLR community. \n\n2. The proposed regularization is general enough that it can be plugged in many of the continuous optimization problems recently proposed for learning DAGs. The work's impact is hence potentially high as it could improve performance of many state-of-the-art methods.\n\n3. The paper is generally well presented. Its claims are well supported by an extensive empirical analysis that illustrates the DAG recovery capabilities of the method on several settings and for noise estimation."
            },
            "weaknesses": {
                "value": "1. Although the adjacency matrix $W$ can be efficiently updated with stochastic gradient steps, the closed-form for the noise scale is evaluated on the full data because it is not decomposable. This makes the method scale poorly to big data. This limitation should be highlighted in the text or an efficient approximation could be discussed and empirically evaluated.\n\n2. It is not clear how Problem 2 is obtained, i.e., under which assumptions the noise-dependent terms appear in the objective. It would be useful to report such derivation in the appendix. This would in particular allow for verifying if the sparsity inducing term $||W||_1$ can be replaced by the score-equivalent term $||W||_0$, used e.g. in [Brouillard et al. 2020, Zantedeschi et al. 2023], without loss in noise estimation performance.\n\n3. The paper does not describe how $\\lambda$ was tuned. In Section 4.1 it is only mentioned that it was \"empirically determined\".\n\n4. Sortnregress should be reported also in Figure 1. Currently the text reports sortnregress results for two values of the noise scale and for a single graph type, but it wouldn't hurt the readability of Figure 1 to add all the results for that baseline for all the settings. Plotting such results would allow to clearly see which settings are trivial and which are of interest.\n\nI would be inclined to increase my rating if these points are addressed."
            },
            "questions": {
                "value": "(minor) There is a sign typo in the second-last equation of page 14."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Reviewer_wJd3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786083632,
        "cdate": 1698786083632,
        "tmdate": 1700839611267,
        "mdate": 1700839611267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "73qQmYNNx7",
        "forum": "fGAIgO75dG",
        "replyto": "fGAIgO75dG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_xkAR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7539/Reviewer_xkAR"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of DAG structure learning from a score-based viewpoint for linear models. \nThe authors propose a new score function that also estimates the noise levels and experimentally show that it can lead to better accuracies by leveraging recent advances in continuous non-convex characterizations of DAGs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is clearly written and the contributions are easy to digest.\n* The proposed score leads to structure improvements w.r.t. sota methods."
            },
            "weaknesses": {
                "value": "* Significance: The paper considers only linear models, hindering the significance of the proposed loss function.\n* Novelty: The authors borrow ideas from concomitant lasso, and straightforwardly apply it to the score function for DAG learning. While it is totally okay with borrowing ideas from prior work, it feels that this is indeed the only technical contribution of the paper. The optimization part feels identical to prior work expect for the extra noise terms."
            },
            "questions": {
                "value": "* With respect to my point in the weaknesses section, in my opinion, it would be more enlightening to show that the proposed score function leads to identify the true underlying DAG. The current contribution feels like just \"another score function\" with no guarantees of identifiability. The non-equal noise variances was also studied in Loh and Buhlmann (2014) where they proposed a weighted LS that would lead to identifiability of the true DAG, this weighted LS depends on the noise levels as well, I wonder if jointly optimizing such objective would also lead to accuracy improvements.\n\n* I wonder if the authors experimented with non-linear models as well?  Given that I would consider this work to be \"empirical\", it would be good to use these ideas into nonlinear models as well. \n\n* I will also note, a recent method called TOPO by Deng et al. (2023) \"Optimizing NOTEARS objectives via topological swaps\" shows improvements in structure estimation for score-based methods. Their theory suggests that given a convex score (as in this paper) their optimization algorithm would guarantee a local optimum. It would be interesting to see if using the proposed convex score + TOPO can  obtain even more accurate DAGs, specially for non-equal variances. Finally, the same authors have provided initial insights into global optimality of continuous DAG learning methods which can also help to motivate this line of work in the continuous-constrained framework, see Deng et al (2023) \"Global Optimality in Bivariate Gradient-based DAG Learning\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7539/Reviewer_xkAR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7539/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698944429565,
        "cdate": 1698944429565,
        "tmdate": 1700699452012,
        "mdate": 1700699452012,
        "license": "CC BY 4.0",
        "version": 2
    }
]