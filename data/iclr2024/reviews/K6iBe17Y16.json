[
    {
        "id": "640LCsu4PO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_j4wF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_j4wF"
        ],
        "forum": "K6iBe17Y16",
        "replyto": "K6iBe17Y16",
        "content": {
            "summary": {
                "value": "This paper proposes to learn planning heuristics for forward search algorithms. The authors propose using truncated Gaussians to model the distribution of the learned heuristics. This modeling change results in a different loss function from the standard MSE loss. Empirical evaluations are provided in three classical planning domains."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I found the exposition interesting: connecting the MSE loss with the assumption that the empirical distribution is a Gaussian distribution with a fixed variance motivates the proposed method well. \n\n2. Employing existing knowledge to obtain lower bounds to further constrain the estimated distribution is a simple and elegant method to make the predictions more informative. The empirical evidence shows such an additional constraint improves the search efficiency."
            },
            "weaknesses": {
                "value": "1. The choice of using Greedy Best-first Search (GBFS) needs more justification as it is not guaranteed to find optimal solutions. Can the authors explain why they did not choose to experiment with A*?\n\n2. Overall the empirical results are quite weak. While the negative log-likelihood (NLL) and MSE losses improve, the downstream search performance, i.e., the average number of node evaluations, compared to baselines (either the non-truncated Gaussian or using $h^{FF}$ directly) is not convincing. \n\n3. A popular direction for evaluating the effectiveness of a learned heuristic is to test how well it generalizes to larger instances of a class of problems. The current evaluations focus only on problems generated with the same set of parameters. Adding this kind of generalization experiment would demonstrate the practical value of the proposed approach better.\n\n4. The three classical planning problems should have sufficiently detailed descriptions."
            },
            "questions": {
                "value": "Please see the above section for my question regarding the choice of the search algorithm.\n\nAdditionally, can the authors add the standard deviation numbers for the average number of node evaluations to Table 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Reviewer_j4wF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697408905418,
        "cdate": 1697408905418,
        "tmdate": 1700680920026,
        "mdate": 1700680920026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TYXrT52SLC",
        "forum": "K6iBe17Y16",
        "replyto": "K6iBe17Y16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_hjVN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_hjVN"
        ],
        "content": {
            "summary": {
                "value": "This paper studies heuristic learning by utilizing the information provided from admissible heuristics as informative bounds. To this end, instead of the traditional approach of minimizing mean square errors, the authors propose to model the learned heuristic as a truncated gaussian and subsequently minimize the loss function resulted from such a statistical/distributional assumption. As such, the authors claim to provide some theoretical understanding which is often lacking in the past work. Experiments are conducted and they show that the proposed method does contain certain merits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The presentation is very clear and overall, the paper is easy to follow and digest. The proposed method also makes intuitive sense. Given that there are additional information available, it makes sense to utilize them in the modeling instead of going with the traditional gaussian case. The experiments also, to certain extent, verify the founding."
            },
            "weaknesses": {
                "value": "The contributions of this paper may seem weak and limited. For example, the connection among MLE, Gaussian, MSE is well-known and Section 3 seems to be elementary. If admissible bounds are available, it seems straightforward/natural to refine the distributional assumption. Overall, I think this paper contains a good practical study but it doesn't seem to be innovative enough to justify the acceptance."
            },
            "questions": {
                "value": "Could the authors provide some discussions on how certain assumptions in the current manuscript would change the result/model? For example, \"In this paper, we assume unit-cost: \u2200a \u2208 A; COST(a) = 1;\" \"In this work, we focus on the scenario where an admissible heuristic is provided along with the optimal solution cost h\u2217 for each state, leaving other settings for future work.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697577566096,
        "cdate": 1697577566096,
        "tmdate": 1699636601507,
        "mdate": 1699636601507,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p8cKf0CCBH",
        "forum": "K6iBe17Y16",
        "replyto": "K6iBe17Y16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_795y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_795y"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new loss function for learning admissible heuristics in AI\nsearch. The authors argue that the widely-used MSE does not accurately model\nwhat we intend to optimize, describe their new loss function, and evaluate it\nempirically."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is well-written, the presented argument is convincing, and the\nempirical results further support it. This is a great paper that successfully\nchallenges the accepted wisdom of using MSE when learning heuristics."
            },
            "weaknesses": {
                "value": "I have no concerns or questions regarding the work."
            },
            "questions": {
                "value": "I have no concerns or questions regarding the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698689500805,
        "cdate": 1698689500805,
        "tmdate": 1699636601398,
        "mdate": 1699636601398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RuPIQebFQf",
        "forum": "K6iBe17Y16",
        "replyto": "K6iBe17Y16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_LPcd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_LPcd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new approach for learning heuristic for forward search (specifically, for greedy best-first search) that can make use of admissible heuristics. The proposed approach is based on modelling the learned heuristic using truncated Gaussian and use the admissible estimates as lower bound for the distribution. The best approach that consists of using truncated Gaussian, learned $\\sigma$, and residual learning (based on the popular h^FF heuristic) significantly outperform other learning-based configurations in terms of accuracy and number of solved instances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n- Novel approach for learning heuristics that makes principled use of admissible estimates.\n- The approach is compatible with residual heuristic learning and is agnostic of the neural architecture.\n- Experiments show increased accuracy and a larger number of planning instances solved under 10^4 evaluations."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- Missing state-of-the-art recent baseline: [Chrestien et al., 2022] is an alternative approach that also argues against using MSE and proposes an alternative approach. This baseline should be compared to the proposed approach for learning heuristics.\n- In the experiments, it looks that the standard h^FF baseline outperforms the proposed approach in terms of problem solved in two out of the three domains.\n- There is no analysis of performance vs. problem size. It would be very useful to see if the patterns depend on problem size.\n\nMinor point: in Section 3, the description of learning heuristic ignores the goal, the learning of heuristics is conditioned on the goal state since the same state will have different estimate conditioned on different states."
            },
            "questions": {
                "value": "I would appreciate the authors' response to the weaknesses listed above. In particular, did you compare to [Chrestien et al., 2022] or other state-of-the-art approaches for learning heuristics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794562057,
        "cdate": 1698794562057,
        "tmdate": 1699636601290,
        "mdate": 1699636601290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HrBy62BYwN",
        "forum": "K6iBe17Y16",
        "replyto": "K6iBe17Y16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_63nm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5741/Reviewer_63nm"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the use of modern machine learning techniques to learn heuristic functions for forward search algorithms. It highlights the lack of theoretical understanding regarding what these heuristics should learn, how to train them, and why they are trained in the first place, leading to a variety of training targets and loss functions in the literature. The authors argue that learning from admissible heuristics using mean square errors (MSE) as the loss function is not the correct approach because it results in a noisy, inadmissible heuristic. Instead, they propose modeling the learned heuristic as a truncated Gaussian, with admissible heuristics used as lower bounds. This approach results in a different loss function than MSE, leading to faster training convergence and better heuristics, with a 40 percent lower average MSE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is enjoyable to read and fairly well organized by raising three core questions (what, how and why). The problem of choice of training target and loss function is also well-motivated. \n\n2. The experiments are relatively comprehensive and the results are pleasing."
            },
            "weaknesses": {
                "value": "1. In my view, one of the main contributions of this paper is to use the NLL as their training loss instead of MSE, and NLL adds the prediction of $\\sigma$ (the variance) where MSE uses the fixed $\\sigma$. However, in my opinion, this technique will improve the performance very trivially since the model will predict better with more parameters.\n\n2. The authors explain the reason for using Gaussian distribution by giving the principle of maximum entropy, but the reason for using Truncated Gaussian seems missing and \nunconvincing."
            },
            "questions": {
                "value": "In section 3, the authors claim that the importance of using NLL loss function instead of MSE. However, from the experiment results, the authors can only claim that $\\mathcal{T} \\mathcal{N}$ obtains around 40 percent lower MSE than $\\mathcal{N}$ +clip on (geometric) average, so does this truncated Gaussian technique play the major role of the outstanding performance of the experiment instead of the choice of loss function? And what is the statistical intuition behind it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5741/Reviewer_63nm"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5741/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699162265186,
        "cdate": 1699162265186,
        "tmdate": 1699636601162,
        "mdate": 1699636601162,
        "license": "CC BY 4.0",
        "version": 2
    }
]