[
    {
        "id": "dBfJTWyVRz",
        "forum": "3d0OmYTNui",
        "replyto": "3d0OmYTNui",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_JeaX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_JeaX"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on aligning large language models (LLMs) through reinforcement learning (RL) while preserving user privacy using Differential Privacy (DP). The paper introduces a new DP framework for this alignment and validates its effectiveness through experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes a differentially private framework for aligning LLMs with RL, offering mathematical guarantees of privacy.\n2. The paper empirically evaluates the framework on tasks like positive review generation and summarization, showing that it offers competitive utility while ensuring strong privacy protections."
            },
            "weaknesses": {
                "value": "1. The paper employs DPSGD to ensure privacy in the alignment of large language models through reinforcement learning. While the use of DPSGD is well-established in the privacy literature. Furthermore, the paper does not introduce significant modifications to the RLHF process. The innovation seems to be more focused on engineering adjustments rather than novel theoretical contributions. \n\n2. The paper discusses the trade-offs between privacy and utility but does not present these results in an intuitive manner. A Pareto frontier could be more illustrative in showing how different levels of privacy (varying \u03b5) impact the model's performance. This would provide a clearer understanding of the trade-offs involved. \n\n3. If the reward in step 2 is DP, is it necessary to use the DPPPO in step 3 as the learning reward is already DP?"
            },
            "questions": {
                "value": "Please check the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Reviewer_JeaX",
                    "ICLR.cc/2024/Conference/Submission6097/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698242036206,
        "cdate": 1698242036206,
        "tmdate": 1700631641206,
        "mdate": 1700631641206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ohPgFfwTif",
        "forum": "3d0OmYTNui",
        "replyto": "3d0OmYTNui",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_vWsq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_vWsq"
        ],
        "content": {
            "summary": {
                "value": "The authors provide privacy-preserving technique for fine-tuning large language models. They apply differentiall-private SGD (DP-SGD) to the PPO reinforcement learning algorithm during model fine tuning. They demonstrate that for single-digit privacy budgets it is possible to fine tune GPT-2 such that there is improved utility on positive reward score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper combines two well-understood algorithms, DP-SGD and PPO in a well-motivated task of reinforcement with human feedback and supervised fine-tuning. The experiments and the results are clear and the application of private model fine-tuning is reasonable.\n\nThe paper is well organized and the writing is clear. The paper is well written and the algorithm is clearly explained. Their claims are based on the GPT-2 family of models and run experiments on the ROUGE metrics and the TweetEval benchmark. The authors offer a privacy-preserving technique to undertake reinforcement learning with human feedback. They combine DP-SGD and PPO with a few adaptations and show utility benefits on NLP benchmarks."
            },
            "weaknesses": {
                "value": "While the examples are helpful, the overall motivation could be a bit stronger. What are we protecting and why? What is the threat model around incorporating human feedback? Are their examples of memorization from human feedback?\n\nThe experiments in the main body do not include error or number of trials details. It is unclear in Table 1 why models with less provacy should do worse than those with more privacy (GPT-2 Medium, eps 4->8, or GPT-2 Large eps 8->Inf). Such results demand further study and/or ablations and are difficult to interpret without confidence intervals. The use of corporate imagery (Reddit / OpenAI) weakens the overall presentation and the generality of the results. Work in differential privacy and RL can be traced to differentially-private policy evaluation (Balle, Gomrockchi, Precup). The paper touches on the privacy accounting implications when  $T_{\\text{PPO}} \\neq 1$\n, but does not offer evaluate the implication of fixing it to the default value of 4."
            },
            "questions": {
                "value": "What are the confidence intervals for your reported experiments?\nAre there other domains where the fine-tuning method can be better understood?\nWhat are we losing by setting $T_{\\text{PPO}} \\neq 1$ instead of 4 in terms of utility? Is this trade-off significant?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Reviewer_vWsq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698253498839,
        "cdate": 1698253498839,
        "tmdate": 1699636657595,
        "mdate": 1699636657595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HKjGsy9YeW",
        "forum": "3d0OmYTNui",
        "replyto": "3d0OmYTNui",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_A7MT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6097/Reviewer_A7MT"
        ],
        "content": {
            "summary": {
                "value": "This paper offers an approach to align Large Language Models with human preferences and feedback via a privacy preserving RLHF methodology and perform some comparison experiments to show the correctness of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The approach proposed in this paper for aligning language models with PPO in a privacy-preserving way is original; (2) The paper is clearly written and well-organized. (3) The paper gives a quite comprehensive analysis of the procedure and emphasize the difficult issues in the implementation."
            },
            "weaknesses": {
                "value": "(1)The DP part is too condensed to understand.  The authors used DP-SGD on several occasions but without a clear explanation of this algorithm. And in the main text, I could not find a concrete DP algorithm and a clear procedure how it is combined with the alignment. (2) Algorithm 1 is not original. I don\u2019t see the reason why it was presented in detail in the paper.  The PRIVATE alignment should be more interesting."
            },
            "questions": {
                "value": "(1)What is non-private REWARD model? What is the private REWARD model? DP mechanisms are not equivalent to adding noises.  Could you specify the DP mechanisms in the alignment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6097/Reviewer_A7MT"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238706200,
        "cdate": 1699238706200,
        "tmdate": 1699636657385,
        "mdate": 1699636657385,
        "license": "CC BY 4.0",
        "version": 2
    }
]