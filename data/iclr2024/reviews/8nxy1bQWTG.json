[
    {
        "id": "lEO4LXJQQN",
        "forum": "8nxy1bQWTG",
        "replyto": "8nxy1bQWTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_jJ4J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_jJ4J"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a version of diffusion models with a simple trainable encoder parameterized as a simple convex combination between  the data $\\mathbf{x}$ and a neural network $f(\\log SNR(t))$. parameterized by the signal to noise ratio ($SNR(t)$), a function of time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is generally well-written and easy to read.\n- The formulation for learnable encoders is simple and clear, and involves minimal added components from the original diffusion model.\n- All derivations are well-documented in the appendix."
            },
            "weaknesses": {
                "value": "- The benefit of the encoder is unclear. There is limited exploration of the learned encoded space outside of the visualization provided in Figure 1. What is gained by this formulation?\n- In spite of the added theoretical overhead incurred by the learned encoder, there is minimal empirical improvement over existing baselines (BPD 2.65 -> 2.63 on the CIFAR10 dataset).\n- Empirical validation is slim. Model performance is only evaluated in terms of BPD on two simple datasets: MNIST and CIFAR10. MNIST is generally no longer used as a density estimation benchmark. As a result, I wonder is the performance generalizable? Is there improvement in terms of any other metrics? There is a lot of research on how likelihood can be a poor proxy for generative model performance. For example, a lot of improvements in BPD can be attributed to modeling semantically unimportant high frequency noise components [1, 2].\n- Parameterization of the learned encoder is very simplistic. Why is the encoder conditioned only on the time $\\lambda_t$ and not the data $\\mathbf{x}$ itself? While I do appreciate elegant derivations, the justification for this parameterization (Appendix I) -- that this makes the math nice -- does not feel entirely sufficient.\n\n[1] A note on the evaluation of generative models. https://arxiv.org/abs/1511.01844\n\n[2] Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. https://arxiv.org/abs/2106.05527"
            },
            "questions": {
                "value": "It is still not entirely clear to me what is learned by the encoder model. Can the authors directly visualize the output of the learned encoder model at different times $t$?\n\nHow does Section 7 (v-prediction parameterization) relate to the rest of the paper? Is it used for experiments in Section 8?\n\nIn which experiments in Section 8 were a fixed noise schedule used, and in which were a learned noise schedule used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721047238,
        "cdate": 1698721047238,
        "tmdate": 1699636188267,
        "mdate": 1699636188267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mJpHvvJOjJ",
        "forum": "8nxy1bQWTG",
        "replyto": "8nxy1bQWTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_Hg56"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_Hg56"
        ],
        "content": {
            "summary": {
                "value": "The paper extends variational diffusion models (VDM) with a time dependent encoder. Instead directly noising the input data points (here images), they are first encoded and then noised. There are two encoding schemes introduced. In one, the encoder has no trainable parameters and is simply a rescaling of the input. In the other, the encoded images are a reweighting of the noise-free image and the output of a time-dependent image-to-image neural network. The authors derive the corresponding loss functions and analyze the model in its infinite depth limit. The results show that the diffusion model equipped with a new encoder achieves state-of-the-art BPD on CIFAR-10."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The high-level idea for the paper is quite natural and something that somebody was bound to try because of its potential impact. Overall, I found the writing fairly clear, with some exceptions that I will mention in the next section. The analysis of the method is fairly extensive and supported by lot of details in the appendix, though these are primarily mathematical proofs and not necessarily an exploration of design decisions that have a high level of practical significance."
            },
            "weaknesses": {
                "value": "The presentation of the method, namely section 3 and 6 could be improved significantly. There are a lot of variable names, and I had to read through the section many time in order to understand what was happening, even though the final procedure is not that complex. Moving the figure provided in Appendix A to the main text might be helpful in this regard. Or you could include an algorithm, or simply a link to your code, as these would all be easier to parse as someone familiar with common diffusion code and parlance. The presentation is also strong emulating the style of VDM even though the method has interesting consequences and potential applications on diffusion models more broadly, and simplifying the description of the algorithm to rely less on graphical models and equations and simply describe where noising and denoising are happening might make the approach more accessible to a broader audience.\n\nBeyond the presentation, the results are pretty limited. Given that the proposed method is an incremental change to VDM, the performance improvement is not overwhelming. It feels like an improvement on par with what might be possible through better hyperparameter tuning or by increasing the model capacity (which is implicitly part of what this method is doing, at least in the case where the encoder has learnable parameters). The results being limited to CIFAR-10 compound my intuition that the results aren't very notable, because it's not clear if they generalize. Though I think method appears sound and significant and general results seem possible, the current results feel like a negligible improvement on a relatively small dataset. It would be great to see results paralleling the original VDM model. If the improvements spanned all the datasets considered in the original paper, I would be more convinced that the proposed encoder is doing something significant."
            },
            "questions": {
                "value": "Do you have a sense for what the weaknesses of your gradient approximation (Section 6) are? It seems that the approximation will be very bad for t near 1. Does this effect the learning of the encoder in a serious way? It might be helpful to explore this a bit more if it is one of the more impactful approximations required to your method practical.\n\nWhat is the intended take-away from Figure 1? Is the significance simply that the learned encoder captures the spatial frequencies that we might expect at each time in the diffusion process? Or do you also conclude that there is an implicit form of segmentation happening? The later feels potentially quite profound but needing more justification while the former feels potentially unsurprising, especially if the gradient signal is just incredibly noisy. It could just be learning very little for t near 1.0."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Reviewer_Hg56"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773954836,
        "cdate": 1698773954836,
        "tmdate": 1700678887701,
        "mdate": 1700678887701,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7KoOj5f6MN",
        "forum": "8nxy1bQWTG",
        "replyto": "8nxy1bQWTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_neQL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_neQL"
        ],
        "content": {
            "summary": {
                "value": "This work delves into the realm of diffusion models through the lens of hierarchical Variational Autoencoders (VAEs). The authors put forth a novel iteration of diffusion models that preserves the advantages of VAEs. To be more specific, they introduce a time-dependent DiffEnc, resulting in encoder variations across different time steps. Furthermore, the authors conduct an in-depth exploration of the assumption that forward and backward variances are equal, underpinned by thorough theoretical analysis. In addition, empirical findings are presented to substantiate the efficacy of the proposed DiffEnc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I believe that implementing a trainable encoder within the context of the diffusion model represents a promising avenue for enhancing diffusion models, particularly in terms of ELBO optimization. This paper offers comprehensive insights into the derivation and analysis, rendering it accessible and straightforward to grasp. The experimental findings are not only persuasive but also harmonize effectively with the theoretical framework. For instance, in Figure 1, we observe a logical outcome indicating that during the initial stages, the encoder primarily attends to local patterns, such as edges. Simultaneously, as the value of $t$ increases, it places greater emphasis on capturing global structures."
            },
            "weaknesses": {
                "value": "1. The organization of sections is perplexing. It's challenging for me to discern whether Section 2 serves as an introductory section or is meant to highlight one of your contributions.\n2. The absence of a central theorem throughout the paper poses a difficulty for readers in anticipating the direction of the derivations and what to expect."
            },
            "questions": {
                "value": "I don't have specific questions regarding the main text, but it's clear that there's a need for improvements in the organization of the content."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2518/Reviewer_neQL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699073268287,
        "cdate": 1699073268287,
        "tmdate": 1699636188079,
        "mdate": 1699636188079,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dtr8Yhrl1x",
        "forum": "8nxy1bQWTG",
        "replyto": "8nxy1bQWTG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_g9hi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2518/Reviewer_g9hi"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to model the integration of a learnable data and time-dependent mean function for diffusion models. The paper investigates the parametrisation of the diffusion losses and shows optimal variance schedules for the forward and reverse processes. The paper shows density estimation results on CIFAR 10 and MNIST and achieves the best performance compared to relevant baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. It adds a simple yet interesting addition to diffusion by introducing a mean shift to the forward diffusion while not being required in the sampling process and therefore ensuring its scalability. The theoretical analysis of the different noise variances adds an interesting flavour too. The results seem to indicate that the added encoder improves the performance in terms of bits per dimension."
            },
            "weaknesses": {
                "value": "The paper has very limited evaluation and doesn't compare to some of relevant baselines that are even mentioned in the paper, like latent diffusion and only compares on Cifar-10 and MNIST. Furthermore, it mentions that some methods only show improvement after longer training, hinting at potential inconsistencies in the results in case of slightly different training setups due to not training till convergence. It is hard to judge whether the proposed changes are a significant improvement due to this.\nThe ablation studies of the different methods are interesting but it's unclear what insight is gained from this, e.g. what does it mean that the loss is dominated by the diffusion loss?\nLastly, the theoretical analysis of the variance ratio is interesting but unclear how it fits into the bigger story of the paper."
            },
            "questions": {
                "value": "- In section 3, I first missed the added subscript in $x_t$ in eq 13. It might make it easier to read to change the notation from $x_t$ to e.g. $e_t$ or something similar to be clear there.\n- in section 6, I was not fully following the parametrisation of $x(\\lambda_t)$ - how does this related to eq (13)? E.g. does eq. (29) indicate that $q(z_t|x) = N(a_t^3 x; ...)$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699315215306,
        "cdate": 1699315215306,
        "tmdate": 1699636188007,
        "mdate": 1699636188007,
        "license": "CC BY 4.0",
        "version": 2
    }
]