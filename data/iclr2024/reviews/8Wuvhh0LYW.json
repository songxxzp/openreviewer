[
    {
        "id": "c50MBXpnem",
        "forum": "8Wuvhh0LYW",
        "replyto": "8Wuvhh0LYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"OmniQuant,\" a quantization method of Large Language Models (LLMs) for efficient deployment. Unlike traditional post-training quantization (PTQ) methods that manually select quantization parameters, OmniQuant learns these parameters, enabling effective low-bit quantization. It features two main components: Learnable Weight Clipping (LWC) which adjusts the clipping thresholds, and Learnable Equivalent Transformation (LET) that shifts quantization challenges from activations to weights. OmniQuant operates within a differentiable framework, making it efficient for both weight-only and weight-activation quantization. Experiments on OPT, LLaMA-1, LLaMA-2, and Falcon model family demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The proposed method, though simple in design, proves to be remarkably effective, notably diminishing the performance degradation for low-bitwidth quantization. The proposed OmniQuant does not introduce extra computation or parameters for the quantized model since the introduced learnable parameters can be fused into quantized weights."
            },
            "weaknesses": {
                "value": "The major weakness of the paper is the noticeable absence of experimental comparisons with Outlier Suppression+ (OS+) (Wei et al., 2023). Despite OmniQuant's learnable equivalent transformation being conceptually similar to OS+, the paper does not provide a direct comparison or a detailed discussion highlighting the distinctions between the two methods. Such a comparison would be invaluable for readers and would significantly augment the paper's credibility and depth."
            },
            "questions": {
                "value": "1.\tThe novelty of the proposed learnable equivalent transformation is limited as the main idea learning channel-wise shifting and scaling is similar to Outlier Suppression+ (OS+) (Wei et al., 2023). A comparative discussion elucidating the distinctions between OmniQuant and OS+ would be beneficial for readers. Additionally, the absence of experimental comparisons with OS+ is a notable omission that should be addressed.\n\n2.\tThe proposed LWC learns a clipping strength instead of clipping threshold in PACT (Choi et al., 2018) and LSQ (Esser et al., 2019). However, the paper lacks a clear articulation of the advantages of LWC over PACT and LSQ, particularly in scenarios where it is combined with LET and weights are frequently changed. A more thorough explanation of the benefits and underlying mechanics of LWC in such contexts would be beneficial. Additionally, an investigation into whether an iterative application of LWC and LET would yield performance improvements could provide valuable insights.\n\n3.\tIn Section 3.1, the authors delineate the incorporation of learnable parameters, denoted as $\\gamma$ and $\\beta$, to learn the clipping threshold. While the methodology is clear, the experimental section does not furnish a thorough illustration of these parameters' distribution across layers. An inclusion of this visualization would strengthen the paper.\n\n4.\tThe authors apply the LET to all linear layers, with the notable exception of the second linear layer of the FFN within the proposed method. This selective application raises an intriguing question: Do all instances of LET actively contribute to the model's final performance? An investigation into the individual and cumulative impact of LET on each linear layer could provide deeper insights into the efficacy and necessity of LET across different layers of the model.\n\n5.\tIn the experiments, the authors mention retaining the Softmax output at full-precision owing to its long-tail distribution. It would be insightful to know the implications of quantizing the Softmax output to 8-bit. How does this quantization impact the overall model performance and accuracy?\n\n6.\tIn the experimental section, the authors mention initializing the channel-wise scaling factor using SmoothQuant (Xiao et al., 2023) and the channel-wise shifting factor with Outlier Suppression+. A pertinent question arises: Is the proposed method sensitive to these initializations? It would be elucidative to explore the effects when the channel-wise scaling factor is initialized to 1 and the channel-wise shifting factor to 0. How does this affect the quantization performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_PTbL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698674197597,
        "cdate": 1698674197597,
        "tmdate": 1700468761702,
        "mdate": 1700468761702,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CNUxdOX1PU",
        "forum": "8Wuvhh0LYW",
        "replyto": "8Wuvhh0LYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH"
        ],
        "content": {
            "summary": {
                "value": "OmniQuant represents Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). In contrast to the traditional min-max scaling threshold, LWC applies a sigmoid function on the factors over min and max. The loss is computed by comparing the distortion of the output, which can be easily back-propagated to the scaling factors. Similar work on weight clipping could be found in PACT [1] and LSQ [2]. However, OmniQuant's formulation is cleaner (no need to incorporate with learnable step-size) and more general (applies to both weight and activation).\n\nThe LET in OmniQuant decides the computation operands ordering for max hardware efficiency. The implementation is based on MLC [3]. \n\n* [1] Choi, J., Wang, Z., Venkataramani, S., Chuang, P.I.J., Srinivasan, V. and Gopalakrishnan, K., 2018. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085.\n* [2] Esser, S.K., McKinstry, J.L., Bablani, D., Appuswamy, R. and Modha, D.S., 2019. Learned step size quantization. arXiv preprint arXiv:1902.08153.\n* [3] Feng, S., Hou, B., Jin, H., Lin, W., Shao, J., Lai, R., Ye, Z., Zheng, L., Yu, C.H., Yu, Y. and Chen, T., 2023, January. Tensorir: An abstraction for automatic tensorized program optimization. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (pp. 804-817)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The biggest contribution of this work is the learnable weight clipping (LWC). \n\n1. When compared to AWQ [1] which only optimize the scale, LWC uses the same objective function (minimize output distortion) but also handles clipping.\n\n2. When compared to PACT, which only applies to activation with positive values, LWC is more general and applies to weight, activation, and negative values.\n\n3. When compared to LSQ, which is a combination of multiple optimization goals (step-size, gradient scaling, and clipping), LWC is straightforward to implement and speedy to train.\n\n[1] Lin, J., Tang, J., Tang, H., Yang, S., Dang, X. and Han, S., 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978."
            },
            "weaknesses": {
                "value": "1. While I highlight the advantage of LWC over AWQ, PACK, and LSQ, this evaluation for PACK and LSQ is lacking.\n\n2. Authors should present Figure 1 (a) in a quantitative way. It lacks actual numbers for cost of time and performance.\n\n3. Learnable Equivalent Transformation (LET) isn't novel and can be seen in common quantization kernels such as LLM.int8. I suggest authors to elaborate more on comparison with PACK and LSQ instead of LET.\n\n4. The latency benchmark only contain fp16 and OmniQuant. Strong baseline such as GPTQ is lacking."
            },
            "questions": {
                "value": "1. How does OmniQuant runtime efficiency compare to GPTQ and AWQ?\n\n2. What is the performance when compared to PACK and LSQ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_ngyH",
                    "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699405626,
        "cdate": 1698699405626,
        "tmdate": 1700495059509,
        "mdate": 1700495059509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iDiUKY61tl",
        "forum": "8Wuvhh0LYW",
        "replyto": "8Wuvhh0LYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc"
        ],
        "content": {
            "summary": {
                "value": "The paper works on quantization for large language models. It first proposes to learn the weight clipping threshold with optimization on the ratio of weight ranges. Then, it proposes to learn the equivalent parameters for learnable equivalent transformation with a block-wise loss. Experiments are done on 4-bit weight activation quantization and 4-bit weight-only quantization with LLaMA and OPT models. Especially, the paper evaluates the inference speed with 4-bit weight-only quantized models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Experiments are done across several datasets including common sense reasoning and perplexity evaluation. Also, the paper tries the hard setting with 4-bit weight and activation quantization. Especially, it evaluates the latency with a 4-bit weight quantized model.\n* The structure of the paper is clear and figures are drawn well.\n* The method is simple and considers the quantization difficulties both for weights and activations."
            },
            "weaknesses": {
                "value": "* The paper lacks a necessary detailed explanation for the motivation and effectiveness of the weight clipping method. \n  * In 3.2, the paper claims that directly employing prior LSQ and PACT would produce unsatisfactory performance, as demonstrated in LLM-QAT. However, LLM-QAT says that the outliers for activation have a notable impact, bringing difficulty for clipping while this method works on weights here. Also, how can the proposed Eq. (2) solve the problem of learning clipping thresholds for outliers? In other words, what is the optimization difficulty (concept given in 3.2) of previous techniques, and how can Eq.(2) solve it? More explanation about the motivation is preferred.\n  * In the appendix, the paper says that LET would decline the convergence of LSQ and PACT because LET alters the weight distribution. However, weight distribution altering is a common case for LSQ and PACT in QAT. Also, combined with LET, the \\gamma and \\beta in the proposed Eq. (2) can also go up and down during learning as it optimizes the ratio of the changeable weig\n\n* What is the core novelty of the LET? I find it looks similar to Outlier Suppression+. While the paper says that Outlier Suppression+ takes a pre-defined migration strength, but this method does not and proposes to optimize the output. I'd like to point out that Outlier Suppression+ did not take a pre-defined strength and proposed to optimize the output for channel-wise scaling parameters earlier than this paper. Meanwhile, the paper also states that AWQ adopts a grid-searched channel-wise scaling, which also seems relevant to the technique in this paper. Therefore, can the paper compare these different designs and explain why the proposed way is the best? I did not find these and this could help us better understand the effectiveness.\n\n* Experiments shall be compared with the paper Outlier Suppression+ because you and they work on the same quantization problem, take the same equivalent transformation, and have similar optimization designs.\n\t\n* I noticed that the paper requires careful equivalent parameter initialization via the compared baseline SmoothQuant. I might wonder how it behaves without good initialization. For example, under asymmetric cases W4A8, W4A6, and W8A4, where the LLM-QAT shows SmoothQuant can behave terribly.\n\t\n* To conclude, I find the proposed two techniques are not novel and the paper lacks the necessary explanation and comparison. I see the challenge of the two techniques is how to combine the two kinds of learning together as they influence each other. However, current techniques seem can not solve this problem well. Thus, I think it would be better if the paper gives more description, and design consideration to the combination part, which might increase the novelty. For example, maybe alternately train these two techniques."
            },
            "questions": {
                "value": "Please check the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_WDuc",
                    "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790618656,
        "cdate": 1698790618656,
        "tmdate": 1700699035942,
        "mdate": 1700699035942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wKCbLSlSOB",
        "forum": "8Wuvhh0LYW",
        "replyto": "8Wuvhh0LYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_ZyAK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_ZyAK"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenges faced by large language models (LLMs) by optimizing quantization parameters. It is based on the SmoothQuant and Outlier Suppression+ and mainly contributes to a learnable pipeline. The idea is simple and trivial. However, the effect is good on various models and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Extend the existing quantization methods based on the thought of transformation to a learnable one, and give a pipeline with a stable optimization process\n- the idea of learnable scaling is simple, but making the learning stable and effective is a good contribution\n- conduct experiments on various models"
            },
            "weaknesses": {
                "value": "- The novelty is limited. The overall framework is based on two existing methods.\n- The learnable idea is not new. In Outlier Suppression+, the scaling has been designed to be learned via a scheme that does not depend on gradient."
            },
            "questions": {
                "value": "- Both outlier suppression+ and this paper highlight the scaling to be learned. An in-depth comparison needs to be provided, including the experimental perspective and the theoretical perspective.\n- The optimization based on little data and backward propagation makes the learning easy to be overfitted. More validation should be conducted to prove the generalization ability of this learning.\n- There are some new kinds of ways to decompose the outliers, e.g., https://arxiv.org/abs/2310.08041. Comprehensive experiments are suggested to further enrich the validation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833956848,
        "cdate": 1698833956848,
        "tmdate": 1699636087202,
        "mdate": 1699636087202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I4ox934Ez5",
        "forum": "8Wuvhh0LYW",
        "replyto": "8Wuvhh0LYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes OmniQuant,  a novel quantization technique for large language models (LLMs). OmniQuant introduces two learnable approaches to calibrate the quantized model, which are Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). The calibration is conducted in a block-wise manner and uses gradient updates to minimize the quantization error. The paper evaluates OmniQuant on various LLMs, quantization configurations, and natural language tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The LWC method proposed in the paper is simple yet effective, outperforming previous clipping-based approaches. The LET method addresses the shortcomings of SmoothQuant and contributes to better activation quantization performance. \n- The OmniQuant framework can be applied to both weight-only quantization and weight & activation quantization. The calibration process is relatively simple and fast.\n- Comprehensive ablation studies are conducted to analyze the effectiveness of each proposed technique."
            },
            "weaknesses": {
                "value": "- It would be beneficial to have additional experiments on more complex tasks. I am wondering how OmniQuant impacts the reasoning ability of LLMs, which can be evaluated by MMLU. GPT-4 evaluation is a bit ad-hoc nowadays, and there are also several better benchmarks to measure the instruction-tuned models performance, such as MT-Bench or AlpacaEval (correction: should be AlpacaEval instead of AlpacaFarm). Evaluating some stronger chatbots like Vicuna-v1.5 on them should be conducted.\n- Some strong related work is not discussed or compared, such as SpQR [1] and SqueezeLLM [2]. For instance, SqueezeLLM outperforms the proposed approach for wiki and c4 perplexity on LLaMA v1 7b and 13b under 3-bit and 4-bit weight-only quantization settings (see table 1 in their paper). Additional discussion and results should be added to compare OmniQuant with them. \n\n[1] Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T. and Alistarh, D., 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint arXiv:2306.03078.  \n[2] Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M.W. and Keutzer, K., 2023. SqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint arXiv:2306.07629."
            },
            "questions": {
                "value": "Please address the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "na"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1585/Reviewer_FuSa",
                    "ICLR.cc/2024/Conference/Submission1585/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834373684,
        "cdate": 1698834373684,
        "tmdate": 1700172189214,
        "mdate": 1700172189214,
        "license": "CC BY 4.0",
        "version": 2
    }
]