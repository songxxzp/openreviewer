[
    {
        "id": "d9sNZ6MOJq",
        "forum": "nR1EEDuov7",
        "replyto": "nR1EEDuov7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_D7DW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_D7DW"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the topic of adversarial signatures within the realm of generative models. At its core, the research introduces a method that involves the integration of a signature injector and a classifier. The ideal scenario is when the classifier can distinguish images that have been signed from the original, clean images based on subtle, almost imperceptible alterations made by the injector.\n\nThe research also explores the idea of securing generative models by embedding adversarial signatures directly into the model's parameters, potentially through a fine-tuning process. By doing so, the outputs from these secured generators inherently carry the adversarial signatures, making them detectable by the classifier.\n\nThe methodology involves processing training data with the signature injector to produce signed images. An existing generative model is then fine-tuned using these signed images, ensuring that the images it generates carry the adversarial signatures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality:\nInnovative Approach: The paper introduces a unique method centered around adversarial signatures within the realm of generative models. This approach, which involves the integration of a signature injector with a classifier, offers a fresh perspective in the domain of adversarial defenses.\n\nQuality:\nComprehensive Methodology: The paper's methodology, which encompasses the processing of training data with a signature injector followed by the fine-tuning of an existing generative model, is thorough. This structured approach suggests a well-thought-out experimental setup.\n\n\nClarity:\nStructured Presentation: The paper appears to be well-organized, with clear sections detailing the introduction of the signature injector and classifier, the methodology, and the broader context of adversarial signatures within generative models. This structured approach aids in understanding the paper's flow and main contributions.\n\nSignificance:\nAddressing a Crucial Challenge: The paper's focus on adversarial signatures within generative models addresses a significant challenge in deep learning. Given the importance of adversarial defenses in various applications, the paper's contributions in this area are relevant."
            },
            "weaknesses": {
                "value": "Originality:\nDelineation from Existing Methods: The paper could benefit from a clearer differentiation from existing methods in the domain of adversarial signatures or defenses. Without specific comparisons or benchmarks, it's challenging to gauge the unique contributions of the proposed method.\n\nQuality:\nLack of Detailed Experimental Results: The paper's methodology is described, but without detailed experimental results or comparisons, it's difficult to assess the effectiveness and robustness of the proposed approach. Comprehensive experiments or evaluations against benchmark methods would enhance the paper's credibility.\n\nPotential Overfitting: The approach of fine-tuning a generative model with signed images raises concerns about overfitting. If the model is too closely tailored to the signed images, its generalizability to unseen data or different adversarial attacks might be limited.\n\nClarity:\nNeed for Enhanced Technical Details: While the paper presents its concepts, more in-depth technical explanations or visual aids, such as diagrams or flowcharts, would help readers better understand the intricacies of the proposed method.\n\nSignificance:\nUnclear Practical Implications: The paper lacks a discussion on the practical implications or real-world applications of the proposed method. Understanding how this approach can be applied in real-world scenarios or its impact on existing systems would enhance its significance.\nQuestions on Scalability and Generalizability: The paper's focus on a specific approach to adversarial signatures within generative models raises questions about the method's scalability to larger datasets or more complex models. Additionally, its generalizability to other types of adversarial attacks or different domains remains unexplored.\n\nIn summary, while the paper presents valuable contributions, there are significant areas that need further exploration, validation, and clarification. Addressing these weaknesses could provide a clearer understanding of the paper's contributions and potential areas of improvement."
            },
            "questions": {
                "value": "Comparison with Existing Methods:\nHow does your approach to adversarial signatures differentiate from existing methods in the domain? Can you provide specific comparisons or benchmarks that highlight the advantages of your method?\n\nExperimental Validation:\nCould you provide more detailed results of your experiments, especially in comparison with state-of-the-art methods? How does your method perform in terms of robustness and effectiveness against various adversarial attacks?\n\nOverfitting Concerns:\nHow do you address potential overfitting when fine-tuning the generative model with signed images? Have you conducted experiments to assess the model's generalizability to unseen data or different types of adversarial attacks?\n\nPractical Implications:\nWhat are the real-world applications of your proposed method? How does it impact existing systems or applications that utilize generative models?\n\nScalability and Generalizability:\nHow scalable is your method, especially when applied to larger datasets or more complex models? Have you tested its generalizability across different domains or types of adversarial attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There were no immediate ethical concerns identified."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616390926,
        "cdate": 1698616390926,
        "tmdate": 1699636347419,
        "mdate": 1699636347419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OppqQUZDLV",
        "forum": "nR1EEDuov7",
        "replyto": "nR1EEDuov7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_3Y41"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_3Y41"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the detection methods for images generated by generative models and proposes to track the generated images by using adversarial signatures to make them more easily recognized by the designed detectors. Specifically, this paper constructs a signature injector W for learning to generate adversarial signatures and a classifier F for learning how to detect the images generated by W. Then, W and F are jointly trained. After that, the samples generated by W are then applied to finetune the original generative model G to obtain G'. The author elaborately designed the loss function as well as the binary code, and it has been shown through extensive experiments that the method can achieve good results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1)\tThis paper introduces adversarial examples into the detection of images generated by generative models, combined with joint training and watermark fine-tuning, which is novelty;\n(2)\tFor the generation model, this paper investigates the latest diffusion-based generation model, which is of good practical significance under the common use of AIGC nowadays;\n(3)\tThis paper is well-structured and logical. The author reviews the effectiveness and limitations of the proposed method from multiple perspectives. It points out the existing challenges, and gives possible solutions."
            },
            "weaknesses": {
                "value": "(1)\tThe two SOTA detection methods compared in the experiment are against the CNN-based and GAN-based generative model, whether there is any relevant paper for the watermarking of the diffusion model at present, if so, please supplement;\n(2)\tMissing a lot of experimental data on ImageNet, please add results comparing with SOTA on ImageNet;\n(3)\tSome formatting issues: (1) Please cite the graphs in order; (2) Please distinguish between periods and semicolons within the algorithm; (3) Please give the value of lamda in Figure 7.\n(4)\tThe authors should indicate possible future directions in conclusion"
            },
            "questions": {
                "value": "(1)\tFine-tuning an arbitrary G with samples generated by W to get G' is not unseen, in a sense it is inserting a backdoor that allows F to be better detected;\n(2)\tWhat is the difference between binary coding and multiclassification loss? What is the advantage of the binary coding?\n(3)\tI'm curious whether some of the latest adversarial defense methods can break this adversarial signature, such as diffusion-based purification. It would be great to add some experimental results in this scenario.\n(4)\tIt mentioned in Sec5.2 that \u201cThe complexity of re-training a detector every time upon the release of a new generator is O(r^2)\u201d, I wonder how to calculate this. For each new generator, the method of re-training a detector only needs to re-training once for the new generator."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699023120737,
        "cdate": 1699023120737,
        "tmdate": 1699636347323,
        "mdate": 1699636347323,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KHBXpkUYUs",
        "forum": "nR1EEDuov7",
        "replyto": "nR1EEDuov7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_ce6G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_ce6G"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a simple idea of finetuning image generator models to embed an adversarial noise signature into the generated images, so that the resulting images can be easily detected by a classifier. This has been achieved in two steps: first learning a signature injector W together with the detector F, and then finetuning the generator G to produce these signatures. Experiments have shown that the inserted signatures are easily detectable by the detector F."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The papers deals with an important problem regarding the safety of generative models and proposes an innovative solution.\n2) The proposed solution intuitively makes sense and is also feasible in practice. \n2) The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "There are five main concerns about the proposed solution.\n\n1) Firstly, the model F has been primarily described as a binary classifier whose goal is to distinguish between real images and \"signed\" synthetic images. What happens when the classifier F is fed with an \"unsigned\" synthetic image (i.e., an image generated from the original generator instead of the finetuned generator)? Shouldn't it be trained in such a way to detect \"unsigned\" synthetic images to the best possible extent along with \"signed\" synthetic images. Otherwise, why is a deep model such as ResNet-34 required to detect such \"signed\" images (a much smaller network should be enough to perform this relatively simpler task)?\n\n2) The proposition 3.3 on persistency against image restoration attacks does not appear to be logical. Why would an attacker try to add noise e to the original image and learn a model M such that M(W(x+e)) = x. Instead the goal of the attacker would be to find e such that W(x)+e = x. In fact, even this image restoration task would not be required if the goal of the attacker is to simply bypass the detector F. The attacker has to just mount a simple adversarial attack to fool the detector F, i.e., add noise to W(x) such that F(W(x)) = real.\n\n3) In terms of experiments, it is surprising that there is no comparison with a simple post-hoc watermarking approach (just add a unique generator-specific watermark to the generated image and categorize images with the watermark as synthetic), though this idea has been discussed in the introduction. It is true that the watermark will be decoupled from the generator, but it would still achieve the stated goals (imperceptibility, persistence, provenance of the generator, etc.). Also, the main paper does not talk about the ability to the detector to work under various image transformations. Only the appendix talks briefly about JPEG compression and cropping. How will the proposed detector work under various image transformations such as different levels of lossy compression, affine transformations, resolution and image format changes, etc.\n\n4) Finally, there has to be more clarity on the threat model for the proposed solution. Specifically, who owns the generator and who will own the detector and what are their motivations? For instance, if the owner of the generator model is not honest (they want to hide their identity), why would they insert the signature in the first place? If a third-party is finetuning the generator, why would the users trust the finetuned model released by the third-party in place of the original model released by the owner. Similarly, who will own the detector and what happens when the owner of the detector is not honest? The proposed solution envisages that both the signature detector and detector are learned jointly. How will this be possible if the owners are different? Who will enforce the rule that every generated image must be signed? If there is no enforcer, the problem again boils down to detecting unsigned synthetic images where generalization is the core issue.\n\n5) How is the proposed approach different from model poisoning attacks? The goal is to insert an \"imperceptible\" trigger pattern into the generated images such that the machine learning model will be easily triggered by these patterns and output  a specific decision (in this case, the synthetic class). One could potentially take any real vs. synthetic classification model and poison it to achieve this goal. Then, one has to just add the trigger to the generated images."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3886/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3886/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3886/Reviewer_ce6G"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699153331299,
        "cdate": 1699153331299,
        "tmdate": 1699636347241,
        "mdate": 1699636347241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s2nHzFAFuY",
        "forum": "nR1EEDuov7",
        "replyto": "nR1EEDuov7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_uR7H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3886/Reviewer_uR7H"
        ],
        "content": {
            "summary": {
                "value": "Injecting a universal adversarial signature into an arbitrary pre-trained generative model, in order to make its generated contents more detectable and traceable."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is explained clearly. \nThe paper is well-written."
            },
            "weaknesses": {
                "value": "The performance with or without the adversarial signature should be presented.\nThe term universal in used incorrectly since the signature depends on each image."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3886/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699602086753,
        "cdate": 1699602086753,
        "tmdate": 1699636347167,
        "mdate": 1699636347167,
        "license": "CC BY 4.0",
        "version": 2
    }
]