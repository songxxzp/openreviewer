[
    {
        "id": "klRstkihk5",
        "forum": "FJjHQS2DyE",
        "replyto": "FJjHQS2DyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Conditional Adversarial Support Alignment (CASA) whose aim is to minimize the Conditional Symmetric Support Divergence (CSSD) between the source\u2019s and target domain\u2019s feature representation distributions, aiming at a more discriminative representation for the classification task. Theoretical analyses are also provide in this work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed CASA addresses the drawback of Adversarial Support Alignment (ASA) by considering discriminative features to align the supports of two distributions, thus mitigating the risk of conditional distribution misalignment caused by indiscriminate reduction of marginal support divergence.\n2. Theoretical target error bound are provided in this work.\n3. Extensive experiments are conducted to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The major difference between this work and ASA is the conditional alignment, specifically CSSD and SSD. However, the label in the target domain is unknown, and the authors utilize the entropy conditioning technique described in [1] to address this issue. As far as I know, the method in [1] is not specifically designed for generating pseudo-labels. Could the authors please explain how they adapt this method to mitigate the error accumulation problem associated with using pseudo-labels? A detailed explanation from the authors would be appreciated.\n2. More SOTA methods are suggested to discuss and compare, such as SHOT [2], BIWAA [3], CoVi [4], etc.\n\n\n[1] Long, M., Cao, Z., Wang, J., & Jordan, M. I. (2018). Conditional adversarial domain adaptation. Advances in neural information processing systems, 31.\n\n[2] Liang, J., Hu, D., & Feng, J. (2020, November). Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In International conference on machine learning (pp. 6028-6039). PMLR.\n\n[3] Westfechtel, T., Yeh, H. W., Meng, Q., Mukuta, Y., & Harada, T. (2023). Backprop Induced Feature Weighting for Adversarial Domain Adaptation with Iterative Label Distribution Alignment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 392-401).\n\n[4] Na, J., Han, D., Chang, H. J., & Hwang, W. (2022, October). Contrastive vicinal space for unsupervised domain adaptation. In European Conference on Computer Vision (pp. 92-110). Cham: Springer Nature Switzerland."
            },
            "questions": {
                "value": "see Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_o6SV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697859199253,
        "cdate": 1697859199253,
        "tmdate": 1699636434762,
        "mdate": 1699636434762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UkYVOFI2KT",
        "forum": "FJjHQS2DyE",
        "replyto": "FJjHQS2DyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the distribution shift problem for the machine learning model. Specifically, the authors consider the label shift scenario and analyze the limitations in current label shift research, i.e., the strict identical assumption on the conditional distribution $P_{X|Y}$. To address this problem, a novel metric is developed based on the symmetric support divergence (SSD). Mathematically, the proposed metric can be taken as the sliced SSD on each conditional distribution $P_{X|Y=y}$. A new generalization upper bound and some theoretical properties of the proposed metric are provided, which ensure the metric-based model can reduce the generalization error and show the relation between marginal SSD and conditional SSD. Experiments are conducted to show the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ A conditional variant of SSD and corresponding theoretical analysis are provided.\n+ A discrepancy optimization model is proposed to address the domain adaptation with label shift.\n+ Superior experiment results are achieved."
            },
            "weaknesses": {
                "value": "- The basic problem in this paper is indeed equivalent to the generalized target/label shift, where label distribution and conditional distribution change simultaneously. However, many important and closely related references are not introduced and discussed.\n- Consider the existing results for generalized target/label shift, the generalization error analysis provided in Thm. 1 seems to be less compact and not informative.\n- Important theoretical results for the main merits, i.e., conditional variant of SSD, are missing, which makes the proposed method less technically sound.\n- The organization and clarity should be improved. Some justification and intuition for the math definition or theoretical results are insufficient.\n- The experiment comparison is insufficient, where some related works are omitted."
            },
            "questions": {
                "value": "1. The essential setting and problem that are considered in this submission is indeed similar to the well-known generalized target/label shift [a-f], which are not properly introduced and discussed. Besides, the label shift problem is also extensively studied and has shown promising theoretical results in many studies. From both the generalized target/label shift view and label shift view, this paper does not provide sufficient discussion with these existing methodological and theoretical results. Thus, it is hard to evaluate this paper's contributions, making this work less persuasive.\n\n2. In the generalized target/label shift literature [e, f], generalization bounds and theoretical analysis are also provided. Compared with these results that compactly decompose the shift on the joint distribution as the terms determined by label discrepancy and conditional discrepancy, this paper induces additional constants, i.e., joint error on both domains and the non-negative constants $\\delta, \\gamma$ induced by IMD.\n\n3. Considering the existing results, the main contribution in this paper is the new conditional discrepancy metric. However, it seems that it cannot be rigorously considered as the class-wise IMD. Specifically, note for the IMD in Def. 3, the weights of the two expected divergence terms are 1; however, in the conditional variant in Def. 4, the divergence terms are weighted by the label probability masses $P(Y=y)$. In such a definition, it naturally raises an crucial question, i.e., is the conditional SSD in Def.4 defines a metric on conditional distribution? This theoretical property is the foundation for the proposed method and should be treated rigorously.\n\n4. The justifications of the derived theoretical results should be improved. Though Thm. 1 ensures that the generalized label shift correction is sufficient to mitigate the label discrepancy and conditional discrepancy, the constants induced in upper-bound seem to be intractable.\n\n5. The discussion in Remark 3 is insufficient and seems to be improper. The advantages of existing results [e] are not properly stated, i.e., literature [e] does not induce additional constant that cannot be controlled by the learning model. Besides, the related works [d,f] are not discussed and compared.\n\n6. Since there are many related works in correcting label shift and conditional shift simultaneously [a-f], they should also be carefully compared in experiment validation. \n\n[a] Zhang, Kun, et al. \"Domain adaptation under target and conditional shift.\" International conference on machine learning. PMLR, 2013.\n\n[b] Gong, Mingming, et al. \"Domain adaptation with conditional transferable components.\" International conference on machine learning. PMLR, 2016.\n\n[c] Ren, Chuan-Xian, Xiao-Lin Xu, and Hong Yan. \"Generalized conditional domain adaptation: A causal perspective with low-rank translators.\" IEEE transactions on cybernetics 50.2 (2018): 821-834.\n\n[d] Rakotomamonjy, Alain, et al. \"Optimal transport for conditional domain matching and label shift.\" Machine Learning (2022): 1-20.\n\n[e] Tachet des Combes, Remi, et al. \"Domain adaptation with conditional distribution matching and generalized label shift.\" Advances in Neural Information Processing Systems 33 (2020): 19276-19289.\n\n[f] Kirchmeyer, Matthieu, et al. \"Mapping conditional distributions for domain adaptation under generalized target shift.\" International Conference on Learning Representations. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4569/Reviewer_UE75"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698254612378,
        "cdate": 1698254612378,
        "tmdate": 1700573999010,
        "mdate": 1700573999010,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U6262DyO3l",
        "forum": "FJjHQS2DyE",
        "replyto": "FJjHQS2DyE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_XqJP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4569/Reviewer_XqJP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source\u2019s and target domain\u2019s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results. However, the model's novelty is incremental."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposed conditional adversarial support alignment (CASA) to minimize the conditional symmetric support divergence between the source\u2019s and target domain\u2019s feature representation distributions. Generally, the paper is well-written and easy to follow. They evaluate the model on several benchmarks from various types of results."
            },
            "weaknesses": {
                "value": "The model's novelty is incremental over multiple loss functions. The alignment loss in Eq(10) is more like pair-wise alignment loss, which has been explored before for cross-domain graph alignment. It is hard to verify the novelty.\n\nFrom the experiments, they show the improvements when \\alpha decreases. However, there is no insight why this happens. It is better to discuss the intuition and data used behind. It needs more visualization to demonstrate the improvement."
            },
            "questions": {
                "value": "The novelty clarification.\nThe performance analysis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809286649,
        "cdate": 1698809286649,
        "tmdate": 1699636434584,
        "mdate": 1699636434584,
        "license": "CC BY 4.0",
        "version": 2
    }
]