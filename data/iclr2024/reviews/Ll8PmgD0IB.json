[
    {
        "id": "7Nkt1LsRE3",
        "forum": "Ll8PmgD0IB",
        "replyto": "Ll8PmgD0IB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_apk6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_apk6"
        ],
        "content": {
            "summary": {
                "value": "This paper improves the efficiency of SVD decomposition in gradient-projection-based continual learning method. They introduce local model space projection (LMSP) to improve the running efficiency of SVD decomposition. At the same time, LMSP can facilitate both forward and backward transfer of gradient-projection-based methods in continual learning. The authors also provide some theoretical analysis of LMSP. Experiments on several datasets evaluate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper introduces local model space projection to GPM to improve its running efficiency."
            },
            "weaknesses": {
                "value": "* This paper writing needs to be further improved.  It would be better to directly state the intuitive idea and its illustration. This would make the main idea clearer and easier to understand. \n\n\n* The authors argue that SVD decomposition is computationally costly. This is true but it seems not an important problem in GPM since SVD decomposition only happens after finishing training each task, not every iteration. Therefore, the computation cost of SVD decomposition is minor compared to the overall training cost. \n\n\n* The authors state that their method could reduce the complexity of SVD basis computation, but there is no empirical evaluation of the overall training efficiency improvement with the proposed method compared to the GPM itself. \n\n\n* From the empirical results, LMSP improves the backward transfer, but the overall accuracy drops in some cases. The paper states that LMSP can improve both the forward and backward transfer, which does not support the claim."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697744961517,
        "cdate": 1697744961517,
        "tmdate": 1699636434117,
        "mdate": 1699636434117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eznEOYYUER",
        "forum": "Ll8PmgD0IB",
        "replyto": "Ll8PmgD0IB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
        ],
        "content": {
            "summary": {
                "value": "Based on the basic framework of orthogonal-projection-based CL methods, this article proposes a local model space projection (LMSP) based efficient continual learning framework to help reduce the complexity of computation. The authors provide a theoretical analysis of backward knowledge transfer. Experiments based on multiple datasets demonstrate the effectiveness of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-structured with clear writing.\n- Leveraging the problem definition from previous research, this study presents a novel local model space projection approach, optimizing continual learning.\n- The authors also provide a theoretical analysis of the convergence."
            },
            "weaknesses": {
                "value": "- The problem definition, framework, and convergence analysis of this work are derived from existing work. While the efficiency approach is intuitive and easy to understand, its novelty causes me concern.\n- The authors use local low-rank matrices defined by anchor points to approximate each layer parameter matrix. However, the accuracy of this approximation, and in particular how it is affected by m, is not discussed. Moreover, the proposed framework and analysis also ignore this issue. \n- The author introduces LLRA to improve computational efficiency. However, they do not perform experiments to evaluate the computational complexity and specifically do not show the saved wall-clock time compared with the LRA method."
            },
            "questions": {
                "value": "- The author states that there is no significant difference between the two methods in selecting anchor points. Can you give some intuitive explanation?\n- Is there some relationship between ranking and the number of anchors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_CGvu"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698421290685,
        "cdate": 1698421290685,
        "tmdate": 1700631783364,
        "mdate": 1700631783364,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "93bR56DBnw",
        "forum": "Ll8PmgD0IB",
        "replyto": "Ll8PmgD0IB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_Shvn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_Shvn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Local Model Space Projection, a method in continual learning that aims at avoiding forgetting and encourage knowledge transfer by performing orthogonal updates of parameters over the sequence of tasks. The method considers three regimes: 1) forgetting avoidance, 2) forward transfer, 3) backward transfer, which can be represented as variants of the problem of finding orthogonal directions for parameter updates. The method constructs local model spaces of each task by selecting some anchoring points from the task's representation. Using these representations, a similarity between tasks can be measured across local representations to determine whether the task has local sufficient projection, local positive correlation or local relative orthogonality. Theoretical analyses along with experimental results are provided. Experiments are reported for 4 benchmark datasets, and compared to a range of SOTA continual learning methods from different families (regularization, replay, orthogonalization). Results are provided in terms of accuracy and backward transfer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes an original method that exploits the idea of orthogonal projections to learn new tasks whilst controlling forgetting and encouraging forward and backward transfer. The consideration of particular regimes for each of these problems, and the fact that each of these regimes can be addressed with the same underlying idea of projections that consider local representations of tasks seems novel and useful. \n- The paper is very clear, easy to follow and mostly complete as it considers both theoretical and experimental demonstrations of how and why it works. \n- The paper is somewhat significant in the sense that it seemingly not only tackles forgetting but also knowledge transfer, and it presents some good results in both accuracy and backward transfer."
            },
            "weaknesses": {
                "value": "- Although the proposed method seems quite competitive in terms of experimental results, there is no report on the performance of forward transfer. This is extremely relevant as forward and backward transfer are usually in trade-off (the more forward, the less backward transfer and vice-versa). How can you guarantee that the good results in backward transfer do not require sacrificing forward transfer, or even just the fact of learning the new task reasonably well?"
            },
            "questions": {
                "value": "- Can you provide actual performance numbers for forward transfer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731960791,
        "cdate": 1698731960791,
        "tmdate": 1699636433837,
        "mdate": 1699636433837,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "esEfKCH2Ai",
        "forum": "Ll8PmgD0IB",
        "replyto": "Ll8PmgD0IB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an interesting topic, continual learning, which aims to learn a series of tasks without forgetting. . The existing CL methods require either an extensive amount of resources for computing gradient projections or storing a large number of old tasks\u2019 data.  . In this paper, a local model space projection\n(LMSP) is proposed to not only significantly reduce the complexity of computation, but also enables forward and backwardknowledge transfer. Extensive experiments on several public datasets demonstrate the efficiency of our approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written.\n2. The research topic is very interesting."
            },
            "weaknesses": {
                "value": "1. Performing the forward and backward knowledge transfer has been done in the existing works.\n2. The proposed approach relies on the task information, which can not be used in task-free continual learning.\n3. The proposed approach does not always achieve the best performance in some datasets.\n4. Although the proposed approach can reduce computational costs but would increase more parameters."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4563/Reviewer_ZQXi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4563/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773698469,
        "cdate": 1698773698469,
        "tmdate": 1699636433757,
        "mdate": 1699636433757,
        "license": "CC BY 4.0",
        "version": 2
    }
]