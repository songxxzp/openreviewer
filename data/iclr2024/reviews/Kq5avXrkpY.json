[
    {
        "id": "tovhJnkBMJ",
        "forum": "Kq5avXrkpY",
        "replyto": "Kq5avXrkpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_mMnw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_mMnw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an innovative method to enhance communication efficiency in distributed machine learning model training. The proposed method incorporates without-replacement sampling and gradient compression, leading to improved performance in comparison to existing algorithms. The paper provides theoretical analysis and experimental results to support the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The paper introduces an innovative method to enhance communication efficiency in distributed machine learning model training. This approach incorporates without-replacement sampling and gradient compression, leading to improved performance compared to existing algorithms.\n\n2.The paper offers a comprehensive validation of the proposed approach by providing both theoretical analysis and empirical results. These findings illustrate the superiority of the proposed method over existing algorithms in terms of convergence rate and communication efficiency.\n\n3.In addition to its contributions, the paper conscientiously addresses the limitations and challenges associated with the proposed approach. It also suggests potential avenues for future research in this area."
            },
            "weaknesses": {
                "value": "The DIANA-NASTYA algorithm's theoretical analysis is conducted without the need for a strongly convex assumption. A strongly convex assumption often allows for more precise and efficient convergence guarantees. To investigate the impact of such an assumption, further analysis would be necessary to determine whether the algorithm's performance improves, and if so, to what extent.\n\nThe experiments primarily revolve around deep learning applications, which are typically considered non-convex problems. However, it's important to note that the paper lacks theoretical analysis specifically tailored to these non-convex problem settings.\n\nThis work appears to be an incremental extension of DIANA. While it introduces some additional techniques, the improvements achieved may not be substantial or readily discernible."
            },
            "questions": {
                "value": "see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839817331,
        "cdate": 1698839817331,
        "tmdate": 1699636472667,
        "mdate": 1699636472667,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QDGpVn1y6d",
        "forum": "Kq5avXrkpY",
        "replyto": "Kq5avXrkpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_TutK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_TutK"
        ],
        "content": {
            "summary": {
                "value": "This paper combines several existing techniques to accelerate the communication complexity of distributed stochastic optimization. It is known that random reshuffling gives better convergence rate of stochastic gradient descent, and gradient compression can save communication bandwidth by sending fewer bits over the network. It is quite natural to consider a distributed learning algorithm with both random reshuffling and gradient compression. However, the noise introduced by the gradient compression might cancel out the improvements of convergence of random reshuffling, thus it is not a priori clear if the combination is actually useful. This paper proves several theoretical guarantees and that random reshuffling can indeed improve upon some existing algorithms with gradient compression. This paper also provide experiments that demonstrate the results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The theorems and proofs are stated very clearly. I don't see any major flaws in the proofs. The paper discusses the improvements over previous results thoroughly."
            },
            "weaknesses": {
                "value": "The results in this paper are not suprising. The proofs only utilize existing methods and techniques and are more or less routine."
            },
            "questions": {
                "value": "I only have one major question. I notice that the authors prove some non-strongly convex results in the appendix B.2 and C.2. Can the authors provide some discussion on this matter? How does the non-strongly convex setting change the results and the improvements? Have the authors considered nonconvex setting? I really like to see more discussion for alternative assumptions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848795613,
        "cdate": 1698848795613,
        "tmdate": 1699636472571,
        "mdate": 1699636472571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NsAgCWpIrb",
        "forum": "Kq5avXrkpY",
        "replyto": "Kq5avXrkpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_VtDw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_VtDw"
        ],
        "content": {
            "summary": {
                "value": "In this work, authors study the behavior of Federated learning with gradient compression and without-replacement sampling. Authors first develop a distributed variant of random reshuffling with gradient compression (Q-RR) and show that the compression variance will overwhelm the variance of the gradient. Next, authors propose a variant\nof Q-RR called Q-NASTYA, which uses local gradient steps and different local and global stepsizes.\n\nThanks for the authors' feedback, I have changed my score accordingly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea sounds interesting to me. In federated learning, the communication cost can be the bottleneck for the scalability of the training system, especially on edge devices. Moreover, without-replacement has attracted lots of interest in recent studies. The motivation of this work is solid."
            },
            "weaknesses": {
                "value": "There are several questions that need to be addressed:\n1) I see no experiment results in the main draft, although authors put those details in the supplementary, I still believe it shall be put in the main part.\n2) The communication compression is at most 50% since only half of the rounds are compressed. This is not the optimal design since there should be a better way for compressing the second round of communication, since you have already assumed that all workers participate the updating of the global x_t at each round.\n3) About the compression, why not use the error-compensated compression (e.g. DoubleSqueeze and PowerSGD) since it is the SOTA method for incorporating compression in optimization? With error compensation, the variance of the compression introduced in the training will be greatly reduced and you even do not need the compression to be unbiased.\n4) The paper is hard to follow, I still cannot get a high level comparison of your algorithms with other works and fail to find a clue about why your design works."
            },
            "questions": {
                "value": "Please refer to my question about the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4883/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4883/Reviewer_VtDw",
                    "ICLR.cc/2024/Conference/Submission4883/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698979894291,
        "cdate": 1698979894291,
        "tmdate": 1700636082707,
        "mdate": 1700636082707,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lpLKc655na",
        "forum": "Kq5avXrkpY",
        "replyto": "Kq5avXrkpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_iwJs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4883/Reviewer_iwJs"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the convergence of federated learning algorithms with gradient compression and random reshuffling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors show that the naive combination of compression and reshuffling (called Q-RR) doesn\u2019t outperform compressed SGD without reshuffling. To alleviate this issue, they develop an algorithm combining Q-RR with DIANA, hence reducing the compression variance. They then introduce a version of the algorithm supporting the local steps."
            },
            "weaknesses": {
                "value": "Also questions included below:\n\n-- The notation for $\\pi$ in section 1.3 doesn\u2019t match the rest of the paper.\n\n-- Algorithm 2, line 3: is $\\pi_m$ sampled for each machine $m$? Do machines have different permutations? Same for other algorithms.\n\n-- Algorithm 3: Are lines 6 and 7 preformed on the server? Then they are not performed in parallel\n\n-- Definition 2.1 - What is the meaning of the sequence? What is the \u201cscope\u201d of the sequence (you have different $\\pi_m^i$ for different \n$t$)? If this sequence is different for each $t$, how do you aggregate $\\sigma_{rad}^2$ over different $t$ (do you take a maximum)? Also, $x_\\star$ is undefined\n\n-- I think that derivations (e.g. on page 25) are rather hard to parse. I would introduce auxiliary notation for $f_m^{\\pi_m^i}$ and for $h \\cdots$."
            },
            "questions": {
                "value": "-- Page 4 - \u201cFinally, to illustrate our theoretical findings we conduct experiments on federated linear regression tasks.\u201d - Did you mean logistic regression? Also, I believe that this line makes your experimental results look weaker than what they actually are.\n\n-- Since you use $\\zeta_\\star$ and $\\sigma_\\star$ in multiple results, they should be defined in an Assumption, instead of being defined in Theorem 2.1\n\n-- Page 7 - \u201cwe can do enable\u2026\u201d\n\n-- Page 15, after Equation (9) - broken math\n\n-- Page 24: $f^{i, \\pi}$\n\n-- \u201cmygreen\u201d in bookmark names, e.g. \u201cAlgorithm mygreen Q-RR\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4883/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699456939190,
        "cdate": 1699456939190,
        "tmdate": 1699636472382,
        "mdate": 1699636472382,
        "license": "CC BY 4.0",
        "version": 2
    }
]