[
    {
        "id": "u3zpfXbdew",
        "forum": "5BoXZXTJvL",
        "replyto": "5BoXZXTJvL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission207/Reviewer_dK5u"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission207/Reviewer_dK5u"
        ],
        "content": {
            "summary": {
                "value": "This study introduces GBLM-Pruner, a gradient-based approach for the unstructured pruning of large language models (LLMs). The core idea of this research is centered around a Taylor expansion applied to the loss function. This method estimates the change in loss by employing a combination of first-order gradient and second-order approximation (OBD). Empirical evaluations using LLaMA and LLaMA-2 demonstrate that GBLM-Pruner outperforms other methods such as magnitude pruning, SparseGPT, and Wanda in terms of performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper highlights the significance of gradients in the pruning of large language models (LLMs). The author presents a Taylor-based approach to identify critical parameters, yielding favorable outcomes in comparison to earlier techniques.\n2. The work sets robust benchmarks by contrasting the proposed methods with various existing baselines, offering valuable insights for the research community."
            },
            "weaknesses": {
                "value": "1. To my knowledge, SparseGPT is similarly a gradient-based approach, utilizing Taylor expansion and second-order Hessian for estimating parameter importance. In light of this, the contribution of the current work may appear somewhat constrained.\n2. As depicted in Figure 2, SparseGPT, Wanda, and the newly introduced GBLM-Pruner exhibit closely comparable results, with only minor differences in Perplexity (PPL). There isn't compelling evidence to suggest that GBLM-Pruner significantly outperforms its predecessors.\n3. It would be beneficial if the author could include data on the latency of the pruned LLMs, particularly in the context of 2:4 sparsity acceleration."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Reviewer_dK5u"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698403164216,
        "cdate": 1698403164216,
        "tmdate": 1699635946149,
        "mdate": 1699635946149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wVcKzLRO1W",
        "forum": "5BoXZXTJvL",
        "replyto": "5BoXZXTJvL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission207/Reviewer_gsUn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission207/Reviewer_gsUn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to integrate the first-order gradient into the unstructured pruning of large language models and achieves superior performance compared to sparseGPT and Wanda."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. A superior method compared to SparseGPT and Wanda on unstructured pruning of large language model\n2. The authors have conducted extensive experiments to assess the method's effectiveness on LLaMa-1 and LLaMa-2. Additionally, the paper illustrates the impact of various gradient and activation combinations on the determination of parameter importance.\n3. The paper is well-written, offering clarity and ease of understanding in its presentation."
            },
            "weaknesses": {
                "value": "1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.\n2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.\n3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important  to the readers to understand the efficiency of the pruned model."
            },
            "questions": {
                "value": "1. Could you specify the error function utilized for calculating gradients in your approach?\n2. Have you conducted any latency experiments on the pruned model, particularly under the 2:4 or 4:8 configurations?\n3. Is the calibration set employed for your methods and Wanda, SparseGPT identical?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Reviewer_gsUn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730280551,
        "cdate": 1698730280551,
        "tmdate": 1699635946062,
        "mdate": 1699635946062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8FOyF3WZuE",
        "forum": "5BoXZXTJvL",
        "replyto": "5BoXZXTJvL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission207/Reviewer_B7C2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission207/Reviewer_B7C2"
        ],
        "content": {
            "summary": {
                "value": "* The paper proposes to integrate gradient information into pruning criteria currently used for LLMs.\n* The corresponding GBLM method is evaluated on Llama models for perplexity and zero-shot tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is easy to follow and describes the proposed method in good detail.\n* The method is evaluated on strong LLama models rather than older LLMs like OPT.\n* Source code is provided, aiding reproducability."
            },
            "weaknesses": {
                "value": "* Integrating gradient information into pruning criteria is a well studied area, see for example [1, 2, 3, 4]. This is currently not discussed under Related Work.\n* Consequently, the novelty of GBLM is quite limited. For instance, the analysis in Section 2.3 is very similar to derivations presented in [2]. Ultimately, GBLM seems to be a minor variation of a diagonal Fisher scheme (using both gradients and activations while slightly tweaking norms in a heuristic manner).\n* The most robust form of evaluation, perplexity, shows only very slight improvements relative to prior work of < 0.1 points, while dropping noticably from the baseline. I am not sure if this is a significant enough improvement in practice.\n* It is unclear how the gradient calculation impacts the speed and compute/memory requirements of the pruning process. Being fast and memory efficient is one of the key strengths of SparseGPT and Wanda, hence I think a detailed comparison/discussion of this aspect would be important.\n\nUnfortunately, at this time, I find neither the method itself nor the empirical results interesting enough to recommend acceptance.\n\n[1] Pruning convolutional neural networks for resource efficient inference, Molchanov et al.\n\n[2] WoodFisher: Efficient Second-Order Approximation for Neural Network Compression, Singh et al.\n\n[4] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, Kurtic et al.\n\n[3] Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al."
            },
            "questions": {
                "value": "* See weaknesses, in particular the compute/memory efficiency point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission207/Reviewer_B7C2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757865107,
        "cdate": 1698757865107,
        "tmdate": 1699635945974,
        "mdate": 1699635945974,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EfNnPFnEDk",
        "forum": "5BoXZXTJvL",
        "replyto": "5BoXZXTJvL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission207/Reviewer_k6s5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission207/Reviewer_k6s5"
        ],
        "content": {
            "summary": {
                "value": "This study introduces GBLM-Pruner, a new post-training pruning technique designed for large language models, which leverages gradient information. The authors provide both theoretical rationale and empirical assessments that demonstrate GBLM-Pruner outperforms other prominent baselines, such as Wanda and SparseGPT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-organized, effectively presenting the method with clear descriptions and comprehensive empirical evaluations.\n- Both theoretical explanations and empirical results are presented to validate the theoretical explanations and empirical results.\n- The paper includes plenty of ablation studies, encompassing diverse sparsity levels, different pruning metrics, assessments of dependency on calibration samples, and visualizations that highlight the specifics of sparse patterns."
            },
            "weaknesses": {
                "value": "- The improvements achieved by GBLM-Pruner, as compared to other baselines like SparseGPT and Wanda, appear to be relatively modest. For instance, in Table 2, under 50% unstructured sparsity, GBLM-Pruner (l1) yields perplexity reductions of only 0.06, 0.09, and 0.05 compared to Wanda on LLaMA-2-7B/13B/70B, respectively. Additionally, in Figure 2, the curves for Wanda and GBLM-Pruner exhibit significant overlap.\n  \n- I'm unclear about the rationale behind experimenting with the pruning metrics listed in Line 7/8. It seems that some of these metrics may not provide meaningful insights.\n\n- It's essential to understand the memory and time requirements during the pruning process of GBLM-Pruner. Obtaining gradient information can impose a significant memory cost, and it may not be feasible to conduct this process in a layer-wise manner. Storing intermediate features for the backward process could further impact memory usage. Thus, it would be valuable to compare these memory and time requirements with those of other baseline methods for a more comprehensive assessment of GBLM-Pruner's practicality."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission207/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698854712805,
        "cdate": 1698854712805,
        "tmdate": 1699635945897,
        "mdate": 1699635945897,
        "license": "CC BY 4.0",
        "version": 2
    }
]