[
    {
        "id": "ZCkRmKUSOA",
        "forum": "1JtTPYBKqt",
        "replyto": "1JtTPYBKqt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_BEfk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_BEfk"
        ],
        "content": {
            "summary": {
                "value": "This is a very interesting paper, considering the burgeoning field of neural architecture designs which has made it increasingly challenging for researchers to position their contributions or establish relationships between their designs and existing ones. The paper presents a novel problem of neural architecture retrieval, aiming to automate the discovery of similar neural architectures. The authors identify the limitations of existing graph pre-training strategies, which are unable to handle the computational graphs in neural architectures due to the graph size and motifs. They propose a creative solution by dividing the graph into motifs, which are then used to reconstruct the macro graph, addressing the identified issues. Moreover, the introduction of multi-level contrastive learning is put forth to attain precise graph representation learning. The paper boasts extensive evaluations of both human-designed and synthesized neural architectures, showcasing the superiority of their proposed algorithm. The authors also construct a valuable dataset comprising 12k real-world network architectures along with their embeddings, laying a solid foundation for neural architecture retrieval. This endeavor may address a pressing issue in the domain with pre-training an embedding database for finding and comparing neural architectures. The paper potentially opens up a new domain within the neural architecture community, paving the way for more organized and efficient exploration in this field by building an embedding database, and the video demo shows promising applications."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The motivation behind the paper is well-articulated and resonates with the ongoing challenges faced by researchers in situating their contributions amidst a plethora of existing neural architectures. The introduction of Neural Architecture Retrieval as a solution to automate the discovery of similar neural architectures is timely and could significantly alleviate the existing bottleneck.\n* The methods used in the paper sound and well-justified. The logic behind each step of the solution is sound and reasonable, showcasing a thorough understanding of the challenges at hand. For instance, the approach to addressing the repeated design of blocks is practical and applicable, demonstrating a commendable level of methodological rigor and a pragmatic stance.\n* The evaluation metrics, particularly the information retrieval scores, are promising, especially when applied to the nas dataset. This suggests that the proposed methods are effective and could potentially set a new standard in evaluating neural architectures. \n* Contribute a 12k real-world computational graph and its corresponding embedding database. The availability of such a dataset could spur further research and development in the domain of neural architecture retrieval and related areas."
            },
            "weaknesses": {
                "value": "* Page 5 Section 3.4 Eq 4: The objective of the first stage may have a better way. The encoder encodes the architecture into motifs, and then concatenates the embeddings.  Directly sampling the highest-frequency motifs $H$ to represent the main design for large models may more reasonable, especially considering that the concatenation of motif embeddings cannot backpropagate the gradients without two stages.\n\n* Page 7 Section 4.3: The details of the evaluation part are lacking. Although it covers the mainstream information retrieval scores, the parameters of the computational score are still unclear. For example, when testing the NDCG, is graded relevance or non-graded relevance used? It would be better to provide a formula here."
            },
            "questions": {
                "value": "Page 5 Section 3.4 Eq 4: What role does the context graph $G_s$ play?\n\nPage 6, Section 4.1: How were the real-world repositories collected, and how were the models obtained? Will the 12k real-world computational graphs, along with corresponding labels, be made available for follow-up work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698461009455,
        "cdate": 1698461009455,
        "tmdate": 1699636566929,
        "mdate": 1699636566929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BHx5lyiFsS",
        "forum": "1JtTPYBKqt",
        "replyto": "1JtTPYBKqt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_wPum"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_wPum"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new problem of Neural Architecture Retrieval, which aims to find similar neural architectures from a large collection of existing and potential ones. The paper introduces a novel graph representation learning framework that leverages the motifs of neural architectures and contrastive learning to achieve accurate and efficient retrieval. The authors also construct a new dataset of 12k real-world neural architectures with their embeddings and evaluates the proposed method on both real-world and NAS architectures, showing its superiority over baselines. However, the paper has some issues that need to be addressed. My detailed comments are as follows."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper presents a new problem called Neural Architecture Retrieval (NAR). This problem is to find similar neural architectures quickly and easily from a big pool of existing and possible designs. It\u2019s a smart way to make the search for neural architectures simpler and more efficient.\n\n2.\tThe authors propose a novel and easy-to-understand graph representation learning framework that addresses the computational graph in neural architectures. This framework adopts motifs of neural architectures and multi-level contrastive learning for accurate graph representation learning.\n\n3.\tThe paper contributes the community by creating a new dataset with 12k real-world network architectures and their embeddings. This dataset is specifically designed for neural architecture retrieval and demonstrates the effectiveness of the retrieval algorithm. It\u2019s a helpful benchmark for everyone working in this field.\n\n4.\tThe experimental results on both human-designed and synthesized neural architectures benchmarks verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1.\tThe proposed NAR appears to focus solely on the topological similarity of architectures. However, it is important to note that the similarity between architectures can vary across different tasks or datasets. For instance, certain architectures might yield comparable results in image classification but diverge significantly in performance when applied to other tasks. Could the authors provide additional insights and elaborations on this matter?\n\n2.\tThe authors introduce a motif-level contrastive learning approach, wherein the corresponding context graph is treated as the positive sample. However, this raises a concern as there may be other context graphs that also encompass the same motifs, yet they are deemed as negative samples, which seems unreasonable. Could the authors provide further clarification and justification for this aspect of their methodology?\n\n3.\tIt is unclear how the authors collect/annotate the ground-truth label of the architecture dataset. Without such labels, it is infeasible to calculate the correlation between the different networks. Please provide more details.\n\n4.\tIn the \"Related Work\" section, it would enhance the manuscript's thoroughness if the authors could offer a more comprehensive discussion on mainstream NAS methods. This should include an exploration of reinforcement learning-based NAS methods, as referenced in [A-G].\n\n[A] Designing Neural Network Architectures using Reinforcement Learning. ICLR 2017.\n\n[B] Learning Transferable Architectures for Scalable Image Recognition. CVPR 2018.\n\n[C] UNAS: Differentiable Architecture Search Meets Reinforcement Learning. CVPR 2020.\n\n[D] Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search. ICML 2020.\n\n[E] Contrastive Neural Architecture Search with Neural Architecture Comparators.  CVPR 2021.\n\n[F] Disturbance-immune Weight Sharing for Neural Architecture Search. Neural Networks 2021.\n\n[G] Towards Accurate and Compact Architectures via Neural Architecture Transformer. TPAMI 2021."
            },
            "questions": {
                "value": "1. In Page 2 line 8, \u201cexact\u201d should be \u201cexactly\u201d.\n\n2. In Page 2 line 11, \u201cthrough\u201d should be \u201cby\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760081494,
        "cdate": 1698760081494,
        "tmdate": 1699636566852,
        "mdate": 1699636566852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HtxEu7b6Kc",
        "forum": "1JtTPYBKqt",
        "replyto": "1JtTPYBKqt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_NBZG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5529/Reviewer_NBZG"
        ],
        "content": {
            "summary": {
                "value": "This paper defines a new question called Neural Architecture Retrieval (NAR) which returns a set of similar neural architectures given a query neural architecture. To address NAR problem, this paper proposes to split the graph into several motifs and rebuild the graph through treating motifs as nodes in a macro graph. Then this paper uses two-level pretrain task to train the architecture representation for retrieval."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work proposes a large new NAS dataset of 12k real-world neural architectures rather than pre-define search space. In my opinion, this thing makes a lot of sense for the NAS field which can explore the diversity of search space species architectures.\n\n2. The idea uses motifs to encode architecture and reduce the graph size is novel and reasonable to encode architecture and capture the connection between structures in one architecture.\n\n3. The two-level pretrain task to train the architecture's embedding is reasonable and effective."
            },
            "weaknesses": {
                "value": "1. In my opinion, the NAR problem to find similar architectures for the query architecture doesn't seem particularly significant for real-world usage, Can the author point out the need for this problem and more application scenarios?\n\n2. Some retrieval-based papers[1-3] for giving a query dataset to search architectures should be discussed.\n\n3. Another question is that I want to know how much performance can be achieved by retrieving similar models only based on the architecture corresponding to the text description or the code by a language mode like Chatgpt or other language-based retrieval model.\n\nRefs: \n\n[1] Task-Adaptive Neural Network Search with Meta-Contrastive Learning. NeurIPS 2021.\n\n[2] MetaGL: Evaluation-Free Selection of Graph Learning Models via Meta-Learning. ICLR 2023\n\n[3] Retrieving GNN Architecture for Collaborative Filtering. CIKM 2023"
            },
            "questions": {
                "value": "One small question is that I can't find how the ground truth topk set is constructed, can the author describe it in detail?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5529/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853954691,
        "cdate": 1698853954691,
        "tmdate": 1699636566760,
        "mdate": 1699636566760,
        "license": "CC BY 4.0",
        "version": 2
    }
]