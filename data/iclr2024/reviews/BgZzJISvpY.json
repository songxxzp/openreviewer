[
    {
        "id": "8EMbeQ5nI1",
        "forum": "BgZzJISvpY",
        "replyto": "BgZzJISvpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_eazc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_eazc"
        ],
        "content": {
            "summary": {
                "value": "The paper incorporates some results in Extreme Mean Theory into distributional RL, especially in the continuous control setting. As the large atoms used in distributional RL may lead to unstable optimization, the authors proposed to discard them. It seems that the remaining part follows an extreme minimum distribution, allowing to directly output the scale and location parameter for the asymptotical Gumbel distribution. A heuristic algorithm is designed in the actor-critic framework by simply discarding the max atoms and shows a similar performance compared with the existing baselines."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper is easy to understand as the method is straightforward."
            },
            "weaknesses": {
                "value": "* **The novelty and technical contribution are trivial and very limited**. Since TQC has already considered the truncation technique to stabilize the RL training process, it is very incremental to use an already known Extreme value theorem to consider this problem again without any technical contribution. Besides, the authors claimed the advantage of the proposed algorithm over TQC is less computational cost, which is not important as both the two algorithms already largely increase the computation burden compared with other distributional RL algorithms. For example, EMD uses critic networks to output scale and location parameters, leading to unstable concerns compared with other distributional RL baselines with only one critic. More importantly, the performance of EMD is very similar to TQC. \n\n* **The methodology is questionable and does not have justifications**. The authors are trying to borrow some existing statistical techniques or conclusions into RL problems without investigating whether those results are valid under real problems. Generally speaking, some existing results in statistics typically rely on distribution assumptions, which are normally not applicable to RL problems without rigorous justifications. In particular, Extreme value theory includes a few types and the paper here only considers the iid case with exponential family. Note that in online RL training, it is not clear whether the resulting distribution is exponential or not and I am afraid no in general based on my knowledge. Therefore, the proposed method relies on a very strong distribution assumption, which is typically not valid in complex environments. Also, the Gumbel distribution is an asymptotical result, and in practice, we are more likely to care about the non-asymptotical scenario. I thus doubt the rationale behind the proposed algorithm let alone the fact that the empirical improvement is very limited.\n\n* **Necessary theoretical parts are missing.** To begin with, I double that the theorems presented in this paper are very likely directly based on the existing results in the extreme value theorem. Without providing the reference, the paper would suffer from the academic integrity issue. Apart from that, it is not clear whether the distributional Bellman operator under the extreme value truncation is convergence or not, which should be rigorously discussed. In addition, Theorem 2 is provided in a very non-mathematical way, which is less rigorous and not convincing to me. \n\n* **Missing literature.** Truncating the extreme value is also highly linked with risk-sensitive RL, but the relevant literature is missing. It is not clear whether truncating the extreme value is helpful or not as it is a trade-off between exploration and stability. However, the paper fails to justify this trade-off rigorously. \t\n\n* **Experiments.** Experiments are very restricted in continuous control cases, and improvement is not significant in Figure 2. Since the overestimation issue is more commonly used in value-based RL, experiments on Atari games are necessary and the current experimental results are weak. \n\n* **Writing**. The writing should be substantially improved and the choice of some words is causal. Some paragraphs like continuous control algorithms in the literature are well-known. The most of parts in Section 4.2 are trivial and similar to existing works."
            },
            "questions": {
                "value": "Please refer to the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2462/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2462/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2462/Reviewer_eazc"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2462/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698561647173,
        "cdate": 1698561647173,
        "tmdate": 1699636182458,
        "mdate": 1699636182458,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4mXbtVWVjJ",
        "forum": "BgZzJISvpY",
        "replyto": "BgZzJISvpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_oowZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_oowZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a solution to tackle the tail behavior of the return distribution. Specifically, the paper truncates the tail atoms and derives the truncated distribution in the asymptotic setting using the Extreme Value Theorem. The critic network can be devised accordingly and the paper provides empirical verification of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "N/A"
            },
            "weaknesses": {
                "value": "### The theoretical part of the work is not rigorous enough to be ready for a conference paper.\n\n1. Theorem 2 is not rigorous. What is an \"iterative process\"? What is the mathematical definition for \"removing extreme maximal atoms\"?  The Proof of Theorem 2 in the appendix is also far-fetched and hard to understand. Why does $\\alpha^n \\to 0+$ imply \"we can maintain $F_n(x)$ represents the smallest portion of the values\"?\n\n2. Theorem 3: what is \"exponential head\"?\n\n### The significance of the theoretical part is not enough. \n\n- Theorems 3~5 are just simple manipulations of the distribution. \n\n### The empirical evidence is not strong enough.\n\n- In Figure 2, TQC mostly performs better or equal to the performance of the proposed EMD. \n\n### The clarity of the paper needs to be improved."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2462/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817473337,
        "cdate": 1698817473337,
        "tmdate": 1699636182384,
        "mdate": 1699636182384,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6OeOsraRcD",
        "forum": "BgZzJISvpY",
        "replyto": "BgZzJISvpY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_7cHh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2462/Reviewer_7cHh"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a new return distribution estimation method for distribution reinforcement learning.\nThe authors state that the tail of return distribution can worsen learning return distribution.\nBy using the property of Gumbel distribution, this paper conjectures that the topmost atoms of the approximated reward distribution are anomalies with large estimation bias by the limit of function approximator, such as quantile-regression networks.\nTo overcome this estimation bias, the proposed method re-estimate the inverse CDF by using extreme minimum distribution.\nIn experiment, the authors modify value-based distributional RL algorithms into a soft actor-critic variant to evaluate in the contiuous control domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The main method is  a novel correction method for existing quantile regression DQN.\nThe existing correction method for return distribution is a simple truncation method, but the propsed method can provide a theory-based distribution correction method, which has been already used in RL research (extreme Q-learning)."
            },
            "weaknesses": {
                "value": "My main concern is the lack of the supporting materials for the main claim is not enough.\nAlthough the propsed method might fit for distribution RL, the main theorem is not an analysis for distributional Bellman equation but a proposition of vanilla probability distribution.\nIn addition, the proposed algorithm cannot outpeform existing distributional RL baselines, such as TQC which has the similar concept and more simple computation structure."
            },
            "questions": {
                "value": "Please refer weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2462/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699063156571,
        "cdate": 1699063156571,
        "tmdate": 1699636182314,
        "mdate": 1699636182314,
        "license": "CC BY 4.0",
        "version": 2
    }
]