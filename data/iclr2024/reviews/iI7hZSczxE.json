[
    {
        "id": "6yu8w5ZcGV",
        "forum": "iI7hZSczxE",
        "replyto": "iI7hZSczxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_cDdv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_cDdv"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the importance of learning disentangled representations for Time Series data, specifically in the context of home appliance electricity usage. The goal is to enable users to better understand and optimize their energy consumption, thereby reducing their carbon footprint. The authors frame the problem as one of disentangling the role of each appliance (e.g., dishwashers, fridges) in total electricity usage.\n\nUnlike existing methods that assume attributes (appliances in this case) operate independently, this work acknowledges that real-world time series data often show correlations between attributes. For instance, dishwashers and washing machines might be more likely to operate simultaneously during the winter season.\n\nTo address these challenges, the authors propose a method called DisCo (Disentangling via Contrastive), which employs weakly supervised contrastive disentanglement. This approach allows the model to generalize its representations across various correlated scenarios and even to new households. The method incorporates novel VAE layers equipped with self-attention mechanisms to effectively tackle temporal dependencies in the data.\n\nTo evaluate the quality of disentanglement, the authors introduce a new metric called TDS (Time Disentangling Score). The TDS proves to be a reliable measure for gauging the effectiveness of time series representation disentanglement, thereby making it a valuable tool for evaluation in this domain.\n\nOverall, the paper argues that disentangled representations, particularly those achieved using their DisCo method, can enhance the performance in tasks like reconstructing individual appliance electricity consumption."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The method is very sound with mathmaticaly correct derivations. \nThe addressed problem of disentagling latent factors in VAE type of models is very important.\nSpecifically, he paper addresses the unrealistic assumption of independence among generative attributes that is often present in traditional untangling methods. In contrast to these traditional approaches, DisCo focuses on recovering correlated data by encoding a wide range of possible combinations of generative attributes in the learned latent space.\n\nThe authors assert that simply encouraging pairwise factorized support in the latent space is sufficient for achieving effective disentanglement, even when data attributes are correlated. This is an important finding. \n\nIn terms of performance, DisCo is shown to be competitive with downstream task methods, exhibiting significant improvements of over +60% across a variety of benchmarks in three different datasets undergoing correlation shifts (Finding 5.1). This is a strong aspect of the work.\n\nAdditionally, the capability of DisCo to adapt across correlation shifts leads to better out-of-distribution generalization, especially when these shifts are more severe. This fulfills one of the key promises of learning disentangled representations, which is to improve the model's robustness and generalizability."
            },
            "weaknesses": {
                "value": "I enjoyed the paper and did not find important weaknesses."
            },
            "questions": {
                "value": "Please discuss how sensitive the method is to hyperparameter selection."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697576616693,
        "cdate": 1697576616693,
        "tmdate": 1699636980774,
        "mdate": 1699636980774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jnzJZdPgw3",
        "forum": "iI7hZSczxE",
        "replyto": "iI7hZSczxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_jyEy"
        ],
        "content": {
            "summary": {
                "value": "This study explores disentangled representations for time series data, with a primary emphasis on achieving representation generalization across diverse, interrelated scenarios. They focused on a specific application of electric load monitoring application where computing different household appliances contribution in a total load is the task. \nIn the context of Variational Autoencoders (VAE), this study draws inspiration from Roth et al., 2023, who addressed correlated attributes in an image processing context by replacing the independence constraint over attributes in the latent space (by a regularization term of the Kullback-Leibler divergence between the posterior of the latent attributes and a standard Guassian distribution), with the Hausdorff Factorized Support (HFS) assumption. The authors have adapted this idea for time series data and introduced the use of cosine similarity instead. Consequently, this approach no longer necessitates independent latent activations for different appliances. \nThe main idea is to address appliance correlations with weakly supervised contrastive disentanglement, promoting similarity for the same appliances and dissimilarity for absent appliances in latent representations. This is achieved through a loss function composed of two terms, one for alignment based on correlation and another to minimize redundancy between latent variables.\nIn addition, the authors proposed l-variational inference layers with self-attention mechanism to address temporal dependencies. Additionally they propose a metric of  Time Disentangling Score (TDS) to evaluate the  disentanglement performance in time series data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents several intriguing novelties.\n1) Using pairwise similarity rather than independence assumption in VAE, to consider the correlated representations. \n2) l-variational inference layers with self-attention mechanism\n3) A metric of  Time Disentangling Score (TDS) to evaluate the  disentanglement performance in time series data\n\n The authors have tackled a captivating problem, successfully adapting image processing techniques to the more complex domain of time series data."
            },
            "weaknesses": {
                "value": "The paper needs some modifications to make it easier to read (some suggestions given in the Questions).\nThe experimental results are very abstract (some suggestions in Questions part)\nThe application worth more explanation, the description lacks either an illustration or it is abstract."
            },
            "questions": {
                "value": "-Using cosine similarity instead of HFS needs more elaboration.\n\n-Section 3.2 would benefit from a dedicated illustration demonstrating ATTENTIVE l-VARIATIONAL AUTO-ENCODERS, along with the corresponding notations used in the text.\n\n-The authors have effectively presented the formulation for the usecase; however, in the experimental results, which I find somewhat abstract, there's a lack of a specific example illustrating how X and Y values for a time window are displayed, along with different rows of Y, etc.\n\n-In section 4.1, should be included how exactly augmentation is performed and how many, it is very abstract now. \n\n-In Section 2, specifically concerning contrastive learning, the evaluation of appliance dissimilarity in \"x\" and \"x-\" is not explicitly clarified. Is labeling used for this purpose? What if the appliances are not the exact same but should exhibit similar behavior? How are such cases addressed? Additionally, the preparation of negative and positive samples is not detailed. Have you considered ensuring that there are no common or similar appliances in these two sets, and if so, how was this determined? Providing further explanation or an illustration could enhance the clarity of data preparation, which is a crucial aspect of the methodology.\n-How many training examples did you use for linking?\n\u201cWe link the learned latent representation to ground-truth attributes using a limited number of pair labels\u201d\n-After equation 2, In this test, the latent variable is represented as \"z,\" which is defined as a matrix of dimensions (M + K) \u00d7 dz, where \"K\" and \"dz\" should be introduced and define."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698396421370,
        "cdate": 1698396421370,
        "tmdate": 1699636980661,
        "mdate": 1699636980661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HPGIuqa3tS",
        "forum": "iI7hZSczxE",
        "replyto": "iI7hZSczxE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_a2rd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7969/Reviewer_a2rd"
        ],
        "content": {
            "summary": {
                "value": "The paper seems to be an application of disentangled representation learning for home appliance electricity usage. The authors propose to combine contrastive and variational losses. Unfortunately, the paper falls somewhere between methodological novelty and application, making it difficult to understand where the main contributions of the paper will lie. In general, I found the paper very hard to read."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The main strength of the paper is its approach to tackling the important problem of disentangled representation learning, which may contribute to reducing carbon footprint."
            },
            "weaknesses": {
                "value": "The paper lacks proper organization and has a tendency to include some unproven (or wrong) claims. For instance, in the introduction, the authors mention that \"Disentanglement via VAE **can be achieved** by a regularization term of the Kullback-Leibler divergence[],\" which is not necessarily true without certain strong assumptions and underlying conditions. The Beta-VAE paper has some qualitative evidence showing how the images are more disentangled compared to VAE. The authors also claim, \"Rather than training separate auto-encoders for individual appliances,\" which requires empirical validation with proper citations.\n\nIn addition to these issues, the notations used are very confusing and are not defined before they are referenced. For example, in the proposed method section, the notation $z_m^+$ is used without description it. It is also unclear how it differs from $\\bf{z}$ or $z$.\n\nThe main goal of this paper remains unclear to me. For example, the authors mentioned, \"The primary goals of this work are twofold: to effectively address the NILM problem and to obtain a disentangled representation of input data.\" However, it is unclear what the NILM problem is, what the nature of the input data is, and how the authors plan to achieve a disentangled representation that distinguishes itself from previous works. One issue might be that the problem statement and preliminaries are somewhat intertwined.\n\nThe color meaning used in the tables of result section is not clear. Even it is not clear how TDS (as a metric) has been compared with VAE and Beta-VAE in Table 1."
            },
            "questions": {
                "value": "- I strongly suggest the authors make their main contributions clear at the end of the introduction. \n\n- There is no related work section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7969/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819937506,
        "cdate": 1698819937506,
        "tmdate": 1699636980532,
        "mdate": 1699636980532,
        "license": "CC BY 4.0",
        "version": 2
    }
]