[
    {
        "id": "A4MXvfqxv1",
        "forum": "xXtD9P2lvH",
        "replyto": "xXtD9P2lvH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_E6dr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_E6dr"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a generative model, similar to denoising autoencoders, for directed graph generation. The encoder adds noise based on a heat equation expression to generate a perturbed representation, which the decoder denoises to reconstructs the desired generated graph via the random walk Laplacian. The authors test their approach in empirical evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Originality and Novelty: The approach that the authors propose is, to the best of my knowledge, original and novel. \n- Significance: Nowadays it is certainly an interesting and important topics to study graphs, as well as generative models on graphs. The topic of directed graph generation has indeed been underappreciated in the literature, so this is a welcome addition. \n- Quality: The technical claims are, to the best of my knowledge, sound and reasonable. Details and questions are given below in the next section. \n- Clarity: The article is written moderately clearly, with ample room for improvement in its exposition. Suggestions are given in the section below."
            },
            "weaknesses": {
                "value": "- The main weakness is in the presentation and empirical evaluation: \n\n1. I suggest that the authors provide more background and motivation on the mathematical prerequisites to make the paper more self contained. \n\n2. The decision to set M to be a constant matrix should be further motivated and explained (to people familiar with this, this is a natural choice, but this can be unclear to the less initiated readers)\n\n3. Crucial concepts rely on very recent work such as the Set Transformer in 2019 and the work of Veerman and Lyons in 2020. This makes the article more difficult to read...I suggest that the authors try their best to make this work more self contained in the presentation. \n\n4. The empirical evaluation is limited to very simple models (ER and SBM) under the squared MMD distance for various descriptors, under hyperparameter settings of 3 blocks and p as 0.6, seemingly without much justification."
            },
            "questions": {
                "value": "1. Is there a concrete reason/justification for why the empirical evaluation is so focused on disconnected digraphs? My impression is that many interesting applications concerns connected/strongly connected digraphs. Is there a possibility where the evaluation metrics (clustering coefficients etc) are just capturing the disjointness of the generated graph, rather than the more fine-grained properties of connectivity within a single connected/strongly connected digraph?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698218394141,
        "cdate": 1698218394141,
        "tmdate": 1699636693727,
        "mdate": 1699636693727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IiVV6UqiS3",
        "forum": "xXtD9P2lvH",
        "replyto": "xXtD9P2lvH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
        ],
        "content": {
            "summary": {
                "value": "The focus of the manuscript is on directed graphs (digraphs) generative process. The authors propose an encoder-decoder architecture. Their encoder is based on the heat diffusion (defined by the graph Laplacian), and does not require any training. The representation is then perturbed such that it corresponds to a nonhomogeneous process. The denoiser is then trained to reverse the diffusion, i.e. to match the initial condition of the process. They then provide experiments to validate their claims."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is well motivated from a theoretical point of view. \n- Focusing on digraphs is interesting as most methods are on undirected graphs."
            },
            "weaknesses": {
                "value": "- The overall writing and organization could be improved. In particular, section 3, which lack continuity. \n- I found the experiments a bit limited, I think a few things are missing:\n    - The authors should report the standard deviation in the table. \n    - I think it is important to report other distances than the current MMD with RBF kernel, either with different kernels and / or with different variance parameters. \n    - In the tables, it is hard to understand the magnitude of the scores. It would be great to add a row for a random method (e.g. random adjacency matrix used in line 2 of Alg.1). \n    - The results in 5.3 could be in the main body of the text by shortening other sections (e.g. related work)."
            },
            "questions": {
                "value": "- How do you choose the initial node representation $X(0)$ ?\n- In eq.1, you also need to specify the initial condition at $t=0$. You could also explain $X(0)$ has $d$ signals that you diffuse following the heat equation (it might give more intuition).  \n- >Finally, some denoising decoder is trained to predict the nodes and/or edges when given only X(T ) as input (see details in Section 3.1).\n    \n    I don't fully understand this sentence. Looking at $\\mathcal{L}_{node}$ it is trained to reverse the diffusion process. It is not trained to predict the node, but rather the initial $d$ signals. \n    \n- Is the decoder conditioned on the noise level (e.g. like in score matching) ?\n- Possible typo: \"our decoders are learned \" $\\to$ \"our decoders are trained \""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Reviewer_APd2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685238179,
        "cdate": 1698685238179,
        "tmdate": 1699636693584,
        "mdate": 1699636693584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PmRCbthgHK",
        "forum": "xXtD9P2lvH",
        "replyto": "xXtD9P2lvH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a generative approach for directed graphs. It loosely follows the idea of denoising autoencoders: a trained input function in R^d over nodes is corrupted through a heat diffusion process as to produce an almost constant function over the graph nodes. This function is then given as input to an encoder that is tasked to project the node representation into a latent space and a decoder that, given two node embeddings, predicts the presence of an edge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper deals with an interesting problem, the one of directed graph generation, which seems to be partially neglected by the main body of literature in graph generation but that is nevertheless relevant.\nTo the best of my knowledge, the proposed methodology is original. It adapts some ideas from denoising autoecoders and the more recent denoising diffusion to the domain of directed graphs, building a principled and sound method."
            },
            "weaknesses": {
                "value": "One of the drawbacks of the paper is that the mathematical notation is a bit intricate. Many quantities are redefined during the method description and is difficult to keep track of all of them. Some equations are also defined but I missed if or where they were used, for eq 6 or the node loss (where is the model \\phi used in the sampling process?).\nPersonally, until section 3, I was imagining some sort of denoising diffusion technique (especially after eq 5 and 6). It took me a while to change my \u201cexpectations\u201d about the following sections.\n\nThe other weakness is about the comparisons. Even if there aren\u2019t many works dealing with directed graphs, it is still worth providing a solid testbench that could possibly be used also from future works that want to compare with the proposed method. Isn\u2019t there more datasets that can be considered or comparative methods that could be adapted"
            },
            "questions": {
                "value": "- The diffusion time is set to 1 in the experiments, but is it a dataset-dependent parameter? I guess that it might be somehow dependent on the graph radius?\n\n- It took me a while to figure out what a kind of node function N had to be. Maybe making it clear from the beginning that it is learned could help the reading. \n\n- Still regarding N, permutation invariance is not an easy thing to learn. How much could it be a problem for the training convergence?\n\n- Your method consists of diffusing an initial random graph. How much important is the starting graph family? Since your noise converges to a constant function, would it be possible to sample directly M?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6309/Reviewer_hJPx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699131360820,
        "cdate": 1699131360820,
        "tmdate": 1699649115592,
        "mdate": 1699649115592,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zU76HZEuEX",
        "forum": "xXtD9P2lvH",
        "replyto": "xXtD9P2lvH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_t7Sc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6309/Reviewer_t7Sc"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an novel approach to crate a one-shot generative model for directed graphs.\nThe approach represents the graph through their heat diffusion with a forcing term Q(t) that forces the limit distribution to be uniform over the nodes. \nTo generate the graph, they train an edge decoder that, given a noisy representation of the heat diffusion, predicts the presence of the edge.\nWith the decoder to hand, they generate a Erdos-Renyi random graph, and add some Bernoulli noise to the adjacency matrix, then obtain the directed Laplacian for the result and compute the heat kernel under the mentioned forcing term and feed it to the edge decoder."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novel approach to cast the one-shot generation of graph\n- can handle directed as well as undirected graphs"
            },
            "weaknesses": {
                "value": "- The experimental evaluation is a bit substandard given the relatively large recent literature on the topic. The authors should at least match SPECTRE (cited) for the evaluation protocol."
            },
            "questions": {
                "value": "While it is clear that the link predictor tries to match what it has seen in the training set, it is not clear how their approach changes the Erdos-Renyi distribution to one more similar to the training set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6309/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699203031918,
        "cdate": 1699203031918,
        "tmdate": 1699636693361,
        "mdate": 1699636693361,
        "license": "CC BY 4.0",
        "version": 2
    }
]