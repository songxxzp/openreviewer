[
    {
        "id": "9hg6AggS9L",
        "forum": "Of2RhzJ8UJ",
        "replyto": "Of2RhzJ8UJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
        ],
        "content": {
            "summary": {
                "value": "The authors present a formal approach to \"explanation certification\" in machine learning. Given a model with query access, a sample point, and a set quality metric, they aim to identify the largest hypercube centered at the sample point. Within this hypercube, any applied explanation should meet the set quality criterion. To achieve this, they introduce \"Ecertify,\" a method that samples different points around the sample point within a hypercube's width and evaluates them based on the quality metric. The size of the hypercube are then iteratively adjusted until the maximum size is identified. The authors introduce several sampling techniques - 'unif', 'unifI', and 'adaptI', and provide probabilistic guarantees on their effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel problem of explanation certification and highlight the benefits, i.e., insight into model behavior in a specific region, stability of explanation, reusing explanations, etc.\n\nThey provide rigorous experimental  analysis to support their findings."
            },
            "weaknesses": {
                "value": "While the problem statement is compelling, the methodology employed seems somewhat heuristic. Given a metric \\(f(\\cdot)\\), the goal is to identify a hypercube wherein all points satisfy the requirement \\(f(\\cdot) > \\theta\\). The approach involves sampling points within the hypercube, querying the function, and subsequently adjusting the hypercube's width size. The probabilistic guarantees suggest that as the number of queries approaches infinity, the error diminishes with high probability. This is reminiscent of the Maximum Likelihood Estimation (MLE) of a uniform distribution \\([0,b]\\) using samples \\( \\max_i x_i \\). Here, \\(\\hat{b} < b\\), and as \\(i\\) tends towards infinity, one approaches \\(b\\) at an exponential rate. What differentiates one sampling method from another if all have similar convergence rate?\n\nThe paper seems to overlook the role of the quality function \\(f\\) in its analysis. The provided probabilistic guarantees necessitate knowledge of \\(f's\\) cumulative distribution function (CDF), which is unknown. Nonetheless, the paper's discussion of special cases, such as characterizing CDFs for linear models and enhancing certification for Lipschitz models, is intriguing. However, relegating this information to the appendix is a missed opportunity. I recommend that the authors emphasize these aspects more prominently in the main body of the paper.\n\nIdentifying a hypercube in a d-dimensional space may lack practical relevance, especially since features in real-world datasets aren't always uniformly scaled.\n\nAdditionally, there's existing literature on robust counterfactual explanations amidst noisy implementations [1] and other related works that focus on identifying a region where a model's prediction remains consistent. These studies also employ a similar strategy of sampling around a point and querying the model. While their formulations might differ from yours, it's essential to acknowledge and cite these works\n\nThe presentation of the methodology in the paper, especially regarding 'unifI' and 'adaptI', could benefit from further clarity. Incorporating a visual representation or figure summarizing the methods might be beneficial. The inclusion of the video in the supplementary material is commendable.\n\n[1] https://arxiv.org/pdf/2203.06768.pdf"
            },
            "questions": {
                "value": "Address weaknesses section.\nGenerally, I find your problem interesting, however I find your methods to be heuristic (sample and query; probabilistic guarantees are directly from a law of large numbers style argument). I would expect for a method a leverage the quality function $f$ for a better solution. I might be missing something and would like the authors to clarify the non-triviality of their method. I am open to revising my score ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_3Uus"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551830541,
        "cdate": 1698551830541,
        "tmdate": 1699636073614,
        "mdate": 1699636073614,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o2R1MgXjmd",
        "forum": "Of2RhzJ8UJ",
        "replyto": "Of2RhzJ8UJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_pTb7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a black box explanation certification method to provide insight into model behavior and ensure explanation stability. Empirical evaluations on synthetic and real data demonstrate the effectiveness of the proposed work. This work seems to have solid theoretical analyses and deals with an interesting topic. However, the work is not well-written. In particular, some intuition and concepts are not clearly provided, making the work difficult to follow."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The work deals with an interesting topic, i.e., finding a hypercube of samples to make the explanations stable for the samples within the hypercube.\n2. The authors provide rich theoretical analyses to support their claim."
            },
            "weaknesses": {
                "value": "1. The design of the methodology is quite trivial. The authors only provide the pseudo-code of the method without clear explanations and descriptions at a high level, which makes the reviewer unable to get the intuition behind the idea.\n2. Although the research topic is interesting, the motivation is not strong and it is unclear how important stability is for the XAI. Is there a practical scenario to illustrate the significance of the research topic?\n3. It is unclear the pros and cons of the three sampling strategies. The authors did not provide a deeper discussion of the three sampling strategies.\n4. The experiments are confusing. What is the purpose of ZO and why is it used as a baseline? Why does the convergence of w to a similar value indicate the accuracy of the explanation?"
            },
            "questions": {
                "value": "1. What is the intuition of method design? At the least, the author should provide one figure to illustrate why the method is designed in this way and how the method works.\n2. Could the authors provide a real-world application to demonstrate the significance of the research?\n3. What are the pros and cons of the three sampling strategies? What kind of scenario are they suitable for and what insight do they give practitioners?\n4. The authors claimed the method is model agnostic and quite general. So can the method generalize to graph data?\n5. To what extent is the found w stable? i.e., the bound of P (f (x);w) >=theta."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754700026,
        "cdate": 1698754700026,
        "tmdate": 1699636073540,
        "mdate": 1699636073540,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ItMb4N6wVd",
        "forum": "Of2RhzJ8UJ",
        "replyto": "Of2RhzJ8UJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_KmSN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel certificate of explanability in a hyper-ball centered at a given sample that is model-agnostic. The authors provide rigorous formulation, solutions with theoretical guarantees and analysis, and experiments on many datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe novel formulation, adopting from certificate for adversarial robustness, is very interesting and could potentially be applied in practice\n2.\tThe analysis and explanation of the proposed methodologies, described in Algorithm 1 is very clear and comprehensive. \n3.\tThe theoretical analysis of the performance guarantees of Algorithm 1 and 2 is novel and useful for different strategies to explore the regions\n4.\tNumerical results are abundant, including large-scale images datasets and synthetic datasets."
            },
            "weaknesses": {
                "value": "1.\tDespite that the authors mentioned in Section 8 Discussion that the l_\\infinty ball can potentially be generalized to hyper-rectangles or l_p balls, all these hyper-balls may not carry semantic meanings of the samples. Therefore even if we can certificate that the pseudo-samples within the hyper-balls have high fidelity, those pseudo-samples may not carry meaningful information, since human interpretation on images is very different from Euclidean distances. It is encouraged that the authors address this concern with a discussion. \n2.\tThe connection between the Algorithm 1 and Algorithm 2 proposed in Section 4 seems to be detached from the original optimization formulated in equation 1. It is encouraged that the authors strengthen the connection.\n3.\tThe authors mention several advantages of the novel explainability certificate, including stability of the explanations and explanation reuse, but did not provide numerical results or analysis on these benefits over existing explainable AI tools. It is encouraged that the authors \n4.\tThis certificate is closely related to adversarial examples, and in related literature there are also studies on the maximal hyper-balls that will not suffer from a decision change. Their algorithm is also similar to the searching algorithm proposed in Algorithm 1. The novelty here is the quality metric and the explanation function. It is encouraged that the authors include more discussion on different explanation functions, or give concrete examples in Section 2. \n5.\tIn XAI, the explainability of features is sometimes more important than individual samples. Is it possible to extend the techniques to features?\n6.\tOne of the most important aspects in XAI tools is visualization. It is encouraged that the authors add visualization of the regions, provide some intuitive explanations, with simple (semi-)synthetic data."
            },
            "questions": {
                "value": "Please refer to the Weaknesses. I will consider raising the scores if the authors could adequately address my questions in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802939701,
        "cdate": 1698802939701,
        "tmdate": 1699636073427,
        "mdate": 1699636073427,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CzCxSj1TDr",
        "forum": "Of2RhzJ8UJ",
        "replyto": "Of2RhzJ8UJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_46DG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_46DG"
        ],
        "content": {
            "summary": {
                "value": "In the explainability problem, we are given a classifier model and an input, and we seek to find an explanation for the classification. This paper considers the problem of finding a certificate for explanations with respect to some input quality metric. The certificate(also called a trust region) is the largest hypercube centered at the example such that at least 1-\\delta of the points in the hypercube meet the quality criteria. The trust region has multiple applications, including explanation reuse and analysis of model behavior. \n\nThe paper first introduces the problem formally and then presents three techniques for certification, the simplest of which involves uniform sampling to determine the density of points that violate the quality metric. The other techniques refine the random sampling strategy for faster convergence.  The algorithms are implemented, and the experiments highlight that the certification technique is faster than comparable methods while providing roughly similar trust regions. An interesting insight from the experiments is that LIME trust regions can be larger than SHAP if the desired fidelity is on the lower side."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality and Significance: The idea of certification of explanations and its potential application to explanation reuse seems to be novel and quite interesting. The experimental evaluation is relevant, and the insights are valuable."
            },
            "weaknesses": {
                "value": "I felt only the first of the three techniques is well motivated, while the other two don't really contribute to the main idea of the paper. Since the main contribution is a formalization of the novel problem, it would be nice to have a simple example to illustrate what a certification looks like.\n\nThere are also a few issues with the readability:\n1) What is fidelity? Perhaps the definition is not strictly necessary for understanding the paper, but since it is used so many times, I think it is critical to define it.\n2) the pseudo-code and the explanation in section 4 are not well written. I may be wrong, but the pseudo-code seems to be executing a binary search, in which case it is needlessly obscured."
            },
            "questions": {
                "value": "For w1>w2>w3, it seems to be possible that hypercubes of halfwidth w1 and w3 could be valid trust regions while w2 isn't. Since the problem statement involves finding the largest certificate, would the certification really be meaningful around the example? For instance, if the explanation passes the quality threshold over most of the domain but is bad around important points, it seems that the certificate could just be the entire domain, revealing little about the point of interest. Is my understanding correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810441924,
        "cdate": 1698810441924,
        "tmdate": 1699636073341,
        "mdate": 1699636073341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FQ49yNvOtK",
        "forum": "Of2RhzJ8UJ",
        "replyto": "Of2RhzJ8UJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88"
        ],
        "content": {
            "summary": {
                "value": "This work addresses an interesting question in the area of robust explanations: How do we find a region around a point such that an explanation remains more or less unchanged? Additionally, they enforce a constraint that one only has black-box access to the function f(x) which computes the fidelity of the local explanation of x0 at x and the evaluation of the model g(x). \n\nThe paper formalizes the problem statement. Then, they provide a strategy for addressing this problem that adaptively expands a bounding box around the point using samples in that region. They provide theoretical guarantees on the performance of their proposed strategy using probabilistic methods. They also provide experimental results on several datasets to demonstrate the efficacy of their approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem is an interesting one from a mathematical standpoint. \nAn algorithm is proposed to find robust regions along with theoretical guarantees. \nIntroducing an adaptive strategy is a good idea.\nExperiments have been performed on a variety of datasets which include image and tabular datasets, as well as two popular explanation techniques namely LIME and SHAP.\n\nIt is interesting to connect it back to Lipschitz functions and also piecewise linear functions in the Appendix."
            },
            "weaknesses": {
                "value": "There is insufficient motivation for this problem. For instance, why would one care for the explanations to remain the same in a region? For instance, if the model itself is undulating in a region, why should local explanations remain stable? The width of the robust region found would accordingly be large or small. How would this be informative? The motivation could be strengthened. Perhaps, a toy example could be useful.\n\nThe term black-box access is unclear since here they are not only accessing the model output but also its local explanation function. I would think black-box access typically means just computing the model output g(x), but additional functions are being computed here. So, calling it black-box access is a bit misleading.\n\nTo certify a single point itself, one requires many queries. Then, to certify multiple points, the query would significantly increase. Could the authors comment further on this computational complexity?\n\nAlso, in the experiments, it seems that this certification is being done for just one point (?) How do the results vary when trying to certify different points? Could you discuss more on the average over the entire dataset?\n\nThe experiments also do not show a uniform trend. What does that mean in this context? The experimental section needs some clarity on what should one expect to see.\n\nThe presentation of the algorithm can be significantly improved with much more clarity. The paper is quite difficult to read. Not much intuition is provided.\n\nAlso, closely related works in the area of counterfactual explanations (and some of the references therein) also talk about robust regions for counterfactual explanations.\n[1] Finding Regions of Counterfactual Explanations via Robust Optimization: https://arxiv.org/pdf/2301.11113.pdf\n[2] Robust Counterfactual Explanations for Neural Networks With Probabilistic Guarantees: https://arxiv.org/pdf/2305.11997.pdf\n[3] Consistent counterfactuals for deep models: https://arxiv.org/abs/2110.03109"
            },
            "questions": {
                "value": "Questions are provided along with the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1448/Reviewer_ot88",
                    "ICLR.cc/2024/Conference/Submission1448/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811744321,
        "cdate": 1698811744321,
        "tmdate": 1700690339904,
        "mdate": 1700690339904,
        "license": "CC BY 4.0",
        "version": 2
    }
]