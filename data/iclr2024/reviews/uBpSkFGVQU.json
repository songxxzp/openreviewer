[
    {
        "id": "cxTLNEPyCU",
        "forum": "uBpSkFGVQU",
        "replyto": "uBpSkFGVQU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_AVEH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new self-supervised representation learning approach (SSL) incorporating novel view synthesis data augmentation and estimated depth maps as input. DPT and AdaMPI are used for depth and novel view synthesis estimation. By incorporating depths and novel views during training the authors found their method more accurate when learning from few data and more robust to noisy test inputs. The authors claim this is the first method to use estimated depth as inputs for self-supervised representation learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Biologically inspired and a corresponding well-written introduction.\n\n2. Good correlation with biological elements in the visual system.\n\n3. Interesting reasoning on why novel views should improve SSL: As the mutual information between depth and input images is high, the effect of depth is negligible on an infinite dataset. However, novel 3D views introduce new information.\n\n4. Simple and clear method, seems reproducible.\n\n5. Clear ablation studies."
            },
            "weaknesses": {
                "value": "1. Figure 1 lacks details. For instance, it is unclear what \"2D  augmentation\" is being performed in PixDepth. It would also be good to visually represent your SSL objective.\n\n3. Depth is dropped from the input to encourage the network to not over-rely on it. However, depth is used to compute the error metrics in the results table. In this case, the comparison could be considered unfair, as the previous methods do not have depth as input. As depth is obtained from a supervised network, this method is not a pure SSL method.\n\n3. Most improvements come from adding the depth channel, which I could not consider an important contribution.\n\n4. What is the reason behind the statement of improvements of SwAV in Table 1 when no results for SwAV are provided?\n\n5. For the results on imagenet-100 no improvement is achieved, it is actually the opposite. This is not clearly reflected in the text in a tricky way. Why are there no results with the combination method (depth + 3D)?\n\n6. I am afraid that the added robustness to corruptions in ImageNet-C and ImageNet-3DCC comes from the robustness of the depth estimation network (trained with depth GTs over a considerable amount of data).\n\n2. Some minor typos: \"a approach\", \"an conceptually\","
            },
            "questions": {
                "value": "1. Don't you think this is too casual language \"we take seriously two insights\" ?\n\n2. Clear metrics comparing against SOTA would be helpful in the introduction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697779590482,
        "cdate": 1697779590482,
        "tmdate": 1699636721589,
        "mdate": 1699636721589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hzZR6ZYPND",
        "forum": "uBpSkFGVQU",
        "replyto": "uBpSkFGVQU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to incorporate depth signals into the self-supervised learning (SSL) framework. Specifically, two baselines are provided: the first baseline directly concatenates RGB and depth signals as the input of SSL, and the second baseline augments novel view generated according to the depth signal for SSL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work investigate the influence of including depth signals into the SSL framework.\n2. The experiments show that with the introduction of depth signals, the existing SOTA SSL methods yield a better performance."
            },
            "weaknesses": {
                "value": "1. Using depth signals as augmentation is not new in SSL. Previous works, e.g., DepthContrast, have explored it thouroughly.\n2. The proposed method lacks generalizbility. Though it can be adopted to any SSL frameworks, the adopted depth estimation model is supervised trained on several datasets. The performance of depth estimation can not gurantee in scenarios that have a huge domain gap compared to the trained datasets. \n3. Also, due to the utilization of the supervised depth-estimation model, it is questionable to claim the proposed method as a SSL framework.\n4. Experiments are all conducted on the subset of Imagenet or the modification of Imagenet. Results on more datasets are expected."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6455/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6455/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6455/Reviewer_BurA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698190879299,
        "cdate": 1698190879299,
        "tmdate": 1699636721468,
        "mdate": 1699636721468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CprTPZRX88",
        "forum": "uBpSkFGVQU",
        "replyto": "uBpSkFGVQU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_YemW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6455/Reviewer_YemW"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a new representation learning method that utilizes an estimated depth map to learn a geometry-aware representation.\nAs mentioned in the abstract, the goal of SSL is to learn useful representation for \"downstream tasks.\"\nHowever, the scale of the conducted experiments is insufficient to claim the effectiveness of the proposed method. \n1) The proposed method is evaluated only in small-scale datasets (e.g., ImageNet-100, ImageNet-1k).\n2) The proposed method is evaluated only in small-scale models (e.g., ResNet-18, 50). \n3) The proposed method is evaluated only in a classification task.\n\nThe proposed method must need to show its effectiveness and scalability in large-scale datasets, various backbone models (e.g., CNN, Transformer variant models), and diverse downstream tasks (e.g., 2D/3D detection, 2D/3D segmentation, 3D reconstruction, 3D view generation, etc)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides a new representation learning method that utilizes an estimated depth map to learn a geometry-aware representation.\nTo train an RGB-D backbone network, the method generates 3D views with an image and estimated depth map and utilizes them with the previous SSL method."
            },
            "weaknesses": {
                "value": "[Quality & Significance]\nAs mentioned in the abstract, the goal of SSL is to learn useful representation for \"downstream tasks.\"\nHowever, the scale of the conducted experiments is insufficient to claim the effectiveness of the proposed method. \n1) The proposed method is evaluated only in small-scale datasets (e.g., ImageNet-100, ImageNet-1k).\n2) The proposed method is evaluated only in small-scale models (e.g., ResNet-18, 50). \n3) The proposed method is evaluated only in a classification task.\n\nThe proposed method must need to show its effectiveness and scalability in large-scale datasets, various backbone models (e.g., CNN, Transformer variant models), and diverse downstream tasks (e.g., 2D/3D detection, 2D/3D segmentation, 3D reconstruction, 3D view generation, etc)\n\n[Clarity]\nI recommend the authors to narrow down the scope of the proposed method from general SSL to a specific SSL method.\nThe current backbone tasks RGB-Depth image as inputs, so targeting downstream tasks for RGB-D inputs (e.g., RGB-D segmentation, 3D recon, 3D view synthesis, human pose estimation) is a more reasonable choice to claim the effectiveness of the proposed method.\nThe current claim is too general and insufficient to support the claim."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6455/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757451099,
        "cdate": 1698757451099,
        "tmdate": 1699636721332,
        "mdate": 1699636721332,
        "license": "CC BY 4.0",
        "version": 2
    }
]