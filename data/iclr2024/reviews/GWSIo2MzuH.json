[
    {
        "id": "7YhGrfv6kw",
        "forum": "GWSIo2MzuH",
        "replyto": "GWSIo2MzuH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_Fiyn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_Fiyn"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a series of PAC information-theoretic generalization error bounds via entropy measures (Shannon entropy and Renyi entropy) of loss. Specifically, a data-independent case is first considered that gives a entropy related characterization of concentration. Then in the data-dependence case, PAC-generalization error bounds by entropy measure are proposed for both leave-one-out and supersample settings, and fast-rate bounds."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed bounds based on loss entropy is advantagous since the loss is one-dimensional and easier to estimate. Many scenarios are considered, e.g., leave-one-out, supersample, and fast-rate. As far as I can tell, the proofs are sound. The theory and quantities in the bounds are well-explained. Experiemntal studies show that loss entropy-based bounds have the highest correlation with the error, and gives tighter numerical bounds in many datasets with different optimization algorithms."
            },
            "weaknesses": {
                "value": "There is no particular weakness of the paper."
            },
            "questions": {
                "value": "Should we assume the loss has finite alphabet so as to allow well defined Shannon entropy?\n\nMinor:\n\nFirst paragraph of Section 3, \"We begin by enhancing existing upper bounds presented in (Kawaguchi et al., 2022; 2023)\", should it be Kawaguchi et al., 2023 only?\n\nFirst paragraph of Section 5, \"chaining strategy\" and \"random subsets or individual\" with the same ref Zhou et al. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1740/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698555229597,
        "cdate": 1698555229597,
        "tmdate": 1699636102784,
        "mdate": 1699636102784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U8xlAILx19",
        "forum": "GWSIo2MzuH",
        "replyto": "GWSIo2MzuH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_YPBm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_YPBm"
        ],
        "content": {
            "summary": {
                "value": "In this paper, several new information-theoretic generalization bounds are derived. These bounds rely on the entropy of the losses of the model. While the proof techniques take inspiration from the information bottleneck approach of Kawaguchi et al, this is combined with the evaluated CMI perspective in the framework of Steinke and Zakynthinou. Both the leave-one-out and standard supersample settings are considered, and fast-rate bounds are derived. The usefulness of the bounds is demonstrated in several ways, both in terms of their correlation with the true error, and in terms of their numerical tightness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper combines ideas from the work of Kawaguchi et al regarding typicality proofs with e-CMI approaches from the line of work starting with Steinke & Zakynthinou, and adds several new ideas on top of this. Information-theoretic generalization bounds have demonstrated the potential to be powerful tools for studying, e.g., deep nets--although their usefulness is still not entirely clear--so the advances are significant.\n\nThe resulting bounds are highly appealing from many perspectives, in that the bounds depend on information measures that are shown to be tighter than existing work in relevant scenarios; high-probability bounds are derived with samplewise metrics and a log-dependency on delta; the quantities are (comparatively) easy to estimate; and the bounds are numerically tight in the studied scenarios.\n\nThe experimental evaluation is extensive, and significant efforts are made to make fair comparisons to prior work (arguably, they are even unfair to the benefit of the prior work). \n\nThe paper contains many instructive comparisons to prior work and discussion of the presented bounds and proof techniques. While the results are not always easy to parse due to the heavy notation and the fact that they simply have a relatively complicated form, I appreciate the \"progressive complexity\" of the presentation: the results are first stated in terms of generic constants, to get an overall view of the result, but they are still explicitly defined immediately after, allowing the reader to see the full details."
            },
            "weaknesses": {
                "value": "Most of the weaknesses arise from the application to potentially unbounded losses. If the results are specialized to the 0-1 loss, all of the notable issues seem to disappear.\n\nThere seem to be some issues with the use of sub-Gaussianity in some proofs (see questions).\n\nRegarding the numerical evaluation, the effect of the binning is not at all discussed, but seems like it can have a huge impact. Furthermore, there seem to be some issues with the optimization of the bounds, as well as the comparison with the binary KL bound. All of these are discussed in the questions below.\n\nThe bounds assume discrete loss functions, which enables the use of the entropy and the typicality arguments. Still, the bounds are applied to, e.g., the cross-entropy loss, which is continuous, with the motivation that all losses can be considered discrete due to floating-point numbers. I am not sure if this is convincing. Increasing the precision used would affect the cardinality of the loss, and potentially degrade the bounds. This does not seem like reasonable behavior, and would require a stronger motivation.\n\nIn the data-independent bounds, it is assumed that the features are generated from a function that depends on an $m$-dimensional nuisance variable, and that the model has finite sensitivity to the nuisances, even in a worst-case sense. This seems like quite a strong modelling assumption, but is not motivated or discussed in detail.\n\nTheorem 3 is said to be most useful in the interpolating setting. The interpolating bound of Haghifam et al decays with log(n), unlike the presented bound. This merits a mention and discussion.\n\nSeveral of the bounds on the generalization gap have an explicit dependence on the value of the test loss itself on the right-hand side. For bounded losses, this can simply be upper-bounded, but otherwise it seems quite puzzling.\n\nThe bounds depend on average quantity (entropy), and are hence not empirical. This may merit a disccussion.\n\nPAC-Bayesian bounds are not discussed much, despite being closely related. They are particularly relevant since you point out that recent bounds are \u201cprimarily restricted to average-case scenarios\u201d. Moreover, the supersample setting is equivalent to almost exchangeable priors (Audibert, \u201cA better variance control for PAC-Bayesian classification,\u201d 2004 and Catoni, 2007). It also seems relevant to compare to single-draw bounds from, e.g., Esposito et al., \"Generalization Error Bounds Via R\u00e9nyi-, f-Divergences and Maximal Leakage\u201d, 2020.\n\nSome other minor points are discussed in the questions below.\n\n*Update*: All of these points have been mostly clarified or resolved. The sub-Gaussianity arguments are not super-clear but appear to be fine. The paper now includes a more well-motivated evaluation procedure with regards to the bins, as well as a study of their effect on the bounds. Related work and discussion points have been added. The appearance of the test-loss in the bounds has been shown to be avoidable in some cases, while for others it is argued that it can be reasonably estimated with validation data---although this definitely limits the applicability of the bounds and should be made very clear. I updated the score to a 6 from a 5 following the discussion."
            },
            "questions": {
                "value": "(See \"*update*\" at the end of weaknesses for summary of responses)\n\nAs noted in Theorem 3, the proofs require sub-Gaussianity under the _selection_ random variable, which has to hold for any fixed instance of the loss matrix/vector. This seems to get lost in the later derivations. First, in Lemma 14, it is assumed that $\\ell(w,X)$ is sub-Gaussian. This seems to mean that it is sub-Gaussian when taking the expectation over $X$. However, in the proof, it is needed that $\\ell(w,x_U)-\\ell(w,x_{1- U}) $ is sub-Gaussian under $U$ for all $w$, $x_0$, and $x_1$ (or under the distribution where $X$ and $W$ are distributed jointly and $U$ is drawn independently). This does not follow from the sub-Gaussianity that is assumed. There are cases beyond bounded losses that one can consider (e.g., Steinke & Zakynthinou, Thm. 5.1 and Sec 5.4). Is my reading correct or did I miss something? The same issue occurs in Theorem 9.\n\nAlso, for, e.g., the equation preceding (29), it is written $P( \\Delta(W, \\tilde S_l, U) \\geq t )$. This is misleading, as it appears that the probability holds over a joint draw of $(W, \\tilde S_l, U)$, but in fact, only $U$ is random while $W$ and $\\tilde S_l$ is fixed. Is this correct?\n\nAt the end of p.4: the supersample generalization gap approaches the true one, but the same is not true for the leave-one-out, as the test loss is still evaluated on only one sample. Does this limit the usefulness of the LOO results?\n\nIn the numerical evaluations, the cross-entropy loss is used, which is continuous. To compute bounds for MNIST etc, a binning with a bin size of 0.5 is used. The usefulness of the results, despite the assumption of discreteness, was motivated by the fact that floating-point numbers are used in practice. Certainly, machine precision is significantly smaller than 0.5. This specific choice is also not motivated. What is the effect when decreasing the binning size? Is there any reason to believe that  this is a sensible choice?\n\nIn the numerical evaluation, optimization is performed over several constants in the bounds. In the derivation, they are assumed fixed, and the bounds hold with a certain probability for this fixed value. In order to have a valid bound while optimizing, one needs to perform some kind of union bound, or other argument to guarantee the validity. Otherwise, it should be stated that the bounds are not actually valid due to this optimization, but are just illustrative. Did I miss something that motivated this optimization?\n\nThe bounds are said to be compared to the Binary KL bound of Hellstrom and Durisi, but Theorem 10 in the paper is not a binary KL bound. Furthermore, in HD Thm. 7, the information measure that appears is not a CMI, but a random KL divergence depending on the supersample and selection variable. Also, the binary KL bound does not imply a bound on the generalization gap, but instead, directly bounds the population loss given a training loss. Finally, the binary KL bound is inherently only valid for bounded losses, as the binary KL is undefined for inputs outside of $[0,1]$. Is the comparison actually to Thm. 10, i.e., the square-root bound of Hellstrom and Durisi?\n\nThe plots show the generalization gap, rather than the bound on the population loss itself. Arguably, the latter is more interesting, since a low generalization gap with a high training loss is not very useful. Is it possible to show the bound on the population loss?\n\n***\n\nI acknowledge that I may have missed something or misunderstood some of the arguments. If the issues above are addressed, especially regarding the soundness of the results and their evaluation, the relevant ratings will be significantly changed.\n\n***\n\nMinor points and typos:\n\nWhile the somewhat unfortunate acronym LOO appears to be standard, the even more unfortunate acronym SS has, as far as I know, not been used before. Perhaps it is preferable to avoid it.\n\nI am not sure the way the term \u201cPAC\u201d is used is really consistent with PAC learning. PAC bounds are typically uniform in some sense, whereas the bounds in this paper are tail bounds under the joint distribution of data and hypothesis.\n\nBefore Thm. 1: grammar issue\n\np.5, \u201cThe most notable improvement of\u2026\u201d here, a comparison is made to CMI bounds instead of e-CMI bounds, which are arguably more relevant; \u201cThis illustrates a novel trade-off\u201d is it really a trade-off? Both are just preferred to be small.\n\nFor the notation of $L^{\\kappa}$: perhaps it would be more intuitive to use $L^{<\\kappa}$ and $L^{>\\kappa}$?\n\nIn Lemma 12, it is written $\\eta = ( \u2026, \u2026 )$. Should this be $\\eta \\in (\u2026 , \u2026)$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Reviewer_YPBm"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1740/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704538235,
        "cdate": 1698704538235,
        "tmdate": 1700428368806,
        "mdate": 1700428368806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EAHNiuWb0o",
        "forum": "GWSIo2MzuH",
        "replyto": "GWSIo2MzuH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_gBmk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_gBmk"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce novel upper bounds for generalization error or validation error in the context of high-dimensional applications, particularly deep models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides some new PAC upper bounds which can be used in deep models."
            },
            "weaknesses": {
                "value": "Weaknesses and Questions:\n\n-  In Theorems 1, 2, 3, and 4, it is essential to specify the distribution under which the inequality holds.\n\n- The paper states that \"the model $w$ is deterministic, which is commonly the case for modern deep learning models,\" but notes that $w$ is not truly deterministic due to initialization.\n\n- In the discussion preceding Section 3.2, the authors mention that their data-independent bounds provide insights into understanding the generalization of the MEE criterion. However, for binary classification, margin-based loss functions, as discussed in [1], cannot be represented as a function of $E^w$.\n\n- The effect of the learning algorithm on Theorems 1 and 2 is unclear. It seems that the model, $w$, remains fixed and independent of the training set, limiting the practicality of these upper bounds.\n\n- The proof of Lemma 13 lacks clarity. More elaboration is needed, especially in the last line of the proof.\n\n- The assumption in Theorem 5 that $\\kappa \\geq B^{W,\\tilde{S}}$ guarantees a valid $C_i$ due to the term $\\log(2-e^{2\\eta \\hat{L}_i^W})$. However, Theorem 6 discusses the case where $\\kappa < B^{W,\\tilde{S}}$. Explain how $C_i$ remains valid in Theorem 6.\n\n- If the loss function is bounded, then $B^{W,\\tilde{S}}$ is also bounded. Can the authors provide an example where the loss function is unbounded, but $B^{W,\\tilde{S}}$ remains bounded?\n\n- Please verify the equation between eq. (41) and (42) in the appendix.\n\n- The discussion after Theorem 5 is unclear. Define what an \"interpolating regime\" is, and clarify whether the empirical risk is considered a random variable in Theorem 5.\n\n- The novelty of this work is limited. \n\nSuggestions:\n\n- Include a table summarizing notations in the Appendix.\n\nReferences:\n\n[1]: Bartlett, Peter L., Michael I. Jordan, and Jon D. McAuliffe. \"Convexity, classification, and risk bounds.\" Journal of the American Statistical Association 101.473 (2006): 138-156.\n\n----\n\n\nAfter Rebuttal, I increased my score to 6."
            },
            "questions": {
                "value": "See the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Reviewer_gBmk",
                    "ICLR.cc/2024/Conference/Submission1740/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1740/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764552275,
        "cdate": 1698764552275,
        "tmdate": 1700739950814,
        "mdate": 1700739950814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p2ydKzON3k",
        "forum": "GWSIo2MzuH",
        "replyto": "GWSIo2MzuH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_xf1n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1740/Reviewer_xf1n"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a series of PAC information-theoretic generalization bounds based on loss entropy. The novel bounds are tighter than the previous bounds. The authors conducted several experiments that show the superiority of their bounds."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well-written and organized. \n- Deep learning generalization analysis through information-theoretic tools has received a lot of attention lately. This paper makes a sound contribution to this line of work.\n- The theory is sound. I skimmed through most of the proofs (I did not go through all of them in detail)  but the proofs are well-structured, easy to follow, and sound. \n- I like the fact that the paper considers not only one but two settings: the Leave-One-Out Setting and the supersample setting. This makes the work more complete."
            },
            "weaknesses": {
                "value": "- The major concern with this work is that the analysis relies on $b^w$ and $B^{w, S}$, which represent the maximum of the loss. I think this makes the bounds sensitive to noise. In fact, it is sufficient that one sample has a relatively high loss to make the whole bound loose. The authors do not perform any experiments or analysis to show how robust their bounds are to noise."
            },
            "questions": {
                "value": "- Compared to the bounds in (Kawaguchi et al., 2023), the introduced bounds in this paper, e.g., Theorem 2, do not have any explicit dependency on the input variable $ X$. Interestingly, It only depends on the label $Y$. Can the authors comment on this point? \n\n- Related to my first question, as the bounds depend directly on $Y$, it would interesting to see how sensitive they are to label noise. Do the authors have any comments on this? I think in this case, it might be interesting to conduct an additional experiment with label noise to evaluate how robust the bounds are."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1740/Reviewer_xf1n"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1740/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828240662,
        "cdate": 1698828240662,
        "tmdate": 1699636102513,
        "mdate": 1699636102513,
        "license": "CC BY 4.0",
        "version": 2
    }
]