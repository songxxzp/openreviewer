[
    {
        "id": "s5MdIxEkgT",
        "forum": "Sbi8BdKcob",
        "replyto": "Sbi8BdKcob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_aK68"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_aK68"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DetermLR, a CoT-style prompting strategy that elicits stronger reasoning capabilities from LLMs. Specifically, DetermLR iteratively identifies the most promising premises, prioritises and \"executes\" them, and then stores useful premises in a memory. \n\nExperiments are performed on 4 complex logical reasoning datasets, and DetermLR outperforms the 5 compared baselines, sometimes by a large margin."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* CoT-style reasoning is an active and important research area for LLMs as they allow strong reasoning capabilities to be elicited. \n\n* Logical reasoning is an important problem that LLMs are traditionally not strong at. Further investigation in this area is certainly welcome. \n\n* The proposed method achieves good performance on the 5 challenging datasets, outperforming the compared CoT-style prompting strategies."
            },
            "weaknesses": {
                "value": "* The proposed method is quite simple. Thus, the technical contribution is light. For instance, the \"systematic premise identification module\" described in Sec. 3.1 is really quite simple, and I don't know whether I'd call it \"systematic\". \n\nBesides, it closely follows the Cumulative Reasoning (Zhang et al., 2023) technique, with the addition of a memory. Thus, the novelty is limited. \n\n* Some important details have been omitted in the paper, making it harder to understand the technical contributions of the paper. I'll detail it below."
            },
            "questions": {
                "value": "* In Eq. (1), (2) and (3), what are the definitions of \\texttt{relevance}, \\texttt{supplement} and \\texttt{verify}? If you follow Cumulative Reasoning (CR), are all these functions realised by the LLM?\n\n* In Sec. 3.3, what exactly is the memory? \n\n* What is the definition of \"state\" in this paper? Is it the number of \"invoked\" premises? \n\n* The results on CR on FOLIO in your paper are different from that in the original paper (on GPT-4), which is much higher (87.45 vs 69.11). Why such a large discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1911/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698467303281,
        "cdate": 1698467303281,
        "tmdate": 1699636121739,
        "mdate": 1699636121739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YPZvQbW3Ad",
        "forum": "Sbi8BdKcob",
        "replyto": "Sbi8BdKcob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_KbK2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_KbK2"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new reasoning framework, DetermLR, aimed at enhancing the logical reasoning capabilities of large language models. The study addresses challenges of LLMs facing in emulating human-like reasoning, including selecting appropriate reasoning structures, efficiently using known information, and incorporating past reasoning into future decisions. DetermLR use premise identification, premise prioritization and exploration, and an iterative process with reasoning memory. Experimental results on logical reasoning tasks show that DetermLR outperforms baselines in terms of reasoning performance and efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-motivated and proposes a novel framework that tackles the challenges in emulating human-like logical reasoning.\n- The method incorporates a prioritized strategy to direct the reasoning process; history reasoning information including valid and invalid intermediate results to continue reasoning, which are key components for effective reasoning.\n- The experimental results demonstrate the effectiveness and efficiency of the proposed framework compared to baseline methods."
            },
            "weaknesses": {
                "value": "1. The paper lacks much necessary information about framework. \n- How do you score relevance and supplement? A transparent LM or directly instruct GPT-4? \n- From Fig.1 first step, the authors seem to filter out determinate premises by words matching (eg, the sentence including Gary). However, in some cases, the relationship between premises and conclusion is only logic-level. eg., If Erin is round, then Gary is quiet.\n- How do you exploit history reasoning paths information? \n\n\n2. [1] paper use a similar idea, alternating between premises selection and inference, despite of lackness of history reasoning paths. But the authors do not emphasis the point in details. In facts, I think those failure cases can help reduce search space.\n\n[1] Creswell, Antonia, Murray Shanahan, and Irina Higgins. \"Selection-inference: Exploiting large language models for interpretable logical reasoning.\" arXiv preprint arXiv:2205.09712 (2022)."
            },
            "questions": {
                "value": "See weakness above. If the authors can perfectly solve my issues, I will consider improving my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1911/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832669485,
        "cdate": 1698832669485,
        "tmdate": 1699636121652,
        "mdate": 1699636121652,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lu896A0VeJ",
        "forum": "Sbi8BdKcob",
        "replyto": "Sbi8BdKcob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_2L5z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_2L5z"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new prompting technique for logic reasoning, DetermLR, which first classifies the given premises into determinate and indeterminate, and then sorts them by priority. Then DetermLR will first explore the premises with high priority and store the new conclusions into the memory for future reference. The proposed method shows superior results on LogiQA, ProofWriter, FOLIO, and LogicalDeduction, over multiple baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method seems to be quite effective on four logic reasoning datasets, compared to multiple baselines.\n2. The paper is mostly clear and well-written."
            },
            "weaknesses": {
                "value": "1. It is not clear how each module in the proposed framework is implemented. Such information is completely missing in the method section while judging from the experiment section, it seems that all of them are implemented by prompting GPT4. However, it is still unclear how the scorers are implemented and what is the threshold $\\theta$ for supplementary premises filtering.\n2. I'm not quite sure why the proposed method can select a reasoning structure. The main technique of the proposed method seems to be classifying the given premises into determinate and indeterminate, and sorting the premises by priority. It seems to adopt a mostly linear reasoning structure with memory reference.\n\nI'm willing to increase my score if my concerns are properly addressed."
            },
            "questions": {
                "value": "1. How is the time/compute efficiency of the proposed method compared to the baselines? I understand that the proposed method visited fewer states during inference, but I'm not quite sure if the compute/number of GPT4 prompting for each state visiting is the same as the baselines."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1911/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699106021899,
        "cdate": 1699106021899,
        "tmdate": 1699636121568,
        "mdate": 1699636121568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vBSaxrFfy6",
        "forum": "Sbi8BdKcob",
        "replyto": "Sbi8BdKcob",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_96xE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1911/Reviewer_96xE"
        ],
        "content": {
            "summary": {
                "value": "They propose an approach for improving the logical reasoning of LLMs. Their approach is to structure the prompt with three main components including their so-called, premise identification, premise prioritization, and iterative process with reasoning memory.  They conduct experiments on four logical reasoning datasets using their prompting strategy. Their approach outperforms other recent strategies for prompting LLMs such as chain-of-though and some other variations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-The authors propose a new strategy for structuring the prompt for LLM to make them perform logical reasoning with a higher accuracy.\n-The experiments show the effectiveness of the proposed approach compared to the existing structured prompting strategies."
            },
            "weaknesses": {
                "value": "-The terminology, notations, and in general the explanation of the proposed approach was not very clear to me. \n-The results focused on the selected subsets of datasets -selected by authors. No comparison with other results on these datasets [outside this work] was made. Or this was not made explicit at least in the paper as far as I understood.   \n-The results were reported only on GPT4.  \n\nSee some details in the Questions section."
            },
            "questions": {
                "value": "-From the provided examples, it was not made clear to me why the term indeterminacy was chosen for some parts of the information.\n-in section 3.1., the authors explained the premise identification, and then only in section 3.2 they started introducing formal notations.\n-The formalization and notation are somewhat superficial and not really used to help understanding. \n-The flow of information and how these modules exactly work is not clear, are you expecting the LLM to do these steps with a few shots of in-context learning? For example, how the model was asked to identify the premises in the first step? These are just very hard to read from the current presentation of the paper. \n--Not clear what the authors mean by verification check? how the verification is performed, and what is the kind of computation used here for verification?\n--What do you mean by states and visited states? I did not see this defined in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1911/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1911/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1911/Reviewer_96xE"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1911/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699326278246,
        "cdate": 1699326278246,
        "tmdate": 1699636121477,
        "mdate": 1699636121477,
        "license": "CC BY 4.0",
        "version": 2
    }
]