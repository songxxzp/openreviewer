[
    {
        "id": "w7osqkCcEQ",
        "forum": "XD0PHQ5ry4",
        "replyto": "XD0PHQ5ry4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission662/Reviewer_ARUW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission662/Reviewer_ARUW"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a framework called SELF where an LLM is trained to acquire meta-skills that it applies to itself so as to improve its own performance on downstream tasks. The LLM is asked to refine its own output and learns from this refinement process, thus generating better and better output on various tasks, and still refining it. SELF has two processes (self-evolution, self-refinement), and the impact of these processes is studied in various datasets in comparison to and combination with a baseline called self-consistency, as well as with various ablations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "To me, the general approach is novel, quite original and potentially very fruitful. I'm convinced that, properly rewritten, this paper could have some impact.\n\nThe various comparisons, combinations and ablations studies efficiently shed light on the impact of the various processes, and provide a convincing picture of the approach. In particular I appreciate that the authors combined their approach with the self-consistency approach they compare theirs to.\n\nIf the authors manage to improve a lot the writing of the next version of their paper (see below), I'll be glad to change my rating towards acceptance."
            },
            "weaknesses": {
                "value": "The paper is poorly written at different levels and suffers from unclarities and from the lack of comparison with similar work. A tentative list:\n- the only work the authors compare to is Wang et al. (2022a) about self-consistency, but this work is not even mentioned in the related work. It should be explained with some details.\n- about related work, the authors ignore many attempts to use RL on large language models without human feedback, using the capability of LLMs to self-evaluate or some rewards coming from the task itself (see e.g. [1, 2] and [3] for some overview). A discussion of the difference to these related works and others found following the referenced papers in these works would be more than welcome. \n- More experimental comparisons would make the paper stronger.\n- there are many typos, some non-sentences, a lot of points are poorly written, I'll try to provide a list below, but the authors should find a way to improve a lot the way the paper is written, either using grammar tools or the help of stronger scientific writers. In particular, I think that the introduction could put more forward the many messages that can be extracted from the empirical study.\n\n\n- [1] Pang, J. C., Wang, P., Li, K., Chen, X. H., Xu, J., Zhang, Z., & Yu, Y. (2023). Language Model Self-improvement by Reinforcement Learning Contemplation. arXiv preprint arXiv:2305.14483.\n- [2] Carta, T., Romac, C., Wolf, T., Lamprier, S., Sigaud, O., & Oudeyer, P. Y. (2023). Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662.\n- [3] Sun, H. (2023). Reinforcement Learning in the Era of LLMs: What is Essential? What is needed? An RL Perspective on RLHF, Prompting, and Beyond. arXiv preprint arXiv:2310.06147."
            },
            "questions": {
                "value": "- can the authors explain how the approach of Wang et al. (2022a) works and how it is related to their work?\n\n- how does the SELF method relate to methods applying reinforcement learning without human feedback to improve LLMs? Could some of these methods be compared experimentally of the same datasets?\n\n- \"SELF facilitates the acquisition of self-refinement ability in smaller LLMs\": this sentence is confusing in several respects. Do you mean that SELF can only be applied to small LLMs? Or that the SELF framework can be used in a context where the refiner network improves another, smaller downstream network? In both cases, this is raising questions that the paper does not answer to: (1) would the method work with larger LLMs? If SELF can be applied in a context where the refiner network improves another, smaller downstream network, what are the corresponding experimental results?\n\n- Related work, about RLHF. I don't understand the sentence \"RLHF involves complicated iteration between the models and reward functions, requiring many hyper-parameters tuning\". Can you explain better what you mean? Provide a reference?  Another problem that the authors do not put forward is the availability of humans to perform RLHF.\n\n- Are LLMs so good at self-feedback? We often read that many of them are very certain to be correct when in fact they are completely wrong. Can you back up the claim that they are good at self-feedback with references? Won't there be many counter-examples?\n\n- How does your method prevent overfitting? When do you stop training and self-refining?\n\n- How many examples in the EvolInstruct testset? You said it for all other datasets.\n\n- Could you explain Fig. 3? What do the colors mean, what should we see, how was it obtained?\n\n- In 4.4: \"We present a comparison between utilizing the entire self-curated data\u2014Unfiltered (4k)\u2014and employing self-filtered data\" -> what is the difference between self-curated and self-filtered? I don't understand the point here...\n\n# Local issues and typos: #\n\nI would add an \"s\" at the end of the title (models).\n\nthese models' innate potential: these models are not biological systems, can we say that they have some innate potential? Don' you mean \"intrinsic\"?\n\nAs depicted in Fig 2 and Fig 1 -> reverse order\n\nrefinement. thereby (remove dot)\n\nSection 3.1 starts with \"We observe the base Vicuna\" but nothing has been said about Vicuna before, this sentence comes out of the blue.\n\nIt's -> It is\n\nThe beginning of Section 3.2 and 3.2.1 is full of typos:\n- the model progressively self-evolving -> evolves\n- the model M_meta generate and refine -> generates and refines\n- for the evolve iteration t -> evolution\n- with each instance in this augmented corpus is noted -> remove \"is\"\n- self-evolution, We initialize -> we\n\nAre shown in 3. -> Figure ? Table ? Section ?...\n\n\"For an in-depth understanding of each column\u2019s meaning and significance.\" -> this is not a sentence, something is missing\n\nless evident for \u201dContinual Training (D^t_self)\u201d -> \u201dContinual Training (D^t_self Only)\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697622265362,
        "cdate": 1697622265362,
        "tmdate": 1699635993705,
        "mdate": 1699635993705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GnqEPIhSef",
        "forum": "XD0PHQ5ry4",
        "replyto": "XD0PHQ5ry4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission662/Reviewer_9itr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission662/Reviewer_9itr"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce \"SELF,\" which enables the continual enhancement of the abilities of LLMs. In this approach, the model first learns two meta-skills: self-feedback and self-refinement. Afterwards, the model can autonomously generate responses from given unlabeled instructions. Moreover, the model can be trained further on these instructions and filtered responses to achieve better improvement. Additionally, during inference, self-refinement can be utilized to attain better performance. Experiments conducted on GSM8K, SVAMP, Vicuna, and Evol-Instruct demonstrate the effectiveness of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of the proposed pipeline is sound.\n2. The experiments in the paper demonstrate the effectiveness of SELF."
            },
            "weaknesses": {
                "value": "1. The paper lacks organization. Some essential details are absent, making it difficult to reproduce the results. Below are some sample questions that need to be addressed in the paper:\n     a) What is the detailed pipeline for collecting the training corpus for meta-skill learning (self-feedback, self-refinement)?\n     b) Which model is employed to generate feedback and refine answers produced by $M_{initial}$?\n     c) What are the hyperparameters used during $M_{meta}$'s training?\n\n2. The second and third rounds of self-evolution utilize self-instruct to generate new questions, whereas the first round only employs the original questions. Is there a particular reason for this discrepancy? Furthermore, if we were to combine all this data into a single round, would the performance be comparable to that of multiple rounds?\n\n3. Table 2 indicates that, during the meta-skill learning stage, the model undergoes training over QA data with ground-truth labels, significantly enhancing the QA performance. Yet, section 3.1 mentions that meta-skill learning doesn't encompass training over QA. How does the paper reconcile this inconsistency?\n\n4. The second row of Table 2 reveals that training over $D_{meta}$ can also boost reasoning performance. Is this improvement attributed to the use of self-refinement during inference? If so, what would the performance be without utilizing self-refinement during inference?\n\n5. The author should compare SELF to a supervised fine-tuning baseline. What is the performance of fine-tuning Vicuna over the GSM8K training set? Will its performance surpass that of SELF? According to the paper titled \"Scaling Relationship on Learning Mathematical Reasoning with Large Language Models,\" direct fine-tuning of the model llama-7b over the GSM8K training set appears to achieve approximately 35% accuracy. This figure is better than the results depicted in Table 2. What advantages does the intricate \"SELF\" pipeline offer in practice? A potentially simpler alternative might involve gathering quality responses with more robust models like GPT-3.5/GPT-4 and fine-tuning the model over those results.\n\nIn summary, while the proposed pipeline is interesting, the authors need to provide additional materials to support their claims and outcomes."
            },
            "questions": {
                "value": "Here are the revised statements:\n\n1. Is self-refinement a necessary component for SELF? Table 1 indicates that SELF with self-consistency is already sufficient. In fact, self-refinement can sometimes even degrade performance. To fully justify the necessity of self-refinement, it is recommended to add an ablation study on its use in self-evolution part.\n2. Some related work on using LLMs for debugging/checking is absent.\n    - Teaching Large Language Models to Self-Debug\n    - Deductive Verification of Chain-of-Thought Reasoning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission662/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission662/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission662/Reviewer_9itr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698722356867,
        "cdate": 1698722356867,
        "tmdate": 1699635993640,
        "mdate": 1699635993640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e9kcYEmDxr",
        "forum": "XD0PHQ5ry4",
        "replyto": "XD0PHQ5ry4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission662/Reviewer_VDXd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission662/Reviewer_VDXd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel framework called SELF (Self-Evolution with Language Feedback) to enable large language models (LLMs) to self-evolve and improve their capabilities over time. In detail, the paper proposes to 1) equip LLMs with meta-skills for self-feedback and self-refinement through meta-skill learning. This allows models to evaluate their own outputs and refine them; 2) Use the meta-skills to generate high-quality training data via self-curated responses and iterative refinement; 3) Conduct self-evolution training where models iteratively fine-tune on self-curated data to enhance capabilities; 4) Apply online self-refinement during inference to further improve response quality. Experiments on math and general domain benchmarks demonstrate SELF can consistently improve model performance through self-evolution cycles. The learned meta-skills also enable smaller models to acquire advanced self-refinement abilities."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of empowering LLMs with meta-skills for autonomous self-improvement is interesting.  \n2. The iterative process of self-generated data, training, and online refinement is intuitive and aligns well with human learning.\n3. Results verify SELF consistently improves performance over baseline models, and that meta-skills boost self-refinement capability."
            },
            "weaknesses": {
                "value": "1. The quality of meta-skills relies on the initial annotator model/human. No analysis on sensitivity to this factor.\n2. Limited insight on how self-evolving training affects model internals and learned representations.\n3. More comparisons to related human preference alignment methods would be useful."
            },
            "questions": {
                "value": "1. How robust is SELF to noise in self-generated data? Are the meta-skills strong enough to filter bad data?\n2. Is there an upper limit or plateau to the self-evolution process? How to tell when to stop?\n3. For real-world deployment, how to prevent unsafe or unethical knowledge from entering self-evolving training?\n4. How dependent is SELF on starting model quality? Could it work for simple baseline models?\n5. How does the computational overhead of SELF compare to regular supervised training? Is it more expensive?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820205957,
        "cdate": 1698820205957,
        "tmdate": 1699635993558,
        "mdate": 1699635993558,
        "license": "CC BY 4.0",
        "version": 2
    }
]