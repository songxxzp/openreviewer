[
    {
        "id": "VFEhG6sCIW",
        "forum": "3bq3jsvcQ1",
        "replyto": "3bq3jsvcQ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_XiDg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_XiDg"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a new form of guided prompting for Q&A settings in which a model 1) *Abstracts* (takes a step back) the key concepts relevant to answering a given question and then 2) *Reasons* by using the Step-back answer in conjunction with the original question to produce a final answer. The authors demonstrate the efficacy of this method other other multi-shot (In-context learning) prompting schemes and Chain-of-Though (CoT) reasoning with PaLM-2L on various datasets. They also compare against vanilla GPT-4."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The methodology is conceptually very clearly explained and motivated. The experiments are extensive, and some useful ablations are carried out. Besides a few points raised below, there is little to critique from the standpoint of methodology or presentation. \n\nThe method itself appears to be quite effective, at least for PaLM-2L. It is simple and novel enough to warrant dissemination, given that there is precedent for the publication of prompting methodologies at top-tier conferences (to offer no comment on the scientific merits of this)."
            },
            "weaknesses": {
                "value": "One consistent issue with the paper is the use of incorrect grammar (plurals, subject references etc.) - this issue should be easily remedied through the use of grammar checkers (e.g. Grammarly) or native proof-reading. Notably, in spite of the somewhat jarring errors in English grammar, the sentences are well-structured and the paper has a clear narrative, such that it remains easily comprehensible.\n\nOn a related note, the authors consistently use the terms \"learn\" and \"teach\" in relation to the step-back question, and the knowledge it provides. This is somewhat confusing, as I don't think any models are fine-tuned etc. to provide this knowledge. Whilst I realise that few-shot prompting is referred to as \"in-context *learning*\", I would recommend steering clear of this language unless you perform actual updates to the model weights at some stage of the Step-Back Prompting process.\n\nThere are two potential methodological weaknesses, which may in fact reflect a misunderstanding on the part of the reviewer.\n1) *Baselines might lack useful conditioning in the prompt*. In particular, in section D.2 you state that the baseline prompts only take the question and initial query, whereas Table 11 shows that Step-Back prompting includes the lines e.g. \"You are an expert at Physics. You are given a Physics problem\". If these are not included in the baseline, then this would appear to be an unfair comparison. Table 15 suggests that the baseline may actually have this information too, so perhaps this is not a concern and section D.2 just omitted this detail. \n2) The fact that the methodology is evaluated only on PaLM-2L. I appreciate that GPT-4 calls are not cheap, and Figure 1. provides some evidence for the consistent behaviour of GPT-4 and PaLM-2L. Nonetheless, it is conceivable that this method would not work equally well on other models, and this concern has not been ruled out by the existing experiments."
            },
            "questions": {
                "value": "Three other things that bear clarifying:\n1) How many exemplars are provided for the standard Step-Back experiments (those shown in table 1 etc.). Ablations in Figure 3. suggest it doesn't matter too much, but it would be good to be clear (from Figure 3 one might infer 1 or 5 exemplars are provided, and of course 5 would seem unfair to the baselines)\n2) The fact that the step-back question is not generic, but rather already conditions on the context of the dataset (e.g. \"What are the physical principles\") is worth making more explicit in the methodology. (On a side-note, It would also be interesting to know how a generic prompt such as \"Abstract the general principles relevant to this query\" would have worked)\n3) Why was Step-Back prompting not attempted on GPT-4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None of note"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Reviewer_XiDg",
                    "ICLR.cc/2024/Conference/Submission2214/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653277710,
        "cdate": 1698653277710,
        "tmdate": 1700642923234,
        "mdate": 1700642923234,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jcTR8zciTF",
        "forum": "3bq3jsvcQ1",
        "replyto": "3bq3jsvcQ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_FiKx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_FiKx"
        ],
        "content": {
            "summary": {
                "value": "This work proposes step-back prompting, which prompts the LLM to ask a question about a higher-level concept/principles first. This works as a \"retrieval step\" which allows it to retrieve relevant facts on which the subsequent reasoning can be grounded. \n\nThe model shows good performance on a variety of knowledge-intensive tasks which are typically effectively tackled with RAG methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The idea of ground reasoning on higher-level abstractions (abstracting away low-level details) is interesting as a principle. It is clear that this kid of reasoning strategy helps on knowledge-intensive tasks, where it helps to retrieve the high-level principles first before proceeding with reasoning."
            },
            "weaknesses": {
                "value": "While the idea of reasoning from abstract to low level is interesting, the approach explored in the paper is arguably rudimentary - A generic question that asks for a higher-level abstraction only works as a byproduct of the fact that LLM already has near perfect knowledge of such concepts (In Fig 4. the low principle error points to this). \n\nWithout a further study of what the kind of abstractions LLM excels at and is still lacking in, my impression is that the method largely functions as better prompt for retrieving relevant facts for knowledge-based (mostly scientific) questions. In my view, the paper would benefit from broadening the exploration of abstraction upon a wider set of tasks."
            },
            "questions": {
                "value": "1. Is it possible to extend the evaluated tasks to ones involving more broader cases of reasoning, such as GSM8k [1] or bAbi [2]?\nThat is, do tasks exist in which LLM can fail at deriving the higher-level principle?\n\n2. The paper mentions decomposed prompting in the relate works - It it possible to compare with any such methods other than CoT\n\n3. I'm also curious about the possibility of combining such methods with step-back prompting.\n\n4. Is it possible to study the effect of applying abstraction more than once or in a multi-step manner?\n\n\n\n[1] Training Verifiers to Solve Math Word Problems, Cobbe et al., arXiv, 2021\n[2] Towards ai-complete question answering: A set of prerequisite toy\ntasks. Weston et. al, ICLR, 2016."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Reviewer_FiKx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856782598,
        "cdate": 1698856782598,
        "tmdate": 1700644802019,
        "mdate": 1700644802019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "asDSVymYhI",
        "forum": "3bq3jsvcQ1",
        "replyto": "3bq3jsvcQ1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_xaDx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2214/Reviewer_xaDx"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to improve the reasoning ability of large language models, especially for complex tasks that require a large amount of prior knowledge and details. The proposed step-back prompting, first asks a relevant high-level question (called stepback question) and then uses the answer for the following reasoning steps. This stepback question could remind LLM of some principles that are fundamental of the question, and thus improve the reasoning process. Extensive experiments are done on several benchmarks that demonstrate the effectiveness of the step-back prompting compared to the CoT baselines."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Step-back is a reasonable improvement over the existing LLM reasoning prompting strategy. It is especially helpful for tasks that need complex prior information to do reasoning, which broadens LLM's reasoning ability. \n\n- The step-back prompting approach is evaluated with extensive and complementary experiments.\n\n-  The proposed step-back prompting shows significant improvements over several variants of chain-of-thought, including the recent \"take a deep breath\" on several benchmarks."
            },
            "weaknesses": {
                "value": "- The step-back prompting is only evaluated on Google's PaLM2. Though it shows significant improvements, it would be difficult for the community to reproduce the results. It would be great to evaluate the proposed prompting approach also on open source LLMs, such as LLaMA2-70B.\n\n-  The step-back question is pretty unique on each benchmark. It seems the specific stepback questions are designed for each benchmark. How to ensure the stepback questions is neat and how to design a perfect stepback question is not clear.\n\n- I feel the stepback question is a special case of the least to most prompting [1], which decomposes complex questions into subquestions and solves them order by order. The stepback questions can also be considered as a subquestion for the following reasoning steps. Can the author further clarify their difference from a principled perspective?\n\n[1] Least-to-Most Prompting Enables Complex Reasoning in Large Language Models, Zhou et al 2023"
            },
            "questions": {
                "value": "Please refer to the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2214/Reviewer_xaDx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2214/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699180895223,
        "cdate": 1699180895223,
        "tmdate": 1699636155067,
        "mdate": 1699636155067,
        "license": "CC BY 4.0",
        "version": 2
    }
]