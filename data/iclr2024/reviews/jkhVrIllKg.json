[
    {
        "id": "hQWqJfqFky",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_M25p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_M25p"
        ],
        "forum": "jkhVrIllKg",
        "replyto": "jkhVrIllKg",
        "content": {
            "summary": {
                "value": "This paper proposes a federated learning algorithm called SABER that seeks to mitigate client drift by making each client optimize a modified local function that is the sum of the actual local function and a bias correction term like SCAFFOLD (Karimireddy et al., 2020) and a prox term like FedProx (Li et al., 2020a). Under second-order heterogeneity with parameter $\\delta$ (Assumption 1 in the paper), SABER is shown to converge to an $\\varepsilon$-stationary point in $\\mathcal{O}(\\frac{\\sqrt{M} \\delta}{\\varepsilon^2})$ rounds of communication for smooth non-convex functions (the paper mistakenly reports the communication complexity as $\\mathcal{O}(\\sqrt{M} \\delta \\varepsilon^2)$ throughout). In addition, under the $\\mu$-PL assumption, the complexity improves to $\\mathcal{O}((\\frac{\\delta}{\\mu}\\sqrt{M} + M) \\log \\frac{1}{\\varepsilon})$. Empirical results on CIFAR-10 and FEMNIST show the benefits of SABER compared to some baseline algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a way to address the important issue of client drift in federated learning. Specifically, it maintains one control variate for all the clients which is efficient.\n\n2. Sharp $\\mathcal{O}(\\frac{\\sqrt{M}}{\\varepsilon^{2}})$ complexity in the smooth non-convex case (although this result is under the strong assumption of $w_{k+1} = \\text{argmin } \\phi_k(w)$ as I have mentioned in Weakness #1).\n\n3. The second-order heterogeneity assumption is well motivated.\n\n4. Appreciable improvement over FedAvg, FedProx and SCAFFOLD (although I have some reservations about these results which I have described in Weakness #4)."
            },
            "weaknesses": {
                "value": "The authors mistakenly report the communication complexity as $\\mathcal{O}(\\sqrt{M} \\delta \\varepsilon^2)$ for the smooth non-convex case; I guess it should be $\\mathcal{O}(\\frac{\\sqrt{M} \\delta}{\\varepsilon^2})$.\n\n**Weaknesses/Concerns:**\n\n**1.** The definition of $w_{k+1} \\approx \\text{argmin } \\phi_k(w)$ in Algorithm 1 is very vague; how is $\\approx$ quantified? \nFurther, for the theoretical results in Section 3.2, as far as I understand (by looking at the proof of Lemma 3), $w_{k+1}$ has been taken to be exactly $\\text{argmin } \\phi_k(w)$. The authors have not discussed this point. *It is unreasonable to expect that $\\phi_k(w)$ can be exactly minimized* and I'd like to see a result where $w_{k+1}$ is an approximate minimizer of $\\phi_k(w)$, for e.g., an $\\varepsilon$-stationary point of $\\phi_k(w)$ as written in Section 3.1. \n\n\n**2.** Moreover, the complexity of minimizing $\\phi_k(w)$ to obtain $w_{k+1}$ has been completely ignored in Section 3.2; this is an important aspect that has not been considered. This could have been captured if there were *local steps* in the proposed algorithm; specifically, I'd find the result more convincing if there were some $H \\geq 1$ local steps of (S)GD on $\\phi_k(w)$ to *approximately minimize* it and the final convergence bound in Theorem 1 would be a function of both $K$ and $H$. \n\nI understand that the emphasis is probably on reducing the communication complexity but deriving results assuming that the local surrogate functions can be *exactly* minimized is unrealistic and too strong according to me. I'd like to see the theoretical results capture the inexactness of the approximate minimizer of $\\phi_k(w)$ as this would probably entail practically important tradeoffs.\n\n**3.** [1] and [2] (cited below) are two relevant works that have not been touched upon by this paper. Step 5 in Algorithm 1 of this paper is virtually the same as Step 4 in Algorithm 1 (PAGE) of [1]. Note that [1] is for the centralized setting. [2] proposes a federated version of PAGE called FedPAGE. *But unlike SABER, (Fed)PAGE does not need to minimize any surrogate function*. Moreover, (Fed)PAGE also attains $\\mathcal{O}(\\frac{\\sqrt{M}}{\\varepsilon^{2}})$ complexity. So it appears that the algorithms proposed in [1] and [2] are similar to SABER and attain the same complexity as SABER *without* its compute-intensive part, viz., minimizing $\\phi_k(w)$. A detailed comparison of SABER with (Fed)PAGE is needed and this includes experiments.\n\n[1]: Li, Zhize, et al. \"PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization.\" International conference on machine learning. PMLR, 2021.\n\n[2]: Zhao, Haoyu, Zhize Li, and Peter Richt\u00e1rik. \"FedPAGE: A fast local stochastic gradient method for communication-efficient federated learning.\" arXiv preprint arXiv:2108.04755 (2021).\n\n**4.** I also have some concerns w.r.t. the experimental results in the paper. As per Section 4.1, $p=0.5$ and 50 out of 100 clients are used in line 8 of Algorithm 2 for CIFAR-10. So 50% of the clients are used in every other round. It is not clear to me if the benefits of SABER are because of this. For FEMNIST, I could not find the total number of clients (the authors should clearly state this) but it is mentioned that 100 clients are used in line 8 of Algorithm 2 again with $p = 0.5$. Anyway, I think $p = 0.5$ is too large and the algorithm is practically useful only if it works with much smaller values of $p$ such as 0.1, etc. I'd like to see an ablation study showing the effect of varying $p$. Also, it is not clear to me if the hyper-parameters were tuned for the other algorithms. Finally, as I mentioned in point #3, there should be some empirical comparisons with FedPAGE too given its similarity to SABER.\n\nDue to the above weaknesses, I can only give a score of 3 for now."
            },
            "questions": {
                "value": "Please address the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697435557754,
        "cdate": 1697435557754,
        "tmdate": 1699636923943,
        "mdate": 1699636923943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XDb3yOvfQM",
        "forum": "jkhVrIllKg",
        "replyto": "jkhVrIllKg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_cfg4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_cfg4"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a novel algorithm for FL under Hessian similarity assumption. Authors provide theoretical analysis for the proposed method, showing $O(\\delta \\epsilon^{-2} \\sqrt{M})$ complexity for general non convex problems and $\\tilde{O}(\\delta/\\mu \\sqrt{M} + M)$ complexity for problems under PL condition. Experiments on logistics and NNs are provided and support the theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Novel method SABER for non convex FL under Hessian similarity. \n2. Theoretical analysis, showing that the convergence rate of SABER aligns with the rate of Gradient Descent ($O(\\epsilon^{-2})$).\n3. Extensive numerical experiments supporting the theory."
            },
            "weaknesses": {
                "value": "1. Solution of the subproblem. The subproblem can be non convex. In this case GD for the subproblem would require $\\epsilon^{-2}$ iterations to find $\\epsilon$-stationary point. That might significantly slow down the convergence of the whole procedure. Moreover, it is assumed that the subproblem could be solved exactly. In my point of view, this not only makes the method impractical, but also simplifies the theoretical analysis.\n2. Partial participation. Authors claim that SABER allows for partial participation (PP). As far as I understand, SABER with PP is listed as Algorithm 2. However, theoretical analysis is only presented for Algorithm 1, where participation of all clients is necessary, consequently relegating Algorithm 2 to a heuristic approach.\n3. The experiments were conducted with a single local step for all methods. This setup might not be appropriate for Federated Learning (FL) algorithms with local steps, given that the primary aim is to reduce the number of communications by leveraging local updates."
            },
            "questions": {
                "value": "1. Abstract, first bulletpoint on page 3, page 6. I think it should be $O(\\delta \\epsilon^{-2} \\sqrt{M})$ instead of $O(\\delta \\epsilon^{2} \\sqrt{M})$. \n2. I think it is better to replace argmin in algorithm with first-order optimality condition and $\\phi_k(w_{k+1}) \\leq \\phi_k(w_k)$, as according to the proof in non convex case finding the actual argmin is not necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785193825,
        "cdate": 1698785193825,
        "tmdate": 1699636923243,
        "mdate": 1699636923243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TWM3EEjPy5",
        "forum": "jkhVrIllKg",
        "replyto": "jkhVrIllKg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_CbZE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_CbZE"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for improving FL in the presence of second order data heterogeneity. This is achieved by mitigating client drift over the training duration by estimating the global update direction while also regularizing the local objective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper does good over outlining prior art and motivating the problem sufficiently. The tackled domain is of particular importance in Federated Learning as assuming in certain instances first-order heterogeneity can be problematic. The method seems sound and the theoretical bounds provided for both general cases as well as in the case of \u03bc-PL are also appreciated.\n\nThe paper is quite dense but given the content packed I found it easy to read."
            },
            "weaknesses": {
                "value": "There are a few questions that I would like to ask the authors,\n\n- The method is claimed to be stateless by design, but what the authors mean in practice is not clearly defined nor tested in the experiments.\n- Lack of discussion on what happens in the case of node errors, communication drop-outs, and/or stragglers.\n-  Lack of discussion on what happens if clients have different dataset sizes and how this affect the training."
            },
            "questions": {
                "value": "I have a few questions about the manuscript,\n\n- Have the authors tampered with the template? I found that I could not search using my accessibility tools and no text was selectable - is there any particular reason for doing this? I find that people that use specialty equipment might struggle with this...\n- Why the code was not included in the paper submission? I understand that this can be made public after review but I think it is an essential for proper review and it can be attached as supplementary material not disclosed publicly.\n- Clarify what the authors mean by \"stateless by design\" as per weaknesses above.\n- What happens if a client drops during the computation? Is that handled?\n- Does the framework assume equal participation of clients? What happens in the case that there are clients that \"dominate\" the training? Does this affect the end result (perhaps, due to the uniform sampling used...?)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Reviewer_CbZE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808568595,
        "cdate": 1698808568595,
        "tmdate": 1699636922955,
        "mdate": 1699636922955,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JjNEYjjTjJ",
        "forum": "jkhVrIllKg",
        "replyto": "jkhVrIllKg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_MTuL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7609/Reviewer_MTuL"
        ],
        "content": {
            "summary": {
                "value": "The manuscript studies the federated leaning problem over clients with second-order data heterogeneity. The authors propose an algorithm called SABER, which combines FedProx and SCAFFOLD. Theoretically, they derive the communication complexity of SABER for both general non-convex and $\\mu$-PL objective functions. Several experiments are performed to evaluate the proposed algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem studied in this paper is well motivated. \n\nThe authors carefully compare the difference between first-order and second-order data heterogeneity. \n\nThey provide a convergence analysis for SABER with deterministic gradient by constructing a local objective function consisting of a bias correction term and a regularization term."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper seems limited. The local objective function constructed by the author is an intuitive combination of FedProx and SCAFFOLD. The similar idea can be found in Lin et al. (2023)\n\n2. The algorithm design of SABER is not surprising to the reviewer in the sense that the proposed SABER algorithm uses the similar idea of SVRG to deal with data heterogeneity. \n\n3. Technically, the main proof techniques used in the paper are standard in federated learning. The authors make no concrete improvements to existing SOTA results. Furthermore, it seems that there is no significant difference in the techniques used to analyze the objective function that satisfies the \\mu-PL condition and the convex condition, especially for the deterministic gradient scenario.\n\n4. The writing of the paper needs to be improved. In section 3.3, the result for \\mu-PL objectives should be formulated as a formal theorem.\n\nReference:\n\nDachao Lin, Yuze Han, Haishan Ye, and Zhihua Zhang. Stochastic Distributed Optimization under Average Second-order Similarity: Algorithms and Analysis. arXiv preprint arXiv:2304.07504, 2023."
            },
            "questions": {
                "value": "Refer to weeknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7609/Reviewer_MTuL"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7609/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827766678,
        "cdate": 1698827766678,
        "tmdate": 1700728229059,
        "mdate": 1700728229059,
        "license": "CC BY 4.0",
        "version": 2
    }
]