[
    {
        "id": "QhDCSH1ghz",
        "forum": "pXEnurdRAx",
        "replyto": "pXEnurdRAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_UdWx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_UdWx"
        ],
        "content": {
            "summary": {
                "value": "This submission deals with generative learning in the wavelet domain. Learning in the wavelet domain is  challenging due to the sparse and correlated nature of coefficients that make it difficult to denoise Gaussian noise for score based generative models (SGM). This submission proposes a multi-scale GANO in the wavelet domain that uses low frequency information to condition learning the high frequency one. The low frequency LL band is learned using a SGM and then the other bands are learned using GANO conditioned on LL band. The LL band is well conditioned, thus learning SGM is easy. Experiments with FFHQ-cat and CelebA datasets show improvements in terms of FID for smaller number of timesteps and smaller architectures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of learning based on only the LL subband for well-conditioned score learning is interesting and innovative"
            },
            "weaknesses": {
                "value": "This work lacks experiments for real scenarios to test generation. FFHQ-cat and CelebA are toy datasets to make a conclusion about the effectiveness of the method. CelebA dataset is known to have very compressible wavelet representation, with a very narrow distribution. It needs more experiments with more realistic scenarios such as imageNet and more ablations to make any conclusions. Even the CIFAR dataset with more classes has not been tested.\n\nThe contributions compared with the previous wavelet based SGM method (WSGM) seems not significant. The already have shown acceleration due to wavelet compression."
            },
            "questions": {
                "value": "For learning high frequency subbands from the LL subband, GANO is used? The motivation behind operator learning, that deals with functional mapping, is not clear here. Why not simply use a Unet and do regression? Or if you want to learn the conditional probability, why not a simple generative superresolution method such as another diffusion? The conditioning of LL bands is very informative to guide any generative method. Ablations are needed to justify the choice of GANO."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698627260788,
        "cdate": 1698627260788,
        "tmdate": 1699637124039,
        "mdate": 1699637124039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VqUhH72DFX",
        "forum": "pXEnurdRAx",
        "replyto": "pXEnurdRAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_3QFf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_3QFf"
        ],
        "content": {
            "summary": {
                "value": "This mainly theoretical paper attempts to couple wavelet-domain diffusion to its spatial counterpart.\nThe paper proceeds with an examination of the distribution of high-frequency wavelet coefficients on the CelebA-HQ dataset. In a next step, their non-Gaussian nature is established theoretically. With the non-gaussian nature of the data in mind, the authors propose to use a multi-scale generative adversarial neural operator instead of a diffusion model, which sometimes makes Gaussian assumptions.\nFinally, speedups are experimentally observed for the proposed model."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is well written, its research question is a good fit for ICLR.\n- Experiments are most likely reproducible. The code is available online.\n- To the best of my knowledge, the paper's contributions are novel. Especially the examination of the generative adversarial setup in the wavelet domain. After all, Guth et al. studied only the diffusion case.\n- Claims are backed up by extensive material in the supplementary part."
            },
            "weaknesses": {
                "value": "- It would be nice if the experimental results were statistically significant, that is, if multiple seeds had been tried. However, since the paper is theoretical, I don't think this is an important issue.\n- The Gaussian noise assumption is not crucial for working diffusion models [1]. It would have been fair to mention as much. \n\n[1] Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise,  https://arxiv.org/pdf/2208.09392.pdf"
            },
            "questions": {
                "value": "- Why is the proof of section 2.2 in Guth et al. insufficient to establish the duality of spatial and wavelet domain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845576570,
        "cdate": 1698845576570,
        "tmdate": 1699637123927,
        "mdate": 1699637123927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4N7Ws2wZXf",
        "forum": "pXEnurdRAx",
        "replyto": "pXEnurdRAx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_LWzA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8926/Reviewer_LWzA"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for image sampling based on wavelet image decompositions. The proposed method generates images using a two step procedure. First, using a score-based model, the method generates a sample of low-frequency wavelet coefficients. As best I can tell, the second step of the sampling procedure involves generating high frequency wavelet coefficients using a conditional GAN.\n\nThe authors provide some heuristic arguments that GAN architectures are better suited for sampling high frequency coefficients because the coefficients are sparse and highly non-Gaussian, whereas low-frequency wavelet coefficients tend to be more Gaussian and tend to have a covariance matrix with improved conditioning. Finally, the authors show some experiments in which the proposed method has improved FID and sampling time compared to original SGM (Song et al. 2020) and WSGM (Guth et al. 2022b)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed method performs well in the experiment shown in Table 1. The samples generated in Figure 3 are reasonable quality."
            },
            "weaknesses": {
                "value": "This paper is egregiously vague in many aspects, some of which are: explanation of the proposed method, motivation for the proposed method, and experimental results.\n\n- Explanation of the proposed method. There does not appear to be any description of key details of the GAN architecture. What is the structure of the generator? Figures 1 and 7 are insufficient and more details are required. How large are its layers? How do you implement skip connections and attention gates? Why is it a neural operator? Does it involve in any way a hard-coded wavelet transform? In what sense does the generator sample high-frequency coefficients in 'one shot'? In what sense is the generator 'multi-scale'? What is the intended scale of the coefficients output by the generator, and why does it make sense to minimize terms like $(G(x^k_L, z^k) - x^k_H)^2$ in (17) and (18) if $G$ does not depend on $k$? Please define the parameters lambda, nu, and alpha, and mention how they are chosen in experiments. It does not seem possible to replicate the experiments in this work based on the details provided.\n\n- Motivation for the proposed method. The authors claim repeatedly that low-frequency wavelet coefficients are better conditioned and 'more Gaussian,' and that high-frequency wavelet coefficients are sparse. To support the first claim, the authors show in Figure 5 that KL divergence between a unit gaussian $\\mathcal{N}_0$, and a Gaussian with mean and covariance matched to the data $\\mathcal{N}_1$, is decreasing as the scale increases. Why does $\\text{KL}(\\mathcal{N}_0 \\mid \\mathcal{N}_1)$ have anything to do with well-conditioning of covariance and/or Gaussianity of the data distribution? The discussion in A.4 is vague and extremely non-rigorous. The second claim, about sparsity of the high frequency coefficients, is discussed alongside some supporting citations, but it would be helpful to choose a subset of plots in Figure 6 to show in the body to at least demonstrate these claims empirically. Proposition 1 in A.5 only shows that high average sparsity of $x^k_H$ can imply high average sparsity of $x^k_H$ conditional on $x^k_S$, but I don't understand why this is relevant. \n\n- Experimental results. The comparison to Song et al. 2020 is unfair in light of much followup work on tuning score-based samplers (for example: Score SDE, Song et al. 2021). This method should also be compared to existing work on accelerating diffusion sampling with feed-forward nets, such as Consistency Models (Song et al. 2023) and SBGM in Latent Space (Vahdat et al. 2021). Also, in Figure 4, it's unclear whether the GAN upsampling step of the proposed method is counted as a sampling step. If the comparison is made between FIDs of SGM at 16 steps, WSGM at 16 steps, and 16 steps of WSGM for down-sampled images + GAN upsampling, then it is an unfair comparison in which the proposed method will obviously win. Ideally, the authors should compare to other methods that combine score-based sampling and feedforward nets, and they should demonstrate that wavelet-based architectures can augment this approach."
            },
            "questions": {
                "value": "See weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238184204,
        "cdate": 1699238184204,
        "tmdate": 1699637123791,
        "mdate": 1699637123791,
        "license": "CC BY 4.0",
        "version": 2
    }
]