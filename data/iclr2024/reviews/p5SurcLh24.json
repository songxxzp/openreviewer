[
    {
        "id": "H3sIVGSE1k",
        "forum": "p5SurcLh24",
        "replyto": "p5SurcLh24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_9w9H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_9w9H"
        ],
        "content": {
            "summary": {
                "value": "The paper deals with the combination of model-free and model-based approaches in online reinforcement learning. A combination of both approaches is presented and tested on the basis of several benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The idea is original.\n* The results are promising."
            },
            "weaknesses": {
                "value": "* The limitations of the approach remain unclear."
            },
            "questions": {
                "value": "1. Are stochastic MDPs among the benchmarks used?\n2. How often were the experiments repeated in each case?\n3. How were the uncertainties in Table 1 calculated?\n\nFurther comments:\n* If \"i.e.\" and \"e.g.\" are written in italics, then consequently \"et al.\" should also be in italics.\n* Some of the uncertainties in Table 1 are given with too many digits. There should be one or two digits and not four as in \"111.4\". So actually \"-176.9 \u00b1 111.4\" -> \"(-18 \u00b1 11) * 10\" or, because this looks a bit messy in the table format and the 111.4 is the only uncertainty with four digits, \"-176.9 \u00b1 111.4\" -> \"-177 \u00b1 111\".\n* Based on the statement \"The characteristic feature of these approaches is an explicit representation of uncertainty in their estimate of the environmental dynamics. Gal et al. (2016) and Gamboa Higuera et al. (2018) are most similar to our approach\" I would like to refer the authors to [1] and ask them to check how the similarity to [1] is.\n* References contain some unintended lowercase: bayesian, pilco, rl\n\n[1] S. Depeweg et al, Learning and policy search in stochastic dynamical systems with Bayesian neural networks, ICLR 2017."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Reviewer_9w9H"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698529309907,
        "cdate": 1698529309907,
        "tmdate": 1700729713566,
        "mdate": 1700729713566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "32soV1YCvR",
        "forum": "p5SurcLh24",
        "replyto": "p5SurcLh24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_Gnaz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_Gnaz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel reinforcement learning (RL) algorithm, named Unified RL (URL), which introduces a way to switch between policies learned via model-free and model-based methods. To achieve this goal, the authors propose equivalent policy sets (EPS), which is the set of all policies for which there does not exist a provably better policy in terms of Bayesian return (expected return over the learned dynamics model parameter).\nIn practical terms, URL works by learning a policy with a model-free algorithm, and a second policy using model-generated transitions. Then, the returns of both policies are evaluated via Monte Carlo estimation using the learned dynamics model with different dropout masks. The model-based policy is selected only if its return is greater than the return of the model-free policy when evaluated in all models/dropout masks. URL is evaluated in robotics tasks and compared with state-of-the-art model-free and model-based RL algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The idea of exploiting the benefits of model-based and model-free algorithms in combination is a very relevant topic in the RL field.\n* Based on the experimental results (e.g. Figure 3), the proposed method is able to switch between model-based and model-free policies when more appropriate."
            },
            "weaknesses": {
                "value": "* It is not clear how the method handles function approximation errors in the model. For instance, if the learned model generates overestimated rewards, it could be possible that the model-based policy learns to outperform the model-free policy when evaluated in the model, even though the model-free policy would perform better in the real environment. This could lead to the model-based policy being incorrectly selected.\n\n* The practical URL (Algorithm 1) only superficially uses the theoretical idea of EPS (Eq. 4). While EPS is the definition of a set of policies for which we can not identify provably better policies using the model, URL only considers a single model-free policy. Moreover, as mentioned above, it is not clear how can we rely on the Bayesian returns when the model is inaccurate.\n\n* The method introduces significant computational overhead, as it requires running two algorithms (one model-free and one model-based) simultaneously. Furthermore, the introduced overhead does not result in significant performance improvements (see Figure 2)."
            },
            "questions": {
                "value": "Below, I have a few questions and constructive feedback to the authors:\n\nThe EPS is defined as the *smallest* possible set of policies that are not provably Bayes-suboptimal. Why is it the smallest set? Also related to this question, what is the domain of $\\pi\u2019$ in the maximization in Eq. (4)? Is it the set of all possibly existing policies?\n \n\u201cIn environments where either model-based or model-free RL strictly dominates, Unified RL matches or outperforms the better algorithm.\u201d\nThis is not true from the Walker or Cartpole results, as the PPO is the best-performing algorithm in these domains.\n\nWhy do the results of the ALM algorithm have such high variance and strange learning curves? Are these results comparable with the results of the work that introduced ALM?\n\nIn Figure 3 (Left), why does URL perform slightly better than SAC? If the MBRL policy is unable to solve the task because of the distractors, it would be expected that the MFRL is always selected, thus they should have the same performance.\n\nThe authors claim that \u201crather than using the model to approximate a single optimal policy, we maintain a set of policies that may be optimal, which is then refined by model-free RL, thereby avoiding over-reliance on potentially inaccurate models.\u201d It is not clear to me how this is true. The algorithm only maintains one model-free and one model-based policy. How are a set of policies refined by model-free RL?\n\nMinor:\n * In the caption of Figure 3, it should be (Left) and (Right) instead of (Top) and (Bottom)\n * \u201cchallenging continues control tasks\u201d -> continuous"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8201/Reviewer_Gnaz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805290019,
        "cdate": 1698805290019,
        "tmdate": 1700966908128,
        "mdate": 1700966908128,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I3fUPOyxWr",
        "forum": "p5SurcLh24",
        "replyto": "p5SurcLh24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_1YmJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_1YmJ"
        ],
        "content": {
            "summary": {
                "value": "# Summary\nThis work first introduces the concept of EPS (equivalent policy sets), which is used to compare whether a given candidate policy is provably suboptimal compared to a reference policy. Then, an algorithm is proposed that approximates this condition for two policies as input: (i) a candidate policy learned in a model-free soft-actor-critic (SAC) algorithm, and (ii) a reference policy learned in a model-based RL algorithm (`MBRL` baseline). If the condition holds, then the candidate policy is provably sub-optimal to the reference policy and the algorithm decides to use the reference policy for interacting with the environment. Otherwise, the candidate policy is utilized for this purpose.\n\nEmpirical analysis is performed to compare the proposed strategy to prior model-based and model-free methods as well as ablations of the proposed method in the form of an MBRL and SAC baseline which constitute the model-based and model-free algorithms used in the proposed method respectively. The analysis is focused on highlighting cases where the algorithm is able to keep the best performance of its individual components (MBRL and SAC), especially in cases where either component alone is expected to fail."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "# Strengths\nTL; DR: Novel idea, sound theory.\n\n1. The proposed tool -- Equivalent Policy Sets, is a strong contribution in isolation (keeping aside Unified RL). Equation 6, which approximates the asymmetric sub-optimality check of two policies, seems like it can be widely used in several RL algorithms that maintain multiple policies, beyond what is presented in this paper. For example, this could be used to compare policies within an ensemble of model-free policies given a reference model-based policies. The theory behind it is sound and the conditions for equality and the intricate consequences of the approximation of this sub-optimality condition are well explained in this work.\n\n1. Unified RL, that is stated to be the simplest realization of EPS in practice, is a good enough (i.e. minimum sufficient) way of testing the effectiveness of EPS -- it uses just one MBRL policy and MF policy. The practical implication of using the approximation of (Eqn 6) -- that the ML policy will be selected more often in Algorithm 1 -- is acknowledged.\n\n1. The design of Unified RL naturally leads to a strength -- the objective mismatch problem of model-based RL is avoided.\n\n1. The ablation experiment for robustness to both model misalignment and excessively high model-free policy entropy clearly show that Unified RL is performing exactly as intended in it\u2019s design -- it maintains at least the better performance of its components i.e. performance(Unified RL) >= max(performance(MFRL), performance(MBRL)."
            },
            "weaknesses": {
                "value": "# Weaknesses\nTL; DR: Empirical evidence not convincing.\n\n1. The empirical evidence demonstrates that performance(Unified RL) >= max(performance(MFRL), performance(MBRL). However, the performance of Unified RL is shown to be just slightly higher than the max of the two components -- being higher in just the Hopper environment and slightly higher in the Walker environment (out of the 6 environments presented). Otherwise, it seems that equality holds. This is problematic as the choices of environments in this paper are not representative of environments where it is absolutely necessary to shift between MF and MBRL multiple times during training. In most environments, it seems that either MF or MBRL is a clear winner. Since most of RL literature (including this work) \u201ctrains on the test set\u201d  i.e.: hyperparams are tuned on each environment and then reward curves are shown on the same environments -- one may simply choose argmax(performance(MFRL), performance(MBRL) and not have that much worse performance than Unified RL on each environment. This empirical evidence needs to convince us of the importance of using Unified RL vs argmax(performance(MFRL), performance(MBRL).\n\n1. The paper uses the phrase -- \u201cmaintaining a set of candidate policies\u201d multiple times. However, the proposed algorithm (Unified RL) never maintains such a set, as it would be intractable -- instead, it just compares two policies (MF and MBRL policies). This is a misleading phrase as it inflates the capabilities of Unified RL.\n\n1. The definition of EPS also seems to either be wrong or have a crucial typo -- shouldn\u2019t it be defined as the *largest* possible set of policies that are not provable Bayes-optimal (\u2026) instead of *smallest* possible set of all policies?\n\n1. ALM baseline HalfCheetah and Ant results don\u2019t seem to match the cited ALM paper. This seems like a serious issue that may invalidate some of the conclusions drawn."
            },
            "questions": {
                "value": "# Questions and Suggestions\n\n1. Correct me if I am wrong, but given that L-hat(pi-MF, pi-MB, \u2026) > minus-infinity, shouldn\u2019t pi-MB be the selected policy in the if-condition of Algorithm 1 as there is a positive lower bound to the performance difference of pi-MB - pi-MF?\n\n1. Can we see plots with number of steps instead of number of episodes on the X-axis for comparison with other works?\n\n1. What version of halfcheetah and other OpenAI gym environments is used? Is it HalfCheetah-v2?\n\n1. I strongly recommend expanding the types and number of environments for empirical evaluation.\n\n1. Does the SAC baseline use the two modifications mentioned in the SAC used for the proposed method (i.e. layer normalization in Q networks and omission of entropy term for Q-net loss)?\n\n1. I think the future work mentioned in Section 6 seems promising!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853290919,
        "cdate": 1698853290919,
        "tmdate": 1699637017203,
        "mdate": 1699637017203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GwPPzQwdRx",
        "forum": "p5SurcLh24",
        "replyto": "p5SurcLh24",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_jTrP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8201/Reviewer_jTrP"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the combination of model-based RL and model-free RL. The authors propose an approach that makes use of the concept of equivalent policy set (EPS) that, based on a Bayesian formulation, represents the set of policies that are not provably Bayes-suboptimal according to the current data and prior. The algorithmic contribution, Unified RL, switches between model-free and model-based RL according to the value of a variational lower bound which is evaluated from the prior and the model-based and model-free policies. The paper provides experimental validation on the set of Mujoco environments and a validation for testing the robustness to misalignment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of combining model-free and model-based RL is surely relevant in the RL community.\n- The paper introduces the novel concept of equivalent policy set which has a nice interpretation from a Bayesian perspective.\n- The paper is well written and the contributions are clearly outlined."
            },
            "weaknesses": {
                "value": "- [Choice of the function $f$] The expression that is used to evaluate when to switch between model-based and model-free policies is based on a function $f$ (eq. 3) which is not further specified (apart from the fact of being concave and increasing). It is not clear to me why you are allowed to choose $f$ arbitrarily (given that it is concave and increasing). Can the authors elaborate? Furthermore, in the experimental section, which $f$ is used?\n\n- [Choice of $p$ and $q$] These elements represent the prior and the approximate posterior that is used in the algorithm to evaluate the loss function for selecting when to switch. How are they selected? $p$ should be a property of the formalization of the problem in the Bayesian context. In the experimental part, how is it selected? Furthermore, $q$ is the approximate posterior and, I believe, its choice greatly influences the performance of the algorithm. In which class of probability distribution is picked?\n\n**Minor Issues**\n- The plots do not report the number of runs and the meaning of the shaded areas (std, confidence intervals)\n- Multiple citations should be in chronological order."
            },
            "questions": {
                "value": "Please refer to [Weaknesses]."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698941738462,
        "cdate": 1698941738462,
        "tmdate": 1699637017103,
        "mdate": 1699637017103,
        "license": "CC BY 4.0",
        "version": 2
    }
]