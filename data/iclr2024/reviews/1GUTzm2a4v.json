[
    {
        "id": "SlDFTYFS9l",
        "forum": "1GUTzm2a4v",
        "replyto": "1GUTzm2a4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_WqYq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_WqYq"
        ],
        "content": {
            "summary": {
                "value": "This research study bridges the gap between two domains of deep learning: attribution and feature selection. They propose a novel unified theoretical framework. The resulting method, although similar to previous work, uses feature selection in order to increase the robustness of the attribution evaluation. Their result show that the proposed Greedy PIG vastly outperforms some previous methods in terms of Softmax AUC and KL divergence AUC."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "In my opinion, explainability and compression are of paramount importance in deep learning. In this paper, the authors show a limitation of existing methods. As a result, Greedy PIG is specifically designed to mitigate this issue and achieves remarkable results."
            },
            "weaknesses": {
                "value": "I have three concerns with this work as it stands.\n1. The method is designed to perform well when evaluated using the Softmax AUC which is not the most commonly used metric (insertion and deletion scores are). How the Greedy PIG compare with other methods using these metrics?\n2. A recent method IDGI [1] was introduced \n3. Although ConvNets are still popular, the study would strongly benefit from an evaluation on Transformers, e.g. ViT.\n\n[1] Yang, Ruo, Binghui Wang, and Mustafa Bilgic. \"IDGI: A Framework to Eliminate Explanation Noise from Integrated Gradients.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "On top of my previous concerns, I would like to ask if the authors could the authors share their code (at least on an example). I am intrigued by the difference in performance with GIG which in my understanding is very similar to the proposed method"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698131926896,
        "cdate": 1698131926896,
        "tmdate": 1699636970694,
        "mdate": 1699636970694,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3XwjmwjGum",
        "forum": "1GUTzm2a4v",
        "replyto": "1GUTzm2a4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_mYhW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_mYhW"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an improvement over Integrated gradients by advocating to make it adaptive. They do so by recursively taking the top-k attribution features, adding it to the current baseline, and recomputing the path gradients. The authors then show that their attribution method outperforms previous modifications to integrated gradients on several performance AUC metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I like the idea of adaptively choosing the baseline in order to break the redundancies between features involved. However, I think this aspect of the paper has not been properly evaluated by the authors. I expand on this in the weakness section.\n\n2. The proposed modification to integrated gradients outperforms previous methods in literature in AUC curves which show that their method chooses features that are more important for prediction than other attribution methods."
            },
            "weaknesses": {
                "value": "The motivation of this work is not adequately backed up with theory or experiments. Moreover the writing is weak making the paper hard to read. I would expand on this in the following points. \n\n1. The stated motivation for greedy PIG is to make the attributions more robust to feature correlations. However this aspect has never been explicitly evaluated in experiments. Lemma 4.4 is an attempt to theoretically justify why integrated gradients would fail when redundant features are present, however no proof is provided in the paper to evaluate the correctness of the statement. Moreover, it is not clear how greedy PIG solves the issue stated in Lemma 4.4. Clarifying this would further strengthen the motivations of this work.\n\n2. The Proof of Lemma 4.3 is not clear. Why is the hessian bounded by K? What is the non-correlation property of g? What is \\bar{H}. The authors say this is average on a path from w to w_{i}. What is the formulae for computing this average? how is the path computed? what is w_{I}. The details should be clarified to the reader. \n\n3. More generally, it is not clear to me what g is in the paper. Is it the neural network function f as in equation 1? Section 3.3 says this is a continuous extension that allows optimization of equation 3, however equation 3 is never optimized in their greedyPIG algorithm. \n\n4. For the experiments, what is the value of z, chosen for the greedy-PIG algorithm in each instance. An ablation study on the effect of z (the number of top-z features selected in each iteration) on the different metrics would be interesting as it would show the robustness of the method on the choice of z. If one would want to break correlations, is the ideal value z=1? \n\n5. It is not clear what is the Sequential Gradient, the authors refer to in this paper. Is it eq (1) evaluated at one single point instead of a discretization on N points? If yes, how is this point selected? how accurate is this estimation?\n\n5. Please describe what the point game is in more detail. I understand it was proposed in an earlier paper, so I recommend this be added to the appendix. Otherwise it is not clear to the reader at all what is been shown. Is the network (that is explained) trained on a new dataset that includes images arranged in a 3x3 grid, or is it through the same network? If yes does it not affect the performance of the original network which was trained on clean imageS?  The statement \"We generate 2x2 grids of the highest prediction confidence images, and obtain the attribution results for each class\" is unclear. What does highest prediction confidence images mean? How are the attribution results obtained?"
            },
            "questions": {
                "value": "Refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725378551,
        "cdate": 1698725378551,
        "tmdate": 1699636970568,
        "mdate": 1699636970568,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lng58tj2Iz",
        "forum": "1GUTzm2a4v",
        "replyto": "1GUTzm2a4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_yEX4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_yEX4"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of feature attribution as an explicit subset selection problem.  Realizing that the main drawback of the path-integrated gradient (PIG) algorithms is their limited ability to handle feature correlations, the authors propose a natural way to account for correlations by a greedy algorithm, i.e., the correlations between already selected variables with the rest of the unselected variables will be eliminated by the greedy selection strategy. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors connect feature attribution and feature selection with a unified discrete optimization framework based on subset selection.\n2. Experiments on a wide variety of tasks, including image feature attribution, graph compression/explanation, and the post-hoc feature selection on tabular data demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed method is limited.  By simply combining feature attribution and feature selection with a unified discrete optimization framework based on subset selection, the authors introduce limited insight into tackling this problem. Equation 7 is a simple extension of Equation 1.\n2. The proposed Greedy PIG may introduce a sub-optimal problem.  By greedily selecting the top-attribution features computed by integrated gradients in each round, the proposed method cannot guarantee a global optimal solution for the feature attribution problem. Further, if seeking the global optimal solution for the feature attribution problem is not the goal of this submission, it may be better for the authors to demonstrate that a satisfactory solution will be attained by the proposed method.\n3. This paper is not well-written, and more explanation is needed to deeply follow this paper. For example, \"feature attribution, the softmax information curve (SIC) of Kapishnikov et al. (2019) can be recovered from (Eq. 3) by setting G(S) to the softmax output of a target class (see Eq. 4).\" is quite confused."
            },
            "questions": {
                "value": "1.  A typo in the second paragraph of the introduction section: \"on considers an entire dataset. For literature surveys, see (Zhang et al., 2021) for feature attribution and interpretability see and (Li et al., 2017) for feature selection.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review is needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699156778255,
        "cdate": 1699156778255,
        "tmdate": 1699636970456,
        "mdate": 1699636970456,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6I3Pi13maG",
        "forum": "1GUTzm2a4v",
        "replyto": "1GUTzm2a4v",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_dYDX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7910/Reviewer_dYDX"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles feature attribution, which aims to explain model's decision on an input by assigning to each input feature a score showing their contribution. Different from previous work, the paper proposes to formulate it as a subset selection problem (Sec 2.2 and 3.2), i.e. select the optimal set of features that best explain the model's decision. Inspired by Path Integrated Gradients (PIG), the paper relaxes the objective set function to a continuous function on a path in the hypercube. The problem is then solved using Greedy PIG, an application of PIG in multiple rounds which selects a batch of features at a time to add to the optimal set.\n\nThe paper shows good performance compared to PIG-based baselines on feature attribution, GNN compression and feature selection on tabular data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Explainability of deep neural networks is an important topic and the paper tackles an important task toward this goal. Casting feature attribution as subset selection is reasonable. \n\nThe paper rightly points out that the correlation of features could lead to wrong attribution. The proposed Greedy PIG algorithm to address this issue seems to result in better performance than the baselines."
            },
            "weaknesses": {
                "value": "The link between subset selection formulation and Greedy PIG seems very weak. The path going from the formulation to the algorithm should be better clarified. In particular:\n  - Why does Greedy PID maximize the objective function? The paper claims that formulating feature attribution as an optimization problem has advantages. But the proposed algorithm seems to be an extension of PIG and has nothing to do with maximizing the real object function.\n  - Is the continuous objective function a submodular function? The paper seems to lean a lot on the submodularity of set functions to argue for the approximate optimality of Greedy PIG.\n\nThe part of  why Greedy eliminates the effect of feature correlation needs clarification. Is there some mathematical evidence to support claims in paragraph \"Why Greedy captures correlations\"?\n\nThe analysis in Sec 4.2 needs clarification\n  - Why is it good that attributions correlate with marginal gains at S=0? If marginal gains are what we want, why don't we directly use them?\n  - The paper suggests that H_ij reflects the correlation between features i and j. Is ther any justification?\n  - Lemme 4.4 needs a short proof. Also, it considers a very particular form of \"feature redundancy\". Is this kind of feature redundancy common in practice?\n\nIn general, the paper's writing needs major improvements."
            },
            "questions": {
                "value": "How does the performance depend on parameter z in Algorithm 1?\nFunction g in Eq. 7 is a typo? Another function g is mentioned earlier in Sec 3.3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7910/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7910/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7910/Reviewer_dYDX"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7910/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699449672782,
        "cdate": 1699449672782,
        "tmdate": 1699636970353,
        "mdate": 1699636970353,
        "license": "CC BY 4.0",
        "version": 2
    }
]