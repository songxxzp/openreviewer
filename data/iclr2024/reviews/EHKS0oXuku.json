[
    {
        "id": "9RQXQW6Mbx",
        "forum": "EHKS0oXuku",
        "replyto": "EHKS0oXuku",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_uNB9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_uNB9"
        ],
        "content": {
            "summary": {
                "value": "Variational inference (VI) has some issues when applied to Bayesian neural networks (BNNs). For instance, the optimization procedure of VI-BNNs is much harder than standard point estimation. This paper argues that this is because of the limitations of the KL divergence, in particular, due to its non-symmetric and unboundedness properties.\n\nTo alleviate this issue, the authors propose to use the JS divergence instead, which is bounded, symmetric, and generalizes the KL divergence. They view the ELBO objective as a constrained optimization objective and replace the KL in the regularization term with the JS. Theoretical study shows that the proposed JS loss induces a greater penalization (than the KL-based loss) as the variational posterior deviates away from the prior under some circumstances.\n\nFinally, empirical findings show that the JS-based loss yields better generalization performances in both CIFAR-10 and Histopathology datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors provide a solid discussion about VI-BNNs and about the recent advance in replacing the KL with other divergences. I also appreciate an extensive discussion about the JS and its variants (JS-G)."
            },
            "weaknesses": {
                "value": "This paper falls short in several key areas (in no particular order): (i) motivation, (ii) presentation, and (iii) experiments.\n\n**Motivation**\n\nThe authors repeatedly mentioned that the problem with the KL-divergence in VI-BNNs is because of its asymmetry and unboundedness, which leads to instability in optimization. I completely agree that instability is one of the glaring issues of VI-BNNs and it needs to be solved. However, when I read the paper, I don't see the connection between them---why do asymmetry and unboundedness lead to instability. This might be obvious for the authors but I don't think it is for the readers. I would suggest the authors either theoretically or empirically show the connection between them. In that way, the proposed JS-based loss can be much better motivated.\n\n\n**Presentation**\n\nI think the paper in general lacks polish---there are many typos or inconsistent naming (CIFAR vs Cifar, etc). But the main issues are at least two-fold for me. First, The authors derive the JS-based loss from the constrained optimization perspective, i.e., simply replacing the `KL(variational_posterior, prior)` with `JS(variational_posterior, prior)`. I'm sure this is correct since the form of `expected_nll + KL(variational_posterior, prior)` is derived from `KL(variational_posterior, true_posterior)`, i.e. it uses the fact that the KL is used (the KL also appears naturally when deriving the ELBO via the marginal likelihood). Put another way, I'm not sure if the proposed objective is still a valid ELBO. It would be great if the authors could discuss this.\n\nSecond, it is unclear what is the significance of the theoretical results in Sec. 3.4.2. The authors should discuss this and relate those results to practical applications, e.g. using the corollary for analytically picking the hyperparameter $\\alpha$. But then again, this doesn't seem to be the case since in practice it is chosen via hyperparameter tuning.\n\n\n**Experiments**\n\nThe experiments need to be expanded if this paper wants to convince practitioners. Currently, only two datasets and a single network are studied. Moreover, there is only a single baseline being compared, the standard KL-based VI-BNN---this is not a particularly strong baseline. It would be much more convincing if the authors compare practically relevant, strong BNN baselines like Laplace [1] and cyclical SGMCMC [2]. In particular, the accuracy on CIFAR-10 is very low for VI (~35%), whereas Laplace attains similar performance to the MAP-estimated network (for ResNet-18, around 92%+). \n\nMoreover, generalization performance is just one side of the coin when talking about BNNs. Uncertainty quantification is the other side. Thus, the authors need to perform extensive experiments on UQ to make the paper complete.\n\n**References**\n[1] Daxberger et al., NeurIPS 2021\n[2] Zhang et al., ICLR 2020"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6418/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697657717418,
        "cdate": 1697657717418,
        "tmdate": 1699636715334,
        "mdate": 1699636715334,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QuMKCSPvzz",
        "forum": "EHKS0oXuku",
        "replyto": "EHKS0oXuku",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_kqYv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_kqYv"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors have identified shortcomings in the KL divergence-based Variational Inference (VI) and have introduced loss functions based on two variants of the Jensen-Shannon divergence, referred to as JS-G and JS-A. Through both theoretical analysis and empirical experiments, they have substantiated that these newly proposed loss functions represent a more generalized and enhanced approach compared to the previous KL divergence-based VI methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Clarity\n- The paper is well-crafted, making it easily understandable for readers.\n- The theorems and proofs presented are straightforward and have a clear, comprehensible structure.\n\nOriginality and Significance\n- This paper proposed new loss functions based on two variants of the Jensen-Shannon divergence, referred to as JS-G and JS-A which overcome some issues of the KL divergence-based loss function.\n- They empirically validate the proposed loss functions show more improved generalization performance compared to KL divergence-based loss function."
            },
            "weaknesses": {
                "value": "Experiment\n- The experiments conducted are somewhat basic and may not completely confirm the efficacy of the proposed loss functions. It is advisable to include additional experiments involving larger datasets and more extensive models.\n- The model's performance on CIFAR10 and CIFAR100 datasets seems suboptimal, potentially due to the absence of batch normalization. It would be worthwhile to explore the impact of incorporating batch normalization and also consider using the Filter Response Normalization (FRN) layer if issues persist.\n- It is recommended to provide ablation results concerning the parameters $\\lambda$ and $\\alpha\" in the experimental analysis.\n\nReferences\n\n[1] Singh, S. and Krishnan, S. Filter response normalization layer: Eliminating batch dependence in the training of deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11237\u201311246, 2020."
            },
            "questions": {
                "value": "Refer to weakness paragraph."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6418/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698390132558,
        "cdate": 1698390132558,
        "tmdate": 1699636715203,
        "mdate": 1699636715203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g4AjBNmlpT",
        "forum": "EHKS0oXuku",
        "replyto": "EHKS0oXuku",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_BhnF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_BhnF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes variational inference using an extended JS divergence as a solution to the problems with KL-based variational inference for BNNs. It introduces JS-A, an extension of original JS divergence itself, to address issues like divergent problems that arise when using JS divergence directly. Utilizing techniques from constrained optimization in variational inference, the paper proposes a new VI approach using JS-A and applies it numerically to problems of BNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes JS-A, an adaptation of JS divergence specifically tailored for Variational Inference (VI), rather than using JS divergence directly. Then the authors tried to theoretically clarify the properties of JS-A, which adds to its strength.\n\nAdditionally, the paper adopts a constrained optimization formulation instead of directly minimizing JS divergence, making the computation more manageable. This aspect demonstrates a thoughtful approach to addressing the challenges associated with VI."
            },
            "weaknesses": {
                "value": "The major concern is that I could not understand how the proposed objective function Eq.(17) is related to the original objective function Eq.(10). The existing constrained optimization formulation of VI, Eq(14), has the explicit relation to the KL minimization of Eq.(4).\nHowever, I wonder whether any relation holds between Eq.(17) and (10). In the current version of the paper, two formulations seem irrelevant, i.e., Eq. (17) is the prior and posterior, and Eq(10) is the minimization between the true posterior and approximate posterior. Please elaborate on these relationships.\n\nThe next concern is the lack of the comparison between JS-A and JS-G. Although the theoretical properties are summarized in Table 1, I could not understand which is better when used in VI. From the theoretical properties, JS-A seems better than JS-G. However, when looking at numerical experiments, such as Fig. 12, JS-G seems better, especially for large-scale experiments. Could you add a comparison for JS-A and JS-G and discuss their differences as VI ?"
            },
            "questions": {
                "value": "In Fig. 1, although the results of both JS-A and JS-G are presented, they are never discussed. What difference between JS-A and JS-G can we interpret from the figure ? Also, the author discussed only the mean. Please discuss the results of the variance in Fig. 1\n\nAlso, in Fig 1 (b) and (d), what is the difference between the red and black curves ?\n\nIn table 4, the proposed JS-A seems slower than the other methods in terms of wall clock time. Why ?\n\nAccording to the explanation of Eq(18), the proposed method has both mode- and mean-seeking properties. I think this resembles alpha divergence VI. Could you show the comparison of alpha-divergence VI numerically ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6418/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698710197763,
        "cdate": 1698710197763,
        "tmdate": 1699636715078,
        "mdate": 1699636715078,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i4wNPG5bXE",
        "forum": "EHKS0oXuku",
        "replyto": "EHKS0oXuku",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_AJUX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_AJUX"
        ],
        "content": {
            "summary": {
                "value": "- This paper proposed a new objective function for variational inference (VI) in Bayesian Neural Networks (BNN). This objective function is built upon a modified version of the generalized Jensen-Shannon (JS) divergence, denoted as the JS-A divergence. Unlike the Kullback-Leibler (KL) divergence, the JS-A divergence has an upper bound and exhibits symmetry when the weight of the arithmetic mean, denoted as $\\alpha$, is set to $0.5$.\n- The authors theoretically confirmed the effectiveness of their proposed objective by comparing the regularization terms of their objective with those of the KL-based VI objective.\n- The authors validated the predictive accuracy of their method using various benchmark datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper tries to expand the choices for objective functions in VI for BNN. It places particular emphasis on exploring the JS divergence family, which has several intriguing properties.\n- The authors confirmed the performance of their proposed method through experiments carried out on BNN with highly sophisticated model architectures, including Convolutional Neural Networks (CNN) and ResNet-18."
            },
            "weaknesses": {
                "value": "I would like to express my sincere respect for all the efforts the authors have invested in this paper.\nUnfortunately, however, I cannot strongly recommend this paper as an ICLR 2024 accepted paper for the following reasons: (1) concerns regarding the validity of the rationale for the problem statement, (2) the validity of changing the divergence to ensure boundedness, and (3) the lack of sufficient experimental data to justify the contributions made.\nThe details are provided below. If there are any misunderstandings, I apologize, and I would appreciate it if you could explain them to me.\n\n## Concerns regarding the validity of the rationale for the problem statement\n- In this paper, the authors adopted the problem that the KL-based VI objective used in BNN, such as [Blundell et al., 2015], is unbounded and lacks symmetry, which negatively impacts optimization stability and generalization performance. As the basis for this assertion, they reference [Hensman et al. (2014); Dieng et al. (2017); Deasy et al. (2020)]. However, in my opinion, these references do not explicitly suggest that the unbounded and non-symmetric nature of the objective affects generalization performance. For instance, [Dieng et al. (2017)] identify the decrease in generalization performance due to **the underfitting of posterior variance** by KL-based VI and demonstrate that minimizing the upper bound based on the chi-squared distribution (called CUBO) can resolve this issue, and it is not stated that this underfit of posterior variance is caused by the unbounded and asymmetric properties of KL objective. This raises significant concerns regarding the validity of the problem setting adopted in this paper unless it is theoretically and/or empirically demonstrated that the unbounded and non-symmetric objectives affect the stability of optimization and generalization performance of VI. There still be room for reconsidering whether the issues of divergence boundedness and asymmetry should be approached in the context of BNN+VI rather than in the context of divergence measures for distributions.\n- The authors mentioned that they have proposed two new objective functions under the problem setting described above. However, according to Table 1, one of them (JS-G) does not eliminate boundedness. This point is not addressed in either the contributions section or the limitations section.\n\n## The validity of changing the divergence to ensure boundedness\n- If we utilize several practical approaches proposed in the context of PAC-Bayes, such as those suggested by [DR18] and [P21] (bounded cross entropy) or appropriately initializing model weights and prior, it becomes possible to bound the KL-based objective in the case of the Gaussian distribution that is the focus of this paper. Therefore, it is necessary to carefully consider whether the approach to resolving the issue of unbounded KL-based objectives by altering the upper-bounded divergence is valid. However, the merits of the proposed method using JS divergence have not been discussed in comparison to these approaches, thus the validity of the approach remains unconfirmed.\n\n## The lack of sufficient experimental data to justify the contributions made.\n- If my understanding is correct, Theorems 2 and the corollary demonstrate that the proposed objective function becomes tight. However, as numerical experiments to verify this have not been conducted, the validity of the theoretical analysis remains unconfirmed. In this regard, plotting the progression of ELBO and the proposed objective values for each iteration would quickly provide confirmation.\n- A comparison regarding the regularization capacity is briefly presented in Figure 1; however, this verification has not been conducted in experiments on the other benchmark datasets. To justify the contributions of this paper based on the relationship between regularization capacity and generalization performance, it is necessary to confirm, for example, the transition of KL and JS divergences during the learning process and the correlation between the final obtained divergence values and predictive performance. Currently, there are not enough experimental results provided to fully support the effectiveness of the proposed method.\n- The authors solely compare the performance of their proposed method and existing methods based on predictive accuracy. However, since BNNs are probabilistic models, it is crucial to consider how well they capture the uncertainty in predictions, which is an important performance metric in many BNN studies, as confirmed experimentally by researchers such as [I21, P21]. To claim state-of-the-art performance, a more comprehensive performance evaluation on the test data, including metrics like test-ELBO, test-NLL, and Expected Calibration Error (ECE), is necessary.\n- There is no information about how predictions are being made. Is this an evaluation based on deterministic predictions using the mean parameters of the variational distribution? Or is it a probabilistic prediction through Bayesian model averaging (BMA) using samples from the variational distribution? Since BNNs are probabilistic models, performance evaluation through BMA is essentially necessary.\n- I have some concerns regarding the hyperparameter setting, particularly $\\lambda$, in the existing KL-based VI objective for BNNs ([Blundell et al., 2015]). This aspect is also related to what is referred to as the *posterior temperature* [A21]. In many contexts, such as [Blundell et al., 2015] and [P21], $\\lambda$ is commonly set as $\\lambda = \\frac{1}{n}$ using the training data size $n$ to balance the objective. However, in this paper, $\\lambda$ is set to $\\lambda=1$. This setting can potentially lead to an explosively large KL divergence in BNNs, which may adversely impact the final performance. Consequently, this experimental setup is considered unfavorable when compared to existing methods, raising doubts about the validity of the conclusion that the proposed method outperforms others.\n- The initial values for network parameters are taken from pre-trained neural networks on ImageNet. However, in actual experiments, the authors use datasets such as CIFAR-10/100 and they do not employ ImageNet. This might seem somewhat unusual, as the reasons for this decision are not clearly articulated.\n\n## MISC\n- Due to factors like typos, the readability has been compromised. Please make an effort to refine the overall presentation. Here are a few examples of corrections.\n- Sec.1, 2th paragraph: What does it mean \"the two most commonly used techniques to approximate them (i.e., Bayesian inference) are the Variational Inference (VI)?\"\n- Sec.1, 2th paragraph: Citation for VI and MCMC could be required, e.g., [JGJS99]\n- Sec.3.1, the first sentence: The Generalized JS... --> The generalized JS...\n- Sec.4.1, the first sentence: ...on two Data sets... --> ... on two data sets...\n\n## Citation\n(Note: I am not the author of the following papers)\n\n[DR18]: Gintare Karolina Dziugaite and Daniel M. Roy. Data-dependent PAC-Bayes priors via differential privacy. In Advances in Neural Information Processing Systems, pages 8440\u20138450, 2018.\n\n[P21]: Mar\u00eda P\u00e9rez-Ortiz, Omar Rivasplata, John Shawe-Taylor, and Csaba Szepesv\u00e1ri. Tighter Risk Certificates for Neural Networks. Journal of Machine Learning Research, 22(227):1\u221240, 2021.\n\n[I21]: Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R\u00e4tsch, and Mohammad Emtiyaz Khan. Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning. In ICML2021\n\n[A21]: Laurence Aitchison. A statistical theory of cold posteriors in deep neural networks. In ICLR2021.\n\n[JGJS99]: M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183\u2013233, 1999."
            },
            "questions": {
                "value": "In connection with the weaknesses mentioned above, I would like to pose several questions related to the concerns raised.\nI would appreciate your responses.\n\n- Is there any research providing a detailed discussion on how the non-unboundedness and non-symmetry of KL-based methods can affect the stability and generalization performance of optimization? If not, can you provide an explanation or theoretical and empirical evidence to support this fact?\n- In terms of boundedness, it's possible to keep KL divergence bounded without using JS divergence by employing clever techniques such as setting appropriate initial values for the weights of priors and models or using techniques like those found in [DR18; P21] to bound cross-entropy. What sets the proposed method apart in this regard?\n- How does the objective value evolve during optimization? Does it align with the theoretical results, with that of the proposed method tending to be smaller?\n- What are the results for ELBO, NLL, and ECE on the test dataset?\n- In the experiments, are you evaluating deterministic predictions using the mean parameters of the variational distribution, or are you using BMA through sampling from the variational distribution? In the context of BNNs, performance evaluation with BMA is necessary.\n- Why did you not adopt the setting $\\lambda=\\frac{1}{n}$ as typically handled in Bayes-by-Backprop [Blundell et al., 2016]?\n- It seems that hyperparameter $\\alpha$ is being optimized using Hyperopt. How sensitive is the model's performance to this setting? If sensitivity could indicate instability, it would be one of the limitations.\n- Why did you choose to initialize the parameters of the BNN with a pre-trained neural network on ImageNet, even though experiments were not conducted on this dataset?\n\n\n================ AFTER REBUTTAL & DISCUSSION ================\n\nDue to the timing of receiving the rebuttal just before the deadline, I have not had adequate time to thoroughly review the revised manuscript. In my view, several issues, including those raised by both myself and other reviewers\u2014such as the validity, motivation, and properties of employing loss functions based on bounded divergence, as well as appropriate experimental settings\u2014still demand careful consideration and discussion.\nTherefore, I keep my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I believe that this work does not raise any ethical concerns because it is a methodological study focused on VI for BNNs."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6418/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6418/Reviewer_AJUX",
                    "ICLR.cc/2024/Conference/Submission6418/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6418/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763678158,
        "cdate": 1698763678158,
        "tmdate": 1700726159440,
        "mdate": 1700726159440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z7FZBys2wV",
        "forum": "EHKS0oXuku",
        "replyto": "EHKS0oXuku",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_PkhQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6418/Reviewer_PkhQ"
        ],
        "content": {
            "summary": {
                "value": "The authors propose novel VI learning methods using variants of JS divergence instead of the usual KL divergence. The authors demonstrate that the learning can be performed using gradient-based and MCMC-based optimization methods.\nThe proposed methods seems to provide a useful regularization when training a BNN for the classification tasks in the paper.\nThe authors demonstrate the advantage of the method for noisy and biased data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors derive a a regularization term and justify it using statistical considerations, which I like.\n* The final loss is rather clean and intuitive."
            },
            "weaknesses": {
                "value": "* The loss relies on 2 hyper-parameters (lambda, alpha) which might make the practical use of the method problematic (e.g., in cases where hyper-parameters search is too expensive).\n* It is unclear if simpler regularization methods (e.g., dropouts) might yield similar improvement.\n* Experimentation is limited to two simple datasets.\n* Comparison to other simpler regularization methods (e.g., dropout) is missing."
            },
            "questions": {
                "value": "Below are comments and questions.\n\nComments:\n* In the introduction: The number of parameters has nothing to do with the ability to provide a robust measure of uncertainty. The model choice allows such ability.\n* In the introduction: The variational distribution is learned by minimizing its dissimilarity with respect to the true posterior  - VI does not minimize this quantity since the true posterior is unknown. It can be derived starting from the KL divergence between the approximate and true posterior though.\n* Derivation of Eq. 5 from Eq. 4 is awkward and the justification for dropping p(D) is not clear (being a constant that multiplies the approximate posterior). Typically derivation entails the use of Jensen\u2019s inequality which also facilitate the relation between Eq. 5 and Eq. 4.\n\nQuestions:\n* Near Eq. 2: G\u03b1(x, y) = x1\u2212\u03b1y\u03b1  - G_alpha is an unnormalized distribution in the general case, is that true?\n* Near Eq.3: JS-G(p||q)|\u03b1 = JS-G(p||q)|1\u2212\u03b1  type, should it be non equality?\n* Figure 2: it is unclear wether other regularization techniques (like dropouts) could narrow down the gap for VI with KL divergence, without the need for the forward KL regularization term in the loss. Also, 2b looks like overfitting for the VI with KL divergence. Did you try other regularization methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6418/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698967327954,
        "cdate": 1698967327954,
        "tmdate": 1699636714852,
        "mdate": 1699636714852,
        "license": "CC BY 4.0",
        "version": 2
    }
]