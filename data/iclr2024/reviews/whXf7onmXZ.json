[
    {
        "id": "l5Df7OeHji",
        "forum": "whXf7onmXZ",
        "replyto": "whXf7onmXZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_pAbZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_pAbZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework for fine-tuning vision transformers over a peer-to-peer network topology. The privacy-aware distributed fine-tuning method proposed in this work includes weight-mixing and gradient-sharing operations. It is applied to three vision transformer variants and evaluated on four downstream image classification datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation to train vision transformers distributedly is reasonable.\n2. The proposed method is applicable to different vision transformers."
            },
            "weaknesses": {
                "value": "1. The proposed method seems to be just an application of the GT-DGD method to vision transformers. In the main text, it mentions newly proposed weight-mixing and gradient-sharing operations. However, I do not see the definition of such new operations. The novelty of this work is limited.\n2. The presentation and organization of this paper is poor. For instance, it includes too many descriptions of vision transformers in the related work, which is not closely related to the distributed training method proposed in this work. \n3. The experiments are weak. Only comparisons with local fine-tuning are shown."
            },
            "questions": {
                "value": "Could the author explain more about the difference between Decentralized Federated Learning and this work? Only Centralized Federated Learning is discussed in this work. \n\n[1] Decentralized federated learning: Fundamentals, state of the art, frameworks, trends, and challenges. Beltr\u00e1n, Enrique Tom\u00e1s Mart\u00ednez, et al. 2023.\n[2] Decentralized Federated Learning: A Survey and Perspective. Liangqi Yuan, et al. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698312841775,
        "cdate": 1698312841775,
        "tmdate": 1699636758056,
        "mdate": 1699636758056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RlO4YTXlsT",
        "forum": "whXf7onmXZ",
        "replyto": "whXf7onmXZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_n2Zn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_n2Zn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a distributed training framework called P2P-FT for fintuning vision transformers under the setting that data is geographically distributed over a network of computational nodes and those nodes cannot share their personal training data due to privacy. The framework uses an efficient weight-mixing and gradient-sharing strategy to achieve good performance while being privacy-preserving . The authors demonstrate empirical results over 3 models (i.e. ViT, DeiT and Swin) and 4 datasets (i.e. Pets, Flowers, CIFAR-10 and CIFAR-100), which is superior than fintuning with only local data on each node."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper considers an important and interesting problem, i.e. distributed finetuning of vision transformers under privacy-preserving setting."
            },
            "weaknesses": {
                "value": "I believe this paper is more about distributed systems, differential privacy, and federated learning. However, I am not an expert in these areas. Therefore, I am not sure about the novelty of this paper, and I did not review the methodology part carefully. My major concerns are listed below.\n\n1. The paper does not explain why the proposed method is specific to **fine-tuning** or **vision transformers**. Distributed optimization of deep neural networks under the privacy-preserving setting is a general problem, **neither for specific stages (e.g. pretraining or finetuning) nor architectures (e.g. transformers, RNNs, or CNNs)**. Indeed, and the proposed strategy (Sec. 2.3 on Page 6, and Algorithm 1 on Page 7) has nothing about the two aspects.\n\n2. The paper does not clarify what the baseline setting (i.e., Local-FT) is.  Based on the context in Sec. 3.2 and Sec 3.3 (\"...It can be observed that fine-tuning Local-FT on local data ...\"), it uses only a local shard of the dataset on each node without inter-node communication. However, in this way, a crucial argument in the abstract (i.e. \"... enables distributed models to achieve similar performance results as achieved on a single computational device **with access to the entire training dataset**.\") is not justified or maybe misclaimed.\n\n3. The paper does not compare with existing works on distributed systems, differential privacy, or federated learning. It only uses one weak baseline (i.e., Local-FT) in the experiments. Why are these works not applicable or competitive for distributed fine-tuning vision transformers?"
            },
            "questions": {
                "value": "See weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662752530,
        "cdate": 1698662752530,
        "tmdate": 1699636757911,
        "mdate": 1699636757911,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m0Xp17tGQJ",
        "forum": "whXf7onmXZ",
        "replyto": "whXf7onmXZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_WLob"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_WLob"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a peer-to-peer distributed fine-tuning framework for vision transformers. It leverages the neighboring nodes information to avoid a single point of failure in the hierarchical distributed networks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to read \n2. It also provides some experimental results on fine-tuning the ViT, DeiT, and Swin on four classification datasets including CIFAR-10 and CIFAR-100."
            },
            "weaknesses": {
                "value": "1. Peer-to-peer distributed training and gradient-tracking methodology are not new. And it's difficult to spot the novelty of the proposed approach. \n2. The results in Table 2 to Table 4 are far from convincing. Besides Local-FT, hierarchical distributed training should be another baseline to be included. \n3. The experiments cannot support the claim, \"the proposed method performs effectively in heterogeneous data settings and eliminates the bias caused by the non-uniform data distributions present across different computational nodes\" since the training and test data for each node are from the same dataset."
            },
            "questions": {
                "value": "1. How would the performances from the proposed approach compare to the hierarchical distributed training topology?\n2. Can this approach generalize to the heterogenous data distribution across training nodes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814617734,
        "cdate": 1698814617734,
        "tmdate": 1699636757781,
        "mdate": 1699636757781,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JBhsl7xhvQ",
        "forum": "whXf7onmXZ",
        "replyto": "whXf7onmXZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_NQ6C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6636/Reviewer_NQ6C"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a privacy-aware distributed training framework for fine-tuning the vision transformers.\n\nThe proposed method P2P-FT uses weight-mixing and gradient-sharing strategies to eliminate bias and achieve optimal results, even when handling unseen data from classes the node was never trained on.\n\nThen this paper illustrates the performance of P2P-FT for fine-tuning distributed ViT, DeiT, and Swin transformer models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed framework can be generalized to any vision transformer with a similar structure. We provide numerical experiments on ViT, DeiT, and Swin transformer models.\n\nThis paper analyzes attention maps generated by P2P-FT and compares them with the maps generated by locally fine-tuned models and models fine-tuned on a single server with access to all data.\n\nThe proposed method performs effectively in heterogeneous data settings and eliminates the bias caused by the non-uniform data distributions present across different computational nodes."
            },
            "weaknesses": {
                "value": "Lacking comparison to other methods with similar settings."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6636/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699385691063,
        "cdate": 1699385691063,
        "tmdate": 1699636757675,
        "mdate": 1699636757675,
        "license": "CC BY 4.0",
        "version": 2
    }
]