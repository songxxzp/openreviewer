[
    {
        "id": "1jG3VYmkW3",
        "forum": "5EniAcsO7f",
        "replyto": "5EniAcsO7f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission332/Reviewer_Hu6J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission332/Reviewer_Hu6J"
        ],
        "content": {
            "summary": {
                "value": "This paper tailors the training of a retrieval model to the challenging task of long-term visual localization. The paper uses language-based data augmentation to improve the robustness of retrieval models to adversarial daytime, weather, and seasonal variations, and further introduces geometry-based strategies for filtering and sampling from a pool of synthetic images. Experimental results show that the proposed method achieves significant improvements over the state-of-the-art on six common benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It makes sense to me to start with state-of-the-art techniques for landmark retrieval and gradually adapt them to intuitions about visual localization. The paper uses language-based data augmentation to improve the robustness of retrieval models to adversarial daytime, weather, and seasonal changes.\n2. The paper is well written and structured\n3. The paper performs nice ablation experiments on all different combinations of pre-trained targets and dataset combinations.\n4. The paper provides a detailed explanation and description of related work."
            },
            "weaknesses": {
                "value": "1. Relative to HOW (Tolias et al., 2020), InstructPix2Pix (Brooks et al., 2023), LightGlue (Lindenberger et al., 2023), the contributions appear to be incremental, such as two contributions training with synthetic variants, and synthetic variant generation and geometric consistency.\n2. To handle large environments, a trade-off must be made between accuracy and computation time. Then, is the approach utilized in practical applications?"
            },
            "questions": {
                "value": "1. What is the difference between the loss function of Ret4Loc-HOW-Synth (Eq. (2)) and Eq. (1)? Does Eq. (2) just use the extended tuple set?\n2. How does the training time gets impacted with the newly introduced loss?\n3. Since the authors discussed synthesizing a training set from generated text to image models, did they also compare on a visual classification task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Reviewer_Hu6J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698216014955,
        "cdate": 1698216014955,
        "tmdate": 1699635960185,
        "mdate": 1699635960185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BD7FAh9cxr",
        "forum": "5EniAcsO7f",
        "replyto": "5EniAcsO7f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission332/Reviewer_dtiA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission332/Reviewer_dtiA"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to improve the retrieval accuracy to enhance the localization performance under challenging conditions such as illumination, weather and season variations between the query and reference images. \n\nTo deal with these challenges, the authors propose to expand the training data by generating a large number of synthetic images with the aforementioned variations via generative text-to-image models. Additionally, a geometry-based method is adopted to filter and sample training images.\n\nThe model trained on expanded training data gives better performance than previous methods on multiple public datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tMotivation. A map used for localization is usually built on images captured at very short the time interval without large appearance changes. However, the query images could be recorded at different times with huge illumination, weather, and season differences to reference images in the map, which makes long-term localization challenging. \nIn the paper, the authors propose to bridge the gap by generating training data with these variations. The idea is straightforward but very useful especially for the retrieval task.\n\n2.\tGeometric constraints. I am more interested in this part. One reason is as mentioned in the paper that this allows synthetic images to preserve location-specific information which cannot be guaranteed by generative models. One more important reason is that localization using 3D map has two steps: coarse localization by retrieval and fine localization with PnP + RANSAC from 2D-3D matches. Due to appearance changes, the best retrieved images may not give the most accurate poses as the final pose is also influenced by how matches can be found between the query and retrieved images. It seems like this point is not discussed in the paper.\n\n3.\tExtensive experiments. The proposed method is evaluated on multiple public datasets including RobotCar-Seasons, Aachen Day-Night, ECMU, Gangnam Station B2, Baidu Mall and Tokyo 24/7. Results demonstrate that regular augmentations are the most effective techniques (Ret4Loc-How vs. How) and the proposed training with synthetic data (Ret4Loc-How-synthetic) and geometric constraints (Ret4Loc-How-synth+ and Ret4Loc-How-synth++) also improve the performance."
            },
            "weaknesses": {
                "value": "The proposed method is simple and easy to follow. My major concerns come from the evaluation.\n\n1.\tResults on RobotCar-Seasons dataset. The textural prompts are from tags in RobotCar-Seasons dataset, but in Table 1, models + synthetic data + geometric constraints, denoted as Ret4Loc-How-synth+ and Ret4LocHow-synth++ do not give obvious improvements to Ret4Loc-How.\n\n2.\tIn Table 2, Ret4Loc-HOW-Synth++ achieves better results than previous single-state methods but gives worse accuracy than approaches with reranking. Since in the paper the authors mention that reranking can also be applied to the proposed methods, why not provide results of proposed methods with reranking? In theory, models such as Ret4Loc-HOW-Synth++ plus reranking should work better than other methods with re-ranking.\n\n3.\tComparison with NetVLAD and GEM. As far as I know, NetVLAD and GEM are the most popular methods used for retrieval on ECMU, Aachen and RobotCar-Seasons datasets. It would be better to include results of these methods. By the way, although ECMU contains images with diverse seasonal changes, it provides sequential images, which makes retrieval and localization easy. Therefore, results on Aachen and RobotCar-Seasons datasets are more convincing.\n\n4.\tResults of localization with 3D map. Fig.10 in appendix shows results with 3D map. However, according to the website (https://www.visuallocalization.net/benchmark/), some more recent methods such as SP+SG, sfd2_4000kpts_netvlad50, KAPTURE-R2D2-APGeM, imp_4kpoints_netvlad50, SP+LightGlue give more much better accuracy with only NetVLAD for reference search. As these methods give the current SOTA results, the proposed method in the paper should be compared with these methods. I can understand that these methods may use different local features and global features to R2D2 and HoW. But it is possible to replace the global feature with Ret4Loc-synth++ and report the results.\n\n5.\tIt is not very clear to me in Fig. 9, the authors show matches given by LightGlue not R2D2, but R2D2 is the local feature used for finding 2D-2D matches in the paper. I also want to know is R2D2 used for geometric constraints?  \n\n6.\tIn Sec.5, related works of visual localization under challenging conditions are discussed. Image augmentation with style transfer such Day-Night transfer is only one way to handle appearance changes. Some other solutions like semantic localization and semantic features (e.g., Visual Localization Using Sparse Semantic 3D Map, Sfd2). Related papers can be found from https://www.visuallocalization.net/benchmark/) are also solutions to challenging conditions as semantics are more robust to appearance changes. Discussions on these works should also be included if this work focuses on improve visual localization under challenging conditions."
            },
            "questions": {
                "value": "Please see Weaknesses for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission332/Reviewer_dtiA"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634041281,
        "cdate": 1698634041281,
        "tmdate": 1700684026312,
        "mdate": 1700684026312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TG0MFSJOYS",
        "forum": "5EniAcsO7f",
        "replyto": "5EniAcsO7f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission332/Reviewer_oYk5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission332/Reviewer_oYk5"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author proposes a new method named for Long-term Visual localization. The author first builds a strong baseline, i.e., a baseline with strong data augmentations for visual localization. Then, the author proposed to use the conditional diffusion model to generate synthetic data, then the author proposed some strategies to train with joint real data and synthetic data. The proposed methods achieve performance gain over previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "In general, I'm satisfied with the paper. The proposed method is simple and effective. Without bells and whistles, the proposed method achieves state-of-the-art performances on major benchmark datasets."
            },
            "weaknesses": {
                "value": "However, I still have some concerns about the paper:\n\n1. Though the proposed method is simple, I think the author could provide more detailed comparisons of the method, like the number of training images, the generating time, etc. The proposed method utilizes a conditional diffusion model to generate the augmented dataset rather than the previously manual data augmentations. Thus it is noticeable and essential to bring the cost (training and memory cost), the training image numbers comparisons, and the training time cost for the model.\n\n2. Though the InstructPix2Pix model is a natural choice for the paper. There are many choices with the model, e.g., Stable Diffusion with ControlNet, etc., the author could provide more extra experiments with different methods (even though ablations on a small dataset) will make the paper more robust. Also, the proposed filtering metric should be discussed over different generation settings."
            },
            "questions": {
                "value": "Please mainly see the weaknesses section for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission332/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687197676,
        "cdate": 1698687197676,
        "tmdate": 1699635960037,
        "mdate": 1699635960037,
        "license": "CC BY 4.0",
        "version": 2
    }
]