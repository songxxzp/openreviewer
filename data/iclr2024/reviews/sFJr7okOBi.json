[
    {
        "id": "zYdinG17CQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_kJGB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_kJGB"
        ],
        "forum": "sFJr7okOBi",
        "replyto": "sFJr7okOBi",
        "content": {
            "summary": {
                "value": "This paper trains a P(protein sequence | metadata) model, where metadata encompasses both natural language descriptions of target attributes of the protein and some control tags for structural features based on clustering structures of natural proteins. There are some interesting modeling ideas, such as fine-tuning GPT-2 and using an RL objective to reward sequences with low Rosetta energy. Samples from the model are evaluated using various sanity checks using protein structure prediction models, etc."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper draws on a number of modeling techniques that are popular in the modern toolbox: RLAIF, fine-tuning foundation models, using protein structure prediction tools to provide eval metrics that are cheaper than wet-lab experiments."
            },
            "weaknesses": {
                "value": "The paper's title/abstract/intro/conclusion have lots of language about the promise of a  'conversational' natural language interface for designing proteins. However, the paper does not explore such text descriptions. It just uses a simple text rendering for converting protein database entries obeying a certain schema into text. For example, \"Provides a protein that contains {domain description}, belongs to {family}, {ESM class} and {ONTO class}.<p>{protein sequence}\" (Fig 1). There are significant resources available for true natural language descriptions of proteins. For example, Uniprot entires have one-line name fields and also longer description fields. Further, there are lookup tables available that map GO terms, EC numbers, Pfam families, etc to free text descriptions.\n\nSimilarly, the paper seems to over-state the novelty of structured-guided design with language models. The paper says \"\u2026none of them enables the sequence generation given target structures due to the lack of structural constraints.\" This ignores the significant body of work using RFDiffusion+ProteinMPNN. Further, the paper's claim that it is doing structure-guided design is quite weak: they take embeddings from an ESMFold model (which presumably encode some structure information), map them down to 2 (!!) dimensions, and then cluster these. Conditioning on a cluster id is the only way that structural information is provided. The confirmation that the generated sequences have desired structures in Table 2 is quite simplistic and anecdotal.\n\nI found the RLAIF setup quite confusing. How does it make sense to use Rosetta energy as an absolute reward function? Doesn't this need to be relative to proteins having a similar fold, similar length, etc?\n\nThe paper fine-tunes GPT-2 (which was not pretrained on protein sequences) on only 1M examples of proteins. It's unclear why this generative model was used. Why not train something from scratch, or why not train on more proteins? No ablations about the impact of using GPT-2 pretraining are provided."
            },
            "questions": {
                "value": "I found it very surprising that no recent papers from the Baker lab were cited (RFDiffusion, ProteinMPNN, etc) were cited. These are really important contributions to the field and highly related to your paper. Can you please comment on these?\n\nI am extremely confused about why you did k-means on the 2-dimensional UMap representations. Can you provide more background about why this approach is more 'intuitive and reliable'? \n\nI don't understand how the rosetta energy function was used as a reward, since the energy needs to somehow be normalized by the energy of ground truth proteins with the desired attributes. You say, \"Generally, protein structures with lower scores are more likely to be closer to the native structure.\" What is 'native structure' and how is it used?\n\nThe rewards in eqs (9) and (10) have an optimum when the model just generates cluster centers, which will severely hurt diversity. When presenting your various eval metrics, I'm curious what would have happened if you had considered a simple baseline approach that just memorized a few exemplars.\n\nI don't understand the overall evaluation setup. What does 'We randomly generate 1000 protein sequences from these models'. What metadata did you condition on? Was it 1000 different sets of metadata? How do you make this comparison fair when using models like ESM that don't have the ability to condition on metadata?\n\nThe \"Protein credibility prediction\" paragraph should mention that progen also confirms  wet-lab experiments.\n\nThe citation format is incorrect. It appears that there are many places where you should have been using natbib \\citet{}."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697465920357,
        "cdate": 1697465920357,
        "tmdate": 1699636049615,
        "mdate": 1699636049615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lPOO1Fabt8",
        "forum": "sFJr7okOBi",
        "replyto": "sFJr7okOBi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_PWDH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_PWDH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new LLMs-based framework \u201cNL2ProGPT\u201d for macromolecular protein sequence generation that bridges the domain gap between natural and protein languages. The authors train a reward model to align the protein laguage model with the Rosetta energy function, following an RLAIF fashion, and empirically verify the effectiveness of NL2ProGPT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors have provided detailed explanations of their proposed methods and presented promising results."
            },
            "weaknesses": {
                "value": "The authors claim that:\n\n\u201cmost existing methods mainly utilize protein sequential or structural information to model the intrinsic properties of protein, lacking the kind of controllable generation in a conversational way like LLMs.\u201d \n\nIt is unclear what advantages can be brought by \u201cgeneration in a conversational way.\u201d"
            },
            "questions": {
                "value": "Misc: \n\nThe citations should be enclosed by parentheses, such as using the\u201c\\citep{}\u201d command instead of \u201c\\cite{}\u201d.\n\nTypo in Table 1: \u201cOntoProtien\u201d, \u201cNL2ProGTP\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721668655,
        "cdate": 1698721668655,
        "tmdate": 1699636049543,
        "mdate": 1699636049543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pOgyYuXMYf",
        "forum": "sFJr7okOBi",
        "replyto": "sFJr7okOBi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_se98"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_se98"
        ],
        "content": {
            "summary": {
                "value": "This work studies the problem of protein design with large language models (LLMs), where the input to their model is a natural description of protein features that contain both functional and structural information via preprocessing with existing MSA tools and pre-trained protein language model (e.g., ESM2). The framework \u2014 NL2ProGPT also consists of two steps of self-supervised fine-tuning on GPT2 and reinforcement learning from AI feedback (with protein-based and cluster-based rewards) to improve the model's prediction.\nThey evaluate the quality of proteins generated by the proposed framework and show relatively good performance on closeness to the real-data distribution and high consistency. The authors also provide interesting findings on exploring disordered regions and case studies to understand cluster representations further."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of protein design is important. With the rapid growth of LLMs, utilizing LLMs for protein design is a timely and interesting problem.\n2. The design of the NL2ProGPT framework seems to be novel in terms of integrating existing techniques used for LLMs with natural languages and techniques specified for protein learning.\n3. As measuring the generation of protein is still open research, the paper makes a good effort in quality evaluation and shows a good performance of the proposed method.\n4. I appreciate the effort of the authors in providing the case study"
            },
            "weaknesses": {
                "value": "- Correctness/Soundness of the framework:\n    - Evaluation: the method seems to use the same model in the framework for evaluation. For instance, they use ESM2 to embed the structure with reward constraints to an ESM-based cluster and use ESMFold (built on ESM2) for structure prediction evaluation. Also, they use the consistency (with Rosetta) reward in Step 3 to constrain the model and evaluation. This may raise the question of model performance benefits from the inductive bias of these pretrained models and tools.\n    - The paper claims to embed the structural information into the description, but it\u2019s doubtful how much the structure is preserved. First, though the ESM2 paper claims their embeddings have structural information, it is still implicit. Second, though the case study shows some insight into the cluster representation, it\u2019s unclear how much information UMAP (into 2-D) and k-mean can preserve, as we know the loss of information after the dimension reduction and the difficulty of clustering.\n- Results:\n    - In Figure 2, it doesn\u2019t seem the proposed method achieves the best performance in any measure. For instance, a similar approach \u2014 ESM2-MR model is closer to the GT and performs better in the first one.\n    - As NL2ProGPT is not the first approach combining natural language and protein (e.g., proteinDT), this raises the question of motivation in which scenario the proposed method is necessary.\n- Novelty/Originality: while I appreciate the novelty in integration methods for protein design, each framework component seems to be incremental in the design for protein learning.\n- Writing or Presentation: Overall, the paper is easy to follow, but the presentation is not at the quality of the top conference and should be improved.\n    - Typos: there are a few typos, such as missing space right before the citation on page 1 (ProGEn-2Nij), (ref2015Park), page 6 (-2(base)), \u2026. These typos somewhat indicate that the paper was not properly proofread.\n    - I can not find the appendix or detailed description of the model, settings, and template. It should be more useful for understanding to provide the sample template.\n    - (Optional) The writing should be improved to be more concise. Some sentences and claims are  vague and less precise, e.g., \u201cThis training process helps us understand the distribution of combined sequences.\u201d  or \u201cOverall, our generated protein sequences may have a higher success rate when performing wet experiments.\u201d Furthermore, the notations, e.g. aw can also be improved for consistency.\n    - I didn\u2019t find the description/definition of the TM score in the paper.\n    - Minor: For Figure 1, step 1, the figure of ChatGPT seems to be a cropped version of the ChatGPT official icon without modification."
            },
            "questions": {
                "value": "Together with previous questions, I have some clarification questions:\n\n1. For structure embedding, have you considered other methods, such as explicitly embedding structure from AlphaFold generation, which some recent papers use?\n2. For step 1, how do you improve the diversity with ChatGPT? Do you also input the protein sequence to ChatGPT?\n3. For step 2, what are the input and output? From the figure, it seems like a pretraining step with self-supervision (next-token prediction). Still, the description in the paper says it\u2019s p(a|w), meaning predicting amino acids from the description. Can you elaborate on this step?\n4. For step 2, what is the initial state of GPT2? Which checkpoint is that? \n5. For step 4, how do you control the diversity of generated sequences given an input protein?\n6. How well do they cluster in 2-d of UMAP?\n7. Terminology: why do you call it conversational protein design? It may be confusing to the dialog or conversation-based LLM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1229/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1229/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1229/Reviewer_se98"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824970542,
        "cdate": 1698824970542,
        "tmdate": 1700640342440,
        "mdate": 1700640342440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uEZmdg5qN6",
        "forum": "sFJr7okOBi",
        "replyto": "sFJr7okOBi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_Gu8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1229/Reviewer_Gu8r"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to train a joint model on both the protein and text modalities (optionally with RL with some rewards around generality and consistency). The model is then used to generate proteins, sometimes with textual constraints that are key to the authors' approach.\n\nThe models are evaluated with respect to several related works on both the generality and consistency dimensions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think the idea of building joint protein text representations for controllable protein generation is important and has a lot of promise."
            },
            "weaknesses": {
                "value": "-Overall I have some questions about the paper below that I feel are critical to my understanding both to understand the method and make sure the evaluation is fair. \n\n(1) I don't quite understand how ChatGPT is used to generate the text descriptions. It says something like:\n\n\"We then feedthese constructed templates into ChatGPT OpenAI (2023) to obtain diverse protein text descriptions\nby using several prompts. These descriptions constitute the training dataset for text-protein pairs,\nserving as a foundation for further research and analysis.\"\n\nIn Section 4.1 it also says:\n\"Our training dataset comprise 1,001,890 text-protein sequence pairs in total.\"\n\nAre these training examples from the above process with ChatGPT? If so, how did you do any verification on the quality of this dataset?\n\n(2) Given that some of them models use text as inputs like the authors' approach and some do not e.g. Progen I am a bit confused as to how all the models are compared e.g. is each model fed a different input and what are these inputs?\n\n(3) When evaluating for generality and consistency are the metrics used the same as that were used for RL? (in which case it would be unfair since the model would be overfitting on the reward). Some clarification would be great."
            },
            "questions": {
                "value": "See questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1229/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699030297500,
        "cdate": 1699030297500,
        "tmdate": 1699636049406,
        "mdate": 1699636049406,
        "license": "CC BY 4.0",
        "version": 2
    }
]