[
    {
        "id": "491duBvQEA",
        "forum": "0bMmZ3fkCk",
        "replyto": "0bMmZ3fkCk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method for fine-tuning large language models. The authors propose an extremely simple modification to the standard procedure, adding noise to the embedding vectors during fine-tuning. Their method is called NEFTune. The authors experiment with several language models and benchmarks, showing strong results across the board."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper has many strengths.\n\n1. For starters, the simplicity of the method is hard to understate. Researchers and practitioners can try this with only a few lines of code. This is very valuable and a great strength of this paper.\n2. The experiments show strong results across the board. The authors study many models and datasets, and ubiquitously see large gains.\n3. Fine-tuning large language models is a vibrant research direction, and as a paper that advances our understanding and capabilities in such enterprise, I believe this would be of interest to many in the community.\n4. There are many ablations in this work that are informative to the readers.\n5. The paper is clear and well written"
            },
            "weaknesses": {
                "value": "1. All experiments in this paper are done with autoregressive models. Studying other kinds of models (e.g. BERT, T5) would be a valuable addition to this paper, since these models are still used for many downstream applications today.\n2. The authors don't present error bars in the experiments, which . Fine-tuning language models can be notoriously noisy (e.g. [1]), and precisly understanding the magnitude of the noise in the presented experiments would be very valuable.\n3. There is little analysis on scaling trends. Despite some experiments with QLORA, almost all experiments in the paper are conducted with 7B parameter models. Showing that the gains from the proposed method do not diminish vanish with scale would greatly strengthen the paper, as it would demonstrate it's potential for larger models.\n4. There is little discussion on hyper-parameter tuning. It would be great if authors would show that the comparisons are fair but ensuring the computational budget spent for tuning hyper-parameters for the baseline and their method is equal. One concern is that the baseline is more poorly tuned that their method.\n\n[1] Dodge, Jesse, et al. \"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\" arXiv preprint arXiv:2002.06305 (2020)."
            },
            "questions": {
                "value": "1. Do the authors think this method would also work for pre-training? \n2. In Table 3, do the numbers next to the method name correspond to the value of alpha? If so, I'm surprised to see such a big variance, and also surprised by the non-convex behavior in many cases. Perhaps this also ties with weakness #2.\n3. This is not a question, but I just wanted to thank the authors for starting their bar plots at zero. It is refreshing not to be visually misled."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698449678679,
        "cdate": 1698449678679,
        "tmdate": 1699636994674,
        "mdate": 1699636994674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vL1VXL25QU",
        "forum": "0bMmZ3fkCk",
        "replyto": "0bMmZ3fkCk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
        ],
        "content": {
            "summary": {
                "value": "The authors demonstrate that adding noise to the embedding vectors during finetuning leads to significant performance increase in large language models. The results were demonstrated using two evaluation benchmarks, AlpacaEval and OpenLLM Leaderboard tasks. Only the former demonstrated significant increase where the NEFT models increased the output length while preserving diversity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors demonstrated empirically that a simple regularization of adding uniform noise to the embedding features lead to an average improvement of 15% across 5 datasets.\n- The performance gains were demonstrated using 3 different LLMS, which suggests the generalizability of the method to other architectures."
            },
            "weaknesses": {
                "value": "- Overgeneralizing performance gains: While the authors demonstrated significant performance gains using the AlpacaEval dataset with 805 instructions, the NEFTune model did not show significant improvements on the OpenLLM Leaderboard tasks (Fig 3), which is more critical since it tests for reasoning and truthfulness. The authors' claim should consider this lack of improvement in their abstract and title so as to not increase the hype of this method without strong foundations. \n\n\n- Contribution of uniform noise is unclear: The authors clarified that longer outputs caused AlpaceEval scores to increase by 16%. To demonstrate that their method did not cause spurious outputs, they evaluated the model output in terms of 2-gram, log-diversity and whitespace lengths (Table 4). I am curious to know how the NEFT method increases output length without increasing 2-gram repetition and maintaining diversity. Is the model output riddled with sentence fillers, and results for 4-gram analysis were not included in the main text? The authors provided a qualitative example of the NEFT output. I am curious to know if this was a specifically chosen sample. The authors conducted human studies of which human annotators preferred the non NEFT in 30 instances (140-(80+22)). It will be helpful to show the output where the human annotators preferred the non-NEFT output over the NEFT ones. Additionally, I do not think Gaussian Noise 5 should be directly compared against Uniform Noise 5? Perhaps other forms of noise distributions should be considered to understand what the uniform noise is doing. \n\n- Need more clarity on computation:  As the authors mentioned in the Conclusion & Limitations, it is unclear why NEFT works on the AlpacaEval and not on the OpenLLM Leaderboard. More evaluation is needed on why there is a huge disparity in performance gains between metrics. Furthermore, a framework or theory on why the uniform noise improves AlpacaEval metric should be explored. Plotting the training and test loss distribution does not add much to the understanding of the computation. A more thorough analysis of why adding uniform noise seemingly improves performance needs to be explored."
            },
            "questions": {
                "value": "- I am curious to know why are there no performance gains on the OpenLLM Leaderboard tasks?\n- Are there any other forms of regularization at the embedding level that replicates this result?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe",
                    "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614027244,
        "cdate": 1698614027244,
        "tmdate": 1700533970875,
        "mdate": 1700533970875,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qUe8l2ybSJ",
        "forum": "0bMmZ3fkCk",
        "replyto": "0bMmZ3fkCk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes NEFTune, an augmentation strategy for instruction fine-tuning language models.  NEFTune augments samples by adding uniform random noise to the embeddings.  Fine-tuning language models with NEFTune helps models generate longer responses leading to better performance on OOD conversational and instruction eval tasks without harming knowledge evaluated through Q/A tasks. The authors further suggest that NEFTune performs well by reducing overfitting on fine-tuning datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The main strength of this paper are its strong results on improving conversational ability.  Authors show that across 5 different eval datasets, NEFTune improves performance by 8-35%, while retaining performance on knowledge and reasoning Q/A tasks. The proposed approach is simple to implement requiring only a simple change to add noise to the embeddings of the LLM.  This makes it so that the authors can easily test the method on large models (7B) and varied datasets.\n\n* The authors have also made efforts to demonstrate the effects of adding noise to the embeddings in particular leading to the model generating more text, and leading to higher loss (reduced overfitting).  \n\n* Augmentations in NLP for training LLMs are relatively unexplored.  The proposed method is similar to prior work in vision, but new to LLM training which typically only trains for a small number of epochs to avoid overfitting."
            },
            "weaknesses": {
                "value": "* While performance looks strong with NEFTune, the authors do not evaluate with other augmentation strategies such as those in https://arxiv.org/abs/2106.07499.  These methods can help with limited datasets and overfitting.  NEFTune can appear stronger with comparison to other augmentation and regularization strategies.\n\n* Experiments are done only with 7B parameter models - primarily LLAMA-2 7B.  While results appear strong, and are applicable to large models, these models are trained on large amounts of data and have learned good embedding spaces.  It will be interesting to know if results with NEFTune scale with model size.  Does NEFTune work with small models and larger models? Particularly for smaller models, where the embedding space may not be as strong, or the model is not as susceptible to overfitting.\n\n* Figure 4 shows that Alpaca loss increases when training with NEFT as this adds noise to the embedding space.  Does this mean that performance will be worse on Alpaca eval?"
            },
            "questions": {
                "value": "Q1: Given that models have access to the same information, but tend to generate more text, there are some concerns around hallucination the model might be doing.  Have the authors checked this?\n\nQ2: Does NEFTune improve smaller models such as GPT-2? \n\nQ3: Following on Figure 4, is it possible to measure Alpaca training loss and Alpaca validation loss difference? If the model is overfitting and NEFTune reduces overfitting, we should see this discrepancy decrease.  It appears not to be an overfitting argument but smoothing the embedding space where data from the OOD dataset like Evol-Instruct may be."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791457048,
        "cdate": 1698791457048,
        "tmdate": 1699636994361,
        "mdate": 1699636994361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g2qFtDDx8N",
        "forum": "0bMmZ3fkCk",
        "replyto": "0bMmZ3fkCk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces NEFTune, a simple yet effective augmentation technique that improves the finetuning process of language models by adding noise to the embedding vectors during training. This is an empirical paper. The authors demonstrate that this method can substantially improve model performance, showcasing a huge increase from 29.79% to 64.69% on the AlpacaEval task when applied to the LLaMA-2-7B model fine tuned with Alpaca. Furthermore, the paper highlights NEFTune's ability to outperform strong baselines on more instruction datasets, reporting a 10% improvement for models trained with Evol-Instruct, an 8% improvement with ShareGPT, and an 8% improvement with OpenPlatypus. The authors also emphasize that even powerful models that are trained with RLHF, such as LLaMA-2-Chat, can still reap benefits from additional training with NEFTune. Overall, the paper establishes NEFTune as a valuable tool for enhancing the performance of language models across various tasks and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The research highlights a critical need to shift focus in the field of Large Language Models (LLMs) from predominantly concentrating on expanding datasets and model sizes, to also giving due attention to optimizing algorithms and regularization techniques. This is essential for enhancing model generalization and addressing overfitting, particularly when working with smaller instruction datasets. I completely agree with the authors on this. More emphasis must be laid on the regularization based methods for LLMs especially while finetuning on small datasets.\n2) The authors introduce a simple yet effective regularization method of introducing noise into the embedding vector. Their comprehensive experimentation across a variety of instruct finetuning datasets and LLMs comparisons substantiates the effectiveness of their approach.\n3) The authors deduce that an LLM trained with NEFTune yields longer and more comprehensive text generations, supported by qualitative examples where the NEFTune-generated text appears more detailed than its counterpart. However, they raise concerns about potential repetitiveness in these extended outputs. To address this, they employ n-gram and log diversity metrics, ultimately concluding that the text encompasses more information than mere repetition. They also conduct experiments to ascertain whether the generation of longer, comprehensive responses inherently leads to improved model performance. Their findings confirm that NEFTune outperforms this baseline, demonstrating its efficacy.\n4) The authors attribute the substantial performance of NEFTune, surpassing that of baseline methods, to its ability to mitigate overfitting and enhance generalization. They support this claim with experimental results presented in Figures 4 and 5, although I do have some questions regarding this aspect."
            },
            "weaknesses": {
                "value": "I hope my comments will further strengthen the work.\n\n1) The authors assert that their method outperforms baseline approaches by reducing overfitting. In this context, I would appreciate the inclusion of standard deviation values for the results, calculated over five random seeds. A small standard deviation with NEFTune would indeed validate its efficacy. Conversely, a large standard deviation might indicate that the stochastic nature of the noise added to the embedding layer doesn\u2019t genuinely contribute to a regularization effect. Inclusion of standard deviation is insightful when finetuning an LLM on small datasets with a goal to mitigate overfitting [1, 2].\n\n2) I noticed that the BLEU and Rouge-L scores on the training data for models with NEFTune applied appear to be relatively low. This observation leads me to wonder if the model is able to effectively learn from the training dataset. To gain a more comprehensive understanding of the training process, I would kindly request the inclusion of the training and validation loss curve plotted as a function of iterations for the +NEFT and without NEFT method. This additional information would be greatly appreciated and beneficial for a more thorough evaluation of the model's learning dynamics. \n\n3) I've noticed that in the tables, such as Table 3, terms like \"+NEFT 5\" and similar are mentioned. It would be helpful if these terms could be explained or described directly in the captions of the tables to provide immediate context and clarity for readers. \n\n4) Though adversarial ML literature (Zhu et al., 2019; Kong et al., 2022) has been cited for the choice of the noise \u03b1/\u221aLd. However, since it is the backbone for the entire work, I would suggest the authors to have a detailed discussion about it in the work explaining the reason behind this choice. \n\n[1] Zhang, Haojie, et al. \"Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively.\" Advances in Neural Information Processing Systems 35 (2022): 21442-21454.\n\n[2] Somayajula, Sai Ashish, et al. \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023."
            },
            "questions": {
                "value": "1) \u201cWhile longer generations do score better, we see that no generation-time strategy comes close to the performance of NEFTune models.\u201d The authors mention this in the paper, however it would be great if they can clarify further about this experiment and display those results in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "details_of_ethics_concerns": {
                "value": "."
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698915384597,
        "cdate": 1698915384597,
        "tmdate": 1700593560187,
        "mdate": 1700593560187,
        "license": "CC BY 4.0",
        "version": 2
    }
]