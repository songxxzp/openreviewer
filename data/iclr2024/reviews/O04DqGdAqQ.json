[
    {
        "id": "7UHNFaTUsY",
        "forum": "O04DqGdAqQ",
        "replyto": "O04DqGdAqQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to finetune a pre-trained LLM for generating training data. It suggests previous methods, which prompt LLM for data generation, cannot obtain complex data. Thus, the paper finetunes an LLM with ten samples, uses the finetuned LLM to generate massive training data, and uses another LLM to obtain the labels. Experiments show the proposed method can output complex training data and improve the downstream models\u2019 performance compared with the prompting baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper shows the potential of finetuning in generating large-scale datasets. With few supervised examples, the finetuned LLM can generate more complex samples with the training distribution."
            },
            "weaknesses": {
                "value": "The paper lacks an explanation or analysis of why 10-shot finetuning can finetune an LLM to generate a whole dataset. The randomly selected ten examples likely cannot span the training distribution. If all training samples are short, the LLM cannot learn to output complex data.\n\nGiven the above concern, other questions arise.\n- Why is the example number 10? Does more/less examples increase performance or lower the generalization ability?\n- Can the finetuned LLM generalize to other datasets? For example, from math to GSM8K or even HumanEval? How do you know the training boundary given the randomly selected ten examples?"
            },
            "questions": {
                "value": "- See weakness.\n\n- What are the performance of baselines and the proposed method given the same initial data? And What are their multi-run performance with different random seeds?\n\n- What are the properties of the selected ten examples?\n\n- Since the proposed method focuses on improving the data generation procedure, it is interesting to know whether it is effective for the pretrained LLM in the general domain. For example, if 10-shot FT is better than 10-shot ICL for directly solving tasks instead of generating training data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_C9NZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698565258923,
        "cdate": 1698565258923,
        "tmdate": 1700622179832,
        "mdate": 1700622179832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "14kGg2pdRW",
        "forum": "O04DqGdAqQ",
        "replyto": "O04DqGdAqQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_MKwz"
        ],
        "content": {
            "summary": {
                "value": "The paper is based on short instruction generation using closed-source LLMs through self-instruction. It proposes fine-tuning open-source LLMs with limited initial data and generating longer instructions using these models. After labeling by closed-source models, a new task-specific model is fine-tuned. The paper conducts tests on code completion, math, and commonsense reasoning tasks, demonstrating improvements relative to self-instruction methods. It also discusses the nature of instruction generation and its impact on the final fine-tuning results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed Ada-Instruct method leverages open-source models for instruction generation, reducing reliance on closed-source large models, which can lower the cost of training task-specific models.\n- Ada-Instruct outperforms self-instruct on well-controlled math and commonsense reasoning tasks, highlighting the method's effectiveness.\n- The paper compares the fine-tuned model with instructions generated via self-instruction, particularly exploring instruction quality and the impact on SFT (supervised fine-tuning), revealing insights into instruction quality and its effect on the final model."
            },
            "weaknesses": {
                "value": "- The exploration of which instructions are useful for sft is not sufficiently clear.\n   - The paper initially points out that the issue with self-instruction is the limited length of generated instructions. However, later experiments show that Evol-Instruct with longer instructions does not perform well. The authors attribute this to \"unnatural\" instructions that do not align with downstream task distributions, but lack experimental validation. The authors can rewrite these instructions with open-source to make it more natural. Visually demonstrating the distribution relationships between training, Ada-Instruct, and Evol-Instruct can also support the argument.\n   - Ada-Instruct performs significantly better than self-instruct on math and commonsense reasoning tasks. The reviewer suggests using Figure 3 or other methods to demonstrate the improved alignment of Ada-Instruct with downstream tasks.\n- The experimental setup for code completion lacks soundness. The data for Self-Instruct and Ada-Instruct experiments are not generated from the same Initial Data, and the SFT Data quantities differ.\n- Minor points:\n   - Distinguish settings under different Params in Table 1 with identifiers. Also, Consider adjusting the placement of \"Code LLAMA-Instruct\" and \"Unnatural Code LLAMA\" in the \"Self-Instruct\" part.\n   - In Table 5, it's suggested to differentiate the \"ratio\" from the correctness of \"Generated\" and \"Real Samples\", rather than listing them side by side. This might lead to confusion regarding the interpretation of the \"ratio.\""
            },
            "questions": {
                "value": "- Is Ada-Instruct sensitive to initial data? The paper asserts that Ada-Instruct performs better than self-instruct because data generated by tuning models with task data fits downstream task distributions better than in-context learning. Analyzing the impact of different initial samples on the final results would provide meaningful insights. Is Ada-Instruct robust when using initial samples that are not as scattered as shown in Figure 3 and do not align well with the training data?\n- In Table 1, Ada-Instruct-HumanEval and Ada-Instruct-MBPP outperform GPT-3.5, but their labels are derived from GPT-3.5. Do the authors have further explanations?\n- Would the authors consider open-sourcing their code? This would facilitate verification and follow-up work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698579831471,
        "cdate": 1698579831471,
        "tmdate": 1699636587920,
        "mdate": 1699636587920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JPHGpqqpRI",
        "forum": "O04DqGdAqQ",
        "replyto": "O04DqGdAqQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
        ],
        "content": {
            "summary": {
                "value": "This works proposes to train an instruction generator for generating diverse and in-domain instructions for a specific use case, and outperforms baseline approaches like self-instruct by a large margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is very simple and effective, and gets to generate diverse, complex queries for constructing instruction tuning datasets. This is an effective method to extrapolate training data from models to improve domain specific instruction tuning."
            },
            "weaknesses": {
                "value": "**Fair comparison is lacking**: The Table 1 does not present an apple-to-apple comparison, where Code LLAMA-Insturct utilizes different amount of data from Ada-Instruct-HumanEval or Ada-Instruct-MBPP. A fair comparison will be to compare self-instruct directly with Ada-instruct by controlling the amount of initial data and SFT data.   \n\n**Comparison to Evo-instruct is lacking**: Though Evo-instruct seems to generat unnatural prompt, it has shown significant improvement over normal prompting. It\u2019s necessary to directly compare Ada-instruct with Evo-instruct.   \n\n**Lack of comparison to self-instruct in Table 4**: There is no comparison to self-instruct in Table 4, and it\u2019s unclear if the proposed method outperforms simply prompting a close-source model."
            },
            "questions": {
                "value": "- Do you use any prompt for training the instruction generator? Or you simply use the raw instruction for training? During inference, do you use any prompt, or simply decode from the beginning?\n- Typo: `Code LLAMA-Insturct` should be `Code LLAMA-Instruct` in Table 1\n- The SFT data generation process for the code llama-instruct model described in Rozi\u00e8re et al., 2023 (i.e., generate 62,000 questions with Llama 2-70b, and then extrapolate) is very different from what the authors described in the paper (i.e., using 10 initial data points to extrapolate with self-instruct), why does there exist such a discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_B9Ki"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715165968,
        "cdate": 1698715165968,
        "tmdate": 1700490933586,
        "mdate": 1700490933586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mGpdLVLg2a",
        "forum": "O04DqGdAqQ",
        "replyto": "O04DqGdAqQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Ada-Instruct, a novel method for generating instructions for complex tasks by fine-tuning open-source Large Language Models (LLMs). It provides an insight that self-instruction based on In-context Learning (ICL) struggles to generate long and complex instructions, whereas fine-tuning can produce task-aligned instructions from a few samples. The evaluation demonstrates that Ada-Instruct matches or surpasses state-of-the-art models on tasks such as code completion, math, and commonsense reasoning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper proposes a novel self-instruct method by finetuning open-sourced LLMs to generate instruction.\n2. The insight is impressive that current self-instruct methods (ICL) prefer to generate short instructions which will lead to a distribution mismatch.\n3. The paper is well written."
            },
            "weaknesses": {
                "value": "1. \nIn terms of innovation, the authors seem to have some misconceptions. Specifically, there have been previous works that used open-source models to generate instructions, such as the use of the open-sourced LLM Llama in [1], rather than ChatGPT or GPT-4. So what the authors mentioned in the introduction is not true: \n>A prevalent approach is called \u201cself-instruct\u201d (Wang et al., 2022), which involves having ChatGPT sequentially generate both instructions and answers (Sun et al., 2023; Peng et al., 2023; Taori et al., 2023; Schick & Schutze, 2021; Honovich et al., 2022; Ye et al., 2022; Meng et al., 2022; 2023).\n\nAnd it leads to the comparison between this work and \"previous work\" in Figure 2 being inappropriate.\n\n2. \nHumanEval and MBPP are both Python program generation benchmarks, the proposed method requires separate training on these two benchmarks which is weird and not practical. Can the authors provide the performance of the same model on both benchmarks?\n\n3. \n Also, I think one model for all reasoning tasks is needed, instead of training a separate mode for each benchmark. I wonder if training a model for all benchmarks will increase the required instruction overhead. Can the authors show some evidence against this?\n\n[1] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision"
            },
            "questions": {
                "value": "1. Just a suggestion: I think the authors can provide some concrete inference examples (LLM output) to show the difference between different self-instruct LLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5652/Reviewer_H9NJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698753803214,
        "cdate": 1698753803214,
        "tmdate": 1699636587695,
        "mdate": 1699636587695,
        "license": "CC BY 4.0",
        "version": 2
    }
]