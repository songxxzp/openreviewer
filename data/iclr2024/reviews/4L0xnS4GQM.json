[
    {
        "id": "t9AbVC8TSd",
        "forum": "4L0xnS4GQM",
        "replyto": "4L0xnS4GQM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_xr4a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_xr4a"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes chain-of-tables prompting for solving table-based reasoning tasks. Specifically, instead of generating reasoning chains in natural language or code, the paper generates reasoning chains as a list of atomic operations on the table, and use the intermediate updated table to represent the reasoning process. LLMs are used to generate the operation chains, and each operation is done by executing it in some programming languages. Experiments show that the proposed method outperforms existing baselines on three benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is new in that it adapts the previous chain-of-thought prompting to the table-related tasks, and it improves the understanding of intermediate reasoning results.\n2. Experiments on three benchmarks show the advantages of proposed method over a broad range of baselines including end-to-end methods and other program-aided methods."
            },
            "weaknesses": {
                "value": "1. The proposed method requires significantly more number of queries to LLMs compared to baselines such as chain-of-thought and Binder, since the method needs to query LLMs twice (one for operation generation and one for arguments generation) for each operation. However, no comparison is provided for performance under the same number of queries. For example, what is the performance of baselines when they adopt the self-consistency idea (which is shown to be beneficial in the Binder paper)? Does the proposed method still have advantage when using the same number of queries?\n2. The pre-defined five atomic operations seem to significantly limit the available operations. For example, aggregations such as SUM, MAX, MIN, etc. can be easily done in SQL. How are these aggregations done in the proposed method? No explanation is provided for why these five operations are used in the paper and why they cover the complete set of operations on table.\n3. The presentation of argument generation process is not clear. Based on the prompts in the Appendix, it seems add_column operation directly uses LLMs to generate the new column, whereas other four operations only prompt LLMs to generate arguments that will be fed to the programming language."
            },
            "questions": {
                "value": "1. The added API calls in Binder also add additional columns to the table, how is the proposed method different from the operation used in Binder?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2097/Reviewer_xr4a",
                    "ICLR.cc/2024/Conference/Submission2097/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703437860,
        "cdate": 1698703437860,
        "tmdate": 1700772393861,
        "mdate": 1700772393861,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dTZc7RsELr",
        "forum": "4L0xnS4GQM",
        "replyto": "4L0xnS4GQM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_3YkD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_3YkD"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to effectively leverage tabular data in the reasoning chain. To achieve this, the paper proposes the chain-of-table that conducts step-by-step reasoning as step-by-step tabular operations to form a chain of tables. Empirical results on three benchmarks verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The investigated problem of leveraging and understanding structural data like tables is important and practical while existing LLMs can not solve it well.\n\nThe proposed method does not require training (or fine-tuning) of the existing LLMs.\n\nThe design of atomic operations is novel and also reasonable.\n\nThe overall reasoning procedure of chain-of-table is step-by-step, explainable, and effective.\n\nThe paper is solid from a technical perspective, and extensive experiments are conducted.\n\nThe presentation and drawn figures are generally clear and easy to understand.\n\nSeveral case studies are also elaborated on in the Appendix."
            },
            "weaknesses": {
                "value": "The proposed method only achieves marginal improvements in some cases, e.g., TabFact and ROUGE-1/2/L datasets. I would suggest the paper discuss the potential reasons.\n\nThe observations in Figure 4 are quite interesting. It seems that a longer chain does not consistently bring more accurate results. What are the underlying reasons for this?\n\nDater (Ye et al., 2023) should be the most important baseline for comparison. I would suggest the paper make a further comparison with in-depth analysis from the perspective of methodology, e.g., a comparison between one-step and multi-step reasoning on tabular data.\n\nBesides, how efficient is chain-of-tables when dealing with large-scale data? It seems that the running-time efficiency is known from the current draft."
            },
            "questions": {
                "value": "Please refer to the above weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810941025,
        "cdate": 1698810941025,
        "tmdate": 1699636141982,
        "mdate": 1699636141982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xyegwnMopa",
        "forum": "4L0xnS4GQM",
        "replyto": "4L0xnS4GQM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_eSFE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_eSFE"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel framework called \"chain-of-table\" for reasoning on tabular data. This framework extends \"chain-of-thought\" reasoning to tables. In the chain-of-table approach, a dynamic planning process is employed, which utilizes LLMs to choose predefined operations. These operations are executed on the table at each step of the reasoning process.\n\nThe proposed chain-of-table framework demonstrates superior performance compared to both single-turn baseline methods for tabular reasoning and traditional textual chain-of-thought reasoning. This improved performance is validated on two table QA datasets, WikiTQ and FeTaQA, as well as TabFact, a dataset designed for table fact verification."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed chain-of-table is simple and effective. This highlights the value of decomposing reasoning in tabular tasks, as opposed to employing single-step table reasoning methods.\n2. The good performance underscores its effectiveness compared to baseline methods across multiple tabular reasoning datasets."
            },
            "weaknesses": {
                "value": "1. The proposed method is an extension of chain-of-thought to tabular data. Each reasoning step is constrained by predefined operations on the table. However, it raises questions about the adaptability of the chain-of-table framework to incorporate new operations or external knowledge, such as contextual information related to the table.\n2. Chain-of-table requires a table, which could be large, in each reasoning step. This may significantly increase the computational cost. (See my questions below.)\n3. While it is not necessarily a weakness, it would be beneficial to evaluate the proposed method with an open-sourced model (e.g. Llama-2) to understand whether the framework can be easily adopted by other models. One concern is that chain-of-table relies on LLMs' ability to comprehend the defined operations and reasoning chains, and it is uncertain whether other LLMs can seamlessly adapt to these requirements."
            },
            "questions": {
                "value": "1. In each dataset, could you clarify how many reasoning steps the chain-of-table method requires? Additionally, it would be helpful to understand the total number of input and output tokens required in comparison to the baselines.\n2. For Figure 2, could you specify the number of examples utilized in each of its parts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2097/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2097/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2097/Reviewer_eSFE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698891014060,
        "cdate": 1698891014060,
        "tmdate": 1699636141923,
        "mdate": 1699636141923,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7z70rzmbzR",
        "forum": "4L0xnS4GQM",
        "replyto": "4L0xnS4GQM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_4HBR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2097/Reviewer_4HBR"
        ],
        "content": {
            "summary": {
                "value": "Chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. Still, these approaches incorporate the reasoning chain in a textual context and can't handle tabular data. The paper proposes a Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. The framework uses in-context examples to iteratively generate operations and update the table to represent a complex reasoning chain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Recently, there has been a lot of interest in developing methods to improve the performance of LLMs on tabular data.\n- The paper addresses one of the key stumbling blocks in improving the performance of LLMs on reasoning over tables.\n- The paper is well-written and easy to follow.\n- Empirical results show that the proposed approach outperforms the baselines on real-world datasets."
            },
            "weaknesses": {
                "value": "One of the issues with the paper is how they evaluate, especially the choice of benchmarks. I encourage the author to evaluate their approach over a diverse class of tasks, especially tasks like Table summarization, Column type annotation, Row augmentation, etc. In addition, I also encourage the authors to include open models too."
            },
            "questions": {
                "value": "Should the entire table fit inside the context length of the models? What happens if the tables are large? I would encourage the authors to evaluate their approach on larger tables.\n\nIn addition, a recent work that tries to modify CoT to leverage tabular structure [1] isn't discussed. I encourage the authors to distinguish with [1], further strengthening the submission. \n\nMost existing LLMs often ignore the semantics of table structure, and it is beneficial to encode the structure in some form. Have the authors considered ways to incorporate the structure and semantics of the tables? As of now, the proposed techniques ignore them sans the operators defined and treat the table as yet another set of tokens.\n\n[1] Jin Ziqi and Wei Lu. 2023. Tab-CoT: Zero-shot Tabular Chain of Thought. In Findings of the Association for Computational Linguistics: ACL 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2097/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698908276545,
        "cdate": 1698908276545,
        "tmdate": 1699636141861,
        "mdate": 1699636141861,
        "license": "CC BY 4.0",
        "version": 2
    }
]