[
    {
        "id": "2K5zqBvJP8",
        "forum": "6FvBXs8t8K",
        "replyto": "6FvBXs8t8K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_atuW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_atuW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework called LAST which can be combined with existing adversarial training techniques to improve robustness without much increase in computational complexity.  The LAST framework utilizes a proxy model which stores a previous state of the target model that is being trained and uses this information when determining the parameter update of the model.  They empirically demonstrate that using this gradient update leads to smoother loss landscapes and improvements in robust performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- paper is well-written and ideas are clear\n- good scope in experiments: authors experiment with multiple datasets (CIFAR-10, CIFAR-100) and combine LAST framework with multiple single step and multi-step AT training techniques to demonstrate improvements.  Also interesting experiments on transfer attacks between models.\n- introduction of method is clear, authors give a good intuition for why LAST framework can help with robustness"
            },
            "weaknesses": {
                "value": "- It would also be nice to see experiments with other model architectures and L2 threat model\n- some improvements in robustness (especially combined with single step AT training) are very small (error bars overlap)"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773650966,
        "cdate": 1698773650966,
        "tmdate": 1699636107342,
        "mdate": 1699636107342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oQavJzWdV9",
        "forum": "6FvBXs8t8K",
        "replyto": "6FvBXs8t8K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_MeBp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_MeBp"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes LAST, a framework for training adversarially robust classifiers. At each optimization step, the adversarial perturbations are computed on the target model, but, unlike in commonly used methods, the model parameters update is computed via a proxy model, which can be seen as a past version of the target model. Such framework has the advantage of reducing the instabilities of robust training like catastrophic overfitting, and can be used in combination with different popular variants of adversarial training. In the experimental evaluation on CIFAR-10 and CIFAR-100, LAST provides improvements in the adversarial robustness of the classifiers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The approach of using a proxy model for computing the updates for stabilizing adversarial training is novel.\n\n- The models trained with LAST achieve higher robustness than the baselines at high perturbation radii."
            },
            "weaknesses": {
                "value": "- The presentation is not clear: there are non-standard expressions like \"parameter-oriented attack\" which are not defined, and sentences difficult to follow and interpret (e.g. first paragraph in Page 4).\n\n- The motivation behind the proposed algorithm, and the steps followed to formulate it, is not clear. If I'm not missing something, I think that LAST can be summarized as computing the adversarial attack on the current model, transferring it to the model at the previous iteration, computing a gradient step on it, and updating the current model with such step (plus SWA). This has the effect of slowing down training until the current and previous model are similar, i.e. end of training with small learning rate, and prevent (or delay) overfitting. However, it is not clear why this should provide higher robustness in general. Additionally, the SD loss looks like a variant of the TRADES loss (Zhang et al., 2019) where cross-entropy is used on the adversarial instead of clean points.\n\n- In Table 1, the difference in robustness at $\\epsilon=8/255$, which is the target threat model seen during training, evaluated with AutoAttack, between the models with LAST and baselines is within the standard deviation, while LAST models have significantly worse clean performance. In Table 2 and Table 3, the robust accuracy at $\\epsilon=8/255$ with AutoAttack is not reported. Then, the improvement provided by LAST in such threat model is not evident.\n\n- For the comparison with multi-step methods, there should be a comparison with SOTA techniques (see e.g. [A]) to support the benefit provided by LAST in such case.\n\n[A] https://robustbench.github.io/"
            },
            "questions": {
                "value": "See details above.\n\nOverall, I think the experiments suggest that LAST provides some improvement in robust accuracy when combined single-step methods and evaluated at larger radii than what used for training (which is not a common evaluation setup), but at the cost of worse standard accuracy. This should be more clearly conveyed when stating the contributions of the paper. Moreover, the presentation and motivation for the proposed method need improvement in my opinion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699456447707,
        "cdate": 1699456447707,
        "tmdate": 1699636107275,
        "mdate": 1699636107275,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "THOhiDrZkF",
        "forum": "6FvBXs8t8K",
        "replyto": "6FvBXs8t8K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_9nzt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_9nzt"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a self-distillation based adversarial training framework that learns from past states to make models adversarially robust. In particular, the paper proposes a two-state weight update rule using the gradient of the past state to the current adversarial perturbation. Results on CIFAR-10 and CIFAR-100 show promise."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ The idea of using past model states to improve the model\u2019s defense against adversarial attacks is interesting.\n+ Paper is in general easy to follow."
            },
            "weaknesses": {
                "value": "**Formulation**\n* Considering the proposed method is a two-phase approach, what is its implication on the training time/computational complexity of the method, when compared to baselines? This may be very important to address, since the paper by itself refers to SAT as a computationally expensive approach. How does the proposed method compare on this?\n* The paper lacks a deeper analysis of the proposed method. For e.g., in Algorithm 1, substituting L13 in L16 yields \\tilde{w} as an additive component to \\theta. This would imply gradient ascent in the direction updated in L12. What does this imply? \n* Why does the model state from the past expected to behave more robustly to perturbations? While an empirical motivation is provided, a more intuitive (or conceptual or theoretical) motivation is required.\n* Sec 2.4 discusses briefly how the momentum may be different from the use of momentum. It would have been better to see empirical results with just momentum to indeed show that this is the case.\n\n**Experiments**\n* It is not clear how the baseline methods for picked for the experimental studies. There is no discussion or motivation for the choices. Compared to recent papers on adversarial training, the number of baselines studied seem fewer.\n* All through the methodology section (Sec 2), there was no mention that the proposed method is an add-on to existing methods. The experiments however only study the method as an add-on, not as a separate stand-alone method. Why is this the case? Without a clear reason for this decision, this makes the contributions weak.\n* Continuing with the previous point, how are the methods in the experiments \u2013 LF-AT, LF-AT-GA, LF-BAT, etc - implemented? Without this detail, it is difficult to follow the results.\n* Results on CIFAR-10 and CIFAR-100 may not be conclusive \u2013 it is important to study datasets of the scale of ImageNet.\n* Why was PARN-18 chosen as the backbone? Why not just ResNet-18 or any other backbone?\n* In Table 2, using the \\uparrow to show improvement across 3 random seeds seemed rather unconventional. Generally, papers report mean and std dev when there are multiple trials. It was not clear what was being conveyed here.\n\n**Literature**\n* The discussion of related work is limited and weak. Beyond not being comprehensive for adversarial attacks and defenses, the paper also misses discussing other earlier efforts that look back at past model states. E.g  Jandial et al, Retrospective Loss: Looking Back to Improve Training of Deep Neural Networks, KDD 2020 (Although this work did not explicitly look at adversarial robustness, it is important to discuss literature comprehensively)\n\n**Clarity and Presentation**\n* I found the plots in Fig 1 difficult to understand \u2013 what is loss gap? Between what quantities?"
            },
            "questions": {
                "value": "Please see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699463711053,
        "cdate": 1699463711053,
        "tmdate": 1699636107199,
        "mdate": 1699636107199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8zPcM0zzP8",
        "forum": "6FvBXs8t8K",
        "replyto": "6FvBXs8t8K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_EcMM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1779/Reviewer_EcMM"
        ],
        "content": {
            "summary": {
                "value": "The authors present a two-stage update rule for adversarial training, resulting in a general adversarial defense framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The motivation is clearly explained.\nThe paper is well-written.\nIntroducing historical state of the target model as its proxy to construct a two-stage adversarial defense framework, is a novel idea."
            },
            "weaknesses": {
                "value": "It would be interesting to see results on out-of-distribution samples."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699609314028,
        "cdate": 1699609314028,
        "tmdate": 1699636107106,
        "mdate": 1699636107106,
        "license": "CC BY 4.0",
        "version": 2
    }
]