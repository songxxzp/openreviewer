[
    {
        "id": "poqewwvlUc",
        "forum": "rmLTwKGiSP",
        "replyto": "rmLTwKGiSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_kCcC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_kCcC"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on nonconvex-nonconcave minimax problems. It introduces a new method called the semi-anchored (SA) gradient method, which extend the idea of PDHG to the nonlinear setting by incorperating the certain Bregman distance as a preconditioner. With a designed Legendre function, the SA-GDmax and its practical version SA-MGDA are studied with convergence result and suitable optimality measure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The convergence theorem of proposed algorithm is presented, along with an inexact practical version. Numerical results validate the effectiveness of the proposed algorithm in solving problems that satisfy the weak MVI condition, showing performance comparable to extragradient-type algorithms."
            },
            "weaknesses": {
                "value": "The paper's motivation should be elucidated in greater detail. Additionally, it is advisable to compare the proposed algorithm with recent papers on nonconvex-nonconcave minimax problems that are based on various regularity conditions, such as dominant conditions and the PL inequality, in order to demonstrate the competitiveness of the proposed approach."
            },
            "questions": {
                "value": "1. The paper is centered on the one-sided extrapolation-based PDHG method, and while all theoretical performance are similar to the extragradient method under weak MVI conditions, the motivation behind introducing this method may benefit from further clarification. The author also alludes to the potential for improving the extragradient method; providing more specific details on such improvements would enhance the paper.\n2. In Theorems 5 and 6, the use of gradient computational cost may not be ideal. The $\\mathcal{O}(\\log(1/\\epsilon))$ cost pertains to the iteration cost of the proximal gradient descent method, where the computational cost of the proximal operator is neglected. Additionally,  the worst computational cost for this class of functions can be  $\\mathcal{O}(1/\\epsilon)$ as mentioned in arxiv:2101.11041.\n3. What is the numerical performance of the algorithm for nonconvex-nonconcave problems without the weak MVI condition? If it performs well in such cases, it may be worth exploring the possibility of relaxing certain conditions to accommodate a broader range of problems. Verifying weak MVI conditions can be challenging, and the need for each stationary point to meet this requirement in the derived theorems could be a limiting factor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Reviewer_kCcC"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698388241402,
        "cdate": 1698388241402,
        "tmdate": 1699637144659,
        "mdate": 1699637144659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jjYUfR5kj0",
        "forum": "rmLTwKGiSP",
        "replyto": "rmLTwKGiSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_Mhw9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_Mhw9"
        ],
        "content": {
            "summary": {
                "value": "The paper consider Bregman divergence based methods for weak Minty variational inequalities. They show convergence of the Bregman divergence between two consecutive iterates for the Bregman proximal point (BPP) method by expressing the scheme as a preconditioned resolvent. By applying the hyperplane projection step of Solodov & Svaiter 1999 they increase the range of $\\rho$ (showing convergence in terms of the tangent residual). By modifying the preconditioner further they obtain a scheme which alternatingly computes a proximal gradient for the min-player and solves a proximal (implicit) update for the max-player. They immediately obtain convergence from the previous results. Finally they consider inexactness of the max-player for which they show convergence for the tangent residual."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow, provides an in depth overview of the relevant literature, and the statements appears correct."
            },
            "weaknesses": {
                "value": "My main concern is with the relevance of the results:\n\nThe only result that seems to exploit the Bregman divergence is regarding the (implicit) BPP without hyperplane projection (Thm. 1 and Thm. 3), and this follows almost immediately from the monotone case.\n\nAll remaining results in the paper instead shows rates for the _tangent residual_ (as soon as either inexactness appears or the hyperplane projection is used). If we are interested in the tangent residual in the first place, then a  $\\mathcal O(1/k)$ rate can be achieved by an _explicit_ scheme  _without_ (inexact) max-oracles by a primal-dual extragradient scheme.\n\nConsider for instance Algorithm 3 and the associated Theorem 8.2 of [Pethick et al. 2023](https://openreview.net/pdf?id=ejR4E1jaH9k). Without stochasticity a $\\mathcal O(1/k)$ rate is recovered for the tangent residual (so no log factor as is otherwise the case when using inexact max-oracles). Algorithm 3 could be simplified further by observing that the bias correction term can be ignored in the deterministic case.\n\nI am left wondering: \n\n- what is the purpose of considering hyperplane projection and inexactness if we cannot provide guarantees in terms of Bregman divergence?\n- why not consider the setting without Bregman, and study a nonlinear variant of PDHG (which seemed to be the original motivation in the abstract and which variants for the convex-concave case is mentioned on page 4)? This would essential be an optimistic variant of the PDEG scheme mentioned above."
            },
            "questions": {
                "value": "- I'm surprised that bounded domain is needed in Thm. 5. Is it not possible to use that inexact proximal point is (approximately) nonexpansive up to an error you can control (through the approximate subsolver of the max-oracle)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Reviewer_Mhw9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670977866,
        "cdate": 1698670977866,
        "tmdate": 1700736043806,
        "mdate": 1700736043806,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zmJZYcsVHk",
        "forum": "rmLTwKGiSP",
        "replyto": "rmLTwKGiSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_vygr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_vygr"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed semi-anchored gradient methods to a structured nonconvex-nonconcave minimax problem under certain assumption, namely the weakly Minty variational inequality (MVI). The proposed algorithm is based on the Bregman proximal point (BPP) algorithm, also resembles the primal-dual hybrid gradient (PDHG) method. The proposed algorithm consists of u and v substeps where the authors proposed using FISTA to solve the v substep approximately. Theoretical convergence is studied for this SA-MDGD algorithm and numerical experiments were provided to show the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-rounded and well-motivated. The authors analyzed the theoretical convergence of the general BPP method for a broader class of problems and then proceed to the specific structured problem. The work also addresses the concern of the practicality of the v substep and proposed an inexact SA-MGDA method to carry out the proposed method in practice."
            },
            "weaknesses": {
                "value": "(Please respond to the questions section directly) It remains unclear how the proposed algorithm performs comparing to the existing works, especially on the theoretical rate of convergence under similar assumptions."
            },
            "questions": {
                "value": "1. As mentioned in the Weakness section, a comprehensive comparison with GDmax and other algorithms, especially in theoretical convergence rate seems necessary. Is the sublinear rate as in Theorem 4, 5 or 6 show improvements over existing methods or achieve certain lower bounds? For example [1] seems to achieve similar sublinear rate. The authors could consider illustrating this in their Table 1.\n\n2. In numerical experiments, I\u2019m not sure if the authors implemented their SA-GDmax or the more practical SA-MGDA algorithm. If it\u2019s SA-GDmax as in (7), then how did the authors conduct the v substep precisely? Also for Figure 2, the authors claimed the parameter $\\tau=0.01$ in section 7.2 but presented two choices of $\\tau$, and from the left figure in Figure 2, $\\tau=0.01$ didn\u2019t show a statistical advantage of SA-GDmax over other works. Last, the authors didn\u2019t compare with a lot of the methods in Table 1, for which they should consider adding more numerical comparisons.\n\nReferences:\n[1] Diakonikolas, Jelena, Constantinos Daskalakis, and Michael I. Jordan. \"Efficient methods for structured nonconvex-nonconcave min-max optimization.\"\u00a0International Conference on Artificial Intelligence and Statistics. PMLR, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774287366,
        "cdate": 1698774287366,
        "tmdate": 1699637144408,
        "mdate": 1699637144408,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dkk45hqKbF",
        "forum": "rmLTwKGiSP",
        "replyto": "rmLTwKGiSP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_uoZm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9088/Reviewer_uoZm"
        ],
        "content": {
            "summary": {
                "value": "This work for the first time extends the primal-dual hybrid gradient (PDHG) method from convex-concave minimax optimization problem to nonconvex-nonconcave minimax optimization problem. The 4 versions of PDHG (with/without projection and with/without max oracle) obtain the same gradient convergence rate $\\mathcal{O}(1/k)$ as the existing extragradient methods, and PDHG without projection and with max oracle upper bounds Bragman distance that is larger than the squared norm measure in the convergence rate of the existing extragradient methods, which yields faster empirical convergence of PDHG without projection and with max over extragradient methods as shown in the experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: This work for the first time extends the primal-dual hybrid gradient (PDHG) method from convex-concave minimax optimization problem to nonconvex-nonconcave minimax optimization problem. \n\nQuality: The theoretical and experimental results make sense. \n\nClarity: Generally I can well understand this paper. \n\nSignificance: PDHG without projection and with max oracle upper bounds Bragman distance that is larger than the squared norm measure in the convergence rate of the existing extragradient methods, which yields faster empirical convergence of PDHG without projection and with max over extragradient methods as shown in the experiments."
            },
            "weaknesses": {
                "value": "The major weakness is the weak advantage of the proposed method over existing works, especially EG+ and CEG+, as elaborated in my questions 1-3 below. \n\nSome typos and unclear points are listed in the questions below."
            },
            "questions": {
                "value": "(1) In Table 1, is it possible to add some columns to reveal your advantage over EG+ and CEG+? The advantage over EG+ and CEG+ in bounding the larger Bregman distance seems to disappear for SA-GDAmax with projection (Theorem 4) and the inexact SA-MGDA methods (Theorems 5 and 6). Other advantages? Also, I think the practical inexact SA-MGDA methods should also be included in the experiments. \n\n(2) The drawbacks of extragradient and advantages of PDHG could be briefly mentioned in the abstract and the beginning of the Introduction, instead of ''there is still room for improvement''. Also, in the abstract, is the ''worst-case convergence rate'' lacking in extragradient methods? If yes, you could mention this in the abstract. \n\n(3) What's your advantage over the works ''Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems'' and ''Stable Nonconvex-Nonconcave Training via Linear Interpolation''? You may cite the latter. \n\n(4) In ''This was studied on a general convex-concave problem, but it has not been found useful in a more general nonconvex-nonconcave minimax problem. In this paper, we demonstrate its natural extension to a structured nonconvex-nonconcave minimax problem'' in the abstract, ''it' and ''its'' are far away from PDHG and thus could be replaced by PDHG. \n\n(5) At the end of the second paragraph of the introduction, \"a new nonlinear variant of the PDHG, named semi-anchored (SA) gradient method\" could be clearer. \n\n(6) In page 4, in the sentence ''the GDmax minimizes the equivalent minimization problem'', ''minimizes'' could be changed to ''solves''. \n\n(7) How to compute $R(x)$? You could explain or cite in your paper. Can $R(x)$ be exactly solved? If not, it is recommended to include such an error in the convergence results. \n\n(8) In Section 4.3, you said ''This has several advantages over the standard BPP, which will be detailed later. '' Later I found only one advantage of a larger range of $\\rho$. Any other advantages? \n\n(9) In Section 5.1,  can we replace $\\widehat{L}$ with the previously defined $\\gamma$? \n\n(10) Right after as ''it resembles GDmax'', you could indicate that we can also obtain SA-GDmax with projection using BPP with projection (4) using h in (5). \n\n(11) In Theorem 3, ''SA-GDmax (i.e., SA-MGDA with $J=\\infty$)'' looks clearer. In Theorem 4, should it be ''SA-GDmax with projection''? \n\n(12) In the toy example, what's the function $\\phi$? Should it be $+\\frac{L^2\\rho}{4}u^2$ and $-\\frac{L^2\\rho}{4}v^2$ to correspond to $+f(u)$ and $-g(v)$ in the problem (1)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9088/Reviewer_uoZm"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9088/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788094165,
        "cdate": 1698788094165,
        "tmdate": 1700405981688,
        "mdate": 1700405981688,
        "license": "CC BY 4.0",
        "version": 2
    }
]