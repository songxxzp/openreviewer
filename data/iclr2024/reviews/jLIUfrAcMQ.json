[
    {
        "id": "DfIZgxXri6",
        "forum": "jLIUfrAcMQ",
        "replyto": "jLIUfrAcMQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
        ],
        "content": {
            "summary": {
                "value": "The paper aims at the fairness issue when deploying models. This paper presents an approach to debiasing transformers using their inherent structure. The authors propose some methods to handle the queries, keys and values to reduce the bias. Also, the memory efficiency in the training phase is enhanced."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The overall writing is clear. \n2. The problem of fairness issue when deploying models is important."
            },
            "weaknesses": {
                "value": "The biggest problem is the experiment settings and results.\n\n1. In the experiment tables, which method belongs to \"Fairness without Demographics\", which method requires Demographics?\n2. The proposed method cannot beat SOTA methods. For example, in Table 2, the proposed method is worse than LfF on EOp dataset. In Table 4, the proposed method is worse than JTT method on EOp and EOd dataset.\n3. Following problem 2, the authors may claim that they have much less energy consumption. However, the comparison is not straightforward. The authors need to show one of the two results to claim this point: 1) same energy consumption and higher accuracy; 2) same accuracy and less energy consumption. If the authors can show these results compared to LfF on EOp dataset, and JTT method on EOp and EOd dataset. I will raise my score.\n4. Energy consumption is not a stable indicator when comparing the models. The hardware may have big influence. I would recommend to use FLOPs to compare these methods."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_DJw2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739665283,
        "cdate": 1698739665283,
        "tmdate": 1700635296310,
        "mdate": 1700635296310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wlhZtX3ANV",
        "forum": "jLIUfrAcMQ",
        "replyto": "jLIUfrAcMQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_mDe1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_mDe1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to address fairness issues in vision transformers and natural language processing transformers without requiring access to sensitive demographic attributes during training. The key contributions are:\n\n- They identify two sources of bias in transformers: misallocation of attention weights and bias in value vector representations. \n\n- To address attention weight bias, they normalize and take the absolute value of query and key vectors before computing attention. This is motivated by theoretical analysis showing it reduces disparity in attention weights between groups.\n\n- For value vector bias, they use a supervised contrastive loss on the core value vectors to encourage consistency between groups. \n\n- The method is evaluated on vision and NLP tasks, showing improved fairness metrics compared to prior work without demographics. It also enables efficiently debiasing pretrained models by only retraining the last encoder layer.\n\n- Overall, the method provides a simple and effective way to improve transformer fairness without needing sensitive attributes, auxiliary networks, or restrictive constraints. The ablation studies demonstrate tradeoffs between fairness and accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important problem - improving fairness in transformers without needing sensitive attributes. This is challenging but highly relevant given privacy regulations. \n\n- The approach of debiasing attention weights and value vectors specifically is novel. Prior work either operates on the full representations or relies on adversarial training, which can be unstable. Deconstructing the transformer in this way is creative.\n\n- The method is simple, leveraging existing operations like normalization and contrastive loss. Avoiding complex auxiliary networks is a plus for efficiency.\n\n- Results on vision and NLP datasets demonstrate improved fairness over prior art like DRO and knowledge distillation. The method also enables efficient debiasing of pretrained models.\n\n- Theoretical analysis provides justification for the attention weight debiasing, and empirically shows the approach achieves near optimal fairness-accuracy tradeoffs."
            },
            "weaknesses": {
                "value": "- For NLP, the scheme of picking top value vectors may be less effective for long sequences. Dynamic selection based on attention may work better.\n\n- The last layer retraining is convenient but provides no guarantees. Analyzing how bias propagates through the full network could further improve this.\n\n- The contrastive loss operates locally on values. Extending the alignment more globally could potentially improve fairness further without sacrificing accuracy.\n\n- No rigorous ablation study is provided to analyze the individual effects of the attention and value debiasing components. Their relative contributions are unclear."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251529928,
        "cdate": 1699251529928,
        "tmdate": 1699636743776,
        "mdate": 1699636743776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hAjEF39pBN",
        "forum": "jLIUfrAcMQ",
        "replyto": "jLIUfrAcMQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
        ],
        "content": {
            "summary": {
                "value": "This work focuses on the attention mechanism to debias the transformers without assuming access to the sensitive attribute information. For this, two steps have been performed. First, they propose a weight relocation mechanism by normalizing (subtracting mean and dividing by standard deviation) and taking the absolute value of query ank key vectors in the attention module. \nA theoretical insight is also provided to show that this bounds the discrepancy for various sensitive attributes. \nThen, a nonlinear mapping is applied on tokens with higher attention values to map them to a latent representation $v \\rightarrow z$. Then, supervised contrastive learning is applied to the latent representation to make sure that the embedding for the samples from the same class has core similarity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is generally written well and easy to follow, even though some parts are missing (will discuss later).\n\nThe idea of focusing on the attention module and debiasing the transformer without assuming access to the sensitive attributes is really interesting. \n\nAs this approach is not computationally intensive (based on the experimental results provided in section 4) and is not making significant changes to the architecture, it can be easily added during training to most of the transformer-based structures and improve fairness."
            },
            "weaknesses": {
                "value": "This paper has some weaknesses and I believe addressing these weaknesses can improve the quality of the paper:\n\n1. Intuitively I understand that as the attention mechanism shows the importance of different patches during training/ inference, it can have a large impact on introducing bias. However, authors need to justify in a more systematic way, why using only the attention mechanism is powerful enough to debias a transformer structure. As an example, an analysis in a controlled setup (when we have access to sensitive attributes) can be provided to show that the attention module is the one that mostly affects the bias, or similar experiments to justify this.\n\n2. Similarly, authors need to justify why the optimization objective in Eqn. (2) (minimizing the disparity in attention weights) can be a good approximation to the fairness metrics? I believe there should be an approximation error as instead of considering the output, we are just considering the attention weights. A detailed analysis is required to justify this alternative definition.\n\n3. The statistics of the $q$ and $k$ are estimated during training and then used as an estimation during inference. Authors should provide more details on the accuracy of this choice, as the distribution shift during training and inference might affect both fairness and classifier performance.\n\n4. In section 3.4., more details are required regarding debiasing the pre-trained network by inserting the encoder layer. This part was not clear to me.\n\n5. The experimental results are not convincing enough as a very limited number of combinations for label $y$ and sensitive attributes $A$ are used. \n- Authors should provide various combinations to show the generalizability of the results. \n- In addition, for different datasets, different combinations are used which may give a bad impression of cherry picking.\n- On average, the proposed method is not better than previous approaches and the main benefit is less compute. I wonder wether this approach can be combined with previous methods to give better performance in terms of reducing bias and preventing the degradation in the classifier accuracy?\n\nsmall typo in Figure 1: dotted line $\\rightarrow$ solid line\n\nI am willing to increase my score if the authors provide proper responses."
            },
            "questions": {
                "value": "please refer to weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_sKv2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699522016297,
        "cdate": 1699522016297,
        "tmdate": 1699636743590,
        "mdate": 1699636743590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IzF7Uz8dHP",
        "forum": "jLIUfrAcMQ",
        "replyto": "jLIUfrAcMQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q"
        ],
        "content": {
            "summary": {
                "value": "**Rebuttal Update** After reading the author's rebuttal, I improved my score from a 5 to a 6. However, I would like to see the writing and clarity improve for a final paper if accepted. Specifically, the introduction and use of notation needs cleaning, as well as the motivations in the introduction. I would also like a bit more explanation of the contrastive learning method and its motivations in the context of the whole work.\n\nIn this paper, the authors propose a new method for debiasing the attention mechanism to achieve fairness without prior subgroup definitions. Their method consists of two components. First, they normalize the token embeddings in their Query and Key matrices and absolute value them to bound the attention weight difference across sensitive attributes. Second, they use a contrastive loss to encourage the embeddings of samples from the same class to be similar to each other, encouraging equal representation across sensitive attributes while maintaining performance. Then, the authors provide extensive empirical evaluation of their fairness method across two vision and two language tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper builds on a wide literature of debiasing methods, and does a great job of synthesizing methods from fairness aware transformers with methods in fairness without demographics. Furthermore, the method is straightforward and the first part can be applied easily to any attention mechanism, whereas the second requires a small amount of fine tuning of a nonlinear head. The entire system can be applied out of the box to existing models with little or no training. Another thing I really liked about the paper is the green analysis of the power consumption of the method. I think this is a great step forward for researchers in the field."
            },
            "weaknesses": {
                "value": "One weakness of the paper is the motivation. While debiasing models / fairness is generally a strong motivation, the authors do not explain in much detail why it is important to debias attention mechanisms. Instead, most of the introduction feels like a \"related works\" section, where the main motivation of the paper is the failings of previous methods. I would like to see a better written introduction that explains why the failings of previous methods are bad or costly, and why a new method is needed. \n\nOther parts of the paper are not very clear as well. What motivates the fairness optimization problem (2)? Why do we care about attention weight values when in fairness we traditionally care about outcomes? While having the same activations results in the same outputs of course, it seems to me to be extreme to limit the expressivity of the model across sensitive attributes.\n\nFurthermore, the notation in the paper is pretty messy and unclear. In section 3.1, in the first paragraph the vectors $\\textbf{q}, \\textbf{k}$ are not introduced as slices of the $Q$ and $K$ matrices. Also the relationship between the dataset $\\mathcal{D}$ and how it is inputted into the attention mechanism/transformer model is not mentioned at all (it just jumps straight from dataset notation to attention notation with no connection between the two). In Section 3.2 near the end, what is $q_{cls}$?\n\nI would like to see a better figure explaining the pipeline or mathematical equation of the entire model. $g$ is only mentioned before eqn. (7), but not ever shown visually, this makes it very unclear as to how to implement the second modification and is a bit misleading as we require an additional layer to train to align the model. \n\nFinally, I would like to see an ablation study of the two mechanisms to compare which one impacts fairness more."
            },
            "questions": {
                "value": "What is the practical/fairness motivation behind the fairness optimization problem, and the motivation behind debiasing attention as a whole? \n\nAlso, it seems as though instead of using subgroup attributes in the contrastive loss, you use classes. However, what happens if each class is dominated by a single sensitive group attribute? For example, if class 1 is all male and class 2 is all female, then requiring all class 1 (male) representations to be similar with each other and different than class 2 (female) will actually cause more disparity by pushing the representations away from each other. Don't we have to assume sensitive groups are balanced within classes?\n\nFinally, just as a curiosity, won't normalizing reduce the expressivity of the model? I would love to see the distribution of attention weights before and after normalizing, as well as before and after absolute valuing (as well as an ablation study of the two steps)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6565/Reviewer_JY1q",
                    "ICLR.cc/2024/Conference/Submission6565/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6565/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699604370633,
        "cdate": 1699604370633,
        "tmdate": 1700526411564,
        "mdate": 1700526411564,
        "license": "CC BY 4.0",
        "version": 2
    }
]