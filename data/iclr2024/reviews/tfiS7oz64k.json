[
    {
        "id": "U7cs5TdRw4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission991/Reviewer_jSYM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission991/Reviewer_jSYM"
        ],
        "forum": "tfiS7oz64k",
        "replyto": "tfiS7oz64k",
        "content": {
            "summary": {
                "value": "This paper introduces a novel methodology for Text-to-Shape synthesis, leveraging latent SDF in conjunction with the diffusion process to facilitate the generation of object shapes. The proposed method allows for generation conditioned on a diverse range of inputs, including text, object categories, and images. To enhance its generative capabilities, some attempts including Gaussian noise and hard example mining are introduced, and experiments verify their efficacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper leverages latent SDF as the 3D shape representation, and the model is lightweight since the diffusion denoising process is conducted within the latent space.\n2.\tThe method allows flexible inputs and generates reasonable results for both text and image.\n3.  The paper is well-written and experiments show good qualitative results compared to baselines."
            },
            "weaknesses": {
                "value": "1.\tAn SDF-based representation can only represent the geometry of the object without appearance, which limits the usage of the generated shape.\n2.\tThe categories of generated objects demonstrated in the experiment are very limited. Is it possible to generate more flexible objects (for example, hamburger)?\n3.\tThe results of image-conditioned generation in Fig.1 seem not so similar to the input image."
            },
            "questions": {
                "value": "Text-to-shape models trained with 3D-text pairs seem to highly rely on the training data, while 3D data is much more difficult to obtain than 2D images. Will this lead to a lower diversity of generated results than methods based on text-to-image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Reviewer_jSYM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697205475505,
        "cdate": 1697205475505,
        "tmdate": 1699636025218,
        "mdate": 1699636025218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DxpgHJCBPg",
        "forum": "tfiS7oz64k",
        "replyto": "tfiS7oz64k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission991/Reviewer_qQcJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission991/Reviewer_qQcJ"
        ],
        "content": {
            "summary": {
                "value": "The paper suggests CDiffSDF, a model that generates a 3d shape conditioned on text. It merges latent diffusion model to process text and generate 3d latents and SDF decoder to generate the final 3d shape unlike many previous approaches that use intermediate 2d generative model instead. Hence it requires paired 3d-text dataset. The authors also suggest to introduce Gaussian noise into SDF training procedure. CDiffSDF seems to get impressive results on a few benchmarks while having noticeably fewer parameters compared to many baselines. Being an implicit 3d model it benefits from higher resolution than voxel-based models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Related work is quite extensive yet concise. The method looks quite original to me. The main ideas are described in lots of detail. The experiments show improvement over SOTA on a few datasets as well as ablations for several model variants. Inference time benchmarks also show the proposed model is pretty fast (as it has relatively few parameters). The authors also used a few well-known improvements over standard SDF architecture. Overall I find it a pretty good paper with a coherent story."
            },
            "weaknesses": {
                "value": "Even though I reread the paper multiple times, I still find it hard to fully understand how exactly the model is constructed. High level understanding is easy to get though. Open source code would be useful for the future research. Improvements in clarity of the paper are highly encouraged."
            },
            "questions": {
                "value": "I suggest to add more quality results. 3D papers tend to show more images and that is important."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714562822,
        "cdate": 1698714562822,
        "tmdate": 1699636025121,
        "mdate": 1699636025121,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RQpHqMEdUM",
        "forum": "tfiS7oz64k",
        "replyto": "tfiS7oz64k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission991/Reviewer_jMCT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission991/Reviewer_jMCT"
        ],
        "content": {
            "summary": {
                "value": "The paper presents CDiffSDF, a text-to-shape model aimed at generating 3D shapes based on textual inputs. By leveraging latent-code-based signed distance functions (i.e., DeepSDF), the model achieves relatively high-resolution shape generation while being lightweight. The paper 1) introduces Gaussian noise perturbations and 2) incorporates hard example mining during the SDF training phase to improve the generation capability. Experimental validations are provided to showcase the model's efficacy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(+) The paper addresses an important and active research problem: text-to-3D shape generation.\n\n(+) The paper provides a clear and well-structured explanation of the main methodology.\n\n(+) The paper studies a less explored perspective on text-to-3D synthesis, leveraging DeepSDF as the 3D shape representation."
            },
            "weaknesses": {
                "value": "(-) Limited technical novelty: The proposed method follows a two-stage approach: Firstly, fitting the DeepSDF latent code and subsequently training conditional diffusion models in the latent space. This latent diffusion paradigm has been standard and extensively studied. The novelty of the paper appears more in the introduction and improvement of minor techniques, like 1) fitting DeepSDF with Gaussian perturbations, and 2) incorporating hard example mining, inspired by Curriculum DeepSDF. Overall, there's a noticeable lack of fresh technical insights.\n\n(-) Recent state-of-the-art methods, such as DiffRF (CVPR 2023), 3DShape2Vecset (SIGGRAPH 2023), and Shap-E (arXiv), have also leveraged the idea of performing diffusion in the latent space of NeRF/SDF/occupancy. This paper lacks an in-depth comparison and discussion concerning these methods.\n\n(-) The ablation study seems to miss experiments that demonstrate the role of hard example mining.\n\n(-) The claim on page 8 regarding the lightweight framework outperforming larger models like Shap-E and Point-E (\"A comparison with Point-E and Shap-E reveals that a lightweight framework can outperform these larger models with training over limited high-quality 3D-text data in the specific geometrical-text- guided 3D shape generation task.\") seems unsubstantiated. To strengthen this claim, the paper should consider re-training or fine-tuning Shap-E/Point-E on the same dataset and comparing results."
            },
            "questions": {
                "value": "- In page 3, Section 3.2, the paper devises its method for obtaining text embeddings. Why not utilize pre-trained language or multimodal models?\n- How does the proposed model perform when subjected to \"out-of-distribution\" or more creatively constructed text prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738933771,
        "cdate": 1698738933771,
        "tmdate": 1699636025055,
        "mdate": 1699636025055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "878smHMLQc",
        "forum": "tfiS7oz64k",
        "replyto": "tfiS7oz64k",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission991/Reviewer_6FpV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission991/Reviewer_6FpV"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on efficient Text-to-Shape generation and proposes a a lightweight model called CDiffSDF. The framework works by learning a diffusion model to predict a latent code from a query text embedding; the latent code is then used to predict SDF samples for the shape, which can be further processed into mesh or point cloud. The model is trained and tested on a text-3D-paired dataset containing over 20,000 shapes. Experiments show that CDiffSDF performs competitively with state-of-the-arts both quantitatively and qualitatively while being compact."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper works on the important task of text-to-3D-geometry generation with a focus on designing a lightweight model. Experiments demonstrate that CDiffSDF performs competitively with state-of-the-arts both quantitatively (Tab. 1.a) and qualitatively (Fig. 3) while being compact (Tab. 1.b). \n\n2. The content of the paper is comprehensive, including extensive experiments and ablation studies, sufficient analysis of limitations, as well as  past failed attempts on voxel-based SDF text-to-shape (supp. B).\n\n3. The paper is well-written and easy to follow and understand. Implementation details are well provided."
            },
            "weaknesses": {
                "value": "1. Most strategies introduced in Sec. 3.3 lack enough novelty.\n\n(1) Augmenting the latent space with Gaussian noise to improve the 3D shape generation quality of an implicit decoder is not new. Example prior work:\n[a] Sanghi et al. CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation. CVPR 2022.\n[b] Park et al. Deepsdf: Learning continuous signed distance functions for shape representation. CVPR 2019.\n\n(2) Using the top-k highest losses for hard example mining is not new. Please consider cite related work and discuss the difference. Example prior work:\n[a] Fan et al. Learning with Average Top-k Loss. NeurIPS 2016.\n[b] Yu et al. Loss Rank Mining: A General Hard Example Mining Method for Real-time Detectors. IJCNN. 2018.\n\n2. The ABO dataset used in the paper contains multiple categories other than chairs and tables. The paper has only shown results of shape reconstruction on multiple categories and text-to-3D generation on chair-like&table. It would be much better to help understand the performance if  the authors could also include some results of text-to-3D generation on other non-chair-like/table categories, eg, lamp, dresser, etc., (although the quality might be a bit worse than chairs/tables due to fewer training examples)."
            },
            "questions": {
                "value": "In supp. fig. 10, the authors have inspected the top-2 nearest neighbors of the generated shapes in training datasets, which is good. Another question is: how does the testing query text differ from the training texts? For example, what are the training text queries that are close to this testing query (eg, inlcluding the testing keywords) and the 3D shapes corresponding to the closest training text queries?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission991/Reviewer_6FpV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission991/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760552823,
        "cdate": 1698760552823,
        "tmdate": 1700669577846,
        "mdate": 1700669577846,
        "license": "CC BY 4.0",
        "version": 2
    }
]