[
    {
        "id": "2GMZzozO5B",
        "forum": "PbGs8PGoCn",
        "replyto": "PbGs8PGoCn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_qCUR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_qCUR"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the so-called stateless mean-field games. It considers the finite-agent cases, and the authors include two information feedback settings: full-feedback and bandit feedback. For each of the settings, the authors propose efficient algorithms to learn the Nash equilibrium of the game. The convergence rates of the algorithms and the numerical validations are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper focuses on a new finite-agent setting for mean-field games. The work proposes independent learning algorithms with the performance guarantees to learn the Nash equilibrium. In addition, the numerical results are provided to verify the theoretical findings."
            },
            "weaknesses": {
                "value": "After reading the paper, some of my concerns are not addressed:\n\n1. The results in Theorems 1 and 2 discount the effect of mean-field approximation and need more explanations. In my personal understanding, the mean-field approximation is implemented to avoid the dependency of the number of agents $N$ in sample complexity (I understand that in the finite-player setting, the complexity can scale as $\\log N$ due to for example the union bound in concentrations). However, Theorems 1 and 2 suggest that $T$ should at least grow as $poly(N)$ for a meaningful result. This implies that the results in this paper cannot be extended to the setting $N=\\infty$ considered in the previous papers, e.g., [1]. This may suggest that the algorithms proposed in this paper is not suitable for the large population problem. In addition, the justification for this polynomial dependency by the comparison with [2] below Corollary 2 may be unreasonable, since mean-field approximation is not considered in [2].\n\n2. The bias term $O(1/\\sqrt{N})$ needs more discussion. Although the authors explain that this potentially comes from the independent learning setting, such a term does not appear in other independent learning settings, e.g., the potential games and two-player zero-sum games. I do not fully understand why there is a bias term in the problem setting considered in this paper.\n\n3. Based on the above points, I am not sure whether these concerns arise from the loose analysis of the upper bounds or from the problem formulation itself. It will be helpful to derive the lower bound for these two concerns. \n\n[1] Guo X, Hu A, Xu R, et al. Learning mean-field games[J]. Advances in neural information processing systems, 2019, 32.\n\n[2] Lugosi G, Mehrabian A. Multiplayer bandits without observing collision information[J]. Mathematics of Operations Research, 2022, 47(2): 1247-1265."
            },
            "questions": {
                "value": "The questions are the same as the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5954/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502463466,
        "cdate": 1698502463466,
        "tmdate": 1699636635161,
        "mdate": 1699636635161,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4aDtoz9isF",
        "forum": "PbGs8PGoCn",
        "replyto": "PbGs8PGoCn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_rayu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_rayu"
        ],
        "content": {
            "summary": {
                "value": "To deal with the challenges of space complexity of multiagent RL, they propose stateless mean field games. They are able to use sampling algorithms to find approximate Nash equilibrium sufficiently quick."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "They propose a (seemingly) novel way to deal with massive state space. The paper is well written."
            },
            "weaknesses": {
                "value": "It seems they find an approximate nash equilibrium rather than an exact one. They also impose lipschitz and monotonous payoffs as their restrictions. However they make a good case that these assumptions are not too limiting."
            },
            "questions": {
                "value": "Is finding a exact nash equilibrium possible with this methodology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5954/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773189805,
        "cdate": 1698773189805,
        "tmdate": 1699636635036,
        "mdate": 1699636635036,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FkNQKMpYQp",
        "forum": "PbGs8PGoCn",
        "replyto": "PbGs8PGoCn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_qRij"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5954/Reviewer_qRij"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a generalization of congestion games where cost functions need only be monotone (rather than monotone increasing), which they term a \"stateless mean-field game\". Since the game can be written as a VI with a monotone operator, they propose the use of (what is effectively) L2-regularized gradient descent as an uncoupled game dynamic to recover a solution. The experiments of the paper implement the proposed L2-regularized GD and claim an empirical improvement over unregularized GD."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The Tor network access experiment (which I read to be a live real-world experiment using real Tor latencies) is a very creative and interesting experiment design that I have not seen before. The empirical results that online mirror descent does not perform as well as L2 regularized online mirror descent is not surprising or completely new (see below) but real-world experiments comparing the efficacy of different game dynamics is always interesting and valuable. The paper is also generally well-written."
            },
            "weaknesses": {
                "value": "It seems that there might be some significant overlap between the theoretical claims of the paper and what is already known about congestion games, variational inequalities, and game dynamics. First, approaching congestion games with variational inequalities instead of explicitly exploiting game potentials (e.g. via best response-dynamics) is somewhat standard; the fact that such approaches still work when one relaxes cost functions from being monotone increasing to just monotone is a fairly direct implication---although this is assuming that my understanding that SMFG = Congestion Game but with both monotone increasing/decreasing costs is correct.\nOne of the main technical contributions claimed by the paper is that, instead of running (what is effectively) gradient descent, the paper proposes to run (what is effectively) L2-regularized gradient descent. It seems there were three claimed benefits: 1) it makes the problem strongly monotone (to avoid needing the extragradient method), 2) it helps with stochasticity (convergence despite noise seems to just follow from normal stochastic approximation though and nothing to do with the regularization in particular), and 3) it allows them to implement the algorithm in a decentralized way across the players. The 3rd point seems to be the primary emphasis. However, I don't believe this is a new observation: a standard way of learning equilibria in online learnable (such as monotone) setups is no-regret dynamics. As a side note, Theorem 1 should probably state the dependence on $K$ instead of hiding it as a constant---it seems like the correct dependence should be $\\log(K)$ at least in the full information setting (e.g. using exponentiated gradient descent)."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5954/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817948105,
        "cdate": 1698817948105,
        "tmdate": 1699636634932,
        "mdate": 1699636634932,
        "license": "CC BY 4.0",
        "version": 2
    }
]