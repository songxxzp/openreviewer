[
    {
        "id": "SnhwWeM1WZ",
        "forum": "m7aPLHwsLr",
        "replyto": "m7aPLHwsLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
        ],
        "content": {
            "summary": {
                "value": "The authors propose DRSM, a custom de-randomized smoothing algorithm for MalConv, an end-to-end convolutional neural network. DRSM works by dividing input malware in chunks, each of them containing a percentage of the input bytes. These are classified independently, and labels are provided through majority voting. Table 3 shows that, using different number of windows, DRSM is able to certify more than 40% of points, with a peak of almost 54%.\nTo empirically show their results, the authors also use state-of-the-art adversarial attacks against DRMS, highlighting that DRMS-12 to 24 are able to stop most of the proposed attacks.\nLastly, the authors also release their PACE dataset, by sharing URLs and SHAs of both malware and goodware programs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper bridges an interesting gap between general machine learning robustness, and its application to complex domains like malware detection. \n2. The certification approach works by splitting malware in chunks, averging its predictions. This is doable thanks to the nature of the architecture. Also, this would have been much more difficult to do with a model relying on hand-crafted features.\n3. The PACE dataset is timely, since it is always difficult to get goodware samples."
            },
            "weaknesses": {
                "value": "**No white-box evaluation.** The authors state that they compute attacks on the base model, thus framing them as white-box attacks (like Partial DOS; Shift, etc). However, these are evaluated as black-box transfer attack, and thus should be clarified on the paper.\n\n**Probable bad params for attacks.** The low success rates of attacks (especially GAMMA) might be due to a wrong initialisation. In the appendix, it is written that 200 as population size and query are used, but the number of queries for the GAMMA attack are computed as population_size * iterations. Also, the number of used sections is missing (which is a crucial point for the attack).\n\n**Dataset concerns.** While the release of a goodware dataset is for sure a great contribution, I am doubtful on the composition of such corpus. In particular, the sources might contain malware  or generic unwanted propgrams (Softonic is known to host plenty of installers and grayware that asks you to install other third-party programs). The authors should better clarify the origins of these data, or at least try to study the quality of the provided ground truth. Otherwise, the dataset might contain biases that reduce the fairness of the publication.\n\n**False statements.** The authors state that \"it is difficult to add more than 10% of content\". This statement is false, since adversarial malware attacks are automated through tools. Papers like [Demetrio et al. 2021a&b / Lucas et al. 2021] can increase the size more than 10% of the file size (Lucas et al. bound it to 5% just to not enlarge too much the input file).\nLastly, Header Modification is not proposed by Nisi et al. 2021, but it is contained inside the SecML Malware library, inspired by the paper (that states which fields are not used by the loader anymore).\n\n**Limitations and related work not addressed.** The paper does not discuss limitations of their methodology, by just saying that it is \n certification is a difficult problem to solve. Also, related work misses a preliminary (but unpublished) paper [1] that addressed the problem in the early months of 2023 (more than 6 moths ago). It would be better to mention the fact that preliminary work on certification for malware detection are already there.\n\n[1] Certified Robustness of Learning-based Static Malware Detectors - https://arxiv.org/pdf/2302.01757.pdf"
            },
            "questions": {
                "value": "1. How you conducted the adversarial attacks? Which library did you use / how you obtained the code of attacks? Inside the provided material, it is not possible to test the attacks from Lucas et al.\n\n2. Can the authors provide better information on the quality of the collected goodware?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_EohB"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697729377991,
        "cdate": 1697729377991,
        "tmdate": 1700648094159,
        "mdate": 1700648094159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P5ym5Tgp6Q",
        "forum": "m7aPLHwsLr",
        "replyto": "m7aPLHwsLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_grdf"
        ],
        "content": {
            "summary": {
                "value": "This paper applies the de-randomized smoothing technique from the image classification domain to the domain of malware detection - proposing a window ablation scheme. Theoretical robustness is argued and certified and empirical robustness (the latter against a broad range of attacks) is tested empirically. A dataset of benign executables will also be made available to support future research."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a well written paper which clearly presents the core idea. The experiments are quite thorough and support the claims.\n\nMaking the dataset available for future research is a positive."
            },
            "weaknesses": {
                "value": "The basic idea is quite simple, and this is not a contribution of major impact in the field. \n\nIt would have been good to see a description of de-randomized smoothing in the related work, as this is core to the idea.\n\nThere are a few grammatical issues throughout the paper - it needs a polish before publication, e.g.\n- \"We will use it as base classifiers\"\n- \"potentially attributing to the issue\"\n- \"convolution neural network\"\n- \"there have been a large amount of work\"\n- \"to some extents\"\n- \"though it has been believed as a robust model\"\n- \"However, it's worth highlighting..\" not sure However is right word.\n\nMinor issues:\n- X \\subset [0,N-1] is wrong. [0,N-1] is a real-valued continuous interval. I think it means X is the set {0, 1, ..., N-1}"
            },
            "questions": {
                "value": "Would it be possible to indicate the values of \\Delta on the x-axis of figure 3? Everything has been in terms of that up to this point.\n\nOne issue would be with attacks which INSERT bytes. Section 3.2 seems to suggest this is possible (\"attacker can modify or add any bytes in a contiguous portion\"). Surely, if added at the start of the file, this can change the contents then of EVERY window, as all the bytes get shifted to the right. So adding bytes does not seem to be in the threat model that would give certified robustness.\n\nIn fact, the above comment may explain why the DOS Extension attack has such a big effect on the DRSM models in Figure 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697783994100,
        "cdate": 1697783994100,
        "tmdate": 1699637009845,
        "mdate": 1699637009845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zmFHsWyfFd",
        "forum": "m7aPLHwsLr",
        "replyto": "m7aPLHwsLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT"
        ],
        "content": {
            "summary": {
                "value": "This paper applies de-randomized smoothing to produce a classifier for malware detection (called DRSM) that is certifiably robust against patch attacks. The proposed classifier can be viewed as an ensemble of base classifiers, each of which operates on a distinct block of the input file. After collecting predictions for each block from the base classifiers, the prediction for the file as a whole is made by majority vote. This architecture admits a patch certificate that depends on the block size, maximum input length and the voting margin. The included experiments show that DRSM (with MalConv base classifiers) achieves a similar accuracy as vanilla MalConv, while producing patch certificates of order 100KB in size. Experiments examining empirical robustness to several attacks are also reported, which generally demonstrate improvements compared to vanilla MalConv and MalConv with non-negative weights. The paper also contributes a new dataset of benign executables, which is useful given the limited availability of publicly available benchmark datasets for malware."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It\u2019s great to see a paper investigating certified robustness outside the vision domain, which has dominated the literature to date. The patch-like threat model seems well-motivated for malware, given it encompasses several existing attacks. \n\n2. Another strength of the work is its simplicity. DRSM is conceptually straightforward to implement and analyze, which could reduce barriers to adoption.\n\n3. I\u2019m pleased the authors have found a way to share a public dataset for malware analysis. The lack of public benchmark datasets is a major impediment for academic malware research. Having this dataset available will save researchers' time, and it should allow for better comparison between papers (which tend to use different datasets currently)."
            },
            "weaknesses": {
                "value": "1. The paper can be seen as applying an existing method (de-randomized smoothing) to a new domain (malware). Although the authors claim that it is \u201cchallenging\u201d to adapt de-randomized smoothing for malware, I\u2019m not convinced. The proposed method is a form of structured ablation, originally studied by Levine & Feizi (2020) for 2-d inputs with homogeneous base classifiers. The modification from 2-d to 1-d inputs and from homogeneous to heterogeneous base classifiers seems straightforward. Moreover, the proposed method has appeared in prior work in a more general form by Hammoudeh & Lowd (2023).\n\n1. The paper claims to be \u201cfirst to offer certified robustness in the realm of static detection of malware executables\u201d. However there is prior work on this topic by Huang et al. (2023) which appeared on arXiv in January 2023. Their work considers a different threat model for malware: edit distance robustness rather than patch robustness.\n\n1. A characteristic feature of the malware domain is that inputs vary in length. However, it\u2019s not clear to me how the proposed classifier architecture handles this. As an example, consider a 100 KB malicious file and a classifier with a maximum input length of 2 MB. For $n$ in the range 4\u201320, the malicious file fits within a single block, meaning it is passed to a single base classifier, while the remaining $n - 1$ base classifiers receive padding as input. Assuming the base classifiers predict \u201cbenign\u201d for padding, the votes are $1$ \u201cmalicious\u201d and $n - 1$ \u201cbenign\u201d giving a prediction of \u201cbenign\u201d. I wonder if I\u2019m missing something here, because it seems the classifier is guaranteed to make false negative errors on small files, which are abundant according to Figure 6.\n\n1. I found the description of the threat model and certificate unclear. Section 3.2 states that the attacker is allowed to \u201cmodify or add any bytes in a contiguous portion\u201d. I understand that \u201cmodify\u201d means overwrite or replace, but it\u2019s not clear what \u201cadd\u201d means in this context. For instance, \u201cadd\u201d could mean \u201cinsert\u201d or \u201cappend\u201d, or it may mean \u201cincrement or decrement by some amount\u201d.  The mathematical description $x\u2019 = x + \\delta$ implies the original sequence $x$ is additively perturbed by $\\delta$ (which is undefined), but this seems to be at odds with the earlier description. Reading between the lines, my understanding is that the certificate covers a contiguous chunk of the original file being overwritten, which may include some bytes being appended to the end of the file. This should be precisely stated somewhere. The current definition of the certificate in Section 5 is in terms of ablated sequences \u2013 it would be helpful to translate this to the input space.\n\n1. It\u2019s great that the authors are planning to release the PACE dataset. However, the current description of the collection process is a bit light on detail. It would be helpful to describe how binaries were selected from the various sources. For instance, was there a preference for recent binaries? Are the binaries for a single platform (e.g., Windows x64) or multiple platforms? How do you know the binaries are benign?\n\nMinor points:\n1. Section 2 states that it\u2019s \u201csurprising\u201d MalConv is still considered state-of-the-art for malware detection on raw byte sequences, given it was released in 2018. The authors attribute this to limited availability of public data. However, I think the main reason is due to difficulties in scaling more complex models (such as transformers) to very long sequences, containing upwards of a million tokens. It\u2019s worth pointing out that the authors of MalConv have released a follow up model known as MalConv 2 (Raff et al., 2021).\n1. Section 3 states that the input vector fed into the network \u201chas to be of a fixed dimension\u201d. This is not true in general, and I don\u2019t believe it\u2019s true for MalConv. I believe it is possible to support arbitrary length inputs in modern frameworks such as PyTorch by specifying `None` for the size of the dimension.\n1. Section 3.1 states that models like EMBER and GBDT \u201ccan work only on feature vectors\u201d. I\u2019m a bit puzzled by this statement. When composed with their feature extractors, these models must be able to operate on raw binaries, otherwise they would be useless as malware detectors?\n1. Section 5 states that vision-oriented ablation techniques such as masking and block ablations are infeasible for byte sequences. However I can\u2019t see why these wouldn\u2019t work on 1d sequences? It seems feasible to mask bytes or ablate 1d blocks.\n1. Section 7 states MalConv NonNeg \u201chas been believed as a robust model for a long time\u201d. It would be good to include a citation for this claim.\n\n**References**\n\n- Huang et al., \u201cCertified robustness of learning-based static malware detectors,\u201d arXiv:2302.01757 (2023). https://arxiv.org/abs/2302.01757\n\n- Hammoudeh & Lowd, \u201cFeature Partition Aggregation: A Fast Certified Defense Against a Union of $\\ell_0$ Attacks,\u201d AdvML-Frontiers 2023. https://openreview.net/forum?id=NX5Nxrz6PV \n\n- Levine & Feizi, \u201c(De)Randomized Smoothing for Certifiable Defense against Patch Attacks,\u201d NeurIPS 2020. https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf\n\n- Raff et al., \u201cClassifying Sequences of Extreme Length with Constant Memory Applied to Malware Detection,\u201d AAAI 2021. https://ojs.aaai.org/index.php/AAAI/article/view/17131/16938"
            },
            "questions": {
                "value": "It would be great if the authors could comment on my feedback about:\n- novelty of DRSM (how does it differ from Levine & Feizi (2020) and Hammoudeh & Lowd (2023)?)\n- how DSRM operates on small files\n- the threat model"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_sYdT",
                    "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698503494251,
        "cdate": 1698503494251,
        "tmdate": 1700636685749,
        "mdate": 1700636685749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cz8XGzFkKA",
        "forum": "m7aPLHwsLr",
        "replyto": "m7aPLHwsLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
        ],
        "content": {
            "summary": {
                "value": "The author employs the de-randomized smoothing technique to develop a certified defense for malware detection. Furthermore, the author introduces a new dataset named PACE, comprising 15.5K recent benign raw executables from diverse sources. Experimental results validate the effectiveness of their approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and generally easy to follow.  \n2. A well-structured and clear presentation.\n3. I appreciate the author for providing a novel dataset."
            },
            "weaknesses": {
                "value": "1.\tThe motivation behind the method somewhat contradicts intuition to a certain extent.\n2.\tThere is a lack of comparison with recent methods for defense adversarial attack."
            },
            "questions": {
                "value": "1.The proposed \"window ablation\" strategy seems to be applicable only to scenarios where perturbations are clustered together (similar to patch attacks in the CV domain). However, many adversarial attack methods for malware disperse the inserted perturbations across various locations within the software. It remains unclear whether the proposed method would still be effective in such cases.\n\n2.The construction of many malware samples follows a piggyback approach, where the majority of the software consists of benign code, with only a small portion exhibiting malicious behavior. That\u2019s to say most of the ablated sequences will be given a benign label. The algorithm proposed by the author may result in false negatives for such malware samples.\n\n3.It is essential to provide a comparative analysis of the author's proposed defense method against existing adversarial example defense techniques for malware."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8147/Reviewer_5AC4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741824122,
        "cdate": 1698741824122,
        "tmdate": 1699637009618,
        "mdate": 1699637009618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D2lYVfEnyr",
        "forum": "m7aPLHwsLr",
        "replyto": "m7aPLHwsLr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_4y7b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8147/Reviewer_4y7b"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a certified defense called DRSM (De-Randomized Smoothed MalConv) through a redesign of the de-randomized smoothing technique in the context of malware detection.\nMore specifically, they introduce a window ablation scheme that creates a series of ablated sequences by partitioning the input sequence into non-overlapping windows.\nExtensive experimentation involving 9 distinct empirical attacks of various types reveals that the proposed defense demonstrates empirical robustness when faced with a diverse range of attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors have gathered 15.5K recent benign raw executables from a variety of sources. These files will be released to the public as a dataset named PACE (Publicly Accessible Collection(s) of Executables). This dataset aims to address the shortage of publicly available benign datasets for research in malware detection and to provide future studies with more representative contemporary data."
            },
            "weaknesses": {
                "value": "* Insufficient theoretical analysis of certified robustness when facing multiple malicious ablated sequences. Specifically, the authors assume that an attacker generates a byte perturbation of size $p$ and can modify a maximum of $\\lceil \\frac{p}{w} \\rceil + 1$ ablated sequences. However, if the attacker simultaneously inserts multiple adversarial code segments at different locations, how will this impact the certified robustness? The authors should offer a more in-depth theoretical analysis of this scenario. Furthermore, the authors should establish the relationship between the window size ($w$) and the resulting certified robustness. For example, does a smaller window size lead to improved certified robustness?\n\n* In my view, there appears to be a contradiction between Figures 1 and 2. As discussed in Section 5, malicious ablated sequences are expected to influence their respective base classifiers, and the predicted winning class should be \"benign.\" However, in Figure 1, the predicted winning class is labeled as \"malware,\". It confuses me. If this work indeed presents a more robust framework that prevents attackers from generating adversarial examples, then the winning class should ideally be \"benign.\" However, if the winning class is consistently benign, it suggests that the proposed framework might miss detecting certain malware instances, creating a contradiction.\n\n* The absence of comparisons with a broader range of real-world antivirus engines accessible via VirusTotal is notable. It would be beneficial, for instance, to determine how many antivirus engines effectively identify the malware and test cases as malicious.\n\n* Another deficiency is the absence of specific details regarding the process of compromising executable files. The authors should provide a more comprehensive explanation of how to generate adversarial examples within the problem-space[Ref-1], which encompasses defining a comprehensive set of constraints on available transformations, preserving semantics, ensuring robustness to preprocessing, and maintaining plausibility.\n\n* The rationale behind the design choice is unclear. Why have the authors chosen MalConv as the baseline classifier? There are numerous alternative models that can serve as the baseline classifier, such as LGBM, RF, and SVM. The authors should consider evaluating their framework with these alternative baseline classifiers.\n\n\nPierazzi, Fabio, et al. \"Intriguing properties of adversarial ml attacks in the problem space.\" 2020 IEEE symposium on security and privacy (SP). IEEE, 2020."
            },
            "questions": {
                "value": "* If the winning class of Fig. 2 is \"benign\"?\n\n* Does a smaller window size lead to improved certified robustness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8147/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840080823,
        "cdate": 1698840080823,
        "tmdate": 1699637009511,
        "mdate": 1699637009511,
        "license": "CC BY 4.0",
        "version": 2
    }
]