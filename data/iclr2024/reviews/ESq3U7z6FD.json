[
    {
        "id": "ABT7BPHLhu",
        "forum": "ESq3U7z6FD",
        "replyto": "ESq3U7z6FD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_9k3r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_9k3r"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles an important technique: the end-to-end optimization of (first stage) neural information retrieval (IR) systems that takes into account the indexing part, i.e. that optimize for search directly. The approach consists in associating with a query or a document both a dense representation and (the indexing part) an identifier sequence. Each identifier sequence corresponds to a subset of documents, hence allowing to search efficiently and effectively.\n\nA specific loss is designed, based on two contrastive losses (one for identifier sequences and one for dense representations), as well as an \"intra-leaf similarity\" that tries to to spread out documents within the possible sequences. The authors also benefit from the tree of possible identifier sequences to provide strong negatives. \n\nExperiments are conducted three datasets (SciFact, FIQA, and MS Marco) showing improvements over a set of baselines including ColBERT, ANCE, Tas-B and dense models (with HNSW index)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed end-to-end loss is interesting (even though it misses the inherent probabilistic nature of sequence generation), and results are promising. The negative sampling procedure is also of interest when training neural IR-dense models."
            },
            "weaknesses": {
                "value": "## Related works and baselines\n\nThe main weakness of the paper is missing related works (both in the discussion and in the reported results).\n\nFirst, there is a work very closely related that is based on product quantization:\n\n- Zhan et al. (ACM WSDM 2022) Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval. https://doi.org/10.1145/3488560.3498443\n\nThere are also missing references to (more efficient) sparse neural IR models (which are also end-to-end):\n\n- Lassance et al. (SIGIR 2021). Composite Code Sparse Autoencoders for First Stage Retrieval. https://doi.org/10.1145/3404835.3463066\n- Formal et al. (SIGIR 2021). From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective. https://doi.org/10.1145/3477495.3531857\n\nThe authors compare to ColBERT but there was since then ColBERTv2 that performs much better.\n\nAltogether, the paper misses strongly related approaches, and do not compare to state-of-the-art results on 1st stage rankers (either dense or sparse).\n\n## Document structure\n\nThe document is not well structured - many figures and tables cited in the main text are in the appendix; the first 8 pages should be self-contained, which is not the case.\n\n## Indexing length\n\nThere is no discussion about an important aspect: the generated identifier sequence is inherently a probabilistic process. However, this is not taken into account in the loss formulation.\n\n## Negative sampling\n\nThe authors propose to leverage their indexing tree to sample better negatives, which is one of the advantages of their model. However, this aspect is only mentioned passing-by, and the effect of the sampling is not studied. This is both a problem (since this might be the main explanation for the better performance: the authors should try to index using HNSW to measure this effect) and an opportunity (this paper could focus on this aspect, and separate the better learning strategy to the indexing/end-to-end problematic)."
            },
            "questions": {
                "value": "- Please provide comparisons with state-of-the-art approaches\n- The ablation study (for negative sampling) should be given\n- Please make the paper self-contained. Appendices should only be used to provide more details about specific apects"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Reviewer_9k3r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580780722,
        "cdate": 1698580780722,
        "tmdate": 1700642035224,
        "mdate": 1700642035224,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bvYelbWM1b",
        "forum": "ESq3U7z6FD",
        "replyto": "ESq3U7z6FD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_vMMh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_vMMh"
        ],
        "content": {
            "summary": {
                "value": "The paper purposed a new model arch for the embedding based retrieval, trying to resolve the issue that the traditional product quantization will lead to suboptimal retrieval performance due to its design. Overall the idea is original and well structured, and the math is relatively clear and sound. From the results conducted by the author, their method showed a decent marginal over the baseline models. I have pointed out several minor issues and suggestions below."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main contribution / original ideas from my point of view are\n1. e2e joint training of query and doc that eliminates the process of product quantization for doc embedding generation process\n2. the careful and solid loss function design based on the goal of EBR, with ablation study (I personally like this part most)\n\nthese two points are original and well founded, and resolves real issues the baseline design has. Theoretically the assumption also makes perfect sense in that PQ is a compromise to the design and the problem being addressed, while prob distribution over the path in a tree should be superior as well.\n\nthe loss function design over the traditional doc product (and others) sounds like a really good example for how optimization goal should be constructed given a real world problem, especially given the ablation study."
            },
            "weaknesses": {
                "value": "Some suggestions from my perspective\n\n1. clear assumption - my first intuition after reading the paper is the story it tries to deliver lacks an good assumption/description of the initiative. The work is an incremental work built on past models, and authors claim that e2e training is better. However the baseline models it compares with are also trained e2e, while it is disjoint because the inference process, i.e. the ANNS requires doc embedding being calculate first. I would phrase the advantage to be to recapture the lost accuracy during ANNS, etc.\n2. hard-nv mining - the process is only mentioned with a few words, for \"fast convergence\" purpose. I kind of feel this is a really important topic when we choose to do so, and possibly causing some performance loss if not doing correctly. More details about how the authors doing it should be necessary.\n3. lack of clarity in model details - it may be due to page limit but the model description is not clear enough. The softmax in the tree, the path embedding, the Wh+1, Uh+1, and p^h+1 looks not that straight forward without more clarification or a graph of training and inference process, etc.\n4. hyper params - the EHI has a lot hyper params as well even compared with baseline model. how are they selected? the ones in loss, in model arch, etc would have a big impact if not chosen correctly IMHO"
            },
            "questions": {
                "value": "see in above weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Reviewer_vMMh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616164545,
        "cdate": 1698616164545,
        "tmdate": 1699636581332,
        "mdate": 1699636581332,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SoXzu1S8yF",
        "forum": "ESq3U7z6FD",
        "replyto": "ESq3U7z6FD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_ftWv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_ftWv"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an EHI for efficient dense retrieval in semantic search. The traditional two-stage process of dense embedding-based retrieval, involving contrastive learning for embedding and approximate nearest neighbor search for retrieval, can lead to suboptimal performance due to misalignment of representations and a lack of consideration for query distribution. EHI addresses these issues by jointly learning the encoder and the ANNS structure, utilizing a dense path embedding to capture the position of a query or document in the tree. Experimental results demonstrate that EHI outperforms existing state-of-the-art (SOTA) methods on various benchmarks, showcasing significant improvements in retrieval accuracy with a reduced computational budget. Key contributions include the proposal of EHI as the first end-to-end learning method for dense retrieval, offering a paradigm shift by integrating encoder and ANNS in a single pipeline. The paper also compares EHI with other techniques in the context of representation learning, approximate nearest neighbor search, and encoder-decoder models for semantic search."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a fresh approach which cleverly tackles the limitations of traditional retrieval methods. In a quite simple framework It utilizes dense path embedding, making it really efficient at keeping track of where information is in the search process. The experiments show that it outperforms the other methods, proving that it's really effective even when it doesn't have a lot of computing power. By suggesting the first end-to-end learning method, it's like it's changing the game for how we do this kind of search. The paper does a great job comparing it with other methods, showing why it's the better choice for understanding and finding stuff.\n\nThe paper presents the results of the performance metrics evaluated on the MS MARCO dev dataset and the NQ320k dataset, demonstrating that EHI performs competitively against various other methods. It achieves compettitive results such as an MRR@10 of 33% and 39% and an R@10 of 89% and 96% on the respective datasets. The results also highlight the superiority of EHI over methods like BM25, DyNNIBAL, DeepCT."
            },
            "weaknesses": {
                "value": "While the paper presents a comprehensive comparison with several state-of-the-art methods, a potential limitation lies in the absence of a comparison with some of the best-performing ranking models, which could provide a more complete understanding of EHI's relative performance. Additionally, the paper acknowledges the current lack of a deeper theoretical understanding of path embeddings, potentially limiting the in-depth comprehension of the model's inner workings. Furthermore, although scalability is briefly discussed, a more detailed analysis of EHI's performance on extremely large-scale datasets would strengthen its practical applicability. The paper suggests future research directions for enhancing the generalization of tail queries, yet it does not extensively elaborate on how EHI specifically addresses this challenge, warranting a more in-depth exploration of this aspect."
            },
            "questions": {
                "value": "In terms of previous works how do you compare EPI to some of the matrix factorization methods that learn query and document simultaneously?[1,2,4] and can you apply this method on cross lingual retrieval as well?[3]\n\nOverall, a well written and enjoyable paper \n\n[1] Zamani, Hamed, et al. \"Pseudo-relevance feedback based on matrix factorization.\" Proceedings of the 25th ACM international on conference on information and knowledge management. 2016. \n[2] Kim, Donghyun, et al. \"Convolutional matrix factorization for document context-aware recommendation.\" Proceedings of the 10th ACM conference on recommender systems. 2016.\n[3] Dadashkarimi, Javid, et al. \"An expectation-maximization algorithm for query translation based on pseudo-relevant documents.\" Information Processing & Management 53.2 (2017): 371-387.\n[4] Gao, Yang, et al. \"Jointly learning topics in sentence embedding for document summarization.\" IEEE Transactions on Knowledge and Data Engineering 32.4 (2019): 688-699"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620975460,
        "cdate": 1698620975460,
        "tmdate": 1699636581212,
        "mdate": 1699636581212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tc8Znm5OFc",
        "forum": "ESq3U7z6FD",
        "replyto": "ESq3U7z6FD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_PLmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5622/Reviewer_PLmq"
        ],
        "content": {
            "summary": {
                "value": "- This paper proposes focuses on the task of learning representations (i.e. dense embeddings) for queries and documents/items jointly with an approximate nearest neighbor search index. This paper attempts to overcome limitations of existing approaches which build an approximate nearest neighbor search index after learning query and item embeddings. \n- The proposed approximate nearest neighbor search index is a tree-based data structure in which each internal node contains a set of trainable parameters, and each query/document is routed to a set of leaf nodes using parameters at each internal node. The parameters of this tree-based data structure as well as parameters of query/document encoder models are learnt jointly."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of jointly learning a tree-based data structure with the parameters of the encoder model is interesting. \n- The motivation for jointly learning the kNN search index with the parameters of the encoder is clear. \n- The paper presents several ablations and analysis that help better understand various design choices."
            },
            "weaknesses": {
                "value": "- Potentially missing baselines and related work\n  - Missing comparison with existing work that learns a kNN index jointly with the encode model. Some examples are \n    - Gupta, Nilesh, et al. \"ELIAS: End-to-End Learning to Index and Search in Large Output Spaces.\" NeurIPS 2022\n    - Gupta, Gaurav, et al. \"BLISS: A Billion scale Index using Iterative Re-partitioning.\" KDD 2022.\n  - The training strategy for baseline dual-encoder models is not clear. Some recent papers propose negative mining strategies that overcome the limitations of having to maintain an up-to-date kNN index during training. Here are some examples (some are already cited by the authors but not sure why a direct comparison is omitted):\n    - Monath, Nicholas, et al. \"Improving Dual-Encoder Training through Dynamic Indexes for Negative Mining.\" AISTATS 2023.\n    - Dahiya, Kunal, et al. \"NGAME: Negative Mining-aware Mini-batching for Extreme Classification.\" WSDM 2023.\n    - Hofst\u00e4tter, Sebastian, et al. \"Efficiently teaching an effective dense retriever with balanced topic aware sampling.\" SIGIR 2021.\n - Overall, it is not clear is jointly learning kNN index and encoder model is useful to get better negatives during training or does it have advantages beyond just better hard negative mining. For this reason, a comparison with related work mentioned above can serve to further strengthen the paper.\n\n- While the paper proposes a trainable nearest neighbor search index and jointly learns the parameters of the encoder models and kNN index, it has some limitations similar to existing methods such as\n  - The documents need to be re-indexed every few epochs to mine hard negative documents for each query. It is not clear is this step is cheaper than building index with off-the-shelf libraries like ScANN and FAISS.\n  - While the index contains trainable parameters at each internal node, the basic structure of the tree index i.e. height and branching factor is fixed. \n\n- Joint training of encoder and nearest neighbor search index may not work very well in zero-shot settings where encoder models are trained on one domain and used on a new domain (with a new set of documents/items and without any training data). \n\n- The encoder model is initialized using a variant of `distill-bert` which is already _fine-tuned using labelled data_  from MS-MARCO dataset. And MS-MARCO is one of the main large-scale evaluation dataset. Thus, it may not be fair to compare with baselines (previous papers) that use models such as `distill-bert`, `bert` or `RoBERTa` which are pre-trained _only using self-supervised objectives_."
            },
            "questions": {
                "value": "1. How are baseline dual-encoder models trained? What negative mining strategy is used.\n2. Why is `distill-bert` further finetuned on MS-MARCO dataset used instead of vanilla `distill-bert` model?   \n   - `distil-bert-cos-v5` model is finetuned on MS-MARCO as well as trained via distillation using teacher models such as cross-encoders. It may not be possible to have similar finetuned models for other domains or in general.\n3. What is cost of training proposed joint model and how does it compare with training dual-encoders separately with existing hard negative mining strategies?\n4. How is load balancing enforced/achieved? What part of the indexing or training step encourages better load balancing\n5. In Sec 4.3, why is using beam-size = branching factored referred to as exact search? Are all documents visited in this setting? Also, how does beam-size = 0.1*branching_factor ensure that we search up to 10% documents? My understanding is that beam-size=b means that we end up at b leaf nodes and then exhaustively rank all documents in those leaf nodes. So unless the tree is of height = 1, setting beam-size=branching factor can not mean that we are exhaustively searching over all documents.\n6. What does this line in Sec 3.6 mean?\n    - \u201c\u2026 we now have a learned mapping of d \u00d7 l, where d is the corpus size, and l is the number of leaves.\u201d \n7. How does using off-the-shelf ANNS indexing method with the encoder model learnt with proposed approach work?\n8. How is percentage of visited documents controlled in ScANN and FAISS-IVF methods? Also, why are graph-based kNN search methods not compared with?\n\n9. Discussion around inference latency\n    - I understand that it might not be possible to compete with highly-optimized kNN search methods such as ScANN and FAISS but I think it is important to highlight any important differences in proposed method vs existing kNN indexing methods to better understand if one method is more scalable or amenable to parallelization-based optimization than others."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5622/Reviewer_PLmq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5622/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638723734,
        "cdate": 1698638723734,
        "tmdate": 1699636581061,
        "mdate": 1699636581061,
        "license": "CC BY 4.0",
        "version": 2
    }
]