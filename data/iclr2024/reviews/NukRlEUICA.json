[
    {
        "id": "VwfEbgr23K",
        "forum": "NukRlEUICA",
        "replyto": "NukRlEUICA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_Ts2r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_Ts2r"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical finding regarding affine invariance in group convolutional neural networks. Namely, the authors:\n- Present background material regarding group theory and convolutional neural networks\n- Study affine invariance for transforms generated by the full GL2 group, and not for a special case such as SO2\n- Show that affine invariance can be measured on lifted signals through convolution on the group\n- Discuss the computation of the convolution, and integration, on GL2"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The findings are interesting to the geometric deep learning community, the authors provide detailed proofs of the theorems, and relaxing the results to the more general GL2 group is a good theoretical improvement."
            },
            "weaknesses": {
                "value": "As it is, the paper reads closer to a work in progress than to a publishable version.\n\nThe paper needs to be re-organised to present the motivation and problem statement, as well as all notations, at the start and not in Section 2.1 (for the problem statement), or scattered throughout Section 1 for notations (e.g. \"Also in this paper we use g and h to denote group elements and f and k to denote functions.\" page 4).\n\nSome of the background material is presented in a sloppy way, e.g.:\n- \"Definition 2 (Lie groups). Special case of groups are Lie groups, which are symmetries of Riemannian manifolds.\" this is not the definition of a Lie group\n- \"Definition 6 (Coset). Let H \u2282 G be a subgroup of G. Then gH denotes a coset given by gH=\b {g\u00b7h|h\u2208H}\" - notations are not defined, and this is the definition of left cosets\n- I think the 1 / |det h| in the equation of the planar correlation is redundant with that in Equation 2\n\nAll equations should be numbered.\n\nSome of the background material seems presented in the wrong order, e.g.:\n- \"To address this, we require the concept of group action and group representations. Nevertheless, frequently, our attention is predominantly directed towards linear group actions operating on vector spaces, and these actions are termed representations.\" (page 4)\n- The definition of cosets (that should be rephrased as left cosets) could be presented at the beginning before introducing normal groups\n\nThere are issues with references and the presentation of various concepts, e.g.:\n- The authors mention multiple times that a major weakness of existing methods is that they rely on solving complex optimization problems over G2, but do not provide any references\n- Page 6 - the part about Clebsch-Gordan theory needs references and its relevance to the topic should be introduced\n- Page 6 - the reference regarding perspective distortions is missing\n- Page 4 \"Moreover from (Bekkers, 2019) we know that, if X be a homogeneous space of G. Then X can be identified with G/H with H = StabG (x0) for any x0 \u2208 X. Finally we have \u03b5-Affine invariance definition.\" this is a known result used in Appendix A of Bekkers, 2019 - a better reference would be one of the many textbooks on group theory\n\nAll in all, significant improvements are required to present the work in a cohesive way.\n\nAnother weakness of this paper is the absence of implementation or experimental results showing whether the theoretical advantages translate to actual group convolutional networks - though I understand the focus is on the theoretical findings."
            },
            "questions": {
                "value": "\"The final step that necessitates computation is the integration within the projection layer. In the context of our affine transformation, the stabilizer is specifically GL2(R). We refrain from delving into the intricacies of this process, as it bears resemblance to the earlier scenario.\" page 9\n\nWhy not include the derivations in the appendix?\n\nPage 6:\n\"It is important to note that in an affine transformation, parallel lines in the original image continue to remain parallel in the transformed image. However, the transformation can introduce distortion in the angles between lines.\"\nthis sounds contradictory, could the authors clarify?\n\nHave the authors implemented the method numerically?\n\nCan their approach be generalized to affine equivariance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Reviewer_Ts2r"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698602384303,
        "cdate": 1698602384303,
        "tmdate": 1699636520813,
        "mdate": 1699636520813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S875G3qfEW",
        "forum": "NukRlEUICA",
        "replyto": "NukRlEUICA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_WzuJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_WzuJ"
        ],
        "content": {
            "summary": {
                "value": "This research studies affine invariance on continuous-domain convolutional neural networks,  focus on the full structure of affine transforms generated by the generalized linear group and introduce a new criterion to assess the similarity of two input signals under affine transformations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors introduce a new criterion to assess the similarity of two input signals under affine transformations and analyze the convolution of lifted signals and compute the corresponding integration over $G_2$ which is of some significance."
            },
            "weaknesses": {
                "value": "1.\tThe writing of this paper is crude and the layout is poor. Besides, there are some typos, such as ? in the section 2.1 which makes the citing not clear.  \n2.\tThe motivation is not convincing and the novelty is not strong.  \n3.\tThe authors should conduct experiments to demonstrate the effectiveness of the proposed method, instead of only presenting theoretical results such as Theorems in section 2."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5224/Reviewer_WzuJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735228680,
        "cdate": 1698735228680,
        "tmdate": 1699636520603,
        "mdate": 1699636520603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kmL44heoem",
        "forum": "NukRlEUICA",
        "replyto": "NukRlEUICA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_WvUd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_WvUd"
        ],
        "content": {
            "summary": {
                "value": "The paper is theoretical (no experiments) and describes two things. Firstly, it shows that if two signals are equal (up to some epsilon error) through an affine transformation, then representations obtained by group convolutional neural networks are also similar up to some (scaled) epsilon. This follows from the equivariance principle (theorems 1-3). Secondly, the paper shows how group convolutions for affine groups can be efficiently computed through 1) seperable factorization of the kernel $k(x,A)$ into $k_1(x)k_2(A)$ and 2) making use of the fact that integration over the GL2 part of the affine transformation (over A) can be further split via a QR decomposition. This result is summarized in Theorem 4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper presents an approach which in theory should be of practical value."
            },
            "weaknesses": {
                "value": "1. Often times, the paper is not very clear. Some definitions are not very precise and often the notation is sloppy (def 2 what is a symmetry of a Riemannian manifold, in def 3 alpha is introduced but not used, example 3 contains a type, equation above (2) uses symbol h, but the expression should not depend on h and moreover h is not defined yet, etc.)\n2. The overall structure of the paper is hard to follow. What precisely is the goal of the paper? The paper loosely talks about the problem of matching of functions f1 and f2 without making concrete how this happens. It only defines the notion epsilon-affine invariance but not how we determine when to functions are epsilon invariant. Moreover, the paper on several occasions says that it it wants to avoid solving complex optimization problems (please be more precise on what kind of problems) but does not explain how the paper does it instead. I suppose the motivation is that one can learn representations of functions through affine equivariant mappings, and then measure similarity between those learned representations, but I can only guess.\n3. Theorems 1 to 3 all seem to be intimately related and could probably be merged, in particular 1 and 2 seem almost identical. Could these not be merged by stating that affine invariance is maintained for any equivariant mapping $\\Phi$ (with the property $\\Phi[\\rho(g) f] = \\rho'(g) \\Phi[f]$, with $\\rho$ and $\\rho'$ representations for the in and output resp.). An intuitive explanation up front would also help; why are these theorems relevant?\n4. Section 2.2 then seems to be the most important result but feels a bit rushed and the essence of it is hard to grasp (I hope my summary above is correct).\n5. The objective of the paper seems to be efficiency, but there is no proof of principle on the actual usefulness of the approach. I.e., I imagine that splitting the convolution over 4 integrals, including forward and backward Fourier transforms and change of variables might not be more efficient than applying out of the box group convolutions.\n6. Example 4 is incorrect and is the result from an earlier typo.\n\nSmaller comments:\n1. The abstract uses $G_2$ as a symbol, but it is unclear what it represents. (The affine Lie group $R^2 \\rtimes GL_2(\\mathbb{R}^2)$, I imagine). I don't think it is ever defined.\n2. There are 3 papers which might be relevant based on the notion of separability of group convolutions. [Knigge et al. 2022] Uses separability convolutions to achieve equivariance to scale-rotation-translation transforms.  [Chen et al. 2021] uses separability for efficient implementations for SE(3) equivariance. [Mironenco and Forre 2023] have almost exactly the same results as this paper (decomposition of the GL group), and also  builds on the works from the group of Taylor, so it seems. This last reference however appeared after the ICLR submission deadline so I of course do not expect you to have known about this paper, it could however a helpful reference in improving the narrative of the paper.\n\n[knigge et al] Knigge, D. M., Romero, D. W., & Bekkers, E. J. (2022, June). Exploiting redundancy: Separable group convolutional networks on lie groups. In International Conference on Machine Learning (pp. 11359-11386). PMLR.\n\n[chen et al] Chen, H., Liu, S., Chen, W., Li, H., Hill, R.: Equivariant point network\nfor 3d point cloud analysis, 14514\u201314523 (2021)\n\n[Mironenco and Forre] Mironenco, M., & Forr\u00e9, P. (2023). Lie Group Decompositions for Equivariant Neural Networks. arXiv preprint arXiv:2310.11366.\n\n3. The contributions again say rather then focussing on complex optimization problems, we study invariance through convolution integrals. I do not follow this logic. Simply studying invariance obviously does not require an optimization problem. What precisely is the alternative to the complex optimization problem? This is unclear to me. I do understand the implications of the theorems, but no where in the paper do I learn how these results are to be used in practice.\n\n4. What is the reason of writing presenting the kernel transformation (above definition 1)? I think part of the story is missing that tells that under and equivariance constraint this operator reduces to a group convolution? I am guessing this is the story because the paper so far closely follows the paper Bekkers 2019 as well as his lecture series on Youtube (might be a useful reference to add? https://uvagedl.github.io)\n\n5. Example 3 contains a typo. It should be $f(R^{-1}_\\theta (y - x)$, the parentheses were missing. This mistake is later on used in Example 4. The resulting kernel operator in example 4 is therefore also false.\n\n6. In definition 2 it is unclear what is meant with symmetries of a Riemannian manifold, it seems like an odd way to define a Lie group.\n\n7. In definition 4, alpha is defined but not used. Instead $\\odot$ is used but not defined.\n\n8. In section 1.2 the isotropic kernel convolution is defined, but this definition should not have the 1/|det h| in it, because the left-hand side does not depend on $h$, and moreover, $|\\det h| = 1$ if $H=SO(d)$ (which is also not defined).\n\n9. Example 4 is incorrect as $g^{-1} \\tilde{x} = R^{-1}(\\tilde{x} - x)$ and the resulting $\\mathcal{K}$ should be $(\\mathcal{K} f)(x,h)=f(x)/|\\det h|$.\n\n10. In section 2.1 when $G_2$ finally gets defined, you should also define the group product. Without a definition of a group product the definition of the group is not complete. Currently only the action on $\\mathbb{R}^d$ is given. I know this is a bit nit-picky because the full group product merely involves the product of the GL2 part (A.B).\n\n11. The short discussion on the Clebsch-Gordan theory is a bit odd. It is not necessarily used for \"categorizing kernels\", rather I would say it is a framework for working with irreducible representations and tensor products relative to basis for vector spaces that transform via the irreps. And then, I do not see the connection to the subsequent sentence \"In contrast to the traditional approach used to establish...\". Some logic here is missing.\n\n12. In theorem 1, there are two types of $g$... One is used as input coordinates to the feature maps, the other is used to parametrize the transformation."
            },
            "questions": {
                "value": "So comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699032247033,
        "cdate": 1699032247033,
        "tmdate": 1699636520509,
        "mdate": 1699636520509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tKGWivpOED",
        "forum": "NukRlEUICA",
        "replyto": "NukRlEUICA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_taA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5224/Reviewer_taA6"
        ],
        "content": {
            "summary": {
                "value": "The authors consider the construction of equivariant neural network\narchitectures that process signals defined on $\\mathbb{R}^2$ (say, images),\nwhich should be invariant/equivariant to affine transformations of domain. This\napplication is important for processing images of the physical world, where\naffine transformations of an image correspond under 'local' conditions to \nviewing the same scene content from a different camera angle, and hence preserve\nlocal semantic features of the input image. The authors develop mathematically a\nmethodology that lifts convolutional filters over the images themselves to\nconvolutional integrals over the affine group $\\mathrm{GL}(2) \\rtimes\n\\mathbb{R}^2$, then projects down to obtain a filtered image, in order to\nconstruct these affine-equivariant filters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors consider the important problem of affine invariance in equivariant\n  neural networks for computing with visual data. This specific problem is of\n  great interest among equivariant networks that operate on visual data, and it\n  is often outside the realm of applicability of studies that focus only on\n  $G$-equivariance with respect to Lie group actions on the signal itself\n  (rather than on its domain).\n\n- The work presents mathematically-rigorous analysis."
            },
            "weaknesses": {
                "value": "- The paper would benefit strongly from an empirical component -- even verifying\n  the computational prescription encompassed by the main result, Theorem 4, in\n  the setting of a toy example. This is a significant aspect of the paper's\n  motivation, and it therefore feels like a significant omission to not have any\n  such practical verification.\n\n- The paper would benefit from substantial polishing of the presentation. Here\n  are three concrete areas:\n  - General motivation and connection to prior work. The problem the authors\n    consider is very important, in my view (as I wrote in the strengths\n    section). However, the authors do not present a very robust argument for the\n    fundamental importance of this problem (mentioning only \"such distortions\n    arise in photos when the camera is close to the subject being captured\" in\n    the intro and conclusion, and applications to CAPTCHA). The authors could\n    make reference to works on 3D correspondence (say, textbooks -- Hartley and\n    Zisserman; Ko\u0161eck\u00e1, Sastry, Soatto, and Ma) that emphasize\n    the ubiquity and fundamental nature (both from a \"practical\" perspective and\n    mathematical perspective) of affine invariance in visual data.\n    They could also discuss more precisely how existing methods have struggled\n    to treat this framework, with in particular a more precise discussion of\n    related works that have similar motivations (such as works on steerable\n    networks which process similarity transformations or scale, by Weiler, Cohen, and others, which also involve a treatment of the noncompact setting).\n  - Precision of presentation. The organization of the paper renders it\n    challenging to read. Section 1.1 presents three pages of background\n    material, including definitions and notation that will not be necessary to\n    parse the main results of the paper. Key concepts, such as convolution\n    integrals on abstract LCH spaces $\\mathcal{X}$ and on topological groups $G$\n    with Haar measure, as well as the associated \"liftings\" that play a key role\n    in the theory, are not emphasized as clearly as a consequence of this\n    clutter, and the notation is correspondingly confusing (e.g., note\n    $\\mathcal{K}{\\boldsymbol{w}}$ in the display at top of page 3, the later\n    notation $\\mathcal{K}$ for convolution kernels in equation (2), induced\n    liftings $\\mathcal{K}$ defined in Definition 10, and projection layers in\n    Definition 12, all with identical notation). Measures of integration are\n    often hard to parse (note $d\\lambda$ in Definition 11, $d\\mu_{\\mathcal{X}}$\n    in the display at top of page 3, and later $d\\mu_{G_2}$ in the main\n    methodology; which of these measures is (left-? bi-?) invariant Haar measure\n    and which are simply general measures often needs to be inferred from\n    context. This will make the paper laborious to parse for a non-expert, and\n    limit the ability of readers to use the authors' methodology for\n    computational improvements -- doubly important given that such experiments\n    are not given in the paper.\n  - General polish of writing. The paper would benefit from a robust\n    proofreading for typos, grammatical errors, and notational imprecisions, as\n    well as general polish for tone. Examples of imprecise language include \"Our\n    focus are affine spaced formed by the generalized linear group ...\" in the\n    intro (the affine group involves translations too; similar issue below with\n    \"...expressed as $\\mathrm{SO}(n)$\"); Definition 2 (this definition of a Lie\n    group cannot be parsed without knowing what a \"symmetry\" and a \"Riemannian\n    manifold\" is); the notation $G_2$ for the affine group on $\\mathbb{R}^2$\n    used throughout the paper (why not use a standard notation for this set?);\n    \"First of all, the input $f$ is usually a picture...\" on page 5; a broken\n    (LaTeX) reference on page 6; suprema throughout section 2 which have\n    'type errors' (see for example Theorem 1's statement, with $g$). These are\n    only a small selection."
            },
            "questions": {
                "value": "- Equation (4) does not include a term for the determinant of either of the\n  matrices $\\boldsymbol{A}$ or $\\boldsymbol{B}$, which seemed to play an\n  important role in the earlier theory, eg Definition 10 and Theorem 1. Can this\n  omission be justified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699039261461,
        "cdate": 1699039261461,
        "tmdate": 1699636520433,
        "mdate": 1699636520433,
        "license": "CC BY 4.0",
        "version": 2
    }
]