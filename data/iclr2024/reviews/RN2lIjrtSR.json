[
    {
        "id": "hewvRLPTT2",
        "forum": "RN2lIjrtSR",
        "replyto": "RN2lIjrtSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_X4CJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_X4CJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the ZeroI2V paradigm for adapting image models to video recognition tasks without introducing additional computation during inference. It utilizes spatial-temporal dual-headed attention (STDHA) and linear adapters to capture video dynamics and handle the domain gap between images and videos. Experimental results show that ZeroI2V achieves state-of-the-art performance while maintaining parameter and inference efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I think that the concept of zero-cost temporal modeling is a promising approach for image-to-video adaptation, and it makes logical sense to me as well.\n2. The paper is easy to follow, and the experimental sections are well-designed.\n3. The experimental results clearly demonstrate significant gains, particularly on the SSv2 dataset."
            },
            "weaknesses": {
                "value": "1. I am not fond of the term \"zero-cost adaptation.\" In reality, the adaptation process, which involves re-training, cannot be considered zero-cost. The zero-cost aspect only applies during inference after the linear adapter has been merged with the existing weights. Referring to it as zero-cost adaptation may be an overstatement.\n2. In my opinion, the full adaptation diagram still requires a significant amount of computational resources and memory for backpropagation, as there are a bunch of tunable parameters located in the shallow layers."
            },
            "questions": {
                "value": "1. Is there a training wall clock time comparison with prior works? What is the total training parameter of linear adapters used during training in Table 2/3?\n2. I noticed that the best figure in Table 2 for SSv2 Top-1 is 66.3. However, in Table 4, the corresponding number is 67.7. Which setting accounts for this improvement?\n3. How do you select a specific head for different $\\Delta t$? Has there been any ablation study conducted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698344475277,
        "cdate": 1698344475277,
        "tmdate": 1699636576110,
        "mdate": 1699636576110,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q9duK1wtuk",
        "forum": "RN2lIjrtSR",
        "replyto": "RN2lIjrtSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_Z2GV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_Z2GV"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce Zero2IV, a method to adapt image models to the video domain that avoids full fine-tuning and does not increase computational cost during inference. Two aspects of the problem are dealt with: temporal modeling and the image-to-video domain gap. The first is addressed by Spatio-Temporal Dual-Headed Attention (STDHA). In STDHA, some heads of the transformer model are assigned to model temporal relations, while the other heads model spatial relations. The second is addressed by densely placed linear adapters. The computational cost of the original image model is kept the same during inference on video inputs by re-parameterizing the model post-training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The results of ZeroI2V in Section 4.3 show that it has consistent advantages over previous PETL methods in terms of accuracy when the inference efficiency of the methods is taken into account.\n- The ablation studies on the hyperparameter settings of ZeroI2V show that the proposed components each contribute positively to its performance."
            },
            "weaknesses": {
                "value": "- My main concern regards the trainability of ZeroI2V: I did not find the training time and training GPU memory usage of ZeroI2V mentioned in the main paper. Since there are linear adapters densely placed throughout the network, this makes it unclear whether ZeroI2V is much more cumbersome to train than previous PETL methods.\n- The authors claim ZeroI2V is a general method to adapt image models to the video domain, but the experiments are only done on the action recognition task, which does not require fine-grained spatial understanding as opposed to tasks like video segmentation. In order to properly support this claim there need to be experiments on another video task."
            },
            "questions": {
                "value": "- Can you clarify the novelty of ZeroI2V compared to the Patch Shift Transformer introduced in [1]?\n- What is the difference between head relocation QKV and head relocation KV (STDHA) in Table 1a?\n- Which configuration/hyperparameter setting is chosen for ZeroI2V, based on the ablations, for the experiments in Section 4.3 that compare it to the state of the art?\n- Is the channel change ratio $R_{c}$ supposed to be the ratio $k:h-k$?\n- In Table 5, why is ST-Adapter missing? It seems most similar to ZeroI2V in terms of efficiency and accuracy on K400 and SSv2.\n\n(writing-related:)\n- What is the benchmark mentioned on page 2 in \u201cestablish a benchmark\u201d on page 2? Usually this means there is a new dataset introduced.\n- How is \u201coffering powerful capabilities\u201d a \"key perspective\" of temporal modeling? It's unclear what idea the authors are trying to get across.\n- Saying ZeroI2V is \u201creducing the difficulty\u201d of image-to-video adaptation is also vague. It would be better to specifically mention reducing the inference cost.\n\nTypos:\n- Page 1 paragraph 1 sentence 2: Missing \u201cand\u201d between \u201cCLIP\u201d and \u201cDINO\u201d\n- Page 1 paragraph 1 sentence 4: Remove \u201cthe\u201d from before \u201cparameter efficient transfer learning\u201d.\n- Page 2 paragraph 2 sentence 2: Why is the word \u201cimages\u201d treated as a proper noun?\n- Page 2 paragraph 4 sentence 5: Incomplete sentence. Remove \u201cfind\u201d from beginning of sentence.\n- Page 3 paragraph 2 sentence 3: Replace \u201cimage\u201d with the plural \u201cimages\u201d\n- Page 3 paragraph 3 sentence 1: Use past tense \u201cwas\u201d instead of \u201cis\u201d. In sentence 3, missing definite article \u201cthe\u201d before \u201cvideo domain\u201d.\n- Last sentence on page 3: Remove \u201cthen\u201d. Use a period after \u201cdetails\u201d and begin a new sentence.\n- Page 4 paragraph 2 sentence 4: Typo in word \u201cdifficulty\u201d.\n- Page 4 paragraph 4 sentence 1: Incomplete sentence. Replace \u201cgiven an input\u201d with \u201cthe input is a ...\u201c\n- Page 4 paragraph 4 sentence 2: Should the groups be of size h-k and k (instead of n-k and k)? \n- Page 5 paragraph 4 sentence 1: Use \u201cAssume\u201c instead of \u201cassuming\u201d.\n- Table 1 caption add a space between \u201csection\u201d and \u201cis\u201d.\n\n[1] Xiang et al. \"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition.\" ECCV, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785283907,
        "cdate": 1698785283907,
        "tmdate": 1699636576015,
        "mdate": 1699636576015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AL9uieQBYP",
        "forum": "RN2lIjrtSR",
        "replyto": "RN2lIjrtSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_PNMk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_PNMk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces zeroI2V, an video model understanding model based on the pre-trained image models. The authors propose an STDHA which performs spatio-temporal modeling at no additional cost at inference time. The action recognition results on SSV2 and K400 are solid and convincing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written, easy to follow.\n- the introduced STDHA works as expected and the results are comprehensive and convincing."
            },
            "weaknesses": {
                "value": "- The results are convincing but needs a bit more illustration. For example MViTv2-L works better on SSv2, is that from the model design or fully supervised training or something else? Same thing for the AIM better on the K400, where does the performance gap comes from, the design or something else? what are the advantage and disadvantages of the proposed STDHA comparing against the commonly used action recognition models (not only the fine-tuned CLIP models).\n\n- The novelty is a bit limited or not well highlighted, as the inter-frame attention is not originally from this paper. The paper still has a solid idea on adapters at no additional cost, i would encourage the authors to give the intuition a bit more illustration, e.g. why pick the inter frame attention as part of the proposed STDHA. \n\n- The Vis. are not clear enough, consider to put a CLIP VIT activation map there for comparison."
            },
            "questions": {
                "value": "- See my first comment in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809937116,
        "cdate": 1698809937116,
        "tmdate": 1699636575877,
        "mdate": 1699636575877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LvTnefLIZV",
        "forum": "RN2lIjrtSR",
        "replyto": "RN2lIjrtSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_HQJm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5589/Reviewer_HQJm"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on adapting pre-trained image transformer to video transformer efficiently. Two main techniques are proposed. One is to split the pre-trained self-attention heads into spatial and temporal heads, where temporal heads are doing self-attention across frames to learn temporal information. The second technique is to use linear adapters to tune the frozen model. Then after training, these adapters could be fused into the backbone, without introducing new parameters/computations. The method achieves competitive performance with previous efficient adaptation works on multiple datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe idea to split the pre-trained self-attention heads into spatial and temporal heads is interesting. And it is reasonable since there are redundancies in the pre-trained ViT.\n2.\tUsing linear adapters to tune the model and fuse it with the backbone later is also technically sound.\n3.\tThe proposed method achieves competitive performance on multiple video datasets with previous works, without increasing parameters and FLOPs.\n4.\tThe paper is well written and easy to follow"
            },
            "weaknesses": {
                "value": "1.\tI am curious about how is the STDHA implemented. Because it needs to split the heads into spatial and temporal, I am assuming it will introduce some other operations, although they may not contribute to FLOPs, but may still slow down the latency. However, in Table 2, the proposed method has exactly the same latency as the baseline.\n2.\tIn Table 1 (b), what is the meaning of 1/2 head?\n3.\tIn Table 4, ST-Adapter ViT-L/14 has a performance of 72.3/93.9, which is higher than the proposed method. I think it would be better to show the full comparison, and I don\u2019t think it will degrade the significance of the work.\n4.\tUCF101 and HMDB51 are very similar to K400. It could better show the effectiveness of the proposed method to show some results on other datasets such as Diving48, Epic-kitchens, etc."
            },
            "questions": {
                "value": "Please see the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5589/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698903564244,
        "cdate": 1698903564244,
        "tmdate": 1699636575770,
        "mdate": 1699636575770,
        "license": "CC BY 4.0",
        "version": 2
    }
]