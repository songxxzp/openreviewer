[
    {
        "id": "GNxUwL4wrP",
        "forum": "xdnoULh5Sv",
        "replyto": "xdnoULh5Sv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_VTuG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_VTuG"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors introduce a novel adversarial defense mechanism named CARSO, which combines adversarial training and adversarial purification techniques to enhance the robustness of image classification models. CARSO leverages an adversarially-trained classifier to map potentially perturbed input data to a distribution of clean reconstructions, aggregating multiple samples from this distribution to make robust predictions. Experimental results across various benchmarks demonstrate that CARSO effectively defends against a wide range of adaptive attacks, significantly outperforming existing methods in terms of robust classification accuracy on datasets like CIFAR-10 and CIFAR-100, especially against AutoAttack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Authors proposed a new method combining AT and purification methods for advesarial attack defence."
            },
            "weaknesses": {
                "value": "1. Author used a method of mixing purification and AT,\n but I don't think it is fair to only have an ablation study in AT.\nI wonder comparison between purification method vs CARSO + purification method.\n\n2. It was said that the motivation for separating the scenarios was due to a lack of clean image accuracy, \nbut as a result, the same difference is shown in (c) and (d).\n So, I don\u2019t think that dividing the scenario and showing the experiment is an important part of the paper. \nRather, the logic that it was used in the expect of obtaining an internal representation \nfrom a more refined classifier in order to perform SOTA on a robust image seems more appropriate.\n\n3. Author made an analogy about the method using terms(cued recall and recognition) from cognitive science, \nbut it doesn't seem to be clear.\n\n4. Too much limited and insufficient experiments: There are no state-of-the-art defense baselines such as AWP [1], SCORE [2], and ADML [3], and no larger-scale dataset such as ImageNet. In addition, based on ADML, not only CNN structure and Transformer structures seems needed to validate.\n\n5. Table 2 conducted an ablation study comparing effect of the CARSO method. Clean image performance significantly decreases but the current state-of-the-art ADML method highly increases clean image performacne despite few epochs within 3-5 epochs based on their official code. I did not understand the major contribution compared by ADML.\n\n\n---\n\n**References**\n\n[1] Wu, Dongxian, Shu-Tao Xia, and Yisen Wang. \"Adversarial weight perturbation helps robust generalization.\" Advances in Neural Information Processing Systems 33 (2020): 2958-2969.\n\n[2] Pang, Tianyu, et al. \"Robustness and accuracy could be reconcilable by (proper) definition.\" International Conference on Machine Learning. PMLR, 2022.\n\n[3] Lee, Byung-Kwan, Junho Kim, and Yong Man Ro. \"Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n---\n\n**Post Rebuttal**\n\nAlthough the authors performed rebuttal, the reviewer would like to strongly recommend this paper be modified due to the following reasons:\n\n(1) It is not convincing that adversarial purification model can improve robustness in current version of this paper. For convincing argument, it is much greater to change the motivation of this paper to ***adversarial purification improves robustness*** in introduction, and prove it in the following sections with theoretical or empirical background. In other words, the authors should show that naively denoising images are significantly different than adversarial purification, and adversarial purification is really the most important part in general adversarial robustness.\n\n(2) More importantly, this paper should make the reader think of its paper as expandable work to general adversarial robustness. For example, it is better to validate that the performance of \"CARSO\" + \"MART\" or \"AWP\" or \"SCORE\" or \"ADML\" is stronger than that of \"CARSO\" + \"AT(baseline)\". This is because CARSO is adversarial purification model.\n\n(3) There should exist a experiment of considering state-of-the-art image denoising model based on deep learning. Simply, the reviewer call this model as \"A\". Here, the authors should validate that \"CARSO\" + \"MART\" or \"AWP\" or \"SCORE\" or \"ADML\" is much stronger than adversarially training \"A\" + \"MART\" or \"AWP\" or \"SCORE\" or \"ADML\", where \"A\" should be, of course, trained with same adversarial strategy with CARSO. This is because CARSO is, again, adversarial purification model."
            },
            "questions": {
                "value": "Refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Reviewer_VTuG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698283041975,
        "cdate": 1698283041975,
        "tmdate": 1700555009483,
        "mdate": 1700555009483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hvUJkEnvQb",
        "forum": "xdnoULh5Sv",
        "replyto": "xdnoULh5Sv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_yvVv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_yvVv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method that incorporates two types of adversarial learning methodologies, adversarial training and adversarial purification, to improve adversarial robustness. The authors conducted experiments with the datasets CIFAR-10/100 to evaluate the adversarial robustness against AutoAttack and clean accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The C+AT/rand-AA improves over the baseline AT/AA, verifying the proposed method improved the adversarial robustness."
            },
            "weaknesses": {
                "value": "- Only evaluate the method on CIFAR-10 and 100, which are from the same image distribution. Should also evaluate other different datasets for comprehensiveness.\n\n- Adversarially pre-trained models are used as the classifier in the proposed method. Should also compare with those methods following the same setting for fairness.\n\n- The authors showed training times for different scenarios. It should be compared with other methods. Also, the comparison on inference time should also be given to evaluate the efficiency."
            },
            "questions": {
                "value": "1. Are there any other previous papers that have considered incorporating adversarial training and adversarial purification in a unified framework? If yes, please list them.\n\n2. The authors mentioned they draw inspiration from neurocognitive processes underlying cued recall and recognition. Please elaborate a little bit more details about this neurocognitive process and why it inspires the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787063712,
        "cdate": 1698787063712,
        "tmdate": 1699636323772,
        "mdate": 1699636323772,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gfrZUgHuNt",
        "forum": "xdnoULh5Sv",
        "replyto": "xdnoULh5Sv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_8cBw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_8cBw"
        ],
        "content": {
            "summary": {
                "value": "This work proposes adversarial defense to natural image classifiers by blending adversarial training with adversarial purification. Besides, a bag of tricks were used to improve defensive capability. Experiments were also conducted on CIFAR-10 and CIFAR-100 to show its effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The studied problem and the proposed method are interesting and the paper is easy to follow.\n- Experiments in Table 2 showed that the robust accuracy of the proposed method is competitive with state-of-the-art methods."
            },
            "weaknesses": {
                "value": "- I'm not sure whether the defensive effectiveness comes from the combination of adversarial purification and adversarial training or the bags of tricks utilized in this work.\n- In Table 2, please explicitly mention the state-of-the-art method and its reference rather than vaguely refer to sota in RobustBench as the sota can change with time, and it's unclear whether it's fair to compare ith sota as the experimental settings (architecture, training setups may vary).\n- Besides robust accuracy, why not compare with sota regarding clean accuracy, while in the present form, it seems the authors only reported comparisons with standard adversarial training (2018).\n\n**post rebuttal**\n\nI have carefully checked the rebuttal and comments from the other reviewers, and I still believe this paper can be much further improved particularly in terms of the presentation (as the readers might be confused by some presented results, contributions, or tricks)."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Reviewer_8cBw",
                    "ICLR.cc/2024/Conference/Submission3670/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805799679,
        "cdate": 1698805799679,
        "tmdate": 1700658448716,
        "mdate": 1700658448716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3wzFH9GhcZ",
        "forum": "xdnoULh5Sv",
        "replyto": "xdnoULh5Sv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_T2NZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3670/Reviewer_T2NZ"
        ],
        "content": {
            "summary": {
                "value": "- Draft presents a method for defending against adversarial attacks\n- Specifically, the proposed method maps the feature representations of (normal or adversarial) inputs (extracted by an adversarially trained classifier) to a sample of tentatively clean reconstructions. This mapping is realized via a conditional variational encoder (VAE).\n- An estimate of the clean inference is obtained by fusing the inference of adversarially trained classifier on multiple reconstructed features (o/p of VAE)\n- Experiments are performed on CIFAR-10 and CIFAR-100 datasets.\n- Better results are reported than other methods compared against."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of generative purification from adversarially trained feature representations - despite being a new combination of existing ideas - appears interesting in the context of adversarial defense."
            },
            "weaknesses": {
                "value": "- Generative model-driven purification as a defense against adversarial attacks has been established to a reasonable extent. Existing works demonstrated the effectiveness of this framework using denoising autoencoders (DAE), UNet-based DAE, GAN, VAE, etc. (section 2 of the draft provides references). However, scaling to complex datasets such as ImageNet has been challenging in this context. This draft restricts the experimental analysis only to simpler datasets (CIFAR). Since sophisticated models that are adversarially trained on ImageNet dataset are available easily, readers would expect the draft to experiment with them too.\n- Clarity of the framework description needs slight improvement (please refer to the questions section of the review).\n- It is not discussed clearly in the draft what the improvement in the proposed method is compared to existing purification methods. The draft claims that their method adds a smoothness penalty to the reconstruction loss. However, not much has been discussed on it.\n\n**post rebuttal**\n- The reviewer thanks the authors for a detailed response. The reviewer has carefully read all the other reviews and corresponding responses.\n- The reviewer strongly opines that the draft needs to be improved w.r.t. the clarity of presentation clarifying all the issues raised by the reviewers. Moreover, the discussion and the experiments presented do not strongly convince about the effectiveness of the proposed framework against adversarial attacks. The reviewer reckons that the authors' notion of a proof-of-concept level of work needs to be substantially supported (with the points mentioned by the reviewers) to be discussed at a venue such as ICLR."
            },
            "questions": {
                "value": "- The draft reads that to sample from the generative model in the proposed framework, the auxiliary encoder $\\mathcal{D}$ is unnecessary. However, it appears that the sampling process needs the encoder's output - which is driven by both the auxiliary encoders $\\mathcal{C}$ and $\\mathcal{D}$ -  concatenation of $z_i$ and $c'$. Authors may provide a clarification for this.\n- In section 4.5, it is mentioned that the proposed approach - instead of including the conditioning on tensor, which is customary to the conditional VAEs- designed a DGDN-based decoder. However, the schematic representation in Figure 1 depicts the decoder being conditioned on the output of the auxiliary encoder $\\mathcal{C}$. Authors may provide clarity on this discrepancy.\n- How is it different to learn the purifier from the feature space of an adversarial trained classifier than learning from that of a normally trained classifier?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3670/Reviewer_T2NZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3670/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698989496919,
        "cdate": 1698989496919,
        "tmdate": 1700633788598,
        "mdate": 1700633788598,
        "license": "CC BY 4.0",
        "version": 2
    }
]