[
    {
        "id": "lO3ryqdRrM",
        "forum": "BqHaLnans2",
        "replyto": "BqHaLnans2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_MXmf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_MXmf"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed an image-text alignment framework for chest X-ray images and report pairs based on LLM models. In addition to the existing vision adapter, the authors reconstruct the adapted vision representation back to the images.  Furthermore, VQA pairs (as image-instruction-answer) are generated using GPT3.5 from the associated reports as a form of data augmentation for the image report pairs.  The reconstruction module is pre-trained as a VQGAN and then frozen when tuning the VQA instructions. The MIMIC-CXR dataset is employed here for the experiments. Superior results of the proposed model are reported in comparison to previous LLM models in report generation and VQA. However, the presented work suffers from several critical flaws that are detailed below."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Tackling the image-text alignment problem in medical imaging, which is not well-researched yet\n+ The manuscript is overall easy-to-follow"
            },
            "weaknesses": {
                "value": "- The authors claim the proposed bidirectional LLM is different from previous ones, as illustrated in Figure 1. However, I found the difference between them (a) and (c) is really minor. The encoder and decoder parts in the VQGAN are indeed equivalent to the vision adapter and image generative model, as listed in (a). Therefore, it is a bit overclaimed that it is novel to introduce bidirectional reconstruction tasks(both image-to-text and text-to-image) in the pre-training.\n- The motivation for training an image-text aligned model is not clearly introduced and justified. First, it will be helpful to discuss how this model could be applied to the downstream clinical tasks. Then, the designed experiments only demonstrate the performance of the proposed method on these instruction tuning tasks, i.e., report generation based on images and VQA. It is hard to appreciate the benefit of adopting such a pre-trained and then SFTed model in practical use without demonstrating downstream applications. There are many chest X-ray benchmarks, and datasets are commonly used for the evaluation of pre-trained models, e.g., disease classifications and localizations. \n- The metrics utilized in the experiments for most tasks are not the commonly used ones, e.g., AUCROC/F1 and Jaccard similarity index for report generation, FID alone for image generation, and accuracy alone for VQA. \n- In the results of report generation, only LLM-based methods are compared. How about a dozen of those SOTA  methods in chest X-ray reporting? IU X-ray is another dataset commonly used for the evaluation of report generation performance. It will be helpful to report results on that as well, mainly when used as a cross-domain evaluation dataset.  \n- I am not sure why the upsampling is performed for the image data as described in section 3.1 since the X-ray images are much larger than this resolution. How this upsampling process will affect the results?\n- In section 2.2, the authors mentioned that the image tokens are parts of expanded embedding tables. I wonder how big K_img should be?"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768654983,
        "cdate": 1698768654983,
        "tmdate": 1699636889039,
        "mdate": 1699636889039,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TNPLXVrCdU",
        "forum": "BqHaLnans2",
        "replyto": "BqHaLnans2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_DPiA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_DPiA"
        ],
        "content": {
            "summary": {
                "value": "This work developed an instruction-finetuning method to integrate visual information into out-of-the-box LLMs, which can be used for bidirectional multi-modal CXR tasks, such as CXR report generation, VQA, and report-to-CXR generation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The image is tokenized by VQ-GAN, ensuring that clinical information is preserved. It is an efficient use of existing resources and knowledge by expanding pre-trained LLM's embedding space to include image tokens.\n- The bidirectional instruction fine-tuning maintains the integrity of the LLM's structure and objectives while expanding its capabilities, which have many applications in the field."
            },
            "weaknesses": {
                "value": "The interpretability and scalability are not discussed."
            },
            "questions": {
                "value": "- How scalable is the tokenization and fine-tuning process for larger medical images (e.g., 3D CT/MR scans)?\n- How interpretable are the model's decisions, especially given the clinical context where explanations for predictions are crucial?\n- The comparison with other related methods is missed. https://github.com/chaoyi-wu/RadFM"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782802010,
        "cdate": 1698782802010,
        "tmdate": 1699636888919,
        "mdate": 1699636888919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QNBuA1wLeW",
        "forum": "BqHaLnans2",
        "replyto": "BqHaLnans2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_FRze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_FRze"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into enhancing Large Language Models (LLMs) with vision-language capabilities, specifically targeting medical imaging like chest X-rays (CXR). Recognizing that current \"adapter network\" methods might limit the deep integration of visual and language features, the authors propose a novel \"instruction-finetuning\" method. Drawing from vision-language pretraining (VLP) techniques, they tokenize images using VQ-GAN, facilitating the generation of combined text and image sequences. Rather than building a new model, they finetune a pretrained LLM with diverse CXR-related instructions. This approach allows the LLM to understand and generate visual data without structural modifications. Their finetuned LLM showcases proficiency in tasks such as translating CXRs to reports and vice versa, and performing CXR-specific visual question answering. The model not only outperforms specialized models in these tasks but also demonstrates the potential of seamlessly integrating visual and language abilities in LLMs for medical applications."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n1. **Novel Approach**: The paper introduces a novel \"instruction-finetuning\" method, which is a significant departure from the prevalent \"adapter network\" techniques. This innovative approach allows for a more intimate integration of visual and language features in LLMs.\n\n2. **Leveraging Existing LLMs**: Instead of starting from scratch, the authors smartly utilize the inherent instruction-following abilities of pretrained LLMs. This approach is efficient and maximizes the potential of existing models.\n\n3. **Broad Application**: The finetuned LLM exhibits versatility in handling a range of tasks, from converting CXRs to reports, generating CXRs from textual reports, to CXR-specific visual question answering. This breadth of application showcases the model's potential in real-world medical scenarios.\n\n4. **Outperformance**: The paper demonstrates that their model surpasses other specialized models in various tasks. This comparative analysis underscores the efficacy of their approach.\n\n5. **Seamless Integration**: By using VQ-GAN for image tokenization, the authors ensure a smooth integration of image and text token spaces without necessitating structural changes to the base LLM. This seamless integration is crucial for practical implementations."
            },
            "weaknesses": {
                "value": "Areas for improvement:\n\n1. **Potential for Catastrophic Forgetting**: As with any endeavor to expand a pretrained model's capabilities, there's the inherent risk of the model losing or diminishing its foundational skills\u2014known as catastrophic forgetting. While the paper's intent is to preserve and build upon the LLM's language capabilities, it doesn't extensively address measures taken to prevent this potential degradation.\n\n2. **Adapter Network Dismissal**: The paper critiques the prevalent \"adapter network\" approach but doesn't provide a comprehensive empirical comparison. A deeper, side-by-side evaluation highlighting performance, adaptability, and computational efficiency would offer a clearer picture of why their \"instruction-finetuning\" method is superior or preferable.\n\n3. **VQ-GAN Limitations**: Using VQ-GAN for image tokenization introduces its own set of challenges. VQ-GANs, while powerful, can sometimes produce artifacts or representations that aren't entirely faithful to the original image. The paper doesn't discuss how it mitigates or addresses these potential shortcomings, leaving room for questioning the quality or accuracy of the generated content.\n\n4. **Lack of Diverse Evaluation**: The paper's evaluation, though rigorous, might benefit from a more diverse set of metrics. For instance, qualitative evaluations or user studies involving medical professionals could provide insights into the model's practical utility. Comparisons with human diagnostic capabilities might also offer a benchmark for the model's proficiency.\n\n5. **RoentGen AUC and FID numbers**: The authors need to address the discrepancy between the AUC and FID numbers provided in their paper and those in the RoentGen paper. It should be clarified whether these numbers were reproduced by the authors or cited from the RoentGen study, and if there are differences, the paper should explain the reasons behind these.\n\n6. **Minor edits**:\n\na. Grammar throughout the text requires attention for better readability, as exemplified by the construction of the second sentence in section 2.2.\n\nb. The term \"txv-all-1024\" utilized in the text should be clearly defined. It is presumed to refer to the 1024-dimensional outputs from a densenet-121 model trained on chest X-ray classification, but this assumption needs confirmation in the paper for clarity."
            },
            "questions": {
                "value": "If the authors address the areas of improvement, I can reconsider my assessment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Reviewer_FRze"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822886656,
        "cdate": 1698822886656,
        "tmdate": 1700664995570,
        "mdate": 1700664995570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XYFDQagnpl",
        "forum": "BqHaLnans2",
        "replyto": "BqHaLnans2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_xBeK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_xBeK"
        ],
        "content": {
            "summary": {
                "value": "The manuscript introduces an instruction-tuning technique geared towards amplifying the image comprehension and generative capabilities of a text-exclusive LLM for CXR imagery interpretation and generation. The outcomes indicate a top-tier performance in both image-text understanding and generative capacities, surpassing earlier models in the field."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method of instruction-finetuning effectively elevates the LLM's capability to intricately map vision-image modalities. Consequently, it markedly excels in tasks like CXR-to-Report generation, CXR-VQA, and Report-to-CXR generation, outshining the open-sourced versions of models such as XrayGPT and UniXGen."
            },
            "weaknesses": {
                "value": "1. The paper's premise relies on the fine-tuning of LLM across multiple instruction tuning tasks. This approach isn't particularly novel, as numerous large-scale medical models adopt a similar strategy but with a broader functional range. Both the innovation in method and the paper's applicative contribution seem to be lacking.\n2. The LLM-CXR, when compared with the open-sourced versions of XrayGPT and UniXGen, appears to be unfair. It's noted that these models aren't trained on a consistent instruction tuning dataset. It's recommended that a uniform dataset is utilized for training before making such comparisons. Furthermore, there's an evident absence of comparison with existing LLM+instruction tuning models, such as Med-PaLM and Med-PaLM 2.\n3. The paper's comparative methods across various understanding and generative tasks aren't comprehensive. For instance, in the realm of report generation, several contemporary methodologies exist, such as \"Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation, CVPR 2023.\"\n4. The evaluation metrics used for report generation seem lacking in depth. For instance, within the paper (table 1), only the AUROC scores across six categories are reported, while prior research typically reports the average Precision/Recall/F1-score across all 14 categories.\n5. The paper's writing style isn't fluid and contains multiple errors, hindering smooth reading and comprehension. For instance, repetitive phrases like \"\u2026 generate these VQAs as shown as shown in \u2026\" and grammatical mistakes like \"During the fine-tuning process \u2026\" detract from the paper's clarity."
            },
            "questions": {
                "value": "Methodology Concerns:\n1. How does the instruction-finetuning method differentiate itself from existing large-scale medical models that employ a similar strategy?\n\nComparative Analysis:\n1. Is the comparison between LLM-CXR and the open-sourced versions of XrayGPT and UniXGen made on a consistent dataset?\n2. Why hasn't the paper compared its methodology with existing LLM+instruction tuning models, such as Med-PaLM and Med-PaLM 2?\n3. Are there reasons for not exploring contemporary methodologies in report generation, like \"Dynamic Graph Enhanced Contrastive Learning for Chest X-ray Report Generation, CVPR 2023\"?\n\nEvaluation Metrics:\nWhy were the AUROC scores for report generation only presented for six categories instead of the standard 14 categories?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The MIMIC Data Use Agreement explicitly prohibits sharing access to the MIMIC data with third parties, including sending it through APIs provided by companies like OpenAI, or using it in online platforms like ChatGPT. (https://physionet.org/news/post/415)"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Reviewer_xBeK",
                    "ICLR.cc/2024/Conference/Submission7413/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698978399987,
        "cdate": 1698978399987,
        "tmdate": 1700703516548,
        "mdate": 1700703516548,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1yzlaEsLKP",
        "forum": "BqHaLnans2",
        "replyto": "BqHaLnans2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_3hh7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7413/Reviewer_3hh7"
        ],
        "content": {
            "summary": {
                "value": "This study investigates the issue of multi-modal data alignment in large language models, using medical images and reports. \n\nIn terms of contributions, the authors propose a bidirectional reasoning generation mechanism that encodes images into tokens, allowing large language models to process and generate both text and images. In model training, a two-stage fine-tuning method is utilized, which not only captures the latent feature distribution of images but also imposes constraints on the model's representation of high-quality samples. \n\nOverall, this paper offers new perspectives on the processing of multi-modal data in large language models, demonstrating their value in the field of medical image processing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The author demonstrates the potential of large language models in radiological diagnostics, offering greater flexibility than previous methods in generating diagnostic reports or creating images.\n\n2. Inspired by VQ-GAN, the author ingeniously encodes images into tokens and integrates these image tokens into the large language model (LLM) for fine-tuning, achieving alignment of language and image features in the feature space.\n\n3. The author defines four tasks in this paper, particularly emphasizing the use of the CHATGPT API for Visual Question Answering (VQA), which further strengthens the association between images and text in the feature space. This part of the design is very interesting.\n\n4. During training, the dataset is cleansed, with initial learning of image latent features using low-quality data; the LLM is fine-tuned using the cleansed data."
            },
            "weaknesses": {
                "value": "1. The experiments in this paper were conducted using only one dataset. As the author mentioned, the quality of the dataset used was limited, necessitating adjustments in the training strategy.\n\n2. The paper lacks visualization of the alignment of images and text in the feature space. Particularly when using VQA for data augmentation, the questions posed by GPT contain rich prior information, focusing the text more on the key lesion areas in the images. I believe that appropriate visualization is necessary to demonstrate the effectiveness of incorporating VQA."
            },
            "questions": {
                "value": "1. This paper demonstrates the potential application of LLMs in the medical field, but the content of the study extends beyond generating diagnostic reports from radiological images. I hope the author could further clarify in the introduction whether there are specific application scenarios for this research.\n\n2. In '2.1 CLINICAL INFORMATION-PRESERVING CXR TOKENIZATION', the author mentions '...causes loss of clinically important information such as characteristics of microscopic lesions...'. However, in this study, generating image tokens is key to aligning images with text during LLM fine-tuning. Is the mere use of L2 reconstruction loss sufficient to effectively reduce the loss of clinical information? Could there be an enhancement of features in the design of the module network structure? I think this is a very important challenge in this paper, and I hope the difference in experimental results before and after the introduction of L2 reconstruction loss can be explained."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7413/Reviewer_3hh7"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7413/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699126428891,
        "cdate": 1699126428891,
        "tmdate": 1699636888565,
        "mdate": 1699636888565,
        "license": "CC BY 4.0",
        "version": 2
    }
]