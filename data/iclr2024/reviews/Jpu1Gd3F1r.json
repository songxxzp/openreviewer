[
    {
        "id": "YWhpkyZpI5",
        "forum": "Jpu1Gd3F1r",
        "replyto": "Jpu1Gd3F1r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_UxXs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_UxXs"
        ],
        "content": {
            "summary": {
                "value": "The article addresses data imputation in a supervised classification setting. More precisely, the proposal frames the contribution within kernel-based approaches. By using weak assumptions on the similarity between the instances, the Gram matrix can be estimated so that the resulting classifier performs well with respect to the replacements made. Then, missing values can be identified using the obtained Gram matrix. The paper first presents the context and introduces the setting. Then, related works are briefly presented. The two-stage data imputation strategy is then detailed, before experiments are reported to study the properties of the proposal, and a short conclusion is drawn."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is overall well written, in a clear and understandable way. The proposal is well described. \n\nThe contribution is sound."
            },
            "weaknesses": {
                "value": "Some key points (in particular, how parameters can be set) are not addressed. \n\nAlthough technically sound and rational, the proposal could benefit from a deeper theoretical justification. \n\nThe results only mildly support the claim that the approach is superior to the others: they are often (but not always) better, but by a small margin only (and the difference cannot be deemed significant), and they are reported over four datasets only."
            },
            "questions": {
                "value": "General comments and questions: \n\nThe presentation of existing works is somehow a bit short: many works on learning from missing data or classification from missing data were not mentioned. There is no discussion on the nature of the missingness process here. The data seem to be considered as missing at random. \n\nIt seems that the approach detailed in Section 3.2 somehow corresponds to an adversarial optimization of the classifier (which should perform well \"on all possible outcomes within a norm sphere surrounding $\\tilde{\\mathbf{K}}$\"): can you elaborate on that ? \n\nThe remark regarding the results obtained for \"extreme\" values of $\\gamma$ seem somehow obvious: the cases covered either correspond to instances being all dissimilar ($\\gamma=1/32$), or similar ($\\gamma=32$), hence the results. This raises the question of the sensitivity of your approach to the choice of $\\gamma$\u2014or, more generally, to the model parameters, the choice of which appears to be crucial, and actually very difficult to make without strong assumptions. Could you elaborate on that ? \n\nThe results do not seem to be significantly different between the various imputation approaches compared, the only exception being the Australian dataset, with $m=80\\%$. Do you have any insights regarding this ? \n\n\nMore minor comments and questions: \n\nIt is not clearly stated whether the missing values are in the training or test data (or both). \n\nDoes ignoring the PSD constraint to solve the problem and then projecting back onto the space of PSD matrices have an impact on the result obtained ? \n\nShould Step 2 be skipped in the first phase, what would be the outcome of the proposed strategy ? It seems that this amounts to implicitly assume that the data are \"perfect\": can you provide any insights regarding this ? \n\nI do not understand why $\\varepsilon^*$ should be positive definite (phase I, step 2, Equation (9), page 6). For complete consistency with Stage I, shouldn't $\\varepsilon$ (or $\\varepsilon^*$) be used as well in Stage II ? \n\n\nSome suggestions on writing: \n\n- page 1, \"we typically use subsets of indices [...] with $\\bfseries{x}_{\\bfseries{o}_i}^i$\": clearly define these notations; \n- page 1, \"the importance of labels has not been fully taken into account\": this statement, somehow a bit assertive, is difficult to understand; \n- page 2, \"since there is $N-1$ supervising information available for each data\": can you clarify ? \n- page 4, Section 3.1, paragraph \"Notations\": sentences should not begin with a mathematical symbol; \n- page 4, Section 3.2: matrix $\\mathbf{K}_\\Delta$ is not properly introduced; \n- page 5, \"is a semi-definite programming\": should be \"is a semi-definite program\"; \n- page 6, $m$ does not seem to be properly introduced; \n- page 7, \"including \\textit{australian}, [...]\": should be \"namely \\textit{australian}, [...]\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Reviewer_UxXs",
                    "ICLR.cc/2024/Conference/Submission7804/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770307394,
        "cdate": 1698770307394,
        "tmdate": 1700582265868,
        "mdate": 1700582265868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gpsH9T4r1M",
        "forum": "Jpu1Gd3F1r",
        "replyto": "Jpu1Gd3F1r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_VYgf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_VYgf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a two stage procedure for dealing with missing data targeted towards classification.  In the first stage, the authors propose a method which jointly finds a Kernel matrix K_{\\Delta} and solves a dual SVM formulation.  Then, in the second stage, the authors use the Kernel matrix K_{\\Delta} \\odot K_o to perform data imputation via solving a non-convex optimization problem provided in Eq. (10)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Both stages of the proposed procedure are interesting and novel.  Moreover, the experimental results are promising.  The writing is also quite clear."
            },
            "weaknesses": {
                "value": "The main weakness lies in the choice of experiments and settings.  In particular, the authors consider a MCAR (missing completely at random) set up for the experiments in which they induce the missingness pattern in the data.  I do appreciate the results in Table 3 and the differentiation based on the amount of missing data, however, it would be stronger to include further validation."
            },
            "questions": {
                "value": "Would it be possible to include an experiment and compare the different methods on a dataset with missing data in which the missingness is not induced artificially? For instance, taking a dataset with missing entries but for which there are sufficiently many labels to train your method and the other methods to which you compare.  This would help to understand the method you propose beyond the artificial MCAR setting, which would be important.  I would be willing to raise my score if an experiment of this sort were to be included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Reviewer_VYgf",
                    "ICLR.cc/2024/Conference/Submission7804/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787092015,
        "cdate": 1698787092015,
        "tmdate": 1700555470343,
        "mdate": 1700555470343,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tLw02wyMFA",
        "forum": "Jpu1Gd3F1r",
        "replyto": "Jpu1Gd3F1r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_h2QN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_h2QN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new kernel-based method for dealing with data missing at random. Their basic idea is to estimate them through the learned imputed kernel matrix. Their main novelty compared to previous papers is in making use of unknown features as well as observed ones while learning this kernel. Once the imputed kernel matrix is obtained, the missing values are estimated numerically by minimizing\nminimize the discrepancy between the imputed kernel matrix calculated from the imputed data. The proposed method is compared with 4 other strong baselines on 4 benchmark data sets. The results indicate that the proposed approach could be better than the baselines particularly when the proportion of the missing data is large."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is generally well written and easy to follow\n+ The proposed approach is well justified and explained clearly enough\n+ The experimental results indicate that the proposed approach might be better than the baselines"
            },
            "weaknesses": {
                "value": "- This paper is mostly an incremental contribution compared to the state of the art\n- There is a long history of research on imputation of data missing at random. Thus, comparing to only 4 other baselines on 4 small data sets (both in number of examples and features) might not be comprehensive enough. It is not clear why those particular 4 data sets were selected (other than being very small).\n- Looking at the error bars, for most of the results the improvement does not seem to be statistically significant"
            },
            "questions": {
                "value": "In addition to the previous comments, it would be useful to show the computational cost for the performed experiments. The largest data set used in the experiments has only 1000 examples. Is this because the proposed method is too expensive to run on larger data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7804/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986745324,
        "cdate": 1698986745324,
        "tmdate": 1699636953843,
        "mdate": 1699636953843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vqSi8XKEXX",
        "forum": "Jpu1Gd3F1r",
        "replyto": "Jpu1Gd3F1r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_Jet1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7804/Reviewer_Jet1"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle supervised learning with missing values, using support vector machines (SVMs).\n\nTo this end, they design a new way to learn a kernel matrix of an incomplete data set, by optimising the SVM loss (what is called \"Stage I\" in their algorithm). After this kernel has been learned, they use it to impute the data set by minimising the squared error between the kernel of the imputed data set and the learned kernel (this is \"Stage 2\").\n\nThey do several experiments on four real classification data sets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The two stages of the algorithms both involve quite clever ideas.\n\nThe first stage in particular is based on the idea of treating the terms of the full kernel matrix that depend on incomplete data as parameters to be optimised. This is an excellent idea that is, to the best of my knowledge, novel.\n\nThe main idea behind the second stage is less innovative but very sensible. I also appreciate the fact that this \"stage II\" is empirically investigated on its own in Section 4.2.1."
            },
            "weaknesses": {
                "value": "Main concerns\n\n1) My main concern is related to the experiments, that have several issues, in my opinion.\n\na) Studying only 4 small data sets ($N\\leq 1,000$) is not particularly compelling, especially given that most standard deviations are quite important (in Table 3, I doubt the author's technique is statistically significantly better than mean imputation in most scenarios).\n\nb) Mean imputation appears to be, by far, the best method (if we ignore the author's method). This is not very consistent with the literature. For instance, in the genRBF paper (Smieja et al., 2019), genRBF is on par or better than the mean on the \"Australian\" data set, while it is much worse than the mean in this submission. Similarly, in the GEOM paper (Chechik et al., 2008), GEOM is essentially always on par with the mean, and is generally worse in this submission.\n\n2) The authors do not study the theoretical properties of their methods. In particular, the assumptions on the missingness mechanism are not discussed. All experiments use missing completely at random (MCAR) data, and the authors do not discuss this experimental design choice. Studying (empirically and/or theoretically) whether or not this algorithm works on non-MCAR data would be interesting.\n\nSecondary concerns\n\n3) The paper read generally well, but the mathematics are at times quite unclear. Several objects are not properly defined and some facts are not really proven, for instance\n- I imagine $K_0 = \\exp ( - \\gamma \\sum_{p \\in o_i \\cap o_j} (x^i_p - x^j_p)^2 )$, but $K_0$ is never defined,\n- in Equation (3), the mathematical meaning of $ (x^i_p - *) $ is unclear\n- why is $K^*_\\Delta$ in Equation (6) in the proper range (between $B_l$ and $B_u$)? After clipping, you project on positive semidefinite matrices, why is it guaranteed that it would be still in the right range?\n\nMinor things\n\n- I find it a bit odd to call $\\mathcal{E}$ a \"noise\", since it not something random but something that you optimize\n- I also find the phrase \"imputed kernel matrix\", used a few times (in different forms) a bit odd: this matrix is always a complete matrix, it is the dataset used to build it that needs to be imputed\n- There has been a significant amount of work on supervised learning with missing values recently. Some of these papers could be interesting to discuss, for instance:\n\nJosse et al., On the consistency of supervised learning with missing values, arXiv:1902.06931, 2020\n\nLe Morvan et al., What\u2019s a good imputation to predict with missing values? NeurIPS 2021\n\nBertsimas et al., Beyond Impute-Then-Regress: Adapting Prediction to Missing Data, arXiv:2104.03158, 2022"
            },
            "questions": {
                "value": "- see questions in the \"Weaknesses\" section, point 3\n\n- In your experiments, I did not understand if you used as a final classifier SVM with $\\tilde{K}$ as a kernel matrix, or with the kernel matrix of the imputed data set ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7804/Reviewer_Jet1"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7804/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699647723265,
        "cdate": 1699647723265,
        "tmdate": 1699647723265,
        "mdate": 1699647723265,
        "license": "CC BY 4.0",
        "version": 2
    }
]