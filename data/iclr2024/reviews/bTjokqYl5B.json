[
    {
        "id": "ltFdDbnb1V",
        "forum": "bTjokqYl5B",
        "replyto": "bTjokqYl5B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_BeRL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to mitigate overfitting in adversarial training. The proposed method changes the magnitude of adversarial attacks in adversarial training and applies AugMix to the small-loss examples within a minibatch if the proportion of the small-loss examples is below the specified threshold. Experiments demonstrate that the proposed method improves the performance of AT, AWP, TRADES, and MLCAT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper addresses an important problem: overfitting in adversarial training because adversarial training suffers from overfitting more than standard training.\n- Experiments use AutoAttack, which is a de-fact standard evaluation method, and several baselines are used.\n- Figure 1 shows somewhat interesting results. Comparing removing small-loss adversarial examples with removing only the perturbation of small-loss adversarial examples is an interesting investigation from a new aspect. However, this result does not lead well to the proposed method."
            },
            "weaknesses": {
                "value": "- There are a lot of undefined words: e.g., non-effective features and learning state. Since these words are frequently used in the analysis for developing the proposed method, readers could not understand why the proposed method is effective to mitigate overfitting.\nNon-effective features and learning state should be defined by using equations and empirically evaluated or theoretically evaluated in the existence of adversarial training.\n- Most parts of claim do not have objective evidence. For scientific articles, most of claims should be supported by the evidence. For example, the following states do not supported by the experimental or theoretical results:   \n_\"In the initial stages of adversarial training, due to the similarity in the model\u2019s learning states between the training and test datasets, the boundary between the robust and non-robust features doesn\u2019t significantly differ between the training and test sets. \"_  \n_\"However, the improvement in the model\u2019s learning state on the test dataset is relatively limited, far from matching the model\u2019s learning state on the training dataset\"_  \n_\" As a result, the boundary between the robust and non-robust features becomes progressively more distinct between the training and test sets.\"_  \n_\"adversarial data with small loss indicates that the model\u2019s learning state on these data is excellent, maintaining a substantial gap compared to the learning state on the test set.\"_  \nI suggest you to provide more empirical results or theoretical results that support your claims and the effectiveness of the proposed method.\n- The proposed method is not clearly written, and its explanation does not have equations or pseudo codes.\nReaders cannot reproduce the results.\nI could not understand how to control the attack strength in the proposed method and how to use data augmentation.\nWhat value of the loss do you call small loss for small-loss adversarial data?\nWhat is the specified threshold for the proposed method?\nRegarding changing adversarial budgets in adversarial training, [a] might be related work, which schedules adversarial budgets for considering loss landscapes.\n\n[a] Liu, Chen, et al. \"On the loss landscape of adversarial training: Identifying challenges and how to overcome them.\" Advances in Neural Information Processing Systems 33 (2020): 21476-21487."
            },
            "questions": {
                "value": "- Do you have any evidence that supports your claims as witten in Weakness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698487000318,
        "cdate": 1698487000318,
        "tmdate": 1699636145709,
        "mdate": 1699636145709,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OlSeaXz5A2",
        "forum": "bTjokqYl5B",
        "replyto": "bTjokqYl5B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_6tzX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_6tzX"
        ],
        "content": {
            "summary": {
                "value": "Summary: \nThis study is dedicated to investigating the fundamental mechanism of robust overfitting in adversarial training. First, the robust overfitting is attributed to the non-effective features that hinder the model from learning generalization ability. Then, the study proposes OROAT with attack strength and data augmentation to alleviate learning on non-effective features. Experiments validate the robustness of OROAT across several adversarial training methods to counter different attacks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "pros:\n1. Provide an innovative view to separate adversarial training data into normal and small-loss adversarial data. And the method of composition that first dives into the problem and then proposes a solution is appealing.\n2. Introduce a plug-and-play method that is experimented with many adversarial attack and training methods."
            },
            "weaknesses": {
                "value": "cons:\n1. Chapter 3 lacks experimental results to support analysis. For example, the study mentions the gap between training and test learning state several times, which would be better accompanied by a figure illustrating the difference of robust overfitting between training and test data.\n2. The writing of analysis and method is too lengthy, whereas the experiment part is too condensed. These sections have to be reorganized to alleviate the reading burden.\n3. Experiments are solely conducted on CIFAR10 and CIFAR100. Larger datasets like ImageNet should be explored.\n4. The ablation studies show that the effect of OROAT turns negative towards adversarial robustness and possible reasons. How to avoid this situation needs explanation. Additionally, Table2 needs to highlight the results that perform best or correspond to the argument in texts.\n5. Lack comparisons with existing mitigations of robust overfitting. The authors has included various previous works in the related works/revisiting section, which is good. However, there is no empirical comparison."
            },
            "questions": {
                "value": "Refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698528377372,
        "cdate": 1698528377372,
        "tmdate": 1699636145639,
        "mdate": 1699636145639,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4EuO7eBvXc",
        "forum": "bTjokqYl5B",
        "replyto": "bTjokqYl5B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_sV3q"
        ],
        "content": {
            "summary": {
                "value": "In this paper, robust overfitting, an interesting and important phenomena in adversarial training, is investigated. The main conclusion is robust overfitting is a result of learning non-effective features, which also leads to new enhancement method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Since robust-overfitting is a specific phenomena in adversarial training. Indeed, it should consider the difference of natural example and adversarial perturbation, as the authors did. To observe the difference, they design good ablation experiments, which indeed could bring new thoughts."
            },
            "weaknesses": {
                "value": "- the main conclusion that overfitting is because of learning non-effective features is too trivial. It may be true for any type of overfitting but specifically suitable for adversarial training. \n\n- as I said, it is good to design interesting experiments to find something. But still it is better to also include theoretical discussions, especially on the specific properties for adversarial training.  \n\n- the methods derived from the main conclusion is not interesting. Data augmentation is almost the most natural way to suppress overfitting. Attack strength adjustment is also common for adversarial training. For example. PGD-based on AT can be regarded as adaptive attack adjustment.\n\n- It is OK if the authors choose to evaluate the proposed method numerically. However, the experiments should be enhanced largely. The performance should be compared not only to vanilla adversarial training but also other robust overfitting suppression method. Notice that the training time should be reported."
            },
            "questions": {
                "value": "please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698643860988,
        "cdate": 1698643860988,
        "tmdate": 1699636145546,
        "mdate": 1699636145546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6bc9ByiAe9",
        "forum": "bTjokqYl5B",
        "replyto": "bTjokqYl5B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_H4Yh"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of robust overfitting in adversarial training and seeks to understand and mitigate it through empirical analysis. They conduct factor ablation experiments in adversarial training and conclude that robust overfitting stems from the normal data. They explain the onset of robust overfitting is due to the learning of non-effective features during adversarial training, and revisit different techniques for mitigating robust overfitting from this perspective. Based on these insights, they propose two methods based on the attack strength and data augmentation to suppress the learning of non-effective features, and thereby reduce robust overfitting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses robust overfitting in adversarial training, which is an important problem and not fully understood.\n\n- The discussion of the phenomenon of robust overfitting leading to section 3.2 is clear and well motivated."
            },
            "weaknesses": {
                "value": "The central discussions of the paper are vague and mainly intuitive in nature. There are no concrete equations or an algorithm for the proposed methods based on attack strength and data augmentation. Furthermore, there is no analysis to support the claims about robust overfitting.\n\nThe discussions in section 3.2 and 3.3, which form the crux of the paper, are not clear. For instance, in the following statements (in Section 3.2.1, page 5), what is meant by the similarity of the model\u2019s learning states on the training vs test data? The paper should explain this in a more principled, mathematical way. \n\n> \u201cIn the initial stages of adversarial training, due to the similarity in the model\u2019s learning states between the training and test datasets, the boundary between the robust and non-robust features doesn\u2019t significantly differ between the training and test sets.\u201d\n\n> \u201cAdversarial data with small loss indicates that the model\u2019s learning state on these data is excellent, maintaining a substantial gap compared to the learning state on the test set.\u201d"
            },
            "questions": {
                "value": "1. In Eqn (4), please clarify that the max is over the perturbation $\\delta_i \\in \\Delta$, and that $x^\\prime_i = x_i + \\delta_i$.\n\n2. For the factor ablation experiments in Section 3.1 and Figure 1, are the results averaged over a few trials to account for randomness?\n\n3. Can you concretely define \"effective features\" and the idea of \"similarity of a model's learning states between the training and test data\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699482953817,
        "cdate": 1699482953817,
        "tmdate": 1699636145463,
        "mdate": 1699636145463,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Y54t5i2tuH",
        "forum": "bTjokqYl5B",
        "replyto": "bTjokqYl5B",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_MKb4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2127/Reviewer_MKb4"
        ],
        "content": {
            "summary": {
                "value": "- The paper investigates the causes and mechanisms behind robust overfitting in adversarial training (AT). Robust overfitting refers to the phenomenon where a model's robust test accuracy declines as training progresses.\n- Through factor ablation experiments, the authors show that the factors inducing robust overfitting originate from the normal training data, not the adversarial perturbations.\n- They explain robust overfitting as arising from the model learning \"non-effective\" features that lack robust generalization. Specifically, due to distributional differences between training and test data, features that are robust on training data may not generalize to be robust on test data.\n- As training progresses, the gap between the model's learning states on training vs test data widens. This facilitates the proliferation of non-effective features. When optimization is dominated by these features, robust overfitting occurs.\n- Based on this understanding, the authors propose two measures to regulate the learning of non-effective features: 1) Attack strength - using higher attack budgets to eliminate non-effective features. 2) Data augmentation - to align the model's learning state on training and test data.\n- Experiments show clear correlations between the extent of robust overfitting and the degree to which these measures suppress non-effective features. The proposed methods mitigate robust overfitting and improve adversarial robustness across different models and datasets.\n- Overall, the work provides an explanation of robust overfitting from the perspective of features and learning states. The understanding and analysis seem quite intuitive and comprehensive. The paper makes a valuable contribution towards demystifying the mechanisms behind this phenomenon."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality: The paper provides a novel perspective on understanding robust overfitting by treating normal data and perturbations as separate factors. The idea of non-effective features that lack robust generalization is also an original concept proposed in this work.\n\nQuality: The study is scientifically rigorous, with principled factor ablation experiments that isolate the effect of normal data. The analysis and explanations are intuitive yet comprehensive. The proposed methods demonstrate consistent effectiveness.\n\nClarity: The paper is very clearly written and structured. The background provides sufficient context. The experiments and results are well-described. The analysis logically builds up an explanation of robust overfitting in an easy to follow manner.\n\nSignificance: Robust overfitting is a major impediment in adversarial training that lacks a satisfactory explanation. This work makes significant headway by unraveling its underlying mechanisms. The insights can inform the design of more effective defenses. Overall, this is an impactful study that advances fundamental understanding of an important phenomenon in adversarial machine learning.\n\nIn summary, the originality of the conceptual framing, rigorous experimental methods, clear writing, and significance of the research problem make this a compelling paper with multiple strengths. It is a valuable contribution that sheds light on the mechanisms behind robust overfitting through a meticulous and insightful analysis."
            },
            "weaknesses": {
                "value": "- While effective, the attack strength and data augmentation measures may not represent optimal or sufficient solutions. More advanced techniques informed by this analysis could further enhance robustness.\n- The theoretical analysis relies on intuitive reasoning. Formalizing the notions of robust/non-robust features and quantifying the learning state gap could strengthen the conceptual framing.\n- The focus is on explaining robust overfitting, less on maximizing robust accuracy. Follow-up work could build on these insights to achieve state-of-the-art robustness.\n- The experiments primarily use simple CNN architectures on CIFAR datasets. Testing the analysis on larger datasets and SOTA models could reveal additional insights.\n- There is limited ablation on the proposed methods themselves. Varying their hyperparameters and components could better isolate their effects.\n- The writing could further improve clarity in some areas, like explicitly defining \"small-loss\" data earlier on."
            },
            "questions": {
                "value": "1. The analysis relies on the notion of a \"gap\" between training and test learning states. Is there a principled way to quantify this gap? Are there any theoretical bounds on the gap size that induces robust overfitting?\n2. Have you experimented with more advanced data augmentation techniques like MixUp or CutMix? Could these further help with state alignment and reducing non-effective features?\n3. How well do your insights transfer to larger scale problems like ImageNet? Are there any key differences in robust overfitting that you observe in such settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2127/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699501233047,
        "cdate": 1699501233047,
        "tmdate": 1699636145373,
        "mdate": 1699636145373,
        "license": "CC BY 4.0",
        "version": 2
    }
]