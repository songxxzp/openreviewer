[
    {
        "id": "FrkBOUQYKd",
        "forum": "vrjDNgAfp4",
        "replyto": "vrjDNgAfp4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_TBCQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_TBCQ"
        ],
        "content": {
            "summary": {
                "value": "The authors try to demonstrate the flow of ides in a document using random walks and mean square distance of representations of words, sentences or groups of words. Previous work in the field of psychology shows that the ability to connect semantically diverse topics are predictive of creativity. The authors use mean square distance to measure the difference between sentence embedding pairs as a function of time delay between sentences or words in a few novels. The MSD plot of popular novels the appears a have steep rise in beginning, flattens in the middle and rise again in the end. They then use introduce a random walker to find similar patterns in word embedding of language model. They extend the method to sentence embedding and add a variation in similarity random walks by using sets of words to guide the walk. The similarity walk shows that the flow is constrained to nodes, while occasionally jumping between clusters of nodes, and eventually converging to an obscure cluster with seemingly unrelated context to the document."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Language models are widely used in creative tasks, and yet there is not much evidence to show that they are creative. This paper attempts to show some evidence that language models are as creative by uniquely combining methods from psychology, microrheology and machine learning. \n- The paper is well structured, uses graphs to effectively visualize paths in random similarity walk, and clearly shows us the jumps across clusters. I find that they have used the right measure to characterize their hypothesis of flow of ideas. The methods are sound."
            },
            "weaknesses": {
                "value": "- It would be beneficial to first visualize the embedding space (of the different corpora used in this paper) in a lower dimension, to first show that  semantically similar sentences or words are close to each other. I think it is useful to also cluster the embedding space, give some examples of how these clusters contain semantically similar words or sentences, and then we can see how random walks could be jumping across clusters. \n- The graphs in section 2 need discussion. In Figure 2b the author tries to demonstrate the difference in similarity walks between common English words and random words. The figure shows nearly 8 lines, and it needs a legend with the words whose paths were plotted here.  Figure 3 needs more discussion., specifically, what are the three corpus being used here, and why were they chosen? How are the random walks different or similar? What are the clusters present in each corpus?\n- More details needed on the experiments in section [2]. Additional data is needed to make their claims about words converging into a cluster which is obscure and is unrelated to the stating word. \n- Figures 1 has multiple flaws. 1d does not show X-ticks, and the legend is incomplete. X-axis label in figure 1c is overlapping the X-axis. Furthermore, the corpus size(N) is not provided in any of the experiments. \n- This paper needs more content in the motivation section, why is studying the flow of ideas important?"
            },
            "questions": {
                "value": "- In Figure 2, how were common words determined?\n- In Figure 5a, what is the starting sentence used? It is hard to say from the graph where the start of this path is.\n- In Figure 5b, what do the different lines represent, and what are their significance?\n- Are there any downsides of using MSD in a high dimensional embedding space? Please discuss the downside of the current approch"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1769/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1769/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1769/Reviewer_TBCQ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1769/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631531759,
        "cdate": 1698631531759,
        "tmdate": 1699636106307,
        "mdate": 1699636106307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dYthdh2l3r",
        "forum": "vrjDNgAfp4",
        "replyto": "vrjDNgAfp4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_Cm35"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_Cm35"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an exploration of text data by using concepts from rheology to perform random walks in setence/word space."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper proposes an interesting idea to measure the creativity and randomness of LLMs by computing latent space distance."
            },
            "weaknesses": {
                "value": "The manuscript doesn't meet the standards of a conference paper.\n\n* The paper is clearly written in a rush (6 pages). For example, the Transformers paper isn't cited\n* While the paper proposes a method for understanding the flow of ideas in text, it doesn't appear to offer substantial empirical evaluation to validate its claims. In machine learning conferences, a rigorous empirical evaluation is usually a minimum requirement.\n* The paper doesn't explore where creativity has been analyzed by other methods in ML\n* While some popular literature is analyzed, it is not done in a formal way. I could imagine a strong version of this paper comparing the MSD between authors and between LLMs\n* The paper doesn't evaluate the creativity of LLMs\n* While the paper presents some mathematical equations, they do not appear to be deeply rooted in machine learning theory. Moreover, similarity in LLMs is often performed using cosine similarity and the MSD metric seems a little too inspired by Pearson, rather than rigorous.\n* The paper introduces many ideas and methods from different domains without sufficient background or explanation, making it difficult to follow.\n* I'm not sure what to take away from section 2. Word based generative processes have been studied extensively with markov chains and I'm not sure how they are relevant to LLM creativity.\n\nI will say, there is a strong version of this paper that introduces a new benchmark of creativity versus a wide spread of human authors, and compares some popular LLMs in their creative writing (leveraging the reddit writingprompts dataset perhaps). I encourage the authors to continue with their work in this area."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1769/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698649443741,
        "cdate": 1698649443741,
        "tmdate": 1699636106227,
        "mdate": 1699636106227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Zv1WZvymz",
        "forum": "vrjDNgAfp4",
        "replyto": "vrjDNgAfp4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_gMZx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_gMZx"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the flow of ideas in the word embeddings from Word2Vec and Transformer-based models by similarity walk in embeddings space. The authors claim that this work can help to understand the creativity ability of the modern language models and further help to produce more creatively generated text."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1.\tThe potential outcome of this research may help large language models to produce more creative texts than now. \n\n2.\tThis work is the first to consider and analyze creativity and the flow of ideas in modern language models."
            },
            "weaknesses": {
                "value": "1.\tMost of the conclusions are not explained and proved. 1) For a broader audience it is not clear why this paper claimed that some patterns in the behavior of the MSD curves are commonly observed in biological cells or complex fluids. 2) It is not clear what is the connection of the presented research and microrheology. 3) From the presented paper it is not clear why it stated that there is a correlation with the flow of ideas in language models and document creativity. \n\n2.\tIn Section 3 \"...in the supplementary csv files\" looks strange. It is better to somehow visualize this example or at least provide a link for these files otherwise most readers cannot access this data.\n\n3.\tMissed references. For example, in the first paragraph in Section 2.1.\n\n4.\tIt is not enough clear what is quidded similarity walk. \n\nOverall, I would be happy to see some mathematical formulation of the problem and maybe some ground truth in a form of a labelled dataset, current the work presents some visualizations but it is not entirely clear how to qualitatively evaluate and improve the proposed /envisioned technology. \n\n5.\tIt will be more interesting to consider in the main part of the paper large language models such as Llama 2."
            },
            "questions": {
                "value": "1.\tCan you explain in more detail what is quidded similarity walk is?\n\n2.\tThe paper claimed that for the Sentence Transformer, the process converged into a cluster of closely linked sentences. However, in Figure 5b the curve starts exponential growing. How you can explain this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1769/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889501858,
        "cdate": 1698889501858,
        "tmdate": 1699636106143,
        "mdate": 1699636106143,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "878RrKKpC7",
        "forum": "vrjDNgAfp4",
        "replyto": "vrjDNgAfp4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_aQUV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1769/Reviewer_aQUV"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to study the \u201cflow of ideas\u201d in text and language models via modeling similarity between sentences on documents and by performing random walks on the word and sentence embeddings spaces. The sentence embeddings distance is plotted against their relative distance in the document. Also, random walk based on cosine similarity is observed for word embeddings from word2vec model and sentence embedding from DistillBERT model."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\u2013 The attempt to connect behavior of embedding models to the flow of ideas and concepts in rheology is interesting.\n\n\u2013 Findings related to random walk over words and sentences converging to a cluster of interconnected hubs, while not surprising, is interesting."
            },
            "weaknesses": {
                "value": "\u2013 My main concern is that I am unable to understand the main claims of the paper. The experiments are unfocused and seem disconnected from the narrative in the introduction and conclusion. \n\n\u2013 Expanding on above, the narrative is vague and the experiments setup is unconvincing. For example, I am not sure how these experiments relate to creativity. Also, I didn\u2019t find a testable theory that is verified empirically against reasonable baselines.\n\n\u2013 Overall, I think the paper is low on substance. In the first experiments on novels, a baseline behavior is not established. Plots b) and d) in Fig. 1 seem to be showing different trends. Connections to literary analysis could be made in this section with a more indepth focus on how factors like genre, authors, stylometrics across a huge collection of books affect the behavior of sentence embeddings would improve the paper.\n\n\u2013 Similarly, the second part of the paper associated with random walks is confusing and low on substance and adequate comparison. For instance, there are a lot of different ways to compute word and sentence embeddings that have been formulated in natural language processing and representation learning research. For example, the MTEB benchmark is a meta-evaluation framework to evaluate different kinds of sentence embeddings. Yet this work focuses on just one model for word and sentence embeddings.\n\n\u2013 The presentation can be significantly improved. For example, Fig 2b), 5b) are unlabeled and it is difficult to understand what the different curves mean. Also, the discussion about the figures is not necessarily congruent with what the figures are showing.\n \n\u2013 In figure 1, MSD is plotted \u2013 it would be helpful to plot the standard deviation as well since the number of instances are different for different distances."
            },
            "questions": {
                "value": "Please see the review above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1769/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699259118035,
        "cdate": 1699259118035,
        "tmdate": 1699636106058,
        "mdate": 1699636106058,
        "license": "CC BY 4.0",
        "version": 2
    }
]