[
    {
        "id": "7eby8KEN9c",
        "forum": "ezscMer8L0",
        "replyto": "ezscMer8L0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_2Lzf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_2Lzf"
        ],
        "content": {
            "summary": {
                "value": "This paper is an improved version of LoRA (low-rank adaptation) that aims to adapt pre-trained Segment Anything Models (SAM) across a diverse array of semantic segmentation tasks. The proposed Conv-LoRA adds an extra convolution operation between the encoder and decoder. Freezing the pre-trained SAM, the encoder, decoder, and extra convolution are trainable to adapt SAM to downstream semantic segmentation tasks. The trainable part of this framework is lightweight. Compared with LoRA and other parameter-efficient fine-tuning approaches, the proposed Conv-LoRA shows better performance for semantic segmentation tasks in medical natural imaging, agriculture, and remote sensing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The manuscript is composed with clarity, presenting a concept that is both coherent and well-motivated.\n+ The experimental setting is clearly stated, and the authors have conducted comparisons with not only LoRA but also a wide array of baseline methods.\n+ The scope of experimentation is thorough. The proposed Conv-LoRA notably surpasses LoRA and a range of other methods aiming for efficient fine-tuning. It also appears to outperform full fine-tuning 100% parameters.\n+ It appears that incorporating convolution operations can improve the SAM features in terms of fine-grained information, such as slim edges, and semantic information (this might be due to fine-tuning)."
            },
            "weaknesses": {
                "value": "- Lack of comparison with training segmentation model from scratch (random initialization). It remains unclear whether the pre-trained SAM model helps transfer to downstream semantic segmentation tasks."
            },
            "questions": {
                "value": "1. The Mixture of Experts in introduced in the paper seems to be multi-scale strategies that are commonly used in image segmentation. It is unclear whether the added complexity of this method is warranted. A more straightforward explanation would likely render the technique more impactful and accecible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Reviewer_2Lzf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699110004972,
        "cdate": 1699110004972,
        "tmdate": 1699636037715,
        "mdate": 1699636037715,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qfpWgapXLJ",
        "forum": "ezscMer8L0",
        "replyto": "ezscMer8L0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_LN6g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_LN6g"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an improvement to the Segment Anything Model (SAM) by incorporating fittings to alleviate some of the original model's deficiencies, together with algorithmic novelties to improve performance. Improvements to SAM are claimed in specialized domains (e.g medical images) and in improving fine grained semantic predictions. Methods are proposed to build upon LoRA (Low Rank Approximation) from NLP, which is modified to incorporate a Mixture of Experts setup with convolutional processing (what they call conv-LoRA). This way, it is claimed that the model is able to aggregate signals from multiple experts that can learn image priors in a specialized way, all while learning with minimal computational overhead, or what they call parameter efficient fine tuning. A few other modifications are used to boost performance and usability (freezing prompts, allowing for a classification head for multiclass classification) over SAM. \n\nResults show improvements over other parameter efficient fine tuning models (e.g. VPT), with little overhead from the model's conv fittings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Solidly written paper, well motivated ideas for SAM improvement\n+ Novelty in the use of MoE\n+ Improvements in producing fine grained classification (e.g. edges), and in specialized domains"
            },
            "weaknesses": {
                "value": "- Purely empirical work. No reasoning is given as to how the model improves performance. \n- MoE is not well motivated. I looked up the original paper for insight, which clarifies things somewhat, but the paper in question should describe it better. \n- Performance improvement is very marginal."
            },
            "questions": {
                "value": "Ablations: I am curious about the effect of various model components on performance. \n- MoE looks a bit opaque. What priors is it learning, and how do we grasp what it is doing? \n- Can the authors give more reasoning for freezing the prompt encoder? I quite liked the prompt encoder in the original model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699141231131,
        "cdate": 1699141231131,
        "tmdate": 1699636037607,
        "mdate": 1699636037607,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5AvJDP6NlV",
        "forum": "ezscMer8L0",
        "replyto": "ezscMer8L0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_3rwQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_3rwQ"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a parameter-efficient fine-tuning approach, i.e., a Conv-LoRA module combining trainable convolutional parameters with MOE scheme. It is developed to overcome the SAM\u2019s performance drop when applied to specialized domains such as medical imagery and remote sensing. The Conv-LoRA module is integrated into the plain ViT encoder, enhancing SAM\u2019s local prior assumption and its ability to learn high-level image semantics. Several previous parameter-efficient fine-tuning approaches are included in the comparison study. The proposed method enables efficient adaptation (with superior results) to real-world semantic segmentation tasks across various benchmarks and domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ a novel LoRA-like add-on module for efficient parameter tuning for SAM, a typical large-vision model. \n+ Superior results of the proposed methods are reported in comparison to vanilla LoRA, VPT, and other adaptor-based methods."
            },
            "weaknesses": {
                "value": "- The motivation for introducing the combination of MOE with convolutional parameters as a couple is not clear to me. It is not clear how each of them will benefit the performance as an add-on to the vanilla LoRA.\n- The proposed method reminds me of the inception structure from the GoogLeNet. What will be the difference between the design of multiple down-scale+conv+up-scale blocks and convolutional blocks with various kernel sizes? Is MOE really necessary here? Will simple addition(or average) work?\n- It will be helpful to clarify how different the training procedure with the add-on module will be in comparison to the original SAM training process.\n- It will also be helpful to have a comparison in computational cost for those parameter-efficient tuning methods.\n- As shown in Table 4, the scales vary amongst different datasets. Will this require extra tuning efforts for picking a suitable scale (experts)? Again, will the way how different sizes are combined in the inception structure be a better option?"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699260499429,
        "cdate": 1699260499429,
        "tmdate": 1699636037530,
        "mdate": 1699636037530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uEDcG8Qk8V",
        "forum": "ezscMer8L0",
        "replyto": "ezscMer8L0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_W4sK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1114/Reviewer_W4sK"
        ],
        "content": {
            "summary": {
                "value": "In \"Convolution Meets LoRA\" the authors introduce a method for parameter-efficient finetuning of the Segment Anything Model (SAM) in specialized domains where it may initially underperform. The approach's effectiveness is demonstrated across a diverse range of datasets and is rigorously compared against a substantial set of baseline methods.\n\nThe method, referred to as Conv-LoRA, involves the incorporation of a modified version of low-rank adaptation (LoRA) into SAM's image encoder. Conv-LoRA improves upon LoRA by introducing a convolutional layer at its bottleneck, and includes a Mixture of Experts module to dynamically select the convolutional layer's scale of operation. Furthermore, the authors extend SAM's functionality to address multi-class segmentation tasks without the need for explicit prompts, allowing it to be deployed in an end-to-end setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper aims at solving the important problem of adapting a foundation model for computer vision to new, specialized domains. The authors propose Conv-LoRA - a new method for parameter-efficient fine-tuning (PEFT) of the segment anything model (SAM), which allows the adaptation of SAM to new domains where it may initially underperform. The combination of low rank adaptation (LoRA) with convolutions at various scales is a novel concept and a valuable contribution to the field of parameter-efficient finetuning for vision transformers.\n\nThe authors provide a substantial assessment of the quality of their work by showing a small but robust improvement over various baselines in a diverse set of datasets. While the relationship to some prior work needs to be elaborated upon, the paper includes a good overview of current efforts for PEFT. The authors improve the reliability of their results by running most experiments three times, thus reducing the likelihood of spurious effects stemming from random initialization.\n\nThe paper is generally well-structured and easy to follow. The authors effectively convey their approach and findings to the reader with only minor clarifications needed (as noted in the reviewer comments).\n\nWith the introduction of Conv-LoRA this paper not only expands the applicability of SAM to a wider range of datasets but also introduces a new method of PEFT that could be applicable to Vision Transformers more generally."
            },
            "weaknesses": {
                "value": "The paper convincingly demonstrates the effectiveness of the PERF method it introduces, but its motivation and explanation for why it works is unintuitive to me and the supporting evidence is insufficient. The authors claim that adding a convolutional layers reintroduces the inductive biases that are helpful in the image domain and hard-coded into convolutional layers. However, in Conv-LoRA, the convolutional layers are not applied to images but on features that do not necessarily adhere to the locality prior by construction. Can the locality prior truly be reintroduced if it might have already been lost, or, should this rather be regarded as a data-efficient method for finetuning that utilizes the learned locality of the early features in the ViT (see e.g. Raghu, Maithra, et al. 2021)? Unless my understanding of this problem is lacking, the explanation for the good performance of Conv-LoRA should be reformulated as a hypothesis.\n\nSimilarly, the authors identify \"SAM's foreground-background segmentation pretraining\" as a weakness, but SAM actually outputs three (prompt-dependent) masks with the idea of allowing the network to identify an object hierarchy (whole, part, sub-part) for ambiguous prompts. To me that seems closely related to multi-class segmentation and requires understanding of the image semantics. I think the paper could be strengthened by elaborating on and providing evidence for this deficiency of SAM.\n\nWhile the paper includes an overview of other efforts for parameter-efficient finetuning (PERF), a clarification on what sets it apart from other work on PERF of SAM would strengthen their work. Specifically, the authors mention the work of Zhang&Liu 2023 (SAMed), which also uses LoRA to adapt SAM to a new domain (medical images) and from my understanding also repurposes SAM to work for multi-class segmentation in an end-to-end fashion. Considering that Zhang&Liu 2023 is a very recent work and has not been published in a peer-reviewed venue, a direct comparison cannot be expected. However, the authors should revisit their claim in the introduction that Zhang&Liu (2023) fail to address SAM's limitation of not capturing \"high-level image semantic information\" (point 2) and functioning as a \"standalone, end-to-end solution\" (point 3). Similarly, the authors cite Shaharabany et al., 2023, who also adapt SAM to work fully automatically (point 3).\n\nIn summary, the authors show through their extensive set of experiments that their method is an effective way of performing PEFT of SAM to difficult domains and is therefore a valuable contribution. The concerns above are only with regards to the representation of prior work and the explanation of why Conv-LoRA is effective, not with the soundness of the method itself and the scientific rigor in showing its effectiveness. If the authors can address these concerns in the discussion or, where applicable, with minor edits to the language in the paper, I recommend accepting this work.\n\nRaghu, Maithra, et al. \"Do vision transformers see like convolutional neural networks?.\" Advances in Neural Information Processing Systems 34 (2021): 12116-12128.\nZhang, Kaidong, and Dong Liu. \"Customized segment anything model for medical image segmentation.\" arXiv preprint arXiv:2304.13785 (2023).\nShaharabany, Tal, et al. \"AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder.\" arXiv preprint arXiv:2306.06370 (2023)."
            },
            "questions": {
                "value": "1. As discussed above, I am not fully convinced of the author's explanation for the improved performance they see with Conv-LoRA compared to LoRA. My understanding is that while ViTs learn about locality in images, this is not a prior that is built into the architecture. What do the authors mean by \"SAM's local prior assumption\" mentioned in the abstract? How can \"image-related inductive biases\" be reinjected on top of features that do not have those biases?\n2. I don't follow the author's reasoning for why SAM's foreground-background pretraining is insufficient (see above). Can you elaborate on this?\n3. My understanding of the cited work Zhang&Liu 2023 is that they also adapt SAM to work without an image-dependent prior and they also extract semantics from the segmentation head. Similarly, Shaharabany, Tal, et al. adapt SAM to work fully automatically. Please make sure to not misrepresent these works in the introduction (see above).\n4. I am confused about the author's choices with regards to the scaling for the convolutional layers. What is the interpolation method used? Is it learned? If not, can the larger upsampling factors do much here with the chosen kernel size of 3x3? Adding a row to table 4 with a scaling factor of 8 would provide valuable insights. \n\n## Minor Suggestions for Improvement\n5. Can you clarify where exactly in the image encoder ViT the Conv-LoRA bypass is added?\n6. In the introduction the authors profess that SAM is underperforming on certain domains. The references supporting this claim can be found in the Related Work section (Tang et al., 2023; Ji et al., 2023; Zhou et al., 2023). I suggest that the authors add these references to the first time the claim is made.\n7. The authors identify the adaptation of SAM for end-to-end multi-class segmentation as one of the major contributions of their work. I suggest that the authors add a paragraph to the related work section discussing other efforts to do so if there are any.\n8. Figures 8, 9, 10 should have their own labels.\n9. Make sure all variable names are defined and used consistently (Figure 4, Appendix A). I wasn't able to follow everything in appendix A. (e.g., where does B come from when looking at eq. 2?)\n10. I suggest adding a figure with example images for the medical domain because it is featured prominently in the abstract.\n11. Consider spelling out low rank adaptation in the abstract before abbreviating it. This would make the paper more welcoming to readers that are new to the field.\n12. I wanna encourage the authors to consider releasing their code. This would not only facilitate further research and collaboration in the field but also help realize the impact of their work by making it easier for users to adapt SAM to new domains."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1114/Reviewer_W4sK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699383409719,
        "cdate": 1699383409719,
        "tmdate": 1699636037443,
        "mdate": 1699636037443,
        "license": "CC BY 4.0",
        "version": 2
    }
]