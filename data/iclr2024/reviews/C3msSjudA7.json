[
    {
        "id": "XXmymhb4KX",
        "forum": "C3msSjudA7",
        "replyto": "C3msSjudA7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to segment and recover a static, clean background and 360-degree objects from multiple scene observations. The idea is that by observing the same set of objects in various arrangements, parts that are invisible in one scene may become visible in others. By fusing the visible parts from each scene, occlusion-free rendering of both background scene and foreground objects can be achieved. The proposed method first performs objects/background segmentation and alignment based on the point cloud-based methods. It then performs radiance fields fusion, where a visibility field is introduced to quantify the visible information of radiance fields. Last, a visibility-aware rendering is used to obtain clean background and 360-degree object rendering. Experiments were conducted on synthetic and real datasets, and the results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies the under-modeled invisible parts of NeRF and introduces a new setting of complementing the invisible parts by fusing multiple scene information.\n\nThis paper introduces a visibility field, a volumetric representation to quantify the visibility of scenes, and proposes novel visibility-aware rendering, which leverages the visibility field to achieve the fusion of visible parts of multiple scenes.\n\nSynthetic and real datasets are created to validate the proposed idea, and the experimental results show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "One major concern pertains to the practicality of the proposed setup. Reconstructing multiple radiance fields for different variants of a scene, assuming diverse object positions and poses, may be challenging in real-world applications. The implicit assumption of fixed illumination for different radiance fields can limit the practicality of the approach. If we can control the scene, we can reconstruct each object independently rather than using this setup. It would be beneficial to address this concern and discuss potential solutions or scenarios where this setup could be more applicable.\n\nThe omission of modeling scene illumination and shadows is a notable limitation. Figure 4 shows that some reconstructed objects have darker or shadowed parts, which raises questions about the fidelity of the reconstructions. I think considering illumination and shadows in the method is critical to enhance the paper's completeness.\n\nWhile the paper introduces a new setup, the technical novelty of some aspects, such as the segmentation and visibility parts, is limited. Traditional point cloud registration methods and straightforward visibility calculations may not sufficiently push the boundaries of the field.\n\nThe quality of results in real scenes appears to be a concern, with reconstructed objects being described as blurry. It is important to provide further analysis or insights into why this issue occurs and potential strategies for improving the quality of real scene reconstructions."
            },
            "questions": {
                "value": "- Please kindly justify the proposed setup is practical in real-world applications.\n- The limitation of not modeling illumination and shadows.\n- Please justify the technical novelty of the method. \n- Results in the real-world scene is not good.\nPlease refer to the Weaknesses part for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_Monn"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697975365567,
        "cdate": 1697975365567,
        "tmdate": 1699668552175,
        "mdate": 1699668552175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sxsMMt2amB",
        "forum": "C3msSjudA7",
        "replyto": "C3msSjudA7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_hrRk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_hrRk"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of reconstructing and rendering a scene in a compositional manner (both background and multiple objects) given multiple videos of the same scene with rearranged objects. It reconstructs each video individually with NeRF, then uses classic methods (point could alignment, DBSCAN) to register and segment each component. The innovation is a visibility-based compositing method, which composes neural fields based on their visibility (how well a point can be reached by rays from all cameras) in each video.\n\nIt shows good qualitative results on a synthetic dataset and a real one."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing is clear and easy to follow. The assumptions are discussed thoroughly.\n- Multi-scene registration and reconstruction is a practical problem to solve. \n- The visibility-based fusion method is sound ."
            },
            "weaknesses": {
                "value": "**Method**\n- Scalability. The paper suggests training a per-video representation and using visibility to compose them. This might not scale well in terms of speed and compute cost if there are many videos (needs MxN NeRFs). Since the geometry/appearance of each NeRF is assumed to be consistent across videos, it seems to make more sense to learn a single representation across all videos for each component (needs M NeRFs).\n\n\n**Experiments**\n- There is no quantitative evaluation, comparison with the closest baseline, or ablation study. I think those are important for a paper.\n  - Evaluation: how well can the background and objects be segmented (2D IoU) and reconstructed (PSNR / 3D Chamfer distance)? \n  - Baseline: I think the shared NeRFs alternative I mentioned earlier is a valid baseline.\n  - Ablation: How well does the method work without the proposed visibility composition? How does the selection strategy affect the result (max selection vs weighted)?\n\n**Setup**\n- I like the simple setup the paper used to verify the idea. However, it seems too ideal to be generalized to the other cases, for example,\n  - Registration failure (e.g., partial scans, nonrigid objects) as mentioned in the limtation\n  - The videos have different illumination conditions as mentioned in the limitation. \n  - The appearance of the background changes over time (e.g., adding a table cloth)"
            },
            "questions": {
                "value": "Minor comments\n- The term \"scene\" can be confusing. The underlying scene is the same across all videos, but the paper refers to them as different scenes. This makes the definition of visibility confusing. I think naming them video 1...N would be clearer.\n- Fig 3 bottom right subfigure, should the points near the end of the ray be red, as they are not visible from the camera?\n- In Eq 4, it might be worth showing the exact equation that controls the peakedness (I assume it is similar to temperature-scaled softmax?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698590179761,
        "cdate": 1698590179761,
        "tmdate": 1699636210704,
        "mdate": 1699636210704,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QGrEjRSA77",
        "forum": "C3msSjudA7",
        "replyto": "C3msSjudA7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
        ],
        "content": {
            "summary": {
                "value": "This paper study an interesting problem in novel view synthesis or compositional scene modeling where the some part of the scene is not visible due to occlusion. This paper propose to capture multiple scenes of the same background and same set of foreground objects with different configurations (poses, lightings) and train different NeRF of each scene configuration. Then they propose to align the scene with the point cloud extracted from a trained NeRF by assuming that background have enough overlap and different scene configuration can be aligned based on the extracted point cloud. With the alignment and the proposed visibility field, they can rendering clean background and 360-degree objects."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper study an interesting problem in novel view synthesis or compositional scene modeling where the some part of the scene is not visible due to occlusion which is largely ignored in previous work.\n2. The paper proposed to capture/scan the scene with different configurations (same background and same foreground objects but with different poses). Each capture is used to train a NeRF to extract point clouds which are used for alignment between different scenes configurations. I think this is reasonable as the invisible part will never be reconstructed unless it becomes visible. \n3. The fusion of different nerf based on visibility field is also interesting.\n4. Experiments results show the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Two assumptions seems reasonable but far from reality? For example, if a car on top of the table is never\n2. The alignment relies on the extracted point cloud from NeRF. In the simple scenarios shown in the paper, NeRF can do a very good job to extract point cloud, but what if NeRF's point cloud is noisy when there is reflection or textless region?\n3. The object segmentation and clustering is heuristic and looks like not very robust. The segmentation of object relies on the clustering algorithm based only point cloud? While I think this simple method works just fine in the simple scenarios studied in the paper, I don't think it will still work in a more complex scene."
            },
            "questions": {
                "value": "1. Why not use some semantic segmentation method to separate the background and foreground and then to foreground matching of each objects? This seems even easier and straightforward. Or maybe just use the object feature to do matching?\n2. If you have control the capturing process, why not just pre capture the scene with only background? And similarly capture the object one by one. I think capturing the scene like this dose not take very much work compared to capturing multiple configurations proposed in the paper.\n3. In object clustering, do you need to predefined the number of objects.\n4. How to obtain a clean background is not very clear to me? Do you mean if a part is less occlusion, then it belongs to the background?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2692/Reviewer_etRq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698680002784,
        "cdate": 1698680002784,
        "tmdate": 1699636210623,
        "mdate": 1699636210623,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zXxSt1tQ06",
        "forum": "C3msSjudA7",
        "replyto": "C3msSjudA7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_bZoV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to improve the reconstruction of NeRF models by fusing the visible parts of an objects captured under different scene setting. In order to achieve the goal, it introduces a visibility field which records the visible information of radiance field, and use it to guide the fusion of input images from different scene. The proposed method is evaluated on synthetic and real world captured objects. An ablation study is included to evaluate the sensitivity of proposed method under different lighting, parameter setting, number of input scene, and variation of object placement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is simple and easy to follow. It also demonstrated some good results to reconstruct objects with 360 degree visibility."
            },
            "weaknesses": {
                "value": "The proposed method are only evaluated using \"toy\" examples. First, the scene captured are all under very simple lighting condition, without any directional light sources, and/or object shadows/reflections. The reconstruction examples are also very simple in its appearance which the observations of the objects do not have any view dependent variations, e.g. specular highlight and/or glossy surface. The different scene setting are also very simple, which each objects can be separated easily. I would consider all the scene setting and examples presented in the paper are well controlled and artificial. Second, in term of technical contribution, once the input images of an object are segmented and are grouped together, we can directly reconstruct the object NeRF model using self calibration method to obtain the relative camera position of each input images. What are the benefits to include the additional visibility field? Besides, I also feel that the proposed method still require very dense observations in order to achieve the high quality reconstruction. Third, the ablation study, except for the evaluation about lighting, I do not think the others are necessary. Instead, I would hope to see deeper analyses on how the proposed method can achieve self-calibration on lighting to resolve that concerns mentioned above about \"toy\" examples. Note that, although I agree the ablation study on lighting is necessary, but I also do not agree that the current ablation study on variation of lighting is good enough. It is also too simple and does not reflect the real world scenarios as discussed above. Lastly, another limitation is that the proposed method cannot handle dynamic objects. If all the objects are static, there are really not much technical challenges, and static objects can be easily registered."
            },
            "questions": {
                "value": "Please try to address my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698977456108,
        "cdate": 1698977456108,
        "tmdate": 1699636210548,
        "mdate": 1699636210548,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vdJYUy4iRq",
        "forum": "C3msSjudA7",
        "replyto": "C3msSjudA7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2692/Reviewer_p7zS"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to perform a \"piecewise\" fusion of NeRFs so that a static background and multiple objects can be separately rendered from arbitrary viewing directions. A scalar volumetric visibility field R^3 -> [0, 1] is proposed, to facilitate the visible part fusion. It assumes all surfaces of all objects are visible somewhere and that accurate pose alignment can be found for all objects. Qualitative results are shown on a synthetic dataset of objects rendered on a flat tabletop surface, as well as a real dataset in a similar setting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Well motivated. Although the assumptions seem too strong for in-the-wild data in its current form, I can see further development happening, to jointly optimize pose, perform completion, and be able to handle different lighting conditions."
            },
            "weaknesses": {
                "value": "No quantitative experiment. Some analysis of the number of images or viewpoints needed would have been interesting. The paper says the method consistently produced satisfactory results, but in some sense, that is guaranteed to happen given satisfactory input. It would have been better if there was more we could learn from the paper experimentally, including failure cases. My interpretation is that the paper has two take-home messages: 1. visible part fusion is a problem that needs more attention. 2. we can define volumetric density functions based on visibility to serve as a blending weight function, assuming we have everything else needed. On second thought, while the presentation is great, I wish I could see more."
            },
            "questions": {
                "value": "Perhaps consider citing FiG-NeRF: \"separating foreground objects from their varying backgrounds\" (instead of the same static background).\n\nWhat if the background isn't flat?\n\nHow robust is this to noise in point-cloud registration?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2692/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699030200875,
        "cdate": 1699030200875,
        "tmdate": 1699636210444,
        "mdate": 1699636210444,
        "license": "CC BY 4.0",
        "version": 2
    }
]