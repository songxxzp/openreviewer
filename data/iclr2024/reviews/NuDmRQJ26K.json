[
    {
        "id": "LbWtEmwHCZ",
        "forum": "NuDmRQJ26K",
        "replyto": "NuDmRQJ26K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_njPU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_njPU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an automated multi-task learning (MTL) framework dubbed LUMEN-PRO dedicated to diffractive optical neural networks (DONN). Then, the authors leverage the rotatability of the physical system and replace task-specific layers with the rotation of the corresponding shared layers. Both effectively reduce the memory footprint.\n\nExperiments also show that the proposed LUMEN-PRO provides up to 49.58% higher accuracy and 4x better cost efficiency than single task and prior art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper organization is great. Even though I do not have a relevant background on DONN, I can still follow the logic to understand the paper. E.g., Table 1 provides a good summary of current MTL methods and how the proposed one is better or more comprehensive.\n\n2. The proposed method leverages the rotatability of the physical system to fine-tune the multi-task DONN. It is like the spatial shift to CNNs and helps with the generalization ability learning of such models.\n\n3. Experiments show that the proposed methods achieve better task accuracy and cost efficiency than previous methods."
            },
            "weaknesses": {
                "value": "I am not an expert on DONN. As for the MTL and NAS:\n\nThe idea sounds like a combination of NAS and MTL. What is unique here for DONN? Is this method the general method that can be applied to other CNN or Transformer models?\n\nYou mentioned that the rotation mechanism has a physical meaning, what is that? Why is the rotation different from spatial shifts in CNNs?\n\nAs for the experiments, MNIST and CelebA are relatively small datasets, why do you consider larger ones? Is that because such DONN has some generalization or scalability issue preventing it from adapting to large scales?"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698351229538,
        "cdate": 1698351229538,
        "tmdate": 1699636220571,
        "mdate": 1699636220571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TZc07v1gtC",
        "forum": "NuDmRQJ26K",
        "replyto": "NuDmRQJ26K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_nw67"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_nw67"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-task learning optical neural network framework, LUMEN-PRO, which uses the physical principles to effectively improve the performance and cost of the model for multi-tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of modeling multitasking by rotating the physical layer is novel and interesting and can effectively reduce costs.\n\n2. Diffractive optical neural networks-based methods can greatly improve the inference performance.\n\n3. This area of research is rare and can increase the diversity of the ML community."
            },
            "weaknesses": {
                "value": "1. The method depends on the rotation of the physical layer. However, the physical layer has at most four directions. Therefore, the method only supports most four tasks.\n\n2. The method is mainly derived from the AutoMTL [1] method.\n\n3. The presentation is not clear enough. Some details are not included in the paper. This can be seen in the questions.\n\n4. The Figures are not annotated; thus, it is difficult to understand the method directly by looking at them.\n\n[1] Zhang, Lijun, Xiao Liu, and Hui Guan. \"Automtl: A programming framework for automating efficient multi-task learning.\" Advances in Neural Information Processing Systems 35 (2022): 34216-34228."
            },
            "questions": {
                "value": "1. What is the meaning of the  LUMEN-PRO in Figure 5? As I understand, this network can only make the inference function different under different tasks by rotating the layers.\n\n2. Why the LUMEN-PRO\u2019s performance can exceed the single task in Figure 5? Normally, it works best to use a separate model for each task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2778/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2778/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2778/Reviewer_nw67"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698502140095,
        "cdate": 1698502140095,
        "tmdate": 1699636220485,
        "mdate": 1699636220485,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o1TzmC1z5X",
        "forum": "NuDmRQJ26K",
        "replyto": "NuDmRQJ26K",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_edBq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2778/Reviewer_edBq"
        ],
        "content": {
            "summary": {
                "value": "This work describes a multi-task learning approach for a specific optical neural network named Diffractive Optical Neural Networks (DONN). It leverages the rotability of the physical system to share the same module across different tasks. Experiments was conducted on MNIST and its variants, and a face attribute dataset. The proposed method is able to outperform existing DONN multi-task learning method on accuracy with lower cost to fabricate the system."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe idea of enabling layer sharing in a optical neural network is interesting. The authors use some existing gradient-based architecture search algorithm (Automtl Neurips 22) and adapt it to the DONN scenario.\n-\tThe proposed method achieves significant performance gain on MNIST and Celeb-Faces dataset compared to existing multi-task methods such as VanillaMT and RubikONN."
            },
            "weaknesses": {
                "value": "-\tThe application of designing multi-task learning method for the DONN is too narrow. DONN is just one type of optical neural network and there is no evidence that this approach generalizes to other physical neural networks. The method may not have much practical usages in real life.\n-\tThe experiments conducted seems only from a mathematical perspective. If we put this solution to produce physical systems, will there be accuracy degeneration due to imperfect fabrication? And is the proposed rotation-based sharing method practical in real fabricated system? The authors did not address these issues.\n-\tDue to my lack of experience with this field, I do not understand a lot of technical details in this work. I believe the authors can improve on the explanation of the key concept to make it easy to understand. For example, the Figure 3 is really confusing. What does a node mean? What does the numbers in the node blocks mean? Are they network weights? There are a bunch of switches on the figure. What is the functionality of these switches and how do they work? Another key concept is the rotation-based layer sharing. What exactly does rotation mean in this scenario? How does such rotation facilitate weight sharing?"
            },
            "questions": {
                "value": "-\tTable 3 is kind of confusing. It seems to contain both ASIC-based solution and physical neural networks. How do you measure the throughput of an optical neural network? The proposed framework has very high throughput but is it really possible in a real system? Since you need to switch the input image physically at such a fast rate. And what does ``Accuracy\u2019\u2019 mean in this table? Is it just the testing accuracy on MNIST?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2778/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632380412,
        "cdate": 1698632380412,
        "tmdate": 1699636220412,
        "mdate": 1699636220412,
        "license": "CC BY 4.0",
        "version": 2
    }
]