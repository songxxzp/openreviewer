[
    {
        "id": "TAEqX5Mt9D",
        "forum": "pyuCmLLluu",
        "replyto": "pyuCmLLluu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_ZMpA"
        ],
        "content": {
            "summary": {
                "value": "This paper deals with the problem of target speaker extraction, where a speech by a speaker having the designated properties is extracted from speech mixtures. Although the previous methods for this task provide a cue of the speaker properties as speeches or images, the proposed method employs text prompts instead and encodes them with LLMs (more specifically, LLaMA-2). Once we can obtain feature embedding for representing the speaker properties, we can extract the target speech by injecting this feature embedding into the existing target speaker extractors. Experimental evaluations with the original dataset demonstrate that the proposed method outperformed the existing target speaker extraction named TD-SpeakerBeam in terms of SDR."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Target speaker (or speech) extraction is one of the hot topics in audio signal processing and many excellent techniques have already been proposed. This work extends those existing methods by introducing text prompts. As presented in Section 2, text prompts (1) are intuitive for humans, (2) have flexibility for designating the target speaker property, and (3) can make effective use of emerging large language models with amazing abilities.\n\n2. This paper is well written, well structured and easy to follow."
            },
            "weaknesses": {
                "value": "1. I could not understand what is the most significant technical hurdles for incorporating text prompts into the existing target speaker extraction methods.\n    - As far as I know, almost all the previous methods rely on feature embeddings for representing speaker properties to be extracted, which indicates that we do not mind which modality should be used for speaker representations. More specifically, universal sound selector [Ochiai+ Interspeech2022] has almost the same structure as the proposed model presented in Figure 3. This type of integration seems to be natural if we try to feed text prompts into target speaker extraction.\n    - The proposed method provides a simple baseline for text-based target speaker extraction. However, I could not find any special tricks for this purpose.\n\n2. Experimental comparisons with other methods should be presented.\n    - I understand that the authors could not find any other baselines for text-based target speaker extraction. However, the author should introduce a simple baseline different from the proposed method for demonstrating the effectiveness of the proposed method (but it might be difficult since the proposed method itself is a naive baseline).\n    - As presented before, universal target selector [Ochiai+ Interspeech2022] can be easily applied to text-based target speaker extraction. If the authors believe that the proposed method has a technical novelty against this work, experimental comparisons with it is mandatory for demonstrating the effectiveness.\n    - Also, ConceptBeam [Ohishi+ ACMMM2022] presented a different network architecture for target speech extraction. Although this work mainly focuses on the use of images for representing the speech property, it can be easily applied to other modalities such as texts."
            },
            "questions": {
                "value": "Please check the above Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698484845183,
        "cdate": 1698484845183,
        "tmdate": 1699636600727,
        "mdate": 1699636600727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UtuB4vtPLf",
        "forum": "pyuCmLLluu",
        "replyto": "pyuCmLLluu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a target speech extraction (TSE) method that uses text-based cues from a large language model (LLM). \nThe model consists of a standard masking extractor with encoder and decoder, where the mask is generated using an audio cue embedding of enrollment speech as well as text cue from the LLM. \nThe model is trained with a range of text prompts to deal with various instructions for TSE tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The use of LLM enables the framework to handle flexible instructions and rich information to extract the target speech. \n* The method demonstrates improved separation performance in terms of SI-SDR score."
            },
            "weaknesses": {
                "value": "The method is technically sound but exhibits a marginal novelty. \nMask generation using embeddings of auxiliary information is a common practice today such as (Liu et al. 2022; Oishi et al. 2023). \n\nWhat has become fundamentally possible given the power of LLMs? \nIf we restrict some form of templates of prompts to feed into the LLM for a handful of separation scenarios to specify gender/language/loudness etc., such information may be provided as one-hot representations."
            },
            "questions": {
                "value": "The ablation study in Table 1 shows some variations of LLM-TSE: audio + text, text-only and audio-only. \n1. Can we train the model with audio + text and switch to text-only or audio-only in inference time? \nIt seems the embedding vectors are concatenated and used for mask generation. \nIn this case, I am wondering how the lack of either modality is handled. \n\n2. How does audio-only LLM-TSE differ from TD-SpeakerBeam?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Reviewer_tdLn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659464624,
        "cdate": 1698659464624,
        "tmdate": 1699636600609,
        "mdate": 1699636600609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1PeEOYzC7V",
        "forum": "pyuCmLLluu",
        "replyto": "pyuCmLLluu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_K7zB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_K7zB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new approach for target speaker extraction called LLM-TSE that incorporates natural language input to guide the extraction process. This aims to enhance flexibility and performance compared to existing methods that rely solely on pre-registered voiceprints.\n\nThe user can provide text input describing various cues about the target speaker, such as gender, language, volume, distance, or even transcription snippets. This text input is encoded by a large language model to extract useful semantic information.\n\nThe text embeddings are fused with optional pre-registered voiceprint embeddings and passed to an extractor module to selectively extract the target speaker from the input mixture.\n\nExperiments demonstrate competitive performance using text cues alone, and SOTA results when combined with voiceprints. The method shows particular gains when text provides complementary contextual cues beyond the voiceprint."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Enhanced flexibility: Can utilize text cues alone without needing pre-registered voiceprints. Allows incorporating a wide range of perceptual cues through natural language descriptions.\n\nImproved controllability: Text input can be used to direct the model to extract or remove a target speaker, going beyond just extracting a pre-registered voice.\n\nSOTA performance: Achieves top results on benchmark datasets, outperforming previous target speaker extraction methods.\nRobustness to acoustic mismatches - Integrating contextual cues from text descriptions enhances robustness when enrollment conditions differ from test conditions.\n\nBroadened applicability: Relies less on requiring voiceprints a priori, expanding applicability to more real-world scenarios where pre-registration is unavailable.\n\nNovel paradigm: This signifies an important advancement in guided and adaptable target speaker extraction, laying the groundwork for future cocktail party research.\n\nLeverages large language model: Utilizes powerful pre-trained LLM to effectively interpret semantic concepts from natural language descriptions."
            },
            "weaknesses": {
                "value": "Lack of psychoacoustic analysis: No analysis related to human auditory perception. For example, see the ICASSP 2023 Deep Noise Suppression challenge. \n\nLimited perceptual cues: Does not yet handle more complex cues like pitch, emotion, timbre, age, topic of conversation, etc. Relies on predefined attributes.\n\nEvaluation on simulated data: Performance needs further evaluation on real-world noisy conditions with multiple concurrent speakers. A real-world test set like used in the ICASSP 2023 Deep Noise Suppression Challenge should be used. \n\nResults (Table 1) are compared with only two other models. It is a stretch to say this is SOTA.\n\nConstrained to simple descriptions: Cannot handle abstract or open-ended perceptual descriptions beyond basic attributes.\n\nComputational complexity: Large language models have high computational costs.\n\nBrittleness of LLMs: LLMs can exhibit biased and unreliable behavior. Robustness needs verification.\n\nSingle speaker extraction: Framework focused on extracting one target speaker, not multiple."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699336556795,
        "cdate": 1699336556795,
        "tmdate": 1699636600525,
        "mdate": 1699636600525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EPSXYYev5i",
        "forum": "pyuCmLLluu",
        "replyto": "pyuCmLLluu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to tackle the target speaker extraction problem by using text-guided approach. Compared to the conventional target speaker extraction that uses the enrolled speech from a specific speaker, the proposed text-guided approach aimed at instructing the model with input text information. A large language model is used to extract semantic cues from text information which is either fused with the audio cues or acting interpedently when used as the prompt for extracting the target speaker\u2019s voice. These input texts were generated by creating some question templates and then expanding it using ChagGPT-3.5-Turbo. The experiment part demonstrates better SI-SDR performance compared to a baseline system."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1, The paper is well written, and the presentation is clear. \n2, The method is innovative which enables the interaction of text prompt and the target speaker extraction."
            },
            "weaknesses": {
                "value": "1, Lack of references and comparison methods. Text-guided speech extraction is essentially a multi-modal speech extraction method, including text and speech modalities. The motivation is to leverage multiple distinctive clues to extract the target speech sound. While not using exact text and speech, such multi-modality-based speech extraction paper has been proposed before [1][2] which include text, image, and sound modalities. It\u2019s necessary to compare such existing methods in terms of performance rather than only comparing it with the speech-only-guided target speech extraction method. \n\n2, The experiment design is limited in the sense that these text prompt such as gender, language, far-near and loudness, are not typical cues which can be widely used for target speaker extraction. For example, if all the speakers are of the same gender in a conversation, which target speaker will be extracted? If all speakers speak the same language, how to extract the target speaker? For these scenarios, speakers are of different genders or speaking different languages, the target speaker extraction problem is indeed an easy problem. A more practical design scheme is needed for the input text. \n\nref\n[1] Ohishi, \u201cConceptBeam: concept driven target speech extraction\u201d, ACMMM, 2022. \n[2] Li et al., \u201cTarget sound extraction with variable cross-modality cues\u201d, ICASSP, 2023."
            },
            "questions": {
                "value": "For the audio cue encoder, have you tried a pre-trained speaker embedding module, such as d-vector model used for speaker recognition?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5734/Reviewer_gyG3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5734/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699507300418,
        "cdate": 1699507300418,
        "tmdate": 1699636600435,
        "mdate": 1699636600435,
        "license": "CC BY 4.0",
        "version": 2
    }
]