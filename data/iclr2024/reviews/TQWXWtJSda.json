[
    {
        "id": "3H6NeG5rO6",
        "forum": "TQWXWtJSda",
        "replyto": "TQWXWtJSda",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_vJXm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_vJXm"
        ],
        "content": {
            "summary": {
                "value": "Contemporary studies in knowledge distillation have revealed that the better teacher models could not promised the better performance of the student models. \nMotivated by this result, this paper aims to define the how to select the better teacher models. \nTo this end, the authors propose a simple yet effective calibration error for the metric. \nThis paper empirically demonstrates the student models well-learn the knowledge of the teacher models by applying the simple temperature scaling to the teacher models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper reveals the calibration error is better than an accuracy for the metric on the knowledge distillation. \nIn addition, it is convincing using the adaptive calibration error (ACE) rather than the expected calibration error.\n2. This paper not only covers the most of the related studies but also is easy to follow."
            },
            "weaknesses": {
                "value": "It is necessary to assess whether the proposed method works well even if various calibration studies are applied. And also they should be covered in the related works."
            },
            "questions": {
                "value": "1. In the introduction, the authors described that two things are important in the KD process. The first is to choose an effective distillation method, and the second is to choose the best one of teachers for students. But in real-world applications, under the deployment conditions, isn't it more typical to choose a student model that can learn the best teacher model?\n2. Wouldn't it be possible to reverse when T=2 and T=4?  If so, the claim is somewhat lacking in persuasion. The reason for the question is that the trend at T=4 and T=5 has reversed and also the variance of the results in Figure 2 (a) is somewhat large.\n3. In Table 2, ResNet56 selected as the student model is a larger-sized model than a few teacher models (e.g., vgg19). Is this the right KD experiment?\n4. After the proposed KD process, is there no need for one more calibration of the student model? I know this paper focuses on increasing the accuracy of the student model, but since it has already been learned from a calibrated teacher model, I think it is also another contribution of this paper if there is no need to conduct the calibration process for student model. Simply put, I would like to know the ECE and ACE of the student model that applied the calibration of the student model in Table 7 after applying the proposed method.\n\n\nFor the rebuttal, if the weakness are well addressed this reviewer is willing to raise the review score.\n\nAfter discussing the authors, my concerns are solved so that I have raised my score from 5 to 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1117/Reviewer_vJXm",
                    "ICLR.cc/2024/Conference/Submission1117/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698052160071,
        "cdate": 1698052160071,
        "tmdate": 1700642855454,
        "mdate": 1700642855454,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4kOOMyrbwq",
        "forum": "TQWXWtJSda",
        "replyto": "TQWXWtJSda",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_f68K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_f68K"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the knowledge distillation problem and reveals that there is a strong correlation between the calibration error of the teacher and the accuracy of the student. To reduce the teacher's calibration error, a temperature-based calibration is proposed.Extensive experiments demonstrate the effectiveness of the proposed method. With the help of MLLD, the proposed method achieves a new SOTA performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is motivated well. The correlation between the calibration error of the teacher and the accuracy of the student is evaluated well (cf. Fig 1). \n2. The experimental results are strong. This work establishes new SOTAs in this field.\n3. This paper is written and organized well."
            },
            "weaknesses": {
                "value": "1. The proposed method is too simple. The temperature scaling has been proposed in the traditional KD. The technique contribution is small.\n2. Althogh this work achieves SOTA performance, it is based on previous SOTA method MLLD. All experiments are conducted on the image classification task. Object detection or other tasks are not considered."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1117/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1117/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1117/Reviewer_f68K"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734435135,
        "cdate": 1698734435135,
        "tmdate": 1699636037960,
        "mdate": 1699636037960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fUFdvAhugw",
        "forum": "TQWXWtJSda",
        "replyto": "TQWXWtJSda",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_F9hQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1117/Reviewer_F9hQ"
        ],
        "content": {
            "summary": {
                "value": "Knowledge Distillation (KD) is a successful method for compressing deep learning models. However, recent research shows that a high-performance teacher doesn't guarantee a better student. This paper introduces a criterion for choosing an appropriate teacher: the teacher's calibration error, which correlates strongly with student accuracy. The paper also presents a temperature-based calibration method that reduces the teacher's error, leading to improved KD performance. This method can enhance other techniques and achieves a new state-of-the-art performance level when applied alongside current models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Practical Applicability: The introduced temperature-based calibration method offers a practical and effective solution for improving knowledge distillation performance.\n\n- Clarity and Conciseness: The paper effectively conveys its key findings and contributions in a clear and concise manner, making it accessible to a wide audience.\n\n- Comprehensive Experimental Validation: The paper backs its claims with thorough experimental evaluations conducted across multiple benchmarks and domains. \n\n- Well-Structured Presentation: The paper is well-structured, with a clear introduction, detailed methodology, and comprehensive experimental results."
            },
            "weaknesses": {
                "value": "All in all, despite the simple idea and somewhat lack of novelty, the proposed method is technically sound and effective. It would be great if the authors of the paper can offer some sort of theoretical insights if possible. \n\nOne additional experiment that is worth conducting is: intuitively, in addition to ECE, the accuracy of the teacher model would also impact the quality of knowledge distillation. Does this mean that we should favor a teacher model with better ECE and accuracy? Are there any inherent tradeoffs between accuracy and ECE among different choices of teacher models? Is it always possible to get the ECE of different models to the same level by adjusting the temperature parameter? Personally I feel that it is worthwhile conducting additional experiments using the same student model, but different teacher models of different depth to demonstrate the effect of accuracy and ECE have on the quality of knowledge distillation. This would further strengthen the empirical contribution of the paper in my opinion.  \n\nLastly, similar experiments were conducted previously to study the behavior between temperature and the effectiveness of knowledge distillation [1]. It would be great if a discussion is done in the related works section on the similarity and differences of this work. \n\n[1] Zhang Z, Sabuncu M. Self-distillation as instance-specific label smoothing. Advances in Neural Information Processing Systems, 2020, 33: 2184-2195."
            },
            "questions": {
                "value": "It would be great if the authors of the paper can help address the question raised in the \"weakness\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1117/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790155753,
        "cdate": 1698790155753,
        "tmdate": 1699636037886,
        "mdate": 1699636037886,
        "license": "CC BY 4.0",
        "version": 2
    }
]