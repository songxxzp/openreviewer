[
    {
        "id": "yP90jjksE8",
        "forum": "AVBw2Ul4X9",
        "replyto": "AVBw2Ul4X9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_75Vm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_75Vm"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issue of graph neural networks (GNNs) calibration by introducing SIMI-MAILBOX, a novel method that categorizes nodes based on neighborhood similarity and confidence levels, effectively mitigating miscalibration across diverse local topologies. The study highlights limitations in current GNN calibration techniques, particularly in relation to neighborhood prediction affinity, and offers valuable insights into improving the reliability of GNN predictions through group-specific temperature adjustments. Experimental validation demonstrates the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-structured, easy to understand, and presents a straightforward, implementable method.\n\n2. The motivation behind the paper is compelling, and it provides experimental evidence to support the existence of the discussed phenomenon, along with proposing a viable solution.\n\n3. The paper extensively validates the effectiveness of the proposed method through a large number of experiments and visualizations, demonstrating significant improvements compared to baseline approaches."
            },
            "weaknesses": {
                "value": "1. The paper lacks essential citations to highly relevant works, which significantly diminishes its actual contributions. For instance, [1] introduces a calibration loss based on grouping, and [2] presents the concept of multicalibration, akin to grouping. While these papers do not directly address graphs, they offer universal methods applicable to graphs, and this paper should reference these works, highlighting the similarities and differences between them.\n\n2. The most closely related research is [3], which was first published on June 8, 2023, on Arxiv. In [3], a similar approach of grouping samples and applying temperature scaling to each group is proposed. [3] also raises concerns about potential miscalibration post-grouping. Moreover, [3] defines a general grouping function, and it's worth noting that the grouping in this paper, obtained through k-means clustering, is a specific (human-defined) grouping function. [3] offers a learning-based method in this regard.\n\n3. Based on these two points, the primary contribution of this paper appears to be the discovery of an effective grouping function in the context of GNNs, accompanied by some experimental analysis. While this discovery holds practical significance, its novelty may not be sufficient for acceptance by ICLR.\n\n[1] Alexandre Perez-Lebel, Marine Le Morvan, Ga\u00ebl Varoquaux. Beyond calibration: estimating the grouping loss of modern neural networks. 2022.\n\n[2] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, Guy Rothblum. Multicalibration: Calibration for the (Computationally-Identifiable) Masses. 2018.\n\n[3] Jia-Qi Yang, De-Chuan Zhan, Le Gan. Beyond Probability Partitions: Calibrating Neural Networks with Semantic Aware Grouping. 2023."
            },
            "questions": {
                "value": "Please address the weaknesses identified in the review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Reviewer_75Vm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698603146394,
        "cdate": 1698603146394,
        "tmdate": 1700590820948,
        "mdate": 1700590820948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CjjQOLx4T5",
        "forum": "AVBw2Ul4X9",
        "replyto": "AVBw2Ul4X9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_jzwr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_jzwr"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the miscalibration issue in graph learning. Different from general deep models, GNNs are often underconfident about their predictions. Previous GNN calibration techniques adapt node-wise temperature scaling by smoothing the confidence of individual nodes with those of adjacent nodes. However, the authors find that calibrated logits from preceding research significantly contradict their foundational assumption of nearby affinity. As a remedy, they introduce SIMI-MAILBOX, which categorizes nodes based on\nboth neighborhood representational similarity and their own confidence. The extensive experiments verify the effectiveness of their proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] The manuscript is well-presented. The authors clearly present the motivations, the methods and the experiments.  \n[+] The paper is motivated by valid empricial study.       \n[+] Extensive experiments are performed to verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "[-] The pioneering work (CaGCN) [1] in this field (GNN calibration) verify the the Accuracy-Preserving property through theoretical analysis and empirical study. Does the method proposed in this manuscript possess such property? It is an important property for calibration methods. More theoretical justifications and experimental results are expected.            \n[-] The contribution of the proposed method is somewhat limited. Specifically, the authors utilize KMeans clustering to classify nodes based on analogous neighborhood prediction similarity and their own confidence. And then, analogous nodes within each cluster is rectified via group-specific temperatures. This is incremental compared to previous temperature-tunning methods [1,2].             \n[-] Many graph calibration methods are built on the concept of neighborhood prediction similarity. Hence, they can achieve superior performance in homophilous graphs. So, can the proposed method work in heterophilous graphs?                          \n[-] The codes for reproducing the results are not provided.                   \n[-] Minor issue: The scarlar (probability) value $p$ in Eq. (1) is bolded while it ($\\textbf{p}$) often denotes a vector.                     \n\nI am happy to raise the ratings providing that the authors can address my concerns during the rebuttal.                   \n\n[1] Be confident! towards trustworthy graph neural networks via confidence calibration. (NeurIPS 2021)                    \n[2] What makes graph neural networks miscalibrated? (NeurIPS 2022)"
            },
            "questions": {
                "value": "1. Does the method proposed in this manuscript possess the Accuracy-Preserving property?             \n2. Can the proposed method work in heterophilous graphs?          \n3. Will the code for reproducing the results be released?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Reviewer_jzwr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806566044,
        "cdate": 1698806566044,
        "tmdate": 1700704670336,
        "mdate": 1700704670336,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gD6nmjZKiO",
        "forum": "AVBw2Ul4X9",
        "replyto": "AVBw2Ul4X9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_3Tgz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2994/Reviewer_3Tgz"
        ],
        "content": {
            "summary": {
                "value": "This paper points out the problems with GNN calibration and proposes a new calibration method to improve them. This method focuses on the correlation between the similarity of neighboring nodes and calibration performance, and uses this information to make corrections. Experiments on multiple baselines with multiple data sets show that the proposed method has good performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Experimentally finding heuristically that conventional GNN calibration methods have biased performance from node to node and the conditions under which they do so.\n- Based on the characteristics found, a method to solve those problems is proposed.\n- Experiments have shown that the proposed method is effective for multiple data sets and baselines"
            },
            "weaknesses": {
                "value": "- The problems with conventional methods are heuristic. I don't deny that in itself, and the experimental results suggest that the finding is correct. However, it would be better to have an algorithmic point of view, even if not proven.\n- Some parts are not reader friendly, for example, Fig.1 and Fig.3 look like similar tables but show different information."
            },
            "questions": {
                "value": "The results are likely to change depending on the setting of the number of clusters in K-means, the scaling of $\\hat{p}_i$,$\\bar{M}^{simi}(i)$ and the setting of $\\lambda$ in equation (10). What is the impact of these different settings? If the sensitivity is high and they are not easy to set up, it is not sufficient to say that the proposed method is as effective as claimed. Because of this concern, I withhold my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2994/Reviewer_3Tgz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698834411352,
        "cdate": 1698834411352,
        "tmdate": 1700577604123,
        "mdate": 1700577604123,
        "license": "CC BY 4.0",
        "version": 2
    }
]