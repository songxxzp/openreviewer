[
    {
        "id": "DYTWTj5uFH",
        "forum": "waeGeAdZUx",
        "replyto": "waeGeAdZUx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission59/Reviewer_TqZE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission59/Reviewer_TqZE"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the challenges of users\u2019 complicated behavior changes in online recommendation systems. In different periods, users\u2019 behaviors change in terms of preferences, return time and frequencies of immediate user feedback. To handle this challenge, the authors propose an adaptive sequential recommendation method to optimize long-term user engagement. Specifically, the authors utilize a context encoder to encode user\u2019s behavior patterns and regularize the encoder to produce a similar latent representation for states with similar state values. An optimistic exploration is further utilized to encourage exploration. Experiments are carried out using a recommender simulator and an online A/B test."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is well-organized and easy to follow.\n2.\tExperiments are carried out on both a recommender simulator and an online A/B test, which is comprehensive.\n3.\tThe authors conduct an ablation study to validate the effectiveness of each component."
            },
            "weaknesses": {
                "value": "1.\tOne major concern about the proposed method is the regularization loss in the proposed context encoder, which seems problematic to me. The motivation of this paper is to handle the distribution shift of the evolving user behavior patterns. However, encouraging states with similar state values to have similar latent encoding representation does not solve the distribution shift issues. The estimated state value function can still face the challenge of user behavior shift, resulting in an inaccurate state value estimation. \n\n2.\tAnother major concern is the novelty of the proposed method. To my knowledge, using context encoder to encoder user behavior patterns is not new in the recommendation context, which is also discussed in the related work section. The novelty of adding regularization loss in the context encoder is limited. The adopted exploration mechanism from RL literature is rather general and it is unclear how it particularly handles the user exploration in the recommendation context, which usually involves large action space. \n\n3.\tAs this paper aims to handle the user behavior evolution challenge in the sequential recommendation setting, some baselines in the Non-RL recommender literature are missing such as [1, 2].\n\n[1] Zhou, G., Zhu, X., Song, C., Fan, Y., Zhu, H., Ma, X., ... & Gai, K. (2018, July). Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (pp. 1059-1068).\n\n[2] Zhou, G., Mou, N., Fan, Y., Pi, Q., Bian, W., Zhou, C., ... & Gai, K. (2019, July). Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence (Vol. 33, No. 01, pp. 5941-5948)."
            },
            "questions": {
                "value": "See the Weaknesses for the questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698619899591,
        "cdate": 1698619899591,
        "tmdate": 1699635930217,
        "mdate": 1699635930217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IcRruQXVAo",
        "forum": "waeGeAdZUx",
        "replyto": "waeGeAdZUx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel paradigm called AdaRec to address the challenge of evolving user behavior patterns in large-scale online recommendation systems. The goal is to optimize long-term user engagement by leveraging Reinforcement Learning (RL) algorithms. By introducing a distance-based representation loss to extract latent information from users' interaction trajectories, AdaRec helps the RL policy identify subtle changes in the recommendation system. To enable rapid adaptation, AdaRec encourages exploration using the idea of optimism under uncertainty. It also incorporates zero-order action optimization to ensure stable recommendation quality in complex environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The problems studied in this paper exist widely in recommendation systems, and have been ignored by previous researchers, which is a very promising and important research direction.\n2. The paper presents a distance-based representation loss to identify the subtle user behavior patterns changes, which is novel and interesting.\n3. Extensive empirical analyses in simulator-based and live sequential recommendation tasks demonstrates that AdaRec outperforms baseline algorithms in terms of long-term performance."
            },
            "weaknesses": {
                "value": "1. The writing of the paper needs further improvement. \n(1)\tWhat is the specific meaning of State Space S?\n(2)\tThe paper should give a brief introduction before using some reinforcement learning concepts.\n2. Although I agree with that the user behavior patterns are evolving in recommendation systems, the study in Section 3 does not make sense to me. At different time stamp, user interaction history statistics are different, that is, the distribution of states are different, so different interaction frequency may not reflect the change of user behavior patterns. There is no evidence to support the argument \u201cAs previously discussed, given the same distribution of states s_t and actions a_t, the users\u2019 return time exhibits fluctuations across different weeks.\u201d \n3. In reinforcement learning, there are many exploration strategies. What are the advantages of the schemes mentioned in the paper, and the comparison experiments with other schemes need to be provided."
            },
            "questions": {
                "value": "1. What is the specific meaning of State Space S?\n2. The comparison experiments with other exploration schemes need to be provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission59/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission59/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission59/Reviewer_dRAr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827266911,
        "cdate": 1698827266911,
        "tmdate": 1700408915648,
        "mdate": 1700408915648,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AV5yAWZjRE",
        "forum": "waeGeAdZUx",
        "replyto": "waeGeAdZUx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission59/Reviewer_kKUq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel paradigm to tackle the challenge of distribution shift in large-scale online recommendation systems. In these systems, the dynamics and reward functions are continuously affected by changes in user behavior patterns, making it difficult for existing reinforcement learning (RL) algorithms to adapt effectively.\n\nAdaRec proposes a multi-faceted approach to address this issue. It introduces a distance-based representation loss, which extracts latent information from users' interaction trajectories. This information reflects how well the RL policy aligns with current user behavior patterns, allowing the policy to detect subtle changes in the recommendation system.\n\nAdaRec's approach to addressing distribution shift in recommendation systems appears promising and aligns with current challenges in the field. The full paper's empirical results and detailed methodology will be necessary to assess the significance and practical applicability of this novel paradigm in real-world recommendation systems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. AdaRec introduces a novel paradigm for addressing distribution shift in large-scale online recommendation systems, which is a significant and challenging problem in the field.\n\n2. The use of zero-order action optimization to ensure stable recommendation quality in complicated environments is a strong point, as it addresses the need for robustness in real-world recommendation systems.\n\n3. The claim of superior long-term performance is supported by extensive empirical analyses in both simulator-based and live sequential recommendation tasks, indicating a commitment to evaluating the proposed solution rigorously."
            },
            "weaknesses": {
                "value": "1. The use of \"optimism under uncertainty\" and zero-order action optimization may introduce additional complexity to the approach, which could be a drawback in terms of implementation and computational cost."
            },
            "questions": {
                "value": "Does the computational cost of reinforcement learning need to be analyzed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission59/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699116951602,
        "cdate": 1699116951602,
        "tmdate": 1699635930014,
        "mdate": 1699635930014,
        "license": "CC BY 4.0",
        "version": 2
    }
]