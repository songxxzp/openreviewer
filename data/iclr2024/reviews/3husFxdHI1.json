[
    {
        "id": "O19ynR8856",
        "forum": "3husFxdHI1",
        "replyto": "3husFxdHI1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_Ac7q"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the relationship between the graphical models and the neural networks. The authors first indicate the equivalence between the belief propagation and the back propagation. Then the duality between the Bayesian neural networks and the graphical models are specified via the relationship between Langevin and the Fokker-Planck dynamics. Based on these observations, the authors propose a new training method, whose efficacy is demonstrated by the numerical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper provides both theoretical analysis and the empirical verification for the relationship between the Bayesian neural networks and the Markov random fields. The results of this paper are interesting for the deep learning community, and they can serve the basis for understanding the training process of neural networks."
            },
            "weaknesses": {
                "value": "1. The theoretical results of this paper are built on the basis of Gaussian linear model. This can be strict in many realistic problems. It would be helpful to discuss the extension of this model. For example, is it possible to extend the results in this paper to the mixture of Gaussian.\n\n2. The writing of the theoretical results can be improved. The authors define many things in the Theorems 1 and 2. It will be more friendly to readers to define these quantities before the statement of the theorems.  In addition, the fontsize of the math characters should be consistent.  For example, in (3) of Theorem 1, the fontsize of characters changes after $=$."
            },
            "questions": {
                "value": "The questions are listed in the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698497844115,
        "cdate": 1698497844115,
        "tmdate": 1699636905033,
        "mdate": 1699636905033,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xkjZKd5h9e",
        "forum": "3husFxdHI1",
        "replyto": "3husFxdHI1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the parallels between probabilistic graphical models and Bayesian neural networks, specifically the main results highlights the equivalence between message passing in probabilistic graphical models and belief propagation in neural networks. Through empirical assessments conducted across diverse scenarios, the paper substantiates this convergence, highlighting their equivalence"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "One of the primary strengths of the paper lies in its problem statement which is exploratory in nature. Specifically, the paper seeks to establish an equivalence between Bayesian neural networks and Markov random fields."
            },
            "weaknesses": {
                "value": "- While the exploratory nature of the problem is acknowledged, the paper could benefit from a stronger motivation. It is essential to explain why the question of equivalence between Bayesian neural networks and Markov random fields is important. Clarifying the practical implications and real-world significance of this equivalence would strengthen the paper's rationale.\n- A brief note on potential future directions would enhance the appeal of the paper."
            },
            "questions": {
                "value": "- The paper asserts an equivalence between Bayesian neural networks and probabilistic graphical models, suggesting the potential for \"enhanced models.\" However, the term \"enhanced models\" remains vague and requires precise definition. Providing insight into how this equivalence can be leveraged to create more effective or advanced models would enhance the paper's clarity.\n\n-  The equations in the paper are dense and lack organization, making the paper challenging to comprehend. To improve readability, the authors should offer intuitive explanations for their main results and their consequences. Additionally, providing a brief sketch of the proofs, especially in Theorems 1 and 2, which establish the equivalence between back-propagation and belief propagation, would be beneficial. Clearly defining terms like \"tensor particles,\" \"stochastic tensor flow,\" \"variational message passing,\" \"sensitivity of probability distribution,\" \"tensor distribution evolution,\" and \"mean field Gaussian variational posterior\" is crucial to ensure clarity and readability.\n\n- The plots in Figure 1 are too small to discern the axes, scaling, and legend effectively. The authors should consider enlarging the plots or providing clearer visual aids to improve the readability of the figures.\n\n-  Figure 2's main point is unclear, and it is not evident why the plots emphasize the diagonal. The authors should provide a more detailed explanation of the purpose and focus of Figure 2 to enhance the reader's understanding.\n\n- The paper's GitHub link for the code is not anonymized, compromising the authors' anonymity and violating the double-blind review protocol. The authors should address this issue promptly to maintain the integrity of the double-blind review process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Reviewer_2p2Z"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829429861,
        "cdate": 1698829429861,
        "tmdate": 1699636904900,
        "mdate": 1699636904900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p07aw0Dy9r",
        "forum": "3husFxdHI1",
        "replyto": "3husFxdHI1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
        ],
        "content": {
            "summary": {
                "value": "This paper identifies and establishes connections between Bayesian neural networks and probabilistic graphical models (Markov random fields). In particular, they establish equivalence between backpropagation and belief propagation in Theorem 1. Their Theorem 2 presents a relationship between Langevin and the Fokker-Planck dynamics. They further leverage these connections to develop a belief propagation-based algorithm to train Bayesian neural networks and show its efficacy through numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The connection between backpropagation and belief propagation is an interesting result. It has the potential to pave the way for the adaptation of various algorithms from the field of probabilistic graphical models to Bayesian neural networks. The authors have also substantiated their theory through a series of numerical experiments, demonstrating that their proposed algorithm consistently surpasses other baseline methods in terms of performance."
            },
            "weaknesses": {
                "value": "The paper is quite heavy on notations and a little difficult to follow at times. Some of the notations are inconsistent and create confusion while reading. Few examples (there could be more):\n1. I believe it would be easier to understand the equations if the dimensions of the parameters were stated clearly. \n2. $f_l$ seems to take arguments in different orders (check eq. for $x_l$ before (4) and then in the fifth line after (5)).\n3. Is there a purpose for italicizing one of the $f_{x_{l-1}}$ in (6)?\n4. In Theorem 1 (2), $T(x_l)$ is written in both regular and bold fonts. In general, there does not seem to be a consistent interpretation of boldface parameters.\n\nCheck the following statement (and other similar statements): \"This research emphasizes the convergence between probabilistic graphical models and neural networks, revealing their intrinsic parallels.\" \nIt seems that the authors are claiming that probabilistic graphical models and neural networks are the same. I understand the parallels but as a whole the statement seems to be too strong. It would be better to qualify such string statements by formal evidence.  \n\nPlease also see the questions section."
            },
            "questions": {
                "value": "1. The term tensor \"particles\" has been used repeatedly throughout the paper without a formal definition or reference. Could you please explain what exactly is a tensor particle?\n2. In the context of probabilistic graphical models, belief propagation comes with no convergence guarantees for graphs with cycles. Does such consideration occur in the proposed method? In general, would the proposed algorithm always converge?\n3. Could you explain the following comment under Theorem 1 (a formal derivation or a pointer to a reference would help)? \n\"The exponential family assumption in (2) generally holds as long as the joint probability density of the random variables is strictly positive, as per the Hammersley-Clifford theorem (Hammersley & Clifford, 1971).\"\n4. I am trying to place this work in the context of the existing work relating belief propagation with neural networks. Has there been any work in this regard or are the authors presenting a completely novel observation? \n\nI am open to changing my score based on the answers from the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7497/Reviewer_kosy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7497/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699509792659,
        "cdate": 1699509792659,
        "tmdate": 1699636904768,
        "mdate": 1699636904768,
        "license": "CC BY 4.0",
        "version": 2
    }
]