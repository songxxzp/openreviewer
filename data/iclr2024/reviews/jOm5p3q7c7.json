[
    {
        "id": "nfAbfLRSWE",
        "forum": "jOm5p3q7c7",
        "replyto": "jOm5p3q7c7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_iFE8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_iFE8"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the sample complexity of learning the optimal policy in average-reward Markov decision processes, under the assumption of a uniformly ergodic MDP and a generative model. The proposed algorithm improves the best sample complexity upper bound in existing works and matches the lower bound of the problem. This is achieved by combining the algorithmic ideas of two lines of existing research, by first reducing the AMDP to a discounted-reward MDP and then establishing an optimal sample complexity upper bound in the setting of uniformly ergodic discounted MDPs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper studies an important problem in reinforcement learning theory. It closes the gap between the upper and lower bounds of the sample complexity for learning an optimal policy. The technical proofs look solid as far as I can tell. The presentation of the algorithm and analysis is also clear."
            },
            "weaknesses": {
                "value": "As the authors have mentioned in the paper, the algorithm is developed by combining the algorithmic ideas from two lines of existing research (Jin & Sidford, 2021) and (Li et al., 2020). While in general we would hope to see technical novelty in terms of algorithm design, it is probably okay with this work because reducing to a discounted MDP (Jin & Sidford, 2021) seems to be a standard approach, and the authors do make improvements over the analysis of (Li et al., 2020) to establish a sharper sample complexity upper bound. \n\nEven though I understand that the main contributions of this work are theoretical, I would still hope to see some numerical results to demonstrate some of the ideas in the paper."
            },
            "questions": {
                "value": "Does your work imply any new results for the case where the uniform ergodicity assumption does not hold, such as the weakly communicating AMDP setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Reviewer_iFE8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697942681631,
        "cdate": 1697942681631,
        "tmdate": 1699636106966,
        "mdate": 1699636106966,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tnRzZIdsuT",
        "forum": "jOm5p3q7c7",
        "replyto": "jOm5p3q7c7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_tPAf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_tPAf"
        ],
        "content": {
            "summary": {
                "value": "The authors considered the sample complexity of average reward MDPs under the uniformly ergodic condition, and provided a novel analysis for the algorithm given in Li et al. 2020, which results in an upper bound that matches the known lower bound."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Theoretical paper that gives a matching bound, therefore fully establishing the optimal sample complexity for AMDP under the uniform ergodicity condition.\n2. The background was explained clearly, and the context of the result to other related settings is also well explained.\n3. In most places, the notations and proofs are done rigorously, more so than the average papers."
            },
            "weaknesses": {
                "value": "1. The result is somewhat thin in the sense that it feels like filling a small gap that was somehow overlooked by several previous groups of researchers, though I personally like the cleanness of the result. \n2. The main contribution is technical, yet the main paper does not really spend the effort to clearly explain the technical critical point that enables the authors to establish the bound. Particularly, it appears Proposition A.1 is the critical step to establish the concentration inequality. More discussion on the technical level on the difference with previous bounds should be given."
            },
            "questions": {
                "value": "1. I'm a little confused about some notation. Is alpha in (2.6) in R^|S|? It should be, but later in the definition of \\bar{alpha}, we need to choose the maximum alpha, which seems incorrect since it is a vector. \n2. Equation (2.4), where is \\eta defined? It seems to appear without any context. \n3. Can you comment on the choice of distribution of Z(s,a)? Does it make a difference if another distribution (non-uniform) is used?\n4. Though I can understand the result is theoretical, have the authors used any numerical results to verify the optimal algorithm behavior that suggests the sample complexity scaling?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Reviewer_tPAf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545946069,
        "cdate": 1698545946069,
        "tmdate": 1699636106899,
        "mdate": 1699636106899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gMRWrWW12o",
        "forum": "jOm5p3q7c7",
        "replyto": "jOm5p3q7c7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_qCaw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_qCaw"
        ],
        "content": {
            "summary": {
                "value": "The authors presents the sample complexity result for the average-reward Markov Decision Processes (AMDPs) under the assumption of ergodicity and with an access to a simulator. This sample complexity is nearly minimax optimal in the class of ergodic MDP, thus closing the gap in mixing time or desired accuracy that appears in previous works. The presented algorithm is basically applies the reduction technique from AMDP to DMDP and efficiently exploits ergodicity assumption in the DMDP setup."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- First minimax optimal guarantees for AMDPs under a generative model assumptions;\n- As a byproduct, authors provide minimax optimal  for ergodic DMDPs\n- Computationally feasible algorithm;\n- Simplicity of the presented approach."
            },
            "weaknesses": {
                "value": "- All the main instruments has already introduced in other papers, and thus this paper may lack of novelty.\n    - Reduction of AMDP to DMDP is presented in (Jin & Sidford, 2021)\n    - Optimal rates with optimal warm-up are presented in (Li et al. 2020);\n    - Rates for mixing DMDP are already presented in (Wang et al. 2023) (specifically, Proposition 6.1 and Corollary 6.2.1);\n\nYujia Jin and Aaron Sidford. Towards tight bounds on the sample complexity of average-reward\nMDPs, 2021.\n\nGen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier\nin model-based reinforcement learning with a generative model. In H. Larochelle, M. Ranzato,\nR. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,\nvolume 33, pp. 12861\u201312872. Curran Associates, Inc., 2020.\n\nShengbo Wang, Jose Blanchet, and Peter Glynn. Optimal sample complexity of reinforcement\nlearning for uniformly ergodic discounted Markov decision processes, 2023."
            },
            "questions": {
                "value": "- What are main barriers to provide an algorithm with dependence not on a mixing time-type quantity but on span of optimal value? This questions has its importance because it is know that this guarantee will be strictly tighter than mixing dependent.\n- Is it possible to provide an algorithm without a reduction to discounted setting?\n- Is it possible to extend this approach to exploration setup and provide a feasible algorithm with guarantees like (Orther, 2020)?\n\nOrtner, Ronald. \"Regret bounds for reinforcement learning via markov chain concentration.\" *Journal of Artificial Intelligence Research* 67 (2020): 115-128."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Reviewer_qCaw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698683175569,
        "cdate": 1698683175569,
        "tmdate": 1699636106827,
        "mdate": 1699636106827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AJ65kIqvdi",
        "forum": "jOm5p3q7c7",
        "replyto": "jOm5p3q7c7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_fwUc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1776/Reviewer_fwUc"
        ],
        "content": {
            "summary": {
                "value": "This paper resolves the issue of sample complexity associated with maximizing the long-term average reward dictated by a uniformly ergodic Markov Decision Process (MDP), predicated on the assumption of a generative model. The findings of this study enhance the pre-existing results by a factor of $t_{mix} and align with the established lower bound. The algorithm introduced herein is a synthesis of the methodologies proposed by Jin & Sidford (2021) and Li et al. (2020)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper addresses a significant issue in the domain of Markov Decision Processes, providing a solution to the sample complexity associated with maximizing the long-term average reward. This is a valuable contribution that could potentially advance understanding and application in this area.\n2. By enhancing pre-existing results by a factor of the mixing time and aligning with the established lower bound, the paper provides a comparative analysis that underscores the improvements made and the relevance of the studies.\n3. This paper is well-written and easy to understand."
            },
            "weaknesses": {
                "value": "1. The algorithm is primarily a synthesis of methodologies from Jin & Sidford (2021) and Li et al. (2020). While this approach has its merits, the novelty of this paper is somewhat limited given its dependence on previous works.\n2. The paper could be enhanced by placing greater emphasis on the challenges addressed by the study and the innovative aspects of the proposed algorithm. Highlighting these elements would help to showcase the unique contributions of the paper and further establish its significance in the field."
            },
            "questions": {
                "value": "1. The paper could benefit from a greater emphasis on the challenges addressed by the study. Could you provide more information on the specific challenges inherent to the problem you are solving and how your approach effectively addresses these issues? \n2. Could you please elaborate on the unique aspects of your algorithm and how it distinctly contributes to the field beyond the synthesis of methodologies from Jin & Sidford (2021) and Li et al. (2020)? Highlighting the novel components of your approach could significantly strengthen the impact of your paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1776/Reviewer_fwUc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1776/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719798700,
        "cdate": 1698719798700,
        "tmdate": 1700616330424,
        "mdate": 1700616330424,
        "license": "CC BY 4.0",
        "version": 2
    }
]