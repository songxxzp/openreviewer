[
    {
        "id": "ZYCZhq4Aer",
        "forum": "XIaS66XkNA",
        "replyto": "XIaS66XkNA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a novel approach to generative modeling based on the idea of idempotence. An idempotent function $f: X \\rightarrow X$, is one for which $f(f(x)) = f(x)$ for any $x \\in X$. The aim of the model proposed in the paper, Idempotent Generative Network (IGN), is to achieve idempotence so that for any $x$ drawn from the source distribution, $f(f(x)) = f(x)$. The paper provides several justifications for why this property is desirable in the context of generative modeling that essentially amount to IGN having many of the nice properties held individually by either GANs, diffusion models, etc. The paper has an interesting discussion of the loss function needed to train such a model and provides one theoretical result justifying the optimization approach. Several experiments are run, including use of the idempotent property of IGN to project noisy or corrupted examples back to the target distribution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Underlying idea:** The use of idempotence in deep learning is appealing given its deep connection to mathematics and theoretical computer science. Because deep learning is essentially a science of function composition, connecting it to category theory and related domains focused on composition in pure mathematics is a rich area for exploration. This is a great example of what first steps in that direction look like. Since idempotents have been studied in great depth, there is likely many additional constructions that could be explored using this work as a starting point.\n- **Simplicity of construction and clarity of writing:** IGN is described very clearly so that readers from a range of backgrounds can understand the constructions. The different parts of the model (particularly the loss function terms) are well-justified both mathematically and informally in the text. As might be expected when researchers utilize a truly foundational idea, constructions are surprisingly simple (as was the case in this paper). \n- **Quality of the outputs:** Despite the fact that this is a novel generative framework, this reviewer felt that the outputs looked quite good. It was especially exciting to see that that $f(f(f(x)))$ looked mostly the same as $f(x)$. As described below, the reviewer would be interested to see $f^k(x)$ as $x \\rightarrow \\infty$. \n- **Opening paragraph:** Though it is perhaps a small thing, this reviewer really appreciated the opening quote. It captures the underlying idea of the paper and made me laugh."
            },
            "weaknesses": {
                "value": "- **Motivation:** While the idea is interesting and utilizes foundational structures from mathematics, the motivation for idempotence in generative models is still a little weak. Several different consequences are described in the introduction (IGN provides outputs in a single step, yet allows additional refinements, etc.) Each of these properties is attractive, but already possessed by an existing mature method (as is pointed out in the paper). It was unclear to this reviewer whether starting from scratch with a completely new paradigm is the right way to obtain the desirable properties of GANs, diffusion models, etc. On the other hand, idempotence is a deep idea whose consequences have been explored in a wide range of settings and directions. It feels likely to this reviewer that there are good reasons for wanting idempotence in a model that go beyond properties we already have in alternative methods. However, these may yet need to be uncovered.\n- **Comparison to other methods:** While the reviewer agrees that it is not fair to hold this novel approach to the standards of more mature methods, it would still be useful to be able to compare the performance of IGN with known Approach. As it is, this reviewer was unable to tell how far IGN is from the performance of a comparably sized GAN, VAE, or diffusion model. \n- **Quantitative results:** It appears that the only way the reader can evaluate the experiments is through a handful of example generations. While these are helpful for getting a qualitative sense of the model (and look good, as described in the Strengths section), it would have made the analysis stronger if quantitative results were also presented. It would be interesting, for instance, to understand network performance for different types of initial $x$, especially since the domain of the network includes both $\\mathcal{P}_z$ and $\\mathcal{P}_x$. It would have also been interesting to quantitatively measure differences in $f^{(k)}(x)$ as $k \\rightarrow \\infty$.\n\n### Nitpicks\n\n- Idempotence is spelled wrong in the sentence below equation (7).\n- The tightness objective is a little subtle. It might make sense to utilize some of the notation already introduced (e.g., $\\mathcal{S}$) to help communicate why this loss term is necessary. \n- It may be a matter of personal taste, but this reviewer felt that while the PyTorch code was appreciated, it might be better to include in the Appendix and instead include additional analysis of the experiments."
            },
            "questions": {
                "value": "- One of the claimed strengths of IGN is that through the idempotence property, it can be used to project noisy or corrupted images back to the data manifold. Based on the reviewer\u2019s understanding of the capability, there are already a range of methods for doing this (e.g., via image denoising with a diffusion model). Is there some additional nuance that the reviewer is missing? It is certainly the case that existing methods will not map an image plus noise $x + \\epsilon$ by to $x$ exactly, but it was our sense from reading the paper that neither would IGN.\n- It would be interesting to see the behavior of $f^{k}(x)$ as $k \\rightarrow \\infty$, does this converge or diverge? If it does converge, what does it converge to?\n- This reviewer did not understand the modified GAN network architecture described in Section 2.3.\n- As the authors note, the development of high-performing generative models has come about through numerous iterative improvements. Would it be possible to build an IGN model using the framework of a known generative approach?\n- What are the current drawbacks to scaling this method up?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3420/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3420/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3420/Reviewer_j2FZ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698385504196,
        "cdate": 1698385504196,
        "tmdate": 1700597898401,
        "mdate": 1700597898401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "diPJnAIYvD",
        "forum": "XIaS66XkNA",
        "replyto": "XIaS66XkNA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_5afM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new class of generative model that is based on idempotent, i.e., $f(f(x)) = f(x)$. It acts like a projection function that project anything to the target data manifold, and it preserves identity for data already in the target manifold.To train such a model, first a reconstruction loss is used to let the model map latent z to the data manifold, and an idempotent loss to ensure the property of idempotent. An additional tightness loss is used for regularization. Some preliminary results of generation is shown."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The biggest strength is that the idea is novel and refreshing. It introduces a new class of generative models with an interesting formulation.\n\n2. Overall the presentation is good. The idea is clearly stated in the pseudo code. The introduction of idempotent is clear and interesting in the first section.\n\n3. Although the results are preliminary, there are some interesting observations. For example, the method can naturally take things other than noise as input (e.g., Figure 5 c)"
            },
            "weaknesses": {
                "value": "1. The biggest issue is that I don't get the motivation for designing such a model. When one designs a new class of generative model that is different from any previous approaches, it is necessary to clarify that it has the potential to address some limitations of previous approach. As a preliminary study, it does not need to have great results, but conceptually the advantage of the model has to be stated. Otherwise, it is just \"another class of generative models\". Since it suffers the same mode collapse and instability issue of GAN, the motivation becomes more important. Why do we need such a model? What are the potential advantages?\n\n2. I don't quite get the point of the tightness loss. Why maximize it is good for regularization? More explanation is needed."
            },
            "questions": {
                "value": "My questions are stated in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784210553,
        "cdate": 1698784210553,
        "tmdate": 1699636293398,
        "mdate": 1699636293398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wuR76FeQvW",
        "forum": "XIaS66XkNA",
        "replyto": "XIaS66XkNA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_Cz9y"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new kind of generative model for image synthesis called the Idempotent Generative Network (IGN). The key idea is to learn a function $f$ such that $f(x) = x$ if $x$ is on the data manifold, and $f(z)$ is on the data manifold if $z \\sim P_0$ for some prior $P_0$ (in which case $f(f(z)) = f(z)$). The network $f$ is learned with three loss terms: a reconstruction loss to encourage data samples to be mapped to themselves, an idempotent loss to encourage two applications of $f$ to a state $z$ from the prior to match a single application of $f$, and a tightness objective encouraging $f$ to map any state $z'$ not on the manifold as far away from the manifold as possible. A theoretical analysis is presented which claims that the proposed method will generated samples from the data distribution if $f$ is perfectly trained and has enough capacity. Experimental results show that the proposed method can generate MNIST and CelebA images from noise."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The work is an ambitious and refreshing approach to generative modeling. Idempotence is an interesting foundation for building a generative model. Overall the paper was enjoyable to read.\n* The central ideas are clearly presented and I could easily follow the training algorithm and motivation of the method.\n* Empirical results show that the method is capable of image generation and editing the features of generated faces."
            },
            "weaknesses": {
                "value": "* I am not fully convinced of the theoretical validity of the method. In particular, Equation (19) does not appear fully rigorous. From my understanding, the value $M$ is the maximum distance between points in the state space. \n  * What if the state space is unbounded? Should we expect states to be restricted to an image hypercube?\n  * Why is it the case that $max_{\\theta*} \\delta_{\\theta*}(y) = M$ for any $y$ when $P_x (y) < \\lambda_t P_{\\theta*} (y)$? Wouldn't that mean the distance between $y$ and the furthest point from $y$ in the state space is $M$ for each $y$? I am not sure that makes intuitive sense to me. It seems that $M$ should not be a constant but rather a function $M(y)$ measuring the distance between $y$ and the furthest point in the state space, which greatly complicates the analysis that follows.\n\n  The proof heavily relies on the simple form of (19), but I am not sure this simple form is valid. Furthermore, in practice the distance $M$ is heavily restricted which leaves a gap between the theoretical development of the method and practical implementation. Since this method departs heavily from the established probabilistic principles used to justify generative models, clearly demonstrating the validity of the method is essential. I am willing to raise my score if these points can be addressed.\n* Although I understand that the current results are meant to be a proof of concept, it would still be good to provide a more formal comparison with prior generative models e.g. by calculating FID scores on CelebA."
            },
            "questions": {
                "value": "Please see my questions about the theoretical justification of the method in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830669363,
        "cdate": 1698830669363,
        "tmdate": 1699636293309,
        "mdate": 1699636293309,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LNE9jfvBw2",
        "forum": "XIaS66XkNA",
        "replyto": "XIaS66XkNA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3420/Reviewer_Sz9m"
        ],
        "content": {
            "summary": {
                "value": "This work presents what appears to me to be a novel way to model the generative process from noise (or corrupted images) using idempotence as the underlying principle for the objective(s). Notably, they train a model which at optimum should maintain given datapoints from the target distribution, yet generate samples whose manifold is maximally \"tight\". These objectives together yield a novel generative model algorithm, and there is theoretical and empirical support."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall I found the approach to be appealing and interesting. Though there isn't necessarily a strong compelling reason here why to use idempotence to train a generative model, I don't find this as a weakness (prefacing here), as there is scientific curiosity here, the intuition is compelling, the theory seems correct wrt the optima fitting the target distribution, and the empirical results support everything neatly. I'd also mention that the results are stellar / SOTA, but as noted in the paper: this is fine. Engineering might improve things or it might not, just generally it looks like good science with a solid story and results to support."
            },
            "weaknesses": {
                "value": "Some points could be clarified, notably I found the discussion around Figure 2 to be less clear than I would have liked. I think I understand the general gist, but I honestly didn't think I understood how the different objectives work together until I got through the theory section. Could this be cleaned up a bit / made a bit more clear? I'd enjoy a bit of some discussion here to see if we can arrive at a better way to present this."
            },
            "questions": {
                "value": "Why does M:= sup(D(y1, y2)) show up in eq 19? Are we saying if y is outside the target data manifold + the margin that the repeated applications of f should maximize the distance? How is this guaranteed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3420/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698873070735,
        "cdate": 1698873070735,
        "tmdate": 1699636293237,
        "mdate": 1699636293237,
        "license": "CC BY 4.0",
        "version": 2
    }
]