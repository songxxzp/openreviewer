[
    {
        "id": "cew9k2vu4x",
        "forum": "KpC3dPumJj",
        "replyto": "KpC3dPumJj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_MMqo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_MMqo"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for measuring the importance of data used to perform supervised fine-tuning (SFT) of large language models. This measure is used to select the most important data examples in order to select a subset of SFT data that is most beneficial for the fine-tuning process. They introduce a measure of data importance they call \"learnability\" which is computed with respect to a specific model and expresses three design choices (i.e. constraints) the authors underline, which can be summarized as (1) assign a low score to uninformative data; (2) assign a low score to hard-to-learn data; (3) assign a high score to efficiently learnable data. The authors compare their prioritized selection method against training on all SFT data and against asking Chat-GPT to filter out uninformative data examples. Experimental evaluation demonstrates that their method generally outperforms these two baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**S1**: Supervised fine-tuning is becoming a crucial task when applying LLMs to specific scenarios and doing it in a data-efficient way is important.\n\n**S2**: The proposed method is quite simple which makes it easy to apply in practice.\n\n**S3**: The experiments demonstrate the effectiveness of the proposed method and show that it is able to remove some \"hurtful\" data which ends up giving a better-performing model that was fine-tuned only on a subset of the SFT data."
            },
            "weaknesses": {
                "value": "**W1**: The problem of selecting optimal subsets of training data is far from novel. It is known as the coreset selection problem and many methods have been proposed. It would be good to add some references to that line of work.\n\n**W2**: The proposed design constraints are argued for using simple reasoning techniques (section 3.3). However, one could also see them as arbitrarily chosen. For example, the statement \"When a piece of data is incomprehensible or overly challenging for the model, *introducing such data during fine-tuning is detrimental*.\" is strange because one might ask where the evidence is that it is detrimental. It would be useful if each of the constraints were empirically shown to be necessary (i.e. individually, not together) using some ablation analysis.\n\n**W3**: The notion of \"learnability\" is referred to across the paper as a \"dimension\", \"measure\" and \"perspective\". To me, it seems like the best fit would be to call it \"measure\" and make this uniform across the paper.\n\n**W4**: I would rephrase the x-axis label in Figures 3 and 4 from \"Scale\" into something more clear (e.g. amount of data selected).\n\n**W5**: There are several strange statements in the related work section: (1) \"the distribution of the data should ideally be *uniform* and aligned with the requirements of the intended usage scenarios\" -- how can a data distribution be uniform?; (2) \"ChatGPT to assess data quality, which carries the risk of data leakage and considers only the inherent quality of the data\" -- what is the meaning of the term inherent quality of data?"
            },
            "questions": {
                "value": "**Q1**: Doesn't the introduction of normalization as described in Section 3.4 contradict constraint 2? Namely, constraint 2 implies that we should be able to detect data that is excessively demanding by observing large $L_{ini}$ and $L_{ref}$ values. On the other hand, in section 3.4 it is implied that this data does not \"meet our expectations\". (also, it is not clear, which expectations?)\n\n**Q2**: What is the difference between the backbone and the baseline model?\n\n**Q3**: What is data mixing (as mentioned in section 3.4)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Reviewer_MMqo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3424/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816769764,
        "cdate": 1698816769764,
        "tmdate": 1699636294154,
        "mdate": 1699636294154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C4mpsLqHbE",
        "forum": "KpC3dPumJj",
        "replyto": "KpC3dPumJj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_b7hv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_b7hv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a way to select fine-tuning data for downstream LLMs using the normalized difference between the pre-trained model and a fine-tuned model, which this paper calls the \"reference\" model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is easy to understand. Details are explained when necessary."
            },
            "weaknesses": {
                "value": "* Typo: in page 2, CharGPT --> ChatGPT\n* It is unclear what the novelty of the proposed method is. Moreover, the model is compared to random selection and ChatGPT, with little evidence to support that these are state-of-the-art baselines. Details on the ChatGPT-based filtering are scarce. For example, what prompt is used?\n* The paper shows results as \"wins\", \"ties\", or \"losses\", without showing a table of actual scores on the test sets.\n* The experimental design for the Alpaca-4 experiments (Section 4.2) is flawed. For instance, the normalized method being better in the Alpaca-3.5 experiments does not indicate that it will also be better in the Alpaca-4 experiment.\n* The proposed method requires a new \"reference\" model to be trained via fine-tuning, which can be prohibitively expensive on top of a downstream LLM."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Reviewer_b7hv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3424/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826472914,
        "cdate": 1698826472914,
        "tmdate": 1699636294083,
        "mdate": 1699636294083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kuiYAsxQvX",
        "forum": "KpC3dPumJj",
        "replyto": "KpC3dPumJj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_JK8S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_JK8S"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the problem of selecting fine-tuning samples for instruction tuning of pre-trained LLMs. Different from prior work that often targets data quality and distribution, this paper introduces a new aspect for selecting data, dubbed \"learnability\".\n\nSpecifically, a reference model is first obtained by fine-tuning a pre-trained LLM (\"initial model\") on full instruction tuning samples. Then, 3 constraints are imposed:\n1. samples lacking information (samples with a small loss on both reference LLM and initial LLM) are removed;\n2. hard samples (samples with a high loss on both LLMs) are removed;\n3. \"learnable samples\" (samples with a high loss on initial LLM but lower loss on reference LLM) are selected.\n\nA set of experiments is conducted on LLaMA-7B/13B and compared with ChatGPT-based filtering methods. The paper claims to achieve better performance after fine-tuning on 3k selected samples compared to fine-tuning on full 52k samples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The perspective of this work is interesting. This angle of sample \"learnability\" is attractive and feels promising. The problem being investigated is timely."
            },
            "weaknesses": {
                "value": "- The structure of this paper is rather loose. The writing style is problematic. The paper contains too many non-specific descriptions for the methods of this work (\"we focus on the new aspect of learnability\") or its results (\"we achieve better performance with 6% of data\"). Its actual technical contribution or specific methodology is not at all introduced until Section 3.3 on Page 4. The abstract says nothing about what this work actually does and the introduction also remains on the descriptive level. This is not the style for a research paper. Subjective descriptions should be used with discretion and only when necessary. The major technical body of this work should be put to the front in the most straightforward manner. The language throughout the paper needs to stay objective and rigorous. \n\n- The methodology described in Section 3.3 is intuitive but somewhat superficial. I do not mind empirical papers based on insights and intuitions. This is nowadays a major drive for the progression of this field. Yet the description in Section 3.3 is too simple for me to feel comfortable. These criteria for samples being \"too hard\", \"too simple\", and \"in-between and good\" are overly subjective. At least, no analysis is provided to ground it to existing frameworks. I guess this threshold is also set in an ad-hoc manner and needs to be tuned with trial and error in each case. It could provide much higher value if the authors could develop it into a principled framework\n\n- The references in this work lack depth. It focuses overwhelmingly on the work during the past year and does not connect to lines of existing research with a richer history (e.g., learnability of samples, data selection problems, simple or hard samples, etc.).\n\n- The term \"supervised fine-tuning\" used throughout this work actually refers to instruction-tuning. Supervised fine-tuning has a much broader reference than the case studied in this work, especially when not confined to text-completion LLMs.\n\n- Recently, there is already a wealth of work on this topic of \"instruction mining\" and I believe many have reported results similar to this work\u2013achieving comparable or better performance with a small fraction of 52k Alpaca instruction samples, which is believed to contain low-quality samples that would hurt the performance. The performance reported in this work isn't particularly stronger than the provided baseline and not many baselines are considered.\n\n\n- Reproducibility: It is unknown how to set the threshold in the proposed constraints. It seems to be manually picked without a principled method or analytical insights.\n\n- Format: Appendix is not cut from the main paper. The PDF provided for the main paper is this 14-page document."
            },
            "questions": {
                "value": "Appendix should not be submitted under the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3424/Reviewer_JK8S"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3424/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698898585435,
        "cdate": 1698898585435,
        "tmdate": 1699636294022,
        "mdate": 1699636294022,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qTCe90IvPQ",
        "forum": "KpC3dPumJj",
        "replyto": "KpC3dPumJj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_rNQa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3424/Reviewer_rNQa"
        ],
        "content": {
            "summary": {
                "value": "The authors present a novel metric for example selection for\nsupervised fine tuning, inspired by a learnability principle."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Selecting fine tuning samples based on learnability principles seems a sensible idea.\n\nThe paper is easy to follow."
            },
            "weaknesses": {
                "value": "The novelty of the proposed learnability metric is unclear to me. The\nauthors propose three different criteria that are just different\naspects of the same criterion, namely relative loss reduction, which\nis the normalized formula they eventually derived (albeit they\napparently did not recognize it as such).\n\nThe experimental evaluation is not entirely convincing:\n\nAbout the comparison with ChatGPT selection: why not using the same\nnumber of data points? e.g. 9,229 points also for your approach? the\ncomparison is not on equal grounds otherwise.\n\nNo comparison in made with alternative data selection procedures, even\nif a number of them are listed in the related work section. I don't\nthink these can be dismissed without a comparison if the authors are\nto claim that their approach is a general solution to SFT.\n\nThe robustness of the approach and the generality of the results\nshould be better assessed. For instance, Figure 4b indicates an\noscillatory behaviour that can be detrimental to the method. I believe\nmultiple datasets should be tested to present robust results."
            },
            "questions": {
                "value": "why not using the same number of data points in the comparison with ChatGPT select?\n\nHow did you enroll participants for human evaluation? how many did you have? \n\nAlso please comment on the concerns I raised in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3424/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909938818,
        "cdate": 1698909938818,
        "tmdate": 1699636293951,
        "mdate": 1699636293951,
        "license": "CC BY 4.0",
        "version": 2
    }
]