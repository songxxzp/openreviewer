[
    {
        "id": "nfmGh37qXR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_iJ4P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_iJ4P"
        ],
        "forum": "KUNzEQMWU7",
        "replyto": "KUNzEQMWU7",
        "content": {
            "summary": {
                "value": "The paper focuses on the evaluation of mathematical reasoning based on visual inputs. This work presents a review of existing work on the topic, a new benchmark (MathVista) made of existing datasets plus three new ones, and a large evaluation of existing large pretrained models on this benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Thorough review of the existing work on the topic, with a proposed taxonomy that clarifies and organizes the capabilities and settings relevant to visual/mathematical reasoning.\n\n- Consolidation of existing datasets into a comprehensive benchmark.\n\n- Large evaluation of existing models, under various settings (zero-shot, few-shot ICL, with various prompting strategies)."
            },
            "weaknesses": {
                "value": "W1. A potential downside of the chosen tasks is that they \"amalgamate\" mathematical and visual reasoning (as stated in the abstract). This does not seem desirable since one would usually also wants to understand the capabilities of a model for these two steps (visual understanding and reasoning) independently. The argument that there exists other benchmarks that do look at these individual capabilities means that this is however not a critical issue.\n\n------\n\nW2. The low human performance on the benchmark (~60% accuracy) is concerning. Could this indicate an issue with data quality of annotation noise? (rather than intrinsic task difficulty)"
            },
            "questions": {
                "value": "Please comment on W2 above.\n\nMinor question regarding the sampling of data for \"testmini\", the text mentions the following:\n\"The KL Divergence and Total Variation (TV) distance between the testmini set and the entire set are 0.008 and 0.035\"\nWhat is being compared with KL and TV distances? Distributions of what?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697221996813,
        "cdate": 1697221996813,
        "tmdate": 1699636775462,
        "mdate": 1699636775462,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FOzqvIkgNI",
        "forum": "KUNzEQMWU7",
        "replyto": "KUNzEQMWU7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_Vb5K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_Vb5K"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a benchmark to evaluate the ability of math reasoning with visual contexts. The test-only benchmark contains a relatively large number of examples (6k), featuring a wide coverage of math reasoning types (7), data sources (28), and task types (5). On the benchmark, representative multi-modal LLMs are evaluated. The results show that Bard is the best performing model among all the evaluated ones, getting 35% accuracy, which indicates that the current models still suffer from math reasoning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The dataset is well-designed and relatively large. It contains 6k examples, coming from various (28) sources including existing ones and newly collected ones. The datasets also evaluate various types of reasoning abilities and tasks. The images contain synthetic, real and text-heavy images, which is a good coverage of different types.\n2. The paper focuses on a specific topic and defines the problem well, which is important for LLM evaluation.\n3. Several SoTA multimodal LLMs, including miniGPT-4, LLaVA(R), Bard, Instruct-BLIP, etc. are evaluated on the benchmark, and show reasonable results."
            },
            "weaknesses": {
                "value": "1. How to disentangle the visual understanding ability or text understanding ability with the math reasoning ability? For example, if a model incorrectly answers \u201chow many cars are to the left of the tree\u201d, it could be the error in spatial understanding (failing to find the cars on the left), or error in counting, or the model cannot understand this question. In the current form of the dataset, there is no disentangled evaluation of the visual/text understanding versus math reasoning. A possible way to address this questions could be having the annotations for **rationales** (results of intermediate reasoning steps or sub-questions), and evaluate the model\u2019s performance against the intermediate steps.\n2. Why is the human accuracy only 60.3% on the dataset? Does this suggest that the dataset is noisy, containing ambiguous/uncertain cases, or simply the task is very difficult thus humans are not good at it as well? An analysis on the 40% of the data where humans cannot answer correctly will be preferred. \n3. While it is good that the paper defines 7 types of fine-grained reasoning ability, what are the take-away messages (of Tab-2) that can be derived with the differences in the 7 types? It would be nice if the expertise of different models can be reflected using the fine-grained types, besides that Bard is the best model\n4. Results of GPT-4V? I understand that the model is not released by the submission deadline, but it would be good to have the results in later versions."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698436566781,
        "cdate": 1698436566781,
        "tmdate": 1699636775341,
        "mdate": 1699636775341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6XUiOrIzyU",
        "forum": "KUNzEQMWU7",
        "replyto": "KUNzEQMWU7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_2daf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_2daf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a mathematical and visual reasoning benchmark to evaluate various visual-based mathematical skills of large-language models (LLMs) and large-multimodal models (LMMs). The authors introduce a taxonomy for mathematical visual reasoning which involves seven different types of mathematical reasoning scenarios and five associated tasks including figure-based question answering, math world problems and geometry problem solving. The visual scenarios are targeted to be diverse involving real images, synthetic scenes, charts and plots, scientific figures, etc. While majority of the benchmark is formed through 28 existing publicly-available datasets, authors form three new datasets targeted to fill gap for mathematical scenarios not covered by existing datasets. \n\nThe benchmark is relatively small and meant to be a zero-or few-shot evaluation benchmark divided into a \"testmini\" (1000 examples) and \"test\" (5141 examples). Evaluation of several model types (including LLMs, LLMs with visual context augmentation and large m) under different setups (including chain-of-thought, program-of-thought, few-shot) is performed and results indicate that current models perform poorly in comparison to humans. A brief error/success analysis and qualitative examples are provided for multimodal bard and context-augmented GPT-4."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The construction of a visual-based mathematical reasoning benchmark to evaluate current LLMs and LMMs is well motivated and relatively novel. The method for construction and statistics of the final benchmark are adequately described with appropriate references to source datasets and prior works.\n\n2. The identified taxonomy covers a broad range of mathematical reasoning scenarios and tasks, and diversity is also maintained in the visual contexts. Further, the 3 newly collected datasets consider new tasks not covered by past works. \n\n3. Evaluation is performed on prominent LLMs (such as GPT-4, ChatGPT, Claude) and LMMs (mPLUG-Owl, InstructBLIP, LLaVa, Multimodal-Bard). LLMs are evaluated in zero-shot, few-shot, chain-of-thought and program-of-thought settings and also when they are augmented with visual contexts. Further, human performance is computed and qualitative analysis and fine-grained result comparisons are performed to better highlight capabilities and limitations of existing models. \n\n4. Paper is generally well written with appropriate figures and details illustrating the dataset examples and analysis, performance breakdown, qualitative examples, model prompts/settings and annotation methods."
            },
            "weaknesses": {
                "value": "1. For data collection of the 3 new datasets, it is not clear if inter-annotation consistency checks were conducted and how the mentioned \"rigorous review process\" was conducted (details are missing).  \n\n2. Few-shot performance is computed only for LLMs and not LMMs. Given LMMs such as Multimodal-Bard, Flamingo/Open-Flamingo and mPLUG-Owl also support few-shot learning, these can also be evaluated few-shot to better evaluate the benchmark challenges. \n\n3. Further, LLMs can also be evaluated on a broader range of K-shot settings (currently only 2-shot is evaluated). Evaluation over {2,4,8,16,32} could provide better evidence of whether mathematical reasoning capabilities can be learned in a few-shot manner.\n\nRelatively minor:\n\n4. Benchmark is relatively small (6141 examples) and meant as an evaluation benchmark primarily drawn from existing datasets (5405 examples) with no finetuning subset which could be useful for improving mathematical reasoning capabilities of current models."
            },
            "questions": {
                "value": "Please see the weaknesses section above (primarily points 1,2 and 3)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6738/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6738/Reviewer_2daf",
                    "ICLR.cc/2024/Conference/Submission6738/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775751942,
        "cdate": 1698775751942,
        "tmdate": 1700588794405,
        "mdate": 1700588794405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "joAYq7Bm9p",
        "forum": "KUNzEQMWU7",
        "replyto": "KUNzEQMWU7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_juci"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6738/Reviewer_juci"
        ],
        "content": {
            "summary": {
                "value": "This work introduces MathVista, a benchmark for evaluating the mathematical reasoning abilities of large language models (LLMs) and large multimodal models (LMMs) within visual contexts. The work uses data from a broad range of existing math and visual question-answering datasets and constructs three novel datasets: IQTest, FunctionQA, and PaperQA. The work evaluates nearly a dozen models with MathVista and finds that Multimodal Bard, the best-performing model, achieves 58% of human performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Curating the datasets from existing sources and developing three new datasets contribute to comprehensive and diverse testing.\n\n2. Evaluating nearly a dozen models and comparing their performances with a human benchmark provides insight into current performance.\n\n3. Mathematical reasoning within visual contexts is an important field."
            },
            "weaknesses": {
                "value": "1. The results have been completely superseded by GPT-4 Vision, which together with GPT-4 are SotA in vision-language models.\n\n2. The methods have been superseded by recent prompting methods.\n\n3. The related work and references are lacking:\n\na. Prompting:\n+ Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement\nL. Qiu, L. Jiang, X. Lu, M. Sclar, V. Pyatkin, C. Bhagavatula, B. Wang, Y. Kim, Y. Choi, N. Dziri, X. Ren, 2023.\n+ Hypothesis search: Inductive reasoning with language models, R. Wang, E. Zelikman, G. Poesia, Y. Pu, N. Haber, N. D. Goodman, 2023.\n+ Large language model (LLM) as a system of multiple expert agents: An approach to solve the abstraction and reasoning corpus (ARC) Challenge, J. T. Min, M. Motani, 2023.\n\nb. GPT-4V:\n+ Lost in translation: When GPT-4V(ision) can\u2019t see eye to eye with text, a vision-language-consistency analysis of VLLMs and beyond,\nX. Zhang, S. Li, Z. Wu, N. Shi, 2023.\n\nc. PoT:\nSolving Linear Algebra by program synthesis, I. Drori, N. Verma, November 2021.\nSolving Probability and Statistics problems by probabilistic program synthesis at human level and predicting solvability\nL. Tang, E. Ke, N. Singh, B. Feng, D. Austin, N. Verma, I. Drori, AIED, 2022.\nA neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level\nI. Drori, S. Zhang, R. Shuttleworth, L. Tang, A. Lu, E. Ke, K. Liu, L. Chen, S. Tran, N. Cheng, R. Wang, N. Singh, T. L. Patti, J. Lynch, A. Shporer, N. Verma, E. Wu, G. Strang, PNAS, 2022.\n\n4. The paper lacks a comprehensive discussion of the limitations of the benchmark.\n\n5. The work is missing an analysis of why specific models perform better than others.\n\nI expect the author response to include GPT-4 Vision results, missing related work and references, and updated methods."
            },
            "questions": {
                "value": "While the paper covers a broad range of models,\n \nit is missing an analysis of why specific models perform better than others?\n\nand what features of each model contribute to performance? \n\nThis would be a valuable contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6738/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849418065,
        "cdate": 1698849418065,
        "tmdate": 1699636775099,
        "mdate": 1699636775099,
        "license": "CC BY 4.0",
        "version": 2
    }
]