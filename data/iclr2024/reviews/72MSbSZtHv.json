[
    {
        "id": "P7Z0GyPcRT",
        "forum": "72MSbSZtHv",
        "replyto": "72MSbSZtHv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_NoCd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to reduce the number of map tokens by the Barlow Twins objective. They try their method on Waymo Open Motion dataset."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. It is important in the autonomous driving field to reduce model size to speed up."
            },
            "weaknesses": {
                "value": "1. As a work focused on efficiency, no statistics about inference memory footprint or inference time are reported. I suggest the authors report these related statistics and compare with open-sourced classic baselines like MTR/QCNet. Otherwise, the claimed advantage of reduction can not be known.\n\n2. Wrong experiment setting. The authors state that *we use 100% of the training data for pre-training and fine-tune on only 12.5%.*. However, the data used for pretraining is already annotated data for motion prediction and I do not see explanations about the difference between pretraining data and finetune data. If this is the case, then it is meaningless to adopt this setting. Thus, I suggest all experiments should be re-conducted with 100% finetune for fair comparison.\n\n3. Self-defined metrics without proper justification. For Waymo Open Motion dataset, widely used (and official) metrics are minADE_6, minFDE_6, mAP averaged over 3s, 5s, 8s. I do not see the necessity of using these similar but different metrics in the paper. For fair comparision, I suggest reporting the official metrics in the paper.\n\n4. No test set results. Waymo Open Motion Leaderboard is easy to submit and I think it is **very necessay** to report the results on it to avoid overfitting the validation set."
            },
            "questions": {
                "value": "See weakness part.\n\n\nIn summary, I enjoy the idea of reducing map tokens, which is important if it can speed up. However, the authors do not mention any inference memory or speed related statistics. Besides, the experiment settings and metrics are unjustified and no test set results are reported, which triggers the concerns of unfair comparison. Thus, I give a reject rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697543170736,
        "cdate": 1697543170736,
        "tmdate": 1699637156320,
        "mdate": 1699637156320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CV69YLo3KG",
        "forum": "72MSbSZtHv",
        "replyto": "72MSbSZtHv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_1Zub"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_1Zub"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel self-supervised embedding strategy to learn representations\nfor motion prediction datasets in the autonomous driving context. Their method have two main technical\ncontributions. The tokenization of the road embeddings is done using a limited set of discretized tokens with a global attention. \nThe second contribution is  is to enforce the network to represent similar representations with similar tokens. That is done by enforcing correlation between inputs with only a small augmentation distance between them.\n\nThey showed competitive results when compared to other self-supervised representation learning methods\nand other motion prediction methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper presents solid contributions on representation learning for autonomous driving scenes in the motion prediction context.\n* The general explanation of the obtained embeddings for the road map is clear and easy to understand. The experiments with local attention provide \n* The presented results are competitive with the state-of-the-art even though the method focused on the pre-trained representation instead of actually achieving SOTA results on motion prediction benchmarks. That shows a potential scalability of this type of approach.\n* they provide already an anonymized implementation of the code."
            },
            "weaknesses": {
                "value": "1. One potential weakness of the paper is the lack of evaluation of the strategy in a closed loop fashion. While motion prediction benchmarks are still used it has been show that motion prediction alone tends to have poorer quality when evaluated in a closed loop. Examples of that are NuPlan [1] evaluation and the more recent waymo closed loop prediction benchmarks.\n\n2.  For the ablations I missed acquiring a bit more understanding on the relevance of RBT versus just a simple tokenization. It would be nice to assess the impact of enforcing the similar augmented inputs into using the same tokens versus just an arbitrary tokenization. From what I understood the ablations were done mainly on the way the representation was fused.\n\n\n\n\n[1] Caesar, H., Kabzan, J., Tan, K. S., Fong, W. K., Wolff, E., Lang, A., ... & Omari, S. (2021). nuplan: A closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810."
            },
            "questions": {
                "value": "I missed a more clear explanation on the motion prediction baseline used when not applying any pre-training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669864159,
        "cdate": 1698669864159,
        "tmdate": 1699637156111,
        "mdate": 1699637156111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sWoHdCIpDd",
        "forum": "72MSbSZtHv",
        "replyto": "72MSbSZtHv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9187/Reviewer_4aEu"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of representation learning for motion prediction. They first pre-train a map encoder to output similar representations for map inputs across augmentations, where augmentations include rotations of the coordinate frame by up to 10 degrees and translations of the coordinate frame by up to 1 meter. They then train a model that takes the map encoding as input as well as historical agent trajectories and outputs future single-agent trajectories parameterized by mixture-of-gaussians. The authors show impressive results in the semi-supervised setting in which the map data for all scenarios is kept, but track data is available for only 20% of the scenes in the waymo open dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper proposes a label-free method for pre-training a map encoder. I think the approach is novel, and I think map-only encoders are increasingly relevant for motion prediction in that, generally speaking, self-driving companies have a stream of motion data, and if the map encoding is fixed, training on new motion data as it comes in should be much more parameter efficient compared to training from scratch every time. I also appreciate the 3D visuals of the motion predictions in the paper, which helps with understanding how the model performs. Table 1 is also impressive, showing that the method proposed in the paper for training the map encoder outperforms a variety of other possible ways to train the map encoder."
            },
            "weaknesses": {
                "value": "I think the main weakness of this paper is that I'm not convinced that both contributions claimed by the paper are valid. Specifically, the first claimed contribution is that the authors design an architecture that reduces the variable-length set of map objects to an encoding of fixed-length. I think the authors should clarify how this encoding is different from \"latent query attention\" used in Wayformer, which also reduces a variable-length set of objects to an encoding of fixed-length. This submission claims \"Compared to Wayformer, we encode past agent trajectories and environment context with separated encoders, offering more flexibility for specialized self-supervised pre-training\". However, wayformer also compares against a \"late fusion\" variant which seems very similar to the architecture proposed in this paper. Some clarification on these architectural choices in comparison to Wayformer are important to add, as well as an explanation of why Wayformer is excluded from Table 2.\n\nAdditionally, this is probably a misunderstanding, but for the second contribution of pre-training the map encoding to be invariant to small rotations and translations, isn't this undesirable? If the map rotates, the motion prediction should also rotate to some extent. I'm not entirely clear on the principle behind this pre-training objective.\n\nSome other comments and requests below:\n\n- Section 4.1 \"dataset\" - I'm lacking the motivation for studying the semi-supervised setting for motion prediction? Collecting training data only requires driving through cities and running perception models, so it seems to me all of the data is always labeled. If the argument is that we can use this approach to fine-tune motion prediction on maps for which we have no motion data, then the authors should explicitly demonstrate that their approach improves generalization to new maps, or pre-train on a dataset of only maps without any motion data. If the argument is that the pre-trained RED tokens are useful for many possible downstream tasks, the authors should benchmark more tasks than just motion prediction.\n- Section 3.2 \"We use the current speed information to learn separate embeddings for static and dynamic agents\" - to be clear, the authors check the current speed and if it's below a certain threshold, they set one of the features in the input to 0 and otherwise they set it to 1?\n- Section 3.2 \"Figure 3c shows the vocabulary size\" - is \"vocabulary size\" the right term? My understanding is that vocabulary size is the size of a set of discrete options. But my interpretation of Figure 3c is that it shows sequence lengths, and none of the inputs to the model are discretized.\n- Section 3.2 \"After the fusion step, we use global average pooling over the feature dimension to reduce the input dimension for an MLP-based motion head\" - is there a reason to do pooling then an MLP as opposed to a transformer layer followed by a linear layer, as in wayformer? Forcing average pooling is not ideal.\n- Section 4.2 \"Interestingly, the Scene Transformer in its joint prediction configuration performs worse than in its marginal prediction version\" - this is expected actually. Joint prediction should be better if minADE/minFDE are evaluated at the scene level, and marginal prediction should be better if minADE/minFDE are evaluated at the per-agent level.\n- Section 4.2 \"our model with 16 trajectory proposals can outperform all other methods\" - I think it's interesting to see a plot of minFDE as a function of samples, but I personally think it's too unfair of a comparison to include a 16-sample version in Table 2.\n- Table 2 - is there a reason the authors don't report mAP? The MTR and MTR++ models are chosen based on mAP which might lead to lower values for minFDE/minADE, so the current comparison isn't exactly apples-to-apples."
            },
            "questions": {
                "value": "If the authors could clarify how their proposed architecture improves over Wayformer, that would be very helpful. I think it's ok if the authors just chose any architecture for the purposes of studying self-supervised pre-training, but since the authors emphasize the design of the architecture as a contribution, I want to understand that contribution better.\n\nIf the authors could also explain in more depth the principle behind their pre-training task, I would also find that to be helpful. From Table 2, it appears to be effective, but I don't yet see why this pre-training objective makes sense for learning good representations of the map information for the purposes of motion prediction."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9187/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699139367081,
        "cdate": 1699139367081,
        "tmdate": 1699637155996,
        "mdate": 1699637155996,
        "license": "CC BY 4.0",
        "version": 2
    }
]