[
    {
        "id": "T3NzGEAc35",
        "forum": "fBlHaSGKNg",
        "replyto": "fBlHaSGKNg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission654/Reviewer_VKdd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission654/Reviewer_VKdd"
        ],
        "content": {
            "summary": {
                "value": "The paper at hand proposes a sampling approach for selecting a smaller but still representative and diverse set from a large dataset. This is applied to the task of semi-supervised learning where the subset selection is applied to unlabeled data which is to be labeled later on."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ relevant problem"
            },
            "weaknesses": {
                "value": "- limited comparison to other sampling methods (only random sampling)\n- marginal improvement over random sampling (Tab. 1 overlapping confidence intervals)\n- not clearly and convincedly presented advantages of the method (e.g., Fig. 2, it's hard to see the claimed benefits)\n \nHonestly speaking ,I'm not sure that the chosen application is the right one. I would recommend focusing on the selection step to approximate the distribution (estimated from the large dataset) with few examples. Compare the proposed approach to related work, discuss pros and cons. Finally sketch various applications (including SSL) briefly which will benefit."
            },
            "questions": {
                "value": "How does the approach scope with low and high dimensions?\nHow does the approach scope with different distributions (overlapping, clusters well separated, etc.)?\nWhat are limitations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780780033,
        "cdate": 1698780780033,
        "tmdate": 1699635992912,
        "mdate": 1699635992912,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CgGJNNZRDy",
        "forum": "fBlHaSGKNg",
        "replyto": "fBlHaSGKNg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission654/Reviewer_Xj3X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission654/Reviewer_Xj3X"
        ],
        "content": {
            "summary": {
                "value": "The paper suggests a maximum mean discrepancy approach to select data for annotation. The approach, referred to as UPA, attempts to capture representative and diverse set of point to improve on active learning. Some theoretical results are presented based on a similar approach to Coarset (Sener et al.) to reduce the risk on the labelled set by selecting a coareset.\nEmpirical validation is presented for a few benchmarks and comparison with a few  active learning approaches and SSAL."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tAn approach the optimizes both diversity and representativeness has advantage\n2.\tSome theoretical results support the advantage of the method.\n3.\tEmpirical results show some advantage over Random baseline and for cifar 10 clear advantage to using flexMatch with UPA"
            },
            "weaknesses": {
                "value": "1.\tThe soundness of the paper is poor: \n \na.\tmostly the empirical validation is lacking additional datasets to support the advantage of the method\n\nb.\timprovement over random baseline is too modest in my view, 0.5 precent improvement when the STD is at 0.5 is not convincing\n\nc.\twe are missing a simple ablation study: if you show results of flexMatch+UPA, you have to show also results of just flexMatch, otherwise it is not clear if the advantage is due to FlexMatch or the addition of UPA.\n\nd.\t3 independent runs is not enough\n\ne.\tWhy are you selecting the particular m values for each data set? why is it different for each data set? I would much prefer seeing a graph over a range of values.\n\nf.\tNot enough method are used in the comparison, what about BADGE by Ash et al -  it is a classical diversity and uncertainty approach that could also be compared to show that representativeness and diversity is better (if it is indeed so\u2026)\n\n2.\tThe requirement that m/n <=0.2 is quite weak and unrealistic, sometimes 0.2 can be a huge data set to annotate! How does that work with Active learning setting in which the budget is limited.\n3.\tClarity is also poor:\n\na.\tThere is no pseudo code describing the method, or at least a set of steps\n\nb.\tWhat is the input space over which the method is used? Is it the actual data? For coreset is it the penultimate layer representation, what are you using?\n\nc.\tRemark 1 is rather confusing, if you present Thm 3.1 and then claim that thm 3.1 doesn\u2019t always work\n\nd.\tFigure 1 can not be deciphered with out a basic explanation\\legend for the colors\n\ne.\tEquation 6: is f_I_p(x_i) why is it defined only for x?\n\nf.\tIn equation (8) what is K in B=2K?\n\n4.\tThe computational complexity of O(mn) should be better explained? Wouldn\u2019t the kernel construction cost more?\n\n5.\tOverall Im not convinced at all that the approach of diversification and representativeness is an optimal one. There are other important trade offs in active learning such as exploration-exploitation, which can outperform this UPA approach, in my view."
            },
            "questions": {
                "value": "1.\tPlease see the questions above in the \u2018weaknesses\u2019\n2.\tPlease explain what does the flexMatch method do , and why you chose it for your UPA approach.\n3.\tWhy do you think the diversification and representativeness is the best one for AL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798439088,
        "cdate": 1698798439088,
        "tmdate": 1699635992833,
        "mdate": 1699635992833,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Mih3oKxonk",
        "forum": "fBlHaSGKNg",
        "replyto": "fBlHaSGKNg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission654/Reviewer_VfL6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission654/Reviewer_VfL6"
        ],
        "content": {
            "summary": {
                "value": "The paper presents the method of selecting instances for annotation, based on the MMD (Max Mean Discrepancy) principle. According to the principle, the method aims to minimize the distance between the full unlabeled dataset, and the sampled instances. An additional parameter $\\alpha$ is introduced to tradeoff sample representativeness with diversity. The Kernel Herding algorithm is used to iteratively find the target set of points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Selecting an optimal subset for annotation is an important problem in scenarios where labels are costly to get. The main contribution of the paper is the demonstration that minimizing MMD for such scenarios helps improve underlying classification accuracy. While there is a clear parallel with the coreset constriction idea, the paper gives a theoretical result which relates the two approaches. The experimental section provides a set of comparisons with the baselines which demonstrate advantages of using MMD."
            },
            "weaknesses": {
                "value": "Introducing the parameter $\\alpha$ doesn\u2019t seem to have enough theoretical or experimental grounding. From the theoretical standpoint, the trivial case $\\alpha=1$ makes all bounds tighter than for any other alpha. From the experimental part, the only ablation analysis of $\\alpha$ is given in table 3, it doesn\u2019t fully convince that using any other value except $\\alpha=1$ is any better. Specifically, the results for $\\alpha=1$ and \"optimal\" $\\alpha=1-1/\\sqrt{m}$ are quite close, and measured only on 3 datapoints for one dataset. More experimental grounding for motivating the choice $\\alpha=1-1/\\sqrt{m}$ over $\\alpha=1$ would help.\n\nWithout taking $\\alpha$ into consideration, there is not much novelty in the introduced methods. Overall, the paper provides good justification of using MMD for sample selection."
            },
            "questions": {
                "value": "In the experimental section, it would be great to have the results of a supervised classifier (not SSL) trained on the selected set of instances, and see at what subset size the accuracy would match the accuracy of a classifier trained on the whole training set."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission654/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission654/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission654/Reviewer_VfL6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849966775,
        "cdate": 1698849966775,
        "tmdate": 1699635992738,
        "mdate": 1699635992738,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8zWtUrlqRQ",
        "forum": "fBlHaSGKNg",
        "replyto": "fBlHaSGKNg",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission654/Reviewer_9fK9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission654/Reviewer_9fK9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for selective a representative as well as a diverse subset for expert annotation. The idea is to weight the terms in MMD-squared distance between the target and the desired subset, such that the trade-off between diversity and representativeness can be explicitly controlled. The weighted distance can be minimized using Frank-Wolfe/Kernel-herding techniques. The kernel-herding algorithm is modified so that samples are not repeated. Error bound for this modified algorithm is presented under certain conditions (Theorem 3.2).\n\nWhen the proposed criteria is used for AL in context of SSL algorithms, empirically it is shown that the methodology outperforms AL and SSAL baselines. (table 2)"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of \\alpha-MMD^2 and its interpretation in (5) are clear and suitable for the AL tasks."
            },
            "weaknesses": {
                "value": "1. The basic methodology essentially selects a representative and diverse subset. There are many methodologies for such a diverse subset selection. e.g., [1*]-[3*]. None of such methods have been discussed nor have been empirically compared with. This makes it difficult to evaluate the significance of the proposal.\n\n2. The weighting idea and corresponding algorithm details are more or less straightforward. (Mainly because it is a simple modification of MMD).\n\n3. Reg. table2. Since UPA is employed above SSL (flexmatch/freematch), it may be important to compare against baseline/SOTA AL criteria when employed with flexmatch/freematch. Then the advantage of the proposed criteria would be explicit. Now it is not clear whether the imporvement is because of flexmatch/freematch or because of the proposed criteria.\n\n\n\n[1*] https://arxiv.org/pdf/2104.12835.pdf\n[2*] https://arxiv.org/pdf/1901.01153.pdf\n[3*] https://proceedings.neurips.cc/paper/2014/file/8d9a0adb7c204239c9635426f35c9522-Paper.pdf"
            },
            "questions": {
                "value": "1. In the proof of theorem3.2, \"j\" is defined as argmin over i=1to n\\I_p for some objective. But in the proof the following is used: f_{I_p}(x_j_{p+1}) <= f_{I_p}(x_j). Is this correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission654/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698860832172,
        "cdate": 1698860832172,
        "tmdate": 1699635992654,
        "mdate": 1699635992654,
        "license": "CC BY 4.0",
        "version": 2
    }
]