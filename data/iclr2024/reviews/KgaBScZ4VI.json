[
    {
        "id": "x4Ee7XnhLg",
        "forum": "KgaBScZ4VI",
        "replyto": "KgaBScZ4VI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_Qrz5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_Qrz5"
        ],
        "content": {
            "summary": {
                "value": "The authors aim to apply simple model cascades to structured output problems of varying length: try to first use a small model, then fall back to a larger model if it appears the small model is insufficiently confident. This task has been used to good effect on problems with simpler output spaces, such as multi-class prediction. In those settings, the log prob of the prediction can be used as a proxy for confidence; low probabilities from a small model trigger inference from a large model. However, in language generation tasks, the length can vary broadly, hence the log probabilities can vary broadly as well. The direct analog of log probabilities would be to sum the log probs of each prediction, but this has undesirable scale issues based on sequence length.\n\nThe authors propose both simple confidence estimate techniques (average, percentiles) and complex methods (learned functions of percentiles, embeddings) that lead to substantial improvements. In some settings the cascade performs better than either model alone, suggesting that the model is attaining some kind of ensemble effect."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors present an accessible introduction to cascades as well as the challenges of application to language generation tasks. The methods they propose are straightforward and easy to implement, and seem to work well.\n\nThe authors evaluate several different tasks, using both simple and complex models, and present reasonable gains.\n\nThe post-hoc methods provide some interesting insights."
            },
            "weaknesses": {
                "value": "The authors only work with a single base model: FLAN-T5. It's not clear how well these results generalize.\n\nThere are other methods of confidence estimation beyond logprobs (see, e.g. https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00598/117737/Calibrated-Interpretation-Confidence-Estimation-in) -- would like to see more analysis here."
            },
            "questions": {
                "value": "In the \"Intermediate embeddings\" approach, only decoder representations are used. However, wouldn't it potentially be useful to characterize aspects of the input? I could see that some inputs might be more reasonable for a smaller model; others might have complexities that are more suited to a larger model. Even input length could potentially be useful. Do you have empirical experimentation here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826590123,
        "cdate": 1698826590123,
        "tmdate": 1699636670601,
        "mdate": 1699636670601,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q1m9Bbj8Cs",
        "forum": "KgaBScZ4VI",
        "replyto": "KgaBScZ4VI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_MA2R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_MA2R"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the model cascade for generation tasks. It notices a crucial difference between cascading for classification tasks and cascading for generation task and points out that the natural extension of predicted class uncertainty to generative tasks, predicted sequence uncertainty, is biased by sequence length, leading to sub-optimal deferral decisions.\nIt then designs a deferral rule to obtain the score/confidence of the small LM and decides when to defer an input to the larger model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tIt demonstrates that simple sequence-level LM confidence measures for deferral can lead to sub-optimal cost-quality tradeoffs due to length bias.\n2.\tThe paper proposes a simple yet effective method employing the quantile of the log-likelihood to design a deferral rule. \n3.\tThe Proposal of a post-hoc deferral rule trained on quantile features and the input embeddings of both the small LM and the large LM. The extensive experiments on FLAN-T5 verify the efficacy of the method."
            },
            "weaknesses": {
                "value": "1.\tCompared with simple averaging the log probability, the major advantage of quantile is that it reflects more about the overall log probability distribution of the sequence and is more robust to outliers. To highlight the motivation of the proposal, more evidence for the existence of the outlier is expected and the showcase in Figure 1 is not sufficient.\n2.\tIn Figure 2 and Figure 3, it seems that the best generation performance is obtained in the middle of the curve, other than the endpoint where all examples are deferred. Does this mean that sometimes smaller LM outperform larger ones?\n3.\tThe author claims that Chow-Sum is overly biased towards deferring longer predictions. However, from Figure3(a) we can obverse that when the output length (y-axis) is between 150 words to 250 words, the score of the oracle deferring strategy is smaller than the Chow-Sum, which says the opposite: the chow-sum isn\u2019t biased towards deferring longer predictions when compared with the oracle.\n4.\tAs another line of work, speculative decoding also aims at the trade-off balance between efficiency and performance and I think the authors should discuss or compare the difference between these two lines of work."
            },
            "questions": {
                "value": "1.\tWhat is the performance of Post-Hoc-Embed-1?\n2.\tHow is the $\\Phi(x)$ computed in detail?\n3.\tWhat is the performance of the post-hoc-quantile when predicting the golden deferring label?\n4.\tIn figure2, figure3 and figure5, all figures use the deferring rate as the x-axis. I am curious about whether we could use the inference time cost as the x-axis."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699026134765,
        "cdate": 1699026134765,
        "tmdate": 1699636670482,
        "mdate": 1699636670482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WkGKctOlae",
        "forum": "KgaBScZ4VI",
        "replyto": "KgaBScZ4VI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel method for uncertainty estimation in large language models (LLM). The method is based on LLM cascades, where a large model predicts difficult instances and a second small model predicts easy instances. In addition, the model uses the Chow-sum and Chow-average as a rule for assigning confidence (uncertainty) to token-level outputs from the LLM cascade.  The main contributions are: i) method for token-level uncertainty estimates, and ii) application of different natural language processing (NLP) tasks. The method shows that the confidence estimates based on the output probabilities from LLMs are biased."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A principled method for uncertainty estimation in LLM (i.e. FLAN-T5).\n- Clear description of background knowledge and related work needed to understand the proposed method.  \n- The authors perform a  comprehensive comparison of the proposed method with different NLP tasks."
            },
            "weaknesses": {
                "value": "- Motivation for the lack of comparison with other uncertainty estimation methods.\n- A possible extra contribution can be the use or discussion of the method for NLP tasks under out-of-distribution (OOD) or domain adaptation."
            },
            "questions": {
                "value": "Please address the following questions during the rebuttal:\n\n- Please elaborate on the relation/difference of the Chow estimates with proper scoring rules (e.g. NLL, Brier score).\n- Could the proposed estimates be directly compared/evaluated to estimates from deep ensembles instead of a cascade or even jointly? (Lakshminarayanan, Balaji et al. \u201cSimple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.\u201d Neural Information Processing Systems (2016).)\n- For the machine translation evaluation: \n\n Is the output generated by beam search? Please speculate on the effect of the hyperparameters used on the generation, do they have an effect on the output length?\n\n Please speculate for the use of the proposed method for robustness to OOD in MT. different domains can be used to evaluate a change in distribution, is the uncertainty estimate robust to such change?\n\n- Please elaborate on the use of output probabilities from the LLMs as uncertainty estimates compared to other methods (e.g. deep enembles, MC dropout)? (e.g. Baan, Joris et al. \u201cUncertainty in Natural Language Generation: From Theory to Applications.\u201d ArXiv abs/2307.15703 (2023): n. pag.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no concerns."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6173/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_WdbA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699473187170,
        "cdate": 1699473187170,
        "tmdate": 1700560004118,
        "mdate": 1700560004118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FQLUZeQU8f",
        "forum": "KgaBScZ4VI",
        "replyto": "KgaBScZ4VI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G"
        ],
        "content": {
            "summary": {
                "value": "This paper is about learning deferral rules for LM cascades -- essentially, how can you predict when to rely on a small model's output and when should you fall back to a large, more expensive model? They propose a connection to the classical problem of classification with rejection, in which a model can choose to reject classifying an instance. The optimal strategy in this case is to reject whenever the model's confidence fails to pass a threshold derived from the cost of rejection. It is easy to see how LM cascades fit into this framework: deferring to the larger model is equivalent to rejecting the output of the smaller model.\n\nHowever, it is not trivial to generalize rejection from classification to generation. In classification the model predicts only one label along with an easy-to-interpret probability score (e.g. from softmax), whereas generation entails producing variable-length *sequences* of tokens. Although each of these tokens has an associated per-token probability, aggregating them is a challenge: simply summing the logprobs causes short sequences to be rejected (this is mathematically necessary, as it is equivalent to multiplying numbers less than one), while averaging them causes *long* sequences to be rejected (this observation is interesting, and surprising to me).\n\nNoting the weaknesses of these two baselines, the authors' main contribution is to introduce alternative techniques to score the model output. The main technique, which they call Chow-Quantile, is based on sorting the per-token logprobs for an instance and then picking the value at some alpha-quantile of this, where alpha is a hyperparameter. For example, picking the 0-quantile is equivalent to scoring sequences based on the *lowest* per-token prob. They also propose various post-hoc techniques that allow classifiers to be trained on top of these quantiles.\n\nThey exhibit experiments applying their various deferral techniques to a variety of tasks, including MT and QA (and also surprisingly MNLI, which seems inapplicable because it classification, not generation). These results seem to show an advantage for using their techniques over the baselines across many deferral levels."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a simple approach that seems to be very effective. The connection to rejection in classifiers is intuitive but had not occurred to. The paper is easy to follow. The clarity of presentation convinces me that it would be easy for me to try the approach myself, either for its own utility or as a replication study. I admire the accessibility of this work."
            },
            "weaknesses": {
                "value": "Although the paper as a whole is very clear, there are places where the experiments lack specifics (see the questions section). Additionally, there are places where the experimental set-up seems to be suboptimal: greedy decoding was used, but this is more prone to hallucination than beam search. Some of the conclusions of the experiments might be artifacts of these hallucinations. I can think specifically of these two: \n\n1) there was a negative correlation between translation quality and sequence length; was this because many of the long sequences were hallucinations? \n\n2) the Chow-average model favored longer sequences, which there is no intuitive reason for. Could it be because hallucinations often get trapped in loops where the same short phrases get repeated with high probability? This would drive up the average."
            },
            "questions": {
                "value": "--Which WMT set was used? It needs to be identified and cited. \n\n--I don't understand how the MNLI experiments work. It is noted in Section 3.4 that MNLI is a multi-class classification problem, but the techniques proposed in this paper are not for classification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6173/Reviewer_7G2G",
                    "ICLR.cc/2024/Conference/Submission6173/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699478855157,
        "cdate": 1699478855157,
        "tmdate": 1700581910455,
        "mdate": 1700581910455,
        "license": "CC BY 4.0",
        "version": 2
    }
]