[
    {
        "id": "h292cV6FX0",
        "forum": "5dlfiJIXoh",
        "replyto": "5dlfiJIXoh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission495/Reviewer_7b3w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission495/Reviewer_7b3w"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new framework S-ViLM for video-language pre-training, which aims to improve the fine-grained understanding of both videos and text. The framework consists of two novel components: inter-clip spatial grounding and intra-clip temporal grouping. The former learns to align video regions and text objects across different video clips, while the latter learns to group video frames and text tokens within the same clip based on temporal coherence. The paper evaluates S-ViLM on four downstream tasks that require temporal localization and semantic reasoning, such as text-video retrieval, video question answering, video action recognition and temporal action localization. The paper shows that S-ViLM outperforms existing methods on these tasks and achieves state-of-the-art results. The paper also provides an extensive ablation study to demonstrate the effectiveness of each component of S-ViLM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The manuscript effectively conveys the concept of aligning fine-grained video and text features.\n- The use of group tokens in the video encoder to align with the concepts in the text is a noteworthy approach.\n- The proposed loss function significantly enhances zero-shot retrieval performance on the MSRVTT dataset."
            },
            "weaknesses": {
                "value": "- The representation of spatial concept features by the group tokens is unclear.\n- The compared methods are outdated and do not compare to state-of-the-art methods.\n- The claimed cut-and-paste operation has been widely used in previous studies.\n- The performance of the current methods on several tasks is inferior to many recent works."
            },
            "questions": {
                "value": "- The methods being compared are outdated. It is recommended to compare them with state-of-the-art (SOTA) methods, particularly in the MSRVTT and TAL tasks. The authors can refer to https://paperswithcode.com/sota for comparing with more recent methods.\n- The text may involve multiple concepts. How can we extract k noun tokens from all sentences?\n- The number of group tokens remains constant during training. How do the group tokens align with different nouns in different sentences?\n- The visualization results for group tokens and noun tokens should be included.\n- It may be beneficial to include a grounding task since spatial alignment is claimed to be achieved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Reviewer_7b3w"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698583193110,
        "cdate": 1698583193110,
        "tmdate": 1700718620986,
        "mdate": 1700718620986,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n1vAsAf0ot",
        "forum": "5dlfiJIXoh",
        "replyto": "5dlfiJIXoh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission495/Reviewer_Dxbj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission495/Reviewer_Dxbj"
        ],
        "content": {
            "summary": {
                "value": "Compared with existing video-language pre-training tasks, the authors focus on instance-level alignment with spatial and temporal granularity instead of global contrastive learning. Specifically, the authors propose two pre-training tasks, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features simultaneously. The experiment results empirically demonstrate the effectiveness of the proposed framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The design of pre-training tasks sounds technically works. And the details of it are comprehensive.\n2. The comparison and ablation study are comprehensive. The visualizations clearly present the actual contribution of inter-clip spatial grounding and intra-clip temporal grouping.\n3. The overall presentation is clear and easily understandable."
            },
            "weaknesses": {
                "value": "1. The novelty may be somewhat weak."
            },
            "questions": {
                "value": "Please see the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Reviewer_Dxbj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637643403,
        "cdate": 1698637643403,
        "tmdate": 1699635976167,
        "mdate": 1699635976167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pCg2ukP5l8",
        "forum": "5dlfiJIXoh",
        "replyto": "5dlfiJIXoh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission495/Reviewer_tkWv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission495/Reviewer_tkWv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a new framework, namely S-ViLM, for video-language modeling. \n\nThe core ideas include:\n\n- Perform intra-clip temporal grouping by appending clips from other videos to the start and end (cut-and-paste) and classifying each clip as foreground / background in a self-supervised manner based on feature similarity\n\n- Perform inter-clip spatial grounding with the help of grouping blocks and tokens, based on feature similarity between prompts of nouns identified by spaCy and grouped tokens\n\n- Joint pretraining by learning intra-clip temporal grouping,  inter-clip spatial grounding, and general video-text contrastive matching\n\nPretrained on VideoCC and ActivityNet-Caption, the model achieved competitive performance against previous state-of-the-arts on four downstream tasks including text-video retrieval, VQA, video action recognition and temporal action localization. Ablation studies demonstrate some important design choices and present some details on the effects of each part."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed methods are well-motivated and bring something new to video-text modeling. The intra-clip temporal grouping with cut-and-paste is an effective way of better capturing scene changes and the inter-clip spatial grounding with grouping blocks and tokens is an interesting way to enhance object (noun) understanding.\n \n- The comparison with previous state-of-the-arts on four different downstream tasks suggest the effectiveness of the proposed method on learning more representative features.\n\n- Ablation studies in Fig. 2 show that intra-clip temporal grouping helps the model better distinguish different scenes and inter-clip spatial grounding  leads to features that are sensitive to objects, both contributing to performance improvement as in Tab. 6. \n\n- Good details are provided, which can make reproduction easier"
            },
            "weaknesses": {
                "value": "- Extra knowledge about determining noun chunks from spaCy is introduced during pretraining, which may lead to some level of unfair comparison with other methods\n\n- The concern regarding benefits from pretraining set are not fully addressed. Although Tab. 5 shows that S-ViLM obtains better results than VCC when both pretrained on VideoCC, it is still unclear how much benefits are brought when comparing with other methods in those downstream tasks. This may also result in unfair comparison to some extent.\n\n- The authors claim that Eq. 3 \"encourages each noun to be grounded to one or a few regions and avoids penalizing regions\nthat cannot find any relevant nouns.\" I want to hear more explanations and discussions on this. In particular, how it can prevent n_k being similar to many regions, or even all regions uniformly.\n\n- I suggest not listing all baseline names in the main text but just citing them in the Table to save space for more important discussions and details."
            },
            "questions": {
                "value": "Please address my concerns according to the Weaknesses part.\n\nBesides what are listed there, I also have some other questions:\n\n- For video action recognition, I wonder if the authors have results on the Something-Something / Something-Else dataset. UCF101 and HMDB51 are both quite biased towards objects presented in the videos and therefore may benefit a lot from the inter-clip spatial grounding. I'm very curious on S-ViLM's performance on a more different action recognition dataset.\n\n- How are start and end clip indices s, e sampled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824564204,
        "cdate": 1698824564204,
        "tmdate": 1699635976101,
        "mdate": 1699635976101,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k1NQaiVzSN",
        "forum": "5dlfiJIXoh",
        "replyto": "5dlfiJIXoh",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission495/Reviewer_adH9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission495/Reviewer_adH9"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses improving video-language alignment. The paper introduces two novel pre-training tasks, namely, inter-clip spatial grounding and intra-clip temporal grouping. The inter-clip spatial grounding aims to associate relevant image regions with text. This task is weakly supervised, without explicit correspondences between regions and nouns. Instead, it employs learnable 'group tokens' to cluster semantically similar image regions.The intra-clip temporal grouping optimizes the model features to be able to distinguish a video clip (start/end time) from a background clip, akin to a metric learning loss approach. The benefit of these tasks is that the supervision can be generated automatically using random cut & paste and pre-processing the captions. The experimental results show that the proposed method outperforms VCC (Nagrani et al., 2022) on multiple benchmarks. The ablation studies on the two introduced losses indicate that each contributes to improving the original method. However, when combined, their combined effect results in a marginal improvement compared to employing each loss separately."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The proposed additional pre-training tasks (inter-clip spatial grounding and intra-clip temporal grouping) are reasonable. The supervision can be generated automatically using random cut & paste and pre-processing the captions for extracting the nouns. \n\nS2. I liked that the authors provide Table 5 that shows the effects on different choices of pre-training datasets. This allows comparing the proposed method and VCC trained on the same dataset (HowTo100M and VideoCC). It is good to know that the proposed method is on par with VCC when trained on HowTo100M, but significantly outperforms VCC when trained on VideoCC on MSRVTT-ZS. \n\nS3. The authors provide an ablation study on combinations of their proposed losses.\n\nS4. The paper presents visualizations of the affinity scores of learned features, alongside the attention map of S-ViLM in Figure 2. These visualizations are interesting and further validate that the model was trained as intended."
            },
            "weaknesses": {
                "value": "W1. The authors state that most video-language pre-training methods neglect scene/action changes along the time in a video. However, there are works like LaViLa [a] that takes temporal and patch-level information into account. \n[a] Learning Video Representations from Large Language Models, Zhao et al., CVPR 2023\n\nW2. Some design choices are not obvious but no ablation study was presented in the paper.\n(1) Instead of Eq 1-2, I am curious if the authors tried BCE loss on z_i clip using the mask m_i. If so, how did the performance differ?\n(2) Instead of using grouping blocks, one can opt for existing region proposal networks or segmentation networks to obtain semantically similar regions, and then aggregate visual features within those regions to compute relevance against the nouns. \n\nW3. The writing could be further improved for enhanced clarity\n\n- It would be nice if \"interaction\" is clarified in \u201cc) modality fusion and interaction\u201d in the second paragraph of Section 1. \n- The term \"region-object groundingness\" is ambiguous in \u201cSpecifically, group tokens aggregate semantically similar video tokens via grouping blocks to promote region-object groundingness and video tokens are utilized in temporal grouping to improve temporal awareness.\u201d\n- I suggest using \u201csemantically similar regions\u201d in \u201cThus, we adopt M learnable group tokens to cluster semantic similar regions in a self-supervised manner.\u201d"
            },
            "questions": {
                "value": "Q1. See W2 (1). \nQ2. In Eq 1, were z_i\u2019s L2 normalized?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission495/Reviewer_adH9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission495/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836415454,
        "cdate": 1698836415454,
        "tmdate": 1699635975991,
        "mdate": 1699635975991,
        "license": "CC BY 4.0",
        "version": 2
    }
]