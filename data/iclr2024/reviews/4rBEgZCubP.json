[
    {
        "id": "vlmz0UE08v",
        "forum": "4rBEgZCubP",
        "replyto": "4rBEgZCubP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_8vFC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_8vFC"
        ],
        "content": {
            "summary": {
                "value": "The proposed paper introduces a novel approach for learning simulations from videos. The approach relies on RGB-D images as input and uses a combination of neural networks (UNet, GraphNetwork, Renderer) to first generate a latent particle representation that is then rendered into output images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses an important research direction -- learning physics directly from videos is a challenging and open problem. \n- To my knowledge, the proposed model and training procedure (with RGB-D images) is novel.\n- The proposed method seems technically sound and the results are convincing. \n- The paper is mostly well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- Some sections of the text could provide more details (more details below). \n- Only synthetic data was used to test the method. \n- Not all experiments are convincing (more details below)."
            },
            "questions": {
                "value": "- Section 3.1.: It is not clear how the per-pixel latent features are generated. Ronneberger et al. uses down convolutions to produce a compressed latent space, which would not result in a per-pixel latent. How does the UNet architecture used in this paper differ from the original one?\n\n- Section 3.1. This section is difficult to follow as some term are not defined or introduced properly. E.g. what is a 'conditioning set'? What is a \"work space\" and how is it defined?\n\n- While the paper mentions 'blurring over very long rollouts', it does not provide a discussion on what this actually means. After how many time steps do outputs become blurry? Can this be quantified? What about baseline methods?\n\n- The authors may also want to discuss the following paper: H. Shao, T. Kugelstadt, T. H\u00e4drich, W. Pa\u0142ubicki, J. Bender, S. Pirk, D. L. Michels, Accurately Solving Rod Dynamics with Graph Learning, Conference on Neural Information Processing Systems (NeurIPS), 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns regarding this submission."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697892106611,
        "cdate": 1697892106611,
        "tmdate": 1699636324313,
        "mdate": 1699636324313,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wtg52GO0ZV",
        "forum": "4rBEgZCubP",
        "replyto": "4rBEgZCubP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_HnBD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_HnBD"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a neural-network-based 3D simulator where a graph neural network is used to estimate the 3D dynamics. The simulator transforms the multi-view RGBD images into a latent point cloud and uses a differentiable simulator to transform the simulated result into images, such that an image-based loss is used to train the simulator end-to-end. The proposed simulator supports 3D state editing, novel view re-rendering and multi-material simulation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper uses a explicit point cloud representation, such that it supports scene editing, novel-view re-rendering. The experimental results have shown the effectienss of the method in such applications."
            },
            "weaknesses": {
                "value": "The claim that existing learned world models are not simulators is not accurate. The function of simulator is to estimate the transition model of the system. \nThe main weakness of the proposed simulator is that it does not support the simulation of the environment response to different actions. Therefore, it can only predict the future state of a passive system.\nSecondly, the author does not validate the extrapolation ability, which is important for a simulator. Experiments on out-of-distribution initial state can be added."
            },
            "questions": {
                "value": "Can the proposed simulator support different physics properties, such as density, friction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698117996316,
        "cdate": 1698117996316,
        "tmdate": 1699636324214,
        "mdate": 1699636324214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YUFOaSRD8T",
        "forum": "4rBEgZCubP",
        "replyto": "4rBEgZCubP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_CLWA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_CLWA"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces VPD, a method to learn particle simulators given multiview RGBD videos. The setup is similar to NeRF-dy (Li et al.). The major difference is an explicit point-based neural simulation and rendering pipeline that avoids learning mappings between images and latent states. \n\nVPD is evaluated on dastsets with deformable and rigid objects, showing better performance than SlotFormer (a video prediction method) and re-implemented NeRF-dy (a ) that roughly matches the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Presentation**\n- The paper is generally easy to follow, and the video results on the website are helpful.\n\n**Method**\n- The point-based representations enable 3D editing capability during/before simulation. \n- The use of PointNeRF is a nice design choice that couples particle-based simulation and differentiable rendering."
            },
            "weaknesses": {
                "value": "**Writing**\n- The idea of learning dynamics/physics simulators from videos is not particularly new (e.g., NeRF-dy, [A-B]), but the intro and related work are positioned in a way that appears those works are not relevant. To give the readers sufficient context, I would recommend putting more effort into discussing the existing \"video->simulator\" works, their limitations, and the key differences in this work. \n\n\n**Efficiency**\n- One advantage of using a latent representation for simulation learning (e.g., NeRF-dy) is its efficiency, especially when the number of particles is large. Given enough training data, one would expect a latent representation to be more performant.\n\n**Motivation**\n- To obtain better physics simulators (of system dynamics), what do we gain by learning from videos? The motivation and evidence could be made more concrete.\n  - Specifically, the problems mentioned in the intro seem can be solved by system identification with a differentiable simulator, e.g., Taichi, Warp, Brax, dojo, which can find physical parameters (e.g., friction coefficient) from input videos without re-learn how to simulate. One example is Le Cleac\u2019h et al., 2023.\n  - If the goal is to speed up the simulation, it seems distilling physics-based simulators into a neural architecture is a strong competitor.\n- Overall, it would be great if the paper could give a compelling example where VPD outperforms (or has the hope to outperform) the physics-based simulator. One aspect might be the generality. Another case is when there are complex disturbances in the environment that cannot be modeled by the physics simulator.  \n\n**Rigid objects**\n- It seems the results on rigid objects (Kubric Movi-a) contain undesirable deformations. This seems to suggest that the learned GNN is not able to enforce rigidity constraints for high-stiffness materials. \n- It would be nice to show how the method works on more challenging cases, such as the collision between rigid objects (or a rigid object and the ground). Those cases typically require a small timestep for physics-based simulation. \n\n\n[A] Qiao, Yi-Ling, Alexander Gao, and Ming Lin. \"Neuphysics: Editable neural geometry and physics from monocular videos.\" Advances in Neural Information Processing Systems 35 (2022): 12841-12854.\n\n[B] Heiden, Eric, et al. \"Inferring articulated rigid body dynamics from rgbd video.\" 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022."
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698600287943,
        "cdate": 1698600287943,
        "tmdate": 1699636324118,
        "mdate": 1699636324118,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9kIUS6MABr",
        "forum": "4rBEgZCubP",
        "replyto": "4rBEgZCubP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_cZVt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3677/Reviewer_cZVt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to learn a particle-based representation, a neural simulator, and a neural renderer from posed RGB-D videos. \n\nFirst, this method projects pixels back to 3D point clouds with known depth values and camera poses. Second, a UNet is applied to the RGB channels to extract per-pixel features. Then, a neural simulator implemented using MeshGraphNet is applied to infer the dynamics of the scene. In the end, a NeRF-style neural renderer is used to synthesize the image, where the feature of a query point is computed by filtering over neighboring points. \n\nWith ground truth RGB-D video sequences, this pipeline can learn the feature extractor, neural simulator, and neural renderer end-to-end. Experiments show that it can handle different dynamics (rigid, soft, contact, etc.). Ablation studies suggest that the designed components are necessary for the entire system."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation and building blocks of this paper are clear and reasonable. The choices of neural simulator and renderer are up-to-date. With a general, unstructured, and differentiable simulator (GNN) and render (NeRF), it is easier to simultaneously train this system end-to-end.\n\nMoreover, training the system end-to-end can indeed improve performance. Without carefully handcrafted dynamics and rendering models, this pipeline can reconstruct complex 3D dynamic information from 2.5D input.\n\nThe learned dynamics can be used to perform editing. And it is also impressive to see this method can also render the dynamic shadow correctly (as in the video demo)."
            },
            "weaknesses": {
                "value": "The method is actually only capturing the surface points. There are no inner points reconstructed with only the depth values. We can also see this effect in the demo videos, where objects seem to be hollow.\n\nThe algorithm might rely on high-quality and multi-view RGBD videos. In the experiments, the background is clean and the objects are relatively simple. There are not so many tests on real-world data where the depth values are noisy and the view angles are sparse.\n\nMore like a systematic integration, this paper has relatively limited technical contribution. The feature extraction, neural renderer, and simulator use existing building blocks. It might be better if there more explorations on how to improve its interpretability, robustness, generalizability, and performance."
            },
            "questions": {
                "value": "What's the memory and time consumption to train this pipeline? The neural simulator might need to take many samples to converge while the NeRF rendering is usually slow (although there are faster implementations, I'm not sure if it's used here).\n\nIn Figure 8, while the PSNR stays the same between 1024->2048 and 4096->8192 while takes big jumps between 2048->4096 and 8192->16384.\n\nThis method seems to treat the same points in different views as different points. \"If more than one camera is present, we can simply merge all the particle sets from each time-step\"\nIs this strategy appropriate, with unbalanced camera distribution? It would make more sense to me if additional steps could be taken to find and resolve the correspondence among views. And how is this method tacking the points across timesteps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3677/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609418320,
        "cdate": 1698609418320,
        "tmdate": 1699636324039,
        "mdate": 1699636324039,
        "license": "CC BY 4.0",
        "version": 2
    }
]