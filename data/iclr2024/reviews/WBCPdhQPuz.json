[
    {
        "id": "k3hugm6DE1",
        "forum": "WBCPdhQPuz",
        "replyto": "WBCPdhQPuz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_Wx7b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_Wx7b"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into distributed minimax optimization problems, specifically addressing a min-max problem with costs allocated across network-connected nodes. The authors introduce an adaptive stepsize distributed method, notably agnostic to the problem's inherent parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel adaptive step size approach tailored for distributed optimization, aiming to ensure consistency among all nodes. Leveraging this strategy, the authors put forth a distributed method, demonstrating its convergence at near-optimal rates."
            },
            "weaknesses": {
                "value": "The paper's assumption that every stochastic gradient remains bounded presents a restrictive condition.\nThe overall presentation and structure of the paper need refinement. The paper initiates with equations without offering adequate motivation or a streamlined introduction. This lack of organization is particularly concerning given the dense notation utilized in the work. Additionally, there's a notable absence of discussions surrounding the principal results and the definitions of each parameter. Delving into these results, understanding their implications, and drawing clear comparisons with other works are essential steps that should not be overlooked."
            },
            "questions": {
                "value": "Regarding the primary result, does it imply that the method converges for all step sizes $\\gamma_{x,y}$, provided the iteration count is sufficiently large?\n\nWhat drove the development of the proposed method? How did you derive those specific updates for the step size?\n\nIs it possible to do away with the bounded gradient assumption? Generally, to demonstrate the convergence of analogous decentralized methods, only the conditions of bounded variance and bounded gradient disagreement are required."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Reviewer_Wx7b"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666863793,
        "cdate": 1698666863793,
        "tmdate": 1699637150756,
        "mdate": 1699637150756,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CLlJgDUn3v",
        "forum": "WBCPdhQPuz",
        "replyto": "WBCPdhQPuz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_wTqn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_wTqn"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the adaptive minimax method in distributed problems. They proposed a distributed adaptive method DAS2C with time-scale separated stepsize control for minimax optimization. By leveraging the transmission of two extra (scalar) variables, non-convergence issue is solved. For nonconvex-strongly-concave distributed minimax problems, it gets a near-optimal convergence rate of $\\tilde{O}  (\\epsilon^{4+ \\delta})$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Minimax is an important optimization problem in machine learning and the study of minimax in a distributed setting is necessary. \n\n2. The paper is organized well. it presents a counterexample to show how to design the algorithm.   \n\n3. The strategy is simple to use and extra transmission variables are scalar."
            },
            "weaknesses": {
                "value": "1. The motivation behind this paper is not clear. What is the advantage of the adaptive method and why do we need these types of methods in the distributed setting?   Although few papers study the application of adaptive methods in distributed minimax problems, are adaptive methods necessary in distributed or federated learning compared with non-adaptive algorithms? \n\n2. Some recent related distributed minimax works are missing.\n\n[1] A faster decentralized algorithm for nonconvex minimax problems, NeurIPS 2021\n\n[2] Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning, NeurIPS 2021\n\n[3] FedNest: Federated bilevel, minimax, and compositional optimization. ICML 2022\n\n[4] Decentralized Riemannian Algorithm for Nonconvex Minimax Problems. AAAI 2023\n\n[5] Solving a Class of Non-Convex Minimax Optimization in Federated Learning. NeurIPS 2023.\n\n\n3. For the convergence results, if this paper focuses on the nonconvex-strongly-concave, results should include the depends on $\\kappa$. \n\n4. The baselines in experiments seem to only solve the issues about the design of the adaptive method in distributed learning. It does not present why we need adaptive  algorithms in (minimax) distributed problems."
            },
            "questions": {
                "value": "1. \"$DAS^2C$ is the first distributed adaptive method guaranteeing exact convergence without requiring to know any problem-dependent parameters for nonconvex minimax problems\". Could you explain what are the  \"problem-dependent parameters \"?\n\n2. The convergence is $\\tilde{O}  (\\epsilon^{4+ \\delta})$. What is the result of its centralized counterpart? It seems that related works does not include their term and this result is not as tight as others.\n\n3. In the eq. (4), first equality should be correct. But for the second equality, if you add a projection operator, is the equality still valid?\n\n4. Why $\\min _x \\max _y 1 / n \\sum_{j=1}^n f_i\\left(x ; \\xi_i+y\\right)-\\eta\\|y\\|^2,$ in Robust training of neural network tasks and Generative Adversarial Networks is NC-SC question?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Reviewer_wTqn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678794390,
        "cdate": 1698678794390,
        "tmdate": 1699637150538,
        "mdate": 1699637150538,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HNgzLdlrOl",
        "forum": "WBCPdhQPuz",
        "replyto": "WBCPdhQPuz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_GJ7M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_GJ7M"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the decentralized minimax optimization problem. It developed an adaptive algorithm. However, it missed some important references and introduced strong assumptions and have errors in convergence analysis."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The problem studied in interesting. \n\n2. The counterexample is good."
            },
            "weaknesses": {
                "value": "1. This paper missed some state-of-the-art literature. \n\n\n[1] A Faster Decentralized Algorithm for Nonconvex Minimax Problems\n[2]  Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning\n[3] Decentralized stochastic gradient descent ascent for finite-sum minimax problems\n[4] Jointly Improving the Sample and Communication Complexities in Decentralized Stochastic Minimax Optimization\n\n2. This paper introduces strong assumptions so that the proof is simplified too much. In particular, it assumes the gradient is upper-bounded in Assumption 4, which is not used in the original TiAda paper. In addition, assuming that the function is strongly concave and has a bounded gradient is not common because the simple quadratic function does not satisfy this assumption. \n\n3. There are some errors in the proof. $\\mathcal{P}$ is not a linear operator so eq (36) is not correct.\n\n4. This paper didn't compare with the aforementioned SOTA algorithms."
            },
            "questions": {
                "value": "1. This paper missed some state-of-the-art literature. \n\n\n[1] A Faster Decentralized Algorithm for Nonconvex Minimax Problems\n[2]  Taming Communication and Sample Complexities in Decentralized Policy Evaluation for Cooperative Multi-Agent Reinforcement Learning\n[3] Decentralized stochastic gradient descent ascent for finite-sum minimax problems\n[4] Jointly Improving the Sample and Communication Complexities in Decentralized Stochastic Minimax Optimization\n\n2. This paper introduces strong assumptions so that the proof is simplified too much. In particular, it assumes the gradient is upper-bounded in Assumption 4, which is not used in the original TiAda paper. In addition, assuming that the function is strongly concave and has a bounded gradient is not common because the simple quadratic function does not satisfy this assumption. \n\n3. There are some errors in the proof. $\\mathcal{P}$ is not a linear operator so eq (36) is not correct.\n\n4. This paper didn't compare with the aforementioned SOTA algorithms."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Reviewer_GJ7M"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853206590,
        "cdate": 1698853206590,
        "tmdate": 1699642055473,
        "mdate": 1699642055473,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DnnjXlmqvk",
        "forum": "WBCPdhQPuz",
        "replyto": "WBCPdhQPuz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_x9Sp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9142/Reviewer_x9Sp"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the decentralized distributed nonconvex-strongly-concave minimax problems, and proposed an efficient adaptive decentralized algorithm to solve these problems. Theoretically, it proved that the proposed algorithm obtain a near-optimal convergence rate. Experimentally, it provided some experimental results to demonstrate the efficiency of the proposed algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studied the decentralized distributed nonconvex-strongly-concave minimax problems, and proposed an efficient adaptive decentralized algorithm to solve these problems. Theoretically, it proved that the proposed algorithm obtain a near-optimal convergence rate. Experimentally, it provided some experimental results to demonstrate the efficiency of the proposed algorithms."
            },
            "weaknesses": {
                "value": "Although the proposed DAS2C algorithm is the first decentralized distributed adaptive method for nonconvex minimax problem, it basically extends the existing adaptive method [1] to decentralized distributed settings. Meanwhile, the DAS2C algorithm can address the issue of inconsistent stepsizes across different nodes by communicating the adaptive step-sizes, which basically follows the same trick in the adaptive federated learning.  \n\n\n[1] Li, X., YANG, J., and He, N. (2023). Tiada: A time-scale adaptive algorithm for nonconvex minimax optimization. In The Eleventh International Conference on Learning Representations."
            },
            "questions": {
                "value": "1)\tIn DAS2C  algorithm, why use the exponential factors satisfying $\\beta < \\alpha$ ?\n\n2)\tThe DAS2C algorithm needs some stricter assumptions (e.g., $f_i$ is second-order Lipschitz continuous for $y$) than the existing decentralized minimax optimization methods. \n\n3)\tIn the experiments, the authors should add some existing  decentralized  minimax optimization algorithms such as the DPOSG of [2] as the comparison methods.\n\n[2] Liu, M., Zhang, W., Mroueh, Y., Cui, X., Ross, J., Yang, T., and Das, P. (2020). A decentralized parallel algorithm for training generative adversarial nets. Advances in Neural Information Processing Systems, 33:11056\u201311070.\n\n4) Some related references are missing. E.g.,\n\n[a] A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization\n\n[b] Jointly Improving the Sample and Communication Complexities in Decentralized Stochastic Minimax Optimization\n\n[c] A faster decentralized algorithm for nonconvex minimax problems"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9142/Reviewer_x9Sp"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9142/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899322480,
        "cdate": 1698899322480,
        "tmdate": 1700712491312,
        "mdate": 1700712491312,
        "license": "CC BY 4.0",
        "version": 2
    }
]