[
    {
        "id": "SPpiA4p0Uq",
        "forum": "i6JcQpiFdR",
        "replyto": "i6JcQpiFdR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_cJrU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_cJrU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an algorithm FixPO that enforces the trust region constraint to be strictly satisfied during policy learning. Specifically, after the policy update with KL-regularized policy loss, FixPO adds a fixup phase to restrict the KL divergence between new and last policies on each minibatch. The authors compare their method with previous PPO methods on mujoco tasks and metaworld benchmark."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The experiment of this paper is extensive. The authors test the method on different domains and conduct comprehensive ablation study.\n- The proposed method exceeds the baselines on several mujoco tasks (e.g., walker2d, swimmer, pendulum) and metaworld transfer learning tasks.\n- The additional computation overhead of FixPO is small, which makes it an efficient plug-in component for existing baselines."
            },
            "weaknesses": {
                "value": "- The authors propose to add a new fixup phase to make the updated policy strictly satisfy trust region constraint, which can reduce the instability during training. However, we can easily address the instability issue by increasing the coefficient of KL regularization $\\beta$, which is not only easier to implement, but also achieves better performances according to the ablation study (fig.5, $\\beta=10$).\n- The advantage of FixPO over previous methods is not consistent. In mujoco domain, FixPo does not exhibit better performance than baselines on halfcheetah, hopper and reacher tasks. There is also no significant different between FixPO and APPO in DMLab tasks. Meanwhile, FixPO cannot even exceed No Fixup phase baseline in fig.5, which makes the effectiveness of fixup phase very questionable.\n\nMinor issues:\n- The line below eq.(1), $L_{\\pi_i}$ should be \"policy loss\" instead of \"policy gradient\".\n- The font of pdf is inconsistent with the given ICLR template."
            },
            "questions": {
                "value": "- What is the exact definition of $D_{KL}^{max}[\\pi_1,\\pi_2]$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Reviewer_cJrU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698081852503,
        "cdate": 1698081852503,
        "tmdate": 1699636832717,
        "mdate": 1699636832717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eseXSkrhIn",
        "forum": "i6JcQpiFdR",
        "replyto": "i6JcQpiFdR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_MD36"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_MD36"
        ],
        "content": {
            "summary": {
                "value": "The paper sets out to combine the strengths of PPO and TRPO, namely, efficiency and ease of implementation of PPO and theoretical guarantees of TRPO. To this end, the paper begins with the PPO algorithm (defined in eq. 2) and makes two important modifications. First, the Lagrange multiplier that controls the importance of the KL term in the loss function is learned via dual gradient descent. Second, an additional fixup phase is added after each policy update, which ensures the satisfaction of the trust region constraints by making a few additional gradient steps. The paper highlights the strengths of the proposed algorithm in simulated experiments."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The research question is very important. The dichotomy between the guarantees of TRPO and the simplicity of PPO has been a longstanding issue in RL.\n- Every design choice is well-motivated and complimented with experiments. At the same time, the algorithm is simple to implement and does not add much time compared to PPO.\n- The paper is extremely clear, concise, and well-written. Most questions I had while reading it (e.g., how come the trust region is guaranteed, what if the fixup phase did not terminate, or what if we removed the fixup phase) were answered later in the paper.\n- The experiments are extensive and diverse. Many relevant environments and ablations are included."
            },
            "weaknesses": {
                "value": "- TRPO is a relevant baseline but is absent from experiments."
            },
            "questions": {
                "value": "- Shouldn\u2019t $D_{KL}$ be multiplied by $C_\\beta$ in line 11 of Algorithm 1 (according to eq. 4)?\n- Is the fixup phase analogous to line search in TRPO?\n- Can an entropy term be added to encourage exploration? I understand the point made in Fig. 4 but still wonder if additional exploration could be beneficial.\n- I find it interesting that in HalfCheetah, FixPO underperforms both PPO (fig. 3) and the constant $\\beta$ ablation (fig. 5). I wonder if using a high constant $\\beta$ (maybe without fixup) results in approximate trust region similar to what clipping does in PPO.\n- Just in case the authors would appreciate more related work on the Lagrangian optimization (where the Lagrange multiplier is learned), similar approaches are used in Adversarial (Inverse RL / Imitation Learning / Generative Networks) (https://arxiv.org/abs/1810.00821), Differentiable Economics (https://arxiv.org/abs/1706.03459, https://arxiv.org/abs/2202.13110), and Multi-Agent RL (https://arxiv.org/abs/2306.08419)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662298035,
        "cdate": 1698662298035,
        "tmdate": 1699636832590,
        "mdate": 1699636832590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cO2gd6TQLl",
        "forum": "i6JcQpiFdR",
        "replyto": "i6JcQpiFdR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_bxPw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_bxPw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an adaptive strategy for improving the performance of trust region optimization for solving RL problems. In particular, the proposed algorithm can adaptively fine tune the parameter beta so that the KL divergence of two consecutive policies can within the user-specified distance, without putting this requirement as a hard constraint."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think this paper provides practical approach to efficiently solve RL problems. To the best of my knowledge, this paper is novel. The proposed algorithm is validated in many test instances, which is very nice."
            },
            "weaknesses": {
                "value": "In general I think the author(s) explain the high level idea of the proposed method very well. However, I do not totally understand the intuition behind equation (4), which is perhaps the most important step in the proposed algorithm. Why it works and controls the distance between pi and pi'? It would be great if this result could be written as a proposition with proof.\n\nAlso it would be great if the author(s) could provide the per-iteration complexity of the proposed method, compared to the other state-of-the-art approaches."
            },
            "questions": {
                "value": "Could you mathematical describe the reasoning behind equation (4)?\n\nHow do you compute the gradients of L_theta and L_beta?\n\nWhat is the complexity of the fixed up phase?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7070/Reviewer_bxPw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699171973448,
        "cdate": 1699171973448,
        "tmdate": 1699636832489,
        "mdate": 1699636832489,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rizu3G1mxe",
        "forum": "i6JcQpiFdR",
        "replyto": "i6JcQpiFdR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_HunE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7070/Reviewer_HunE"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel trust region policy optimization method that follows the line of TRPO and PPO."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a novel policy optimization algorithm testing the proposed approach on different domains.\n\nThe idea seems novel, although incremental."
            },
            "weaknesses": {
                "value": "### Presentation \n\n- The paper does not provide context on the setting considered. Preliminaries are completely missing, the general tone is too colloquial, it is not clear which RL setting is considered (finite-horizon, infinite-horizon, average reward etc. ) \n\n- The advantage function $\\hat{A}$ is never defined.\n\n- $L_\\beta$ is introduced and not used later.\n\n- What is $L_{VF}$?\n\n- How is it computed $L_\\pi$? \n\n- Figure 1: How can we extrapolate from the figure that the performances are reduced?\n\n### Experimental evaluation\n\n- The results of PPO-clip on Mujoco control tasks domain are not coherent with the original paper [1].\n\n     - Why are your results on Inverted Pendulum PPO-clip ~5000 when in [1] they achieved ~8000?\n\n     - Why are your results on Swimmer PPO-clip ~50 when in [1] they achieved ~100?\n\n- The comparison with TRPO is missing (as with other policy optimization methods), although the two methods are quite similar.\n\n- In general, the experimental evaluation is not convincing since the proposed method does not provide better results compared to PPO and the comparison with other policy optimization algorithms is missing."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7070/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699396430810,
        "cdate": 1699396430810,
        "tmdate": 1699636832380,
        "mdate": 1699636832380,
        "license": "CC BY 4.0",
        "version": 2
    }
]