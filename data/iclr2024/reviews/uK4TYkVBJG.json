[
    {
        "id": "9XsKgnUNQ7",
        "forum": "uK4TYkVBJG",
        "replyto": "uK4TYkVBJG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_4QxB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_4QxB"
        ],
        "content": {
            "summary": {
                "value": "SAM is a pre-trained foundation model for image segmentation. However, prior arts have shown that SAM performs poorly on medical images due to the semantic gap between its training images and the medical domain. This paper tackles the poor performance of SAM on medical images. It concludes that the segmentation results can be greatly improved with a proper combination of bounding box prompts and point prompts. Based on this, automatic prompts are generated from the pre-trained encoder of SAM for medical queries. In addition, different adaptors are designed for tailoring SAM from 2D to 3D format. Experiments on two different medical datasets show the effectiveness of the proposed method over prior arts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The target problem with SAM is very important. Properly addressing it would significantly improve medical image segmentation in the field.\n\n(2) The motivations of the technical innovations are clearly presented. The paper is easy to follow and understand.\n\n(3) Extensive ablation studies are conducted to demonstrate the effectiveness of the adaptation."
            },
            "weaknesses": {
                "value": "(1)  A significant drawback of this paper lies in the insufficient methodological exposition. The specifics regarding the implementation and training procedures of the proposed adaptor modules remain unclear. For instance, how is each of the proposed adaptor modules implemented and trained? What are the loss functions? Are all of the adaptors trained jointly? This lack of comprehensive information hinders the reproducibility of the research.\n\n(2) The figures presented in the paper are somewhat perplexing. They require more elaborate descriptions and additional contextual explanations. \n\n(3) Minor issues of typos and gramma errors."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698513746322,
        "cdate": 1698513746322,
        "tmdate": 1699636392616,
        "mdate": 1699636392616,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GyU9wCwal1",
        "forum": "uK4TYkVBJG",
        "replyto": "uK4TYkVBJG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_NtPf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_NtPf"
        ],
        "content": {
            "summary": {
                "value": "The paper endeavors to adapt an existing model to a three-dimensional context without relying on external prompts. It introduces a multi-scale strategy as the cornerstone of its method for a particular downstream task. In this model, a multi-scale prompt generator is seamlessly integrated with the image encoder in SAM, enabling it to autonomously generate auxiliary masks. These masks are then used to form bounding boxes that act as box prompts. Furthermore, the model employs the Distance Transform method to identify points that are farthest from the boundary, which are subsequently utilized as point prompts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper showcases advancements in prompt generation and model adaptation, particularly in 3D contexts and medical imaging applications.\n\nThe proposed generator leverages multiple levels of information from the feature map, enhancing its effectiveness and versatility in processing complex data.\n\nIt demonstrates an intriguing approach by combining box points and masks to generate prompts. This hybrid method potentially offers a more robust way of prompt generation compared to traditional methods.\n\nThe method's efficacy is thoroughly tested and validated on two different datasets.\n\nA remarkable innovation is the development of a 3D depth-fused adapter. This enables pre-trained 2D SAM models to extract and interpret 3D information, making it useful in adapting to medical imaging, a field where 3D data is prevalent"
            },
            "weaknesses": {
                "value": "A major part of the method is introducing a self-prompted approach within the SAM framework, primarily focused on segmentation without external prompts. However, the most compelling aspect of SAM is its zero-shot learning capability across various datasets. By entirely removing manual prompts, the framework diverges from its zero-shot learning roots, shifting more towards a transfer learning approach. This change might be seen as a step back from the innovative aspects of SAM, as it no longer operates under the originally intended zero-shot learning setting.\n\nThe DfusedAdapter appears to be a critical element, especially evident in Table 3 of the paper. However, its presentation in the methodology section is somewhat unclear or insufficiently detailed. This lack of clarity could hinder the understanding and reproducibility of the research.\n\nThe concept of using an adapter or fine-tuning the SAM model to achieve effective segmentation without manual prompts is not entirely new. Similar approaches have been previously proposed in related works. Therefore, the paper might yield insufficient innovation in this specific aspect of the research."
            },
            "questions": {
                "value": "Please see my comments in \"weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762138094,
        "cdate": 1698762138094,
        "tmdate": 1699636392545,
        "mdate": 1699636392545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gQumunFC87",
        "forum": "uK4TYkVBJG",
        "replyto": "uK4TYkVBJG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_VLNv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_VLNv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a  self-prompt SAM adaptation framework for medical image segmentation, \ntermed Self-Prompt-SAM. A multi-scale prompt generator is combined with the image encoder in\nSAM to generate auxiliary masks. The auxiliary masks are used  to generate bounding boxes as box prompts  and Distance Transform is utilized to select the points farthest from the boundary as point prompts.\n\nResults are shown on two benchmark datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Detailed complex architecture of the proposed Self-Prompt-SAM, as in in figure 2.\n\n3D transformer block designed; utilizing the 2D SAM with designed adapters to equip\nspatiotemporal reasoning capability for 3D medical image segmentation.\n\nGood that implementation requires only one GPU with large memory, as reported."
            },
            "weaknesses": {
                "value": "Only one equation as analytics; That too is comes without continuity of text - just in between two paras.\n(although purpose in clear).\n\nNo algorithm for learning (new of modified) proposed.\n\nVery difficult to infer goodness of the results in figs. 3 & 4, as details are too minor for the naive (non-med experts) eye . Should have zoomed onto certain regions/parts of images to exhibit the better detection of proposed method.\n\nHow is the DT feature helping the cause of better detection - is not clear.\n\nAlso how the 3D data is analyzed and corresponding results obtained wrt GT - is not vivid in presentation."
            },
            "questions": {
                "value": "What is the performance in too much of clutter\nor detection of small objects in medical images ?\n\nWhy are failure cases not discussed?\n\nWill the code be released ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764591707,
        "cdate": 1698764591707,
        "tmdate": 1699636392448,
        "mdate": 1699636392448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vbhw9JywPq",
        "forum": "uK4TYkVBJG",
        "replyto": "uK4TYkVBJG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_wtde"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4254/Reviewer_wtde"
        ],
        "content": {
            "summary": {
                "value": "In this manuscript the authors propose to adapt the Segment Anything Model (SAM) to 3D prompt-free medical image segmentation tasks, using 1. adapters; 2. a prompt generator network which produces prompts (essentially a pre-segmentation stage). The proposed approach is evaluated on abdominal CT and cardiac MRI segmentations. It demonstrates improved segmentation performance compared to some pre-SAM approaches that do not involve large-scale pre-training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The topic of leveraging foundation models for domain-specific applications is of practical value, despite the surge of similar works. \n\nImproved segmentation accuracies are shown on two public medical image segmentation datasets, compared with pre-SAM approaches. \n\nThe idea of improving automation by removing the requirement on manual prompting is of great clinical value."
            },
            "weaknesses": {
                "value": "Similarity to existing works: The readers may argue that merely combining 1. adapters (has been studied by [1,2,3]), 2. modified prompt encoder/prompting mechanism (studied by [2,5]) for auto-prompting, and 3. operations along the depth dimension ([1,2,3]), may not meet the high standard of ICLR. ICLR encourages theoretical insights into the fundamentals of the problems, and novel ideas that can inspire related works. Despite the claimed further automation compared with manually-prompted SAM's, [1,4,5] can also be directly trained and tested in without manual prompts, meeting the same level of convenience as the proposed work. \n\nThe writing style needs to be improved: \n\n1. The manuscript is poorly structured. It is understandable that the authors went through a chain of thinking; but this mixed and convoluted narration, without organization, may severely mislead readers, especially for the introductory paragraph. The authors are expected to make their language concise: straightforward to the key arguments, and then talk about supporting evidences and/or reasoning behind. Fixing this structural flaw may require substantial amount of efforts that is beyond what can be expected for a rebuttal.  \n\n2. Arguments with little clarity/grammatic errors:  \na) \u201cTherefore, adapting SAM to medical image segmentation tasks is the main direction by modifying the structure of SAM\u201d, what does this mean? \nb) \u201cWe not only maximize the utilization of the capabilities of SAM but also adapt SAM to medical image segmentation, which is a trade-off.\u201d why? \nc) \u201cWhen we obtain all outputs for the total classes, each output for a certain class has a different distribution from other outputs, since each output is generated by a specific prompt and trained by a sigmoid function. Therefore, it will obtain very bad results if we directly use a softmax function for all output, which is shown in Figure 3(b).\u201d What does this mean? \n\n3. Unsupported claims:  \nd) \u201cMost of the papers abandoned and designed the prompt encoder or mask decoder to avoid the requirements of prompts. But this way is not advisable since it would destroy the consistent system of SAM and abandon the strong abilities of the prompt encoder and mask decoder, which are trained via large-scale datasets.\u201d Given that you are tuning on new data anyway, discarding the prompt encoder may not necessarily be a drawback unless there are substantial evidences. \ne) \u201cSAM, as shown in Figure 1(a), is the first prompt-driven foundation model for natural image segmentation.\u201d Wrong. Works such as CLIPSeg come out much earlier. \n\n4. Sloppy mathematical notations in Sec. 3.2. The authors are encouraged to use superscripts and subscripts properly. \n\n5. Fig 2. is complicated and confusing. \n\nThe experiments are overly simplistic: segmenting abdominal CT on Synapse dataset and cardiac MRI on ACDC dataset, in late 2023, are overly simplistic, and they are commonly agreed to be solved problems. The authors are expected to demonstrate how SAM-based models shed light on unsolved problems, e.g., dealing with more challenging segmentation targets and/or using less training data. Also, the authors are expected to make comparisons with SAM-derived, manual-prompt-free approaches, e.g., [1,2,4] \n\n \n[1] Medical sam adapter: Adapting segment anything model for medical image segmentation \n\n[2] Auto-Prompting SAM for Mobile Friendly 3D Medical Image Segmentation \n\n[3] 3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation \n\n[4] How to Efficiently Adapt Large Segmentation Model(SAM) to Medical Images \n\n[5] AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder"
            },
            "questions": {
                "value": "The authors are encouraged to improve the clarity of writing, e.g., becoming more straightforward to the key arguments, instead of reaching an conclusion with convoluted chain of arguments; using more concise language; avoiding groundless claims.\n\nThe authors are encouraged to improve the clarity of the illustrations, especially for Fig. 2. It would be more readable if a high-level overview of the approach can be first presented, before diving into endless details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors need to be aware of potential biases inherent in deep learning models for medical imaging applications (not specific to this work)."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4254/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4254/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4254/Reviewer_wtde"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4254/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765032476,
        "cdate": 1698765032476,
        "tmdate": 1699636392283,
        "mdate": 1699636392283,
        "license": "CC BY 4.0",
        "version": 2
    }
]