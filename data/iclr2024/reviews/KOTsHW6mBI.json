[
    {
        "id": "HOEnxgDW6i",
        "forum": "KOTsHW6mBI",
        "replyto": "KOTsHW6mBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_Fyz9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_Fyz9"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new multi-bit watermark for language models. This strategy ensures the robust extraction of extensive watermarks without requiring model access or fine-tuning, preserves text quality, supports zero-bit detection, and withstands intensive adversarial alterations such as text interleaving and paraphrasing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The introduction of a multi-bit watermark presents an innovative concept within this research domain.\n\nExperimental results affirm the robustness of the proposed watermark algorithm."
            },
            "weaknesses": {
                "value": "1. The multi-bit watermark methodology, at a cursory glance, seems akin to an expansion of the red-green list from [1] into a more nuanced multi-color list. Given that the detection methodologies remain consistent, the paper should elucidate how the multi-bit approach enhances the robustness intrinsic to [1]. As this constitutes the crux of the paper's contribution, it is imperative for the authors to furnish theoretical insights and a robust analysis concerning the multi-bit watermark's robustness.\n\n2. There is a noticeable absence of a comparative robustness analysis between the proposed method and that established in [1] within the experimental evaluations. Considering the resilience of the [1] watermark against text alterations, it is essential for the authors to demonstrate superior robustness to substantiate their contributions effectively.\n\n3. While the authors purport that the multi-bit watermark sustains the caliber of the text, the empirical evidence, particularly in Table 1, suggests parity in quality between texts generated by the multi-bit and zero-bit watermarks. It is incumbent upon the authors to extend their comparison to texts generated without watermarks to convincingly affirm that the multi-bit watermark preserves text quality.\n\n4. Figure 5(a) depicts a correlation wherein an augmentation in bit-width correlates with diminished robustness. This trend ostensibly advocates for the utilization of the zero-bit watermark, prompting the question of whether, in terms of robustness, [1] might present a more formidable solution than the proposed multi-bit watermark.\n\n[1] A Watermark for Large Language Models. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. ICML (2023)"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1040/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697986432025,
        "cdate": 1697986432025,
        "tmdate": 1699636030445,
        "mdate": 1699636030445,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4LLc4kidSQ",
        "forum": "KOTsHW6mBI",
        "replyto": "KOTsHW6mBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_hfyX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_hfyX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a scheme for \u201cmulti-bit watermarking\u201d texts generated by language models by extending a work on \u201czero-bit\u201d watermarking by Kirchenbauer et al. [2023a].\n\n---\n\n## Comment After Rebuttal\n\nThank you for taking the time to address all my concerns and answering all my questions!\n\nI would appreciate it if the author(s) could incorporate in the revised version my suggestion regarding the steganography, TPR/FPR of the watermark \"detection,\" and other clarifying remarks the author(s) have provided. After reading the other reviews, I also echo the importance of reporting the results on instruction-tuned models. I still would like to advocate for the slightly different formulation of the \"list decoding\" I proposed, but that's not a big deal.\n\nOverall, the main weaknesses of the paper are the lack of technical novelty and the less-than-ideal empirical results. Regardless, I believe that my original review may have been too harsh on these aspects. The author(s) have convinced me to see the practical value of the problem and the fact that they are one of the first to study it. Given these facts, I believe that the weakness in terms of the empirical results is not so significant, and this line of work is worth iterating on in the future. As a result, I'm willing to raise my rating from 5 to 6."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality\n\nThe proposed method is relatively simple and is a nice extension of the zero-bit watermarking scheme. This means that any intuition as well as tools and analysis that are based on Kirchenbauer et al. [2023a] should also carry over and benefit the understanding of this scheme.\n\n### Quality\n\nThis paper contains thorough experiments considering multiple sets of parameters, models, metrics, and attacks. There is a good amount of numerical results and textual analyses in both the main text and the appendix, making the results convincing.\n\n### Clarity\n\nThe paper is well-written and easy to follow. Section 3 and Figure 1 provide a simple description of the scheme. The messages are effectively conveyed; all the figures and tables are easy to interpret."
            },
            "weaknesses": {
                "value": "### Significance\n\nWatermarking for detecting machine-generated content is an intellectually interesting work, but I\u2019m not sure about its practical benefits for two reasons. First, with open-source LLMs, it is virtually impossible to enforce the \u201cpost-processing\u201d watermarks. Second, under the assumption that there\u2019s an adversary trying to remove the watermark (e.g., via back-translation or paraphrase), it is very difficult to achieve a satisfactory TPR with a low enough FPR. This problem is heavily asymmetric in the sense that the cost of an FP is gigantic (wrongly detecting a human-written text as a machine\u2019s) compared to an FN so an acceptable FPR has to be very low in practice (~1e-5).\n\n### Comparison to Steganography\n\nThe paper briefly mentions steganography and claims that it is different from watermarking. However, I\u2019m not convinced that they are different. The undetectability property of steganography is exactly desired by LLM watermarking, i.e., a watermark should not modify the distribution of the \u201ccover text\u201d (or LLM-generated text) in a noticeable way. Generally, steganography and watermarking differ in their purpose, but in the instance of LLM, they are identical concepts. So it seems important that this paper (and other watermarking papers) is **compared (theoretically and/or empirically) to steganography on natural language** such as [1] and [2].\n\n### Methodology\n\n- List decoding can be better parameterized by a confidence threshold rather than a fixed length $|\\mathbb{L}|$ like a 95%-confidence interval. More precisely, given a confidence threshold (says 95%), we want to make a statement like: \u201cWith probability 95%, the true message is among these five messages\u201d (think of [conformal prediction](https://en.wikipedia.org/wiki/Conformal_prediction), for example). There should be a principled way to either model this directly or compute this by aggregating the confidence score at each position ($c_i$)\u2014This can be a bit tricky since $c_i$\u2019s are not independent.\n- Also including the list decoding as part of the bit accuracy metric (e.g., in Figure 4 and 5) seems irrelevant and unjustified to me. It\u2019s not clear how the practitioners would be able to utilize the 16 other less-confident messages in the LLM detection/attribution setting.\n- One very nice property of the zero-bit watermarking scheme proposed by Kirchenbauer et al. [2023a] is the theoretical guarantee on the number of tokens in the green list (Theorem 4.2). It would be nice to see a similar theoretical analysis on this paper.\n\n### Experiments\n\n**Metric.** AUC is a relatively misleading metric in practice for watermarking. As mentioned earlier, each FP is very costly so an appropriate metric in this scenario is usually something like TPR at a very low level of FPR (e.g., 1e-2, 1e-4, 1e-6, etc.).\n\n**Comparison to zero-bit watermarks.** I would like to see a comparison of the efficiency of this multi-bit watermark to the previously proposed zero-bit ones (at least, Kirchenbauer et al. [2023a]) for just detecting machine-generated texts. This is important because prior to attributing the text to which model/service, we have to first decide whether the text is generated by a machine or a human. In theory, this scheme should trade off the ability to encode longer messages with the watermark detection efficiency.\n\n### Other Minor Issues\n\n- Figure 2 caption: \u201cClean bit error - corrupted bit error\u201d was a little bit unclear as the metrics have not been defined at this point.\n- [Typo] Page 9: \u201c**Across Model Scales, Datasets, Hash Schemes.** The results for larger models (13B, 70B) and other datasets are in Appendix A.4.\u201d should be Appendix A.6?\n\nReferences\n- [1] https://arxiv.org/abs/1909.01496\n- [2] https://arxiv.org/abs/2210.14889"
            },
            "questions": {
                "value": "- Page 4: How exactly is the hash function $f$ applied to the previous tokens (step 1 on page 4)? What\u2019s the hash length $h$? How does $h$ affect the robustness or the bit accuracy?\n- Page 6: In \u201c**List Decoding** The linear nature with respect to\u2026\u201d, what does \u201clinear nature\u201d refer to?\n- Page 7: In the last paragraph, it is unclear how the scheme works when $\\gamma \\ne 1/r$ (e.g., $\\gamma = 0.25$ and $r=2$).\n- Page 7: In the last paragraph, how is the $p$-value computed? Why is it stated relative to the runner-up instead of a null hypothesis? I\u2019m not sure if this has been mentioned previously.\n- Figure 4: Why does $r=4,\\gamma=0.25$ perform better than $r=2,\\gamma=0.5$ when $b$ and $T$ are fixed? Detecting two \u201coptions\u201d should be easier than four. Is this due to the effective bit-width (i.e., the message when $r=2$ is actually twice as long as when $r=4$)?\n- Page 8: Where can I find more detail on how the copy-paste attack was carried out?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1040/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1040/Reviewer_hfyX",
                    "ICLR.cc/2024/Conference/Submission1040/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1040/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698046650248,
        "cdate": 1698046650248,
        "tmdate": 1700869036925,
        "mdate": 1700869036925,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VmMJAs5HNt",
        "forum": "KOTsHW6mBI",
        "replyto": "KOTsHW6mBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_eLnz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_eLnz"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel technique, \"Multi-bit Watermark via Position Allocation\" (MPAC), to prevent malicious misuses of large language models beyond the identification of machine-generated text. Traditional methods mainly focus on detection, but the current approach embeds traceable multi-bit information during the model's generation. This makes it easier to find the original creator of the item and to notice it, which is important when combating serious misuses like the dissemination of false information on social media. This watermarking method is built upon the previously proposed zero-bit watermarking technique and allows for the embedding of longer messages without the need for fine-tuning the model. The embedded watermark remains robust against manipulations like human text interleaving and paraphrasing. Experiments show that great accuracy can be achieved when embedding messages up to 32 bits. The method bridges the gap between machine and human text distinction while embedding valuable information, offering a viable way for countering large language model abuses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strength:\n\u2022\tThe introduction gives a concise overview of the significance of machine-generated text identification, the approaches used in the past, and the reasons why more sophisticated techniques are required.\n\u2022\tThe paper provides various graphs and visual representations, aiding in understanding the performance under different conditions. \n\u2022\tOne of the strengths of the paper is its demonstration of the model's performance under copy-paste attacks, which tests the robustness of their watermarking method."
            },
            "weaknesses": {
                "value": "Weakness:\n\u2022\tThe performance of the model seems to be reliant on token length. When messages are longer, there seems to be a dip in performance, which may not be suitable for all applications. For example: As mentioned under Figure 5, embedding longer messages at fixed bits per token seems to decrease the performance, especially when reaching up to 64-bit.\n\u2022\tMetrics like AUC and bit-accuracy are essential, and the paper might benefit from a more qualitative or user-centric analysis. Reliance solely on quantitative metrics might not capture the full user experience or real-world applicability. For example: The paper primarily discusses results in terms of AUC, bit-accuracy, and other such metrics. A discussion on real-world applications potential user feedback could provide a more complete picture of the method's usefulness.\n\u2022\tWhile the paper evaluates against human-induced copy-paste attacks, it doesn't delve deeply into other potential real-world challenges. For instance, how would the system perform against more sophisticated adversarial attacks or in scenarios with heavy noise?\n\u2022\tThe method's reliability on decoding depends heavily on the pseudo-random generator function. If someone can reverse-engineer or predict the pseudo-random function, the watermark can potentially be tampered with or removed."
            },
            "questions": {
                "value": "\u2022\tWhile the paper states that the watermark is relatively robust under strong attacks like interleaving human texts and paraphrasing, are there any known methods or strategies that might weaken or remove the watermark effectively?\n\u2022\tHow easily can the MPAC method integrate with existing large language models and platforms? Are there any potential challenges to be aware of?\n\u2022\tA detailed comparison between MPAC and other watermarking techniques, in terms of performance, accuracy, and robustness, would provide more context to the reader regarding the advantages of MPAC."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1040/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1040/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1040/Reviewer_eLnz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1040/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728751631,
        "cdate": 1698728751631,
        "tmdate": 1699636030269,
        "mdate": 1699636030269,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "edvoPdr4qg",
        "forum": "KOTsHW6mBI",
        "replyto": "KOTsHW6mBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_TksX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_TksX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel method titled \"Multi-bit Watermark via Position Allocation\" (MPAC) for tracing the misuse of large language models beyond just identifying machine-generated text. The method embeds traceable multi-bit information during language model generation, enabling robust extraction of the watermark without any model access, embedding and extraction of long messages without fine-tuning, and maintaining text quality. The paper also demonstrates the robustness of the watermark under strong attacks like interleaving human texts and paraphrasing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper proposes an innovative and practical method to address the problem of misuse of large language models, extending beyond mere identification to traceability.\n\n(2) The proposed method, Multi-bit Watermark via Position Allocation (MPAC), allows embedding and extraction of long messages without model access or fine-tuning, which is a significant improvement over existing methods.\n\n(3) The authors also offer detailed empirical findings from their experiments, showing that their method can effectively embed 8-bit messages in short text lengths with over 90% bit accuracy."
            },
            "weaknesses": {
                "value": "(1) The paper did not provide comparisons to other multi-bit watermarking methods, such as the method presented by Yoo et al. in \"Robust multi-bit natural language watermarking through invariant features\" Providing comparisons to related work would have strengthened the paper.\n\n(2) The detection of the watermark is an important part. More details on the watermark detection methodology would have been beneficial.\n\n(3) Embedding capacity limited by the text distribution and model. Low entropy distributions like code have an inherently lower capacity.\n\n(4) The author could consider testing on other models besides LLAMA."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1040/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699050788340,
        "cdate": 1699050788340,
        "tmdate": 1699636030197,
        "mdate": 1699636030197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1qAQTh8O65",
        "forum": "KOTsHW6mBI",
        "replyto": "KOTsHW6mBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_7Lgp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1040/Reviewer_7Lgp"
        ],
        "content": {
            "summary": {
                "value": "The paper extends the  Kirchenbauer et al. watermarking scheme to encode multi-bit messages. These allow for finer tracking of AI generated content at a more granular level. \n\nThe paper does this the use color lists and positional allocation. Kirchenbauer et al. partitions the vocab into 2 lists, but here the authors propose partitioning it into multiple color lists, and picking the color for sampling based on the information in the message at a certain position. The position in the message is determined by hashing the previously seen context, and then the color in the message at this position is looked up. The authors run various experiments to look at the quality of responses, detectability and robustness of the watermark, and present evidence illustrating the challenges of encoding longer messages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Encoding multi-bit information is an important problem, and a useful extension to 0-bit watermarking. This lets us specific identify users abusing LLMs, going beyond the problem of simply detecting LLM generated content.\n\n2. The approach presented seems reasonable, and an improvement over simple baselines of: 1) a unique watermarking key for each message we want to encode,  or 2) r=2 (the Kirchenbauer set up) and then simply picking the red and green lists based on the position.\n\n3. The experiments are thorough, and the method seems to work for encoding multi-bit messages without degrading the quality anymore than the 0-bit scheme (the 0-bit scheme already degrades the LLM quality a bit)."
            },
            "weaknesses": {
                "value": "1. I found the presentation a bit difficult to follow. I think the paper would be much easier to follow if there were fewer forward references, and the paper was a bit more self contained. It might also be worth dedicating space to central contributions in the paper, and having a list of claimed contributions in the paper. \n\nI would like to see the decoding scheme in the main paper, and not the appendix since that is central to understanding a lot of the other details that are in the main paper.\n\n2. There are concerns that there's not enough bandwidth to even do 0-bit watermarking, particularly for LLMs that have gone through RLHF/SFT as that process significantly reduces the entropy available for watermarking. It is unclear to me if the proposed approach is useful in practice for encoding multi-bit messages.\n\n3. There are a few details missing. It is claimed that list decoding improves performance, but it is unclear to me how the decoded list is used in this accuracy computation. If one of the messages in the list matches the encoded message, is that counted as a success?\n\n4. I think the innovation over the 0-bit UMD scheme is somewhat incremental for a machine learning conference, at least from an ML standpoint."
            },
            "questions": {
                "value": "1.  The UMD scheme is inherently distortionary. Is there a way to apply the ideas presented in this paper to the non-distortionary schemes (e.g., https://arxiv.org/abs/2307.15593)?\n\n2. Can you measure the performance of the multi-bit encoding scheme on IT/RLHF/SFT models like Llama or Vicuna? They have much lesser entropy and are closer to the models deployed in the real world. It would be interesting to see if there is bandwidth for multi-bit watermarking for those models.\n\n3. How is the accuracy computed with list decoding?\n\n4. Can you show some ablation experiments depicting the comparisons between list decoding, and random bit flips in the most likely decoded message?  \n\n5. The Kirchenbauer et al. method degrades the language model with increasing delta. Would it be possible to compare the degradation with changing delta for their 0-bit scheme, and the multi-bit scheme presented here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1040/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699531902994,
        "cdate": 1699531902994,
        "tmdate": 1699636030120,
        "mdate": 1699636030120,
        "license": "CC BY 4.0",
        "version": 2
    }
]