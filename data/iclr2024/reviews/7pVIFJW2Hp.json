[
    {
        "id": "9CtS3JFq0X",
        "forum": "7pVIFJW2Hp",
        "replyto": "7pVIFJW2Hp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_3Ti9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_3Ti9"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel framework and benchmark dataset for improving figure caption generation using human feedback. The reinforcement learning with human feedback (RLHF) framework uses a small amount of expert annotations about the quality of figure-caption data and uses this to fine-tune figure-caption generation models.\nExtensive experiments show that their RLHF framework outperforms standard fine-tuning for various models like BLIP, ViT+GPT2 etc. For example, BLIP-RLHF achieves 35.7% gain in BLEU over fine-tuned BLIP.\nMoroever, they release the new benchmark dataset of figure-caption pairs labeled with human feedback such as helpfulness, takeaway, OCR etc. to promote further research in this area."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**\n\nThe paper presents a new RLHF framework that utilizes limited human feedback to optimize caption generation models. The technique of learning an \"oracle\" model to predict feedback scores at scale is creative. Applying offline RL methods like upside-down RL in this context is also novel.\n\n**Quality and Clarity** \n\nThe authors conduct extensive well-designed experiments on multiple models, ablations, and metrics to demonstrate the effectiveness of their approach. The paper is well written and easy to follow.\n\n**Significance**\n\nThe focus on aligning figure captions to human preferences is significant for spurring progress in vision-language models for scientific literature. The paper proposes a feedback-based approach to align captions to human preferences and the annotated benchmark dataset will significantly help in improving research in this domain."
            },
            "weaknesses": {
                "value": "Please refer to the questions section for discussion of weaknesses. There is nothing in particular I would like to point out here."
            },
            "questions": {
                "value": "**Questions**\n\n1. From Figure 2 (and also intuitively), it seems like the captions that are small in length are often uninformative and hence unhelpful. Would it make sense to add a baseline where the model does not incorporate the Oracle model but just uses this heuristic-based \"good\" or \"bad\" token during training? \n\n2. The improvements on ViT+GPT2 are very minor (Table 2). I wonder would the CLIPCap model be a better backbone model as it learns an adaptor to transform the image features to the language model space, by combining it with required prompts or the human feedback scores.\n\n3. There are relevant missing related work and baselines: Matcha, ACL'23 (https://arxiv.org/pdf/2212.09662.pdf )and Deplot, ACL'23 (https://arxiv.org/pdf/2212.10505.pdf). Although both of these works do not focus on caption generation, they are very relevant for ChartQA task and can be used as backbones that can be fine-tuned for the task of figure caption generation. \n\n**Minor**\n\n1. Make it clear in the main paper that the BLEU metric is BLEU-4. \n\n2. The information about how human scores look like comes very later in the paper after section 3.3/3.4. A very brief overview of what these scores are in the introduction would be helpful to the reader."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698151407238,
        "cdate": 1698151407238,
        "tmdate": 1699636630253,
        "mdate": 1699636630253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zbRoaUeZxn",
        "forum": "7pVIFJW2Hp",
        "replyto": "7pVIFJW2Hp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_oWEK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_oWEK"
        ],
        "content": {
            "summary": {
                "value": "This ppaer proposes a RLHF framework, FigCaps-HF, a novel framework that leverages domain expert feedback to generate high-quality figure captions tailored to reader preferences. Empirical results show that the caption performs better than BLIP finetuning. The authors also release the benchmark and human feedback data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed pipeline holds promise for enhancing image captions, with compelling results demonstrated on smaller datasets.\n\n2. The comparison to the fine-tuned BLIP model highlights the superior performance of the RLHF method, as evident in both human evaluations and standard metrics.\n\n3. The release of human feedback datasets is a valuable contribution that can benefit a broad range of research communities. The authors also provided detailed human data collection interface."
            },
            "weaknesses": {
                "value": "1. The experiments conducted in this study are limited in scale. The choice of a relatively weak baseline and potential data distribution shifts raises concerns about the fairness of the comparisons. \n\n2. The main technical novelty seems to be `HUMAN FEEDBACK PREDICTION MODEL`. However, this is limited discussion why such explicit prediction model can help RLHF. A fair comparions would be how proposed method performe better than end-to-end RLHF system."
            },
            "questions": {
                "value": "The authors mentioned `As can be seen from the results, our model is able to achieve good results on the validation set. This highlights that our human-feedback prediction model demonstrates out-of-sample generalization and proves the statistical significance of our model.` How different is the data distribution? How would you measure the genearlization ability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698528623228,
        "cdate": 1698528623228,
        "tmdate": 1699636630149,
        "mdate": 1699636630149,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yPT4rpkuEW",
        "forum": "7pVIFJW2Hp",
        "replyto": "7pVIFJW2Hp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_LPWT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_LPWT"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on existing Figure-to-Caption models that fall short of metrics like helpfulness, explainability, and visual-descriptiveness. To this end, they introduce an RLHF framework for figure-to-caption generation with a small amount of actual human feedback for generating high-quality captions. They also propose a new benchmark of figure-caption pairs with caption quality scores for a better understanding of the read-aligned figure-caption pairs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **The proposed method is effective**. As shown in Fig.2, the RLHF has improved the BLIP and Vit+GPT2 models with a clear improvement.\n\n2. **The benchmark of figure-caption pairs is helpful**. This paper provides a new benchmark of figure-caption pairs is helpful for the research community, and they have done a great release."
            },
            "weaknesses": {
                "value": "1. **Time Complexity Analysis.** This paper claims that using offline reward-conditioned behavioral cloning for model optimization is computationally efficient. It is not convinced, that you should compare the time complexity analysis between the offline RLHF and online RLHF methods.\n\n2. **The proposed method is not novel enough.** The framework of the RLHF for figure-to-text can not provide more insight for the understanding of the read-aligned figure-caption pairs. It may not be enough for the technical contribution.\n\n3. **The writing needs to be improved**. There are many typos in the main text.\ne.g. figure-ti-caption -> figure-to-caption in Section 6."
            },
            "questions": {
                "value": "As shown in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784594062,
        "cdate": 1698784594062,
        "tmdate": 1699636630050,
        "mdate": 1699636630050,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OCmu6TLr4D",
        "forum": "7pVIFJW2Hp",
        "replyto": "7pVIFJW2Hp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_MqnR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5924/Reviewer_MqnR"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of generating high-quality captions for scientific figures. Traditional methods follow the simple image captioning approach for this task. The paper made two contributions: (1) introduction of an eval benchmark (2) conduct the first RLHF-based method for this problem. The dataset and code are open-sourced. Experimental results also suggest promising gains over baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper presents a set of evaluation methodology and benchmark, which can be useful for future research in the field.\n2. Experimental results show clear gains over the baseline.\n3. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The method used in the paper is a combination of existing techniques, i.e. RLHF. The technical innovation is hence limited.\n2. The applicability of the method is narrow since it is targeting for figure captioning task only. It will be more inspiring if it can be extended to general image/video captioning."
            },
            "questions": {
                "value": "I found it a bit confusing to the two contributions in this paper together as a \"framework\", as I don't feel they share synergy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815965376,
        "cdate": 1698815965376,
        "tmdate": 1699636629951,
        "mdate": 1699636629951,
        "license": "CC BY 4.0",
        "version": 2
    }
]