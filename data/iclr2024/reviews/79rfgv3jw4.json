[
    {
        "id": "ICoKJzXWXk",
        "forum": "79rfgv3jw4",
        "replyto": "79rfgv3jw4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_XZq8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_XZq8"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of AI agents acting in team settings, particularly where the AI agent is more capable at the task than it's partner(s). Chess with both sides played by two players is used as a study domain, with two formulations of team play considered:\n1) stochastically switching between which of a pair of agents on the team chooses for any given move\n2) having one agent choose the piece type and the other choose the specific piece and move to make\n\nThese formulations are meant to target the capabilities of the stronger agent to work with the weaker agent either due to temporal uncertainty (1) or only indirect/partial control on action selection (2).\n\nThree agents are evaluated to play the role of the strong agent:\n1) Tree agent: Uses Monte Carlo Tree Search (MCTS) to augment a baseline policy (agent)\n2) Expector agent: Maximize win probability over a short horizon given access to models of all agents\n3) Attuned agent: Fine-tunes the agent from traces of agent pairs playing\n\nIn all cases the models have full access to models of the other agents in the game. Evaluations demonstrate that all three agents proposed improve over a naive baseline where a strong and weak agent are paired. Comparisons are made in terms of win rate and change in win probability for moves. Detailed studies are made to examine the mechanisms for the improvements, along with a small study of the ability of the expector agent to generalize to imperfect partner models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "# originality\nFraming collaboration (compatibility) as the ability to cope with partners under the specific cases here is novel. While AI teaming in chess is well-established, using stochastic turn taking or hand-brain teams as a way to assess agent coordination is novel. The proposed agents are all reasonable extensions of methods for opponent or partner modeling to these scenarios.\n\n# quality\nThe experiments thoroughly examine the performance improvements of the varying approaches under two different conditions. The tests quantify uncertainty and demonstrate a clear benefit both to long-term win rates and intermediary probabilities of selecting good moves (optimal with respect to a given agents' predictions). These demonstrate that the models for agent pairing are achieving the desired effect of producing teams stronger than their individual composite agents.\n\n# clarity\nThe text is clear on the technical approach and provides good framing of the core frameworks.\n\n# significance\nHuman-AI collaboration is of growing interest (and public concern) as AI systems are more widely deployed. Developing new methods to assess how teams work together and can be robust to differing agent capabilities is a timely and important topic. This will be widely of interest across subcommunities at ICLR (and beyond).\n\nWorking specifically in games has clear significance to research in game playing, reinforcement learning, and efforts at multi-agent coordination."
            },
            "weaknesses": {
                "value": "# originality\nNo major comments. If there is room, the related work could make further reference to the literature on zero-shot coordination, which shares the goal of enabling teams of agents to work well together, but subject to the constraint of lacking a model of the partner agents. Some examples:\n\n- Hu, Hengyuan, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. \"\"other-play\" for zero-shot coordination.\" In International Conference on Machine Learning, pp. 4399-4410. PMLR, 2020.\n- Lupu, Andrei, Brandon Cui, Hengyuan Hu, and Jakob Foerster. \"Trajectory diversity for zero-shot coordination.\" In International conference on machine learning, pp. 7204-7213. PMLR, 2021.\n- Treutlein, J., Dennis, M., Oesterheld, C. and Foerster, J., 2021, July. A new formalism, method and open issues for zero-shot coordination. In International Conference on Machine Learning (pp. 10413-10423). PMLR.\n\n# quality\nResults on the mechanisms behind the improvement gained by different focal agents are murky. For example, \"tricking\" may be implicated for EXP, but not TREE or ATT (though I may have misunderstood the text on this point). In general, it is hard to assess how strong some of these results are and what the implications may be. More specific questions are provided below.\n\nConversely, few results test the generalization capabilities of the agents, particularly when relaxing the assumption of access to a perfect model of partners and opponents.\n\n# clarity\nThe results presentation is somewhat murky, particularly in the detailed analysis of mechanisms (see the questions for specific details). In general the results would benefit from a clear statement of the outcomes of the analyses at the start of the section, followed by the existing detailed description of results. This will help readers glean the overall conclusions that get lost in some of the details.\n\n# significance\nThe primary limitation to the impact of the results is the requirement for agents to have models of all relevant agents in the scenario. One experiment slightly relaxes this constraint and the results become substantially weaker. It would be beneficial to add some remarks on this point and future directions mitigate these weaknesses with imperfect partners. As a first work on this method I do not view these as substantial limitations, but they deserve some additional comment."
            },
            "questions": {
                "value": "- Tables 1 & 2\n\t- Minor suggestion: Combine these two tables into one with a row defining the task (STT vs HB).\n\t- If possible it would help to include the error ranges in the main text.\n\t- Is there any intuition for why TREE does so much better than EXP in HB? The improvement is the opposite of what is observed in STT. Is the problem that in HB EXP has too limited of a search depth to handle the relevant decision horizon?\n- Section 4.2.2\n\t- The final conclusion from this analysis was unclear. \n\t- What are the implications that the standardized and non-standardized board position distributions differ substantially for TREE and ATT, but not EXP? This seems to suggest tricking is the not the main effect (as opposed to some more general distributional difference), but that was not completely clear either.\n\t- It may be better to remove this analysis (to the appendix) and provide more detail in the main text on the normalization applied in 4.2.3.\n\t- Section 4.2 would benefit from stating the conclusions drawn on tricking, helping, and indirect effects at the start of the section. A similar comment applies to section 4.3.\n\t- I could not understand from the text: which agents demonstrate tricking (and under what analysis)?\n\t\t- For helping it is more clear (based on the more complex analysis) what is happening.\n- Section 4.4\n\t- Are there results when the MAIA agent is selected without a rating prior? This would be interesting to see to understand how well a focal can compensate when the \"amount of weakness\" of the partner is highly uncertain.\n\t- One approach would be randomizing the skill of the partner agents over a reasonable range of ratings.\n\t- Are there comparable results for using TREE? I understand ATT is more computationally demanding to add, but those results would also be interesting.\n\t- In general I consider these results to be some of the most important for the work and would trade less detailed analysis from sections 4.1, 4.2, and 4.3 for further elucidation of how well the approach generalizes when given imperfect partner information.\n- On the supplement\n\t- The results on using MAIA1900 and stockfish are both good information to add to the main body. These provide more color to the discussion about capabilities to partner with other agents and would be welcome in the main text.\n\t- I personally would trade these for the space used in sections 4.1, 4.2, and 4.3 as the final conclusions on the mechanisms at play remain unclear to me, while the conclusions about capacity to generalize (or not) seem more clear. \n\t- \"This does indicate, that our agents' compatibility is not a function of merely skill, but also style.\" An important point to make in the main text! Also very relevant for future work that would investigate other ways to enhance agent compatibility.\n\nTwo broad conceptual questions came to mind that I would appreciate further comments on:\n- Why is inter-temporal coordination the right framing for evaluating agent compatibility?\n\t- Perhaps sketch some specific applications where this is a natural interaction paradigm outside chess. The example that came to mind is autonomous vehicles (and robotics more generally), where humans may take over at arbitrary points from an AI driver.\n- How correlated is this notion of interpretable with prior efforts?\n\t- The term interpretable is quite loaded in the literature, so I am trying to understand why it is a good description of the method here.\n\t- Prior efforts normally aim to enable inspection of the behavior of a model, but that is not obviously what is demonstrated here. Weak agents can perform better when in scenarios prepared by the strong agents. But it's not obvious that is due to any ability to \"interpret\" these situations, as opposed to being given better initial states to use.\n\n---\n\n**Author discussion amendment**\n\nI have revised my score up to reflect the improved paper structure and clearer promising results on generalization with other types of agents. These reflect the methods generalize across some dimensions (human players and skills) but not others (whatever makes Stockfish different to humans). These point toward important future research efforts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Reviewer_XZq8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698022925141,
        "cdate": 1698022925141,
        "tmdate": 1700694719607,
        "mdate": 1700694719607,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qfIHhXl7k9",
        "forum": "79rfgv3jw4",
        "replyto": "79rfgv3jw4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_D1Jx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_D1Jx"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the interaction between highly skilled agents with less-skilled counterparts. The paper uses chess as a benchmark due to the availability of both a variety of highly and less-skilled agents at a multitude of skill levels. The paper introduces three methodologies to compare a tree agent, an expector agent, and an attuned agent (finetuned self-play RL agent). Prior art has considered simultaneous teamwork between AI and humans at tasks but does not consider if they are inter-temporally compatible. The paper evaluates against state-of-the-art chess AI and finds that their combined method can outperform. The paper evaluates in two scenarios: random swapping between the high-skill and low-skill AI after each move and a variant of chess called \u201chand and brain\u201d, which requires one teammate to select the piece to move while the other teammate chooses the move selection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+The wide variety of chess bots available at different skill levels and variety of play styles at each skill level makes for a very interesting and realistic benchmark. I could see this having direct influence on human-AI teaming.\n\n+The motivation for this paper is very strong. It is very interesting to consider the advice mismatch between superhuman AIs and mere human players. This is an open problem.\n\n+Introduces a new version of interpretability: \u201cinterpretable iff a weaker agent can follow\u2013up\u201d\n\n+Clear discussion of limitations."
            },
            "weaknesses": {
                "value": "-Although there are many comparisons between the lower-skilled agents and humans, this does not guarantee that the results will be the same with humans. It would be interesting to have a small experiment to confirm the results in this setting.\n\n-The description of the main results do not claim that there is a best guidance for what another researcher should try as the main takeaway"
            },
            "questions": {
                "value": "1) Tables 1-3 could be explained more clearly. Please consider bolding best results\n\n2) Why is the EXP method N/A in table two?\n\n3) Do you have any intuition as to why EXP performs best overall? Why does the paper not claim this as the best method if it has the highest winrate?\n\n4) The results are explained well after reading through the experiments section many times. However, I believe that this could be clearer. Please consider using a chart that summarizes the best combinations (no score, just to guide the intuition)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Reviewer_D1Jx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784807324,
        "cdate": 1698784807324,
        "tmdate": 1699637020856,
        "mdate": 1699637020856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "udv7i3m70h",
        "forum": "79rfgv3jw4",
        "replyto": "79rfgv3jw4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_9rEp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_9rEp"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenge of enabling powerful AI agents to interact effectively with agents of lower computational abilities. Using collaborative chess variants as models, the study proposes a framework to evaluate the compatibility of high-performing AI with lower-skilled entities. Traditional chess engines, though nearly optimal, struggled in this domain, leading to the development of three methodologies to create skill-compatible AI agents. These agents demonstrated superior performance in the proposed collaborative frameworks than conventional chess AI systems, such as AlphaZero. The paper emphasized that achieving raw performance in AI is not enough; the AI should also be compatible with lower-skilled agents for successful interactions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Innovative Concept: The paper introduces a novel and timely concept of \"skill-compatibility\" that addresses the real-world challenge of AI and human collaboration.\n\n- Empirical Evidence: The use of collaborative chess variants as model systems offers practical insights and empirical proof-of-concept.\n\n- Multiple Methodologies: The paper presents three distinct methodologies, showcasing the versatility of approaches to achieving skill-compatibility.\n- Comparative Analysis: By comparing newly proposed agents with state-of-the-art chess AI, the research demonstrates the tangible benefits of their approach."
            },
            "weaknesses": {
                "value": "The main weakness is that the methods proposed don\u2019t show a clear path toward broader human-AI collaboration. Since you use the weaker chess engine as a subroutine in search, it isn\u2019t clear how you could make a version with humans since they can\u2019t communicate at test time. Furthermore, these methods seem limited to chess. It would be more interesting to have methods that would work for a variety of cooperative or cooperative/competitive games such as Bridge or Hanabi. \nMinor: formatting is off"
            },
            "questions": {
                "value": "1. How can the methodologies be adapted for more dynamic, less rule-bound environments outside of chess?\n2. Would the proposed techniques be effective in real-time scenarios where the lower-skilled entity is a human?\n3. How does the proposed skill-compatible AI adapt to the continuous learning and evolving capabilities of lower-skilled agents?\n4. In real-world applications, how can the balance between raw AI performance and skill-compatibility be optimized without compromising on crucial tasks?\n5. Could the techniques be integrated into current AI frameworks for instant benefits, or would they require a complete overhaul of the system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8221/Reviewer_9rEp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802163510,
        "cdate": 1698802163510,
        "tmdate": 1700591490539,
        "mdate": 1700591490539,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s7yuXgYsxo",
        "forum": "79rfgv3jw4",
        "replyto": "79rfgv3jw4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_SKaW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8221/Reviewer_SKaW"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of making super-human AI chess programs more \"skill compatible\" with humans, in that the moves they recommend are more likely to be moves that a human could successfully follow up and thus would actually have success playing in a game. This is studied by building on open-source engines like Leela.\u00a0 The proposed methods either do not require training or only require fine-tuning versions of a powerful base model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is clearly an important problem and is, to my knowledge, quite understudied. This appears to be a natural set of approaches that should be tried, and the thorough empirical evaluation of these approaches is useful for the community.\n\nThey also appear to be the first to have formalized this problem, and they have set up about as good a methodology for evaluating their method as one could hope for short of human trials."
            },
            "weaknesses": {
                "value": "Given that these approaches do not train from scratch, there is some concern that the local improvements may not be representative of the improvements you would hope for if the models were retrained.  \n\nSection 4.4 should be emphasized more from the beginning. It seems the biggest challenge with this line of work is the model of their partner.  It's not enough just to match the \"skill level\" of the human; but it is also important to match the style of the human and adapt to the patterns in human mistakes. In the rest of the paper, it appears this aspect is ignored, but in Section 4.4, evaluation is done by fine-tuning to mimic specific human players and thus could hope to account for the exact sorts of errors those humans are more prone to. I believe this section greatly strengthens the work, and it should be emphasized to ensure the reader understands where much of the remaining difficulty is.\n\nMinor points:\nThe \u201cexpector\u201d agent appears to be quite similar to prior work[1,2], though without fine-tuning against the fixed opponent. It is also closely related to the opponent-exploitation work in the AI-poker community related to work like [3]. I would expect there to be some more similar work to this to be in the exploitability literature in game theory and in the adversarial attacks literature, investigating slightly different questions than this work.\n\n[1] Timbers, F., Bard, N., Lockhart, E., Lanctot, M., Schmid, M., Burch, N., Schrittwieser, J., Hubert, T., and Bowling, M. Approximate exploitability: Learning a best response in large games.\n[2] Wang, Tony Tong, et al. \"Adversarial Policies Beat Superhuman Go AIs.\" (2023).\n[3] Ganzfried, Sam, and Tuomas Sandholm. \"Safe opponent exploitation.\""
            },
            "questions": {
                "value": "What is the relationship between this work and that of the exploitability and adversarial attacks fields?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8221/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839317220,
        "cdate": 1698839317220,
        "tmdate": 1699637020584,
        "mdate": 1699637020584,
        "license": "CC BY 4.0",
        "version": 2
    }
]