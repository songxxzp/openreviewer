[
    {
        "id": "JvekSLNvgu",
        "forum": "g7YB6K2eXj",
        "replyto": "g7YB6K2eXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_fWCS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_fWCS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new method for multi-bit joint training and mixed-precision super-net training in deep neural networks. This method aims to make model compression more adaptable for varying hardware and storage needs. The authors propose a Double Rounding quantization technique, which only requires storing the highest-bit integer model, making it more storage-efficient. They address the challenge of inconsistent gradients in multi-bit training by adaptively adjusting learning rates for different bit-widths. Additionally, a weighted probability training strategy for mixed-precision super-nets is introduced, improving the method's versatility. The paper also presents a decision-making approach using integer linear programming to find the best bit-width combination for different model layers, targeting optimal solutions. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. The proposed double rounding quantization technique, which offers a storage-efficient solution by only necessitating the storage of the highest-bit integer model."
            },
            "weaknesses": {
                "value": "The paper has several notable weaknesses, including lack of empirical evidence, ambiguities in methodology, unclear contributors to performance, missing Baseline, etc. Please see questions for details."
            },
            "questions": {
                "value": "1.\tIn Section 3.2, the authors attribute the notable divergence in convergence rates between the highest and lowest bit-widths during once-joint training of multi-bit models to the inconsistency in gradient updates between high-bit and low-bit quantization phases. However, this assertion lacks empirical evidence, as no experimental results are presented to highlight these inconsistent gradients. Additionally, the introduced Multi-LR approach, which adjusts learning rates based on different bit-widths, is heuristic in nature. It would be beneficial to understand if there is an underlying rationale or guiding principle for the selection of these learning rates to ensure they are both effective and justifiable.\n\n2.\tIn Section 3.3, the authors introduce the use of weighted probability for the supernet training. However, the methodology behind computing the sampling probability for various bit-widths within a given layer is not explicitly explained. This omission can lead to ambiguities in replicating the approach and understanding its full implications. Providing a clearer, step-by-step computation process would enhance the reproducibility of the proposed method.\n\n3.\tIn Section 3.3, the authors mention the capability of their method to swiftly produce multiple candidate configurations under specified constraints by adapting the ILP (Integer Linear Programming) algorithm post super-net training. Yet, this assertion lacks empirical backing as the section doesn not offer any associated experimental results. Providing tangible evidence or case studies would substantiate this claim.\n\n4.\tThe authors propose double rounding quantization that only keeps the highest bitwidth model instead of the full-precision counterpart. However, the performance drop brought from double rounding quantization is not clearly investigated. A deeper dive into the proposed method on the model's accuracy and efficiency would have provided a more comprehensive understanding of its real-world applicability and limitations.\n\n5.\tIn Figure 2, the authors present the gradient statistics of activation scales for ResNet20, offering insights into the network's behavior. However, a critical aspect that is not clarified is the initialization of these scales. For a fair and meaningful comparison, it is essential to ascertain whether all scales started from the same initialization point.\n\n6.\tIn Tables 1 and 2, the proposed method shows a significant performance improvement over the state-of-the-art methods. However, the specific components of the proposed method that primarily drive this performance enhancement remain ambiguous. A breakdown or ablation study highlighting the individual contributions of each component would provide deeper insight into the key drivers behind the observed improvements.\n\n7.\tThe experimental section appears to lack a crucial baseline comparison. It would be valuable to understand how the proposed method stacks up against an independent approach that trains different bit-widths separately. Such a comparison would shed light on the relative efficacy and advantages of the proposed joint training technique.\n\n8.\tA pivotal detail seems to be overlooked in the paper. It is unclear how many samples the authors utilized to compute the Hessian trace across various layers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698203857385,
        "cdate": 1698203857385,
        "tmdate": 1699636884851,
        "mdate": 1699636884851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KxQaWOeHZB",
        "forum": "g7YB6K2eXj",
        "replyto": "g7YB6K2eXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_DGQc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_DGQc"
        ],
        "content": {
            "summary": {
                "value": "The key innovations in the study are as follows: \"Double Rounding\" is proposed to maintain integer-weight parameter storage without compromising representation values. \"Multi-LR\" introduces a training strategy for multi-bit models that effectively reduces the training convergence gap between high-precision and low-precision models. \"Weighted Probability\" determines the access probability of bit-width for each layer based on the layer's sensitivity, aligning with the subnetwork's decision-making process during inference. Experimental results on ImageNet datasets demonstrate that the proposed method surpasses state-of-the-art techniques across various mainstream network architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper exhibits a well-defined structure, making it easy to navigate and understand.\n\n2. The primary objective is to address the complex issue of multi-bit quantization and demonstrate notable performance improvements compared to similar methods."
            },
            "weaknesses": {
                "value": "1. I find the distinction between Adabits and the proposed Double Rounding in the figure somewhat minimal. It seems that the primary difference lies in the altered value range. Could these two methods essentially be equated with one having a zero point and a different scale value?\n\n2. It may be beneficial to include the algorithm for weight probability in the main paper. This approach could reduce the volume of explanatory text, ultimately enhancing clarity for readers.\n\n3. Evaluating the proposed method on large transformer models as well as tiny models, that are particularly susceptible to the effects of quantization, would provide valuable insights.\n\n4. In my overall assessment, I believe that the three proposed techniques may fall short of meeting the publication standards."
            },
            "questions": {
                "value": "1. Could you provide further clarification regarding the distinction between Adabits and Double Rounding?\n\n2. The results presented in Table 1 raise the question of whether Knowledge Distillation (KD) plays a more significant role than the three proposed techniques.\n\n3. Table 1 exclusively presents uniform bit-width results. Is there a specific reason for not including mixed precision results in the table?\n\n4. In Table 4, the epoch duration remains consistent, but there is a variance in training cost. Can you clarify the specific factor or factors that account for this difference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781754258,
        "cdate": 1698781754258,
        "tmdate": 1699636884741,
        "mdate": 1699636884741,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PdpNe7iqdS",
        "forum": "g7YB6K2eXj",
        "replyto": "g7YB6K2eXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_GakW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_GakW"
        ],
        "content": {
            "summary": {
                "value": "A multi-bit quantization framework (Double Quantization) is proposed, which quantizes a pre-trained model for once and enables inference with different pre-defined bit-width. To help convergence, a Multi-LR method is introduced to use seperate learning rate for each bit-width. Mixed-precision is also studied."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper deals with an important problem of network quantization, i.e., the multi-bit quantization problem. The proposed Multi-LR method seems to be useful for stable training. Experiments with various bit-widths and mix-precision results are provided."
            },
            "weaknesses": {
                "value": "The proposed multi-bit quantization framework consists of three main parts, i.e., the Double Rounding quantization scheme, the Multi-LR learning rate selection method, and the Weighted Probability mixed-precision method. However, these improvements seems to be a little bit incremental.\n- I didn't see the difference between the double rounding quantization and adabit quantization. The adabit quantization can also represent with [\u22121, 1] but not limited to [0, 1]. \n- The multi-lr method is a hyper-parameter tuning, which is more like a tuning trick to me.\n- Both mixed-precision quantization and mixed-precision based on multi-bit quantization have been widely studied in previous works. \n\nThe improvements over Adabit are not quite significant if KD is not used."
            },
            "questions": {
                "value": "Does all baseline methods use the same pre-trained model in Table-1? The full-precision baseline accuracy should be reported."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828675264,
        "cdate": 1698828675264,
        "tmdate": 1699636884584,
        "mdate": 1699636884584,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4JEJXQQw1K",
        "forum": "g7YB6K2eXj",
        "replyto": "g7YB6K2eXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_mFBC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7387/Reviewer_mFBC"
        ],
        "content": {
            "summary": {
                "value": "The authors present a novel approach to mixed-precision quantization, allowing for post-training bit-width selection. This method uses quantization-aware training, with the central concept being the training of a model at the highest permitted bit-width and obtaining lower precision representations through bit shifting. The authors introduce a double rounding technique that allows switching between high and low precision configurations without necessitating retraining. To address the challenges associated with simultaneously training a model for varying bit-widths, the authors advocate the use of distinct learning rates for quantization scaling parameters across different configurations, where fewer bits correspond to a smaller learning rate. The proposed method builds upon the Bit-Mixer framework (Bulat & Tzimiropoulos, 2021) with the following key differences that enhance its performance:\n1. The incorporation of double rounding for efficient switching between low and high precision via bit shifting.\n2. The utilization of the trace of the Hessian information during the training phase to determine the bit precision for each layer separately, with lower trace values indicating lower precision.\n3. The application of different learning rates for each bit-width configuration to mitigate training instability.\n4. The use of probabilities that align with Hessian information instead of employing uniform probabilities.\n5. The adoption of an Integer Linear Programming (ILP) approach to determine the optimal configuration while adhering to specified constraints (e.g., FLOPs, storage).\n\nEmpirical validation on various models applied to ImageNet and CIFAR-10 datasets demonstrates the superior accuracy achieved by the proposed algorithm while using fewer or equivalent bit-widths."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method allows for training models capable of dynamically adjusting their precision levels, offering adaptability for deployment on diverse edge devices.\n2. Leveraging the Hessian information of each layer during bit-width assignment in the training phase enables an estimation of the number of bits required for each layer."
            },
            "weaknesses": {
                "value": "1. The introduction of additional $\\mathcal{O}(n^2)$ batch normalization layers, although a minor concern, should be noted, as it may lead to additional storage costs. Nonetheless, it's worth highlighting that the size of batch normalization layers is typically smaller than that of Linear or Convolutional layers.\n2. The paper could benefit from more detailed explanations of key techniques, such as the use of the Hessian trace, the precise formulation of the ILP problem, and the weighted probability method.\n3. The use of Integer Linear Programming (ILP) to find the optimal bit-width configuration may be computationally intensive due to the NP-completeness of the problem. Depending on the problem size, achieving convergence to the optimal configuration may require a substantial amount of time."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7387/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7387/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7387/Reviewer_mFBC"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7387/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829127916,
        "cdate": 1698829127916,
        "tmdate": 1699636884439,
        "mdate": 1699636884439,
        "license": "CC BY 4.0",
        "version": 2
    }
]