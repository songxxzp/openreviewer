[
    {
        "id": "smKeJoJAdN",
        "forum": "KXOB15k1br",
        "replyto": "KXOB15k1br",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_dM8C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_dM8C"
        ],
        "content": {
            "summary": {
                "value": "I have already reviewed this paper for another conference and I could not find substantial changes in the paper and/or presented method. Hence, I will repeat my summary and some of my main points of the other review. I would kindly ask the authors to point out if I missed substantial updates of the paper.\n\nThis paper introduces a data augmentation method for time series forecasting (specifically long-term forecasting). The proposed method starts from a partially trained base model and then searches over a given augmentation space for a good augmentation policy with Bayesian optimization. The method alternates between policy selection (using the Tree-Structured Parzen Estimator) and fine tuning the models with Asynchronous Successive Halving to reduce the required computation. The authors evaluate the method on multivariate and univariate forecasting tasks using several transformer variants and N-BEATS and find that the proposed method reduces the MSE/MAE over these base models (and reduces the error further than random augmentation). In addition, the authors present which policies have been selected by their method and perform an analysis on the impact of the selected hyperparameters of their model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality \n\nThe authors develop an automated method for data augmentation policy selection for time series forecasting. While the individual components (augmentations, the Bayesian Optimization procedure) are not novel, the combination and application to time series forecasting has not been proposed before (to my knowledge). The application of Bayesian Optimization to select policies has been proposed before (Zhang, Cui, Yang 2019, https://arxiv.org/abs/1905.02610).\n\n### Quality \n\nThe paper analyses several baselines and datasets in the univariate and multivariate setting. The authors also provide analysis on parameter choices and the selected policies for their method, which is interesting. I think the evaluation has some major weaknesses, which I will discuss in the Weaknesses section.\n\n### Significance \n\nThe proposed method is an interesting finding for time series forecasting, but the significance for adjacent fields is limited because the authors do not develop any novel technique but rather adapt existing techniques for time series forecasting."
            },
            "weaknesses": {
                "value": "One weakness of the paper is the choice of baselines. One interesting observation is that the largest error reductions are observed for the Exchange and Weather datasets. Given that Exchange is a dataset from the finance domain it likely shows a random-walk like behaviour. Weather has 10 minute recordings of meteorological measurements, so I assume that this data likely has a large random component because the forecasted horizon is probably too small to capture seasonality over the year (5 days for the 720 time step horizon (720/(24*6))=5). This is in contrast to the small improvements observed for the other datasets that have a strong seasonality component (like Electricity, Traffic, and ILI). Thus, I\u2019m wondering whether the main impact of the data augmentation mostly improves the forecasts for random-walk data and how this compares to baselines that are more suitable for this kind of data. I would kindly ask to the authors to include baselines in their evaluation that are suited for random-walk data. Two suggestions are the naive method proposed in Bergmeir et al., KDD 2022 (https://link.springer.com/article/10.1007/s10618-022-00894-5) and exponential smoothing. For completeness, it would be interesting for the reader to understand what the gain is of using complex transformer models plus sophisticated data augmentation relative to rather simple baselines for seasonal data. Comparing to DHR-ARIMA (also mentioned in that paper) for seasonal methods would be interesting (or D-Linear, which the authors cite in their introduction, seasonal naive for a baseline for seasonal time series). \n\nGiven that some of the improvements are small, I would suggest a different way of presenting the data that includes a significant test and takes the variance of re-running the models with different seeds into account. I suggest to use critical diagrams for that (Dem\u0161ar, JMLR 2006, https://jmlr.org/papers/v7/demsar06a.html). This would make it easier for the reader to judge the significance of the error reduction that is achieved here."
            },
            "questions": {
                "value": "These are my main questions as explained in the Weaknesses section: \n\nHow do simple baselines suitable for random walks (naive method and exponential smoothing) compare in this evaluation? \n\nWhat is the significance of the improvements of TSAA when presenting the results with a critical diagram that includes a statistical test (Dem\u0161ar, JMLR 2006, https://jmlr.org/papers/v7/demsar06a.html)? \n\nI would consider increasing my score if these questions are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658463177,
        "cdate": 1698658463177,
        "tmdate": 1699636068469,
        "mdate": 1699636068469,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zlTpcSOpHM",
        "forum": "KXOB15k1br",
        "replyto": "KXOB15k1br",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_dahn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_dahn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an automatic time series augmentation (TSAA) search approach formulated as a bilevel optimization problem. TSAA uses Tree-structured Parzen Estimator with Expected Improvement as a surrogate model to search for promising augmentation strategies. To avoid training from scratch each time given an augmentation policy, TSAA finetunes from a set of shared weights which is got from training without augmentation. Further, TSAA uses Asynchronous Successive Halving to discard unpromising runs to improve the optimization efficiency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Data augmentation is much less explored for time series problems than other data modalities. This work presents a meaningful way to automatically search for useful augmentation strategies on different datasets.\n2. The writing is clear and easy to follow with good visualizations."
            },
            "weaknesses": {
                "value": "1. From Table 6-14, it can be observed that under a lot of settings, TSAA even makes the performance much worse than the baseline, especially for some specific datasets, e.g. Exchange. I did not find the interpretation of such results, which makes it a bit hard for the community to understand when TSAA could fail.\n2. TSAA is mainly evaluated on Transformer-based models. As a data augmentation approach, we would expect it to be agnostic to the model architecture. The effectiveness of TSAA on more recent MLP-based architecture [1] or convolution-based architecture [2] is not shown.\n3. Overall, TSAA seems to be an application of multiple well-established approaches (e.g., TPE estimator and ASHA) in hyperparameter optimization and time series augmentation. While these approaches were not adapted to time-series modality before, I did not find enough takeaway insights from applying these approaches to time-series data.\n\n[1] Are Transformers Effective for Time Series Forecasting? AAAI 2023\n[2] TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. ICLR 2023"
            },
            "questions": {
                "value": "1. In Figure 4, Flip and Seasonality Downscale is used for NBEATS-G. Is there some way to understand why prediction + TSAA fixes the underestimated prediction in this case? \n2. It is claimed that Trend Downscale is frequently used for multiple datasets, which is probably because deep models tend to overestimate. Based on such a claim, I would expect to see some examples in Figure 4.\n3. What is the compute time to apply TSAA compared with vanilla training? It seems that the first half of training time remains the same (since $\\beta=0.5$), but the second half would involve an expensive search process and significantly increase the compute time of training.\n4. How do interpret the performance degradation on a specific dataset, e.g. Exchange?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1405/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1405/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1405/Reviewer_dahn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797814094,
        "cdate": 1698797814094,
        "tmdate": 1699636068382,
        "mdate": 1699636068382,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w8wTZ7E9gI",
        "forum": "KXOB15k1br",
        "replyto": "KXOB15k1br",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_E2oo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_E2oo"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a method called Time-Series AutoAugment (TSAA) that efficiently searches for an optimal data augmentation policy. The method involves a two-step process: a partial train of the non-augmented model followed by an iterative split process.The iterative process alternates between finding a good augmentation policy using Bayesian optimization and fine-tuning the model while pruning poor runs. The authors evaluate TSAA on challenging univariate and multivariate forecasting benchmark problems and show that it outperforms several strong baselines in most cases. \nThe main contributions of the work are three-fold:\n1.Development of a novel automatic data augmentation approach for long-term time-series forecasting tasks\n2.Analysis of the optimal policies found by the approach\n3.Extensive evaluation on TSAA and benchmark problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.Originality:While data augmentation techniques are commonly used in vision tasks, their application to time-series forecasting is relatively new. TSAA proposes a novel automatic DA method for TS data using a set of time-series transformations that manipulate certain features of the data while leaving others unchanged.This approach fills a gap in the field of time-series forecasting.\n2.Quality:The paper demonstrates the effectiveness of TSAA through extensive experiments on multiple datasets and models. The authors compare TSAA with several baseline models and other automatic augmentation methods, providing a comprehensive evaluation of its performance. The experiments are conducted with multiple seed numbers on the same system and the results are averaged, ensuring robustness and reliability. \n3.Clarity:The paper is well-written and organized, making it easy to follow the proposed approach and experimental results. The authors provide detailed explanations of the time-series transformations used in TSAA and the overview of TPE and ASHA ensuring a clear understanding of the augmentation process. The link between the task of automatic augmentation and a bi-level optimization makes the two-step process of TSAA better understood.\n4.Significance:The paper addresses the challenge of data augmentation in time-series forecasting.By proposing TSAA, the authors offer a practical solution to improve the performance of time-series forecasting models. The extensive experiments demonstrate the effectiveness of TSAA in improving forecasting accuracy across multiple datasets and models. The findings of this research have the potential to benefit researchers and practitioners working on TSF tasks."
            },
            "weaknesses": {
                "value": "1.The paper does not extensively discuss the limitations of TSAA or potential challenges in its application. It would be beneficial to address any constraints or assumptions made in the proposed approach, as well as potential scenarios where TSAA may not be as effective.\n2.The paper does not further explain the relationship of the dataset to its selected transformations.For instance,in the univariate time-series forecasting results,why Exchange need more Upscale but ECL need more Mixup?"
            },
            "questions": {
                "value": "1.In the 5.2 section, we can see TSAA yeild stronger performance in the long-horizon settings.Can you give a more clear analysis and explanation for this experimental phenomenon?\n2.In Table.1 and Table.2, some datasets can be highly enhance such as Weather,but some datasets yield very little gain such as ETTm2.How does this difference come about and is it due to the dataset? Or is TSAA a weekness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823251443,
        "cdate": 1698823251443,
        "tmdate": 1699636068277,
        "mdate": 1699636068277,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fpnILW8Did",
        "forum": "KXOB15k1br",
        "replyto": "KXOB15k1br",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_9xnW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1405/Reviewer_9xnW"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an automatic data augmentation approach for time series forecasting. The main idea is to use bi-level optimization to learn weights as an augmentation policy from a designed dictionary of time-series transformations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "S1. The paper covers both univariate and multivariate time series forecasting tasks.\n\nS2. Table 3 explores various time series transformations."
            },
            "weaknesses": {
                "value": "W1. The novelty of the proposed auto-augmented framework is not enough as it has been studied in image domains (Bayesian optimization [1] or bilevel/meta-learning methods).\n>-  [1] 2019 Learning Optimal Data Augmentation Policies via Bayesian Optimization for Image Classification Tasks \n    \nW2. The paper's investigation of time series data augmentation is limited, as it does not fully explore existing learnable augmentation strategies, such as generative augmentation [1,2], adaptive weighting schemes [3], and gating-based selection [4]. These approaches are not discussed or compared in the paper.\n>- [1] Two Birds with One Stone: Series Saliency for Accurate and Interpretable Multivariate Time Series Forecasting\n>-  [2]  TSA-GAN: A Robust Generative Adversarial Networks for Time Series Augmentation\n>-  [3] Adaptive Weighting Scheme for Automatic Time-Series Data Augmentation \n>-  [4] 2022 Dynamic Data Augmentation with Gating Networks for Time Series Recognition\n\nW3. I believe that data augmentation should be a general technique applicable to various domains/datasets/tasks, and thus, the paper's claims that less attention has been given to automatic augmentation of time-series problems, such as long-term forecasting, may not be entirely convincing. To strengthen the paper's argument, it would be beneficial to evaluate the proposed method on a wider range of time series tasks. Additionally, it is unclear how the optimal augmentation policy for forecasting differs from that for classification, and further clarification on this point would be helpful.\n\nW4. Table 1 and Table 2 are misleading. It is unclear what base networks are trained with the auto-augment method (the col of TSAA). Moreover, it would be better to compare existing time series auto-augmentation methods (on various base ts models) in the experiment section.  \n\nW5. Table 5 in the Appendix compares only Fast AA and RandAugment, and I noticed that Fast AA performs significantly worse than the baseline on ETTm2, Traffic, and ILI. Could you provide some insights into the possible reasons for this difference?\n\nW6. Unlike image transformations, selecting appropriate hyper-parameter ranges for time series transformations can be challenging. As demonstrated in Figure 5, many of the augmented samples are significantly different from the original time series. This raises the question of how to construct a dictionary of time-series transformations that is suitable for various tasks. Could you elaborate on this issue and provide more insights on how to design such a dictionary?\n\nW7. Given that the data will be transformed and passed through the network multiple times (equivalent to the number of transformations), is there a significant increase in computational cost associated with this approach?"
            },
            "questions": {
                "value": "Please see my questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1405/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699069039432,
        "cdate": 1699069039432,
        "tmdate": 1699636068177,
        "mdate": 1699636068177,
        "license": "CC BY 4.0",
        "version": 2
    }
]