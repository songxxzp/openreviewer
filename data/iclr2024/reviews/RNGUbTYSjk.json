[
    {
        "id": "ZS9EZnaXfF",
        "forum": "RNGUbTYSjk",
        "replyto": "RNGUbTYSjk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_ii6f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_ii6f"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the weak Minty variational inequality for unconstrained problems and shows that the range for the problem constant $\\rho \\in (-1/(2L), \\infty)$ can be increase to $\\rho \\in (-(1-1/e)/L, \\infty)$ for an explicit scheme. They achieve this by modifying an extragradient method to compute the extrapolated point using _repeated_ applications of the forward operator. They first generalizing the hyperplane projection approached used in Pethick et al. 2022 to an abstract condition on the extrapolated point. They then show tight conditions on $\\rho$ when the extrapolated point is generated by two steps of GDA with time-invariant stepsize. To extending to $\\rho < -(1-1/e)/L$ they consider $n>2$ inner steps under the restriction that the sum of stepsizes $\\sum_i \\gamma_i < 1/L$. They finally propose a method which heuristically attempt to maximize the projection distance in order to relax the requirement on $\\rho$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper introduces a very nice idea and does so in an approachable way. It is interesting that an explicit scheme can extend the range of $\\rho$ and that this is achievable by such a simple and elegant approach. The work opens up exploration of other subroutines."
            },
            "weaknesses": {
                "value": "I definitely recommend accept. I only have one concern regarding Algorithm 1 and otherwise some suggestions for improving the content/writing.\n\n- It seems problematic that the max distance algorithm (Algorithm 1) has worse complexity when the operator $F$ has _more_ structure (e.g. when cocoercive such that the subroutine itself is sufficient for convergence). In other words, it seems that the scheme is trading off complexity in easier problem for a convergence in a larger class. What is the observed oracle complexity if you run the algorithm on a strongly monotone problem or cocoercive (for which the subroutine convergence) in comparison with e.g. EG+? (e.g. take $f(x,y)=x^2 - y^2 + xy$)\n\nSome suggestions:\n\n- I would state the minimal $n$ that achieves the $\\rho_0$ in Thm. 3.4. This way you can also specify the oracle complexity.\n- Lemma 2.4 is very related to [Solodov and Svaiter 1999](https://www.emis.de/journals/JCA/vol.6_no.1/j149.pdf) Lemma 2.1 which might be worth mentioning\n- Maybe make it explicit that the multiple steps for the extrapolation steps are not \"anchored\" to approximate a prox ala [Nemirovski 2004, page 3](https://www2.isye.gatech.edu/~nemirovs/SIOPT_042562-1.pdf)\n- Concerning $\\rho < -1/L$ threshold maybe mention [Bauschke et al. 2019, e.g. Table 1](https://arxiv.org/pdf/1902.09827.pdf) characterization for cohypomonotone problems.\n- Page 2 (first paragraph): It is stated that a better bound is derived in the 2-step case, but this seems not to be the case $\\rho > -(1-1/e)/L$ is looser than $\\rho > -0.5834/L$.\n- \"common practice is to choose $\\sigma_k$ near its lower bound\": There is a trade-off (lower bound yields weaker conditions on $\\rho$, but higher complexity, so only recommended if guaranteed convergence for the largest class possible is important)\n- The discussion after Thm. 3.4 is confusing to me:\n    - The lower bound could still be tight for time-invariant stepsize (as Thm. 3.4 is concerned with)\n    - The numerical evaluation only _suggests_ that improving the lower bound is possible if time varying stepsize is allowed. Stating that a counterexample proofs something in this setting can be misleading (especially in the context of the word \"lower bound\" being used in a slightly unconventional way).\n- Maybe use \"range of $\\rho$\" instead of lower bound where possible to avoid conflation with negative results (e.g. after Thm. 3.4)\n\nConcerning the writing:\n\n- A lot of whitespace\n    - To compress page 6 you could consider writing the inner loop of (n-step EG) with $\\forall i \\in [n]$ in one line.\n- Unspecified in places (e.g. section 2.1 should define $\\alpha_k$ and $\\gamma_k$, \"recommended parameter\" requires specification, etc)\n- Before section 2 (Preliminaries) the $\\rho > -1/L$ should only be a $\\rho > -1/(2L)$.\n- Assumption 2 is missing $z \\in \\mathbb R^d$\n- page 4 \"[...] does not necessarily have to be attained by a _single_ forward operator evaluation\"\n- $\\bar \\alpha_k$ is sometimes used for a pre-defined (nonadaptive) stepsize in the literature, but it is up to you what convention you stick to."
            },
            "questions": {
                "value": "- Eq. 3.6 (and consequently Thm 3.2) is tight for time invariant stepsize? Since this would be a stronger claim I would state it explicitly, and contrast it with e.g. Thm. 3.4.\n- Why define $z^0 = z^{init}$?\n- Is it possible to prove anything for Algorithm 1?\n- After Algorithm 1 it is suggested to use the stepsize choice of n-step EG, but how do you guarantee $\\sum_i \\gamma_i < 1/L$ in Algorithm 1 if the horizon is not known? (since the horizon is adaptively chosen by the max distance stopping criterion)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2999/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751580703,
        "cdate": 1698751580703,
        "tmdate": 1699636244380,
        "mdate": 1699636244380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OEq3sH3owf",
        "forum": "RNGUbTYSjk",
        "replyto": "RNGUbTYSjk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_xeFY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_xeFY"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the AdaptiveEG+ proposed in [1] and studies the extragradient-type algorithms that take multiple intermediate steps for the unconstrained weak MVI problem, aiming to relax the weak MVI condition. Their analysis essentially relies on the condition that the iteration point and the solution point can be separated by the border hyperplane defined by the weak MVI condition w.r.t. the iteration point. In detail, for the iterate $z_k$, the authors defines the adaptive stepsize for computing $z_{k + 1}$ to be $\\lambda_k(\\rho \\|F(z_k)\\|^2 - \\langle F(z_k), z_k - \\bar z_k\\rangle)/\\|F(\\bar z_k)\\|^2$ with $\\lambda_k \\in (0, 2)$ and certain intermediate iterate $\\bar z_{k}$ (not necessarily obtained by one GD step), which requires $\\bar z_k$ to satisfy the condition that $\\rho \\|F(z_k)\\|^2 > \\langle F(z_k), z_k - \\bar z_k\\rangle$, assuming that the operator $F$ satisfies weak MVI condition with (negative) parameter $\\rho$. To guarantee this, the authors derives certain conditions on the stepsizes of intermediate step for multi-step extragradient, which leads to wider range on weak MVI condition parameter, roughly $\\rho > -\\frac{1 - 1/e}{L}$ (not tight), and improves upon the previously best known range $\\rho > -1/2L$. Their findings are further supported by preliminary numerical results. \n\n\n[1] Thomas Pethick, Panagiotis Patrinos, Olivier Fercoq, Volkan Cevhera, et al. Escaping limit cycles: Global convergence for constrained nonconvex-nonconcave minimax problems. In International Conference on Learning Representations, 2022."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors provides an intuitive generalization of AdaptiveEG+ from the perspective of weak MVI halfspaces. As long as one can find some intermediate iterate $\\bar z_k$ such that $\\rho \\|F(z_k)\\|^2 > \\langle F(z_k), z_k - \\bar z_k\\rangle$, the algorithm framework (2.9) are proved to converge with rate $O(1/k)$ for the average guarantee. An adaptive exploration scheme to find such $\\bar z_k$ is also given in Algorithm 1.\n2. This paper gives a quite comprehensive analysis and discussion on the stepsize requirements for intermediate steps with connection to the restriction on the parameter of the weak MVI condition, relaxing the previous contraints $\\rho > -1/2L$ to roughly $\\rho > -(1 - 1/e)/ L$."
            },
            "weaknesses": {
                "value": "I do appreciate the discussion and analysis for the more general multi-step extragradient. However, I note that the convergence rate remains the same (see Thm 2.5 in the paper) as AdaptiveEG+[1] in prior works, and the wider range of $\\rho$ only provides slight (constant) improvements. One may argue that although the rate is the same, MDEG and multi-step EG can solve for problems in more nonmonotonic regions. However, the numerical experiment as in Figure 4 shows that CurvatureEG+[1] can actually perform similarly as MDEG proposed in this paper. All of these limit the contribution of the paper."
            },
            "questions": {
                "value": "1. Could the authors further explain the derivation for the inequality in (3.1)?\n2. Besides the adaptive exploration proposed in Algorithm 1, I am curious for $n$-step EG whether it is expected to get better range on $\\rho$ with larger $n$, or it is sufficient to only take $n = 3$ with proper intermediate stepsizes for better range on $\\rho$? Based on the current (non-tight for $n \\geq 3$) analysis in the paper, one can get better range on $\\rho$ with 3 steps than taking $n$ steps with $n$ tends to infinity (in which case we get $\\rho > -(1 - 1/e)/L$)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2999/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2999/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2999/Reviewer_xeFY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2999/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779128098,
        "cdate": 1698779128098,
        "tmdate": 1699636244289,
        "mdate": 1699636244289,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mUugfpvufJ",
        "forum": "RNGUbTYSjk",
        "replyto": "RNGUbTYSjk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_mxmH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_mxmH"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the weak MVI condition and proposes algorithms that allow the condition to be weaker than what was analyzed before. In particular, the main idea is that by using algorithms with multiple exploration steps, one can further relax prior used assumptions. In addition, the authors design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written, and the idea is easy to follow. The authors did a great job in the introduction of motivating the problem, explaining their approach, and clearly stating their main contributions."
            },
            "weaknesses": {
                "value": "However, I believe the paper has several issues regarding the presentation of the sections after the Introduction that make the reader's life more challenging.  The narrative around the statements of the theorems and the figures in the experiments section is not clear. In general i find that the main part of the paper has several holes and requires an effort from the reader to connect the dots.\n\nLet me provide some details below:\n\n### Some Basic Issues: \n\n1)  Figure 1 is included in the paper without reference to it at any part of the paper. What is its purpose? Can the authors elaborate more? The figure mentions \"range of gradient: while the definitions are related to operators. Does this hold only for the specific operator at the end of page 2 (saddle gradient operator)? Why is this figure helpful for the reader?  In several parts of the paper, the authors refer to the \"gradient\". If the results hold for general operators, it would be beneficial to be mentioned. \n\n2)  What is the $S^c$ in equation 2.6? \n\n3) Proof of Lemma 2.3 is missing (even if it is trivial the authors should include it for completeness).\n\n4) The presentation of Lemma 2.4, its importance, and the discussion after the results should be improved as this is a vital part of the understanding of the paper. I would suggest the authors include more information and explain the motivation behind it.  For example, what is the purpose of parameter $\\lambda_k$ here (why is not simply equal to 1)? Also, how does algorithm 2.9 satisfy the assumptions of Lemma 2.4? More explanation is needed.\n\n5) in section 3 the authors mentioned, \"The key is to calculate stepsize from projection and assure its positiveness.\" I get the statement but then how is that related to what they have as equation (3.1)? important connections are missing. Why is that projection? \n\n### More Serious Presentation Issues in the main part of the paper : \n\n6)  Sections 3.1 and 3.2 are focused on the algorithms and statements of the theorems. However, there is no explanation of what the results mean and how they are positioned in the literature. What are the benefits of the final statements? As a reader, I find it impossible to understand why for example, Theorem 3.2 is useful and what it means. The whole section 3 and in particular, pages 5 and 6 include several theorems one behind the other without a proper presentation and paragraphs explaining in more detail the main outcome. \n\n7) Section 4 is devoted to the Max Distance Extragradient where the extrapolation process will not stop until the projection distance stops increasing. This is interesting, but an intuitive understanding of what this means is missing from the main paper. The Discussion from Appendix B.1 and Figure 6 should have been part of the main paper. \n\n8) Important: An indication of the lack of proper presentation is the following. There is no clear explanation in the main paper of why by taking multiple extrapolation steps, one can have a weaker MVI. This is the main selling point of the work, but all the discussion of this is in the appendix. By simply reading the main paper, this is not clear.\n\n9) Unfortunately, the same pattern happening in the experiments section as well. The problems are presented, but an in-depth comparison of the results and paragraphs focusing on the output of the proposed methods and how they are compared to other algorithms is missing. How do the experiments correspond to the main theorems? A full page is devoted only to the plots (Figures 3-5) but without proper presentation and connection to the theoretical results.\n \n### On related work\n\n9) Finally, I believe the cited work of Choudhury et al. 2023 includes convergence of the deterministic optimistic methods for solving weak Minty inequalities with $\\rho>-1/2L$. It might be worth comparing with the Optimistic method as well in experiments and in the comparison of theory in the main paper."
            },
            "questions": {
                "value": "See Weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2999/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698989933252,
        "cdate": 1698989933252,
        "tmdate": 1699636244203,
        "mdate": 1699636244203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "elvXEIYyOj",
        "forum": "RNGUbTYSjk",
        "replyto": "RNGUbTYSjk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_xJXM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2999/Reviewer_xJXM"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on solving structured, non-monotone variational inequality problems that satisfy the weak Minty inequality (eq. 2.3). The proposed approach is based on multi-step methods (per iteration), where multiple evaluations of the operator are computed at consecutive points extrapolated through an iterative subroutine at each parameter update. \nIt is demonstrated that an extragradient method with two or finite extrapolation steps can solve this problem, provided that the problem's parameters are known. Additionally, a variant is proposed that adaptively determines the number of extrapolation steps required, relying on knowledge of a parameter of the assumed problem structure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper addresses a challenging problem, establishes convergence guarantees in a unifying way, and provides some intuition. The first method slightly modifies existing ones, whereas the second one (Alg. 1) builds on the theoretical results."
            },
            "weaknesses": {
                "value": "*The guarantee requires knowing $\\rho$*. \nTo ensure convergence, it is necessary to know the $\\rho$ problem parameter. However, this presents a challenge as the convergence guarantee depends on it. Although the authors suggest ways to estimate it, it remains uncertain if the proposed approach will converge when using a noisy estimate. This limitation also applies to Alg. 1, which also requires knowledge of $\\rho$ to function correctly.\n\n\n\n*The presentation of the theorems*. \nThe theorems in the second part are presented in a very convoluted manner, making it difficult to interpret the added parameters. Some of the included assumptions are not evident from the statements, which further adds to the confusion. After carefully re-reading the second part, it is still unclear what the precise assumptions, convergence rate, and convergence measure are.\n\n\n*Writing.*\nThe writeup needs further refinement. See the non-exhaustive examples below (listed as Minor).\n\n\n*Additional practical concerns.*\nThe studied methods involve a variable number of inner steps, which makes it more complex than the methods discussed in this paper. As a result, I am uncertain whether it can be applied to a wider range of problem classes. However, it is reassuring that it offers guarantees for the specific problem class addressed in this paper.\n\n\n\n# Other\n\n- Many of the figures are not referenced, making it unclear when the reader should consult them. Moreover, these are not explained, for example, it would be helpful to describe how Fig. 2 helps understand the theorems.\n- I wouldn't classify it as a *more than local* method (page 1) because the information is still very much local, but it's applied to multiple points, which has obvious computational disadvantages.\n- $\\rho$ should be listed as input to Alg. 1.\n\n\n# Minor \n\n\n\n- The abstract has numerous typos, for example:\n   - Min-max problem -> The min-max problem\n   -  application in ML ->  applications in ML\n   - exhaustive researches -> exhaustive research\n   - problem on nonconvex-nonconcave setting -> problem of nonconvex-nonconcave setting \n   -  is proved -> has proved\n   - process exploit  [...] tackle -> process exploits [...] tackles\n\n- Introduction:\n   - $\\rho, L$ are mentioned without introducing them. It would be helpful if at least (2.3) is moved in the introduction and if it is mentioned that $L$ is the Lipschitz constant\n   - typo: algorithm that exploit -> exploits\n   - similarly, it was unclear what is $e$ at the beginning of page 2\n   - In the first paragraph of page 2, since the comment *this lower bound (on $\\rho$) is not necessarily tight* is for two cases (1) when the sum of the step sizes at one update step is smaller than $1/L$ and (2) when that is not the case, it remains very unclear if it is tight for the case (1), because that comment states *that does not meet the step size restriction*. In other words, this statement is problematic because two different cases are being compared while claiming that the bound for one case is not satisfied.\n    - The second paragraph on page 2 also assumes that the reader is familiar with the mentioned idea; otherwise, it is unclear why *this algorithm tackles cyclic behaviors*. Moreover, that is also the case for standard extragradient, so it is unclear what the point is here.\n- In Fig. 1, it would be helpful to annotate $L$ constant.\n- page 3: instead of the *objective function* $F$, it is better to use *vector field* or *operator*\n- page 3: our methods [...] is -> are\n- page 3: described next section, missing *in*\n- ..."
            },
            "questions": {
                "value": "1. On page 4, you mentioned that the lower bound can be represented by parameters in $G_k$. However, I am unsure how convergence can be proven if $\\rho$ is estimated as commented therein. Could you please elaborate on this?  \n2. In Thm. 2.5 what is the assumption on $\\rho_0$?\n3. Could you describe in more detail how Fig. 2 was obtained? (are $L,\\rho$ from a specific problem?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2999/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699326612589,
        "cdate": 1699326612589,
        "tmdate": 1699636244127,
        "mdate": 1699636244127,
        "license": "CC BY 4.0",
        "version": 2
    }
]