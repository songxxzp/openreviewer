[
    {
        "id": "YJC8r78ZzP",
        "forum": "vESNKdEMGp",
        "replyto": "vESNKdEMGp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_WaDP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_WaDP"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors identify some of the challenges regarding the presence of multilingual jailbreak in two different scenarios: intentional and unintentional. Moreover, they introduce a multilingual jailbreak dataset, analyze the effects of language as a jailbreak method, and show that medium and low-resource languages are more likely to generate unsafe content compared to high-resource ones. Finally, they propose a framework called SELF-DEFENCE, which starts from a set of English seed input-output pairs, including both unsafe and general examples, and augments the dataset using the LLM and these seed examples. Then, it uses the LLM to translate the instruction pairs into multiple target languages and merges these language-specific corpora to form the final training dataset for fine-tuning. Finally, the authors use the fine-tuning access for ChatGPT to evaluate the effectiveness of their framework. Their results show improvements in the multilingual safety capabilities of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors introduce a multilingual jailbreak dataset called MultiJail, which they claim is the first multilingual jailbreak dataset. The process involves incorporating native speakers for human translations in order to prevent noisy translation.\n- Section 3 provides a detailed evaluation of the multilingual safety challenges of ChatGPT and GPT-4 and provides insight into the effects of language in two cases, intentional and unintentional."
            },
            "weaknesses": {
                "value": "- The data generated for finetuning is mostly generated by the LLM itself (except for the small number of seed examples). While this makes the data generation easier and cheaper, I assume the translations by the LLM for the low-resource languages are still very noisy and might be the reason why usefulness reduces significantly as the safeness metric increases. The authors mention that a potential reason for this decrease in general capability can be attributed to the fact that LLM rejects to answer unsafe questions. In that case, it would be useful to have a numerical comparison between the cases that get low usefulness scores because of this issue and the cases that fail due to noisy translations. As the authors mention, it would be useful to include a brief explanation of why the question is unsafe to ensure the problem is not, in fact, due to noisy translations of the low-resource language by the LLM.\n- In my opinion, the SELF-DEFENCE framework lacks novelty. It simply implies that providing more training samples for lower resource languages can improve LLM's understanding of them, and as we include samples translated to these languages during the fine-tuning stage, the model becomes safer with respect to them.\n- There is no information on the effectiveness of the SELF-DEFENCE framework compared to any other safety improvement techniques mentioned in the related work section."
            },
            "questions": {
                "value": "- How does the SELF-DEFENCE framework perform compared to other safety improvement methods, such as RLHF?\n- For low-resource languages, where the LLM's understanding of the language is limited, how reliable are their LLM-generated translations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Reviewer_WaDP",
                    "ICLR.cc/2024/Conference/Submission9250/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776529810,
        "cdate": 1698776529810,
        "tmdate": 1700722063609,
        "mdate": 1700722063609,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gosrHR23sE",
        "forum": "vESNKdEMGp",
        "replyto": "vESNKdEMGp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_imbS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_imbS"
        ],
        "content": {
            "summary": {
                "value": "The paper shows that the safeguard ChatGPT and GPT-4 are not robust for non-English languages, especially low resource languages. By translating curated harmful prompts in English and translating them into 9 other languages, the paper shows the increase of unsafe rates in the responses of LLMs. To this end, the paper proposes a self-defence method that generates finetuning data for chatGPT and a tuned version of chatGPT on this data exhibits a lower unsafe rate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The methodology of collecting harmful prompts, running human translation, and using human evaluation for the 15 harmful prompts in the preliminary results. I also like the approach of using jailbreakchat.com for collecting intentional jailbreak prompts.\n\n\n- Overall, the paper is well motivated and covers interesting analysis such as the translation quality, jailbreak languages, the trade off between safeness and usefulness."
            },
            "weaknesses": {
                "value": "- Lack of details of the self-defence algorithm. What are the seed samples and what is the percentage of unsafe samples generated by chatGPT that are actually unsafe if it is prompted to chatGPT to get the answer?\n\n- How wide-coverage in terms of topics, scenarios,...of the data that the self-defence algorithm can generate? While it\u2019s good to show some improvement when evaluated on \tMultiJail testset, I think self-defence just scratches the surface of the problem. \t\n\n- I generally think it\u2019s important to ensure safety for all the languages but it irks me about the argument about Bengali at the end of page 5th. Even though there are 285 million native speakers, how many of those are using chatGPT and could be affected by unsafe outputs?"
            },
            "questions": {
                "value": "See the question in the above section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785840364,
        "cdate": 1698785840364,
        "tmdate": 1699637164785,
        "mdate": 1699637164785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iIJgtXUerM",
        "forum": "vESNKdEMGp",
        "replyto": "vESNKdEMGp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_QBvZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_QBvZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors explore how LLM safety generalizes across languages.  They translate a set of unsafe prompts across multiple languages, finding that ChatGPT and GPT-4 safety metrics degrade by language prevalence and that the models are also more vulnerable to a jailbreak when combining languages.  They propose additional fine-tuning data to improve safety across these languages, finding that this fine-tuning improves safety but comes with a cost to usefulness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The question of safety across many languages and generalization of LLMs across languages is of significant importance to this technology.\n\nS2. There has been some but not a lot of work so far doing a thorough evaluation of LLM safety across languages, making this evaluation valuable for the community.\n\nS3. The paper studies this question from numerous angles - overall safety performance, jailbreaks, amount of data in CommonCrawl, mitigation through fine-tuning, trade-off with usefulness."
            },
            "weaknesses": {
                "value": "W1. I think for the study of safety in other languages, it should not be framed as a jailbreak.  Simply ensuring the model performs safely for people who speak those languages is important on its own.  In some places this acknowledged but I'd make it more clear early on and consistent.\n\nW2. Small sample size and error bars - samples are generally quite small (15-30 samples per language in some cases; only testing on a single jailbreak, AIM).  It'd be good to see this work done at a larger scale (especially given much of it is automated) and to include error bars given the small sample sizes.\n\nW3. Also given the small sample size, it is hard to gauge what diversity of safety issues are studied.  More details on the types of safety issues and how those vary across language would help contextualize the results (although I believe this is less critical).\n\nW4. Experiments are only run on ChatGPT and GPT-4.  It'd be useful to see similar experiments on other models, e.g. through HELM or just a few other APIs (again not critical but valuable).\n\nW5. The method seems somewhat over-complicated, when the data eg from Anthropic could be translated and used directly.  How come this more complex algorithm is necessary/valuable?  \n\nW6. Overall I'd consider the mitigation section the weakest section - it'd be nice to see the authors find a way to improve safety without hurting usefulness, and to understand the benefits of additional data with as much nuance as the earlier sections (eg results don't seem to be broken down here by level of language resources anymore)."
            },
            "questions": {
                "value": "While W1 is mostly a writing suggestion, further experimental details and answers for W2-W6 above would be appreciated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698976269632,
        "cdate": 1698976269632,
        "tmdate": 1699637164664,
        "mdate": 1699637164664,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vXOvKQwpvQ",
        "forum": "vESNKdEMGp",
        "replyto": "vESNKdEMGp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_Pbxw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_Pbxw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes multilingual jailbreaks in two scenarios, 1) the unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, 2) the intentional scenario entails malicious users combining jailbreak instructions with multilingual prompts to attack LLMs deliberately. Empirical experiments show that proprietary models are vulnerable in unintentional scenarios. Besides, a self-defense strategy is proposed where both harmful and harmless prompts and desired responses in different languages are utilized to fine-tune chatgpt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes an effective jailbreak strategy, with detailed analysis on a defense attempt as well.  \n2. Two jailbreak scenarios are investigated, one is intentional and the other is unintentional."
            },
            "weaknesses": {
                "value": "1. Using output from GPT4 as evaluation measure is not a good idea although its output looks consistent as human evaluation demonstrated in Fig.2\n\nAs shown in Table 1, GPT-4 still suffers from jailbreak when the user prompt is in English: as high as 28.25% for unintentional case. \n\n2. It's hard to distinguish the high success attack rate is attributed to rarely unseen language or the AIM harmful instruction, since the harmful instruction itself alone can achieve very high success attack rate in English, as shown in Table 1."
            },
            "questions": {
                "value": "In section 3.1. Setup, Since Anthropic's red teaming dataset contains multi-turn dialogs, it's likely that the first user prompt is harmless. Perhaps you should consider both task_descripton_harmlessness_score and the number of turns (turn=1 to ensure that the 1st user prompt is harmful if the score satisfies some pre-defined criterion). Moreover, there are 38961 instances covered by the red teaming dataset, the details for how to select 300 from 38k instances should be elaborated.\n\n**Suggestion**\n\nIn tables, all the unsafe rate values are presented in percentile. But in figures, values are shown in decimal. A consistent presentation throughout the paper is preferred for easier understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Reviewer_Pbxw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699140651882,
        "cdate": 1699140651882,
        "tmdate": 1700792858998,
        "mdate": 1700792858998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Xz7ydwtzrH",
        "forum": "vESNKdEMGp",
        "replyto": "vESNKdEMGp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_emGp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9250/Reviewer_emGp"
        ],
        "content": {
            "summary": {
                "value": "This paper reports the vulnerabilities of large language models from the perspective of two scenarios: unintentional and intentional situations. In the unintentional scenario, queries that are simply translated into non-English languages unexpectedly make users face unsafe content. On the other hand, the intentional scenario considers translated multi-lingual \u201cjailbreak\u201d prompts. The paper evaluates ChatGPT and GPT-4, and reveals that the models are more risky as the target language is lower-resource. Lastly, they propose \u201cSelf-Defense\u201d method that fine-tunes ChatGPT with augmented and translated unsafe data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper addresses timely and crucial issue of LLMs, as those models are usually evaluated in English, despite they have been widespread around the world. I think we should pay attention to strengthening the safety level of LLMs for the various languages including low-resource languages. In that sense, the multilingual jailbreak and the evaluation results are valuable."
            },
            "weaknesses": {
                "value": "Although the significance of the theme of this paper, in overall, the soundness and presentation are quite needed to be improved to strengthen this paper. Here are my comments and feedback. Some might be nice-to-haves but not strictly necessary, so feel free to argue why it is a good idea not to incorporate them.\n\n1. Overall, when you mention differences or improvements in performance, please mention the significance levels (ie., p-values) together. Also, please denote standard deviations (inside tables or figures), because the number of test samples is limited (15 prompts for the preliminary experiment).\n2. Lack of human evaluation details. Moreover, decisions on whether generated content is safe or unsafe are subjective and depend on annotators. The paper should include the human evaluation process, instructions, and annotator information.\n3. Recently, there has been an argument that the evaluation of model-generated output with LLM-based evaluators has a potential bias \u2014 that is, GPT-4 might prefer and give higher scores on GPT-4\u2019s outputs than ChatGPT [1, 2]. Also, (in Table 1), GPT-4 might have been aligned more than ChatGPT concerning GPT-4\u2019s safety evaluation standards. Moreover, GPT-4 as an evaluator might have led to length or positional bias [2]. The paper should mention this if you have already considered this, or reinforce the results with repetitive experiments.\n4. Moreover, in Section 2, the authors mentioned that \u201cFurthermore, Figure 2 illustrates a notable level of agreement between human annotators and the GPT-4 evaluator.\u201d, I think you can provide the correlation or agreement values (numbers) rather than just pointing out similar trends of the two lines.\n5. Release of MultiJail dataset and evaluation results. \u2014 I believe the authors would be able to publish their data.\n6. This paper evaluated only two black-box models, whose training corpus and portion of languages are unveiled. What about evaluating other open-sourced models such as LLaMA, Vicuna, and others, to see the correlation between the portions of language resources in training data and the attack success rates?\n7. The proposed \u201cSelf-Defense\u201d framework is simple and able to show the effectiveness. However, in somewhat points, the result comes out naturally, since the ChatGPT was further fine-tuned with the augmented multi-lingual unsafe dataset. Can we conclude that the improvement depends on the extent of the low-source level? Or another insight can be concluded from the framework?\n8. Lastly, the authors mention that \u201cAdditionally, the translation process enables\u00a0*the transfer of knowledge and safety guidelines across multiple languages,*\u00a0thereby improving the safety alignment in a multilingual context.\u201d However, the safety guidelines can be differently applied across different cultures and societies. I am not certain that the adversarial examples used in this paper are general enough to be transferred across other languages and societies. However, at least the guidelines follow the U.S. or several companies, and I think we should carefully consider the sensitivity of safety level in the target society.\n\n- [1] Liu et al., G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment, https://arxiv.org/pdf/2303.16634.pdf\n- [2] Zheng et al., Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena, https://arxiv.org/abs/2306.05685\n- [3] Wang et al., Large Language Models are not Fair Evaluators, https://arxiv.org/pdf/2305.17926.pdf"
            },
            "questions": {
                "value": "- In Table 1, unlike to the unintentional scenario, in the intentional scenario, it\u2019s hard to find any tendency among HRL, MRL, and LRL.\n- The generated outputs in non-English language are again translated into English to be assessed by a Human or GPT-4 evaluator, through Google Translates. In this process, why didn\u2019t you assess the generated output itself, but translate it back to English? Do you think there could be errors?\n- For simplicity, the authors set the temperature as zero for the two models. Do you think the results could vary if nucleus sampling was applied?\n- \u201cunsafe and general\u201d phrase could cause readers to misunderstand; \u201cgeneral\u201d means \u201csafe\u201d or \u201chelpfulness (usefulness)\u201d instruction pairs. Or even it can be interpreted as an unsafe prompt - a general (safe) response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9250/Reviewer_emGp"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699291415936,
        "cdate": 1699291415936,
        "tmdate": 1700664094975,
        "mdate": 1700664094975,
        "license": "CC BY 4.0",
        "version": 2
    }
]