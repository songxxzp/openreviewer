[
    {
        "id": "scMhKWJnxf",
        "forum": "T9w5ttdqLV",
        "replyto": "T9w5ttdqLV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_dx4A"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_dx4A"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to tackle the issues of value decomposition. Initially, it explores the expressive capacity of linear mixing functions and then develops a decomposition structure to achieve complete representational capabilities. Additionally, it identifies and offers a solution to the problem of optimal representation interference. Experimental results substantiate the efficacy of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a substantial and meticulously detailed theoretical derivation and proof.\nThe proposed method to construct the complete representational structures is interesting and novel."
            },
            "weaknesses": {
                "value": "The paper suffers from some unclear expressions and inconsistencies with prior research.\nThe experimental evaluation is lacking.\nSee questions below for detail."
            },
            "questions": {
                "value": "1. The relationship between this paper and [1] requires further elucidation. What's the connection between \"the LMF in indecomposable MMDP has unbaised TD target in sarsa\" and \"the on-policy LMF with fitted-q-iteration has at least one fixed-point Q-value that derives the optimal policy\" ? It appears that the results in [1] might be stronger.\n\n2. The paper mentions that addressing single-step matrix games is applicable to solve the optimal policy, but only for sarsa. Does this imply that the theoritical results can only be upheld through on-policy training, or is this just a conclusion unrelated to the subsequent paper?\n\n3. Why is Eq6 used to define complete representational capacity in this paper? The meaning of complete representational capacity under IGM remains unclear. Is this consistent with prior works?\nQTRAN is known be necessary and sufficient for IGM, but it has incomplete expressiveness in the context of this paper. Does this imply that methods with incomplete expressiveness can also theoretically guarantee optimal outcomes?\n\n4. Confusing about the \"monotonic mixing\" and \"strictly monotonic mixing\". I think the original \"monotonic\" in previous work means \"strictly monotonic\" in this work, rendering the \"monotonic\" in this work redundant. To the best of the reviewer's knowledge, none of the existing methods under IGM are non-monotonic. In fact, non-monotonic mixing might not satisfy IGM.\n\n5. The paper introduces multiple mixers to cover a wider range of mixing functions. This needs further clarification. Is this theoretically necessary, or is it merely a technique for improved performance? It would be valuable if an ablation study on different numbers of mixers in SMAC is conducted.\n\n6. The meaning of Eq11 requires further explanation. Why is the optimal representation ratio defined as such, and what does a low w* signify in terms of ORI?\n\n7. The final algorithm, particularly the expression of Q_tot, is unclear. How are Eq12 and Eq13 applied in Eq10?\n\n8. Does the final algorithm possess complete expressiveness or is it necessary and sufficient for IGM?\n\n9. Why is the single-step matrix game trained in an on-policy manner? Can the proposed method converge to the optimum through off-policy training? Additionally, can QTRAN achieve convergence to the optimum in this game?\n\n[1] Wang et al. Towards understanding linear value decomposition in cooperative multi-agent q-learning. 2020."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698066983042,
        "cdate": 1698066983042,
        "tmdate": 1699636951479,
        "mdate": 1699636951479,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p86aGgoJAt",
        "forum": "T9w5ttdqLV",
        "replyto": "T9w5ttdqLV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_mg9a"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_mg9a"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on addressing the important problem of representational limitation in value decomposition methods for Multi-Agent Reinforcement Learning (MARL) problems. The contributions of the paper are as follows:\n\n1. The paper defines the circumstances under which the linear representational limitation occurs, specifically for Linear Mixing Function (LMF).\n\n2. It introduces a two-stage mixing framework called Mixing for Unbounded Difference (MUD) that addresses the representational limitation by ensuring complete representational capacity under the Independent Global Max (IGM) constraint."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper exhibits several notable strengths across multiple dimensions.\n\n_Originality_\n\n   - The introduction of the Mixing for Unbounded Difference (MUD) framework, as far as the reviewer is concerned, represent new solutions to address the representational limitation issue. (Although it shares some similarities with other methods, see the weakness section).\n\n_Quality_\n\n   - The proposed MUD framework is supported by mathematical reasoning, enhancing its quality as a potential solution."
            },
            "weaknesses": {
                "value": "1. The first contribution about LMF and the second contribution on SMMF seems to be separated.\n\n2. The findings about LMF is not quite surprised, as previous work indicates its limitations. What could be interesting is why LMF can work well (empirically) on many tasks, especially when used with gradient-based RL methods.\n\n3. Why do the experiments demonstrate advantage of the proposed method on SMAC, but the performance is similar to baselines in relatively easy task of Predator-and-prey?\n\n4. (*) The proposed MUD share similarities with the QPLEX framework. So the empirical comparison is very important. The reviewer is curious why the performance of QPLEX on SMAC is __significantly__ different from what was reported in the original paper. \n\n4.1 Please discuss the difference from QPLEX in detail.\n\n5. (*) The representational interference problem is not unique to the MUD framework and has been discussed by previous work [1]. \n\n(4 and 5 are the main reasons for the overall negative score.)\n\n[1] Ye, J., Li, C., Wang, J. and Zhang, C., 2022. Towards global optimality in cooperative marl with sequential transformation. arXiv preprint arXiv:2207.11143."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718945987,
        "cdate": 1698718945987,
        "tmdate": 1699636951371,
        "mdate": 1699636951371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8w4n6i6H2U",
        "forum": "T9w5ttdqLV",
        "replyto": "T9w5ttdqLV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_weNa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7786/Reviewer_weNa"
        ],
        "content": {
            "summary": {
                "value": "The paper studied the problem representation limitation in value decomposition method in fully cooperative multi-agent reinforcement learning. The author proposed a novel idea that the representation limitation comes from two different perspectives, task property and mixing function.\n\nThe authors theoretically proved that LMF is free from representational limitation only in a rare case of MARL problems. SMMF suffers from representational limitation due to the bounded difference between the outputs of greedy and current actions.\n\nTo address these issues, the authors also proposed a new framework of mixing for unbounded difference and test with experiments on several different environments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper studied a very basic problem in value-based multi-agent reinforcement learning, and theoretically proved the limitations of both the problem itself and existing methods."
            },
            "weaknesses": {
                "value": "The writing quality could be further improved. It's a little bit hard to follow the paper currently."
            },
            "questions": {
                "value": "1. We still have some other combination types that is stronger than IGM but is not perfect. Please discuss about this part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7786/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840235882,
        "cdate": 1698840235882,
        "tmdate": 1699636951265,
        "mdate": 1699636951265,
        "license": "CC BY 4.0",
        "version": 2
    }
]