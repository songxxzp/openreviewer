[
    {
        "id": "9gdwsFsy8k",
        "forum": "Dgc5RWZwTR",
        "replyto": "Dgc5RWZwTR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_7uDV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_7uDV"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of training a neural solver for multi-task combinatorial optimization. It proposes to use a bandit algorithm to choose the task instances for training to optimize the final model performance over all tasks. It proposes to use the average change of the training loss (approximate by first order gradient signals) due to the chosen task as the reward. Experimental studies on 12 tasks of various size and type are done using EXP3 algorithm and compared to single task learning and other multi-task learning algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Significance: The paper formulates an interesting and useful problem. The proposed algorithm outperforms many existing algorithms.\n2. Clarity and quality: the paper is well written and flows smoothly. The problem is clearly motivated and an effective solution is proposed."
            },
            "weaknesses": {
                "value": "1. Novelty: given the specific structure of the problem, the paper could improvise more specifically tailored bandit algorithms for the problem; a simple example would be using a structured/contextualized bandit algorithm that takes into account the complexity and type of the tasks.\n\n2. Section 3.2 could benefit from some clarifications: the reward design could be simplified and avoid some repetition of notations. On the other hand, the discussion around Assumption 1 needs more elaboration; it is not clear how the equality is established or if it is an approximation indeed."
            },
            "questions": {
                "value": "1. $\\eta_t$ is never defined. \n\n2. Given that neural solvers usually have a large number of parameters, what do you think about the space complexity of saving the gradient information in step 2 of Algorithm 1?\n\n3. In Section 4.1, it looks like the STL model trained on the large instances perform on par with your model, based on the \"Mean Results on *<COP>*\" plots, except only for CVRP. Then why do you think \"Our method can handle various types of COPs under the same number of training epochs, which is **impossible for STL** ... \"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Reviewer_7uDV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1353/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698022362193,
        "cdate": 1698022362193,
        "tmdate": 1699636062603,
        "mdate": 1699636062603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vhwm4SjZQF",
        "forum": "Dgc5RWZwTR",
        "replyto": "Dgc5RWZwTR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_B49D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_B49D"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to train a multi-task combinatorial neural solver under limited budget constraints. The combinatorial optimization problems chosen for their setting are the Travelling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP), the Orienteering Problem (OP), and the Knapsack Problem (KP), and each of them with three problem scales that vary from 20 to 200 tasks. The key idea (as with any multi-task learning setting) is to learn the shared parameters across tasks as well as the parameters for the individual task itself. This loss decomposition is captured in section 3.1. I think the key novelty of their paper lies in designing the reward for tasks so that they can use an MAB (Multi-armed bandit) algorithm to first select a task, calculate the loss, use the designed reward to update the MAB algorithm and proceed to the next iteration. The reward is designed using the cosine similarity function in such a way that it facilitates learning between tasks under the same COP and across tasks under different COPS and an influence matrix is constructed with it. Finally, they use the column sum of this influence matrix (which captures the influence of one task on all others) to get an average reward for each task and use it to select the next task. They conduct an empirical evaluation demonstrating two things mainly: (1) Under identical training budgets, their method effectively solves for multiple COPs by learning a shared representation (influence) rather than solving each task individually; (2) Given the same number of training epochs, their method comes up with a neural solver that demonstrates better generalization capability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper is of low to moderate significance as the key findings from their empirical results are mostly known. Nevertheless, I find it interesting because they use MABs as a sirt if uncertainty quantification on top of a multi-task neural solver which leads to a practical algorithm.\n2) They provide a well-justified theoretical decomposition of their loss function for both GD and ADAM settings.\n3) They conduct extensive experiments across all four COP settings under different budgets and scale levels. They demonstrate that their neural solver performs better than typical MTL methods and require less budget."
            },
            "weaknesses": {
                "value": "1) The writing needs more improvement. For example, they never talk about the specific MAB algorithm they use till Appendix G and H (except one line in the experimental setting). It seems they are using Thompson Sampling, or Discounted Thompson Sampling, or Exp algorithms as base MABs. These choices need more justification.\n2) The key findings from their empirical evaluations are mostly known: a) Training combinatorial neural solvers on one problem scale leads to higher benefits on similar problem scales than on those that are further away. b) Negative transfer exists among different tasks. c) Easier tasks require less budget.\n3) No analysis of budget allocation is done, even though this is fairly well understood in the multi-task setting (both online and offline)."
            },
            "questions": {
                "value": "1) It is not clear to me how the four different COPS can be considered as different tasks, and how the difference is captured. There is no discussion on this. Moreover from Appendix F, it is not clear how the Gradient norm captures the difference between the four different COPS as well as the scales within a COP. Can the authors elaborate on this?\n2) You use the column sum of the influence matrix to calculate the average reward for each task and then select the next task based on that. How do you ensure that you conduct a sufficient exploration/exploitation of task? Don't you want to use an exploration bonus (like UCB) over the average reward \\bar{r}^i_j?\n3) Why do you only use the column-sum of the rewards, and not the row-sum from the influence matrix? Note that the row-sum denotes the influence of all tasks on one task. Intuitively even that should facilitate learning for an individual task.\n4) This question is again related to the diversity of tasks. How is your neural solver affected when some tasks are more difficult than others? How does the budget get proportioned then? What happens to the influence matrix then? Can you elaborate on this?\n5) Some design choices in the experiment section need more explanation. For example why the balanced allocation is chosen in 1:2:3 ratio from small to large scale?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1353/Reviewer_B49D"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1353/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698119823258,
        "cdate": 1698119823258,
        "tmdate": 1699636062521,
        "mdate": 1699636062521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Swc79uxhPp",
        "forum": "Dgc5RWZwTR",
        "replyto": "Dgc5RWZwTR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_ezHK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_ezHK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-armed bandit (MAB) framework to learn a general neural combinatorial optimization solver. The key idea is to use the neural solver loss gradient to construct an influence matrix, which guides the sampling of the next training task. The sampled tasks are used to co-train a global encoder in an encoder-decoder framework. The proposed method was demonstrated on several combinatorial benchmarks. It is compared against other multi-task learning benchmarks and achieves good performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is clearly written. Experiments are well designed, and generally has promising results.\n\nThe proposed MTL strategy builds on the idea of loss difference to construct an influence matrix (Fifty et al., 2021) and extends it to gradient difference. As far as I am aware, this is a new and interesting contribution."
            },
            "weaknesses": {
                "value": "1. There is a general lack of motivation for the proposed method throughout the paper.\n- How is the technique proposed specific to solving COP? As far as I understand, it could be applied to MTL setting. Should it outperform other MTL baselines in a general scenario? If not, why is it performing well on the COP tasks?\n\n- The paper went on to describe a heuristic reward design, but ultimately why is it better? There is neither theoretical guarantee nor complexity analysis, so the method is somewhat unconvincing to me.\n\n2. Regarding significance:\n- Experiment results are generally promising, but error bar should be provided to quantify significance.\n- It feels strange that the authors brought up (Fifty et al., 2021) in the intro as the inspiration for the proposed method, but did not compare to it. Is there a specific reason for this?"
            },
            "questions": {
                "value": "1. What is responsible for the efficiency of the proposed method? It is about 15 times faster than the closest method (Table 3) despite only changing the sampling scheme. Can the authors provide a breakdown of how other MTL techniques compute the influence matrix?\n2. Can we apply a naive co-training scheme (with random sampling of training tasks) to achieve the same effect as the proposed method? I figure it would serve as a good ablation study.\n3. How often does assumption 1 hold in practice?\n4. Why are some entries in the diagonal blocks negative (whereas many off-diagonal entries are positive). It feels unintuitive that tasks from the same group have weaker influence on one another than tasks from different groups.\n5. Can the method be generalized to account for unseen tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1353/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698635734463,
        "cdate": 1698635734463,
        "tmdate": 1699636062438,
        "mdate": 1699636062438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u1KPuqEP1g",
        "forum": "Dgc5RWZwTR",
        "replyto": "Dgc5RWZwTR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_48WT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1353/Reviewer_48WT"
        ],
        "content": {
            "summary": {
                "value": "This paper studies how to efficiently train a multi-task neural solver for multiple combinatorial optimization problems (COPs). They use the gradient information to construct a measure of the similarity of tasks, which is in turn used in the construction of rewards for a multi-armed bandit sampler used to balance the training of multiple tasks. Extensive simulation results are presented to validate the performance of the proposed algorithm over single-task learning and multi-task learning baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "# Origionality\n- Applying MTL to solve multiple COPs seems to be novel. \n# Quality\n- The experiments are extensive and in detail.\n# Clarity\n- The paper is in general well-written and smooth to follow, with the exception that some notation in Section 3 are a bit messy.  \n# Siginificance\n- This paper might be of interest to researchers in MTL as it successfully solve multiple COPs with different scales."
            },
            "weaknesses": {
                "value": "My main concerns are about the novelty and significance of this work. \n- Although applying MTL to COPs is less studied, the methodologies presented in this paper (e.g. similarity measure based on gradient information, MAB algorithms) have been well developed. This paper likely attempts to combine them within the context of COPs. In fact, extracting similarity measures using gradient information has been considered in the literature (Wang et al., 2020; Yu et al., 2020). Applying bandit algorithms in MTL has also been studied before (Mao et al., 2021).\n- This is a pure experimental paper without theoretical guarantees. I would appreciate some performance guarantees based on the well-developed theoretical results in MAB problems. \n- In terms of the performance of the proposed method shown in the experiments, I don't think it \"achieves much higher overall performance, ..., compared to standard training schedules\". First, in the comparison under the same training budget, each COP naively receives an equivalent budget of $B/4$. Yet, $STL_{avg}$ still archives the best gap in many tasks (Table 2). Second, for comparison under the same training epochs, although I agree this is not a fair comparison, the performance is just equivalent to 100-200 epochs of STL, not significantly larger than the naive calculation $1000/12 \\approx 83$ epochs. I would appreciate it if the authors could provide more evidence about the superiority of the proposed algorithm."
            },
            "questions": {
                "value": "- It seems that the correlation of training the same COP with different scales might be negative, e.g. training TSP-20 pm TSP-100. This is very counterintuitive to me. Could the author explain why this happens?\n- I'm curious whether the bandit algorithm is really bringing significant improvement in balancing the training. If we just randomly sample the tasks or naively assign them weights according to the scales of the tasks, what would be the performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1353/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698711678044,
        "cdate": 1698711678044,
        "tmdate": 1699636062350,
        "mdate": 1699636062350,
        "license": "CC BY 4.0",
        "version": 2
    }
]