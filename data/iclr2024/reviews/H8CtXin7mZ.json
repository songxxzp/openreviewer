[
    {
        "id": "ZNv3jYAd9b",
        "forum": "H8CtXin7mZ",
        "replyto": "H8CtXin7mZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
        ],
        "content": {
            "summary": {
                "value": "The manuscript uses a neural network to construct a preconditioner for solving Poisson equations on various geometries with varying boundary conditions. The neural network takes 5-7 days to train before it is a reasonable preconditioner across various geometries. It then provides a modest speed-up over some of the existing preconditioning techniques. The biggest gap in the manuscript is that the preconditioner is not symmetric positive definite, and a highly nonstandard iterative method is advocated that will be challenging to develop a general convergence theory. The multigrid preconditioner remains a good choice for fixed geometries, but here, the neural network preconditioner seems beneficial when applied to a range of geometries."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The Deep Conjugate Direction Method (DCDM) is a novel approach that leverages deep learning to approximate the solution of large, sparse, symmetric, positive-definite linear systems of equations. This manuscript is improving on this solver for the setting of Poisson on various geometries. The practical simulations (supplementary) are relatively impressive, demonstrating the effectiveness of their preconditioner. \n\nAfter training, the constructed preconditioner for Poisson's equation is surprisingly effective at reducing the number of iterations of the DCDM-variant for various geometries."
            },
            "weaknesses": {
                "value": "Without a positive definite preconditioner, it is likely that the simulations in Figure 2 become unstable (or unphysical) if run for long enough. I think the authors should carefully consider both the computational cost per time step and the numerical stability of the time-stepping scheme when using various preconditioners. This direction is mentioned as a future work, but I think the paper's contribution is compromised without ensuring symmetric positive definiteness.\n\nThe lack of theoretical convergence analysis of the preconditioner and iterative solver means that the solver is of limited general use. Since the preconditioner is not positive definite, I suspect that the theoretical convergence of the iterative scheme is extremely difficult to understand.  \n\nTraining the neural network with a large memory requirement takes a very long time before it can be successfully used in time simulations."
            },
            "questions": {
                "value": "Can the method be used to construct a neural network-based positive definite precondition? If so, then it at least takes into consideration that the original problem is symmetric, positive-definite linear systems of equations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698017127831,
        "cdate": 1698017127831,
        "tmdate": 1699636708229,
        "mdate": 1699636708229,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SDxayL6OUd",
        "forum": "H8CtXin7mZ",
        "replyto": "H8CtXin7mZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_ddbQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_ddbQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a neural network architecture inspired by multigrid to learn an approximate inverse of the discrete Laplace operator, which serves as a preconditioner for an iterative PDE solver. The key contributions are: 1) Handling mixed Dirichlet and Neumann BCs, which prior works cannot; 2) A novel network structure that incorporates boundary information through spatially-varying convolutions; 3) State-of-the-art performance vs AMG, IC and other baselines on fluid simulation examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The network design that encodes boundary information through spatially-varying convolutions is novel and well-motivated. This likely contributes significantly to the method's success in addressing mixed BCs.\n\n2. Comprehensive empirical evaluation on simulation benchmarks demonstrates clear superiority over optimized baseline methods on problems. Statistical analysis provides convincing evidence of the method's benefits."
            },
            "weaknesses": {
                "value": "1. While the spatially-varying convolutions encode boundary data effectively, their implementation as CUDA kernels is noted to be a computational bottleneck. Further optimization could yield additional speedups.\n\n2. Enforcing symmetry and positive-definiteness of the preconditioning operator was not achieved, limiting the method to a generalization of CG instead of CG itself.\n\n3. The network does not yet leverage sparsity, so may not scale gracefully to extreme sparse problems with many empty grid cells.\n\n4. Only isotropic problems were considered - extending the approach to anisotropic or nonlinear problems is an open question.\n\n5. Training data generation from Lanczos vectors is costly. Finding cheaper alternatives while maintaining accuracy could be valuable for certain applications.\n\n6. Proving theoretical convergence properties like those of standard multigrid would strengthen understanding, though difficult given the learned nature.\n\n7. Quantitative ablation of design choices like coarsening levels, memory usage, etc. could provide further insights.\n\n8. While extensive, the benchmark set considers a subset of potential problem domains - generalizing to new classes would reinforce claims."
            },
            "questions": {
                "value": "1. Can you exploit sparsity directly in the network/algorithm to better handle extremely sparse problems that arise in practice\n\n2. Can you investigate enforcing symmetry and positive-definiteness of the preconditioner to allow use of standard CG?\n\n3. Can you explore non-Euclidean/anisotropic problem settings to broaden the method's scope?\n\n4. Can you develop cheaper training data generation techniques or consider self-supervised learning alternatives?\n\n5. Can you perform sensitivity studies on architectural hyper-parameters like coarsening levels, memory usage, etc?\n\n6. Can you attempt theoretical analysis of convergence properties to strengthen understanding of the method?\n\n7. Can you broaden empirical evaluation to new problem classes to further validate generalization abilities?\n\nI would like to improve my score if the concerns above are well addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634204769,
        "cdate": 1698634204769,
        "tmdate": 1699636708082,
        "mdate": 1699636708082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EwvnhJ9FMC",
        "forum": "H8CtXin7mZ",
        "replyto": "H8CtXin7mZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach to construct preconditioner for Poisson equations based on neural networks. The architecture is to mimic the geometric multigrid. The approach has the capability to handle arbitrary shape of the fluid and mixed boundary conditions embeded in a rectangle or cube."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The neural network can effectively approximate the inverse of the stiffness matrix from the Poisson equation such that the resulting iterative solver is much faster than the existing methods."
            },
            "weaknesses": {
                "value": "- The primary contributions of the paper, including the understanding of convolution as a smoother, pooling as the restriction, and the connection between multigrid and convolutional neural networks, have been extensively explored in the following reference:\n\n  Juncai He and Jinchao Xu. \"Mg-Net: A unified framework of multigrid and convolutional neural network.\" arXiv:1901.10415, 2019.\n\n- It appears that the comparison in the paper is limited to iterative solvers and does not account for the time required to train the preconditioner. This approach may not provide a fair assessment. It's worth noting that the setup time for AMG can be comparable to the training time. If one considers the extended matrix on the uniform grid, a fixed hierarchy can be used, and the learning task could focus on the prolongation and restriction operators for different $\\mathcal I$. The approach outlined in the following paper may offer valuable insights:\n\n  Alexandr Katrutsa, Talgat Daulbaev, Ivan Oseledets. \"Deep Multigrid: learning prolongation and restriction matrices.\" [arXiv:1711.03825](https://arxiv.org/abs/1711.03825)."
            },
            "questions": {
                "value": "- The data generation process described on the bottom of page 6 needs further clarification. It's not entirely clear whether it involves variations in the domain geometry. If it does, it would be helpful to specify the nature of the randomness associated with the geometry. Additionally, the presentation seems to only consider two types of discretization sizes. If that's the case, it's important to explain how this approach can be applied to systems with different discretization sizes. Does the neural network need to be retrained for each new size? Furthermore, more details about the randomness involved in generating the right-hand side of the equations would be appreciated.\n- Could the authors provide an explanation of what the neural network truly learns for an iterative solver to maximize its performance? Specifically, does it focus on learning the smoother or the prolongation operator, or both? Clarity on this aspect would be valuable for understanding the role of the neural network in enhancing solver performance.\n- Is it necessary to fix the levels of coarsening in the proposed approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742404389,
        "cdate": 1698742404389,
        "tmdate": 1699636707931,
        "mdate": 1699636707931,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OZteYJtCcU",
        "forum": "H8CtXin7mZ",
        "replyto": "H8CtXin7mZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new data-driven preconditioner for Poisson problems with mixed boundary conditions with a MAC discretization. The PDE arises from the moving boundary of a multiphase flow, thus traditional AMG is unfavorable in terms of the MG hierarchy rebuilding. The key method is built upon the work in Kaneda et al ICML 2023. The authors also tackles a difficulty through the optimizer that the DNN preconditioning approach is non-symmetric, which achieves better result than approaches such as `BLOPEX` in hypre. From the perspective of someone who worked in the business of traditional multigrid solvers/preconditioners for PDEs, the PDE discretization is nothing new, the methodology used is pretty predictable, and there are a few unclear things, however, I still think this is overall a solid work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- In the context of traditional approaches of multigrid, either solver or preconditioner, either geometric or algebraic, imposing (nonhomogeneous) Neumann BC can be challenging and is usually an ad-hoc business.\n- The study, from preconditioning pov, is quite well-motivated by introducing the ever-changing BCs for the Poisson problem through the multiphase flow, since traditional AMG has to rebuilt the AMG hierarchy each time step.\n- The phase variables as channels are neat practices, which is nicely motivated by the ever-changing phase in simulation. But references should be given (e.g., arXiv:1707.03351).\n- The spatially varying kernel (or instance-dependent kernel) is an extremely nice practice, and rightfully so motivated by the mathematical nature of the problem. This is highly connected to attention (even though it is nonlinear) applied to PDE problems (where attention is viewed as a kernel integral), please consider giving a few references in this regard.\n- I ran the code myself, and was amazed by the excellent reproducibility of it. However, the instruction of how to install `AMGCL` is problematic in `README.md` for someone with existing header files for `AMGCL`."
            },
            "weaknesses": {
                "value": "- Personally, I am quite uncomfortable to impose Dirichlet boundary conditions on the pressure variable for the fluid problems among many formulations I have played with for NSE or Stokesian flow. While the temporal discretization of NSE presented on page 3 is a standard splitting scheme (aka \"projection method\" or \"pressure-correction scheme\"), the reference given is likely not the right one. In Chorin's 1967 JCP paper, he proposed the famous pressure marching scheme to impose divergence free condition at $(n+1)$-th time step (implied by (4) in the paper), but did not solve a PDE for pressure. I checked Temam's book as well as J. Shen (Temam's student)'s review paper on projection methods, and the scheme featured in this paper is probably attributed to Chorin's 1968 Math. Comp. paper (equation (21) therein for pressure). However, therein, only the Neumann BC is imposed for the pressure. So please give the reference, preferably with equation numbers that where does the Dirichlet part comes from.\n- The current presentation of the overall iterative procedure is unclear in section 4. For example, in Figure 1, only $\\mathbf{r}\\mapsto \\mathcal{P}^{\\text{net}}(\\mathcal{I}, \\mathbf{r})$ is shown. I suggest move A.1 to the main part.\n- The biggest weakness is perhaps that the \"CNN\" used is linear, then it is nothing but a single parametrized convolution, as stacking convolutions without nonlinearity is still a convolution just with a different kernel sizes.\n- Maybe this is up to debate, that the authors said \"AMG setting up stage takes too long\". I think this is an unfair comparison, on a fixed mesh, AMG hierarchy has to be set up only once, and for GMG it is automatic. Moreover, AMG setting up is automated on any mesh and (mostly) robust for specific PDEs that needs no training stage. Another counterpoint would be that the current approach (based on CNN) is limited to Cartesian mesh-based discretization such as MAC.\n\n### Minor things\n- I suggest the authors rewrite the abstract. As writings like \"The Poisson equation is ubiquitous in scientific computing: it governs... The most popular Poisson discretizations yield large sparse\nlinear systems\" are more appropriate in the introduction than the abstract, a better way to put things in the contexts is just saying large systems arising from PDE's discretization is hard to solve.\n- The acronym DCDM goes undefined across the paper.\n- Page 1: \"matrix-norm\" should be \"matrix norm\".\n- Page 2: \"their loss functions is...\".\n- Page 2: \"disretizations\" -> discretizations.\n- Page 5: if $P = LL^{\\top}$ is the preconditioner (assuming the new system is $PA\\mathbf{x} = P\\mathbf{b}$), then CG steps build subspaces that are $L^{\\top}A L$-orthogonal, not $L^{-1} A L^{-\\top}$-orthogonal (to converge to $L^{-1}\\mathbf{x}^*$ for $\\mathbf{x}^* = A^{-1}\\mathbf{b}$).\n- Personally, I am not in favor of referring $\\mathcal{I}$ an \"image\" in this context because one may confuse this term with the image of an operator. Of course, it is up to the authors' judgement on this."
            },
            "questions": {
                "value": "- The targeted audience who works in the solver business would usually not know what \"PIC/FLIP blend transfer scheme\" is, I suggest elaborate it somewhere in the context of this paper, especially considering a similar acronym IC stands for incomplete Cholesky.\n- Judging by the name of Harlow 1964 Meth. Comp. Phys. paper, it is the ref for PIC? The correct origin of MAC should be Harlow and Welch's 1965 paper in The Physics of Fluids, some people like to cite this tech report as well (https://www.osti.gov/biblio/4563173).\n- Page 4: \"The PSDO algorithm can be understood as a modification of standard CG that replaces the residual with the preconditioned residual as the starting point for generating search directions and, consequently, cannot enjoy many of the simplifications baked into the traditional algorithm\". This sentence is somewhat too long in reviewer's humble opinion, and up to the authors' judgement, I suggest rewrite it a little. Meanwhile, please consider spending a few words to explain the reason why PSDO cannot achieve CG's automatic $A$-conjugate directions. It says that PSDO does \"an exact line search\", so it does not a subspace search like the CG does?\n- Page 5-6, the default behavior of upsampling using the interpolation in `nn.functional` is `nearest` which gives a piecewise constant function that is unfavorable in this context, please test `bilinear` which is MG prolongation.\n- The single-precision for NN and double for the CG is a good practice. However, it would be interesting to see whether this combination can reach relative residual `1e-8` (cf. those in Figure 5)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772718691,
        "cdate": 1698772718691,
        "tmdate": 1699636707736,
        "mdate": 1699636707736,
        "license": "CC BY 4.0",
        "version": 2
    }
]