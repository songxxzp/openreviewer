[
    {
        "id": "xAHiDTihCy",
        "forum": "MCl0TLboP1",
        "replyto": "MCl0TLboP1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_iqAo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_iqAo"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a technique known as Heuristic Blending (HUBL) to enhance the performance of model-free offline Reinforcement Learning (RL) algorithms that are based on value bootstrapping. The primary aim of this approach is to mitigate the challenges associated with bootstrapping and achieve stable performance. \n\nHUBL essentially adapts the rewards and discount factor within the offline dataset utilized by the base offline RL algorithm. It achieves this by modifying the reward through a blending process, combining it with a state-specific heuristic derived from the Monte Carlo return of the behavior policy. It also reduces the discount factor discount factor. The degree of reduction in the discount factor and the reward blending is determined by a trajectory-dependent blending factor. This factor is designed to be high for trajectories in which the behavior policy performs well and low otherwise. \n\nThe authors offer three distinct methods for selecting this blending factor. They support their algorithm with theoretical analysis and experimental results."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed algorithm is simple and can be implemented with minimal overhead. \n2. The authors provide a complete theoretical analysis, and also conduct extensive experiments, on both deterministic and stochastic environments (although just one) . \n3. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "$\\tilde{r}$ **dependence on the behavior policy may cause problems when data is collected using multiple behavior policies.**\n\nConsider a dataset with data from a mixture of policies (say  the medium replay case in D4RL), and for simplicity suppose we assume  a constant lambda, say 0.5. Now, since the reward $\\tilde{r}$ is conditioned on the behavior policy, doesn\u2019t the effective MDP (reconstructed using the relabeled dataset) become non stationary? Is the performance not affected much due to the deterministic nature of the environments chosen for evaluation?\n\n**Choosing $\\lambda$ may not be straightforward**\n\nThe choice of $\\lambda$ should be such that it is high for trajectories with high returns, and low otherwise. But this is hard to determine if the dataset consists of trajectories that perform equally well. This can be seen in results shown in figure 2, as significant improvement is seen on datasets with with a mixture of behavior policies because there is a way to determine the appropriate choice of $\\lambda$ based on relative performance. Now, suppose you have a dataset with low rewards (the random variant in D4RL), then the ranking method will still assign high ranks to most trajectories, and this might result in poor performance as the Monte Carlo estimates using the behavior policy might cause instability."
            },
            "questions": {
                "value": "See weaknesses, in addition to them, \n\n1. Why is the training of a value function needed during step 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Reviewer_iqAo",
                    "ICLR.cc/2024/Conference/Submission7779/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270812913,
        "cdate": 1698270812913,
        "tmdate": 1700615853975,
        "mdate": 1700615853975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cZ9GJxKSx4",
        "forum": "MCl0TLboP1",
        "replyto": "MCl0TLboP1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_zyzv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_zyzv"
        ],
        "content": {
            "summary": {
                "value": "The paper deals with offline RL. It focuses on Q-function based offline RL methods and presents a heuristic to increase the performance of these methods. In extensive experiments a performance increase is observed on average."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The presentation is very good.  I would like to emphasize that the limitations mention both the restriction to trajectories and the lack of stochastic MDPs among the benchmarks used. This is exemplary. Also that already in the first sentence `We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping` it is clearly stated to which class of algorithms the paper refers.\n* The method is investigated as a modification of not just one, but four state of the art bootstrapping-based offline RL algorithms."
            },
            "weaknesses": {
                "value": "none"
            },
            "questions": {
                "value": "No questions, but a few notes and comments:\n* \"methd\" -> \"method\"\n\n* At \"Step 1: Computing heuristic ht\" ht should be set boldmath, analogously in Step 2 and Step 3.\n\n* At `Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep\ndata-driven reinforcement learning, 2020` the indication where it was published is missing.\n\n* In the references there are some unintentional lower case letters, e.g. mdp, Monte carlo.\n\n* Just for completeness, I\u2019d like to point out that there are also purely model-based batch/offline RL methods that do not use Q-function and are thus bootstrapping-free [1-4]. See [5] for a discussion. Since the authors have precisely formulated at the very beginning that this paper deals with the algorithm class with Q-function and bootstrapping, a mention of these bootstrapping-free algorithms is probably not necessary.\n\n[1] Schaefer et al., A recurrent control neural network for data efficient reinforcement learning, 2007\\\n[2] Deisenroth and Rasmussen, PILCO: A Model-Based and Data-Efficient Approach to Policy Search, 2011\\\n[3] Depeweg et al., Learning and policy search in stochastic dynamical systems with Bayesian neural networks, 2017\\\n[4] Swazinna et al., Overcoming model bias for robust offline deep reinforcement learning, 2021\\\n[5] Swazinna et al., Comparing Model-free and Model-based Algorithms for Offline Reinforcement Learning, 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698349937623,
        "cdate": 1698349937623,
        "tmdate": 1699636950097,
        "mdate": 1699636950097,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zD7Yqjd1Ne",
        "forum": "MCl0TLboP1",
        "replyto": "MCl0TLboP1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_Gwab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_Gwab"
        ],
        "content": {
            "summary": {
                "value": "This work presents Heuristic Blending (HUBL), a new technique that can be attached to offline RL methods to improve their performance. \n\nHUBL works by modifying the rewards and discounts in the dataset that the offline RL algorithm consumes, blending heuristic values to partially replace bootstrapping. \n\nTheoretical and empirical results show HUBL consistency when improving several offline RL algorithms performance by 9% on average over D4RL and Meta World datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* This paper meets very good originality, quality, clarity and significance criteria. Good job!\n\n* Section 2 does analyze the main differences of the proposed approach with respect to cited works. It is clear that no previous model has addressed the data relabeling as it has been proposed in this manuscript for the particular setting of offline RL. The use of both data relabeling and heuristic in combination with RL has been explored before, but not in the offline scenario.\n\n* HUBL is an original method to incorporate into existing offline RL methods. It improves them without any need for a complicated modification within any algorithm; only a relabeling of the dataset is needed. It is indeed done dynamically depending on the return of every trajectory. This makes this method quite significant, especially when working with low-quality data.\n\n* All the claims made by the authors are addressed through a comprehensive theoretical and empirical study. The paper is very well written; the notation is excellent, and all the details and formulas are clear.\n\n* The experimental setup proposed is thorough. The paper analyzes how the modification of up to four state-of-the-art offline RL methods behaves across more than 25 different benchmarks.\n\n* The theoretical analysis developed in Section 5 (and the corresponding annexes) is very robust and truly helps to understand what implementing HUBL entails."
            },
            "weaknesses": {
                "value": "* For someone not already familiar with offline RL, it can be challenging to follow the comprehensive theoretical analysis developed in this paper, especially in the appendices. \n\n* This claim should be justified: \"Despite their strengths, existing model-free offline RL methods also have a major weakness: they do not perform consistently.\" It would be fantastic if the authors could provide some evidences regarding this issue, as they do in section 3.2 (second to last paragrpah), but providing more details.\n\n* In the experiments section I miss the learning curves where a reader can compare how the reward evolves with an without HUBL.\n\n* I believe the manuscript needs a brief discussion on how the proposed model would be applied to problems with sparse rewards. For instance, in environments like Antmaze, has it been tested?\n\nMinor comments:\n\n-Section 4.1 HUBL introduce -> HUBL introduceS"
            },
            "questions": {
                "value": "I've tried to detail most of the limitations and weaknesses of the proposed model in previous section, with some points that would need to be addressed in a rebuttal.\n\nOverall, I see here a strong manuscript with some ideas that are adequate for an ICLR conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work describes a model for learning agents from offline data. If the dataset is biased, the models will be biased too.\n\nThe work described in the manuscript was carried out in simulation and as such is unlikely to have produced unethical results, except the impact of large-scale training on CO2 output."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Reviewer_Gwab"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762270892,
        "cdate": 1698762270892,
        "tmdate": 1699867484293,
        "mdate": 1699867484293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7qlCvqtGnU",
        "forum": "MCl0TLboP1",
        "replyto": "MCl0TLboP1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_8Sx2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7779/Reviewer_8Sx2"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the HUBL technique for offline RL algorithms, which partially replaces the bootstrapped values with heuristic ones estimated using Monte-Carlo returns.\nTheoretical analysis has been made to understand the improvements brought by HUBL to the original offline RL algorithm, as well as its associated bias and regret.\nExperimentally, results on the D4RL datasets and Meta-World benchmarks show that HUBL offers an average improvement of 9% over the current four SOTA offline RL algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed HUBL is a general technique that can be seen as a correction to the offline dataset itself, improving the performance of offline RL algorithms.\n* Through theoretical analysis, the introduction of HUBL is discussed as an MDP reshaping, and the analysis of bias and regret is conducted.\n* Extensive experiments empirically demonstrate that HUBL is indeed an effective enhancement technique."
            },
            "weaknesses": {
                "value": "* **Presentation**:\n    * The presentation of the experimental results in the graphs lacks clarity. The absence of a horizontal baseline at 0 makes it unclear whether there's an improvement or decline. I believe a horizontal baseline at 0 should be added, and different colors could be considered to depict increases and decreases.\n    * The experimental tables in the appendix have a similar problem. The best performances should be bolded for easier readability.\n* **Limitations**:\nAs the authors discussed in the limitations section, offline datasets based on disconnected transition tuples are challenging to utilize with the HUBL trick unless heuristic values are computed during the construction of datasets."
            },
            "questions": {
                "value": "* Please improve the presentation of figures and tables in the paper as i mentioned above.\n* I've noticed that on tasks where baseline offline RL algorithms already perform well, HUBL might decrease the performance. Is there a way to ensure that its enhancements are consistently non-negative?\n* On D4RL, HUBL shows significant improvements over baseline algorithms for the hopper task several times. I wonder whether there exist any shared or general characteristics in situations where HUBL offers substantial advantages. Could this be discussed, or did I perhaps overlook any mention of this aspect?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7779/Reviewer_8Sx2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7779/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839507193,
        "cdate": 1698839507193,
        "tmdate": 1699636949847,
        "mdate": 1699636949847,
        "license": "CC BY 4.0",
        "version": 2
    }
]