[
    {
        "id": "bHHqWivV2g",
        "forum": "K98byXpOpU",
        "replyto": "K98byXpOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_RgCX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_RgCX"
        ],
        "content": {
            "summary": {
                "value": "This paper considers bi-level optimization problems with constrained lower-level problems (LCBO). A single-loop method is proposed to solve the LCBO, which returns an approximately stationary point with a non-asymptotic convergence rate. The main technique is to use the Gaussian smoothing to approximate the hypergradients. Moreover, momentum methods are applied to update both the upper and lower-level variables. Numerical experiments show the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is a single-loop algorithm, which is more efficient than the existing methods.\n2. The application of the Gaussian smoothing is new to me. This provides new insights for solving the LCBO."
            },
            "weaknesses": {
                "value": "1. The technical analysis is not sound. It is not clear why $F$ is differentiable (e.g., in Lemma 5) and how Assumption 3 works. Indeed, for a simple example of LCBO, $F$ can be non-differentiable at some points. For example, $F(x)=|x|$ in the following problem is not differentiable at $x=0$\n$$\n\\min_{x,y} -xy \\text{ s.t. } y\\in\\arg\\min_{z\\in[-1,1]}xz.\n$$ \nHowever, these kinds of cases are not discussed in the paper. As a consequence, Lemma 5 and (10) are not convincing.\n\n2. Assumption 4 is also strange to me. More discussions are needed for this kind of boundedness assumption."
            },
            "questions": {
                "value": "1. Why is the convergence rate only related to $d_2$ but not $d_1$?\n\n2. In the experiments, are the lower-level constraints active at the solution returned by the proposed algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Non"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697683448867,
        "cdate": 1697683448867,
        "tmdate": 1699636298099,
        "mdate": 1699636298099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hWsbI4pbrx",
        "forum": "K98byXpOpU",
        "replyto": "K98byXpOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_voeq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_voeq"
        ],
        "content": {
            "summary": {
                "value": "To address lower-level constrained bilevel optimization problem, the authors leverage the Gaussian smoothing to approximate the hypergradent. Furthermore, the author proposes a single-loop single-timescale algorithm and theoretically prove its convergence rates. Two experimental settings have been tested to demonstrate the superiority of proposed algorithm."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The experimental results are great for proposed algorithm."
            },
            "weaknesses": {
                "value": "1. The proposed algorithm DMLCBO is based on double momentum technique. In previous works, e.g., SUSTAIN[1] and MRBO[2], double momentum technique improves the convergence rate to $\\mathcal{\\widetilde O}(\\epsilon^{-3})$ while proposed algorithm only achieves the $\\mathcal{\\widetilde O}(\\epsilon^{-4})$. The authors are encouraged to discuss the reason why DMLCBO does not achieve it and the theoretical technique difference between DMLCBO and above mentioned works.\n\n2. In the experimental part, the author only shows the results of DMLCBO in early time, it will be more informative to provide results in the later steps.\n\n3. In Table 3, DMLCBO exhibits higher variance compared with other baselines in MNIST datasets, the authors are encouraged to discuss more experimental details about it and explain the behind reason.\n\n\n[1] A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum\n[2] Provably Faster Algorithms for Bilevel Optimization"
            },
            "questions": {
                "value": "Check the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Reviewer_voeq"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700990965,
        "cdate": 1698700990965,
        "tmdate": 1699636297939,
        "mdate": 1699636297939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5RKiRQ1qx7",
        "forum": "K98byXpOpU",
        "replyto": "K98byXpOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_VJdq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_VJdq"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a bilevel optimization problem in which the lower-level problem has a convex set constraint which is independent of the upper-level variable. Using Gaussian smoothing to approximate the gradient of the projection operator, the authors propose an approximation to the hypergradient and a single-loop algorithm. Theoretical analysis and numerical experiments are provided."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed algorithm is a single-loop single-timescale approach."
            },
            "weaknesses": {
                "value": "1. Assumption 3 is restrictive to satisfy. Furthermore, even the problems examined in the numerical experiments fail to meet this assumption.\n\n2. In order to achieve a stationary point with $\\|| \\nabla F (x) \\|| \\le \\epsilon$, as outlined in Remark 2, the proposed algorithm necessitates a choice of the smooth parameter on the order of $O(\\epsilon d_2^{-3/2})$. Consequently, the algorithm would require a minimum of approximately $\\tilde{O}(d_2^8/\\epsilon^8)$ iterations. It appears, however, that the authors aim to obscure this fact within their paper and retain the smooth parameter in their complexity result.\n\n3. The problems explored in the numerical experiments may not necessarily adhere to the strongly convex assumption for the lower-level problem stipulated in Assumption 2 (3). Moreover, the selection of values for the parameters $Q$ and $\\eta$ does not align with the theoretical requirements specified in Theorem 1."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Reviewer_VJdq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829352046,
        "cdate": 1698829352046,
        "tmdate": 1699636297836,
        "mdate": 1699636297836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NJFGjDIsJg",
        "forum": "K98byXpOpU",
        "replyto": "K98byXpOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_AArG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3454/Reviewer_AArG"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a novel hypergradient approximation method for lower-level constrained bilevel optimization problems with non-asymptotic convergence analysis, utilizing Gaussian smoothing. This method incorporates double-momentum and adaptive step size techniques. The experimental results, in the context of data hyper-cleaning and training data poisoning attacks, showcase the efficiency and effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The work is well motivated. Finding a simple yet effective method for lower-level constrained bilevel optimization problems is both interesting and important.  \n\nS2. The paper is well written and easy to follow. The algorithm design is new and non-asymptotic convergence is provided.\n\nS3. The authors conduct numerous experiments to showcase the efficiency and effectiveness of the proposed approach. Additionally, the paper includes several ablation studies in the Appendix."
            },
            "weaknesses": {
                "value": "W1. By Remark 2 on page 7, $\\tilde{\\mathcal{O}}(\\frac{\\sqrt{d_2}}{\\delta K^{1/4}})\\leq \\epsilon$ implies that $K=\\tilde{\\mathcal{O}}(\\frac{d_2^2}{\\delta^4 \\epsilon^4})$, NOT $\\tilde{\\mathcal{O}}(\\frac{\\sqrt{d_2}}{\\delta \\epsilon^4})$. Additionally, since $\\delta=\\mathcal{O}(\\epsilon d_2^{-3/2})$ by (11), the iteration number $K=\\tilde{\\mathcal{O}}(\\frac{d_2^8}{\\epsilon^8})$.\n\nW2. The authors should consider comparing their method with closely related papers addressing lower-level constrained bilevel optimization problems, including:\n\n[1] Han Shen, Tianyi Chen. \u201cOn Penalty-based Bilevel Gradient Descent Method.\u201d ICML 2023.\n\nW3. Since there is an additional loop to approximate the matrix inverse, it can be noted that the proposed algorithm DMLCBO is not fully single-loop."
            },
            "questions": {
                "value": "Q1. Could you provide some representative class of problems that satisfy Assumption 3? Consider the simple example: $g(x,y)=(y-x)^2/2$, $\\mathcal{Y}=[-1,1]$ and $\\mathcal{X}=[-3,3]$. The projection operator $P_Y$ has a closed-form solution, but $\\mathcal{P}_{\\mathcal{Y}}(z^*)$ is not continuously differentiable in a neighborhood of $z^*$ when $x=1$ or $x=-1$.\n\nQ2. Is Assumption 3 satisfied for all small values of $\\eta$? \n\nQ3. What measures can be taken to verify that Assumption 4 is satisfied, or are there specific checkable sufficient conditions to ensure its validity?\n\nMinor Comments:\n\n(1)On page 3, in Equation (4): The minus sign in the expression of $\\nabla y^*(x)$ was omitted.\n\n(2)On page 4, in Remark 1: What is $F_{\\delta}(x)$?\n\n(3)On page 5, in Lemma 2: \u201c$\\| A \\| \\leq 1$\u201d should be \u201c$\\| A \\| < 1$\u201d. Make similar changes after Lemma 2. \n\n(4)On page 5, in Equation (9): By the definition $c(Q)$, the term $u^Q$ in $\\bar{\\xi}$ is not used.\n\n(5)On page 6, in Algorithm 1: swap the positions of $v_1$ and $w_1$. Should \u201c$g(x_1,y_1)$\" be replaced with \u201c$\\nabla_y g(x_1,y_1)$\"? Make similar changes in Section 3.3. \n\n(6)On page 6, in line 3 from below: Should \u201c$\\nabla F_{\\delta}(x_k)$\u201d be replaced with \u201c$\\nabla F (x_k)$\u201d?\n\n(7)On page 7, Lemma 5: The absolute value symbol for $\\mathcal{G}$ in Equation (10) was omitted. Should \u201c$\\nabla f(x_k, y_k)$\u201d be replaced with \u201c$\\nabla_x f(x_k, y_k)$\u201d? Additionally, what is $\\tilde{x}_{k+1}$?\n\n(8)On page 8, in line 3 from below: correct the sum within the max part.\n\n(9)On page 9, Do the bilevel optimization problems related to training data poisoning attacks satisfy the smoothness and convexity assumptions?Note that \u201ca network with two convolution layers and two fully-connected-layer layers for MNIST and a network with three convolution layers and three fully-connected-layer layers for Cifar10, where the Relu function is used in each layer.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3454/Reviewer_AArG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3454/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699251851759,
        "cdate": 1699251851759,
        "tmdate": 1699636297765,
        "mdate": 1699636297765,
        "license": "CC BY 4.0",
        "version": 2
    }
]