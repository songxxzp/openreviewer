[
    {
        "id": "gPsA0B1oNq",
        "forum": "bjFJrdK0nO",
        "replyto": "bjFJrdK0nO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission967/Reviewer_RzKy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission967/Reviewer_RzKy"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an innovative framework that leverages viewpoint information to enhance the controllability of an image editing system. The approach consists of three main components: Large Language Model (LLM) Planner, Pose Estimation and Synthesis, and Image Synthesis.This work provides an approach for object modifications while preserving the visual coherence of the entire composition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written.\n\nThe proposed task is both intuitive and has practical applications.\n\nThe experiments show compelling performance."
            },
            "weaknesses": {
                "value": "The lighting estimation, crucial in 3D object integration, solely relies on the diffusion model's generative priors. Considering the complexities distinguishing indoor and outdoor lighting, can the authors provide more outdoor examples?  Specifically, is the model capable of rendering images with realistic lighting effect (diffuse and specular)?  How does its performance compare with a physics-based model on this scene? E.g., cars on streets or buildings during sunset.\n\nAlthough the paper claims to satisfy the consistency of the object shape, certain details are still modified, e.g., the text on the hat in Fig. 1. While this issue might be intrinsic to Stable Diffusion, the 3D realm demands stricter shape fidelity. Could author provide any improvement for it\uff1f\n\nThe paper could benefit from showcasing more qualitative results, emphasizing the model's controllability. E.g, how does the model perform when provided with varying textual descriptions to generate a chair in different poses within a living room?"
            },
            "questions": {
                "value": "This framework uses an LLM planner for text guidance. How do the model obtain theses text conditions during testing? Using LLM expand relative camera view labels into sentences?\n\nHow does this work address conflicting conditions? E.g., when provided text descriptors (like sofa color or shape) diverge from the original reference, what would be the model's output?\n\nInserted objects seem basic. Are there any example involving more intricate subjects, e.g., Pikachu, a Picasso painting, or a badminton-playing prince?\n\nI am glad to raise my rating once concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission967/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission967/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission967/Reviewer_RzKy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697998153674,
        "cdate": 1697998153674,
        "tmdate": 1699636022590,
        "mdate": 1699636022590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GteRzPWq4g",
        "forum": "bjFJrdK0nO",
        "replyto": "bjFJrdK0nO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission967/Reviewer_j7Dh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission967/Reviewer_j7Dh"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to improve the performance of reference-based image editing by explicitly integrating viewpoint information.\n\nThe main contribution is a novel system that combines LLM, diffusion models and a camera pose estimation network to achieve high-quality image editing results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of leveraging viewpoint information for reference-based object editing in images is novel and interesting.\n\n2. The result shown in the paper looks promising, and the improvement over the SOTA is obvious."
            },
            "weaknesses": {
                "value": "1. The scale of technical novelty is limited. The whole system looks like an ensemble of various existing approaches (e.g., GPT-4, Segment-Anything, Zero-123, and ControlNet), with any significant modification to them. While I do acknowledge the engineering effort that the authors have put into carefully selecting proper components and figuring out a reasonable way to compose them together, the novelty of the paper, from a technical perspective, is insufficient for ICLR.\n\n2. The evaluation is incomplete. First, the results shown throughout the paper are all on indoor scenes. An experiment should be done to test the ability of the proposed method in handling outdoor images, which is not included in the current paper. Second, only two out of the three dimensions including consistency, controllability and harmony are considered in the experiments, as shown in Table 2.  The controllability of different methods should also be tested, e.g., through a human evaluation where subjects can be asked to tell how well the pose of the synthesized object aligns with that of the corresponding object in the source image."
            },
            "questions": {
                "value": "1. In Fig.2, what is the personalized pre-trained diffusion model? What is the trainable \u201cpersonalization\u201d module and how is it trained?\n\n2. Can the system work well if there are more than one object to be replaced in the source image? For example, what about if the source image in Fig. 2 contains two laptops on the table?\n\n3. What dataset is used for the comparisons in Sec. 4.2\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752840024,
        "cdate": 1698752840024,
        "tmdate": 1699636022521,
        "mdate": 1699636022521,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zZlH7czOHx",
        "forum": "bjFJrdK0nO",
        "replyto": "bjFJrdK0nO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission967/Reviewer_pMRb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission967/Reviewer_pMRb"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method for inserting an object into a scene based on a user-defined view. This method consists of three steps: First, using the Large Language Model (LLM) Planner to identify the object and its pose from user input. then, using a segmentation network to isolate the object and determine its pose, followed by pose synthesis that respects user-specified views. Finally, using a personalized diffusion model and ControlNets for the final image synthesis, ensuring the object blends naturally with the scene. This process aims to improve the composition effects using precise pose estimation and optimal view conditions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper treads on a fresh ground by presenting a new and practical problem setting, which always adds value by opening up new directions for research. By merging existing works, the paper provides a comprehensive approach to tackle the problem."
            },
            "weaknesses": {
                "value": "1. While the problem setting is fresh, the technique to address it lacks originality, relying heavily on previously established works without introducing significant innovative components or showing strong technical contributions.\n2. The proposed pose estimation method, based on the relative camera rotation matrix and translation vector, is rudimentary. Its simplistic model might effect its generalization in a practical settings. For example in figure 1 and 3, all the reference photos are in roughly the same orientations (e.g. 30 degrees left forward facing) and the source objects are either in the same orientations with the reference or forward facing directions. The paper might not have sufficiently demonstrated the effectiveness of their method in diverse scenarios."
            },
            "questions": {
                "value": "1. Considering the similar orientations in figures 1 and 3, how does the method perform with objects in diverse orientations? Would it be possible to provide more challenging examples or demonstrate the same object transformed under various orientations and scales?\n2. the performance of pose estimation shows a RMSE value of 9.7, even in the best scenario as shown in table 3, seems relatively high. Is there room for improvement? \n3. In section 'EFFECTS OF VIEW CONDITIONS', suggesting that predictions are marginally affected within a 20-degree range, is perplexing. This statement seems to undermine the paper's central argument, suggesting that pose estimation and view conditions may not be as critical or even necessary. Could the authors elaborate on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824011427,
        "cdate": 1698824011427,
        "tmdate": 1699636022444,
        "mdate": 1699636022444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C4NuTfvisK",
        "forum": "bjFJrdK0nO",
        "replyto": "bjFJrdK0nO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission967/Reviewer_H34R"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission967/Reviewer_H34R"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new framework for image synthesis that integrates view conditions to enhance the controllability of image editing tasks. The framework satisfies three essential criteria for an effective image editing method: consistency, controllability, and harmony. The paper surveys existing object editing methodologies and distills these criteria, which should be met for an image editing method. The paper describes the various processes involved in the framework, including object and angle extraction, text prompt processing, reference object synthesis, and image synthesis. The paper also presents evidence of the framework's superior performance across multiple dimensions through comprehensive experiments and comparisons with state-of-the-art methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The framework integrates view conditions to enhance the controllability of image editing tasks, allowing for precise object modifications while preserving the visual coherence of the entire composition.\n\n- The framework satisfies three essential criteria for an effective image editing method: consistency, controllability, and harmony.\n\n- The paper presents evidence of the framework's superior performance across multiple dimensions through comprehensive experiments and comparisons with state-of-the-art methods."
            },
            "weaknesses": {
                "value": "- I think the title of this paper does not really satisfy its content. \"View-conditioned image synthesis\" normally refers to controlling the camera view of the entire image, but the proposed method actually sounds more like editing local regions with view conditions. It is more like a system design and the current title does not fit it well.\n\n- To put the new object in the original scenes with the same 6D pose, the authors proposed to use a pose estimation module. I have several related questions regarding this part. \n    1) Does this pipeline require the new object to be presented in the canonical view? It seems that the pose estimator only produces relative camera parameters.\n    2) Why does the generated image in the presented results always follows the 6D pose of input images? What if we choose a different pose? Will it fail the framework?\n    3) In most results, the newly added object and the existing object are from the same category. What about different categories?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission967/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699345649784,
        "cdate": 1699345649784,
        "tmdate": 1699636022376,
        "mdate": 1699636022376,
        "license": "CC BY 4.0",
        "version": 2
    }
]