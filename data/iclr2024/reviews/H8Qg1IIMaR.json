[
    {
        "id": "CmRCW5vlUO",
        "forum": "H8Qg1IIMaR",
        "replyto": "H8Qg1IIMaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
        ],
        "content": {
            "summary": {
                "value": "This paper explores and experiments with the permutation attack on current large vision-language models for multiple-choice question answering (MCQA). The authors show that these models are susceptible to adversarial permutations in the answer sets for multiple-choice prompts. This is concerning because the models should be invariant to the permutations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Well written\n- Interesting and important area of work that many are overlooking \n- An important area to look at specially for industry wide adaptation of large vision-language models. \n- Experiments are easy to understand"
            },
            "weaknesses": {
                "value": "1. I don't see any related work section. That would have made the work more sound\n2. The comparison tables on LLMs seems to be inconsistent. For example Table 3 has GPT-3.5 turbo but Table 5, 6 (that are also comparison on LLM) do not have GPT-3.5 turbo\n3. On the experimentation section, it would be good if the authors described some more details. For example - did they used model APIs or model weight's for testing? This would give us an idea on the consistency of experiments"
            },
            "questions": {
                "value": "1. In Table 3 for GPT3.5 turbo, how did the authors do the testing? Is it using OpenAI API?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_FKE2"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698338598104,
        "cdate": 1698338598104,
        "tmdate": 1699636595176,
        "mdate": 1699636595176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8uD5iPRv9o",
        "forum": "H8Qg1IIMaR",
        "replyto": "H8Qg1IIMaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_EvuZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_EvuZ"
        ],
        "content": {
            "summary": {
                "value": "The paper aims at the vulnerabilities of large language and vision-language models, specifically concerning permutations in multiple-choice question answering (MCQA). The authors conduct experiments that reveal performance degradation when models are exposed to permutation-based attacks, even though these models have shown capabilities in various tasks. The findings underscore the need for a deeper analysis of robustness before deploying such models in real-world applications."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper addresses the robustness of widely used models, a pertinent topic given the real-world deployment of these models."
            },
            "weaknesses": {
                "value": "1. The methodology's depth and novelty are not entirely clear. More analysis could be provided on how natural variations in the way are ordered may impact models.\n\n2. While the paper focuses on permutation-based vulnerabilities, it might benefit from a broader discussion on other potential vulnerabilities or comparisons to other attack methods.\n\n3. The experiments provided are on a specific dataset. It would be beneficial to see how the models fare on diverse datasets to ensure the findings, which are not specific to one dataset's characteristics."
            },
            "questions": {
                "value": "See the above weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676030205,
        "cdate": 1698676030205,
        "tmdate": 1699636595065,
        "mdate": 1699636595065,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rwQM7vZu2N",
        "forum": "H8Qg1IIMaR",
        "replyto": "H8Qg1IIMaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
        ],
        "content": {
            "summary": {
                "value": "This paper examines permutation sensitivity in MCQA for generative language models and vision-language models. They show that a wide range of models display severe permutation sensitivity in a variety of MCQA benchmarks, and show that existing mitigation strategies do not solve permutation sensitivity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "A large number of language models and vision language models are used in the experiments. The datasets chosen are diverse and there are enough datasets to draw strong conclusions. \n\nThere is concurrent work in the area (pointed out by the authors themselves), but the broadness of the experimental evaluation is original. While limited in scope to MCQA, it is significant, as it may point to underlying problems in LLM reasoning that cannot be easily fixed."
            },
            "weaknesses": {
                "value": "I think it's overclaiming to call this an \"adversarial\" attack. Permuting the choices is done as a matter of course in evaluation (see Section 4.1 in [1]). \n\nAdditionally, I think important context is missing. We know parameters like the temperature and the sampling strategy have a significant effect on output. But I don't see these numbers reported. Do the numbers change if we reduce / increase the temperature?\n\nThere's no discussion on prompting. Does providing in-context examples effect permutation sensitivity? In-context examples for MCQA should always be available. They also provide a convenient way to alter the posterior distribution (for example, you could set the answer to always be the last answer in the in-context examples; would the positional bias then change?)\n\nAlso, no information is reported on the model perplexity / confidence during permutation. It may be possible that certain permutations have much lower perplexity / higher confidence, and hence the model's answer on those is more trustworthy. So in practice, we might evaluate the model on all the permutations, then select the model's most confident answer. Of course, if we can do this, it is better to avoid MCQA entirely and have the model assess the answers one-by-one.\n\n[1] MMBench: Is Your Multi-modal Model an All-around Player?"
            },
            "questions": {
                "value": "Please see the weaknesses section. I am willing to raise my rating if the weaknesses are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5690/Reviewer_RBKq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716254074,
        "cdate": 1698716254074,
        "tmdate": 1700715162695,
        "mdate": 1700715162695,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YSdj47oHH2",
        "forum": "H8Qg1IIMaR",
        "replyto": "H8Qg1IIMaR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_d9tg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5690/Reviewer_d9tg"
        ],
        "content": {
            "summary": {
                "value": "This paper reveals an interesting phenomenon of existing LLM and VLLMs: they are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper reveals an interesting phenomenon:  they are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which they should not be ideally.\n\n2. Experiments are conducted across different multiple LLM / VLLMs, demonstraing the universality of the phenomenon."
            },
            "weaknesses": {
                "value": "While the finding is interesting, I wonder if authors could provide any intuitive explanations on the observation? Position bias (Zheng et al., 2023a) may not be able to fully explain this phenomenon, but it can be potentially one of the reasons why they are vulnerable to the adversarial permutation in answer sets from my understanding. I am willing to see more analysis on the potential causes of this observation. Can the explanations on similar problems in other tasks besides MCQA apply to this case\uff1fAuthors may put more efforts on it."
            },
            "questions": {
                "value": "Please refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5690/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814841908,
        "cdate": 1698814841908,
        "tmdate": 1699636594835,
        "mdate": 1699636594835,
        "license": "CC BY 4.0",
        "version": 2
    }
]