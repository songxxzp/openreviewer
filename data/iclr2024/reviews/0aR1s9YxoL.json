[
    {
        "id": "89rMrft0Hi",
        "forum": "0aR1s9YxoL",
        "replyto": "0aR1s9YxoL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the plasticity loss problem in visual reinforcement learning. By using Fraction of Active Units (FAU) as a metric to measure the plasticity loss, several observations are concluded:\n- Data augmentation is critical.\n- The critic part in the actor-critic framework suffers from the plasticity loss most.\n- Plasticity loss in early states is irrevocable.\n\nThe authors also propose a technique to mitigate the plasticity loss issue, by using adaptive replay ratio.  Experiments are mainly conducted across 6 DMC tasks, showing the improvements of the proposed technique."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Nice paper writing**. The motivation, experiments, and conclusions from observations are well stated, making this paper interesting to read.\n- **Good ablations**. The careful experiments on different modules and stages are interesting, which could give some insights into Visual RL.\n- **Interesting measurement**. The select measurement (FAU) is interesting. Though similar things have been well studied in pure deep RL [1,2], this work might be an early work to study the activation units in visual-based deep RL.\n\n[1] Nikishin, Evgenii, et al. \"The primacy bias in deep reinforcement learning.\" ICML 2022.\n\n[2] Sokar, Ghada, et al. \"The dormant neuron phenomenon in deep reinforcement learning.\" ICML 2023."
            },
            "weaknesses": {
                "value": "- **Limited insights revealed**. As the analysis in this work shows, the most critical factor for visual RL is **data augmentation (DA)**. However, this has been a very well-known factor for the community, and a quite large amount of recent works have extensively studied DA in visual RL. I think this work mainly gives a new perspective to explain the effectiveness of DA. This is certainly interesting, but considering the metric is directly from the previous works and similar measurements have been also extensively studied, applying FAU simply to a well-known critical factor does not give enough insights to the community.\n\n- **Limited technical contributions**. The only technical contribution of this work is to apply a low replay ratio and a high replay ratio in different stages. This is simple yet not effective enough to support its simplicity and also is not very related to the entire story that this work tells.\n\n- **Limited evaluations**. The experiments are only conducted on 6 DMC tasks, and therefore the diversity in domains and tasks are both limited. It could be good to see the observation and the technique are more universal, considering their simplicity.\n\nThough I like the analysis given in this work, I think this paper is not qualified for the acceptance of ICLR, with the reasons above."
            },
            "questions": {
                "value": "See *weakness* above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_c6tM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698579067146,
        "cdate": 1698579067146,
        "tmdate": 1700740619799,
        "mdate": 1700740619799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qZSIU3HmIh",
        "forum": "0aR1s9YxoL",
        "replyto": "0aR1s9YxoL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_m8KG"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the interaction between data augmentation, replay ratios, and dormant neurons in deep reinforcement learning. In particular, it demonstrates a striking similarity in the efficacy of data augmentation and resets in preserving plasticity in deep RL agents, and shows that data augmentation mitigates a decline in the fraction of active units in the network. It further shows that the fraction of active units can also be used to design an adaptive replay ratio scheme which tailors the replay ratio to the phase of learning in the network."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is an enjoyable read. It is very well-written and presented; claims and evidence are clear and easy to follow, and the figures are clearly explained and easy to interpret\n  - The paper offers a fresh take on plasticity -- I wouldn't have predicted that data augmentation would be such an effective regularizer, and this phenomenon is not an obvious conclusion from past literature.\n  - The study of manipulation tasks presents a nice counterpoint to prior work on plasticity which has focused largely on Atari games.\n  - I appreciated the comparisons against a variety of interventions in Figure 2, highlighting the effect size of data augmentation in contrast to other regularizers that had been studied in the literature.\n  - The work is able to ground its evaluation of plasticity not only in the performance of the agents on the RL tasks, but also using the Fraction of Active Units metric, which gives some insight into what is going on inside the network. This is useful because performance-only metrics are unable to disentangle plasticity from other factors in the RL training process such as exploration and generalization.\n  - The idea of adapting the replay ratio based on the stability of the network's training process makes a lot of sense and, despite the potential issues I highlight below with this particular instantiation, could be a generally useful quantity to keep track of.\n  - Section 5, which identifies \"critical periods\" in the training process, is particularly insightful as it suggests that regularization can be modulated throughout training"
            },
            "weaknesses": {
                "value": "- The paper highlights the FAU as a major indicator of plasticity in the network, but glaringly omits evaluations of ReDO as a baseline. \n    - The domains studied in this paper are all from similar environments, and I'm not sure how much this paper is telling us about general properties of data augmentation, replay ratios, and plasticity, vs how much it is telling us about the interaction between these things in a specific class of robotic manipulation tasks. \n    - The causality around FAU and plasticity is quite vague in this paper. I wasn't sure whether it was claiming that the FAU is a *symptom* of plasticity loss, and that DA and ARR are affecting some hidden causal factor which reduces both plasticity loss and increases FAU, or whether the claim was that *by maintaining the FAU* these interventions are able to avoid plasticity loss.\n    - Data augmentation has the opposite on FAU in the actor and the critic, but it is not explained why a higher FAU would be beneficial in the actor but not the critic. In particular, data augmentation *dramatically* reduces the FAU in the actor, without seeming to have any ill effects. This seems to suggest that FAU might not be a particularly useful measure of plasticity for all learning problems, or at least that the story isn't as simple as that described in the paper.\n    - The specific mechanism by which data augmentation is helping to prevent plasticity loss is unclear. While it is observed to correlate with a reduction in the number of dormant neurons, it isn't obvious whether this is a causal mechanism I would like to see an evaluation of a few data augmentation classes on at least one environment to see how the choice of data augmentation influences its effect on plasticity.\n    - Minor nit: the claim that using a fixed encoder network means that the effect of plasticity on the representation is completely eliminated is too strong, as there is still some representation learning happening in the actor and critic MLPs. This should be noted in the paper text."
            },
            "questions": {
                "value": "- The definition of FAU is a bit unclear: is it measuring average activity of neurons over a large batch, i.e. the sparsity of the network's representation, or is the expression inside the indicator function nonzero if the unit is active for *any* input, i.e. FAU =  1 - (fraction of dormant neurons)?\n- The replay ratio adaptation method is not described in sufficient detail. How specifically is the \"recovery of plasticity\" measured? How robust is the method to different variations on this measurement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632278388,
        "cdate": 1698632278388,
        "tmdate": 1699636425294,
        "mdate": 1699636425294,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NZm0clqzQh",
        "forum": "0aR1s9YxoL",
        "replyto": "0aR1s9YxoL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the issue of plasticity loss where RL agents struggle to learn from new experiences after being trained on non-stationary data. The authors suggest three key findings. Firstly, they discovered that data augmentation (DA) holds greater significance in alleviating plasticity loss compared to reset. Secondly, their extensive experiments demonstrate that the bottleneck for training lies in the plasticity loss of the critic module. Lastly, restoring the critic's plasticity to an adequate level in the early stages is an important factor. They analyzed plasticity of the neural network for the static high Replay Ratio (RR) during RL agent training and based on their findings, they introduced an adaptive replay ratio which shows superior performance compared to its static counterpart and effectively addresses the high RR dilemma."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe paper is well-written and easy to follow.\n\n-\tTheir suggested method, adaptive RR, is straightforward and intuitive. The results demonstrate promising potential for the proposed approach.\n\n-\tWell-established experiments demonstrated the effectiveness of DA and the importance of Critic's plasticity."
            },
            "weaknesses": {
                "value": "-\tTheir proposed methodology, adaptive RR, is based on the analysis of the correlation between replay ratio and plasticity, which is relatively unrelated to the DA. This makes main message of this paper unclear. Are the authors emphasizing the importance of DA or are the authors emphasizing the importance of adaptive RR? The manuscript would benefit if it were to make their main message clearer by aligning their findings and their proposed method. It seems that although the findings regarding the relationship between DA and reset is intriguing, the main method is unrelated with their explanation regarding the effectiveness of DA.\n\n-\tTheir experiments on data augmentation w/ and w/o reset contradict existing research. According to Nikishin et. al. [1], they conducted a comparison between DrQ w/ reset and DrQ w/o reset, finding that the DrQ w/ reset demonstrated superiority over DrQ w/o reset (Check Figure 18 in [1]). Perhaps the results driven by the author are driven by the high variance of data augmentation w/o reset as in [1] or due to the difference in the DMC environments selected for evaluation. I have intentions to raise my score if the authors are able to show experimental results that their findings regarding the relationship between DA & reset also holds for other DMC environments. \n\n-\tIn addition, the current replay ratio values utilized for adaptive RR seems to be limited to a small range (e.g., [0.5, 1, 2]). In order to properly evaluate the efficacy of the proposed adaptive RR method, the authors should conduct further experiments for RR=4 and RR=8. Although higher RR does require longer training time and can be computationally expensive, it is essential for the authors to provide the following results in order to verify whether the proposed adaptive RR is indeed superior compared to static RR. It will also be best if the authors are able to provide results for high RR & reset & DA to further validate their findings that DA is indeed better off along and not paired with reset.\n\n-\tAs the authors note as a limitation, their experimental environment is confined to DMC, and they only demonstrate the effectiveness of Adaptive RR under basic configurations. However, it appears that the paper can provide a promising potential for a new methodology to deal with plasticity loss.\n\n[1] The Primacy Bias in Deep Reinforcement Learning, Nikshin et. al., ICML 2022"
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_Kom1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815997188,
        "cdate": 1698815997188,
        "tmdate": 1700635594621,
        "mdate": 1700635594621,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OzenJ56N6N",
        "forum": "0aR1s9YxoL",
        "replyto": "0aR1s9YxoL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D"
        ],
        "content": {
            "summary": {
                "value": "This paper scrutinizes plasticity loss in visual reinforcement learning (VRL) by exploring data augmentation (DA), agent modules, and training stages. It reveals that DA is pivotal in mitigating plasticity loss, particularly within the critic module of VRL agents, which is identified as the main contributor to the loss of plasticity. To this end, the paper proposes an Adaptive Replay Ratio (RR) strategy that adjusts the RR based on the critic module's plasticity, significantly enhancing sample efficiency over static RR methods, as validated on the DeepMind Control suite."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper offers a novel perspective on the loss of plasticity in a visual reinforcement learning setup which includes:\n\n-  **Analysis of Data Augmentation:**  A significant strength of this paper is its thorough analysis of why data augmentation is so effective in reinforcement learning. By conducting extensive experiments and analysis, the paper sheds light on the underlying mechanisms through which data augmentation enhances plasticity, particularly in visual reinforcement learning settings. This nuanced understanding helps bridge the gap between empirical success and theoretical underpinnings, providing valuable insights for future algorithm design.\n-  **Module-Specific Analysis of Plasticity Loss:** Another strength is the paper's detailed dissection of plasticity loss across different agent modules, namely the encoder, actor, and critic. The authors' methodical comparison uncovers that the critic module is the primary culprit behind plasticity loss, challenging previous assumptions about the encoder's role. \n- **A simple method to tackle the plasticity loss:** By proposing a simple, adaptive method to modulate the update ratio based on plasticity metrics, the paper presents a novel approach that improves upon the static replay ratio. This innovation has the potential to enhance the sample efficiency of diverse reinforcement learning models by aligning training intensity with the model's current capacity to learn."
            },
            "weaknesses": {
                "value": "This paper offers interesting insights and experimental results within the domain of visual reinforcement learning (VRL). Despite this, there are aspects that call into question the robustness of the findings:\n\n- **Metrics-Related Concerns:** The primary metric used to assess plasticity loss, the fraction of active units, might be inadequate. The reliance on this metric is questionable since different activation functions such as CReLU and LeakyReLU can inherently bias the number of active units. In addition, as noted by Lyle et al. [1], fraction of active units does not fully encapsulate plasticity dynamics. I recommend that the authors broaden their analysis with additional metrics\u2014like feature rank, weight norms, or even the curvature of the loss surface (e.g., the Hessian's rank)\u2014to substantiate their claims more convincingly. While no single metric may definitively quantify plasticity loss, a consistency of findings across multiple measures would strengthen the empirical foundation of their arguments.\n\n- **Analytical Concerns:** The paper claims the indispensability of data augmentation (DA) in mitigating plasticity loss, but it lacks a comprehensive analysis of why DA is superior. Other methods like layer normalization may have similar effects; what happens when these are combined, as done by Lee et al.? The paper would benefit from deeper exploration into why DA outperforms other methods. Providing a theoretical framework or empirical evidence for DA's efficacy in reducing plasticity loss would be a valuable addition to the field, potentially guiding future development of more robust VRL systems.\n\n- **Applicability Concerns:**  I recognize that calling for additional experiments might seem an unfair request, as it can be levied against any paper. However, I do believe that extending the empirical validation to include at least one more analysis would greatly benefit the paper's credibility. It would be enlightening to see how the findings translate to non-actor-critic methods or across diverse benchmarks. For instance, assessing the proposed adaptive replay ratio method on a well-established sample-efficiency benchmark such as Atari-100k, with Rainbow algorithm, would improve the practical significance of the adaptive RR method. Without such additional validation, the scope and applicability of the conclusions remain in question.\n\nI'm willing to increase the score if the authors address the outlined limitations.\n\n[1] Understanding plasticity in neural networks., Lyle et al., ICML 2023.\n\n[2] PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning., Lee et al., NeurIPS 2023."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4493/Reviewer_7W3D",
                    "ICLR.cc/2024/Conference/Submission4493/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4493/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698922624684,
        "cdate": 1698922624684,
        "tmdate": 1700572562343,
        "mdate": 1700572562343,
        "license": "CC BY 4.0",
        "version": 2
    }
]