[
    {
        "id": "clGzIcrXOy",
        "forum": "bC50ZOyPQm",
        "replyto": "bC50ZOyPQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_avag"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_avag"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed REcurrent ADaption (READ), a lightweight and memory-efficient fine-tuning method for pre-trained foundation models. READ inserts a small RNN network alongside the frozen backbone model, which is trained for downstream tasks. READ can achieve comparable or better accuracy compared to existing parameter-efficient fine-tuning methods on the GLUE benchmark, using less training memory and energy consumption."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Firstly, the paper is generally well-written and easy to follow.\n2. There is no need for an extra step pre-training the side network due to the compact design, making the transfer learning pipeline simple. \n3. The proposed method achieves a better accuracy-energy trade-off compared to existing methods on GLUE."
            },
            "weaknesses": {
                "value": "1. The writing quality should be improved. For example, Appendix?? and Table ??  in Sec 4. \n2. The experiments are quite restricted, only using T5 on GLUE benchmarks. It would be better to evaluate more model architectures like GPT-style (if LLaMA is too expensive for the hardware setup, maybe evaluate smaller ones like GPT-2) and more tasks. \n3. The latency evaluation in Figure 6 is confusing: why are BitFit and LoRA slower compared to vanilla fine-tuning? Both methods do not introduce extra parameters into the base model (LoRA weights can be fused). It is not reasonable that READ is faster than BitFit/LoRA. \n4. What is the non-recurrency setting in Table 2? Why are there more training parameters?\n5. The power statistics from NVIDIA's smi are highly unstable. Have you found the calculation to be stable based on a minute-level sampling?\n6. One good thing about transformers is the better training parallelism compared to RNN models. Does the design prevent parallel training due to the recursive nature? (I think it can still be parallelized if using the vanilla RNN)"
            },
            "questions": {
                "value": "Please see the questions in the weakness sections. I will wait for the authors' feedback for the final ratings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3893/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698266283940,
        "cdate": 1698266283940,
        "tmdate": 1699636348197,
        "mdate": 1699636348197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4dtpRNkxdi",
        "forum": "bC50ZOyPQm",
        "replyto": "bC50ZOyPQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_UGG1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_UGG1"
        ],
        "content": {
            "summary": {
                "value": "This paper presents READ (Recurrent Adaption), a light weight and memory-efficient finetuning method by inserting a small RNN network to bypass the back propagation to the large backbone network. Results show that READ can achieve great reduction in both memory and energy consumption."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) Clear description of the proposed method in Section 2.\n\n(2) Good evaluation of the proposed method on T5 model and GLUE benchmark."
            },
            "weaknesses": {
                "value": "(1) The main result described in abstract/introduction (memory consumption reduced by 56%, and gpu energy reduced by 84%) are compared with full-tuning, not the SOTA memory/energy-efficient finetuning methods. At least we need another data point: a direct comparison to the current best SOTA. From the figure 2, I believe that the comparison with SOTA will lead to much smaller reduction percentages.\n\n(2) A drawback in figure 2 is that it\u2019s hard to justify the advantage of the proposed READ method by just a single data point. Ideally, it\u2019d be helpful to have multiple data points for READ, under different energy/memory consumptions, in order to build a \u201cPareto curve\u201d. In this way, it\u2019d be much easier to tell whether READ advances the whole Pareto frontier.\n\n(3) In Table 1, it is unfair to only try the proposed method on the larger T5-large. You need to also try other methods + larger T5 in order to prove your points.\n\n(4) As the authors also mentioned in limitation section, it\u2019d be great to add evaluation on GPT-style models and corresponding downstream tasks.\n\n(5) To me, the overall proposed idea make sense but not very exciting. It seems more like an incremental work compared to existing methods (the RNN being the only main novelty)."
            },
            "questions": {
                "value": "See the concerns I listed in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Reviewer_UGG1"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3893/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738112906,
        "cdate": 1698738112906,
        "tmdate": 1699636348107,
        "mdate": 1699636348107,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "scu5kaE6sC",
        "forum": "bC50ZOyPQm",
        "replyto": "bC50ZOyPQm",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_BXAc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3893/Reviewer_BXAc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a efficient adaptation method, recurrent adaption (READ), for Transformers.  The idea is to use recurrent neural networks plus the so-called Joiner networks to learn the correction for the output of the original transformer backbone during fine-tuning.  The experiments verify the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- According to the experimental result presented in this paper, the saving in memory consumption and energy usage are decent.\n\n- The idea of learning _corrections_ for the output of the original Transformer is well-motivated and looks interesting to me. That being said, given my limited familiarity with existing methods in this domain, I'm unable to ascertain the novelty of this idea.\n\n- The authors are upfront about the limitation of the current paper in Sec. 6."
            },
            "weaknesses": {
                "value": "- I find the notion of recurrence in this paper somewhat tricky. Typically,  Recurrent neural networks process the input sequentially by taking one token as the input at a time and updating the hidden state. However, in this paper, recurrence does not occur in the sequence level. Instead, it occurs at the model layer level, which seems non-standard for me. I highly recommend the authors to clarify this to avoid confusions.\n\n- The scope of the method and experiment seems narrow. It only considers the encoder-decoder Transformers on a single natural language understanding benchmark. I believe the paper will be strengthened significantly by introducing more Transformer settings (e.g., decoder only) and/or more benchmark datasets (e.g., other NLP/vision tasks).\n\n- Training time is not reported in this experiment. I believe it is important to add this Information for a comprehensive comparison among different methods.\n\n- Currently the presentation of the paper is flawed. There are many formatting issues which impedes readability.\n  - In the abstract on OpenReview, Latex commands (`\\textbf`) are not deleted. $\\\\%$ is missing in the discussion of the reduction in memory consumption and GPU energy usage.\n  - The format of citation is bad in this paper. The authors should have used `\\citet` and `\\citep` appropriately.\n  - In the abstract, the first letter of \"Transformer\" is capitalized but in the main body of the paper it's not. The authors should make this consistent and I recommend to always capitalize the first letter of \"Transformer\".\n  - Missing references. \"Appendix ??\" and \"Table ??\" in Sec. 4.\n  - Bad notation. Eq. (4) uses $\\bar U$ to denote a cumulative sum. This can be misleading because people are more likely to interpret $\\bar U$ as an average.\n\n- Minor comments.\n  - Abstract. \"empirical evaluation of the GLUE benchmark\" $\\to$ \"empirical evaluation on the GLUE benchmark\".\n  - Page 4. \"the following equation systems gives\" $\\to$ \"the following equation system gives\"\n  - Page 4. \".\" is missing in the last sentence of the last paragraph."
            },
            "questions": {
                "value": "Please refer to the **Weaknesses** part.\n\nOne more clarification question:\n- What does \"normalized\" mean in the caption of Fig. 1 mean? What are the original results and how are they normalized?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3893/Reviewer_BXAc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3893/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790872366,
        "cdate": 1698790872366,
        "tmdate": 1699636348019,
        "mdate": 1699636348019,
        "license": "CC BY 4.0",
        "version": 2
    }
]