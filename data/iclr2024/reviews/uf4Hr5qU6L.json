[
    {
        "id": "wM4luR1L3s",
        "forum": "uf4Hr5qU6L",
        "replyto": "uf4Hr5qU6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_2rHa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_2rHa"
        ],
        "content": {
            "summary": {
                "value": "This paper is a version of Chain of Thought (CoT) methods for LLMs that improves performance by explicitly including problem representation as well as goal state and method. They illustrated this improvement using three benchmark datasets of reasoning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strongest aspect of this paper was the experimental section because of broad application of techniques. The datasets they used were standard and well suited to the tasks at hand as well as providing a clear baseline. \n\nI additionally appreciated the clear examples interspersed through the paper and the detailed appendix.  \n\nFinally, I thought the analysis section was well written and thoughtful. \n\nIn terms of the key dimensions:\n\n- Originality: I thought the original part of this paper was using the psychological notion of goal and solution state to improve LLMs. The actual solution of \"a better prompt\" did show promise but felt to me more like a variation on a theme of existing methods than on anything substantially new. \n\n- Quality: Results were well baselined and quantified.\n\n- Clarity: This was well written, with a clear setup and motivation.\n\n- Significance: Incorporation and understanding of abstract reasoning with LLMs is a major challenge and any improvements there are significant from the perspective of improved LLM performance as well as from the perspective of explainability and generalizability."
            },
            "weaknesses": {
                "value": "The first weakest part of this paper in my mind was the methods section, which seemed to have less details than the other parts of the paper.  I was specifically looking for more details on the few-shot and zero-shot methods. I also thought that the robustness claims made in the experimental section needed to be introduced as part of the methods. \n\nPerhaps more significant weakness in this paper is that the performance results didn't improve as much as I would expect based on the stated hypothesis that key parts of problem are being left out of the problem formulation. I'm not sure that is a fundamental weakness and might be addressable in the discussion or the introduction. \n\nThe specific LLMs used are not state of the art, so there is also a question of if these methods make as big a difference with the most recent generation of generative models."
            },
            "questions": {
                "value": "I really appreciated the robustness results in the experimental section, but one of the questions I had after reading the paper is about robustness and stability of the prompting method. It is not difficult to make LLMs change their response based on the prompt, and I don't think this work is sufficient to to claim that the performance improvements are attributable to the specifics of the prompt used rather than the natural variation due to changes in prompts more generally. \n\nAlso, I believe the references are ill-formated - page 13 and 14 are a single paper. Can you check that those are correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Reviewer_2rHa"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698177112722,
        "cdate": 1698177112722,
        "tmdate": 1700580784825,
        "mdate": 1700580784825,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QmJNN4QB9C",
        "forum": "uf4Hr5qU6L",
        "replyto": "uf4Hr5qU6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_9iGg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_9iGg"
        ],
        "content": {
            "summary": {
                "value": "This paper integrates the construction of problem representation into an LLM's problem-solving process to enhance its reasoning capability. The construction of problem representation involves defining the initial and goal states of a problem. The PreCoT method comprises two key stages: 1) In the first stage, when provided with a problem statement, it guides the LLM to extract the given information and the objective. 2) In the second stage, it appends the extracted information and objective to the problem statement, and employs zero-shot or few-shot CoT prompting to guide the LLM in its reasoning process. The experiments involving arithmetic, commonsense, and symbolic reasoning showcase that PreCoT achieves higher accuracy than CoT in both zero-shot and few-shot settings in most cases. Furthermore, additional analyses reveal that PreCoT exhibits greater resilience to irrelevant information in arithmetic reasoning and demonstrates relatively higher contextual awareness in commonsense reasoning.\n\nContributions: 1) This paper underscores the significance of \"problem representation construction\" as a preliminary step before initiating the reasoning process within LLM. It entails the extraction of the given information and problem objective to form the problem representation. 2) Empirical results provide concrete evidence of the method's effectiveness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) The method is simple yet effective, consistently outperforming the baseline CoT in the majority of cases.\n\n(2) The study is supported by a wealth of comprehensive experiments."
            },
            "weaknesses": {
                "value": "(1\uff09 Its technical novelty is somewhat limited, as the idea of initially guiding LLM to extract given information or objectives lacks novelty. In two of the previous works https://aclanthology.org/2023.acl-long.147.pdf and https://arxiv.org/abs/2306.03872, they also involve extracting the given information and (or) objectives of the given problem.\n\n(2\uff09 The problem representation method is only combined with CoT no matter zero-shot or few-shot. It would be better to explore the combination of the problem representation method with more distinct types of solution search methods like decomposition-based methods (e.g., L2M), to validate the generality the proposed problem representation method."
            },
            "questions": {
                "value": "Additional suggestions:\n\n(1) In the \u201cAbstract\u201d and \u201cIntroduction\u201d sections, it would be beneficial if the authors could emphasize some interesting experimental findings, such as PreCoT's enhanced robustness against irrelevant information and context sensitivity, as well as some types of errors are mitigated by PreCoT. Highlighting such results would help readers to quickly grasp that in which cases PreCoT is more effective and why it works.\n\n(2) Regarding Figure 1, it would be better to visually distinguish the two stages more clearly. In the current version, \"#1 Problem Representation Construction\" and \"#2 Solution Searching\" on the left may be deem as simultaneous inputs of an LLM at the first sight. I recommend referring to Figure 1 in the Least-to-most paper (https://openreview.net/pdf?id=WZH7099tgfM) for inspiration on how to improve the visual separation.\n\n(3) I noticed that in the paper, the implementation of PreCoT involves three separate prompts, resulting in three API calls. I'm curious if consolidating these three prompts into two or even one prompt could still achieve good performance. If so, such a consolidation could significantly reduce API calling costs and improve efficiency.\n\n(4) In Figure 3, \"Math World Problem\" should be \"Math Word Problems.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Reviewer_9iGg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698231486688,
        "cdate": 1698231486688,
        "tmdate": 1699636527448,
        "mdate": 1699636527448,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6T7yDCygr1",
        "forum": "uf4Hr5qU6L",
        "replyto": "uf4Hr5qU6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_c8na"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_c8na"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a new prompting method aiming at improving and extending Chain of Thought (CoT) prompting. \nDrawing inspiration from cognitive psychology, it proposes to structure prompting according to human problem representation, specifying a problem as the initial state and the objective of the problem as the goal state.\nEvaluation has been performed for two major LLM (PaLM2 and GPT-3) on a number of reasoning benchmarks, ranging from arithmetic (GSM8K) to symbolic and common sense reasoning. During evaluation, PRECoT was compared to CoT under both Zero-shot and Few-shot prompting. Results suggest that PRECoT improves performance in the majority of tested conditions, while in-depth analysis suggests one possible mechanism for improved performance via the reduction in major semantic-logical errors."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has clear objectives and is generally well-structured, with a good balance between main article and supplementary material which is useful to understand the details of the prompting method. \nDespite the amount of recent work on prompting variants, it can still be considered original work in that the method is not immediately reducible to one of the pre-existing CoT or CoT-like approaches. \nThe strongest element of the paper is the amount of experimental work and the clarity of presentation of its results, especially Tables 1, 3, 5."
            },
            "weaknesses": {
                "value": "The paper repeatedly mentions human problem-solving as a rationale and an inspiration for the approach, which raises two independent issues. The first one is the lack of convincing and up to date backing for the rather central claim that humans actually decompose problems as suggested in the paper. Despite referring to \"accumulated insights\", references in section 2 are by no means recent ones, nor would some of them be considered to pertain to cognitive psychology. Secondly, whether seeking inspiration from human problem solving would actually improve LLM reasoning remains a debated issue and, to say the least, the jury is still out on this topic. Some similarities between human an LLM reasoning, such as step by step decomposition, tend to be rather simplistic, while more rigorous findings, such as similarity in the influence of content over reasoning (as per the Wason selection task [Dasgupta et al., 2022]) may not scale up to all forms of LLM reasoning. \nThe other issue is simply whether the PRECoT method actually works by the hypothesized mechanism of problem decomposition, rather than through a clarification of questions that would facilitate content-based inference through a better specification of relationships between 'variables'. \nThe moderate effect on StrategyQA and CSQA is interpreted as a difficulty in constructing good representations, but this could be seen as self-referential, and alternative explanations could be suggested based on the above.\n\nDasgupta, I., Lampinen, A.K., Chan, S.C., Creswell, A., Kumaran, D., McClelland, J.L. and Hill, F., 2022. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051."
            },
            "questions": {
                "value": "How is performance affected when not including \"Let\u2019s think step by step.\" in the PRECoT case?\nHow would you normalize the experiment for the additional length of PRECoT prompts (whether it implies additional information or information repetition in different places)?  \nWhy did you not use GPT-4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5277/Reviewer_c8na"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698889773590,
        "cdate": 1698889773590,
        "tmdate": 1699636527345,
        "mdate": 1699636527345,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7DoctDOqQL",
        "forum": "uf4Hr5qU6L",
        "replyto": "uf4Hr5qU6L",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_Hado"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5277/Reviewer_Hado"
        ],
        "content": {
            "summary": {
                "value": "This paper presents PRECoT, a prompting strategy that prompts the LLM on extracting the problem information before coming up with the solution. The results show that on multiple benchmarks PRECoT is a better prompting strategy than CoT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. \n- The authors perform experiments on multiple benchmarks and utilize the state-of-the-art models. \n- Error analysis experiments are performed to show the importance of problem representation."
            },
            "weaknesses": {
                "value": "The claim that PreCOT helps LLM enhance its reasoning doesn\u2019t make sense to me as the LLMs are more or less performing approximate retrieval based on the training data that has been fed into them. When these datasets include not only solutions but also the derivative paths leading to them, methodologies like PreCOT might improve an LLM's ability to generate more probable outputs, thus appearing to solve problems more effectively [1]. However, given the vast and varied nature of the content available on the web, we have no clear idea as to what the entire web contains.  In my view, to substantiate the claim that LLMs possess the ability to reason, some kind of diagonalization should be done in the kind of benchmarks that the approach is being tested on. An example of such diagonalization is the obfuscation method that the authors in [2] propose, while testing the planning abilities of LLMs. My primary concern is that the benchmarks used in the proposed work are susceptible to be part of or mirror the training data of the LLM. While testing these benchmarks are useful, the claims made in the paper can be upheld if there is some form of diagonalization in the benchmarks that are being evaluated. \n\n[1] McCoy, R. T., Yao, S., Friedman, D., Hardy, M., & Griffiths, T. L. (2023). Embers of autoregression: Understanding large language models through the problem they are trained to solve. arXiv preprint arXiv:2309.13638.\n\n[2] Valmeekam, K., Marquez, M., Sreedharan, S., & Kambhampati, S. (2023). On the Planning Abilities of Large Language Models--A Critical Investigation. arXiv preprint arXiv:2305.15771."
            },
            "questions": {
                "value": "1. How many examples were given in the few-shot setting?\n\n2. (Minor) Why do you call it problem representation? Isn\u2019t it problem information that is being extracted? I feel like calling it problem representation might make the reader believe that LLM is extracting how the problem is being represented rather than what are the crucial information pieces in the question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5277/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699265665170,
        "cdate": 1699265665170,
        "tmdate": 1699636527250,
        "mdate": 1699636527250,
        "license": "CC BY 4.0",
        "version": 2
    }
]