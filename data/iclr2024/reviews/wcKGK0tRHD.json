[
    {
        "id": "sz16GAqwb2",
        "forum": "wcKGK0tRHD",
        "replyto": "wcKGK0tRHD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_kPrg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_kPrg"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on forward-forward (FF), an alternative to backpropagation proposed by G. Hinton in 2022 (arxiv only). FF is based on a contrastive loss applied layer-wise between the original points and \"negative\" points. The authors apply three modifications to the original algorithm (that they call \"the Trifecta\"), which include a different loss term, batch normalization instead of layer normalization, and a blockwise model where some layers receive feedback from the subsequent layers. With these three modifications they are able to scale FF from MNIST to CIFAR-10 with good performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Finding alternatives to backpropagation that scale to larger datasets is a very interesting research direction. In this sense, the results shown here are good if compared with state-of-the-art models with no backward passes, although the authors are not providing these comparisons (see below). The paper is well written and easy to read, apart from the mathematical notation that could be drastically improved."
            },
            "weaknesses": {
                "value": "I see a few weaknesses making me lean towards rejection.\n\n1. NOVELTY: the \"Trifecta\" (which is a bit excessive as name) is composed of three small modifications, of which one was already published and applied to FF (different loss term), one was published but not applied to FF (Overlapping Local Updates), the final one is using batch normalization instead of layer normalization. These 3 methods are also justified in an informal way or empirically by some visualization (which are mostly copy-paste from a W&B run). Overall, this looks more like a report of someone experimenting on FF than a true scientific publication. This is made worse by the fact that (a) FF is unpublished, and (b) there is no convergence guarantee to begin with on FF optimization.\n\n2. RELATED WORKS: the paper is very shallow in its analysis of the related works, which should include other backpropagation-free techniques such as zeroth-order gradients (https://arxiv.org/abs/2305.17333), forward gradients (https://openreview.net/forum?id=JxpBP1JM15-), error-driven input modulation (https://proceedings.mlr.press/v162/dellaferrera22a/dellaferrera22a.pdf), etc. Many of these methods are similar to FF.\n\n3. EXPERIMENTS: connected to 2, it's unclear why the authors are only validating with respect to standard FF and not other alternatives to backpropagation (e.g., direct feedback alignment). Many of these methods are on-par with the results shown here."
            },
            "questions": {
                "value": "I don't have many questions on the paper. Improving related works and the experimental evaluation are good points (see above), but the paper remains of an incremental value and lacking any formal guarantees of convergence. Hence, I do not think this is a valuable contribution to the conference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698058654193,
        "cdate": 1698058654193,
        "tmdate": 1699636538683,
        "mdate": 1699636538683,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "buP6t96DLl",
        "forum": "wcKGK0tRHD",
        "replyto": "wcKGK0tRHD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_xPoW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_xPoW"
        ],
        "content": {
            "summary": {
                "value": "The authors propose The Trifecta\u2014three modifications to the recent Forward-Forward algorithm to improve convergence stability and scalability to more complex datasets. Specifically, they transfer existing techniques into their design of a modified Forward-Forward training algorithm and empirically demonstrate that their Trifecta Forward Forward (TFF) algorithm outperforms vanilla Forward-Forward and brings the accuracy on a collection of datasets equal or closer to accuracy achieved by training the same models via backpropagation. Motivation for this research is the consideration and development of alternative training paradigms for deep learning models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022 Design of Trifecta Forward Forward (TFF) algorithm for backpropagation free neural network training\n\n\u2022 Large-scale empirical analysis of TFF on 3 architectures and 4 datasets to compare performance against vanilla Forward-Forward (VFF) and backpropagation\n\n\u2022 TFF offers sizeable improvements compared to VFF\n\n\u2022 CIFAR-10 accuracy of TFF/d achieves nearly-SOTA performance for gradient-free training"
            },
            "weaknesses": {
                "value": "\u2022 Work feels largely incremental\u2014authors adopt existing technologies (SymBa loss, Batchnorm, Overlapping Local Updates) to design TFF algorithm\n\n\u2022 Main paper seems to contain information that could be summarized more concisely or moved to appendix (e.g., Section 5 seems very long for experimental setup)."
            },
            "questions": {
                "value": "1. While the performance of TFF on CIFAR-10 is very good for backpropagation-free training, I found the design of the algorithm to borrow from existing technologies. I wanted to give you the chance to clarify if there were additional contributions in the design of TFF beyond adopting existing methods from the literature.\n\n2. On p. 8, you say that \u201cFurther, on simple datasets, our algorithm is on par with our backpropagation baseline, in the same amount of epochs.\u201d Considering Table 2, this only appears to be the case for MNIST. Am I missing something?\n\n3. What is the walltime per epoch when using TFF compared to BP? I know this will vary per architecture and dataset so let\u2019s say the largest model on CIFAR-10."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Reviewer_xPoW"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802209009,
        "cdate": 1698802209009,
        "tmdate": 1699636538590,
        "mdate": 1699636538590,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BOsoinzlFC",
        "forum": "wcKGK0tRHD",
        "replyto": "wcKGK0tRHD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_ugNW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_ugNW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes 3 techniques that together significantly help training a deep network with forward-forward algorithms. Those techniques are:\n* Improved loss function which treats false positives and false negatives equally.\n* Use of batch normalisation instead of layer normalisation.\n* Use of overlapping local updates, so that each layer is also optimised to produce useful features to help the next layer goodness loss.\n\nThe method is evaluated on MNIST, Fashion-MNIST, SVHN and CIFAR-10 using CNNs and FCNs. While ablating the added techniques the performance on CIFAR-10 is brought from 44% to 75%. When training longer with more layers it reaches 83% while the backprop reaches 89%."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The main text is overall easy to read without significant previous knowledge. The three techniques are motivated and refer to previous work. The main contribution seems to be using them all together on a FF setup for a combined improved accuracy and to map to the community the sort of techniques that can bring FF closer to BP training.\n\nThe ablations are clear, e.g. the layernorm->batchnorm has a very significant effect on allowing the subsequent layers to improve on the decisions from previous ones (Figure 1). Table 1 also shows the effect of the BN and OLU applied on top of a SymBA loss baseline."
            },
            "weaknesses": {
                "value": "It appears to me that the proposed techniques run somehow counter to (what I on my limited knowledge) perceive to be some of the motivation of FF:\n* Weight updates as described in section 3, are supposed to be done in 2 forward steps, one for positive and one for negative samples (maybe aiming to be able to completely decouple them further in time eventually (sleep section in Hinton 2022)). However here by removing the threshold it appears one needs the batch to contain both positive and negative examples and resembles a self-supervised contrastive setup.\n* Overlapping local updates although not using non-global gradients is a step away from being able to introduce black boxes in the middle of the forward pass.\n* I also can't stop to notice that batch normalization as done here also disallows training by seeing one example at a time (though maybe keeping batch statistics would easily address that?).\n\nCombined together with CNN and evaluated on a supervised contrastive training it is maybe of no surprise that this set of techniques obtain better results than vanilla FF. Despite that, I think this work may still be interesting to help mapping the set of techniques that bridge the gap to BP and leave the discussion to whether they are in the spirit of VFF or biologically plausible to the reader."
            },
            "questions": {
                "value": "A) Since in the loss discussion and appendixes the batch dimension is mostly left out, I feel like some details were unclear. E.g. is g_pos and g_neg used in the SymBa loss the mean of l2_norm of all the positive and negative examples in the batch? How is the batch constructed, is it highly correlated (e.g. contains the same input in both positive and negative case)?\n\nB) Do you think keeping moving averages of the norm of positive and negative activations would allow one to train this without requiring both positive and negative examples to be in the batch?\n\nC) Given the size of networks would it be practical and interesting to provide ablation of BN and OLUs on top of a VFF without the SymBa loss? Or am I missing something?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Reviewer_ugNW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831037058,
        "cdate": 1698831037058,
        "tmdate": 1699636538496,
        "mdate": 1699636538496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CzFzxYZVd2",
        "forum": "wcKGK0tRHD",
        "replyto": "wcKGK0tRHD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_i3k6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5349/Reviewer_i3k6"
        ],
        "content": {
            "summary": {
                "value": "Modern machine learning models are able to outperform humans on a variety of non-trivial tasks. However, as the complexity of the models increases, they consume significant amounts of power and still struggle to generalize effectively to unseen data. Local learning, which focuses on updating subsets of a model\u2019s parameters at a time, has emerged as a promising technique to address these issues. Recently, a novel local learning algorithm, called Forward-Forward, has received widespread attention due to its innovative approach to learning. Unfortunately, its application has been limited to smaller datasets due to scalability issues. To this end, we propose The Trifecta, a collection of three simple techniques that synergize exceptionally well and drastically improve the Forward-Forward algorithm on deeper networks. Our experiments demonstrate that our models are on par with similarly structured, backpropagation-based models in both training speed and test accuracy on simple datasets. This is achieved by the ability to learn representations that are informative locally, on a layer-by-layer basis, and retain their informativeness when propagated to deeper layers in the architecture. This leads to around 84% accuracy on CIFAR-10, a notable improvement (25%) over the original FF algorithm. These results highlight the potential of Forward-Forward as a genuine competitor to backpropagation and as a promising research avenue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The methods are relatively straightforward to be applied to the existing Forward-Forward Algorithm.\n- Paper is well-written. It is easy to follow the explanations written in the paper itself.\n- Experiemnt results have shown that following these recipes improves the vanilla Forward-Forward Algorithm and exhibits an on-par result with network learned from backpropagation in terms of classification accuracy.\n- Code is available."
            },
            "weaknesses": {
                "value": "My main concern is with regard to the generalizability and scalability of the proposed method when it is applied to the Forward-Forward algorithm. This can be viewed from multiple lenses as follows:\n- First of all, while the image classification results are promising when Trifecta is applied to the Forward-Forward algorithm, that does not mean that the only task we need to tackle is the classification task. It is desirable to see how well the Trifecta fares with the vanilla FF and SGD on a wider variety of tasks such as Machine Translation, image generation, or different types of learning mechanisms such as self-supervised learning, reinforcement learning, multimodal learning, etc.\n- In addition, the network used for evaluating this method is relatively small compared to most experiments that are performed in this avenue. It would bring substantial improvement to the paper to use larger networks such as ResNet and attention-based models to evaluate the performance of Trifecta on a given task compared to the vanilla FF and SGD.\n- Furthermore, in the ablation, it would be interesting if we include ablation studies on how the vanilla loss function and incorporate either BN or OLU and compare it when we use the SymBa loss function [2] with one of these two other ingredients just like the results in Table 1. \n- Finally, even though it is not desired to perform evaluation on large datasets such as ImageNet, I am curious regarding the performance of TFF when it is compared to VFF and BP on image classification task, since most of the methods proposed in this avenue are evaluated on large datasets."
            },
            "questions": {
                "value": "See Weaknesses\n\n[1] Hinton, Geoffrey. \"The forward-forward algorithm: Some preliminary investigations.\" arXiv preprint arXiv:2212.13345 (2022).\n\n[2] Lee, Heung-Chang, and Jeonggeun Song. \"SymBa: Symmetric Backpropagation-Free Contrastive Learning with Forward-Forward Algorithm for Optimizing Convergence.\" arXiv preprint arXiv:2303.08418 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5349/Reviewer_i3k6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5349/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699534748675,
        "cdate": 1699534748675,
        "tmdate": 1699636538358,
        "mdate": 1699636538358,
        "license": "CC BY 4.0",
        "version": 2
    }
]