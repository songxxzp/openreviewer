[
    {
        "id": "yg1UyHXEBB",
        "forum": "QUhCObWGw5",
        "replyto": "QUhCObWGw5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a 2 stage method \u201cPATHS\u201d to address the main weakness of the current text-to-video retrieval CLIP-based models, i.e., overfitting. The proposed method includes 2 stages: 1) select best params by monitoring the fluctuations of params, which is much cheaper than e.g. per 50 step eval; 2) train an adapter module STMA with CLIP params frozen.\n\nThe proposed method is generic enough to be applicable to any existing CLIP-based models. The result achieves SOTA in the common text-to-video retrieval tasks: LSMDC and MSRVTT."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is relatively simple and the result is strong with several SOTA.\n\nThe experiments are extensive with many baselines; STMA is applied to a wide range of models to show its effectiveness.\n\nThe paper is well written and very readable.\n\nThe code is open-sourced."
            },
            "weaknesses": {
                "value": "5.4 ablates the param selection strategies, which is great; also it shows stage-1\u2019s importance in Tab 4. However, it\u2019s still unclear what the performance would be like if we apply stage-2 ONLY to the common param selection strategies (i.e. skip stage-1). This might make it clearer how important stage 1 and 2 is respectively?"
            },
            "questions": {
                "value": "Appendix seems not uploaded?\n\nIIUC, the red curve of Fig 3 is comparable with the green curve of Fig 2 (both are X-CLIP eval), but it seems there\u2019s some difference (e.g. in the end of epoch 5)? If my understanding is correct, maybe it would be clearer if we plot the green curve of Fig 2 in Fig 3 as well?\n\nIn 4.1 \u201cTwo-stage Process\u201d: in the 2nd stage, if params are frozen, why do we still need to \u201cload these parameters back into the model at the end of each epoch to perform the pivoting\u201d? And could you please explain a bit what \u201cperform the pivoting\u201d exactly means?\n\nmild comments:\ntypo in 2.2: \u201cadapters have been *unsed* for progressive learning\u2026\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2484/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2484/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2484/Reviewer_hfBT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702812382,
        "cdate": 1698702812382,
        "tmdate": 1699636184957,
        "mdate": 1699636184957,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aUCc5FYB5t",
        "forum": "QUhCObWGw5",
        "replyto": "QUhCObWGw5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_zzzT"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author proposes a two-stage training method (PATHS) to improve the results of video-retrieval tasks. In the first stage, it selects 5 candidates according to the parameter ranking. And the results of these candidates are better than those epoch candidates. In the second stage, it uses the previous best checkpoint for initialization and adds an adapter for further fine-tuning. Further experiments demonstrate its effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is clear and the method is simple to reproduce."
            },
            "weaknesses": {
                "value": "- The paper is not well organized. \n  - The PRELIMINARIES section seems to be redundant, and the `3.1` and `3.1.1` seem to stage for a single section (subsection). \n  - Some details are not clear. For example, how does the ranking work? How does a two-stage scheme w/o STMA work (no adapter and freeze the backbone)? How to split scenes in STMA?\n- The method seems to be tricky, which overfits a specific test set.\n- The Figures 2 and 3 are in low resolution and hard to read."
            },
            "questions": {
                "value": "- In Page 5, `Main Idea` part, `In accordance with Figure 1, we denote end of each epoch with dotted line` should be `Figure 2`.\n- In Page 5, `Motivation` part, `When the model starts to diverge after passing the optimum point, the model parameter values exhibit strong fluctuations. This often involves rearranging parameters in terms of importance (or the value of parameters)`, how can the author get the conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811977721,
        "cdate": 1698811977721,
        "tmdate": 1699636184851,
        "mdate": 1699636184851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "U6M5XeXbmv",
        "forum": "QUhCObWGw5",
        "replyto": "QUhCObWGw5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_Cekr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_Cekr"
        ],
        "content": {
            "summary": {
                "value": "The paper tries to address the weight corruption problem arising when extending pre-trained CLIP weights to tasks in the video domain.\n\nIt proposes a new learning strategy named \"Parameter-wise Adaptive Two-stage training Harnessing Scene transition mask adapter\" (PATHS), which involves a two-phase learning process. The first phase focuses on determining the optimal weights by monitoring parameter fluctuations, while the second phase concentrates on understanding scenes using an adapter module. \n\nThe paper demonstrates the effectiveness of their approach by achieving leading performances on major text-video benchmark datasets such as MSRVTT and LSMDC."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method does not require frequent evaluations at every N step, which is distinct from recent approaches that incur extra computational overhead.\n2. PATHS can be applied to strong baselines in a plug-and-play manner and has shown consistent performance improvements."
            },
            "weaknesses": {
                "value": "1. The elaboration in Section 4.2 on the proposed **Co-Attention** module in STMA is not clear enough. The sentence '*the co-attention layer takes different queries, keys, and values to enable the learning and updating of two pieces of information regarding each other*' is confusing. What are the settings of QKV in your Co-Attention? Considering this module is part of the core designs, more formulation or illustration is needed for better understanding.\n2. If the **Alignment Attention** in Figure 4 is the so-called *'attention layer' utilized to identify the crucial parts of the video*, the authors should clarify its module name in the paper. Is **Alignment Attention** in STMA a vanilla attention module? How does the attention layer *identify the most crucial part of the video throughout the entire video and each scene*? More explanation and evidence are needed to support this.\n3. According to the paper, the authors set the hyperparameter $K$ as 5. How do the authors choose this value? More ablation studies on $K$ should be conducted.\n4. What if in some certain samples, there does not exist a scene transition or contains more than one transition? Can the proposed STMA be applied to all possible conditions?"
            },
            "questions": {
                "value": "Please see the Weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698897972668,
        "cdate": 1698897972668,
        "tmdate": 1699636184784,
        "mdate": 1699636184784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SegimeK2xu",
        "forum": "QUhCObWGw5",
        "replyto": "QUhCObWGw5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_tWWe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2484/Reviewer_tWWe"
        ],
        "content": {
            "summary": {
                "value": "This work tackles the problem of text-to-video retrieval and mainly focuses on the methods based on the pretrained CLIP. To mitigate the overfitting onto a target video dataset, this work proposes a two-stage training method where the first stage optimizes the image-to-video weight transfer, and the second stage introduces an adaptor to further improve the video understanding. The proposed method PATHS is a plug-and-play module and can be added to other existing methods such as CLIP4Clip, X-CLIP, DiCoSA. When tested on MSVD, LSMDC and MSRVTT retrieval benchmarks, PATHS improves over different baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed PATHS method is a plug-and-play model that can be added to other existing methods and consistently improves the retrieval performances.\n* The code is available which helps the reproducibility of the method."
            },
            "weaknesses": {
                "value": "* As the method requires more frequent evaluation than the standard per-epoch evaluation, the method still introduces computation overhead and leave the frequency as a hyperparameter which will potentially vary for different datasets.\n* The contribution of the STMA adaptor seems marginal. For example in Table 1, the gain from STMA is only 0.2 point.\n* In the ablation section, all BP, SP, USP methods exhibit very similar results. It is hard to tell if one quantifying strategy is better than others, raising a question whether the quantifying strategy is a main component of the proposed method.\n* Typo \"raking\" --> \"ranking\" in section 5.4"
            },
            "questions": {
                "value": "* While the proposed method focuses on the retrieval task, would PATHS method also applicable to other video-text tasks such as video captioning or video QA?\n* Please see the weaknesses section for other questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2484/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698959299293,
        "cdate": 1698959299293,
        "tmdate": 1699636184723,
        "mdate": 1699636184723,
        "license": "CC BY 4.0",
        "version": 2
    }
]