[
    {
        "id": "NXOoNofHP6",
        "forum": "LIW88mwqgv",
        "replyto": "LIW88mwqgv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_TzhJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_TzhJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an iterative bootstrapping method called Iter-CoT in chain-of-thoughts prompting for large language model reasoning. The major innovation of Iter-CoT is to self-correct the errors in reasoning chains by leveraging iterative bootstrapping and obtaining more precise and comprehensive reasoning chains. Experimental results on ten reasoning datasets among three different reasoning tasks demonstrate that our approach significantly outperforms the previous methods, achieving new state-of-the-art results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes iter-CoT, which uses the experiences with wrong cases to construct the demonstrations for CoT prompts. \n\n2. It is a new CoT method that focuses on the demonstration selection to improve the in-context learning performance. \n\n3. The evaluation is conducted on a wide range of benchmarks and various LLMs, showing advances in performance."
            },
            "weaknesses": {
                "value": "1. The idea of this paper is not very exciting.\n\na) The idea of self-correction is discussed in recent studies [1,2]. By constructing the correction demonstration pool, the samples that the LLM is prone to getting wrong are gathered and a correct CoT is prepared for each. By sampling from these, the model can get stronger prompt.\n\nb) The idea of \u2018bootstrap\u2019 is not very appealing. First, LLM bootstrap has been proposed in [3], where the demos are totally generated by the proposed system. However, in iter-CoT, both \u2018w/ label\u2019 setting and \u2018wo label\u2019 setting are under supervised, i.e., the golden label or another more powerful LLM.\n\n2.  (Minor) The analysis does not make this paper more convincing. The settings of demo selection in the ablation can hardly uncover what the model learns from history errors and corrections. The \u2018rising and then falling\u2019 trend in Figure 6 is not obvious. Also, the comparisons are coarse-grained. The demos, method, and reasoning steps are all different across all settings and may require in-depth analysis.\n\n[1] Learning from Mistakes via Interactive Study Assistant for Large Language Models\n\n[2] Large Language Models Cannot Self-Correct Reasoning Yet\n\n[3] Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP"
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820076556,
        "cdate": 1698820076556,
        "tmdate": 1699636463198,
        "mdate": 1699636463198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5xQmyzxdZf",
        "forum": "LIW88mwqgv",
        "replyto": "LIW88mwqgv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_uFPN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_uFPN"
        ],
        "content": {
            "summary": {
                "value": "The authors first validate that LLMs possess self-correction ability. They further establish that using more accurate demonstrations in the prompt leads to better results. Building on these two findings, the authors proposed Iter-CoT (iterative bootstrapping in Chain-of-Thoughts prompting), aiming to enhance the performance of large language models on reasoning tasks. This method generates the CoT demonstrations by guiding the LLM to rectify errors and summarize the reasoning chains on questions with appropriate difficulty levels. As such, during the inference, the demonstrations are more accurate compared with other prompting methods. The authors demonstrate the effectiveness of the method on three reasoning tasks on ten datasets, including arithmetic, commonsense, and symbolic reasoning. The results show that using demonstrations generated by Iter-CoT outperforms using traditional prompting methods, and achieves state-of-the-art performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The motivation is strong and supported by two findings, i.e. the self-correction ability of LLMs and the value of revised examples.\n\n2. The Iter-CoT method is straightforward to follow. Experimental results across three categories of reasoning tasks and ten datasets prove the effectiveness of this method.\n\n3. A series of ablation experiments are conducted to investigate the two phases of the method."
            },
            "weaknesses": {
                "value": "1. The bootstrapping phase relies on GPT-4 to assess examples without labels, which does not address the issue of hallucinations and might lead to misjudgment of correct responses.\n\n2. The summarization phase employs an LLM, which could also lead to hallucinations, especially when dealing with lengthy multi-turn conversation contexts, potentially impairing LLM performance and resulting in suboptimal summarizations. Additionally, including the entire correction process in the summarization phase might lead to misleading summaries.\n\n3. I found the innovativeness of the proposed method is limited, as the verify-and-correct approach has been previously explored in works such as ReAct and self-ask. Additionally, more efficient methods for demonstration sampling, like k-nearest neighbor sampling, are available.\n\nReference:\nYao, Shunyu, et al. \"React: Synergizing reasoning and acting in language models.\"\nPress, Ofir, et al. \"Measuring and narrowing the compositionality gap in language models.\u201d\nLiu, Jiachang, et al. \"What Makes Good In-Context Examples for GPT-3 ?.\u201d"
            },
            "questions": {
                "value": "1. What is the inference cost between Iter-CoT and other baseline methods? And what is the size of the demonstration pool?\n\n2. How many demonstrations are given in the prompt during inference? Is it consistent for all baseline methods?\n\n3. In Figure 6, which specific task does the accuracy pertain to? I assume it relates to tasks with labels, but it is important to note that accurate evaluations on labeled tasks do not necessarily translate to accurate evaluations on tasks without labels, given the varied nature of these tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698896910487,
        "cdate": 1698896910487,
        "tmdate": 1699636463106,
        "mdate": 1699636463106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ekEFkkySeb",
        "forum": "LIW88mwqgv",
        "replyto": "LIW88mwqgv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_3Dhx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_3Dhx"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new way of preparing chain-of-thought demonstration examples by iterative bootstrapping with the training data. To construct the few-shot demonstration pool with reasoning chain from the training set, the framework first generate the initial reasoning chain with Zero-Shot-CoT. It then iteratively refines examples with incorrect answers with the LLM itself, and finally summarize the final reasoning chain once the correct answers are derived. Since the refined require guidance on correctness of the answer, the authors experiment with both using the ground truth label, and using an LLM as evaluator. At inference time, N random examples are sampled from the prepared pool and fix as the few-shot examples. Experiments on 10 datasets covering arithmetic, commonsense and symbolic reasoning showcase that the proposed method achieves new state-of-the-art results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper propose a new way of constructing few-shot chain-of-thought examples to use for in-context learning. It is the first to apply self-refinement and LLM self-evaluation in construction demonstration examples, and the experimental results are strong with and without ground truth labels.\n2. The authors evaluate the method with both proprietary GPT models and open source LLAMA models, and performs well in both cases. \n3. The authors conduct thorough ablation to examine different stages of the construction pipeline and impact of LLM evaluator's performance on final accuracy. These results are helpful for understanding the model performance and consistent with the main motivation to use iterative bootstrapping to improve exemplar quality."
            },
            "weaknesses": {
                "value": "1. It is mentioned that during the inference stage, a random N exemplars are samples as the fixed demonstrations for the entire test set. I have a few questions on this choice:\n    1. If only N random exemplars are used for the entire test set for inference, is it still necessary to construct the demonstration pool based on the entire training set? Or what is the use of \"pool\" here since only N exemplars are used.\n    2. Is is possible to have comparison with the baselines use the same set of few-shot examples, just with different reasoning chain annotations. Since the contribution of the paper is mainly on the construction of the reasoning chains, I feel it is better to have the selection of examples consistent for fair comparison. Meanwhile it would be interesting to see if the example selection strategies used in Complex-CoT and Auto-CoT are helpful for Iter-CoT or not.\n2. While the experiments are conducted over 10 different datasets, most of them are on the easier side and has some synthetic nature. This also reflects on the results, where the LLMs generally achieve very high scores on these datasets already. It would be better if the authors could evaluate on other more realistic and challenging datasets."
            },
            "questions": {
                "value": "1. Please see weakness above on the random selection of demonstration examples."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4803/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4803/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4803/Reviewer_3Dhx"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699161825478,
        "cdate": 1699161825478,
        "tmdate": 1699636463015,
        "mdate": 1699636463015,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lpmZITrm3E",
        "forum": "LIW88mwqgv",
        "replyto": "LIW88mwqgv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_gmng"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4803/Reviewer_gmng"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an approach called Iter-CoT that generates accurate rationales through iterative bootstrapping. Initially, Iter-CoT produces a rationale via zero-shot-CoT. If the answer derived from this rationale is incorrect, the rationale is revised until it's correct. Finally, all generated rationales are simplified through summarization. The authors apply this framework to construct few-shot demonstrations of training examples and demonstrate that this method outperforms existing baselines for zero-shot-CoT and even the baselines with ground-truth annotations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Overall, the writing is clear and easy to follow. In addition, the organization of the main draft is well-established.\n2. Improving the reasoning capability of LLMs is an interesting and important problem. To this end, considering the quality of few-shot demonstrations is a reasonable and well-motivated direction. \n3. The proposed method is simple and can be applicable regardless of the types of LLMs. Also, it shows a significant improvement compared to the existing baselines, well-studied in the same problems."
            },
            "weaknesses": {
                "value": "1. While the improvements are significant, the explanation for such gains is not sufficient. For example, the proposed Iter-CoT outperforms both Manual-CoT and Complex-CoT in Table 1, which use the ground-truth annotations of rationales. Since Iter-CoT is a method to generate correct rationales of unannotated samples, both methods can be considered as the upper bound as they always use the ground-truth rationales. Therefore, it\u2019s not natural that Iter-CoT shows better performance than those ones. What is the source of such improvement?  \n2. One of the major concerns is an increase in cost due to the iterative usage of LLMs, but there is no discussion regarding this. How many costs are required for Iter-CoT, compared to other methods? Also, it would be better if the authors could have a corresponding discussion in the draft. \n3. While the proposed method is applied to generate rationales of the training examples for few-shot demonstrations, it can be applicable during the inference. Also, the framework of bootstrapping the rationale has been widely explored [1,2]. Hence, I\u2019m wondering if the improvement can be enlarged when Iter-CoT is also applied during inference. \n\n### Minor\n\n1. It would be better to change the order of the presented method in Table 3, with the decreasing order for better presentation.\n2. Also, as the authors presented in the Appendix, STaR is highly relevant to the proposed Iter-CoT. It seems to be better to add this baseline to the main table if possible.  \n3. Regarding Figure 6, the authors mention that \u201cwe utilize the best exemplars in this section\u201d. What is the meaning of best exemplars? Also, how do you select them? Do you utilize another validation set to choose them?\n\n[1] Madaan et al., Self-Refine: Iterative Refinement with Self-Feedback., NeurIPS 23  \n[2] Shinn et al., Reflexion: Language Agents with Verbal Reinforcement Learning., NeurIPS23"
            },
            "questions": {
                "value": "Please answer the questions and concerns in weaknesses. If the responses are enough to address my concerns, I will raise my score accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4803/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699185019394,
        "cdate": 1699185019394,
        "tmdate": 1699636462947,
        "mdate": 1699636462947,
        "license": "CC BY 4.0",
        "version": 2
    }
]