[
    {
        "id": "PUklggInmT",
        "forum": "OWpp0TjdTt",
        "replyto": "OWpp0TjdTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_CHNF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_CHNF"
        ],
        "content": {
            "summary": {
                "value": "This paper studies batched policy learning in settings where both labeled and unlabeled data are available, both with known propensities. Based on an observation on the variance of (truncated) inverse-propensity-weighting (IPW) estimators, this paper proposes to penalize the IPW estimator with estimated KL divergence between the policy and base policy which is estimated using missing-label datasets. This idea is instantiated with a concrete algorithm with gradient descent, and extensive experiments are conducted to demonstrate the superior performance of the proposed method compared with other competitors."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper contributes to a significant research question, i.e., policy learning from logged data. It contributes to the existing literature by introducing a KL-regularization framework, and proposing to estimate KL with unlabeled data (while still being powerful when KL is in-sample estimated). \n\n2. Originality. The idea of regularization is not new (in fact there is some missing literature I'll discuss in the Weaknesses section), but the method presented here contains new techniques and contributes new insights. \n\n2. The quality and clarity of the paper is good overall. This paper is well organized and clearly presented. It contains fruitful results covering theory, algorithm, and experiments."
            },
            "weaknesses": {
                "value": "1. Related literature. \n\nIn general this paper does a good job of relating to existing literature, but the idea of penalizing with KL divergence is closely related to the literature on \"Pessimism\" for policy learning, e.g., [1] in offline RL, where value estimation is penalized with uncertainty estimation (related to how close a target policy is to the behavior policy), or [2] which is closer to the batched policy learning problem. The point is that by penalization and optimizing a lower confidence bound, one can achieve better performance (which the framework of Joachims et al, 2015 cannot due to truncation). I am curious whether the results in this paper can be useful in deriving a different LCB for that case. Maybe it is impossible to do so given the space limit, but some discussion can be interesting. \n\n[1] Jin, Y., Yang, Z. and Wang, Z., 2021, July. Is pessimism provably efficient for offline rl?. In International Conference on Machine Learning (pp. 5084-5096). PMLR. \n[2] Jin, Y., Ren, Z., Yang, Z. and Wang, Z., 2022. Policy learning\" without''overlap: Pessimism and generalized empirical Bernstein's inequality. arXiv preprint arXiv:2212.09900.\n\n2. Evaluation metric in experiments. \n\nThe paper could be improved by being more explicit on the evaluation metric -- is that the value of learned policy, as I understand the target as learning an optimal policy? It is a bit unclear to me what \"accuracy\" means in this context. \n\n3. benchmark in experiments. \n\nIs BanditNet the only competitor (in terms of complex learners) in this literature? At least there should be a discussion on this point."
            },
            "questions": {
                "value": "1. Is unlabeled data so important for estimating KL?\n\nIt seems the only use of unlabeled data is to estimate KL (imputation is ruled out with solid discussion which I appreciate), but even if the KL is estimated with labeled data, the performance is fine. I guess the reason is that the estimation error of KL is secondary in Equation (8) because of a $1/n$ factor. Labeled data should be able to learn KL with $O(1/\\sqrt{n})$ error which doesn't really hurt? \n\n2. How does this method relate to pessimism-based methods?\n\nThis is the same as Weakness point 1. \n\n3. Two questions on experiments in Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Reviewer_CHNF"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5165/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698541747944,
        "cdate": 1698541747944,
        "tmdate": 1700775873959,
        "mdate": 1700775873959,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s5JKeTYGL8",
        "forum": "OWpp0TjdTt",
        "replyto": "OWpp0TjdTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_G8No"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_G8No"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of off-policy learning using logged bandit data, where some data have unobserved action rewards, called missing-rewards data. To handle this problem, the authors first derive an upper bound on the true risk under the inverse propensity score estimator in terms of divergences between the logging policy and a parameterized policy. These KL divergences are independent of the reward function. The authors leverage this to propose a learning algorithm with reward-free regularization which leverages the\navailability of the logged known-reward data and the logged missing-reward data.  For the practical algorithm, they also propose a consistent and asymptotically unbiased estimator of KL divergence and reverse KL divergence between the logging policy and a parameterized policy. Finally, the effectiveness of their proposed algorithm is demonstrated through simulation experiments, demonstrating their advantages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow. The theoretical definitions and most of the related works are clearly explained. \n- The approach and the derived upper bound for the true risk are original.\n- The experiments are enough to demonstrate their approach."
            },
            "weaknesses": {
                "value": "- Although the derived upper bound is new, the approach for the learning algorithm to leverage this bound is quite straightforward. In addition, the practical algorithm for their approach is lack of a theoretical guarantee (i.e.,  an upper bound for the regret).  \n\n- The experiments only include two datasets. It would be better to add a real-life example in the bandit setting."
            },
            "questions": {
                "value": "- The authors mention that the direct method where we estimate the reward function fails to generalize well. What happens if we use a neural network to estimate the reward function?\n\n- Compared to the CRM framework (Swaminathan & Joachims, 2015a) which uses empirical variance minimization, they minimize the divergence KL. Could the authors explain the advantages of using this divergence KL compared to the empirical variance?\n\n- In Table 1, why does the proposed algorithm significantly outperform the logging policy for the FMNIST dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5165/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899752109,
        "cdate": 1698899752109,
        "tmdate": 1699636511798,
        "mdate": 1699636511798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wn3PyBBCIS",
        "forum": "OWpp0TjdTt",
        "replyto": "OWpp0TjdTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_KCYk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_KCYk"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the problem of batch off-line learning for contextual bandits when some of the rewards are missing from the logged dataset. The proposed approach comprises of two components: augmenting the logged dataset by predicting the missing rewards in the logged dataset using an outcome model trained on the observed samples, and estimating the importance weights on both the missing and non-missing reward observations (contexts and actions are not assumed missing). Experimental results show strong performance of the proposed estimator."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Batch off-policy learning is a common and important problem in practice. The authors identify a particularly interesting setting where rewards are missing from the responses. The use of the importance sampling weights is well motivated by noting the relationship between the importance weights and the risk of the respective policies. The use of pseudo labels is also an interesting approach."
            },
            "weaknesses": {
                "value": "My main issue with this work is that the presentation is such that it is difficult to parse the contribution of the work. The authors place great emphasis on their result on relating the risks to the KL-divergence (an other Bregman divergences), but then also introduce methodology without a lot of details. It's also not entirely clear to me which assumptions are being employed here in order to make the method applicable. The authors note that access to the true logging policy, however it would also seem that at a minimum there is also necessary conditions on overlap. It also isn't entirely clear to me what assumptions are being made on the missingness process. Are we assuming that the reward is missing completely at random here? If not, there would seem to be an issue with a biased estimate of the missing rewards."
            },
            "questions": {
                "value": "I inlined a few questions above, but it would be great if the authors could more completely describe the assumptions here and where they need to be employed in the theoretical results. \n\n* Given that you are augmenting missing observations with an outcome model, it would seem that a relevant comparison would be a Q learning approach for off-policy optimization. \n\n* It would seem that the authors should also discuss some of the work on causal inference with missing outcomes, e.g. https://arxiv.org/abs/2201.00468 and https://arxiv.org/abs/2305.12789"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5165/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699391614353,
        "cdate": 1699391614353,
        "tmdate": 1699636511711,
        "mdate": 1699636511711,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r5U5AIswM3",
        "forum": "OWpp0TjdTt",
        "replyto": "OWpp0TjdTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_p1oi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5165/Reviewer_p1oi"
        ],
        "content": {
            "summary": {
                "value": "They provide a semi-supervised method for logged data with missing rewards. Their method is built on top of the IPS policy gradient methods, with regularization terms. The regularization term is the KL divergence or the reverse KL divergence between the learned policy and the logging policy, which basically constrains the learned policy not too far away from the logging policy. \n\nThey propose an upper bound on true risk under the IPS estimator in terms of different divergences."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1, The idea is straightforward and well-motivated. \n2, Theorem 1 bound the IPS reward on all samples by the known reward samples, which build the foundation of their proposed algorithm.\n3, Experiments result of the neural network policy is good and the improvement compared to the baseline is huge."
            },
            "weaknesses": {
                "value": "1, The paper writing is not clear enough. \n2, The proposed method lacks novelty. This is a policy constraint method.\n3, Doesn't explain what's the advantages of their method when dealing with the missing reward samples.\n4, The writing of introduction only introduces the background and doesn't include the motivation of their method. Also, the logic of the intro and the related work is not good enough. For example, in section 5, the Pseudo-labeling algorithm part is confusing, which makes me believe that you are studying a Pseudo-labeling algorithm. I think they should move this part to the intro or related work. \n5, The experiment is not convincing since they only have 2 datasets. Also, the linear case has bad results. The cifar-10 dataset is bad when using a linear classifier in supervised learning, so I wonder why they use a linear policy. \n6, They assume the logging policy is known, which is not always practical. And we all know that the policy constraint methods suffer from the estimation of the logging policy."
            },
            "questions": {
                "value": "1, When to use WCE and when to use KL divergence? \n2, How to choose v in to truncate the importance weight?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5165/Reviewer_p1oi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5165/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699414923518,
        "cdate": 1699414923518,
        "tmdate": 1699636511587,
        "mdate": 1699636511587,
        "license": "CC BY 4.0",
        "version": 2
    }
]