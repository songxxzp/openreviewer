[
    {
        "id": "AiFwOTXIV4",
        "forum": "SQpnEfv9WH",
        "replyto": "SQpnEfv9WH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_KGKG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_KGKG"
        ],
        "content": {
            "summary": {
                "value": "- The paper introduces Social-Transmotion, a model for human trajectory prediction leveraging transformer architectures to process diverse visual cues.\n- The model innovatively utilizes the concept of a \"prompt\" from Natural Language Processing, which could be x-y coordinates, bounding boxes, or body poses, to augment trajectory data.\n- Social-Transmotion is adaptable to various visual cues and employs a masking technique to ensure effectiveness even when certain cues are unavailable.\n- The paper investigates the importance of different keypoints and frames of poses for trajectory prediction, and the merits of using 2D versus 3D poses.\n- The model's effectiveness is validated on multiple datasets, including JTA, JRDB, Pedestrians and Cyclists in Road Traffic, and ETH-UCY."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of prompting human trajectory prediction seems novel to me. Incorporating (optional) bounding box sequences and/or 2D/3D sequences makes sense, which would likely lower prediction errors and also be useful in real-world applications. The proposed framework could also be potentially scalable to other prompts (e.g. video features, scenes, etc.)\n- This paper is in general well-written, with adequate experiments to support the claim."
            },
            "weaknesses": {
                "value": "- I like the motivation of 'What if we have imperfect input?'. Nonetheless, to better support the claim, authors could consider more realistic input artifacts (e.g. use real detectors, masks for detection failures, etc.) in addition to Gaussian noises. The notation of '-188.4%' is somewhat confusing; using '+' should be fine.\n- Qualitative comparison with regard to existing baselines would be beneficial for better understanding the performance improvement."
            },
            "questions": {
                "value": "This work uses multiple prompts but only decodes trajectories. Can the authors also discuss how to explore predicting finer body motions (pose) and the relationship with regard to previous works in multi-person motion prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9161/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9161/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9161/Reviewer_KGKG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9161/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570386617,
        "cdate": 1698570386617,
        "tmdate": 1699637153474,
        "mdate": 1699637153474,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xUtvKUGliC",
        "forum": "SQpnEfv9WH",
        "replyto": "SQpnEfv9WH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_qMC2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_qMC2"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach for pedestrian trajectory prediction, focusing on incorporating various visual cues such as 3D poses and bounding boxes, in addition to the traditional trajectory information. The model, named Social-Transmotion, utilizes a dual-transformer architecture to effectively integrate these visual cues and enhance the prediction accuracy, especially in challenging scenarios involving interactions among pedestrians.\n\nThe results demonstrate that the inclusion of 3D poses significantly improves the model's performance, outperforming other state-of-the-art models and various ablated versions of itself. The model also shows robustness against incomplete or noisy input data, highlighting its practical applicability in real-world scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Incorporation of Visual Cues: The model effectively utilizes additional visual cues like 3D poses and bounding boxes, which is a significant advancement over traditional trajectory-only models.\n\n- Robustness: The model demonstrates robust performance even when faced with incomplete or noisy input data, showcasing its reliability for real-world applications.\n\n- Performance: Social-Transmotion outperforms various state-of-the-art models and its own ablated versions, indicating its effectiveness in pedestrian trajectory prediction."
            },
            "weaknesses": {
                "value": "- Lack of Commonly Used Benchmarks: Some commonly used datasets are not used such as nuScenes, Agroverse 1/2, Waymo Open Motion Dataset, etc. These datasets are often used to evaluate the performance of trajectory prediction methods.\n\n- Complexity: The inclusion of various visual cues and a dual-transformer architecture might make the model computationally intensive, potentially limiting its applicability in resource-constrained environments.\n\n- Dependence on Accurate Pose Estimation: The model's performance is significantly enhanced by the inclusion of 3D poses, which necessitates accurate pose estimation. Inaccuracies in pose estimation could potentially degrade the model's performance.\n\n- Limited Exploration of Failure Cases: While the paper mentions the provision of failure cases in the appendix, a more thorough exploration and discussion of these cases within the main text could provide valuable insights for further improvements.\n\n- Missing Relevant Recent Baselines: [1-4] are some recent methods that are relevant to this work.  \n[1] Uncovering the Missing Pattern: Unified Framework Towards Trajectory Imputation and Prediction, CVPR 2023  \n[2] Query-Centric Trajectory Prediction, CVPR 2023     \n[3] Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction. CVPR 2023   \n[4] AdamsFormer for Spatial Action Localization in the Future, CVPR 2023"
            },
            "questions": {
                "value": "1. How does the computational complexity of Social-Transmotion compare to other state-of-the-art models, and what are the implications for its real-world applicability?\n\n2. Could you elaborate on the model's performance in scenarios with inaccurate or noisy pose estimations, and what strategies could be employed to mitigate potential performance degradation?\n\n3. Are there specific types of interactions or scenarios where Social-Transmotion particularly excels or struggles, and what insights can be drawn from these cases?\n\n4. How does the model handle occlusions, and what is the impact on its performance when key visual cues are partially or fully obscured?\nCould you provide more details on the failure cases mentioned, and what lessons were learned from these cases to further improve the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9161/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791267692,
        "cdate": 1698791267692,
        "tmdate": 1699637153361,
        "mdate": 1699637153361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "J9MPhC5rU8",
        "forum": "SQpnEfv9WH",
        "replyto": "SQpnEfv9WH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_mJke"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_mJke"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a transformer based approach for motion prediction. It focus on using visual cues alongside agent location to predict the future. The future can be position, pose or bounding box. The future is made into a prompt along side the visual cues.  A cross modality transformer is used to combine the different modalities the another transformer is used for motion prediction.  The reported results are good in comparison with previous work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The model can handle different \"types\" of motion predictions [pose, position, bounding box] \n- The selective masking techniques which can be seen as a form of data balancing is being employed for better results \n- The latent input is a nice approach for these problems, similar encoding can be found in [1] where an encoder was used to generate a codebook. \n- The discussion section is rich. For example the analysis of imperfect data with degradation percentage is valuable for the domain. The masking behavior is similar to the work of [2]. \n\n\n\n[1] MotionGPT: Human Motion as a Foreign Language\n[2] Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks"
            },
            "weaknesses": {
                "value": "- In the introduction, it was mentioned that \"traditional predictors have limited performance, as they typically rely on a single data point per person (i.e., their x-y coordinates on the ground) as input.\" This can not be a general statement as works such as [1] and following ones do tackle the point using such cues.  Also, it seems this work is not mentioned in the related work section. \n- In the related work section there need to be a balance between the 3 modes supported in the work, where there is a literatures for each mode with different directions. \n- Figure 2 doesn't show the path for the visual cues mentioned in 3.2. Or there is a confusion between the word \"visual\" and \"spatial\" cue? \nDid the authors mean spatial cues such as pose, bounding box or visual cues such as the partial image of the scene beside the spatial cues? \n\n\n\n\n\n[1] SoPhie: An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints"
            },
            "questions": {
                "value": "- I'm strongly wondering about the ADE/FDE results. The proposed model output is deterministic where most of the method reported in the table are probabilistic except Social-LSTM. I'm only aware of [[1]-appendix c] where there is an approach to compare deterministic and probabilistic models. What is the authors comment on this? \n\n- Another suggestive study, the impact of training data amount on the model performance. It seems from section 3.2 that data imperfection might impact the performance. What about the impact of data quantity? like using 10%, 20% ... etc on the results? \n\n- The confusion between naming \"visual\" and \"spatial\" cues is impacting the readability/expectations of the article and need to be addressed. \n\n[1]Social-Implicit: Rethinking Trajectory Prediction Evaluation and The Effectiveness of Implicit Maximum Likelihood Estimation"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9161/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899646960,
        "cdate": 1698899646960,
        "tmdate": 1699637153239,
        "mdate": 1699637153239,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ylq02dsF0H",
        "forum": "SQpnEfv9WH",
        "replyto": "SQpnEfv9WH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_BUFd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9161/Reviewer_BUFd"
        ],
        "content": {
            "summary": {
                "value": "The paper presents another take on transformers being applied to the field of trajectory prediction. The authors claim the novelty of the approach being the inclusion of other visual cues into the transformer framework like 2D, 3D bounding boxes and keypoints. The paper reports results on various academic datasets and presents ablation on various input modalities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper is well written and easy to understand\n2) The paper presents good ablative analysis based of the input modalities that are used for the task of trajectory prediction. \n3) The paper evaluates results on various publically available datasets."
            },
            "weaknesses": {
                "value": "1) The paper lacks novelty as the use of transformers using multi-modal inputs for the task of trajecory prediction is already been studied extensively including works like Wayformer for example.\n2) The paper does not use larger industrial-academix datasets like Argoverse or Waymo open motion dataset to compile results against other transformer based benchmarks popular in the field today.\n\nGiven the above two major weaknesses, even with the nice experimental section and exhasutive results it is difficult to see how this work adds value to the field."
            },
            "questions": {
                "value": "It would be great to compare the architecture against more relevant transformer based baselines on industrial level datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9161/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698944181654,
        "cdate": 1698944181654,
        "tmdate": 1699637153135,
        "mdate": 1699637153135,
        "license": "CC BY 4.0",
        "version": 2
    }
]