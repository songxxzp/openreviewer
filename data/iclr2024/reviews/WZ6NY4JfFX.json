[
    {
        "id": "rzhDL7fbvb",
        "forum": "WZ6NY4JfFX",
        "replyto": "WZ6NY4JfFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the generative vision-language models, which have been the focus of many recent works. The authors introduce a novel approach, VisualGPTScore, for employing these generative models in discriminative tasks, particularly image-text alignment and retrieval. Empirical experiments suggest that blind language models occasionally outperform established methodologies. Building on this insight, the authors propose an additional post-processing step during testing to control \"language bias.\" In essence, this paper presents a promising avenue for harnessing generative model confidences effectively."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper introduces a probabilistic approach to generative model prediction confidences, exhibiting superior performance in comparison to Image Text Matching (ITM) formulations.\n* Rigorous experimentation and ablation analyses showcase the efficacy of VisualGPTScore.\n* The paper maintains a well-structured and articulate presentation."
            },
            "weaknesses": {
                "value": "* The paper's results under standard training and testing assumptions ($\\alpha = 0$) are exceptional, but the rationale for deviating from this assumption lacks proper motivation. Measuring $\\alpha^*$ demands test-time privileged information, which raises concerns about the approach's validity.\n\n* Addressing language bias is a crucial aspect, yet the method employed for debiasing, measured by $\\alpha$, appears to operate at the dataset level ($P_{test}$ vs. $P_{train}$) rather than the instance level. Evaluating the total effect, as $P(t|i) - P(t|i=\\phi)$, would provide a more meaningful approach to remove the \u201clanguage bias\u201d.\n\t\n* While the analysis is extensive in the context of I-to-T retrieval tasks, it falls short in terms of assessing a broader range of downstream tasks. Incorporating analyses of zero-shot classification, VQA2.0, and GQA tasks would offer a more comprehensive perspective."
            },
            "questions": {
                "value": "* If privileged information is employed to measure $\\alpha^*$, how does this impact individual biases such as \"black apple\" vs \"red apple\"? Shouldn't the value of  $\\alpha^*$ vary based on the specific target ($t$)?\n* What is the performance of VisualGPTScore on zero-shot classification? This would contribute to a more comprehensive evaluation of the proposed approach.\n* In the case of \"blind models,\" how is the measurement of $P(t^i_{positive})$ with respect to $P(t^i_{negative})$ conducted for each $i^{th}$ instance? Elaboration on the methodology for evaluating individual test instances in \u201cblind models\u201d is needed.\n* Is it plausible that negative captions rarely occur in web corpora? This factor might be affecting the performance of \"blind models\" and deserves further investigation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission721/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_AoET",
                    "ICLR.cc/2024/Conference/Submission721/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682934353,
        "cdate": 1698682934353,
        "tmdate": 1700627907989,
        "mdate": 1700627907989,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dOuQGfS6Od",
        "forum": "WZ6NY4JfFX",
        "replyto": "WZ6NY4JfFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission721/Reviewer_ANt2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission721/Reviewer_ANt2"
        ],
        "content": {
            "summary": {
                "value": "This is a scientific work that empirically analyzes language bias in image-text retrieval tasks and generative vision-language models (not image generation models, but text generation models). They first characterize the ability of generative vision-language models to match images to text in a zero-shot manner by measuring the probability that a textual sequence may be generated from an image. They then turn to a benchmark-centric view, empirically showing that several benchmarks can be solved even by blind LLMs in this manner, simply by their ability to flag linguistically unlikely captions from language priors. They show that with postprocessing, generative approaches can outperform handcrafted discriminative approaches on image-text retrieval tasks, even highly compositional ones."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "It is an open question to what degree vision-language models are doing the task of image-text retrieval as opposed to exploiting spurious correlations. This is dependent on the degree to which benchmarks themselves can be beaten by exploiting correlations. The major strength of this paper is that they (at least partially) answer this question. The experiments are convincing and cover a broad range of vision-language models. They show that even blind LLMs and VLMs do surprisingly well on these benchmarks, suggesting that existing benchmarks are in some sense, still not \"hard enough\" and contain correlations that can be exploited or can be solved by using language priors. \n\nThe proposed method for debiasing vision-language models works well, given the simplicity. I consider the simplicity and generality a strength. \n\nThe scientific conclusions of this paper are novel and useful for everyone designing new benchmarks for vision-language tasks."
            },
            "weaknesses": {
                "value": "A minor weakness of the paper is that they do not compare with the recent crop of truly \"large\" generative vision-language models like BLIP-2, LLAVA, etc. However, this is a minor weakness and I do not think it needs to be really addressed in this work, since these models are still new enough that training them is extra engineering work. Also, if anything, the language prior should be worse in LLM-based VLMs."
            },
            "questions": {
                "value": "I have no questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692215474,
        "cdate": 1698692215474,
        "tmdate": 1699635999321,
        "mdate": 1699635999321,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p5k7CtlbQe",
        "forum": "WZ6NY4JfFX",
        "replyto": "WZ6NY4JfFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission721/Reviewer_85bY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission721/Reviewer_85bY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a simple method to use the class of generative VLM for image to text similarity tasks. Although the proposed method is quite general, the paper evaluates it on the BLIP model and, as previously observed in the literature, shows that some Vision-Language benchmarks can be solved by only looking at the text modality. The authors also propose to reduce the reliance of VLMs on the language prior by using a probabilist post-processing calibration technique aimed at controlling the amount of linguistic bias of generative VLMs which is shown to improve image-to-text search results on the proposed benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The idea of calibrating a pre-trained generative VLM models to adapt to the \u201cformat\u201d of the test dataset is interesting and impactful."
            },
            "weaknesses": {
                "value": "1. **(Lack of novelty)**: Measuring similarity using the average log likelihood of strings given a fixed context (an image+prompt in this case) is not new. For example in the language community it has been previously used multiple times to provide alternative similarity measures to using encoder models (based on dot product similarity). Furthermore, such idea has already been extended to VLMs in [1], where it has been shown that \u201cmodels trained on captioning can perform on-par with models trained with the usual contrastive image-text matching loss.\u201d What is particularly novel about \"VisualGPTScore\"?\n2. **(Soundeness)** The paper briefly points out a connection between the proposed calibration approach and Mutual Information based approaches. How does this connection help the reader? What is the intuition that motivates using point-wise mutual information to improve the calibration of the deployed generative VLMs? Can the authors comment more on this? As of now, this seems more an afterthought rather than a clear and sound motivation.\n\n\n**Minor:**\n\nWhile the proposed VisualGPTScore is more efficient to be computed than next-token generation it is fair to point out that it is much slower than computing similarity scores based on dot products (e.g. CLIP) especially when the size of the retrieval index grows. See for example, [1, 2] and their techniques to limit the computational cost of performing image-to-text search over large databases. Can the authors comment more on this in the manuscript?\n\n[1] Antonie Miech et al. \u201cThinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers\u201d\n\n[2] J. Li et al., \u201cAlign before fuse: Vision and language representation learning with momentum distillation\u201d"
            },
            "questions": {
                "value": "1. The proposed trick to estimate the marginal over text p(t) is not sound. Why should averaging Gaussian noise fed as input to the VLM work more efficiently than averaging over the distribution of natural images? Is there any theoretical guarantee that this is the correct thing to do? Especially given the fact that Gaussian noise has never been used during training of the VLM and is therefore out of distribution for the model.\n    - I suggest the authors to perform the experiments using a more recent VLM like (LLaVA or BLIPv2, also BLIP is not SoTA) which both can be directly used to compute the language marginals.\n2. If VLMs are deployed as Zero Shot models why do we care about the gap between test and training (p_te vs p_tr) since the model do not use p_tr? \n3. I am not fully convinced by the empirical evaluation. Isn\u2019t it obvious that the proposed method with the optimal alpha performs better than any other method since it has been optimized (through Cross Validation) to find the best possible alpha for each dataset independently? I\u2019d expect a comparison with other baseline methods that calibrate the model\u2019s predictions before inference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728699414,
        "cdate": 1698728699414,
        "tmdate": 1699635999248,
        "mdate": 1699635999248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "azxSEdvutt",
        "forum": "WZ6NY4JfFX",
        "replyto": "WZ6NY4JfFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the zero-shot performance of generative VLMs in image-text retrieval tasks. A novel metric, VisualGPTScore, is introduced to compute the match score by generating a specific text string based on an image. Notably, the authors identify the train-test distribution shift and present a probabilistic post-processing method. This approach enables the regulation of linguistic bias in generative VLMs during testing without necessitating model retraining or fine-tuning. The proposed method sets new state-of-the-art results on several image-to-text retrieval benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Applying generative VLMs to image-to-text retrieval tasks is an intriguing endeavor.\n2. Comprehensive experiments have been conducted, achieving state-of-the-art results.\n3. The problem's formulation as a train-test distribution shift, followed by a probabilistic derivation, leading to the adjustable parameter alpha, is both logical and intriguing."
            },
            "weaknesses": {
                "value": "1. My primary concern is the application's real-world viability. Image-text retrieval, often utilized in search engines, demands high time efficiency. With this method, for every new image, the VLM must process all texts, resulting in substantial computational costs. In contrast, traditional methods like CLIP pre-compute text embeddings and only require a dot product with each image embedding. Hence, while the experimental results are impressive, I question this method's practical value.\n2. As demonstrated in Table 7 in the appendix, as the dataset size increases, the OTS scores progressively deteriorate, and the gap with ITM widens even when using the optimal alpha. How can this be explained? Might this indicate an inherent limitation of the method?"
            },
            "questions": {
                "value": "1. Following the first weakness, could you provide a time-efficiency assessment comparing various methods?\n2. The paper mentions, \"To apply this to our generative VLMs, we choose to sample 'null' inputs as Gaussian noise images.\" Why are Gaussian noise images suitable as \"null\" inputs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission721/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission721/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission721/Reviewer_eKtF"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission721/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811199144,
        "cdate": 1698811199144,
        "tmdate": 1699635999134,
        "mdate": 1699635999134,
        "license": "CC BY 4.0",
        "version": 2
    }
]