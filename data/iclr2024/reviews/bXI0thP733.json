[
    {
        "id": "4TcDljqNFt",
        "forum": "bXI0thP733",
        "replyto": "bXI0thP733",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_y1uP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_y1uP"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a sanitization-based defense against backdoor attacks.  Specifically, the authors learn generative models over class conditional feature space representations.  These learned distributions are then used to filter suspect training data, with the final model trained over the sanitized dataset."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Machine learning models are brittle, and as models are deployed in settings critical to human well-being, model failures can lead to real-world harm. This paper proposes a simple, intuitive method to improve model robustness. The paper follows the classic paradigm of using generative models to improve discriminative models' performance (i.e., robustness)."
            },
            "weaknesses": {
                "value": "Like all other empirical defenses, the authors' method comes with no guarantees on its effectiveness.  In my view, empirical defense papers must meet two necessary conditions to be fit for publication.  Unfortunately, this paper does not meet either.\n1. The paper should explicitly note that their method comes with no guarantees and contrast this weakness against the plethora of papers (e.g., [1]) that provide methods certifiably robust to training data attacks.\n2. The paper should ideally evaluate against an adaptive attacker who is aware of their defense and actively tries to avoid it.  At the very minimum, the paper should include a convincing discussion of why an adaptive attacker is not feasible or reasonable.\n\nThe authors define $D^y_{F}=\\\\{f_{\\theta_F}(x_i) \\vert y_i = y \\\\}$. As I understand it, $D^y_F$ contains the set of features for all training instances whose **true label** is $y$.  (Note at the bottom of page 3, I believe $y_i$ is defined as the true labels).  Of course, $y_i$ for the poisoned data is unknown (otherwise, the problem is trivial).  I cannot determine whether there is a problem with the notation or method, but it seems ${D}_F^y$ is not known as defined.  Perhaps the authors are assuming a clean validation set to learn these generative models (as in other work), but I do not see a discussion of that.  This is a major concern and one reason I rate the soundness as 1.\n\nIn the \"Questions\" section below, I detail a concern about overstating the paper's novel contributions.  I will wait for the authors' response before categorically defining it as a weakness.\n\nI **strongly** recommend either removing or redesigning Figure 2. The flow of the figure is very non-linear and non-intuitive. Best I can determine, the figure could have a linear progression starting at the initial poisoned dataset and terminating with the final trained model. Perhaps the authors chose this non-linear progression to save space, but I would view this as an especially poor choice.\n* One potential way to solve this problem entirely is to change Figure 2 to an algorithm.\n\nSeveral typographical issues exist in the paper.  I provide a non-exhaustive list below. These did not affect my overall score.\n* Page 1: \"...stealthines...\"\n* The authors repeatedly use `\\citet{...}` in place of `\\citep{...}`. See for example the two citations on page 1 in the paragraph that begins \"In this work, ...\".\n* Page 7: \"...fllowing...:\n* Page 7: \", Sample specific\" (\"S\" should not be capitalized here)\n* Page 7: \"sample0specific\"\n* Page 13: In multiple places, the letter \"x\" is used in math mode when specifying dimensions resulting in the \"x\" being italics.  Either do not place the x in math mode or better use `\\times` instead of \"x\".\n\nTable 4 should show the minimum poisoning rate where either the attacks or the defenses start to fail.  For example, does your defense still work at 0.1% poisoning rate.\n* Poisoning rate is also only one dimension of an attack's strength. Perturbation strength is an orthogonal dimension of attack strength against which the authors' defense is surely highly susceptible but is not explored in the empirical evaluation.\n\nThe empirical evaluation's main results use either a 10% or 25% poisoning rate.  In my view, those attack rates are wholly unrealistic for any marginally plausible real-world scenario.  I would go so far as to consider those poisoning rates not meaningful to study since I cannot see a case where an attacker is inserting 25% poisoned data.\n\nThe proposed method is studied only in the vision context. Other modalities are not explored or discussed.\n\n### References\n\n[1] Levine et al. \"Deep Partition Aggregation: Provable Defenses Against General Poisoning Attacks\" ICLR'20201."
            },
            "questions": {
                "value": "On page 2, you summarize the paper's second contribution writing, \"*We propose the first backdoor defense based on generative modeling*.\"  This is a very broad claim that I suspect is not true. For example, [1] uses backdoor modeling for a backdoor defense back at NeurIPS 2019.   Please speak more to the basis of this claim.\n\nAt the beginning of Section 3.1, the authors write, \"*We consider backdoor defenses that avoid standard supervised learning due to its sensitivity to poisoned labels and susceptibility to overfitting*.\" When I encountered this sentence when reading through the paper the first time, I did not understand what the authors meant, and after completing the paper, I am not sure I understood it.\n\n### References\n\n[1] Qiao et al. \"Defending Neural Backdoors via Generative Distribution Modeling\" NeurIPS'2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper proposes a defense against backdoor attacks. Better defenses can always theoretically facilitate better attacks in the future, but that risk is unavoidable and in this case remote."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698384091777,
        "cdate": 1698384091777,
        "tmdate": 1699637143819,
        "mdate": 1699637143819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WdaL9c9tV9",
        "forum": "bXI0thP733",
        "replyto": "bXI0thP733",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_U1ob"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_U1ob"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method to detect and mitigate data poisoning attacks by means of per-class generative modeling. Instead of training the generative models on image space, which is claimed ineffective, the paper proposes to model the latent embeddings extracted by a self-supervised-learning feature extractor, using per-class normalizing flows. Then, it detects backdoor classes based on the average log-density over all foreign examples. Next, it computes a poisoning score to split samples of the identified target classes into clean, poisoned, and uncertain sets. Finally, the samples in the poisoned set are relabeled and combined with the clean set to produce a cleansed dataset for training. The method effectively mitigates common dirty-label attacks and one clean-label method on CIFAR-10, ImageNet, and VGGFace2."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper explores a new approach for poisoning attack mitigation using generative modeling. While training the generative models on image space is ineffective, the paper proposes to use model the latent embeddings extracted by a self-supervised-learning feature extractor.\n- The method effectively mitigates common dirty-label attacks and one clean-label method on CIFAR-10, ImageNet, and VGGFace2."
            },
            "weaknesses": {
                "value": "- The last two sentences in Section 4.5 are confusing. Although the numerator is extremely low, why is the score significantly higher? Also, if the poisoned samples score significantly higher than clean samples, aren't those poisoned samples mislabeled into the clean set? Finally, the arguments may be invalid by ignoring the change of the denominator.\n\n- The paper experiments with only one clean-label attack, which injects adversarial noises into the poisoned data. Hence, it assumes that with clean-label attacks, the poisoned samples are completely distinct from the rest of the dataset in the self-supervised feature\nspace. That assumption may be incorrect with other clean-label attacks such as SIG [1] and Refool [2].\n\n- The paper should discuss some adaptive attacks. For instance, the attacker can tune the backdoor trigger to fool a surrogate SimCLR model trained on clean data.\n\n- The proposed method depends on too many hyper-parameters (\\alpha, \\lambda, \\beta_{ND}, \\beta_D). I cannot see how the selected values of these hyper-parameters are general and can work for all scenarios. The authors should ablate the choice of these hyper-parameters, particularly under different poisoning rates.\n\n- \\alpha is set as 0.15. \n  - First, it means 70% of samples of the identified target class are uncertain and will be removed, which is a lot. It will weaken the cleansed dataset significantly, particularly when multiple (or all) classes are poisoned.\n  - Also, all samples in the D_p are relabeled to a different class (Eq. 7), which is problematic if the percentage of the poisoned examples is less than 15% of the number of samples in the target class. For instance, in the case of CIFAR-10 with a 1% poisoning rate, the poisoned examples cover less than 9% of the samples in the target class, meaning more than 6% of the clean images in the target class are relabeled wrongly.\n  - 15% of the samples in the target class are set as clean. It becomes problematic if the poisoned examples cover more than 85% of the samples of the target class. That situation can happen when the number of classes is high, e.g., a classification task with 100 classes and a poisoning rate of 10%.\n\n- How does the algorithm behave in case the dataset is clean? And how does it behave under all2all attacks?\n\n[1]. Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In ICIP 2019.\n[2]. Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In ECCV 2020."
            },
            "questions": {
                "value": "- The last two sentences in Section 4.5 are confusing. Although the numerator is extremely low, why is the score significantly higher? Also, if the poisoned samples score significantly higher than clean samples, aren't those poisoned samples mislabeled into the clean set? Finally, the arguments may be invalid by ignoring the change of the denominator.\n- The authors should run the analysis in Fig. 1 and the experiments in Table 1 using SIG and Refool attacks?\n- The authors should define and examine some potential adaptive attacks?\n- The authors should ablate the choice of the hyper-parameters (\\alpha, \\lambda, \\beta_{ND}, \\beta_D), particularly under different poisoning rates.\n- A more in-depth discussion on the impact of the value choice for \\alpha.\n- How does the algorithm behave in case the dataset is clean? And how does it behave under all2all attacks?\n\n-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698641658315,
        "cdate": 1698641658315,
        "tmdate": 1699637143712,
        "mdate": 1699637143712,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t7Z9N8zXf1",
        "forum": "bXI0thP733",
        "replyto": "bXI0thP733",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_gUtc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method for robust training of neural network classifiers against backdoor data poisonings. In short, backdoor attacks aim to create hidden associations between a trigger and a target class by poisoning a small portion of the training data. This can be done via attaching small triggers to the image and optionally changing the labels associated with each image. If the label is changed, the attack is called poisoned-label attacks, while attacks that do not change the labels are called clean label attacks.\n\nIn this paper, the authors proposes a three stage process for purifying the poisoned dataset and training a robust model that is free of backdoors. In the first step, they use a self-supervised method such as SimCLR to train a feature extractor on the poisoned dataset. Using this feature extractor, they then get the feature representations of all the training samples. In the second step, they train a generative model (here normalizing flows) for each class representation. Using these normalizing flows, they then defined a likelihood-based score function to identify poisonous samples from clean ones. This step is motivated by earlier observations on the feature space representation of backdoor attacks using self-supervised models. In particular, for samples that are likely to be poisoned-label attacks, the proposed method can identify the target class and correctify their labels. Some samples are also removed from the training dataset if they do not belong to any of the previous categories. Once this step is done, a neural network is finally trained over the purified dataset.\n\nExperimental results over CIFAR-10, ImageNet-30, and VGGFace indicate the effectiveness of the proposed method against poisoned-label and clean-label backdoor attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method is novel and interesting. It is based on an empirical observation around the feature space representation of poisoned data in the feature space of models trained with self-supervised learning. The use of normalizing flows to model the per-class latent space distribution is also novel.\n\n- Empirical results indicate that this three stage method can mitigate the effect of backdoor attacks. Even more interestingly, it can revive the poisoned-label samples and re-use them in the training process."
            },
            "weaknesses": {
                "value": "- Even though the proposed method is working well, it is highly inefficient and requires lots of compute. In particular, the proposed method starts with self-supervised pre-training of a feature extractor using SimCLR. Then, it trains _one_ normalizing flow _per each class_ to finally be able to get rid of poisonous samples and start training a robust model. Such extensive use of resources is quite intensive, and frankly speaking, might be redundant. The field of backdoor defense has came up with alternative solutions such as [1-4] that are far more efficient than the proposed solution where some of them just take one training round to give robust models. Apart from the expensive self-supervised training at the beginning, the proposed solution requires one flow-based model per class which means that its resources grows linearly with the number of classes. \n\n- In lieu of the previous issue, first the paper needs to include more recent baselines [1-3], and second, it is important to include the total training time (from start to delivering a robust model) for all of the methods. This way, the readers can have a better understanding of the computational efficiency of current methods.\n\n- There are certain parts in the paper that might cause confusion. For instance, the explanations given in Sections 4.5-4.6 are seem contrasting. On the one hand, the paper says that for clean samples the score $s\\_{y}(\\boldsymbol{z})$ is higher. On the other hand, the same score is also higher for disruptive poisoning. Figure 4 also shows the same trend for both the clean samples as well as disruptive attacks. I think that these two sections should be re-written (see below for questions), because currently it seems that some of the clean samples can also initially be removed by this method. If this is the case, it should be explained. Optionally, adding a diagram of step-by-step poisoned sample removal might also be helpful.\n\n[1] Liu, Min, et al. \"Beating Backdoor Attack at Its Own Game.\" _ICCV_, 2023.\n\n[2] Huang, Hanxun, et al. \"Distilling Cognitive Backdoor Patterns within an Image.\" _ICLR_, 2023.\n\n[3] Dolatabadi, Hadi, et al. \"Collider: A robust training framework for backdoor data.\" _ACCV_, 2022.\n\n[4] Hayase, Jonathan, et al. \"Spectre: Defending against backdoor attacks using robust statistics.\" _ICML_, 2021."
            },
            "questions": {
                "value": "- Claiming that this work is \"the first backdoor defense based on generative modelling\" is inaccurate. For one, MESA [5] has also used generative modelling as a solution to neural backdoors.\n\n- Why do we need to identify/remove poisonous samples using class conditional normalizing flows and then train another neural network of our task? In other words, can't we just use the per-class normalizing flow for classification as well? Running experiments on this scenario is highly encouraged.\n\n- Can you repeat the same process for generating Figure 1 for other attacks?\n\n- Based on the Figure 1 (left), the proposed method heavily relies on the fact that the poisoned samples in the target class are scarce. What happens if the number of poisoned samples (those with triggers) that use the same trigger are abundant? Experiments on this scenario is highly encouraged.\n\n- Section 4.5 and 4.6 are rather confusing. Can you please elaborate on the filtration procedure? The paper currently says that \"We include the samples with $\\alpha$ highest poisoning scores in $\\hat{\\mathcal{D}}\\_{\\mathrm{P}}$, and include the samples with the $\\alpha$ lowest poisoning scores together with samples from identified clean classes in $\\hat{\\mathcal{D}}\\_{\\mathrm{C}}$.\" Do these two steps done on the same score graph? Does this mean that some of the clean samples are also removed? Potentially, is this the reason for the under-performance of the proposed method in the case of high poison rate (Table 4)?\n\n- What experimental settings (number of epochs, etc.) are used for SimCLR? What is the architecture of normalizing flows?\n\n- As mentioned above, add the mentioned baselines and report the total training time for all of the methods to see the computational efficiency.\n\n- Why so many number of epochs (200) is used for training models? Usually, 120 epochs is enough to train ResNet models with SGD on CIFAR-10.\n \n[5] Qiao, Ximing, Yukun Yang, and Hai Li. \"Defending neural backdoors via generative distribution modeling.\" _NeurIPS_, 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698893478462,
        "cdate": 1698893478462,
        "tmdate": 1699637143617,
        "mdate": 1699637143617,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qZplFIaMi3",
        "forum": "bXI0thP733",
        "replyto": "bXI0thP733",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_rmHx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9081/Reviewer_rmHx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to model the per-class distribution with a generative model and uses it to sanitise the data against backdoors. The approach operates in a latent dimension as opposed to the input space. Under the assumption and empirical observation (Figure 3, Figure 4) that the poisoned samples will exhibit different density scores for their target classes, they use a threshold to identify the poisoned samples in two scenarios"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper builds on an empirical observation that comparing per-class densities over extracted features can reveal poisoning behaviour. \n\n- The ablation study shows that choice of feature representation doesn\u2019t significantly vary the performance with two models resulting in marginal differences in attack success rate and accuracy"
            },
            "weaknesses": {
                "value": "- I think the paper can improve with investigation of what makes a generative model stand out? Perhaps an investigation of how adaptive attacks can circumvent the threshold based detector?\n\n- Similarly, the paper can also benefit from investigation of the choice of threshold \\beta_ND and \\beta_D. \n\n- Figure 1 and its legend are a bit small to read"
            },
            "questions": {
                "value": "- How sensitive is this method with respect to choice of thresholds?\n\n- It is unclear what the limits of this approach are? Are there scenarios where this detector will fail to detect?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9081/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699331933145,
        "cdate": 1699331933145,
        "tmdate": 1699637143497,
        "mdate": 1699637143497,
        "license": "CC BY 4.0",
        "version": 2
    }
]