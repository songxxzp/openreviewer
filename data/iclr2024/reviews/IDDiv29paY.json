[
    {
        "id": "rHLCuLXagC",
        "forum": "IDDiv29paY",
        "replyto": "IDDiv29paY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
        ],
        "content": {
            "summary": {
                "value": "This paper examines the possibility of attacking pre-trained CLIP models, i.e. generating a fooling image to maximize its embedding cosine similarity with some given prompts. Three possible methods of attacking including stochastic gradient descent, latent variable evolution, and projected gradient descent are considered. Experiments are done on imagenet showing the effectiveness of proposed methods. It is also found that fooling images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Measuring the similarities between images and text is an important topic and it is surprising to see that with simple adversarial attacks, we are able to generate a fooling image that can simultaneously maximize its cosine similarity across multiple different prompts.\n\nThe use of latent variable evolution in the context of adversarial attack is interesting. With LVE, it does not require access to the model weights and thus overcoming the limitations of common gradient-based methods such as SGD and PGD.\n\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "I am not very convinced by the setting of the problem. In particular, I am not convinced why we need a fooling image that maximizes its possibility with many other prompts. What are the potential concerns of this vulnerability?\n\nIt seems that the methodology is not different enough with the existing literature. In particular, as admitted in the paper, none of SGD, LVE, or PGD is a technical contribution of this paper, and it seems that this paper is just evaluating those methods in the context of fooling CLIP.\n\nOnly original CLIP models are evaluated in the paper but there are many later improvement such as TCL (https://arxiv.org/pdf/2202.10401.pdf), ALBEF (https://arxiv.org/abs/2107.07651), BLIP (https://arxiv.org/abs/2201.12086), and so on. Do they have the same vulnerability? How does this vulnerability scale with the size of pre-training data (https://arxiv.org/abs/2210.08402)?"
            },
            "questions": {
                "value": "The title is misleading. If I understand correctly, the paper does not have pre-training experiments but only carry out evaluations of pre-trained CLIP.\n\nPlease see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_1X8T"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698177539364,
        "cdate": 1698177539364,
        "tmdate": 1699636220214,
        "mdate": 1699636220214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Zw16aeToMQ",
        "forum": "IDDiv29paY",
        "replyto": "IDDiv29paY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the adversarial attacks of CLIP models and proposes CLIPMasterPrints, a type of images that can maximize the CLIP scores with a wide range of prompts. This paper proposes three ways to mine these CLIPMasterPrints, SGD, PGD and gradient-free optimization when the model weights are inaccessible. Details experiments on image recognition tasks to show that extracted CLIPMasterPrints can fool the pretrained CLIP model on wide categories. The authors also study how to mitigate the attack risks from CLIPMasterPrints by mitigating the modality gap between text and image encoder in CLIP"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a new type of adversarial attacks for CLIP models CLIPMasterPrints. Given the fact that CLIP is the foundational vision-language models that have wide application, this topic plays an important role in mitigating the risks of misusing CLIP.\n\n2. This paper proposes several technical ways to mine the CLIPMasterPrints, from gradient based methods to non-gradient methods, which could cover diverse scenarios based on if the CLIP weights are accessible or not.  \n\n3. This paper also studies the way to reduce the risks of CLIPMasterPrints. Although the solution points to the existing finding (multimodality gap), it is still good to see the solution."
            },
            "weaknesses": {
                "value": "1. Limited experiments: this paper also conducts experiments on ImageNet. It is interesting to see if the conclusion still holds for other recognition dataset ( note CLIP is evaluation on dozens of datasets). Moreover, this paper also uses CLIP ViT-L models. It also interesting to see the performance on CLIP ResNet models."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2771/Reviewer_zgK9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698600198532,
        "cdate": 1698600198532,
        "tmdate": 1700689247610,
        "mdate": 1700689247610,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uGOlB8OAIg",
        "forum": "IDDiv29paY",
        "replyto": "IDDiv29paY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_W2Qv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2771/Reviewer_W2Qv"
        ],
        "content": {
            "summary": {
                "value": "The paper focus on mining images referred as \u201cfooling master images\u201d or \u201cCLIPMasterPrints\u201d that fool the CLIP model by obtaining higher image-text similarity scores compared to clean image-text scores. These CLIPMasterPrints are optimized to obtain hight similarity scores across different text embeddings. Authors show that these CLIPMasterPrints also generalize to semantically related text prompts that are not directly considered in the optimization process. They attribute the rationale for existence of such CLIPMasterPrints  to the not well aligned CLIP image-text embeddings. They countermeasure CLIPMasterPrints  by adjusting the CLIP alignment i.e. by shifting the centroids of image and text embeddings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tSimple approach to successfully fool against different text embeddings.\n-\tPaper is easy to read and understand.\n-\tIllustrations are clear."
            },
            "weaknesses": {
                "value": "Prior works have already shown that CLIP is vulnerable to adversarial attacks (Noever & Miller Noever, 2021; Daras &Dimakis, 2022; Goh et al., 2021). This work on fooling master images is a variant of such adversarial attacks utilizing already existing optimization algorithms like SGD, PGD and, LVE (Latent Variable Evolution) to craft an image with perturbations. The difference here is that adversarial objective aims to fool different text embeddings. Furthermore, countermeasuring CLIPMasterPrints is performed by shifting the centroids of image and text embeddings that is proposed in prior work Liang et al. (2022) (cited in the paper). Therefore, this limits the originality, novelty and technical contributions of this work."
            },
            "questions": {
                "value": "Despite the concept of CLIPMasterPrints for CLIP is interesting, the paper does not meet the criteria for conference acceptance. I suggest this paper to be a fit for workshop."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2771/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872156853,
        "cdate": 1698872156853,
        "tmdate": 1699636220065,
        "mdate": 1699636220065,
        "license": "CC BY 4.0",
        "version": 2
    }
]