[
    {
        "id": "QYsA0d5ruR",
        "forum": "wyHCt1P7SR",
        "replyto": "wyHCt1P7SR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach for reference-based multi-view synthesis that supports both image inpainting from reference samples and novel view synthesis of the reference image. They are all formulated as contextual inpainting tasks. The proposed ARCI enhances attention mechanisms in T2I models by learning correlations across different reference view with self attention and control novel view synthesis with cross attention. Both qualitative and quantitative experiments have been conducted to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper regards multi-view image synthesis as reference-based inpainting task, which is novel and provides an interesting direction for realizing NVS.\n    \n2. The results of novel view synthesis are excellent.\n    \n3. The proposed Block Casual Masking bridges the gap of converting a diffusion model to a AR-based generative model."
            },
            "weaknesses": {
                "value": "The paper needs to improve readability by structuring the content more logically."
            },
            "questions": {
                "value": "1. Do view embeddings require retraining for each image?\n\n2. For multi-view NVS task, does the LDM need fintuning for each new image tested?\n\n3. How to control the view direction of multi-view synthesis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Reviewer_RjKx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698469041827,
        "cdate": 1698469041827,
        "tmdate": 1699635993854,
        "mdate": 1699635993854,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5dc9FWJl4t",
        "forum": "wyHCt1P7SR",
        "replyto": "wyHCt1P7SR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission666/Reviewer_d1up"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission666/Reviewer_d1up"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the task of novel view image synthesis. The authors introduce Attention Reactivated Contextual Inpainting (ARCI) technique for both reference-based inpainting and novel view synthesis. The authors show comparison results on MegaDepth, ETH3D and Objaverse."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper conducts many experiments and shows extensive quantitative and qualitative comparisons for different applications in novel view synthesis.\n2. The authors show some good results in reference-based image inpainting."
            },
            "weaknesses": {
                "value": "1. The definition is a bit confusing. Why do we need the concept of local synthesis and global synthesis for novel view synthesis? What\u2019s the core challenge? Besides, the claimed local synthesis is actually the reference-guided inpainting task, and the proposed ARCI is built upon an inpainting model, which makes the target more like inpainting. \u00a0\n    \n2. The presentation is not clear enough. for example, in Figure 1, the authors should explain what the purple mask means. What is the input of the model, the green bounding box or the purple one? The inputs and outputs look very different in (a)-(d).\n    \n3. The authors claim in the introduction that they can use efficient task and view prompt tuning for novel view synthesis with frozen SD, However, in the experiments, the authors finetuned the whole stable diffusion and reported some results with Lora and fully finetuned results. I think the authors should grandly review and improve their presentation and make them clearer.\n    \n4. The comparison in Figure 5 is not fair. The authors should report the original results of Zero123 instead of re-training it with a suboptimal training setting. Besides, zero123 is able to take specific camera poses to generate the novel view, what\u2019s the input used here for both zero123 and ARCI? The authors should make sure all the models take appropriate inputs.\n    \n5. Some important experiments are missing. The authors should conduct an ablation study on adaptive masking. For comparison with zero-123 for novel view synthesis, the authors should show comparisons on GSO and RTMV following zero-123. For reference-based inpainting, the authors should also show comparison results by using some very different references following the setting of Paint-by-Example to verify the effectiveness of the ref-inpainting technique for open-domain cases.\n    \n6. What\u2019s the used task and view prompt? Specifically, do the task prompts indicate local or global? How to set the view prompts? Rotation angles or front/side/back view? If the model tasks as input an image beyond these views, then how does the ARCI work?"
            },
            "questions": {
                "value": "It is a bit hard for me to fully understand this paper. It looks like there are two different tasks (without much analysis and relations) in the same paper, i.e., reference-based inpainting and novel view synthesis. Besides, instead of following existing standard benchmarks, the authors design new comparison settings for different tasks in this paper. Many details are missing especially about the detailed setting of different tasks and view prompts. For example, how to correlate the concrete prompt with concrete views in the training dataset?  Please find more details in the Weaknesses and address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698546085558,
        "cdate": 1698546085558,
        "tmdate": 1699635993774,
        "mdate": 1699635993774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cv5DGw6QHc",
        "forum": "wyHCt1P7SR",
        "replyto": "wyHCt1P7SR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method to generate multi-view images. The major novelty is a combination of local synthesis and global synthesis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The results seem to be good. The numerical results indicate the method to achieve state-of-to-the-art performance.\n2. The proposed methods are general. It can be applied to multiple tasks, e.g. single-view NVS, multi-view NVS."
            },
            "weaknesses": {
                "value": "1. The writing is too bad. The paper has many confused words, unclear explanation, and unconvincing arguments. Here are some examples:\n    * Confused words. In abstract, \"Our contributions of ARCI, built upon the Stable Diffusion fine-tuned for text-guided inpainting, include skillfully handling difficult multi-view synthesis tasks with off-the-shelf T2I models, introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, and implementing block causal masking for autoregressive NVS. \" The sentences indicates there are 3 contribution: 1) handling difficult multi-view synthesis tasks with off-the-shelf T2I models, 2) introducing task and view-specific prompt tuning for generative control, achieving end-to-end Ref-inpainting, 3) Implementing block causal masking for autoregressive NVS. The phrase 'block causal masking' is first introduced here, which confuses readers. In addition, why implementing this can be counted as a contribution? I suggest the author explain what the block causal masking is first.\n    * Unclear explanation. In introduction, \"This task can be broadly categorized into two facets: local and global multi-view image synthesis from reference images. \" If I understand Figure 1 correctly, there seems to be 2 tasks, ref-impainting and novel view synthesis here, but the author write one task with 2 facets. I think this is a confusing definition.\n   * Unconvincing arguments. In introduction, 'They struggle to capture fine-grained correlations, including object orientations and precise object locations, between reference and target images. These nuanced details are pivotal for tasks such as multi-view generation, as exemplified by Ref-inpainting.' Is there any reference paper or experimental results supporting this arguments? \nThe above shows 3 typical examples and there are many more here and there. The paper writing does not satifiy ICLR bar and requires major re-writing.\n2. What's the motivation of this paper? 'However, adapting them for multi-view synthesis is challenging due to the intricate correlations between reference and target images.' This sentence is correct, this is not the problem of current methods, e.g. Zero-1-to-3. Why does this proposed method better than zero-1-to-3. I suggest the author to write the motivation clearly in abstract. \n3. In related work, 'Compared with these aforementioned manners, the proposed ARCI enjoys both spatial modeling capability and computational efficiency.' What is spatial modeling capability? Please define spatial modeling capability.\n4. In method, please define and formulate the two tasks first. \n5. ATTENTION REACTIVATED CONTEXTUAL INPAINTING is not intuitive. It's hard to image the technique by the name. I recommentd to use more intuitive method name.\n6. The method design seems to be complicated or the method writing is bad, which make the method design look complicated."
            },
            "questions": {
                "value": "I have many questions, but many of them are due to unclear writing. I recommend the author to re-write the entire paper and I can re-rate the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission666/Reviewer_frhM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698554013456,
        "cdate": 1698554013456,
        "tmdate": 1699662267958,
        "mdate": 1699662267958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "biY0IsteNW",
        "forum": "wyHCt1P7SR",
        "replyto": "wyHCt1P7SR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission666/Reviewer_a7pP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission666/Reviewer_a7pP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Attention Reactivated Contextual Inpainting (ARCI), a unified method to leverage pre-trained text-to-image diffusion models (e.g., Stable Diffusion) to complete inpainting tasks. The key ideas of ARCI are i) using the attention layers in T2I diffusion models to learn correlations across different views and ii) using cross-attention layers to inject extra control via prompt tuning. Experiments are conducted on Ref-inpainting (local inpainting) and novel view synthesis (global inpainting). ARCI shows superior performance on both tasks compared to existing methods and requires fewer extra parameters and training costs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes a unified framework for solving two challenging tasks. Although reusing the attention layers of pre-trained T2I diffusion models for various purposes has been common in recent works, such a general framework applicable to different generation tasks is still interesting.\n\n- Extensive experimental results are presented for both tasks, and the proposed ARCI is resource-friendly, requiring only a tiny amount of extra parameters and fewer training costs than other approaches. \n\n- The Ref-inpainting results are promising. The model is lightweight and also achieves state-of-the-art inpainting quality. The attention visualization is convincing and shows that the model learns to look at the correct locations in the reference images."
            },
            "weaknesses": {
                "value": "- The biggest issue of the paper is the writing quality, which makes the paper very hard to follow. Details are listed below.\n\n    1. The introduction is extremely long and poorly organized. Many points are made, but I cannot find a precise sentence that emphasizes the essential contribution of the paper. The two applications (Ref-inpainting and NVS) seem to stem from the unified ARCI approach, but the introduction always tries to separate them when discussing their challenges and claiming improvements.\n\n    2. While the introduction makes separate claims for the two tasks, the descriptions of the two tasks in Sec.3 are heavily entangled, making it hard to clearly understand how the model works for each task.\n\n    3.  Fig.1 to Fig.3 are very difficult to parse. The texts in the figures are too small. The inputs and outputs for each task are not clearly explained. The captions are not self-contained, and it is also very hard to link them to certain parts of the main text.  \n\n\n- There seems to be no *quantitative evaluation* for multiview image generation -- Table 2 of the main paper only provides results with a single target view. Since the paper claims improvements in multiview image generation, it is important to formally evaluate the consistency of the generated multiview images. \n\n- The proposed ARCI is limited by the autoregressive generation design and cannot produce many multiview images. While a potential tradeoff is to constrain the length of the condition, it would definitely sacrifice the quality/consistency of the generated images. Note that an important goal of multiview image generation is to extract the 3D object/geometry. Both the number of views and the multiview consistency are important when exporting a 3D model from the generated images. The proposed designs are suboptimal compared to recent works such as [SyncDreamer](https://arxiv.org/abs/2309.03453) and [MVDiffusion](https://arxiv.org/abs/2307.01097), which can generate 16 or more views in parallel.\n\n- Although the authors claim that ARCI outperforms Zero123 in novel view synthesis (NVS), Zero123 itself is not merely an NVS model. Zero123 can serve as a general diffusion model backbone with 3D shape/global priors, which facilitates many other approaches via fine-tuning or score distillation (e.g., [Magic123](https://arxiv.org/abs/2306.17843), [One-2-3-45](https://arxiv.org/abs/2306.16928), [SyncDreamer](https://arxiv.org/abs/2309.03453), etc). It is true that ARCI outperforms Zero123 in NVS, especially when the training budget is limited, but the scope of ARCI is much narrower than Zero123 given the more complicated designs."
            },
            "questions": {
                "value": "As detailed in the weakness section, I believe the poor writing organization significantly impairs the quality of the paper. At the same time, the value of the proposed ARCI for multiview NVS is not convincing -- there are no quantitative evaluations for multiview image generation, and the method design seems to be suboptimal when we need to generate a large number of consistent views."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission666/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745988830,
        "cdate": 1698745988830,
        "tmdate": 1699635993563,
        "mdate": 1699635993563,
        "license": "CC BY 4.0",
        "version": 2
    }
]