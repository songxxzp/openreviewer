[
    {
        "id": "qLd4lwPOZX",
        "forum": "fLf589bx1f",
        "replyto": "fLf589bx1f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_3d3T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_3d3T"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a value-based method for code generation with pretrained LLMs. In their opinion, their method fills a vacuum, due to the fact that all present LLM-based code-generation systems are policy-based in order to easily leverage pre-trained LLMs. The authors get around this difficulty via proposing initialisation and semi-gradient policy-improvement schemes in which the LLMs logits are visualised as Q-values. In addition to this, they propose a conservative Bellman operator in which the \"best action\" is not chosen based on the trained Q-values, but on the logits of the initial pre-trained LLM, which in their view stabilises training and alleviates the usual convergence problems of value-based RL.\nThe authors test their framework on a standard code-generation benchmark (the APPS benchmark) and use a relatively straightforward reward design. They report performance superior to, or competitive with, with an exhaustive set of baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors are correct in that value-based methods for code-generation were absent from the literature, and that the gap should be filled.\n- Their solution for doing so is quite novel and elegant, especially the idea of visualising the LLM's logits as Q-values, and use them for both initialisation and policy improvement.\n- They consider an exhaustive set of baselines.\n- They show performance superior to, or competitive with, baselines, and on a programming benchmark notorious for its high difficulty."
            },
            "weaknesses": {
                "value": "- While the framework is novel, its comparison with baselines gives mixed results: in some cases, it does not actually come out on top. This contradicts the authors' claim that value-based method should be more suited to code generation: all of the baselines are policy-based.\n- The authors filter programs at evaluation time using a different reward than the one used during training. This makes it hard to interpret their performance numbers. Could it be the filtering that is mostly responsible for them?\n- The authors' method needs to be applied to a pre-trained LLM, which is used not only for initialisation but also for training. This is only possible with LLMs whose checkpoints are publicly available. In the case of the largest LLMs available today, even with a checkpoint available, considerable computational resources would be needed to apply the method.\n- The paper has a bit of an idiosyncratic structure, with e.g. a whole section dedicated only to the reward filtering at evaluation time, a very small Related Work section, and no outline of the paper's contents in the introduction, which makes it a bit hard to read.\n- Some more proof-reading is needed as typos can be found here and there."
            },
            "questions": {
                "value": "Main question and concern: why do the authors use a filtering procedure at evaluation time based on a different reward model $\\tilde{r}(s,a)$ than the environmental reward $r(s,a)$ they use for training? Given that the real reward $r$ can be easily and cheaply computed (a point which the authors themselves make in the introduction), I cannot see the need for such a procedure, and the authors do not elaborate on it. The authors should either:\n - Justify this choice.\n - Recompute their performance numbers with filtering based on $r$ at evaluation time.\n - Re-train their method with the modelled reward $\\tilde{r}$ from the outset.\n\nOther concerns/questions/recommendations:\n- Could the mixed performance numbers be due to the conservative Bellman operator, which keeps the policy close to the initial checkpoint? Did the authors consider alternative definitions for $q(a|s)$, or to update it at some point during training?\n - Value-based methods, including DQN, can suffer from the so-called \"Deadly Triad\" of deep RL. Can the authors elaborate on how they avoid this pitfall?\n- It seems that the authors add a value head $V^r_\\theta(s)$ (besides its LM head) to their LLM. How is that done, and can it be done with a generic LLM?\n- In figure 2, it seems that the pass@1 metric is actually at its best at very beginning of training and that it never recovers afterwards. Is the figure only meant to be illustrative, or are those the real training curves?\n- Can the authors outline their paper's content and structure at the end of the intro?\n- In figure 3, it would be helpful to explicitly define what $\\ell$ and $p$ stand for.\n- Why is the Nucleus Sampling outlined at the end of section 3? In my opinion it would make more sense to have in section 5, since it appears to be part of the evaluation protocol. Or is it also used during training?\n- It would be helpful if the authors defined (e.g. in a caption) what \"Intro\", \"Inter\", \"Comp\", and \"All\", mean in tables 1 and 2. While the text does mention it, it would be helpful to readers to be more explicit.\n- Why do the authors not consider $k=1000$ in table 2, as they do in table 1?\n- The study on generalisation of the reward model (notwithstanding my concerns on its use, detailed above) would better be moved to an appendix.\n\n# Post rebuttal edit:\nThe authors have exhaustively addressed my concerns and revised their paper as I suggested. I shall therefore raise my score to 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1982/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1982/Reviewer_3d3T",
                    "ICLR.cc/2024/Conference/Submission1982/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1982/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420473240,
        "cdate": 1698420473240,
        "tmdate": 1700734956625,
        "mdate": 1700734956625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v4xdti95hK",
        "forum": "fLf589bx1f",
        "replyto": "fLf589bx1f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_F9ov"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_F9ov"
        ],
        "content": {
            "summary": {
                "value": "This paper derives a value-based approach for fine-tuning LLM's for code generation, motivated by the success of value-based methods in domains with similar constraints on data and reward modeling. The authors build on top of the Dueling DQN family of algorithms and design architectures and pretraining schemes for the Q/A/V functions that carefully incorporate the pretrained LLM policy in a way that provides mathematical guarantees that certain properties are met, such as ranking human solutions highly, leading to a policy function that is identical to the original LLM policy, and minimizing initial Bellman error. They then use a conservative Bellman operator based off greedy decoding, and show that they can derive a reward model from an inverse conservative Bellman operator without any additional training needed. Their approach is called Bellman-Coder. Experiments show that Bellman-Coder performs comparably to related work, and is often the best. In addition, the reward model generalizes to new domains and models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is very well-written. The authors do an excellent job of discussing related work, motivating the design of a value-based approach for code generation, explaining the challenges behind designing a value-based approach, and describing how their design tackles these challenges. The technical sections are presented with care to make sure readers don't get lost in symbols, and I found them easy to understand (at least at a medium-to-high level) despite not being a RL expert. The experiments are presented well. Everything about the writing and clarity is excellent.\n- The paper's approach is original and very high quality. It does appear that no prior work has leveraged value-based methods for code generation to the degree of success the authors present. Making progress on value-based methods for code generation seems like a significant research direction to pursue, given the importance of code generation as a primary application area of AI and the promise of value-based methods in other areas (as motivated by the authors). The only uncertainty of the significance might be due to the limited performance gains, but this is easily understandable given this paper seems to be the first to attempt it, and the uphill battle of designing such a method given the comparable ease of using a LLM as an off-the-shelf policy for policy-based methods. The authors took the harder route, and seem to have managed to get it to work comparably.\n- I'd like to emphasize how high quality the mathematical derivations seem. The authors seem to have a very valuable expertise in RL and have put it to good use in making this method work. The ablations in figure 2 attest to the effectiveness of their formulation, with some caveats."
            },
            "weaknesses": {
                "value": "The primary weakness of the paper is that the results are not much better than prior work. However, given how difficult the perceived approach seems to be, I think this is okay. \n\nI am not very up-to-date with the different techniques and evaluation metrics for RL code generation, so I can't comment too much on whether the evaluation is missing anything. But taking a quick look at related work, the authors seem to use the same benchmarks."
            },
            "questions": {
                "value": "In figure 2, why don't the approaches train for the same number of iterations? In the same vein, what do the dots signify on the dashed lines, and why do their distances change? \n\nis it true that the reward model coming for free in section 4 is possible for any Bellman operator? what are the preconditions necessary for this? I ask because the \"further comments\" right before Proposition 3.1 made me think that the conservativeness of the Bellman operator defined was needed, but it sounds like in section 4 that it works for any Bellman operator. (I do not have much expertise in this area)\n\nThe related work paragraph in section 1 only discusses work on supervised LLM training and RL alternatives for sequence generation. given the amount of highly related work discussed in other sections of the introduction, it seems like this paragraph should be renamed to something more precise. \n\nAlphaGo is described has having lots of off-policy data, but how does AlphaZero relate to the taxonomy of Figure 1? Isn't AlphaZero pretty much the same approach, but with all on-policy data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1982/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1982/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1982/Reviewer_F9ov"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1982/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767884500,
        "cdate": 1698767884500,
        "tmdate": 1699636129749,
        "mdate": 1699636129749,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wx9Om7UI3i",
        "forum": "fLf589bx1f",
        "replyto": "fLf589bx1f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_R5vH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_R5vH"
        ],
        "content": {
            "summary": {
                "value": "this work proposes a value-function based training/inference procedure for program synthesis. rather than training a model (a policy) that ingest context (e.g. given as natural language comments) and generates tokens of code, this work leverages a value function Q(s,a) = V(s) + A(s,a), where V and A are implemented as neural networks.\n\nIn order to make Q learning work, this paper proposes two \"tricks\", one being a good Q-value initialization, the other being a conservative bellman update. Without these tricks, the training procedure would either not converge at all or improve very slowly. \n\nOverall, this paper insists that they want to tackle the task of program generation by training a value function, and overcome a lot of challenges to make it work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "## quality: good\nthe overall quality of the paper is good, in that it even worked at all, and the lack of adhoc tricks.\n\n### that it even worked at all\n\nThe biggest strength of this paper is that it has shown that code generation _can be_ done with a value network, despite the large number of state x action pairs needs to be considered, the unavailbility of a good initial Q function, and the volatility of the Q function \"blowing up\". Evaluation shown that the proposed method is on par with the policy based methods in terms of pass@k metrics.\n\nThe knowledge that value-based approach for code generation can be done _at all_ is a great merit, and cannot be under stated.\n\n### lack of ad-hoc \"tricks\"\n\nI find all the \"tricks\" presented in this work well founded and easy to understand. The reward hacking part is minimal and frankly very reasonable. Thus I am convinced this approach would work on different program synthesis domains just as well."
            },
            "weaknesses": {
                "value": "## significance\n\nlack of evidence on why we need to do this to begin with\n\nThe following statement from the conclusion section is a very good motivation \" it is recognized being sample-inefficient (see e.g. Nachum et al., 2017), meaning poorly using off-policy data, even data previously generated\". Does this work actually demonstrate this with an experiment? I believe it should, to show (in a table) that the proposed method can \"learn more with less\", with x axis showing the amount of training data, and y-axis some performance metric. In case I have misread the paper and this is indeed one of the experiments, it should be top and foremost contribution this paper needs to claim. Currently this paper reads akin to \"we did it because we can\" but lacks the justification -- in the form of an experiment -- that this is something valuable to begin with.\n\nI could think of many other benefits of having a value function, for instance, sometimes people might want a top-k ranked programs, which could be tricky to do based on only policy, but very intuitive with a value based model.\n\nIf the authors are able to justify their work with some experimental evidence, showing the advantages of a value-based modeling, this work will be that much better.\n\n## clarity\nthis paper would benefit from a Figure1 that outline the overall \"workflow\" of the method. During reading of the paper, the phrase \"which is yet another difficulty one must overcome to train a value function .... we introduce technique X to tackle this problem.\" The number of tricks quickly adds up, making the paper difficult to follow.\n\nI feel the paper is overboard with definition and philosophy of \"what is policy-based\" and \"what is value-based / RL\". I believe these things have a very technical definition that the community can all agree with. The clean story could've been \"off-policy learning through value iteration for generation would be good, as it use less data. here's how to do it, here's an experiment showing how well we can do as a function of amount of human data required\".\n\nDefining program synthesis as \"generating program from natural language\" is imprecise, as there are other works of \"program synthesis\" that generates program from input-output examples alone, or those that work with constraint solvers that generates programs from correctness specifications. A better wording could benefit here, as I instinctively reacted poorly by the opening definition of program synthesis, thinking it is way too narrow."
            },
            "questions": {
                "value": "## time to take sample?\n\nThe \"go to\" solution of program synthesis in the presence of given input-output examples (test cases) is to simply generate a vast number of programs, test all of them against spec through execution, and pick the ones that passed the test cases.\n\nThus, suppose I am a practitioner, the metric that I really care about is, given a synthesis problem, complete with natural language descriptions and a set of input-output test cases, and a time budget of 1 minute, can my synthesizer find a correct program?\n\nThis allows different methods to be compared head to head. A \"dumb\" enumerator -- for instance, a bigram generator -- might actually be reasonable of it can generate vast numbers of programs quickly, and a \"smart\" enumerator might actually be non-performative due to the time it takes to sample a full program to even check.\n\nThus, I would like to ask for the times it take to sample a full program on average, when compared against some of the other policy based methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1982/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813538437,
        "cdate": 1698813538437,
        "tmdate": 1699636129670,
        "mdate": 1699636129670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cD3oi8YwqH",
        "forum": "fLf589bx1f",
        "replyto": "fLf589bx1f",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_dB3F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1982/Reviewer_dB3F"
        ],
        "content": {
            "summary": {
                "value": "The task is to generate code (sequence generation) given a natural language description, and to have that code pass the associated unit tests.\n\nThis is framed as an RL problem, and unlike most prior work it does not use an algorithm from the policy gradient family, but uses a sort of Q learning approach.\n\nThe main focus is on how to overcome the very long and large action space you get by treating each token as an action, and the sparse reward at the end. The main idea is to make good use of pre-trained language models that have been fine-tuned to generate code for given ground truth (description, code solution) pairs. \n\nUsing this is non-trivial however, and the paper takes a creative an impressive approach. \n\nThe main ideas are known but very nicely combined. Some highlights:\n- the pretrained model is used to initialise a sensible Q function before the RL training\n- decomposing Q(s,a) as state value + state-action advantage (dueling dqn)\n- conservative bellman updates to stabilise the rl training loop.\n\nA drawback of the approach is that it takes multiple steps, uses various tricky pre-training and residual fitting steps, and could be difficult to apply in practice. But this is not suprising since using Q learning here is challenging, and it is impressive the authors managed to make it work so well.\n\nDetailed comments:\n\n2.1 Policy: should say \"assigns an action *from the set* $\\Delta(\\mathcal A)$\" and generally this notation here needs checking.\n\nIn figure 2, why is zero training iterations the best? anyway this is a nice ablation / teaser for the paper.\n\nFig 3 is very cool indeed.\n\nGeneral question - since you sample a softened Q as your policy, you could actually apply policy gradients as well. What are the connections here? Could you combine your scheme with a policy gradient type of update?"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "See the summary."
            },
            "weaknesses": {
                "value": "See the summary."
            },
            "questions": {
                "value": "See the summary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1982/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698996503503,
        "cdate": 1698996503503,
        "tmdate": 1699636129612,
        "mdate": 1699636129612,
        "license": "CC BY 4.0",
        "version": 2
    }
]