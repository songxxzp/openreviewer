[
    {
        "id": "lreo7mzeZD",
        "forum": "9bSDTTDUIp",
        "replyto": "9bSDTTDUIp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_Xq2C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_Xq2C"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ERM++, which introduces several techniques to improve the ERM baseline on multi-source domain generalization. The techniques consist of that for training data utilization, model parameter selection, and weight-space regularization. Experiments on 5 widely-used domain generalization datasets with different backbones show the effectiveness of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper investigates many techniques for the ERM model to improve multi-source domain generalization, which helps other researchers in this field to find suitable methods for their research.\n\n2. The paper provides rich experiments to show the benefits of the utilized techniques for domain generalization."
            },
            "weaknesses": {
                "value": "1. Although the paper includes many techniques to improve the ERM model on domain generalization, most of these technologies have been proposed or are widely known. The improvement with these techniques is not surprising and inspiring.\n\n2. The paper argues to propose a general baseline for future domain generalization works with the existing techniques. However the experiments, such as Table 3 (a), Table 4, and Table 5 show that different techniques benefit different datasets or settings, which is not general to the domain generalization task.\n\n3. Except for the averaged accuracy, it is better to provide more insight of different techniques on how they benefit or harm the performance of domain generalization on different datasets."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685250664,
        "cdate": 1698685250664,
        "tmdate": 1699636642469,
        "mdate": 1699636642469,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WKVXB6kr4x",
        "forum": "9bSDTTDUIp",
        "replyto": "9bSDTTDUIp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_72G7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_72G7"
        ],
        "content": {
            "summary": {
                "value": "A training protocol containing three main components is introduced to compose ERM++ which is a new baseline proposed in the submission. By observing the critical of the training length, a two-stage training procedure is conducted by determining the training length with the source domain validation set performance while training on the source training set in the first stage and then in the second stage training on the whole set. Model weight initialisation and weight-space regularisation, namely MPA application, are also studied. For the weight space regularisation idea, I hold my option for the later discussion. ERM++ is evaluated on a large variety of DG benchmarks with significant improvement comparied with vanilla ERM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experiments are dense and comprehensive. The proposed baseline is evaluated on most existing domain generalisation benchmarks comprehensively. \n2. ERM++ explore existing practical technique tricks in DG with a detailed description."
            },
            "weaknesses": {
                "value": "1. The name is a little misleading as MPA is part of ERM++. If this is the case, it is natural to wonder what ERM + MPA performance will be like and by removing MPA from ERM++, then how ERM++ will perform. I think the closest setting is Table 4. Once MPA is added, for example comparing #1 and #2, the performance boosts significantly. The rest setting cumulates each tech one by one. But it is also important to know whether each one contributes independently. \n\n2. One of the main points made in DomainBed is that without a complicated algorithm design with fair hyperparameter running ERM is a very strong baseline. However, ERM ++ is way more complicated than ERM. \n\n3. Besides, since, it is justified by the submission that MPA works well with other introduced tricks, also it is good to know whether ERM++ is compatible with other advanced optimisation algorithms like SAM, GASM, SAGM, which is the benefit of using ERM. \n\n4. In terms of the training cost, ERM ++ is compared with other models such as MIRO and DIVA, but the comparison with ERM is more important."
            },
            "questions": {
                "value": "See the above sections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771407414,
        "cdate": 1698771407414,
        "tmdate": 1699636642358,
        "mdate": 1699636642358,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Jb1o4ZyrW4",
        "forum": "9bSDTTDUIp",
        "replyto": "9bSDTTDUIp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_4v1r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_4v1r"
        ],
        "content": {
            "summary": {
                "value": "This work provides a detailed study of techniques used to improve the performance of empirical risk minimization (ERM) in domain generalization. Three categories of improvements are combined (data utilization, initialization, and regularization) to achieve state-of-the-art performance with lower computational cost than competing strategies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The strategies used to maximize ERM performance for domain generalization are practically useful, and the resultant model is a competitive baseline for future work in the area.\n2. Extensive experiments were performed with many different methods, architectures, and datasets. The ablation studies are also well done.\n3. Careful analysis of the results were performed and edge cases were highlighted in the text. In particular, many of my initial questions were answered upon a closer read of the analysis, for example the discussion of VLCS performance in Section 5.1."
            },
            "weaknesses": {
                "value": "1. While the premise of the contribution - that ERM can match SOTA DG algorithms when appropriate data utilization, initialization, and regularization are applied - is important, the strong performance of ERM has been known since [1] and is not exactly novel. The main contribution of this paper is in applying recent \u201ctricks of the trade\u201d to further boost ERM numbers. While this may be helpful for practitioners, no new insight is offered as to why the proposed techniques are useful for ERM specifically or why ERM is a good baseline for domain generalization in the first place.\n2. Many of the proposed improvements to ERM are not actually specific to ERM and can be applied to other state-of-the-art methods to improve performance. For example, it would be helpful to see a comparison where some methods in Table 2 are run with better initialization (say, AugMix) to see whether ERM still outperforms them. As is, the comparisons are made on somewhat unequal footing.\n3. I believe the investigation of weight-space regularization is incomplete. First, why is ERM++ not run with SWAD, and is there any advantage of MPA in this scenario? Second, there is another category of weight-space regularization which is not included in the paper, namely sharpness aware minimization (SAM) [2] based techniques (e.g., SAGM [3]). SAM and SWA have been extensively compared [4] and found to each be beneficial in different circumstances. It would be interesting for the community to compare these two techniques in a DG setting, and I believe this experiment is necessary to claim a fully rigorous investigation of weight-space regularization for DG.\n\nThere is also a fair amount of confusing writing and typos, detailed in the next section."
            },
            "questions": {
                "value": "Here, I list some minor questions as well as suggestions for improving the writing.\n\n1. The reference [5] cited in Table 2 should also be cited in the introduction.\n2. The bar graph in Figure 1 is pixelated. If it was made with `matplotlib`, this can be fixed by setting the DPI or saving it as a PDF.\n3. There is an inconsistent use of dataset names and abbreviations in the tables (e.g., TerraIncognita vs TI vs TerraInc vs TerraInco). I would recommend using the full name of the dataset everywhere, and perhaps reducing the font size when it doesn\u2019t fit. The same goes for model names (e.g., Meal-V2 vs Meal V2 vs MV2).\n4. There is an inconsistent use of spaces before citations, (e.g., Author(Citation) vs Author (Citation)). I encourage the authors to use the ~ LaTeX character to create a small space before the citations, and to keep this consistent throughout the text.\n5. \u201cSketch\u201d is misspelled in Table 6.\n6. The headings in Table 7 are not explained. What are R0, R1, etc and P, I, Q, etc?\n\n***Recommendation***\n\nOverall, while this paper provides a useful benchmark on maximizing ERM performance for domain generalization, my concerns about novelty and the incomplete investigation of weight-space regularization cause me to lean slightly towards rejection rather than acceptance.\n\n***References***\n\n[1] Gulrajani and Lopez-Paz. In Search of Lost Domain Generalization. ICLR, 2021.\n\n[2] Foret et al. Sharpness-Aware Minimization for Efficiently Improving Generalization. ICLR, 2021.\n\n[3] Wang et al. Sharpness-Aware Gradient Matching for Domain Generalization. CVPR, 2023.\n\n[4] Kaddour et al. When Do Flat Minima Optimizers Work? NeurIPS, 2022.\n\n[5] Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 1999."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Reviewer_4v1r"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775692107,
        "cdate": 1698775692107,
        "tmdate": 1699636642246,
        "mdate": 1699636642246,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "moXuSza8pK",
        "forum": "9bSDTTDUIp",
        "replyto": "9bSDTTDUIp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_Pkyt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5994/Reviewer_Pkyt"
        ],
        "content": {
            "summary": {
                "value": "The paper present a new strong baseline, named ERM++, for the study of domain generalization (DG). By incorporating multiple previous results in training data utilization, parameter selection, and regularization, ERM++ achieved state-of-the-art performance on the DomainBed benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The obvious strength of the paper is the exciting results. The experimental settings are carefully described."
            },
            "weaknesses": {
                "value": "Despite presenting exciting results with elaborated experiments, the paper lacks technical insight into the effectiveness of various components, especially when they are used together. While I do see the merit of the engineering approach and agree that the field should appropriately acknowledge this as a baseline for large DomainBed, I do not think the current contribution of ERM++ is fit for a venue like ICLR. Thus, I cannot recommend acceptance for the paper."
            },
            "questions": {
                "value": "Why did the evaluation results for CMNIST and RMNIST not included in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5994/Reviewer_Pkyt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5994/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817779836,
        "cdate": 1698817779836,
        "tmdate": 1699636642120,
        "mdate": 1699636642120,
        "license": "CC BY 4.0",
        "version": 2
    }
]