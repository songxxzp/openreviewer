[
    {
        "id": "S6eHBBvPnh",
        "forum": "RlfD5cE1ep",
        "replyto": "RlfD5cE1ep",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_JjF9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_JjF9"
        ],
        "content": {
            "summary": {
                "value": "This paper represents an extension of prior work in the field of Self-Supervised Learning (SSL) theory, with a specific emphasis on elucidating how non-contrastive SSL methods prevent the issue of feature collapse. The paper's primary focus centers on the examination of the final feature normalization step and its role in the underlying dynamics. The authors furnish compelling evidence concerning the dynamics of the underlying eigenmodes, and the theory finds support through numerical simulations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper addresses an important and relatively underexplored issue regarding the role of feature normalization in non-contrastive Self-Supervised Learning (SSL). The authors demonstrate that the normalization step introduces sixth-order dynamics, resulting in the dynamic emergence of a stable equilibrium, even when dealing with initially collapsed solutions.\n\n2. The authors present compelling evidence, and their underlying assumptions appear to be quite reasonable.\n\n3. Numerical simulations validate the predictions made by the theory."
            },
            "weaknesses": {
                "value": "I would anticipate the theoretical framework to align with the behavior observed in real datasets. However, the paper does not investigate the dynamics in more complex scenarios."
            },
            "questions": {
                "value": "The authors mentioend BarlowTwins and VICReg. They effectively are still contrastive. How do you think their 'feature normalization' behavior is related?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698527185182,
        "cdate": 1698527185182,
        "tmdate": 1699636277922,
        "mdate": 1699636277922,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PPYkDUknaW",
        "forum": "RlfD5cE1ep",
        "replyto": "RlfD5cE1ep",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_HYk4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_HYk4"
        ],
        "content": {
            "summary": {
                "value": "This paper follows the previous setting in Tian et al. (2021) which explores simplified modeling for non-contrastive learning. It posits the representation model as an identity function, with both the projection layer and prediction layer streamlined into linear components. What distinguishes this study from its predecessors is the exploration of the commonly used cosine loss in practical applications. By applying these simplifications and introducing additional assumptions, the authors demonstrate that the norms tend to concentrate around some constants, which helps to simplify the learning dynamics with feature normalization. With further assumptions, the paper disentangles the learning dynamics into the sixth-order eigenmode dynamics in which a stable equilibrium emerges even if there is no stable equilibrium with the initial parametrization and regularization strength."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- This work proves that the feature norm concentrates around a constant with proper parameter initialization."
            },
            "weaknesses": {
                "value": "1. Some of the assumptions are quite stringent, especially since this paper is not pioneering work, and they may not provide much reference value for practical non-contrastive learning with negative pairs. \n2. Assumptions 2 and 3 in section 4 are rather strict. Assumption 2 requires that the input data follow an isotropic Gaussian distribution, which is hard to accept in practical situations. Perhaps a mixture of isotropic Gaussians could be considered. Assumption 3 pertains to the width-infinite limit.\n3. In section 5, the authors consider the norms of these linear layers as constants (Assumption 5). This assumption, however, is still far from providing a real dynamic analysis for the cosine loss. Since feature normalization may not guarantee convexity, smoothness, and Lipschitzness, its dynamic analysis should focus on proving the convergence rather than simplifying its complexity to obtain closed-formed dynamics. The existing conclusions do not provide much contribution and insight to understanding non-contrastive learning dynamics.\n4. The relevant numerical results still do not fully validate the reasonableness of these assumptions, such as the increasing error between $W$ and $W^\\top$, and the decrease in $N_{\\phi}$ and $N_{\\psi}$. Therefore, while I appreciate the authors for using Hanson-Wright inequality to demonstrate that some norms concentrate, it is still not particularly remarkable."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698545627406,
        "cdate": 1698545627406,
        "tmdate": 1699636277852,
        "mdate": 1699636277852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H47Qh9SzZJ",
        "forum": "RlfD5cE1ep",
        "replyto": "RlfD5cE1ep",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_sECc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_sECc"
        ],
        "content": {
            "summary": {
                "value": "This article proposes an extension of the theory of non-contrastive learning (BYOL, SimSiam) to consider the cosine loss rather than the L2 loss, showing how feature normalization changes from third-order dynamics to sixth-order. \nThey show that three regimes exist depending on the norms of the layers, which results in a shift between the three regimes as the norms decrease until the stable regime, where the eigenmodes converge."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This article presents an improvement other than the theoretical framework of non-contrastive learning using solely the Euclidian loss.\nThe paper is well-written and easy to follow. The assumptions taken are relatively well justified and allow for an interesting analysis."
            },
            "weaknesses": {
                "value": "**Previous literature** There have been recent contributions to the literature of non-contrastive learning which do take into account the cosine loss, and which are not referenced in this article. In particular, Halvagal et al., Implicit variance regularization in non-contrastive SSL, 2023. The eigenmode dynamics seem extremely similar (after some changes in the notation) and it seems extremely important to me that the authors compare themselves to this article. The authors also do not seem to have a similar conclusion on the implicit variance regularization that Halvagal et al. focus on.\n\n**Regimes** The three regimes found in Section 5.2. seem to have been found solely by categorizing the regimes experimentally shown in Figure 2 while reading, which seems like a weak justification for those regimes. A clearer analysis of the equilibrium points at least in the Appendix seems necessary. \n\nThe authors claim that as the norms decrease, the regimes fall to the stable one. However, in the stable regime, the norms will increase as the eigenmodes increase to the saddle point $p^+$. Is there a risk of the acute and stable regimes alternating between each other? \n\n**Experiments** Numerical experiments on the SimSiam model remain on linear networks in Section 5.4. A similar Figure to Figure 6 for a real network such as ResNet (maybe only focusing on the linear projection head) would help confirm the theoretical findings in the linear case. Otherwise, the link with a real SimSiam network remains relatively limited, except for the weight decay argument.\n\n**Figure 6c** I also find Figure 6.c. hard to read. Are the intervals the theoretical intervals using the values of the norms? In this case, what is the theoretical value of the saddle point? Having the values of a single eigenmode gives very little information on the values of the spectrum of $W$. Do all the values stay relatively constant like here?"
            },
            "questions": {
                "value": "The notion of Thermodynamic limit is novel to me in optimization and needs to be further explained. How is it different from the Neural Tangent Kernel regime, is it the constant ratio $\\alpha$?\n\nDo the authors have more intuition on the role of the exponential moving average in BYOL with their new findings?\n\nSmall remarks:\n* Intro: \"Folklore says\" is a somewhat strange way to quote an article.\n* Sec 4. Assumption 2. $\u03a3 = I $ seems superfluous to add after $D = N(0, I)$.\n* After Lemma 3: $\\Phi x'$ is not defined. \n* Equation (5): $\\hat H$ is not defined.\n* After Equation (9): \"unite learning rate\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3290/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3290/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3290/Reviewer_sECc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685812354,
        "cdate": 1698685812354,
        "tmdate": 1700737509597,
        "mdate": 1700737509597,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E4RTwnpHrB",
        "forum": "RlfD5cE1ep",
        "replyto": "RlfD5cE1ep",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_ZH8r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3290/Reviewer_ZH8r"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the dynamics of non-contrastive self-supervised learning (e.g. BYOL, SimSiam etc.) and shows how feature normalization can play a role in preventing the collapse of all representations to a single point. By studying this in the infinite dimensional limit, the paper shows, that with the cosine loss, the training dynamics are different from that with L2 loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The technical analysis appears rigorous and reasonably clear to follow."
            },
            "weaknesses": {
                "value": "1. Considering the majority of the analysis assumes norms of all features are nearly same, due to the high dimensional limit, I do not see how this analysis can show the effects of feature normalization on non-contrastive learning. \n\n2. Moreover, the notions of \"6-th order dynamics\"and \"3rd order dynamics\" are not sufficiently explained in the paper. \n\n3. Most importantly, I'm not convinced this an interesting problem to study in the context of prior work providing key understanding regarding how non-contrastive SSL training dynamics work."
            },
            "questions": {
                "value": "1. Considering the majority of the analysis assumes norms of all features are nearly same, due to the high dimensional limit, I do not see how this analysis can show the effects of feature normalization on non-contrastive learning. Can the authors explain why they believe this analysis is showing anything meaningful about feature normalization. \n\n2. Practically, are there any differences in the conclusions of the training dynamics of the cosine loss and the L2 loss? (while they may be of \"different orders\")."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3290/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818723337,
        "cdate": 1698818723337,
        "tmdate": 1699636277658,
        "mdate": 1699636277658,
        "license": "CC BY 4.0",
        "version": 2
    }
]