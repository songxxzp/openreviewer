[
    {
        "id": "hxKPlteW3l",
        "forum": "tFYcEUlUTt",
        "replyto": "tFYcEUlUTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_wp4W"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_wp4W"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a coarse-refine framework for long-term simulation. The model first predicts future states, providing auxiliary information for further refinement of the next step predictions. The results show lower errors comparing with baseline GNNs. The errors of long-term predictions accumulate less faster than baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method obtains lower errors for both short-term and long-term predictions."
            },
            "weaknesses": {
                "value": "The writing of the paper is poor. Several examples are as follows:\n1. Many duplicated expressions. E.g. Section 4.1 \"which are flexible to produce flexible outputs at any given timestamp\".\n2. Lack proper explanations and details. E.g., any explanation for the \"multi-task learning\" that is used to train the model?\n3. Poor organizations and missing labels for figures. E.g., in Fig 5, what do the \"a,b,c,d\" refer to? The color in Fig 5 is not clear to distinguish different settings."
            },
            "questions": {
                "value": "Can the coarse-refine framework apply to the baselines? Will the proposed framework benefit them also?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Reviewer_wp4W"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2108/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698323206401,
        "cdate": 1698323206401,
        "tmdate": 1699636143509,
        "mdate": 1699636143509,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HGaoOQ3u0D",
        "forum": "tFYcEUlUTt",
        "replyto": "tFYcEUlUTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_L1zc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_L1zc"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a new meshed-based physics simulation approach named FAIR. The key idea is to use a coarse-to-fine learning paradigm to reduce the accumulated errors in long-term prediction. The model uses a continuous graph ODE module for generating coarse long-term predictions, subsequently refining them using interpolation techniques to derive short-term predictions. \n\nFAIR is extensively evaluated on four benchmarks, showing superior performance in both short-term and long-term forecasting when compared to various baselines. The paper also provides ablation studies, sensitivity analysis, and visualization to validate the effectiveness of the proposed model. \n\nIn general, the paper presents a well-motivated approach that delivers impressive empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clarity: This paper is clearly written and easy to follow.\n2. Novelty: Unlike previous one-step prediction methods, the paper introduces a two-stage, foresight-and-interpolation method to improve the long-term prediction results of learning-based physics simulators. Overall, the architectural design of graph ODE and channel aggregation is reasonable.  \n3. Experiments: The proposed model showcases impressive results over existing models."
            },
            "weaknesses": {
                "value": "1. The proposed model is closely related to GMR-GMUS (Han et al., 2022), which similarly aims to enhance long-term prediction in mesh-based simulators. Notably, GMR-GMUS achieves long-term predictions for 400 time steps in the case of Cylinder Flow. Including a comparison between FAIR and this approach in the experiments would be valuable.\n2. In Table 1, for the DeformingPlate experiment with prediction lengths of 50, MS-GNN-Grid has the best result with an RMSE of 2.78. It would be beneficial to highlight this result in bold and provide some discussions hopefully.\n\nMinor issues: \n\n3. In Figure 5, it would be helpful to annotate the four sub-figures with (a-d) to improve reference.\n4. In the last paragraph of Section 4.2: 'the inference algorithm can be found in Algorithm 1' should be revised to 'the learning algorithm...'.\n5. The paper includes duplicate notations for $L$, referring to both the prediction length and the number of stacked layers.\n\nReferences:\nHan, et al. Predicting physics in mesh-reduced space with temporal attention. ICLR 2022."
            },
            "questions": {
                "value": "1. I am a little confused with the equation $t_l = t_0 + rl \u2212 r + 1$ in Section 4.1. What does step size mean in the paper? \n2. In Section 4.2, it would be beneficial if the authors could provide an explanation for their choice to compute the offset in the observation space, as opposed to calculating an updated latent vector in the latent space.\n3. In Eq.(11), does the notation $\\{z_i^{t_1}\\}_{i \\in \\mathcal{V}}$ indicate that the offset of node $i$ is computed using representations from all nodes in the set $\\mathcal{V}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2108/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654572184,
        "cdate": 1698654572184,
        "tmdate": 1699636143438,
        "mdate": 1699636143438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nx7y6yuuGe",
        "forum": "tFYcEUlUTt",
        "replyto": "tFYcEUlUTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_vkK1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_vkK1"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a two-stage approach to predicting physical systems with graph networks. The method aims to provide long-term stability on complex non-linear dynamics. The first stage conducts temporally coarse predictions of multiple system states. To this end, a neural ODE approach was embedded in the graph structure. In principle, this allows for flexible timesteps by the means of the ODE integrator. The second stage then takes the series of frames to refine the first prediction. An autoregressive rollout of both stages then yields the long-term dynamics of the system. The method is evaluated against a series of baselines on steady-state physics scenarios."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Originality\nThe paper introduces a method that combines a prediction model with a subsequently applied refiner. There is some novelty in applying such an approach to physics predictions with graph networks. However, the fact that similar procedures have been studied in video prediction (see e.g. *\u201cFlexible Diffusion Modeling of Long Videos\u201d* by Harvey et al., NeurIPS 2022) should be denoted in the paper. Furthermore, similar methods exist for PDE predictions (*\u201cDYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting\u201d* by Cachay et al., NeurIPS 2023, *\u201cPDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers\u201d* by Lippe et al., NeurIPS 2023). However, the latter methods are recent and can be considered contemporary, i.e. a direct comparison against them is not explicitly required.\n\n### Clarity\nThe majority of the paper apart from section 4 is clearly phrased. The line of argumentation, while not coherent in the bigger picture, can be understood decently. The visual quality of some figures is quite good. Especially Fig. 2 helps in understanding the model architecture.\n\n### Quality and Soundness\nThe authors compare their method to many baselines and the choice of these baselines seems sound with only few exceptions (Social-ODE stands out as a rather odd choice). However, the results obtained with the baselines raise serious concerns about the implementation of these methods as well as the general experimental setup, especially for the MeshGraphNet. Source code is provided for the paper, however no baseline methods are included, at least at a quick glance (I did not investigate the source code in much detail).\n\n### Significance\nThe authors target improvements in the long-term stability of physics predictions. This is indeed an open problem and of high significance for autoregressive architectures. The method introduced in the paper touches upon a series of changes that could be promising avenues toward achieving this stability. For instance, the shown experiments study variations in the length of the trajectory used in training the predictor ($L$), as well as the relative size of the prediction timestep ($r$). An ablation study in the paper also confirms that the addition of the refiner network improves the accuracy of the method across the entire studied time horizon."
            },
            "weaknesses": {
                "value": "### Presentation\n\n**P1:**\nThe line of argumentation is in some cases weird, unclear, vague, or lacks citations, especially in section 3 and 4:\n- \u201cGiven these limitations\u2026.\u201d (difficult to understand, top of page 2)\n- \u201cOur graph ODE model is flexible to generate\u2026\u201d (sound weird, middle of page 2)\n- \u201cThe model would be evaluated\u2026\u201d (sound weird, end of section 3.1)\n- \u201cHowever, they are inferior in modeling\u2026\u201d (unfounded/missing citation, end of section 3.2)\n- \u201cTo capture the hierarchy structure among meshes\u2026\u201d (unclear, end of section 3.3)\n- \u201cAs a foundational step, it is\u2026\u201d (sounds weird, begin of section 4.1)\n- \u201cTo learn interacting dynamics\u2026\u201d (very unclear, begin of section 4.1)\n- \u201cFurthermore, we notice that data-driven\u2026\u201d (unfounded/missing citation, middle of section 4.1)\n- \u201cFirstly, our FAIR\u2026, Secondly, our approach\u2026\u201d (unclear, end of section 4.1, very similar at the begin of section 4.3)\n- \u201cmulti-task learning framework\u201d (unclear, what other task?, frequently across the paper)\n\n**P2:**\nSection 4 describing the methodology has structural issues, is generally difficult to follow, and it is not clear to me how the proposed method actually works in detail. Similar problems can be found across the other parts of section 4, but here an example issue from section 4.2: The term interpolation might be misleading for the refinement step. To my understanding, the refinement is mainly used to improve the outputs at discrete timesteps where a coarse prediction already exists. This refinement seems to be necessary to stabilize recurrent applications. Please clarify the refinement procedure in the paper.\n\n**P3:**\nPresentation problems for the different evaluations are listed in combination with the corresponding evaluation problem **E** below.\u00a0\n\n**P4:**\nMinor issues and Typos:\n- Fig. 1 is broken across a range of PDF viewers on Ubuntu (Firefox, Envince) and also does not print properly. It was only possible to display in Google Chrome\n- \u201cunstructured surfaces\u201d; I think you mean unstructured grids\n- 2x which learns coarse... (directly before section 3)\n\n\n### Evaluations\n\n**E1 - Datasets:**\nPlease provide some physical characteristic numbers, i.e. Reynolds and Mach numbers, for the Navier-Stokes cases. This would greatly improve the comparability of this work to future research. In particular, it would be interesting to know whether the Aerofoil case includes the transonic regime. The conclusion section briefly mentions that the method currently fails when predicting non-steady dynamics. Three out of the four datasets were originally introduced in *Pfaff et al. (2021)*, and include a variety of steady-state as well as transient cases. To me, it is unclear whether this full dataset was used in training and testing, or whether only a small subset with steady-case simulations was used.\n\n**E2 - Table 1:**\n- I have some general concerns regarding the evaluation fairness here. Why would a long term prediction+interpolation approach work better on single step predictions across data sets compared to all other methods? To me that is highly unintuitive and not very plausible.\n- Only after closer inspection of the appendix and the reproducibility statement (*\u201cThe experimental results of the baselines are consistent with Cao et al. (2023).\u201d*) it becomes apparent that the entirety of the baseline results in Tab. 1 are directly copied from *Cao et al. (2023)*. Nevertheless, it is stated throughout the paper and appendix that the authors use their implementation of these methods. In my opinion, this is highly problematic and I want to urge the authors to clearly state the source of the values in the table heading. A fair evaluation against FAIR would require the usage of one consistent setup for all methods, as it is possible that e.g. details in the error aggregation change results.\n- Furthermore, I have serious doubts regarding the soundness of these quantitative evaluations. In three out of four cases, the paper reuses the datasets from *Pfaff et al. (2021)* (i.e. CylinderFlow, Airfiol, DeformingPlate), and proceeds to use the MeshGraphNet trained within that publication as the main comparative baseline for visualizations. However, all results in Tab. 1 significantly differ (sometimes by more than an order of magnitude) from the ones reported by *Pfaff et al. (2021)* for the very same datasets (e.g. CylinderFlow RMSE on 50 steps: 43.9e-3 here vs. 6.3e-3 from *Pfaff et al.*). At the same time, the limited information in the appendix suggests that exactly the same architecture (e.g. number of message passing layers) was used as in *Pfaff et al. (2021)*. No explanation is given for these large differences in the error metrics.\n\n**E3 - Fig 3 and Fig. 4:**\n- It is unclear which quantity is shown in the figures (velocity amplitude? x or y component?). In general, I would recommend visualizing the vorticity for these flows. Additionally, the flow fields in Fig 3. seem to have strong artifacts, even in the smooth ground truth. There appears to be an issue with the visualization procedure.\n- The scale of the shown quantity is not mentioned, please add a colorbar and use the same scale for all plots. Fig 4. even lacks time step descriptions. For both cases, adding a Reynolds number would be helpful to assess the complexity.\n- The paper aims to *\u201cenhance the capacity to capture non-linear complex patterns\u201d* and *\u201ccapture the long-term dynamics\u201d*, yet the figures visualize a simulation converging to a steady state (i.e. a simulation state with zero temporal derivatives). Visualizing these particular cases fails to emphasize the supposed strength of the presented method, which targets complex non-linear dynamics. In the ground truth, the flows seems to reach a steady state after t=100 for Fig. 3 (and is directly steady from the leftmost frame in Fig. 4), so obviously long-term predictions with interpolation work better than any autoregressive rollout, since no interpolation has to be performed.\n- Considering these issues, it would be highly necessary to show a diverse set of videos of predictions, as well as unsteady example cases for a convincing argumentation. In videos, I would expect temporal coherence issues in the predictions for unsteady cases as well, caused by the prediction + interpolation setup.\n- In the original MeshGraphNet visualizations (https://sites.google.com/view/meshgraphnets#h.p95069gdfedm) all the prediction results are a lot closer to the ground truth than what is shown here. This raises concerns about the reimplementation and training of the model (as both architecture and data set are claimed to be identical with the work from *Pfaff et al. (2021)*).\n- In Fig. 4: The MeshGraphNet visualization seems to be based on an entirely different case compared to all others. Judging from the results shown in *Pfaff et al. (2021)*, it seems unlikely that this architecture fails even on a 1-step prediction, while simultaneously reproducing a trajectory from a different part of the dataset.\n\n**E4 - Fig. 5:**\n- I assume this study was once again performed with data that only converges to a steady state solution? This raises the same issues discussed above in *E3*. Prediction videos corresponding to the shown results (especially for c)) would be required once again.\n- a): For which fixed L was the sweep across r computed?\n- a) and b): Apart from the model architecture, there are differences between FAIR and the baselines in terms of training procedures. I think it is important to emphasize the relative improvement of variations in $r$ and $L$ over the \u201cstandard\u201d approach of choosing $r=1$ and $L=1$. While setting $r=2$ lowers the RMSE by ~2%, choosing $L=4$ is far more important and reduces errors by ~12% (evaluated for 50 steps). Crucially, $r$ and $L$ can also be realized in the baselines. The $r$ could similarly be a larger timestep in other methods, while $L$ is equivalent to an unrolled trajectory. In fact, matching an entire training trajectory instead of just the next step is known to yield better results, especially over long horizons (see e.g. *\u201cDPM: A deep learning PDE augmentation method with application to large-eddy simulation\u201d* by Sirignano et al., JCP, 2020;\u00a0 *\u201cMachine learning\u2013accelerated computational fluid dynamics\u201d* by Kochkov et al., PNAS, 2021;\u00a0 *\u201cSolver-in-the-loop: Learning from differentiable physics to interact with iterative PDE-Solvers\u201d* by Um et al., NeurIPS, 2020). The long-term stability of FAIR might be entirely due to choosing a large $L$. I would suggest testing the $L=1$ model on unsteady, longer inference horizons to see whether this holds true in experiments. Additionally, training the baselines with the same $r$ and $L$ as chosen for FAIR would yield better comparability.\n- c): I would suggest plotting this in log-scale.\n\n**E5:**\nTo summarize, the scope of this paper is limited to steady-state predictions, where no temporal changes should occur once the solution converges. In this context, it is questionable whether fine temporal resolutions are at all desirable. Thus, the temporal interpolation capabilities have little value in the current state of the paper. Some existing approaches can even directly predict the steady state solution without intermediate steps at all (e.g. *\u201cDeep Learning Methods for Reynolds-Averaged Navier-Stokes Simulations of Airfoil Flows\u201d*, Thuerey et al., AIAA Journal, 2020). Due to the steady-state test cases, the evaluations in this paper mainly focus on two qualities:\n1) whether a particular model architecture can calculate the steady state in the first place, and\n2) whether the model can stop the solution from drifting over long horizons when the steady state is already reached.\n\nThese two aspects should be clearly emphasized across the paper. To achieve a fair comparison, I would suggest training the baseline models on the same timestep as FAIR (i.e. with r=2) to reach a comparable level of recurrency between the models. In addition, investigating unsteady cases for which most baselines were designed seems necessary as well. Alternatively, comparing to methods that directly predict the steady state solution would be an option, however I would expect a recurrent approach to have drawbacks in this scenario.\n\n### Summary\nOverall, in addition to substantial presentation problems, this paper has fundamental issues across all performed evaluations and results, that could only be fixed with a major revision well beyond the scope of a conference rebuttal. Thus, I would oppose the publication of this paper in its current state and recommend an overall score of strong reject."
            },
            "questions": {
                "value": "**Q1:**\nFrom the paper and figures I assume that the predictor outputs a series of temporally coarse frames, which are then predominantly used to refine the first prediction. The procedure is then repeated with this predicted and refined first prediction. To me, this means that $L>1$ predictions are made to advance the simulation by one frame. Is this the case? This should also be clarified in the paper.\n\n**Q2:**\n*Pfaff et al. (2021)* show that their method was stable across many steps. In Section 3.2 it is mentioned that *\u201c[...] they are inferior in modeling long-term interacting systems that are continuous in nature and could suffer from severe error accumulation when making long-term predictions\u201d*. Can you elaborate on this, especially regarding the inherent inferiority of GNNs?\n\n**Q3:**\nIn Section 4.1: *\u201cInstead of using inefficient iterative rollouts [...]\u201d* - It is not entirely clear to me how iterative approaches are inherently inefficient at solving PDEs. To my understanding, even the method presented in the paper fundamentally relies on iterative temporal advancement of the solution.\u00a0\n\n**Q4:**\nIn Section 4.1: *\u201c[...] thereby improving the capability to capture evolving patterns under potential noise\u201d* - Can you clarify which evaluations from the paper hint at better performance under noise?\n\n**Q5:**\nIn Section 4.1: *\u201cWhile previous approaches often integrate neighborhood information into ODEs to model interacting dynamics, they typically fall short in explicitly capturing the evolving dynamics of edges\u201d* - I do not fully understand this statement. Edges are not an inherent characteristic of the vector fields described by ODEs. Can you specify what you mean?\n\n**Q6:**\nIn Section 4.2: The loss formulation $\\mathcal{L}_{re}$ in equation (13) optimizes the refiner to match a ground truth $x^{t_1}$. The graph-ode predictor has, however, produced a series of future frames ($L$ to be precise). Is the refiner never trained to refine these steps?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As mentioned in the weakness **E2** above, this paper directly copies most results in Table 1 from *Cao et al. (2023)* and does not explicitly declare this. Only one hidden sentence in the reproducibility statement mentions this fact, while the rest of the paper and the appendix refers to the baselines results as coming from *\u201cour implementation\u201d*. I am not sure if this problem is sufficiently severe to fall within the definition of scientific misconduct, but I still wanted to explicitly raise this issue here as well, just in case. In my opinion this is problematic, as the results of MeshGraphNet reported by *Cao et al.* also clearly deviate from the original paper by *Pfaff et al.*, while using the same architecture on the same data sets."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2108/Reviewer_vkK1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2108/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755468983,
        "cdate": 1698755468983,
        "tmdate": 1699636143354,
        "mdate": 1699636143354,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kAwDAuJxpb",
        "forum": "tFYcEUlUTt",
        "replyto": "tFYcEUlUTt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_kYvB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2108/Reviewer_kYvB"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for mesh-based physical simulations focusing on improve the performance of long-term predictions. The proposed method first learn a graph ODE model for modelling coarse long-term predictions, subsequently enhancing the short-term predictions through interpolation. A continuous graph ODE model is utilized to integrate previous states in the progression of interacting node representations, which make it possible to capture long-term trajectories within a multi-task learning framework. The method can output long-term trajectories achieving considerable error reduction rate on benchmark datasets comparing to baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of using future coarse long-term predictions to refine the short-term predictions is be sound and novel to me. Struggling to capture long-term dynamics is an important issue for the autoregressive style learning-based physical simulators due to the error accumulations over the iterative rollouts. The proposed method tackle this problem from a new perspective and has the potential to be further investigated.\n- The experiments shown that the proposed method can obviously outperform the existing methods on the benchmark datasets.\n- The paper is well written and organized with profound analysis."
            },
            "weaknesses": {
                "value": "- The current experiments provided in the paper are mainly in 2D. It is unclear whether the proposed method can exhibit similar performance when applied to larger scale 3D cases.\n- There are no experiments for generalization ability for the proposed models. It is unclear how the trained models will perform given a test dataset generated by a variant (or same equation with different parameters) of the PDE which used to generate the training dataset."
            },
            "questions": {
                "value": "- A \"multi-task learning framework\" is mentioned multiple times in the paper for the first stage training. Does it indicate the encoder and decoder as multiple tasks?\n- As the equations to solve in the experiments are generally PDEs, however the coarse long-term predictor i.e., Graph ODE is designed based on ODE. I am wondering are there any intuitions for how a ODE based neural solver can be applied on PDEs?\n- Comparing to the existing autoregressive style methods, the refinement module in the proposed model is further trained on future states, which indicates the model get more chances to fit to the training data, I am wondering whether it may have overfitting issues? How does the model's generalization ability comparing to the existing autoregressive methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2108/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760112663,
        "cdate": 1698760112663,
        "tmdate": 1699636143280,
        "mdate": 1699636143280,
        "license": "CC BY 4.0",
        "version": 2
    }
]