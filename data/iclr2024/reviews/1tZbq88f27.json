[
    {
        "id": "q2mOyEQoWo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX"
        ],
        "forum": "1tZbq88f27",
        "replyto": "1tZbq88f27",
        "content": {
            "summary": {
                "value": "This paper presents a recipe for reproducing the GPT-4 vision model. The approach involves aligning the pre-trained vision encoder of BLIP-2 (ViT + Qformer) with the LLM Vicuna using a noisy image-caption dataset. The model is then fine-tuned with a curated detailed image description dataset, which is generated by refining the initial model predictions using ChatGPT and manual refinement. Experimental results demonstrate the capabilities of the proposed MiniGPT-4 model in challenging vision-language tasks like Poem Generation and Meme understanding. While qualitative results are mostly presented, quantitative findings indicate that MiniGPT-4 outperforms BLIP-2 on image captioning when evaluated with ChatGPT, although it tends to generate more hallucinations than BLIP-2."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The exploration to achieve a GPT-4 level VLMs by combining LLMs & vision encoders is greatly appreciated. Additionally, the provision of open-sourced model checkpoints and dataset enhances the reproducibility and transparency of the research.\n- The qualitative results presented in the study demonstrate the potential effectiveness of the proposed recipe. The process of aligning the model through training on a noisy dataset and subsequently fine-tuning it with a high-quality dataset shows promise in improving the model's performance and response quality."
            },
            "weaknesses": {
                "value": "- The quantitative evaluation setup lacks convincing elements:\n  - The paper only compares the proposed model with BLIP-2 as the baseline, which utilizes a FLAN-T5 model as the base LLM. To enhance the credibility of the results, it would be beneficial to compare the proposed model with more recent open-sourced components that utilize similar base LLMs, such as LLaVA[1], InstructBLIP[2], and mPlug-Owl[3].\n  - The selection of the four Advanced Abilities tasks appears arbitrary, and the evaluation sets for these tasks are relatively small (25 samples for each task). To provide a more comprehensive evaluation, it is recommended to include evaluation on recent multimedia benchmarks such as TouchStone[4] and MMBench[5].\n  - In Table 2, the coco caption evaluation with ChatGPT seems questionable. The approach of randomly selecting a ground-truth caption and asking ChatGPT if the generated text covers the main visual concepts in the label text may lead to instances where the selected \"ground-truth\" caption and the generated text depict the image in different aspects, yet both are considered correct. Reporting standard metrics such as BLEU and Rouge scores, which consider multiple candidate ground-truth captions, could mitigate the issues.\n  - Table 5 lacks the CHAIR_S score for different methods. Additionally, it indicates that MiniGPT-4 fails to follow the instruction's requirement of 20 words, as evidenced by the average length of 27 words for MiniGPT-4 (short). To me, instead of showcasing the promising results on so-called adavanced tasks, satisfying the user requirement of a basical instruction should be of higher priority.\n- Some claims made in the paper are misleading:\n  - In Section 3.2, the paper claims that no equivalent datasets exist (for multi-modal instruction tuning) in the vision-language domain. However, there have been recent studies addressing this problem, such as LLaVA[1], InstructBLIP[2], M3IT[6], and MultiInstruct[7]. To accurately reflect the current state of the field, the author should properly cite these studies and consider exploring further fine-tuning with these datasets.\n  - In Section 4.4, the paper claims that MiniGPT-4 achieves similar results with Qformer, implying that Qformer does not play a critical role in advanced skills. However, this claim may not be fully supported by the AOK-VQA and GQA results, along with additional case studies. Additionally, if the advanced skills require inference between multiple images, the downsampling effect of Qformer (i.e., downsampling visual patches from 256 to 32) could be crucial and should be considered.\n\n## References\n\nIt is not a curated list, and most of them appeared  more than 3 months before the submission deadline.\n\n[1] Visual Instruction Tuning, https://arxiv.org/abs/2304.08485\n\n[2] InstructBLIP: Towards general-purpose vision-language models with instruction tuning, https://arxiv.org/abs/2305.06500 \n\n[3] mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality, https://arxiv.org/abs/2304.14178\n\n[4] TorchStone: https://github.com/QwenLM/Qwen-VL/blob/master/touchstone/README.md\n\n[5] MMBench: https://opencompass.org.cn/leaderboard-multimodal\n\n[6] M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning, https://arxiv.org/abs/2306.04387\n\n[7] MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning, https://arxiv.org/abs/2212.10773"
            },
            "questions": {
                "value": "Q1: What criteria did you use to select the Advanced Abilities tasks? Additionally, can you clarify if the images used for evaluation in these tasks overlap with the dataset used in stage 1 pre-training and stage 2 fine-tuning?\n\nQ2: Could you specify the version of ChatGPT that you adopted for answer revision in your experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6224/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_auSX",
                    "ICLR.cc/2024/Conference/Submission6224/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697388127741,
        "cdate": 1697388127741,
        "tmdate": 1700625799927,
        "mdate": 1700625799927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ygbOM4ceck",
        "forum": "1tZbq88f27",
        "replyto": "1tZbq88f27",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_A8a5"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces MiniGPT-4, which combines a frozen visual encoder with a frozen large language model, Vicuna, using a single projection layer. The research revealed that aligning visual features with a sophisticated large language model can replicate many of GPT-4's advanced multi-modal functions. A two-step training process is proposed, and the second step plays a vital role in generating long and meaningful responses. Additionally, MiniGPT-4 showcased unique capabilities like writing stories or poems inspired by images and teaching users cooking techniques based on food photos."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Easy Approach: The paper presents an easy two-step approach to combining a visual encoder with a large language model, providing insights into GPT-4's enhanced multi-modal capabilities.\n\nDiverse Capabilities: MiniGPT-4 replicates many of GPT-4's features, such as writing stories based on images and culinary guidance from food photos.\n\nInteresting Experiments: The authors provide qualitative and quantitative experiments to show the various abilities of the proposed method, including the human study of advanced abilities."
            },
            "weaknesses": {
                "value": "* The necessity of using a large number of image and text pairs to train a projection layer is not addressed sufficiently. \n\n* The comparisons between BLIP-2 and MiniGPT are unfair because BLIP-2 uses a weaker LLM and is not designed for the advanced abilities mentioned in the paper. Furthermore, the designed new evaluation method using ChatGPT to check whether the generation covers all the objects and visual relations is not compelling. It is possible because many hallucinations are generated, as Table 5 illustrates."
            },
            "questions": {
                "value": "* Is it possible to use less data from the combined dataset to train the projection layer in the first pertaining stage? It is better to provide insights on the size data necessary to align the vision and text space.\n\n* Surprisingly, the performance drops a lot when three projection layers are used instead of one. Could you try different designs of the projection module, e.g., two or more than three projection layers, or adding activation layers between projection layers to provide insights on why one projection layer is better and why the drop is so huge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698185880461,
        "cdate": 1698185880461,
        "tmdate": 1699636679655,
        "mdate": 1699636679655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BnhmXKLIro",
        "forum": "1tZbq88f27",
        "replyto": "1tZbq88f27",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to create Multimodal LLMs that can understand the visual inputs. To do this, MiniGPT-4 uses a frozen visual encoder along with a frozen vicuna model and only trains the single projection layer. Various emerging properties of the M-LLMs are discussed and a novel detailed image description-based fine-tuning stage is proposed to further improve the image understanding.\n\nOverall this paper has contributed significantly. However, it\u2019s not yet ready for publication. Other rounds of revisions are needed with extensive benchmarking."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* One of the first works attempts to introduce visual modalities in pre-trained LLMs in a parameter-efficient manner.\n* MiniGPT-4 requires 40 A100 GPU hours for training and is able to outperform the BLIP-2. \n* Various qualitative examples are shown along with ablation studies to measure the effectiveness of the two-stage training procedure. \n* Limitations of the MiniGPT-4 (in terms of hallucinations and spatial relations) are discussed."
            },
            "weaknesses": {
                "value": "* Benchmark is very limited. Rigorous benchmarking on different downstream tasks is needed. For reference, MMBench, TextVQA, etc.\n* Only one baseline is reported. However, for a holistic understanding of the approach, more baselines are needed. \n\nSome of the missing References:\n* Liu, Haotian, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. \"Visual instruction tuning.\" arXiv preprint arXiv:2304.08485 (2023).\n* Mou, Chong, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. \"T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.\" arXiv preprint arXiv:2302.08453 (2023)."
            },
            "questions": {
                "value": "* Instead of having the data post-processing steps for stage 2 training, why not use GPT-4 itself to get the image captions and perform knowledge distillation as stage 2? Getting ~3000 such image-caption pairs should not be that costly.\n* Does MiniGPT-4 apply a unique projection layer to all output vectors corresponding to the learned queries? Or some other processing steps are involved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6224/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6224/Reviewer_TFFX",
                    "ICLR.cc/2024/Conference/Submission6224/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731289659,
        "cdate": 1698731289659,
        "tmdate": 1700626977400,
        "mdate": 1700626977400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ygupOwUBFq",
        "forum": "1tZbq88f27",
        "replyto": "1tZbq88f27",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_7oDs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6224/Reviewer_7oDs"
        ],
        "content": {
            "summary": {
                "value": "This work introduces MiniGPT-4, which combines a frozen visual encoder with a frozen advanced LLM, Vicuna, through a single projection layer. Remarkably, MiniGPT-4 exhibits advanced multi-modal abilities similar to GPT-4, extending to generating detailed image descriptions, creating websites from hand-drawn drafts, writing stories and poems inspired by images, and providing cooking instructions based on food photos. However, initial training on short image captions led to issues like repetition and fragmentation in the generated text. To remedy this, the authors curated a detailed image description dataset for a second stage of fine-tuning, significantly enhancing the model\u2019s reliability and overall performance. They have made their code available for further research and validation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This submission is one of the earliest attempts at open-source reproduction of large vision-and-language models. It provides a parameter-efficient solution to this difficult problem, and makes use of the existing open-source models as much as possible, which inspires many follow-up works."
            },
            "weaknesses": {
                "value": "There are two main weaknesses with regard to the model structure and the evaluation setup. My evaluation of this submission will be greatly improved if the authors properly address these two main issues. \n\n\n**(1) Model Structure:**\nThe introduction of MiniGPT-4 in the submission presents a novel approach in the domain of vision-and-language models. However, a noticeable limitation lies in its capacity to process only a single visual input per run. This design contrasts with other large-scale models such as Flamingo and GPT-4, which have demonstrated capabilities in handling multiple visual inputs simultaneously, as acknowledged in the related work section of the submission.\n\nAdditionally, the literature review appears to omit significant works [1,2,3,4] in the realm of large vision-and-language models that facilitate in-context learning with multiple visual inputs. The inclusion of these works could provide a more comprehensive backdrop for the MiniGPT-4, situating it within the broader context of ongoing research in the field.\n\nThe MiniGPT-4\u2019s visual encoder is founded on the Q-Former architecture, converting each visual input into a fixed-length learned visual query. With reference to the BLIP-2 paper, this length is specified as 32 dimensions. Considering that this is considerably shorter than the maximum input length supported by the LLM backbone, there seems to be an opportunity to extend the model\u2019s capacity to accommodate multiple visual inputs. An exploration into why the MiniGPT-4 is constrained to single image inputs, alongside potential avenues for extending its flexibility and application range, would strengthen the submission.\n\n**(2) Evaluation Setup:**\nThe submission primarily focuses on showcasing qualitative results derived from the MiniGPT-4, complemented by various ablation studies. However, there is an apparent gap in benchmark comparisons with other established large vision-and-language models. A number of benchmarks [5,6,7], encompassing diverse aspects and task setups related to vision-and-language evaluation, have included results from MiniGPT-4 alongside comparisons with other models. The incorporation of such direct comparisons would offer a more definitive assessment of MiniGPT-4\u2019s performance, enhancing the overall credibility of the evaluation presented.\n\n\n\n**Missing references**:\n\n[1] OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models\n\n[2] Otter: A Multi-Modal Model with In-Context Instruction Tuning\n\n[3] mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality\n\n[4] MIMIC-IT: Multi-Modal In-Context Instruction Tuning\n\n[5] MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models\n\n[6] MMBench: Is Your Multi-modal Model an All-around Player?\n\n[7] VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"
            },
            "questions": {
                "value": "(1) What is the maximum input and output length for the training and inference stage? \n\n(2) In Section 3.2, the authors mention they would examine if the generated sentence exceeds 80 tokens or not \u2013 how is the length of 80 determined? Is it through empirical observations?\n\n(3) What\u2019s the length of the learned visual queries from the Q-Former in Mini-GPT4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6224/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732170343,
        "cdate": 1698732170343,
        "tmdate": 1699636679402,
        "mdate": 1699636679402,
        "license": "CC BY 4.0",
        "version": 2
    }
]