[
    {
        "id": "m7HyEgs8QG",
        "forum": "z0B4GJuRjo",
        "replyto": "z0B4GJuRjo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_Ck8E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_Ck8E"
        ],
        "content": {
            "summary": {
                "value": "The paper explores how to reduce hallucination in text generation models. It makes the claim that previous methods have focused on token level checks, and claims novelty by instead focussing on sentence (sequence) level scores. In particular it introduces 2 measures: \n1: average of token level log-probabilities\n2. agreement of an entailment model of candidate decoding compared to several other options decoded via some decoding method. \n\nThese two measures are then used to guide decoding, or selection of decoded utterances, in order to reduce hallucination. \nResults are given claiming that these measures a helpful at detecting hallucination, and helpful at preventing hallucination when used in decoding strategy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "An important problem, with a meaningful contribution given in this paper, particularly in using entailment across candidates (ie consistency) to detect hallucinations."
            },
            "weaknesses": {
                "value": "* The reported metrics section (4.2.3) introduces a faithfulness percentage. This is a clear and obvious measure ... however it isn't described how this is calculated! I presume it was manually done, rather than inferred by some model or automatic metric? If so who manually graded this? \n\n* It is very odd that top-k and top-p sampling methods result in reducing hallucination in a knowledge-grounded dialog generation. There is a lack of detail on what the actual task is here, and although the FaithDial dataset is referenced, it would help the reader a lot to actually describe what precise task is being addressed. There are comments as well about hallucinating in wider tasks like dialog generation, which I take to mean NLG tasks not grounded on factual information in the inputs. If this is so, how is hallucination even defined there?\n\n* Results for the number 1 hypothesis should not be in the appendix!\n* I'm unclear if the statistical tools used to make claims during \n* What is the entailment model used in measuring equation 1? How important is this to the final results? Does changing the entailment model result in entirely different outcomes, presumably so. \n* The writing is poor in that it repeats sections multiple times. For example probabilistic and semantic certainty are introduced 3 or 4 times."
            },
            "questions": {
                "value": "* Interested for the authors thoughts on why likelihoods are based on product of of token probs, but uncertainty is better detected via mean of token (log) probs? \n* Were other stats beyond the mean (min, max, variance, etc) considered in forming p-crr?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698169983782,
        "cdate": 1698169983782,
        "tmdate": 1699636026998,
        "mdate": 1699636026998,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3nZHTPIbJ4",
        "forum": "z0B4GJuRjo",
        "replyto": "z0B4GJuRjo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_qG74"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_qG74"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a sequence-level certainty to addresses model hallucinations in Natural Language Generation (NLG). Expecially they demonstrate a significant correlation between probabilistic certainty (perplexity of the generated tokens) and semantic certainty (the entitlement score between all the pairs of generated response) in model responses and hallucination metrics. They introduce two Certainty-based Response Ranking (CRR) methods, Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR), which effectively mitigate hallucination in NLG across various datasets and models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strengths:\n\n- The paper introduces an interesting approach that addresses model hallucination in Natural Language Generation (NLG).\n- The experimental methodology is rigorous, providing statistical significance in the context of NLG tasks.\n- The paper offers fair comparisons with existing hallucination reduction decoding methods."
            },
            "weaknesses": {
                "value": "Weaknesses:\n\n- The paper's scope is limited as it focuses on a specific NLG task, knowledge-grounded dialogue responses. In this setting, hallucination are not very pronounced, since the gold document is provided as input to the model. I suspect that a larger LLM (prompted correctly) can improve any of the provided metric  without any \"special\" decoding method (greedy). I invite the authors to report this baselines if possible."
            },
            "questions": {
                "value": "Check weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "nan"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698422509702,
        "cdate": 1698422509702,
        "tmdate": 1699636026926,
        "mdate": 1699636026926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "a5vEF6iKc9",
        "forum": "z0B4GJuRjo",
        "replyto": "z0B4GJuRjo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
        ],
        "content": {
            "summary": {
                "value": "This work tries to reduce the hallucination in NLGs.  To this end, this work has done the following jobs:\n\n1. Based on the previous token-level uncertainty work, this work has defined sentence-level probabilistic certainty and semantic certainty as indicators to evaluate the level of hallucination in model generations. Then, experiments verified the effectiveness of the sentence-level probabilistic certainty and semantic certainty.\n\n2. This work provides theoretical proof to show that the black-box semantic certainty is a good estimator of the white-box probabilistic certainty.\n\n3. To mitigate the hallucination, this work proposes a Certainty-based Response Ranking (CRR), which re-ranks the response according to the sentence-level probabilistic certainty or the sentence-level semantic certainty."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work has revealed that both a higher level of probabilistic certainty and a higher level of semantic certainty are significantly correlated with a lower level of hallucination in model generations. It brings an insight for us to reduce the hallucinations in NLGs.\n\n2.  As a white-box metric, probabilistic certainty can not be calculated in practice.  To this end, the authors find that the semantic certainty of a model response is an aligning and unbiased estimator of its probabilistic likelihood.\n\n3. Based on their findings on  Probabilistic certainty and Semantic certainty, this work proposes two ranking methods P-CRR and S-CRR to improve the faithfulness via re-ranking. Experiments can demonstrate their effectiveness, especially the black-box S-CRR."
            },
            "weaknesses": {
                "value": "1. The proposed methods may significantly damage the diversity of the generated text. In the proposed Certainty-based framework, both P-CRR and S-CRR encourage the backbone model to generate safe, receptive, general responses. Safe responses have higher probability certainty in either a language model or a conditional language model and are always similar to each other. For example, if a model generates three samples, 'I don't know.' , 'I don't know it.' , 'I do not know it.'; then,  both the P-CRR and S-CRR will give higher scores although they are boring.\n\n2. The organization of this work is hard to follow and verbose:\n\n-  As an important concept,  `uncertainty' lacks enough introduction when it first appears. Even in Sec 2.1, the authors only gave a simple literal definition.  Readers can't understand what it is until the 4th page. Meanwhile,  there is no formulation to introduce the token-level uncertainties.\n\n- The authors tended to repeatedly introduce the same thing (for example,  findings proposed by (Xiao & Wang, 2021)) in different sections.\n\n-  In Equation 1, we can not directly infer what $Entailment( *,*)$ is from the nearby context.  The authors only give a very rough Introduction in Sec 3.3.1 without any formulation. \n\n3. The evaluation of dialogues (for example, Table 2) only considers faithfulness. It is necessary to report other metrics (BLEU, ROUGE, DISTINCT, etc.) as well."
            },
            "questions": {
                "value": "Can you report results on more metrics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1010/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1010/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698569098716,
        "cdate": 1698569098716,
        "tmdate": 1699636026851,
        "mdate": 1699636026851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M4MYmQZSDM",
        "forum": "z0B4GJuRjo",
        "replyto": "z0B4GJuRjo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_Rt2T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1010/Reviewer_Rt2T"
        ],
        "content": {
            "summary": {
                "value": "This paper presents sequence-level certainty as a common theme over hallucination. The authors categorize sequence-level certainty into probabilistic certainty and semantic certainty and explore the correlation between sequence-level certainty and the level of hallucination. Based on the observation, the authors proposed a decoding-time method called Certainty-based Response Ranking (CRR) to mitigate hallucination. The experimental results show CRR can reduce model hallucination under certain settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The author's proposal of sequence-level certainty as a common theme over hallucination is a good idea, as is the design of probabilistic certainty and semantic certainty.\n- The logic and structure of the paper is clear and easy to follow. The design details and experimental setup are explained thoroughly. This work is comprehensive, well-organized, and complete."
            },
            "weaknesses": {
                "value": "-\tThe last sentence of the first paragraph in the Introduction sets too narrow of a definition for hallucination in the KGDG task. In the current era of large models, we need to view the hallucination problem from a broader perspective, such as fact-conflicting and context-conflicting hallucinations [1], rather than limiting to input-conflicting hallucination. Meanwhile, judging from Table 6 in the Appendix, the cases are too simple and the chosen models like GPT-2 have insufficient capabilities. Through prompting, I found that gpt-3.5 does not hallucinate on those examples.\n- Evaluating large models' hallucination phenomena at the sequence level using semantic certainty is an good idea, but the evaluation method for semantic certainty seems a bit crude. Also, the proposed decoding method simply selects better generations from the candidate set based on different metrics, which is quite similar to similar to Minimum Bayes Risk Decoding[2].\n- The paper uses a RoBERTa-Large-based hallucination classification to evaluate whether generated text contains hallucination. However, the accuracy and effectiveness of this method for judging hallucination are not explained. Accurately assessing whether generated text hallucinates is fundamental to the analysis and experiments in this paper, so it is an important part.\n- Hypothesis 2 proposed in Section 3.4 is an important basis for subsequent work, but there may be issues with the verification process. On one hand, it is reasonable that low certainty can lead to hallucination. But on the other hand, when models hallucinate, certainty is not necessarily low (especially under the consistency-based certainty evaluation designed by the authors). Based on my personal experience and experiments with LLM, they sometimes hallucinate confidently, i.e. sampling multiple times yields consistent outputs, especially for knowledge and numeric hallucinations (but I have not statistically verified this phenomenon rigorously). Also, the PBCC test is sensitive to class distribution. With imbalanced categories like fewer hallucination examples, it can still give a positively correlated relationship between certainty and hallucination, ignoring cases of high certainty hallucination. As described in 3.3.1 and Table 2, there is indeed a class imbalance in the FaithDial dataset under nucleus sampling, where hallucination examples make up less than 10% of the data.\n- The analysis of efficiency for the P-CRR and S-CRR methods is missing. These multi-sampling approaches may greatly increase time and computational costs.\n- The abstract contains too many unnecessary details and is somewhat convoluted. modifications are needed. Less essential details like the introduction of P-CRR and S-CRR can be briefly summarized. More explanation of the motivation behind this work could be added.\n\n[1] Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\n\n[2] Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"
            },
            "questions": {
                "value": "Refer to weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658541730,
        "cdate": 1698658541730,
        "tmdate": 1699636026769,
        "mdate": 1699636026769,
        "license": "CC BY 4.0",
        "version": 2
    }
]