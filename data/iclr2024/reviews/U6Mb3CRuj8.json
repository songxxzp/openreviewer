[
    {
        "id": "vURGF9NfgW",
        "forum": "U6Mb3CRuj8",
        "replyto": "U6Mb3CRuj8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_L1D2"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed TADA, which uses SNR to control the strength of augmentation functions applied to the images in different timesteps when training diffusion models. The experimental results show the effectiveness of TADA on FFHQ and AHFQ-v2 datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Although the idea of learning the augmentation strength is studied in image classification, using SNR as a measure to control the augmentation strength in generative diffusion models seems novel to me.\n\nThe paper is easy to follow."
            },
            "weaknesses": {
                "value": "(1) As TADA uses multiple types of augmentation, including h-flip, the improvement of TADA over the h-flip baseline may come from the use of other augmentations but not the proposed weighting scheme. The author should include more experiments to show the effectiveness of the time-aware weighting scheme, for example, comparing with a baseline with a fixed augmentation strength while keeping other configurations the same as TADA.\n\n(2) TADA underperforms the AR baseline in various settings. The author should repeat the experiments multiple times and report the average and standard deviation of the results.\n\n(3) The proposed timestep-aware weighting scheme is motivated based on the observation of two toy examples on 200 images from the FFHQ-5k dataset. Many of the parameters are selected manually: the augmentation probability, the choice of the augmentation functions, the SNR-sensitive region, and the maximum augmentation strength $\\delta$. As observed from Table 9, the FID and KID increase if we use a sensitive region of [-3, 1] instead of [-3, 0]. Although the current setting is found to be effective on FFHQ and AHFA, it is unclear whether these pre-defined parameters need to be fine-tuned on any new datasets.\n\nWhile it is acceptable that there are no solid theories supporting the timestep-aware mechanism, the method should be validated on sufficiently many scenarios, showing that the method is effective in general. I believe evaluating the method only on human faces (FFHQ) and animal faces (AHFQ) is not enough."
            },
            "questions": {
                "value": "The proposed TADA can only apply a timestep-aware global strength for all augmentations. The method cannot assign strength and probability values for individual augmentations, which may result in limited augmentation flexibility. In Table 8, applying only the color transformation leads to the best result. However, TADA cannot discover such an augmentation policy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697530991764,
        "cdate": 1697530991764,
        "tmdate": 1699636040361,
        "mdate": 1699636040361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pmkHfkvVp5",
        "forum": "U6Mb3CRuj8",
        "replyto": "U6Mb3CRuj8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_xARD"
        ],
        "content": {
            "summary": {
                "value": "This paper studies data augmentation in the training of diffusion models. Through two pre-experiments, the paper found that the level of impact of the data augmentation transformation on the output differs at each time step, especially in the interval called \"sensitive time-steps,\" where the inverse diffusion process of the diffusion model significantly alters the input image. Based on this finding, the paper proposes a method to adjust the strengths of the data augmentation at each time step. Experiments show that the proposed method partially outperforms the existing baseline on FFHQ and AFHQ-v2. On the other hand, there is a lack of discussion on how the change in transformation strengths at each time step affects learning data distribution. In addition, the experiments are not extensive in evaluation datasets and diffusion model formulations, raising questions about the generality of the proposed method."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is the first to investigate the impact of data augmentation at each time step in training a diffusion model.\n+ Based on experimental observations, the paper proposes a heuristic data augmentation method that adjusts the strengths of the transformation at each time step."
            },
            "weaknesses": {
                "value": "- While the observations by Toy Examples 1 and 2 provided by the paper are interesting, the paper fails to explain why the distribution shifts arise in the diffusion model from these results (-> **Q1** and **Q2**).\n- The theoretical foundation for the proposed method is weak. It is not clear why the proposed method solves the problems of distribution shift and overfitting (-> **Q3**).\n- The experiments are limited in terms of the variations of datasets and the diffusion models. This limits the impact of the paper's claim and is inconsistent with the claims made in the Introduction (-> **Q4** and **Q5**).\n- The experimental setup is unfair. The proposed method introduces color transformations that are not used in the baseline (AR), making it impossible to evaluate the proposed method with the experimental results. Also, in some of the experimental evaluations (Table 4 and Fig. 4), there is no comparison with the AR and the presentation is selective (-> **Q6** and **Q7**).\n- The evaluation of the distribution shift in Fig. 3 is qualitative and subjective, and not convincing.\n- The quality of the presentation with figures and tables is poor. Fig. 1(a), 4, and 5 cannot be accurately understood by a colorblind person like me."
            },
            "questions": {
                "value": "**Q1.** Why do the results in Fig. 1 indicate a distribution shift at sensitive time steps? To begin with, the definition of \"distribution shift\" here is vague and difficult to understand intuitively. According to the protocol in Sec. 3.2, the experiment in Toy Example 1 was performed by applying a data augmentation $T$ to an input image $x_0$ to generate $T(x_0)$, generating $x_t$ by adding noise, and applying an inverse diffusion step. Finally, the LPIPS between $\\hat{x}_0$ and the original image $x_0$ are measured. This can evaluate the denoising performance of each step of the diffusion model, but it is unclear what LPIPS($x_0$, $\\hat{x}_0$) means. Since $T(x_0)$ differs from $x_0$, we do not expect the inverse diffusion process to reconstruct $x_t$ into $x_0$. Therefore, it is unlikely that it makes sense to compare them. Also, the paper should provide more explanations about what it means theoretically to observe this LPIPS gap across data augmentations.\n\n**Q2.** What does Toy Example 2 imply? The paper should specify what it wants to investigate by replacing $\\hat{\\epsilon}$. At least, I did not understand how the findings from this experiment are connected to the proposed method.\n\n**Q3.** What is the theoretical background of the proposed method and what is the expected semantic behavior as a data augmentation? Why does the strength of the transformation at each time step help to suppress distribution shifts and overfitting?\n\n**Q4.** Why does the paper experiment only with the facial datasets, i.e., FFHQ and AFHQ-v2? How about the performance of the proposed method on CIFAR-10 and ImageNet, which are used in EDM [a]? In the introduction, it is claimed that the proposed method is \"applicable across various diffusion model settings\", but since it is evaluated on fewer datasets than previous studies like EDM, the current results are not sufficient to claim the applicability.\n\n**Q5.** Is the proposed method effective for formulations other than ADM? For example, how about using latent diffusion [b]? The paper should show that $r_\\text{rough}$ and $r_\\text{fine}$ are robust across the model formulations.\n\n**Q6.** Why does the proposed method add transformations that are not used in AR? The paper states in section 4.1 that \"we incorporate color transformations, which were not used in EDM\". We could not compare the proposed method and AR without using shared transformations for the data augmentation because the effects of adding transformations cannot be separated from the effects of the proposed method.\n\n**Q7.** Why are the AR results not listed in Table 4 and Fig. 4? Similarly, na\u00efve data augmentation baselines that do not use the proposed method but use the same data augmentation transformations should be added to all experiments. The lack of these evaluations damages the significance of the paper.\n\n[a] Karras, Tero, et al. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems 35 (2022): 26565-26577.\n\n[b] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698135117344,
        "cdate": 1698135117344,
        "tmdate": 1699636040278,
        "mdate": 1699636040278,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g5lwpY6sWY",
        "forum": "U6Mb3CRuj8",
        "replyto": "U6Mb3CRuj8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_zkiV"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new data augmentation technique for the generative models using diffusion models. Data augmentation of generative models causes a distribution shift, which leads to the generation of unintended out-of-distribution samples. To avoid the distribution shift, the proposed method uses augmented data in early and last time steps in the reverse process. To justify this approach, this paper empirically reveals that only the middle time steps, which are called sensitive time steps, cause a distribution shift. \nExperiments demonstrate that the proposed augmentation method outperforms the augmentation regularization method and 50% horizontal flip."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written, and experiments are well-designed to present the claim of this paper.\n- The empirical analyses about sensitive time steps are interesting and important. I think the existence of sensitive time steps will have an impact on future data enhancement techniques for diffusion models. I am not an expert in diffusion models, and it might not be very surprising that the process in the middle time step is sensitive. Even so, two toy experiments carefully confirm this phenomenon. \n- This paper evaluates the proposed method from various perspectives.\nFor example, this paper evaluates the proposed method on different scales of the model, with/without transfer learning, and on high-resolution data. An ablation study is also contained, and I think the proposed method is sufficiently evaluated."
            },
            "weaknesses": {
                "value": "- It is difficult for the reader to reproduce the experiments in this paper alone. The proposed method is not very clearly presented because this paper does not contain pseudo codes or an explanation of the procedure of the whole proposed method. The specific data augmentation methods used in the proposed method are left to [Karras et al. (2020)] and not explained.\nThus, the paper by itself does not tell the readers how the strength of data augmentation $w_t$ works.\n- This paper seems to evaluate the proposed method on only a limited data setting. Since data augmentation for discriminative models tends to improve performance when using full datasets besides limited data size settings, I would like to see the performance of the proposed method when a dataset has a sufficiently large size. If the proposed method does not improve the performance in this situation, it is the limitation of the proposed method."
            },
            "questions": {
                "value": "- Does the proposed method work well if a dataset has a sufficiently large size?\n- Where can the readers understand the whole procedure of the proposed method? I think the readers have to carefully read several points e.g., Augmentation pipeline in Section 4.1, besides Section 3 to understand the proposed method, which is a bit burdensome."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698323737755,
        "cdate": 1698323737755,
        "tmdate": 1699636040166,
        "mdate": 1699636040166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9GSN3QQlKF",
        "forum": "U6Mb3CRuj8",
        "replyto": "U6Mb3CRuj8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_o4Pj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1139/Reviewer_o4Pj"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an interesting and novel approach to data augmentation in diffusion models by introducing a timestep-aware augmentation strategy. The authors identify a significant issue in the standard application of data augmentation to diffusion models, specifically the distribution shifts that occur and are tied to certain timesteps within the generative process. The proposed TADA approach is a logical and promising solution to this issue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality:** The paper addresses a gap in the literature by focusing on diffusion models where timestep-related distribution shifts during data augmentation have been understudied.\n\n**Relevance:** Given the increasing importance of diffusion models in various generative tasks, the paper's focus is timely and relevant to the community.\n\n**Technical Depth:** The paper appears to offer a comprehensive study and a well-thought-out augmentation strategy that is sensitive to the unique requirements of diffusion models.\n\n**Empirical Evidence:** The authors provide experimental results that suggest their method is robust across different diffusion model settings and datasets, which is commendable."
            },
            "weaknesses": {
                "value": "**Clarity and Depth of Experiments:** While the experiments are diverse, the review is based on a limited preview, and a more thorough assessment of the experimental design and results is necessary.\n\n**Comparison with Baselines:** The paper would benefit from a more detailed comparison with existing data augmentation methods, particularly in how the results translate to qualitative improvements in the generated samples.\n\n**Impact and Significance:** The practical impact of the proposed method, while promising, is not fully established. The paper could strengthen its argument by demonstrating clear use cases where its method significantly outperforms existing techniques."
            },
            "questions": {
                "value": "**Methodology Clarification:**\n\nCould you provide more detail on how the augmentation strength is adjusted during different timesteps? Is there a theoretical framework or empirical evidence that supports the chosen method?\n\n**Experimental Design:**\n\nIn your experiments, did you explore the impact of the timestep-aware augmentation on the convergence speed and stability of the training process for diffusion models?\nHow does the proposed augmentation method affect the computational resources required for training and inference compared to traditional augmentation techniques?\n\n**Baseline Comparisons:**\n\nHow does TADA compare with other state-of-the-art data augmentation methods in terms of qualitative and quantitative results?\nCould you provide insights or metrics on how the proposed method improves the robustness and generalization of diffusion models in scenarios with limited data?\n\n**Limitations and Challenges:**\n\nWhat are the potential limitations or drawbacks of your proposed method?\nAre there specific types of diffusion models or data domains where TADA might be less effective?\n\n**Scalability and Generalization:**\n\nHow scalable is the TADA method with respect to model size and dataset complexity?\nHave you tested the method on out-of-distribution samples or on datasets with significant class imbalances?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1139/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699432775464,
        "cdate": 1699432775464,
        "tmdate": 1699636040091,
        "mdate": 1699636040091,
        "license": "CC BY 4.0",
        "version": 2
    }
]