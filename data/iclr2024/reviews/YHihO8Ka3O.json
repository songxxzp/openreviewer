[
    {
        "id": "QvKCdZxkWj",
        "forum": "YHihO8Ka3O",
        "replyto": "YHihO8Ka3O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_nEXk"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a scalable Federated Graph Transformer\n(FedGT) to address the data heterogeneity and missing link challenges. In contrast to GNNs that follow message-passing schemes and focus on local neighborhoods, Graph Transformer has a global receptive field to learn long-range dependencies and is, therefore, more robust to missing links. Moreover, a novel personalized aggregation scheme is proposed. Extensive experiments show the advantages of FedGT over baselines in 6 datasets and 2 subgraph settings."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The paper is well-written and organized. The details of the models are described clearly and are convincing.\n2.The limitations of applying GNNs for subgraph federated learning are clearly illustrated in Figure 1 and Figure 4 in appendix. The motivation for leveraging graph transformers is easy to understand.\n3.The authors proposed a series of effective modules to tackle the challenges, including scalable graph transformers, personalized aggregation, and global nodes. The contribution is significant enough.\n4.FedGT is compared with a series of SOTA baselines, including personalized FL methods, federated graph learning methods, and adapted graph transformers. Extensive experiments on 6 datasets and 2 subgraph settings demonstrate\nthat FedGT can achieve state-of-the-art performance."
            },
            "weaknesses": {
                "value": "1.The authors are suggested to clearly discuss the case studies in the main paper.\n2.Leveraging local differential privacy mechanisms to protect privacy in FL is not new.\n3.Please provide more explanations of the assumptions in Theorem 1."
            },
            "questions": {
                "value": "1.Can the authors introduce more about the roles of global nodes in FedGT?\n2.Is FedGT applicable to other subgraph settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698164170169,
        "cdate": 1698164170169,
        "tmdate": 1699636357603,
        "mdate": 1699636357603,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HG76TReo2z",
        "forum": "YHihO8Ka3O",
        "replyto": "YHihO8Ka3O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a scalable Federated Graph Transformer (FedGT) for subgraph federated learning, which addresses the challenges of missing links between subgraphs and subgraph heterogeneity. It uses a hybrid attention scheme to reduce complexity while ensuring a global receptive field and computes clients\u2019 similarity for personalized aggregation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper is easy to read, and generally well-written.\n2.\tThe idea of using the Graph Transformer to address the issue of missing links across clients is well-motivated."
            },
            "weaknesses": {
                "value": "1.\tHow to aggregate global nodes is not clearly illustrated. On page 6, the authors state, \u201cthe global nodes are first aligned with optimal transport and then averaged similar to Equation 8\u201d. However, it is unclear which optimal transport method is applied and how the similarity between global nodes from different clients is calculated. The authors should clarify whether the normalized similarity \u03b1_ij used for model parameters is also employed for global nodes or if a different similarity calculation is used. Besides, in Algorithm 3 lines 11 and 13, the aligning process for the global nodes seems to be performed twice, which needs a clearer explanation.\n\n2.\tSince the weighted averaging of local models, i.e., Equation (8), is the same in [1], the authors should provide a discussion or experiment to explain why their similarity calculation is superior to that in [1].\n\n3.\tTo show the convergence rate, Figure 5 and Figure 6 did not contain FED-PUB, which is the runner-up baseline in most cases. \n\n4.\tIn the ablation study, the authors only conduct experiments on w/o global attention and w/o personalized aggregation. Results of w/o the complete Graph Transformer (i.e., without local attention) should also be provided.\n[1] Baek J, Jeong W, Jin J, et al. Personalized subgraph federated learning[C]//International Conference on Machine Learning. PMLR, 2023: 1396-1415."
            },
            "questions": {
                "value": "1.\tThe authors opt for a consistent number of global nodes n_g across all clients. However, how does the methodology account for scenarios in which clients have a varying number of nodes, with some having significantly more and others noticeably fewer? Is there a suggested approach for determining varying n_g values that are customized to each client\u2019s node count?\n\n2.\tIn the typical federated learning framework, the number of training samples is considered when aggregating the model parameters. However, Equation (8) only uses the normalized similarity for the weighted aggregation. Why can we ignore the number of training samples here? Or do we assume the number of training samples is equivalent across clients?\n\n3.\tThe Hungarian algorithm only finds a bijective mapping while optimal transport can be generalized to many-to-many cases, could the authors explain the reason for making a one-to-one alignment of the global nodes?\n\n4.\tSince the global nodes are dynamically updated during the training, and the representations of the nodes are not stable at the beginning of the training, would this impact the effectiveness of similarity calculation based on the global nodes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3964/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3964/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3964/Reviewer_uFDb"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698645675259,
        "cdate": 1698645675259,
        "tmdate": 1700640616530,
        "mdate": 1700640616530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DU3AsjAHlS",
        "forum": "YHihO8Ka3O",
        "replyto": "YHihO8Ka3O",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_jUaw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3964/Reviewer_jUaw"
        ],
        "content": {
            "summary": {
                "value": "The authors propose to use Graph Transformer and optimal-transport-based personalized aggregation to alleviate the fundamental problems in the subgraph federated learning algorithm such as missing links and subgraph heterogeneity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Leverages graph transformer architecture within subgraph FL for the first time in the federated graph learning literature.\n\n(2) The algorithm is compatible with local DP.\n\n(3) Experimentally shows that Transformers are useful for subgraph federated learning. \n\n(4) Theoretical analysis of global attention being able to capture and approximate information in the whole subgraph is provided."
            },
            "weaknesses": {
                "value": "(1) How Graph Transformer deals with the missing links is unclear. \n\n(2) The assumption that nodes are equally distributed to the global nodes seems unrealistic due to graph partitioning.\n\n(3) Theorem is not rigorous as it is a known fact that more nodes less error [1]\n\n(4) Local LDP does not guarantee privacy for sensitive node features, edges, or neighborhoods on\ndistributed graphs [2,3]. Using LDP does not reflect an actual privacy guarantee for this case.\n\n[1] Kim, Hyunjik, George Papamakarios, and Andriy Mnih. \"The Lipschitz constant of self-attention.\" International Conference on Machine Learning. PMLR, 2021.\n[2] Imola, Jacob, Takao Murakami, and Kamalika Chaudhuri. \"Locally differentially private analysis of graph statistics.\" 30th USENIX security symposium (USENIX Security 21). 2021.\n[3]Kasiviswanathan, Shiva Prasad, et al. \"Analyzing graphs with node differential privacy.\" Theory of Cryptography: 10th Theory of Cryptography Conference, TCC 2013, Tokyo, Japan, March 3-6, 2013. Proceedings. Springer Berlin Heidelberg, 2013."
            },
            "questions": {
                "value": "(1) Could you please compare FedGT with FedDEP [1]? \n\n\n\n[1] Zhang, Ke, et al. \"Deep Efficient Private Neighbor Generation for Subgraph Federated Learning.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3964/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788786822,
        "cdate": 1698788786822,
        "tmdate": 1699636357405,
        "mdate": 1699636357405,
        "license": "CC BY 4.0",
        "version": 2
    }
]