[
    {
        "id": "YpBa9oydfe",
        "forum": "eddd0YTCiq",
        "replyto": "eddd0YTCiq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_dWk6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_dWk6"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new self-supervised technique for graph neural networks. Grounded on joint-embedding predictive architecture (JEPAs), the proposed Graph-JEPA is designed to predict the latent embeddings for multiple subgraphs based on a random subgraph. Experiments are performed on graph-level tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe design of the loss objective is good.\n\n2.\tThe ablation studies and discussion are detailed and insightful."
            },
            "weaknesses": {
                "value": "1.\tMissing literature in related work. In addition to contrastive methods and generative methods, self-supervised graph representation should also include existing predictive methods [1]. For example, CCA-SSG [2] and LaGraph [3] are two existing works using latent embedding prediction. Such predictive methods should be discussed in related works, and they should be used as baseline methods to compare results. \n\n2.\tThe performance improvement is marginal based on the main results in Table 1. \n\n3.\tMost existing SSL methods can handle both graph-level and node-level tasks. However, the proposed Graph-JEPA only supports graph-level downstream tasks. \n\n[1]. Xie, Yaochen, et al. \"Self-supervised learning of graph neural networks: A unified review.\" IEEE transactions on pattern analysis and machine intelligence 45.2 (2022): 2412-2429.\n\n[2]. Zhang, Hengrui, et al. \"From canonical correlation analysis to self-supervised graph neural networks.\" Advances in Neural Information Processing Systems 34 (2021): 76-89.\n\n[3]. Xie, Yaochen, Zhao Xu, and Shuiwang Ji. \"Self-supervised representation learning via latent graph prediction.\" International Conference on Machine Learning. PMLR, 2022."
            },
            "questions": {
                "value": "1.\tAuthors claim that the Graph-JEPA is more efficient than contrastive methods since it doesn\u2019t require data augmentations or negative samples. I\u2019m wondering how efficient it is. Could you add a quantitative comparison for the efficiency?\n\n2.\tThe proposed Graph-JEPA uses Transformer encoder blocks. However, most baseline models are based on simpler models like GIN and GCN. Is it an unfair comparison? Can you use GIN/GCN encoder?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2151/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2151/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2151/Reviewer_dWk6"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698265182201,
        "cdate": 1698265182201,
        "tmdate": 1700668831389,
        "mdate": 1700668831389,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SNW6F2b5uA",
        "forum": "eddd0YTCiq",
        "replyto": "eddd0YTCiq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_15Jn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_15Jn"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Graph-JEPA. Graph-JEPA uses two encoders to receive the input and one of the encoders predicts the latent representation of the input signal based on another encoder."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The writing is clear. First JEPA for graph. The authors provide an analysis to explain why JEPA works for the graph domain."
            },
            "weaknesses": {
                "value": "1. The method is not novel. The proposed Graph-JEPA is very similar to MLM in BERT, which utilizes the context to predict the masked word type. \n\n2.  The proposed method is too simple and the motivation is not clear. We have graph MAE and contrastive learning. Why do we need JEPA for the graph domain?\n\n3. Compared with graph MAE and S2GAE, the performance is not good enough to show it can inspire future research."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698463005528,
        "cdate": 1698463005528,
        "tmdate": 1699636147978,
        "mdate": 1699636147978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4NDkIA2XpX",
        "forum": "eddd0YTCiq",
        "replyto": "eddd0YTCiq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_b3AV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2151/Reviewer_b3AV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Graph-JEPA, the first Joint-Embedding Predictive Architectures (JEPAs) for the graph domain.\nThe application of JEPA to graphs seems to be novel."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The proposed method is technically sound\n- Based on the experimental results, the improvement between Graph-JEPA over the baselines seems to be strong"
            },
            "weaknesses": {
                "value": "- The overall method seems to be a direct application of JEPA to graphs.\n- The discussion of \"why does graph-JPEA works\" is useful, but not information. Any theoretical analysis here will be useful.\n- The experimental settings are confusing. It is unclear to me why \"GCN\", a GNN model, can be compared with \"Graph-JEPA\", which is a graph self-supervised training method.\n- All the figures and tables are not professional and could be improved to be more appealing. Font sizes and colors should be improved."
            },
            "questions": {
                "value": "- What makes applying JEPA to graphs special and non-trivial?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2151/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822398426,
        "cdate": 1698822398426,
        "tmdate": 1699636147900,
        "mdate": 1699636147900,
        "license": "CC BY 4.0",
        "version": 2
    }
]