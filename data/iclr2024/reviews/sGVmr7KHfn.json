[
    {
        "id": "AZmy549oHO",
        "forum": "sGVmr7KHfn",
        "replyto": "sGVmr7KHfn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_dY2C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_dY2C"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Memory-Assisted Sub-Prototype Mining (MemSPM) method for Universal Domain Adaptation. The primary goal is to better align classes and reduce feature gaps between source and target domains, addressing the limitations of most existing models that do not consider intra-class structures, especially when significant concept shifts exist within the same category. The proposed MemSPM enhances model performance by effectively identifying and learning from sub-classes that have considerable concept shifts, leading to a more reasonable feature space, improved transferability, and interpretability. Experimental results across several scenarios (UniDA, OSDA, PDA) show that the MemSPM method outperforms existing benchmarks in many instances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The Memory-Assisted Sub-Prototype Mining (MemSPM) brings a novel perspective to the Universal Domain Adaptation domain by addressing the challenge of significant concept shifts within the same category. The paper highlights the limitation of existing models and provides a robust solution to enhance transferability and reflect inherent sample differences. Experimental results support the claims and showcase the model's superior performance in many scenarios."
            },
            "weaknesses": {
                "value": "While the paper introduces a novel approach, it could benefit from clearer explanations and visual aids, making the methodology more accessible to readers. Additionally, the paper could delve deeper into potential drawbacks or limitations of the proposed method. Comparison with a broader set of benchmarks might also provide a more comprehensive understanding of the model's applicability and robustness."
            },
            "questions": {
                "value": "- How does the Memory-Assisted mechanism integrate with existing architectures, and what is its computational overhead?\n\n- Are there any specific scenarios or datasets where the MemSPM method might not be as effective?\n\n- Can the authors provide more insights into the annotation/training costs mentioned and how the proposed method addresses this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Reviewer_dY2C"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636000183,
        "cdate": 1698636000183,
        "tmdate": 1699636405991,
        "mdate": 1699636405991,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DzW8HBFRod",
        "forum": "sGVmr7KHfn",
        "replyto": "sGVmr7KHfn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_mKia"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_mKia"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) for the UniDA problem, which involves the idea of sub-prototypes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of sub-prototype is reasonable and worth trying for UniDA."
            },
            "weaknesses": {
                "value": "1. The comparison of DCC and MemSPM+DCC is not fair, due to the usage of CLIP in MemSPM. The author SHOULD replace the CLIP with a learnable encoder to achieve a fair comparison.\n2. Only involving the idea of sub-prototype is not novel enough for the acceptance of top conference. How to use it to solve the concept shift in Fig.1 for UniDA is the key. However, in my opinion, the loss in MemSPM (i.e. Cycle Consistent Matching) is exactly the same with DCC. So, I can not find anything new here."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746856220,
        "cdate": 1698746856220,
        "tmdate": 1699636405904,
        "mdate": 1699636405904,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CLeEc6PT95",
        "forum": "sGVmr7KHfn",
        "replyto": "sGVmr7KHfn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_S3ot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_S3ot"
        ],
        "content": {
            "summary": {
                "value": "In this study, the authors introduce the Memory-Assisted Sub-Prototype Mining method, which emphasizes the significance of the internal structure within the category. Unlike conventional methods that treat the classes as a whole, MemSPM refines class features and mines sub-prototypes to represent sub-classes, thereby enhancing adaptation performance. The study provides extensive experiments on four benchmarks to validate its performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper proposes a novel method called Memory-Assisted Sub-Prototype Mining, designed to significantly improve the model\u2019s adaptability. Additionally, the approach provides insightful visualizations, providing a clear understanding of the methodology.\n2.\tThe effectiveness of the proposed method has been assessed through extensive experiments."
            },
            "weaknesses": {
                "value": "1.\tWhile the proposed method demonstrates effectiveness, it appears that the learned sub-prototype does not effectively handle the most challenging aspect of UniDA, such as identifying common and private samples. The Sub-Prototype Mining method seems better suited for general domain adaptation problems rather than specifically addressing the complexities of UniDA.\n2.\tThe paper lacks explicit clarification on how the sub-prototype is initialized and updated, leaving a gap in understanding the methodology.\n3.\tSection 3.5 is unclear and requires further elaboration. The concept of cycle-consistent alignment needs to be more explicitly reflected in the training phase. Additionally, a comparison of performance differences with other alignment strategies would enhance the paper's comprehensiveness.\n4.\tA significant portion of the paper relies heavily on DCC, and the proposed method appears to primarily involve weighting input-oriented embeddings to obtain task-oriented embeddings, which seems to be the only deviation from DCC. This raises concerns about the novelty and originality of the proposed approach."
            },
            "questions": {
                "value": "1.  In part 3.3.1, there are N memory items stored. However, the explanation of how these N items are produced and the relationship between them is not clear. Additionally, this paper does not provide a clear explanation of how the sub-prototypes are generated and updated.\n\n2.  In Table 6, why only show the performance without the domain alignment loss?  Need to report the performance without the regularized loss and the reconstruction loss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698852537789,
        "cdate": 1698852537789,
        "tmdate": 1699636405844,
        "mdate": 1699636405844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MlQAIn0txc",
        "forum": "sGVmr7KHfn",
        "replyto": "sGVmr7KHfn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_59Jy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4348/Reviewer_59Jy"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC [24] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.\n\n* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.\n\n* The paper is fairly well-written and easy to understand.\n\n* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show improvements over some existing methods using the CLIP-pretrained model, this will serve as a new strong baseline for future UniDA work."
            },
            "weaknesses": {
                "value": "* Missing sensitivity analysis to hyperparameter $K$ in adaptive thresholding\n    * There is no information on how $K$ is chosen to be used in the top-$K$ operation used to compute the adaptive threshold $\\lambda$ in Eq. 5. It may be difficult to tune for new datasets.\n    * As per Table 6, using a fixed threshold drops the performance by almost 10%, which is alarming. Hence, the sensitivity to $K$ could be important and needs to be studied.\n    * One simple baseline to get rid of $K$ could be to use a higher temperature in the attention computation, i.e. to make the attention distribution sharper.\n\n* Incomplete analysis of sensitivity to hyperparameter $S$\n    * As per Fig. 3b, the performance improves drastically when $S$ is increased. But the analysis is only up to $S=40$ and it seems that the performance should improve further if $S$ is increased more.\n    * The paper says that \u201cWhen $S \\geq 20$, the performance achieves a comparable level\u201d but there is almost a 3-4% increase when $S$ increases from 20 to 30 and also from 30 to 40. Then, it seems that increasing $S$ further should yield more improvements.\n    * In any case, it would be good to identify when the performance starts saturating, to help users select the hyperparameter properly for other datasets. \n\n* Missing discussion on training time and memory requirements\n    * Given the extra prototypes involved, there should be some discussion on training time and GPU memory usage compared to the baseline DCC."
            },
            "questions": {
                "value": "* Please see the weaknesses section.\n\n* Minor comments\n    * Paragraph above Eq. 6 has typos, add space after citation of Gong et al. 2019.\n    * Fig. 3: increase the font size of text inside the figure to match the caption font size. This will improve the readability of the plots.\n    * Paragraph on \u201cEffect of Loss\u201d has a typo: \u201cWe\u201d \u2192 \u201cwe\u201d.\n    * Table 6 (last row) has a typo: use math mode in LaTeX for $\\mathcal{L}\\_{cdd}$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4348/Reviewer_59Jy"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4348/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698973448591,
        "cdate": 1698973448591,
        "tmdate": 1699636405781,
        "mdate": 1699636405781,
        "license": "CC BY 4.0",
        "version": 2
    }
]