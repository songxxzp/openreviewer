[
    {
        "id": "5nAW4xXSAz",
        "forum": "dIynM7bHCG",
        "replyto": "dIynM7bHCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_mbZH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_mbZH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes DOM2, a decentralized training and execution Offline MARL approach using diffusion policies. Their algorithm combines CQL (critic) and DPM-Solver (actor) as well as data augmentation to increase the dataset size with good trajectories. DOM2 allows for better generalization to slightly shifted environments as well as improved data efficiency. DOM2 is evaluated on MPE and HalfCheetah-v2 and shows improved performance across all datasets, and even shows good performance on random datasets where all other previous methods fail."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The experimental results are impressive and it is especially surprising to see DOM2 perform well in random datasets across all tasks, which is challenging for conservatism-based offline RL algorithms.\n\n2. While DOM2 uses decentralized training without any consideration of the non-stationarity in MARL (see Weaknesses), I find it interesting that decentralized training is able to perform well across all datasets and even exhibit multi-modal behavior."
            },
            "weaknesses": {
                "value": "1. Since the policy training loss in Eq. 5 is decentralized, there is no guarantee that DOM2 maximizes the global Q-function for the overall Dec-POMDP. The $Q(o_j, a_j)$ is just an individual utility function which ignores the non-stationarity of the environment due to the other agents\u2019 policies being updated during training. Since many environments require policy dependence to be considered (see e.g. [1]), there should be some clear insight as to (a) what kinds of environments and (b) what specific aspect of DOM2/diffusion models in general can allow for the community to consider decentralized training.\n\n2. The paper appears to be a relatively simple combination and application of existing work, namely CQL for the critic loss, DPM Solver for the diffusion policy. This is not a problem in and of itself but there is not insight or deeper analysis of the specific properties of DOM2/diffusion policies which makes it suitable for offline MARL. \n\n3. While the results on MAMuJoCo and MPE are impressive, DOM2 should be tested on more complex environments requiring agents to coordinate at a higher level e.g. Google Research Football, SMACv2.\n\n4. The main contribution or key insight of DOM2 from the diffusion model perspective is not clear. The analysis below Algorithm 2 only refers to the architectural differences compared to other algorithms.\n\n5. The claim that DOM2 is ultra data efficient is not convincing to me as data augmentation is included in the DOM2 algorithm but it seems the same augmentation technique could be used for other baselines.\n\n[1] Revisiting Some Common Practices in Cooperative Multi-Agent Reinforcement Learning (Fu et, al. ICML 2022)"
            },
            "questions": {
                "value": "1. If a Dec-POMDP is considered, why does the reward $r_j^t$ index on agent ID? Are different reward values given for each agent in the environment during the experiments as well?\n\n2. Is there any insight regarding why decentralized training is enough for DOM2 perform well? For instance, it could be the case that (1) the environments considered are too simple or (2) some specific property of using diffusion policies make it such that there is some implicit dependence among policies or (3) continuous control environments in practice require less  dependence among policies. \n\n3. If the critic is trained using the CQL loss, then it seems that the Q values will just be conservative to OOD actions. This means that in order to produce policy diversity, the dataset must already contain the diverse behavior. Is my understanding correct here? I also considered the possibility that data augmentation helps with behavior diversity but Figure 6 suggests that it is not crucial. \n\n4. As far as I can tell, the data augmentation technique seems orthogonal to the DOM2 algorithm itself. If that is the case, shouldn\u2019t all baselines also include the data augmentation technique? Is it really possible to say DOM2 is \u201cultra\u201d data efficient without a fair comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697959476291,
        "cdate": 1697959476291,
        "tmdate": 1699636170270,
        "mdate": 1699636170270,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FiZPOoIfQm",
        "forum": "dIynM7bHCG",
        "replyto": "dIynM7bHCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
        ],
        "content": {
            "summary": {
                "value": "Recent works in offline Reinforcement Learning (RL) rely on conservatism. The paper presents Diffusion Offline Multi-agent Model (DOM2), which improves policy design and diversity using a diffusion model. DOM2 utilizes a diffusion model in the policy network and makes use of a trajectory-based data augmentation scheme during offline training. The data augmentation technique trains DOM2 agents on a replay buffer wherein trajectories with higher rewards are duplicated. Ablation studies and experiments demonstrate the empirical effectiveness of proposed design choices."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well-written and organized.\n* Empirical evaluation provided by the authors is comprehensive."
            },
            "weaknesses": {
                "value": "* **Claims on Conservatism:** My main concern is the claims on conservatism used within the DOM2 model. Authors argue that different from prior works they do not rely on conservatism for policy design. The paper also states that learning policies and values with conservatism is inefficient. However, DOM2 model significantly relies on Conservative Q Learning (CQL) to train Q values and hence the policy $\\pi$. Furthermore, CQL forms the key part of DOM2 as it is the only ingredient used for policy improvement. This can be validated from ablation studies wherein the conservative policy improvement scheme contributes to most gains in the performance of DOM2. Thus, claims on conservatism severely contradict the paper's central idea.\n* **Data Augmentation:** The proposed data augmentation scheme prioritizes highly rewarding trajectories while downweighing lower ones. DOM2 agents, thus, have access to privileged data samples rather than augmented samples as the datapoints itself have not been modified in any way (eg- shifting, scaling, transformed, etc.). In this view, the training process appears to be biased resulting in a near-expert dataset for DOM2 agents and sub-optimal dataset for baseline agents. Note that other baselines do not have access to privileged data samples but only the orignal dataset. This leads DOM2 to outperform prior methods as a result of dataset selection and not algorithmic modifications.\n* **Choice of Baselines:** While the empirical evaluation provided in the paper is comprehensive, authors only compare DOM2 to multi-agent baselines. It would be worthwhile to consider other offline RL algorithms in multi-agent settings which have demonstrated cutting edge performance. The paper could compare DOM2 to independent IQL learners [1] or BRAC agents [2] in the multi-agent setting. Similarly, authors could assess the choice of policy improvement scheme using a different offline RL algorithm such as BEAR [3]. This would help validate the claims of conservatism and evaluate the importance of CQL during training.\n* **Differences from Prior Work:** I struggle to understand the central contribution of DOM2 within the offline RL literature. Using diffusion models for learning policies is a common practice in offline RL. In addition, the data augmentation scheme corresponds to a top-k sampling strategy wherein trajectories with higher rewards are sampled. It is thus unclear as to what is the novel contribution of DOM2 within multi-agent offline RL literature. It would be worthwhile if authors could highlight the differences between DOM2 and recent algorithms such as Diffuser [4], EDP [5], MADIFF [6], OMAC [7] and OMIGA [8] explicitly. Additionally, authors could discuss the benefits or design choices which are not found in standard multi-agent learning algorithms.\n\n[1]. Kostrikov et. al., \"Offline Reinforcement Learning with Implicit Q-Learning\", ICLR 2022.  \n[2]. Wu et. al., \"Behavior Regularized Offline Reinforcement Learning\", arxiv 2019.  \n[3]. Kumar et. al., \"Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction\", NeurIPS 2019.  \n[4]. Janner et. al., \"Planning with Diffusion for Flexible Behavior Synthesis\", ICML 2022.  \n[5]. Kang et. al., \"Efficient Diffusion Policies for Offline Reinforcement Learning\", arxiv 2023.  \n[6]. Zhu et. al., \"MADIFF: Offline Multi-agent Learning with Diffusion Models\", arxiv 2023.  \n[7]. Wang et. al., \"Offline Multi-Agent Reinforcement Learning with Coupled Value Factorization\", AAMAS 2023.  \n[8]. Wang et. al., \"Offline Multi-Agent Reinforcement Learning with Implicit Global-to-Local Value Regularization\", arxiv 2023."
            },
            "questions": {
                "value": "* Why is learning policies and value functions with conservatism inefficient? Can you please explain the reliance of DOM2 on CQL for conservatism?\n* Does trajectory-based augmentation provide high-quality samples only to DOM2? What if other baselines are trained with a similar scheme? Were any samples modified/augmented using shifting, scaling , etc. during training?\n* How does DOM2 compare with other offline multi-agent RL baselines such as IQL or BRAC? How effective is the usage of CQL for policy improvement? Can the policy improvement scheme be replaced/compared with another offline RL algorithm such as BEAR?\n* How is DOM2 different from Diffuser [4], EDP [5], MADIFF [6], OMAC [7] and OMIGA [8]? Can you please discuss some recent related works comparing DOM2 with offline RL and multi-agent RL literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_SRZp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678683095,
        "cdate": 1698678683095,
        "tmdate": 1699636170095,
        "mdate": 1699636170095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sR7ngPC4bh",
        "forum": "dIynM7bHCG",
        "replyto": "dIynM7bHCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_pMHc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_pMHc"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Diffusion Offline Multi-agent Model (DOM2), an offline MARL algorithm that is based on diffusion policy. DOM2 first augments the dataset by replicating the high-return trajectories. Then, each agent is trained independently by the Diffusion-QL-style learning method while using CQL loss for the critic. In the experiments, DOM2 outperforms the baselines in diverse MARL benchmarks including standard and shifted environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The empirical performance of the proposed DOM2 is strong. It outperforms the baselines in various domains and in shifted environment setting."
            },
            "weaknesses": {
                "value": "1. While the paper claims \"beyond conservatism\", it still relies on conservatism both in value learning and policy learning, both in critic learning (i.e. using CQL loss) and actor learning (i.e. using BC loss).\n2. The novelty is limited. It seems the proposed DOM2 is a straightforward extension of Diffusion-QL (Wang et al., 2023) with additional data augmentation. Since the overall training is done in a fully decentralized way, it seems there is no additional/special consideration in the algorithm for the 'multi-agent' setting. 'Why diffusion model for multi-agent RL' is not well-motivated in the paper.\n3. Given that each agent is trained independently (decentralized training, rather than centralized training), it may be suboptimal even in a very simple domain (e.g., like OMAR in XOR-game as described in [1]). Can DOM2 solve simple XOR-game-like domains?\n4. The proposed data augmentation (section 4.3) does not seem doing actual data augmentation. It is not generating novel data samples, but rather just replicating the existing data samples in the dataset. It just corresponds to changing the 'data sampling distribution' (uniform -> non-uniform depending on the trajectory return).\n\n[1] Matsunaga et al., AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation, NeurIPS 2023"
            },
            "questions": {
                "value": "Please see the weaknesses section above.\n- What is the difference between DOM2 with Diffusion-QL, except for the data augmentation? Also, could you elaborate on the core contribution of DOM2 to solve 'multi-agent' RL?\n- Why does DOM2 show better generalization performance than other baselines? Is it due to using diffusion policy, or from other factors?\n- I am also curious about the offline single-agent RL performance of DOM2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699297567712,
        "cdate": 1699297567712,
        "tmdate": 1699636169946,
        "mdate": 1699636169946,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DyGf4jVH0r",
        "forum": "dIynM7bHCG",
        "replyto": "dIynM7bHCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_tEv5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_tEv5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed to incorporate diffusion-based policy into multi-agent offline reinforcement learning, which is a straightforward extension of the diffusion-based policy from single-agent setting into the multi-agent counterpart. Most of the techniques are known to the community, but empirical results are quite strong."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Very strong empirical results."
            },
            "weaknesses": {
                "value": "* I don\u2019t find any new insights from this paper. Most of the techniques are from the existing work, and I don\u2019t get any intuitions on why we should do that."
            },
            "questions": {
                "value": "* What are the unique hardnesses of the multi-agent setting, compared with the single-agent setting? I feel there are no differences between the algorithm for single-agent setting and multi-agent setting, except that authors replace the state with the observation that can contain other agents\u2019 information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2375/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699370692181,
        "cdate": 1699370692181,
        "tmdate": 1699636169880,
        "mdate": 1699636169880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I6tceIrlF1",
        "forum": "dIynM7bHCG",
        "replyto": "dIynM7bHCG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes DOM2, which applies the Diffusion QL to cooperative multiagent settings following the independent learning paradigm. Extensive experiments on multi-agent particle and multi-agent MuJoCo environments show the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The writing of the paper is clear.\n* Extensive experiments on multi-agent particle and multi-agent MuJoCo environments are conducted."
            },
            "weaknesses": {
                "value": "* **The contribution of the paper is minor.** The proposed method DOM2 seems to be a simple application of Diffusion QL [1] to cooperative MARL. Besides, DOM2 just follows the independent learning paradigm (more like single-agent learning problems). \n* There are very few differences between DOM2 and Diffusion QL [1]. Replacing the DDPM-based diffusion policy with a faster first-order DPM-Solver should not be the main contribution of the paper. \n* **The proposed method DOM2 has little to do with multiagent.** Since the conservatism-based approaches in single-agent RL have limitations, why not directly apply the diffusion-based method to single-agent domains? As the proposed DOM2 is a decentralized training and execution framework (i.e., independent learner), evaluating the method in the single-agent domain is more straightforward.\n  * MA-DIFF (Zhu et al., 2023) have done some special designs to apply diffusion models to MARL under the CTDE paradigm, while DOM2  is a straightforward application of Diffusion QL [1].\n* The description of the motivating example shown in Figure 1 is not clear. \n* Since MA-SfBC (the extension of the single agent diffusion-based policy SfBC) is compared, MA-Diffusion QL (the extension of the single agent Diffusion QL) should also be compared.\n\n\nReferences\n\n* [1] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2375/Reviewer_dHvd"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2375/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699864763543,
        "cdate": 1699864763543,
        "tmdate": 1699864763543,
        "mdate": 1699864763543,
        "license": "CC BY 4.0",
        "version": 2
    }
]