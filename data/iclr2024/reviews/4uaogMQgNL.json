[
    {
        "id": "E5VIlReyaz",
        "forum": "4uaogMQgNL",
        "replyto": "4uaogMQgNL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_rcbB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_rcbB"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce UpFusion, a view synthesis method derived from a collection of unposed images. The core design philosophy of UpFusion centers around the use of a Scene Representation Transformer, combined with a Diffusion Model to infuse intricate object details. Subsequently, instance-specific neural representations are introduced to achieve 3D-consistent rendering outcomes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Scene Representation Transformer (SRT) is renowned as an effective neural renderer that can seamlessly generalize to the rendering of novel scenes and views. However, the SRT represents an image in the latent space, often resulting in blurry rendered outcomes, as evidenced in Fig 2 of this paper. UpFusion examines the constraints in SRT and suggests employing a following diffusion model and an instance-specific 3D representation to enrich the details. Specifically, the denoising diffusion model and a control net branch are employed to master a generative model for novel views of an object, and the instance-specific representation adheres to the paradigm in Score Distillation Sampling to extract a consistent 3D representation."
            },
            "weaknesses": {
                "value": "- The reviewer recommends that the authors emphasize the object-level NVS configuration in the title since the methodology chiefly addresses \"objects\" and the experiments were executed on the \"CO3D\" dataset.\n- Object-level 3D generation (sourced from unposed images or a single image) remains a hot research topic. There exists a plethora of related papers [1,2,3, 4]. However, pivotal experimental comparisons with [1,2,3,5] are missing. Notably, single-view based NVS can ignore the requirment for camera poses: [1,2,3] all necessitate an object-specific representation, while [5] solely requires a forward-pass for view generation.\n- What are the specifics regarding the training time for each instance? Considering [4] also employs a 3D representation to tackle a similar scenario, but does not incorporate the SRT and diffusion model, it's useful for the authors to showcase the merits of solely leveraging a 3D representation. Further, comparisons excluding the SRT/Diffusion model or contrasting it against [4] would be insightful.\n- The CO3D dataset is characterized by various backgrounds, yet the authors omit the background modeling in the manuscript (possibly using the masks in CO3D). Even though object-level NVS publications typically sidestep background modeling, it's essential to accentuate this specific operations in the experimental framework.\n- In terms of the claim 3D consistent generation, it would be useful if the authors could provide diverse rendered video of different objects, as well as producing metrics for your claim.\n\n[1] One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization  \n[2] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior  \n[3] NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors  \n[4] Few-View Object Reconstruction with Unknown Categories and Camera Poses  \n[5] Zero-1-to-3: Zero-shot One Image to 3D Object"
            },
            "questions": {
                "value": "See the recommendated experiments in **Weaknesses** ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774655569,
        "cdate": 1698774655569,
        "tmdate": 1699636379871,
        "mdate": 1699636379871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v0KYX061c2",
        "forum": "4uaogMQgNL",
        "replyto": "4uaogMQgNL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_UGEs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_UGEs"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on view synthesis from unposed images. Scene Representation Transformer, diffusion model, and controlnet branch are utilized to effectively perform the task on object-level novel view synthesis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "SRT + Diffusion are adopted to study the challenging task of novel view synthesis from unposed images."
            },
            "weaknesses": {
                "value": "The experiments are conducted on object-level generation. As the authors also mentioned in the related works section, single view image-to-3d is highly related to the task UpFusion trying to solve. Single view input can also be considered as input image without pose. As a result, I believe the contributions of UpFusion can be better justified when comparing to existing single view novel view synthesis works, for example [1].\nBesides, a couple of references are missing [2] (also on Co3D dataset), [3][4] (SDS-based).\n\n[1] Zero-1-to-3: Zero-shot one image to 3d object\n\n[2] NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors\n\n[3] RealFusion: 360\u00b0 Reconstruction of Any Object from a Single Image\n\n[4] NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360 views"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817779939,
        "cdate": 1698817779939,
        "tmdate": 1699636379802,
        "mdate": 1699636379802,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EgdZetBGJt",
        "forum": "4uaogMQgNL",
        "replyto": "4uaogMQgNL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_yMjS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_yMjS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework for 3D-aware novel view synthesis given sparse 2D views without camera poses. Leveraging the features of the 2D views from an UpSRT encoder-decoder, it predicts a view-aligned spatial feature for the target view and a set-invariant feature, and feeds them to Stable Diffusion as conditioning inputs to generate the novel views. Moreover, it also incorporates an underlying 3D representation with NeRF to further enforce view consistency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper combines view-aligned spatial features and set-invariant features from unposed 2D views and leverages in novel-view diffusions, which sounds natural.\n- Both qualitative and quantitative results show that the proposed framework outperforms the prior works not integrating these modules."
            },
            "weaknesses": {
                "value": "- Experimental comparisons with existing works seem limited. The following works are also related and could be discussed in the paper: [1,2,3,4,5]. Several of these works, as well as the single-/few-view NeRF synthesis works mentioned in the related work section, can be compared with the proposed method experimentally. Specifically, the single-view works also don't rely on relative poses, though the setup is not exactly the same as this paper, they can still be compared.\n- Quantitatively, the UpFusion 3D model has much better numbers than the 2D model, but visually it loses a lot of geometric details compared to the 2D results. Is it limited by the representation power of the 3D NeRF? Or is it because the learned features are not very view-consistent?\n\n[1] Ye, Yufei, Shubham Tulsiani, and Abhinav Gupta. \"Shelf-supervised mesh prediction in the wild.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n[2] Deng, Congyue, et al. \"Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n[3] Tang, Junshu, et al. \"Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior.\" arXiv preprint arXiv:2303.14184 (2023).\n[4] Liu, Minghua, et al. \"One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization.\" arXiv preprint arXiv:2306.16928 (2023)."
            },
            "questions": {
                "value": "- Comparing the 2D and the 3D UpFusion model, I understand that the PSNR and SSIM of the 3D model are better, but why is the LPIPS also better? -- LPIPS is not a pixel-aligned metric, while visually the 2D results look cleaner and have much more details than the 3D ones.\n- It seems that the generated views sometimes have inconsistent colors as the input view (e.g. the blue bench and the blue umbrella in Fig. 8, Appendix A). Is there any explanation for this?\n- I wonder how the proposed method compares to this baseline: first running COLMAP to estimate the relative poses of the input views, and then running a pose-dependent 3D synthesis method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698880629537,
        "cdate": 1698880629537,
        "tmdate": 1699636379728,
        "mdate": 1699636379728,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NSt7mTGJBa",
        "forum": "4uaogMQgNL",
        "replyto": "4uaogMQgNL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
        ],
        "content": {
            "summary": {
                "value": "This paper presents UpFusion, a system that can generate novel views from a sparse set of uncalibrated multi-view images. Technically, UpFusion consists of two parts: 1) the first part is a modified UpSRT which encodes unposed images into a set representation and renders the feature maps for the target view, 2) while the second part is a diffusion-based ControlNet, which generates the novel view conditioned on the set representation and the decoded feature map.\nDuring training stage, UpSRT and ControlNet are optimized separately."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is well-written and well-structured. The problem setting is both innovative and ambitious, as it seeks to address two issues of considerable interest within the research community: 1) reconstruction from sparse views and 2) generation from unposed images, simultaneously.\n\n+ The proposed method is intuitive, and this paper presents specific details and dedicated designs that are well-suited for the challenges inherent to the problem under investigation.\n\n+ The empirical results demonstrate the method's remarkable performance in generating novel views from a few-shot unposed images when compared to the baseline approaches."
            },
            "weaknesses": {
                "value": "- The problem formulation lacks clarity. Without specifying poses from images, it becomes ambiguous to define the pose of a target view unless canonical poses are provided. However, canonicalization necessitates per-category calibration, and it has been observed that such methods are specific to certain categories. To further validate the effectiveness of the approach, it is recommended to test the pre-trained UpFusion on additional data domains, such as Blender, LLFF, or Shiny datasets [1].\n\n- Empirical comparisons are insufficient in relevant baseline models. For the task of novel view synthesis, it is advisable to include comparisons with end-to-end pose optimization baselines, such as BARF [2] and NoPe-NeRF [3]. Since this paper asserts novel view generation from sparse views using diffusion, it would also be equitable to compare with state-of-the-art single-image-to-3D baselines such as Zero-123 [4].\n\n- From a technical perspective, despite notable engineering efforts, the proposed method appears to be a combination of existing methods: SRT, ControlNet, and DreamFusion. It also structurally resembles GeNVS [5].\n\n- The paper lacks a discussion and comparison with some relevant prior work, particularly with references [6] and [7].\n\n[1] Wizadwongsa et al.NeX: Real-time View Synthesis with Neural Basis Expansion\n\n[2] Lin et al., BARF: Bundle-Adjusting Neural Radiance Fields \n\n[3] Bian et al., NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior\n\n[4] Liu et al., Zero-1-to-3: Zero-shot One Image to 3D Object\n\n[5] Chan et al., GeNVS: Generative Novel View Synthesis with 3D-Aware Diffusion Models\n\n[6] Smith et al., FlowCam: Training Generalizable 3D Radiance Fields without Camera Poses via Pixel-Aligned Scene Flow\n\n[7] Fu et al., MonoNeRF: Learning Generalizable NeRFs from Monocular Videos without Camera Poses"
            },
            "questions": {
                "value": "1. The evaluation scheme proposed in Sec. 4.1.2 is designed to mitigate pose ambiguity. However, the enforcement of per-view alignment may introduce more confusion in the evaluation results, making it challenging to assess whether the proposed method can accurately generate views at specified camera poses and maintain smooth views along a camera trajectory. To enhance the evaluation methodology, it is recommended that the authors consider implementing a global alignment across all views collectively, rather than performing alignment on a per-frame basis.\n\n2. Could the authors provide insights into the motivation behind making the specific modifications to UpSRT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4145/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4145/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4145/Reviewer_Egt9"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698914277711,
        "cdate": 1698914277711,
        "tmdate": 1699636379663,
        "mdate": 1699636379663,
        "license": "CC BY 4.0",
        "version": 2
    }
]