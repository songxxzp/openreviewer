[
    {
        "id": "IGZLiCCDRN",
        "forum": "g1fE8fSOm5",
        "replyto": "g1fE8fSOm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_Q462"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_Q462"
        ],
        "content": {
            "summary": {
                "value": "This paper studies why GNNs work from the perspective of feature learning theory, which is proposed by recent work to analyze why CNNs work. This paper identifies the convergence conditions for GNNs under the defined specific data generation model, i.e., SNM-SBM. Overall, this work justifies why GNNs can achieve better generalization ability than CNNs on graph data from the perspective of feature learning theory."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation for analyzing why GNNs work theoretically is important and potentially inspiring for better GNN designs.\n\n2. The formulation of the feature learning theory in GNNs is grounded and well-presented.\n\n3. The simple simulation experiments are clearly shown."
            },
            "weaknesses": {
                "value": "1. (I am not an expert in deep learning theory, so my assessment of this might be less confident.) The novelty of the proposed method seems to be limited. According to my preliminary check, the main theory and proof of this work follow the prior feature learning theory paper closely [1]. It seems to be directly adapted from [1]. Could the authors summarize the main difference/contributions of this submission, compared to [1]?\n\n2. The main results of why GNNs work are not surprising and not adding new insights to the community. It is well-known that GNNs can help smooth the node features corresponding to the same class, thus making the classification tasks easier [2]. In this sense, the results from this paper do not add new insights or provide potential future topics in the field.\n\n3. The experimental results are also widely observed by previous work in the community.\n\n[1] Cao, Yuan, et al. \"Benign overfitting in two-layer convolutional neural networks.\" Advances in neural information processing systems 35 (2022): 25237-25250.\n\n[2] Li, Qimai, Zhichao Han, and Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 32. No. 1. 2018."
            },
            "questions": {
                "value": "N/A\n\nTypo: \"by\" above Eq. (3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698183222417,
        "cdate": 1698183222417,
        "tmdate": 1699636377330,
        "mdate": 1699636377330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uPvh1is9F3",
        "forum": "g1fE8fSOm5",
        "replyto": "g1fE8fSOm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_DNKm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_DNKm"
        ],
        "content": {
            "summary": {
                "value": "The paper compares GCN to MLP/CNN when learning on graph data. The paper sets out to show the benefits of using the graph structure in GCNs vs learning a MLP on the node features with no diffusion of features along the edges (which is called CNN in this paper). This is done by proposing a simple generative model for graphs and considering a two layer GCN model, providing exact formulas for gradient descent training, and proving theorems about the accuracy of the trained model. The results of this GCN model are compared to equivalent results of a MLP/CNN model. This way, GCN is shown theoretically to be superior to MLPs/CNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper treats all aspects of learning with regard to the proposed model. Namely, they derive a formula for the training process and for the resulting accuracy. Hence, the paper provides a complete theoretical analysis, which rigorously proves the benefits of GCN over CNNs (for the proposed data model). The paper is clearly written."
            },
            "weaknesses": {
                "value": "The model of the data is very specific and limited, even for a two class model. The two classes are modeled as some template signal and its negative, without noise that directly affects the signal, e.g., additive noise. The noise is supported on indices which are disjoint from the signal. It is ok to consider simplistic models when doing mathematical analysis, but this should be clear for the reader. The fact that the model is simplistic should be clarified better in the text. The motivation for choosing such a simplistic model can be explained better. Namely, to be able to clearly decouple the signal learning aspect from the noise memorization aspect of learning."
            },
            "questions": {
                "value": "In section 3.3, why do you call this model a CNN and not a MLP? You apply a MLP on each node separately, and there is no diffusion/mixing between the features of the different nodes. It is only equivalent to a 1x1 standard CNN. I would call it a MLP that treats each node as a separate data point.\n\nPage 8, Verification via real-world data: In your model, the two features come from one template with positive and negative sign, while in the experiment you have two features that come from two different templates. Can you explain in the paper how the experiment differs from the theoretical setting (if I am not missing something)? Can you run an experiment fully modeled as the proposed data model?\n\nSince the experiment synthetically builds graph data that corresponds to the proposed data model, it does not show that the proposed model can describe real data. Is there some real data that can be approximated by your model? Namely, can you fit the parameters of the model to data, and check its accuracy/ability to represent real data? Or is there no real data that corresponds to your mode?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765223140,
        "cdate": 1698765223140,
        "tmdate": 1699636377248,
        "mdate": 1699636377248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VXwxEISjPy",
        "forum": "g1fE8fSOm5",
        "replyto": "g1fE8fSOm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_9JJs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_9JJs"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel angle to view the training set and generalization ability of GNNs compared to CNNs. It is based on a data model that combines both the signal-to-noise for the node feature part and the stochastic block model for the graph structure. For the analysis, it introduces a signal-to-noise ratio quantity to better describe the shape of the dataset, which is closer to the utility of GNNs. Experiments on simulation datasets coincide with the theoretical analysis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The whole paper is well-structured and problem is well formulated, especially both the training phases and generalization ability are discussed.\n- Detailed and quantitative explanation are entailed along with the arguments of this paper, also the explanation of the numbers are helpful in understanding such proofs with practical impressions.\n- Proof stretches are friendly to readers."
            },
            "weaknesses": {
                "value": "- I fully agree that it is a theoretical paper that sufficient assumptions are necessary, while the node feature part is somehow far from the reality, as there are no such golden methods to divide any feature into such two orthogonal groups. Therefore, more connection of the applicability of this node feature model to the real-world setting is demanded, e.g. by showing some repression/clustering results of some real-world node features on such two orthogonal part, which is not required to be identifiable.\n- Homophily is considered to be a key factor for GNNs, especially unavoidable for those analytical works based on SBM, where the interclass and intraclass connections are explicitly modeled. However, this paper does not include such a discussion, but rather focuses on the distribution of the node features.\n- For Figure 3 and 4, it is better to provide variance bars by repeating the experiments of several times to show the stability of the results."
            },
            "questions": {
                "value": "- For the signal-noise model of the data model part, $x^{(1)}$ means the most ``informative'' node features. And does it imply that a binary classification setting according to $y$ is sampled from $\\{-1,1\\}.$ Is it possible that some parts of the conclusion of the paper can be influenced in a multi-classification setting, or just for the sake of proof?\n- Should the subsection of 3.4 be part of 3.3? This is a minor issue. Another small suggestion is to standardize the use of the names MLP or CNN.\n- For verification on real data, how is the graph of points 1 and 2 of MNIST constructed? By the same possibility of interclass and intraclass connections?\n- It would be greatly appreciated if the authors could provide some intuition behind the dimensionality of the two parts of the node features in the data model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Reviewer_9JJs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797101922,
        "cdate": 1698797101922,
        "tmdate": 1699636377176,
        "mdate": 1699636377176,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e1jZPVXFGv",
        "forum": "g1fE8fSOm5",
        "replyto": "g1fE8fSOm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_UPB7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_UPB7"
        ],
        "content": {
            "summary": {
                "value": "This study aims to address the knowledge gap by investigating the role of graph convolution in feature learning theory under a specific data generative model. They conduct a comparative analysis of optimization and generalization between two-layer graph convolutional networks (GCNs) and their convolutional neural network (CNN) counterparts. They indicate that graph convolution significantly improves the range of low test errors compared to CNNs. This highlights a significant discrepancy in generalization capacity between GNNs and MLPs, a conclusion further supported by our empirical simulations on both synthetic and real-world datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Several theoretical results about two-layer GCNs are obtained to compare with CNNs."
            },
            "weaknesses": {
                "value": "The practical significance of the theoretical results is unclear."
            },
            "questions": {
                "value": "1. I am confused with the significance of the theoretical results provided in this work. What's the purpose? Guide to improve GCN's training efficiency? or explain why GCNs better than CNNs?\n\n2. The main difference between GCNs and CNNs is the introduction of graph structure information, such as adjacency matrix A and Laplacian matrix L, where the theoretical results in this work have not entirely reflected the structure information (A or L), please explain this in detail."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4120/Reviewer_UPB7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833035044,
        "cdate": 1698833035044,
        "tmdate": 1699636377096,
        "mdate": 1699636377096,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kyiNeiNxKi",
        "forum": "g1fE8fSOm5",
        "replyto": "g1fE8fSOm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_LXCw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4120/Reviewer_LXCw"
        ],
        "content": {
            "summary": {
                "value": "The paper provides grounded theoretical exploration to analyze the characteristics, effectiveness, and superbness of GCNs (compared with other common modules), revealing the role that graph convolution plays. It has several interesting findings, especially the ones about test erros."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(placeholder for future edit, please allow me extra hours to finish the writing.)"
            },
            "weaknesses": {
                "value": "(placeholder for future edit, please allow me extra hours to finish the writing.)"
            },
            "questions": {
                "value": "(placeholder for future edit, please allow me extra hours to finish the writing.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4120/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839780792,
        "cdate": 1698839780792,
        "tmdate": 1699636376935,
        "mdate": 1699636376935,
        "license": "CC BY 4.0",
        "version": 2
    }
]