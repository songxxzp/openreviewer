[
    {
        "id": "audVAO2MYt",
        "forum": "qgLyKwXVDs",
        "replyto": "qgLyKwXVDs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_hgDz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_hgDz"
        ],
        "content": {
            "summary": {
                "value": "The paper propose FreeLM as a finetuning-free method for downstream tasks. Compared with pretraining then fine-tuning paradigm, FreeLM fuses the task data into pretraining data and is iteratively trained in the pretraining stage. On several classification tasks, FreeLM is shown to be better than much larger GPT-3."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Injecting task bias into pretraining stage is an interesting idea.\n- The paper is mostly well written, with intensive experiments and analysis on various aspects of the proposed method."
            },
            "weaknesses": {
                "value": "- **Need more in-depth analysis regarding the influence on NLG tasks**: As the author mentioned that FreeLM has a higher perplexity than GPT-2, it would be more informative to provide a numerical comparison on NLG benchmarks instead of only showing a few cases.\n- **Missing comparison with multitask fine-tuning**: Table 4 does not include the results of the multitask fine-tuning (https://openreview.net/forum?id=gEZrGCozdqR) baseline, which is also learning-free when applied to other tasks. It would be important to compare with multitask fine-tuning results and analyze the generalization ability compared with FreeLM.\n- **Scalability**: In-context learning is another learning-free method when performing downstream tasks and the performance of which gains a lot after scaling. However, there is a lack of comparison with in-context learning when model scales up."
            },
            "questions": {
                "value": "- Can FreeLM benefit broader tasks (e.g., language generation, code generation) beyond binary classification?\n- How about the performance and generalization ability compared with multitask fine-tuning?\n- How about the FreeLM performance compared with in-context learning after model scaling?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698461980617,
        "cdate": 1698461980617,
        "tmdate": 1699636459678,
        "mdate": 1699636459678,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VzJgpLCE7q",
        "forum": "qgLyKwXVDs",
        "replyto": "qgLyKwXVDs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_NrTr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_NrTr"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce a novel approach to pretrain a decode-only language model, eliminating the need for subsequent fine-tuning. Drawing on prompt-based learning principles, their model, FreeLM, excels in a variety of downstream tasks. Impressively, it surpasses the performance of larger models like GPT-3 and InstructGPT on the GLUE benchmark, despite having substantially fewer parameters."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. By merging pretraining and finetuning into one unified stage, the authors present a potentially more efficient training approach.\n \n2. Remarkably, despite its smaller size, FreeLM surpasses larger models like GPT-3 and InstructGPT in GLUE's downstream tasks."
            },
            "weaknesses": {
                "value": "1. While the \"teacher iterative\" method is an interesting take on prompt-based learning, merging pretraining and prompt-finetuning into a single step may not be as novel as it seems. A clear distinction and justification for this fusion could strengthen the paper.\n\n2. The choice of the GLUE benchmark for evaluation does not make sense to me for a few reasons:\n\n    1. GLUE is typically used to assess single-task, supervised learning models, mainly encoder-only (BERT-like) or sometimes encoder-decoder LMs (T5-like). However, decoder-only LMs, like FreeLM, may not perform as well on these tasks. For instance, decoder-only LMs generally score around 80 on MNLI, while state-of-the-art encoder-only models can achieve scores over 90. Even BERT-Base can achieve an accuracy of 84.\n  \n    2. The paper's approach to evaluating \"generalization to unseen datasets\" does not follow standard practices. A more established benchmark for zero-shot generalization of LMs on classification tasks is T0-Eval (https://arxiv.org/pdf/2110.08207.pdf), which includes 62 training datasets and 12 evaluation datasets.\n\n    3. It is more appropriate to assess decoder-only LLMs on generative tasks, such as MathQA, HumanEval, MBPP, or OpenBookQA, rather than GLUE.\n\n3. The comparison between FreeLM, GPT-3, and InstructGPT may not be entirely fair. A better approach would be to create a pretraining-finetuning baseline or to fine-tune an open-source LM like LLaMa on the GLUE benchmark for comparison.\n\n4. Lastly, the paper's writing could be improved for clarity and readability. The language should be fluent and succinct, rather than coming across as if it was translated from another language."
            },
            "questions": {
                "value": "1. Can you please provide a clearer distinction between the proposed \"teacher iterative\" method and existing prompt-based learning methods? What specifically sets your approach apart, given that both methods seem to be simply a mix of pretraining and fine-tuning?\n\n2. Why was GLUE chosen as the primary benchmark for evaluation, especially given that it is generally used for single-task, supervised learning models and not typically suited for decoder-only language models like FreeLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698516171327,
        "cdate": 1698516171327,
        "tmdate": 1699636459580,
        "mdate": 1699636459580,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hBqORcV4Xi",
        "forum": "qgLyKwXVDs",
        "replyto": "qgLyKwXVDs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_JFLv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_JFLv"
        ],
        "content": {
            "summary": {
                "value": "The paper unifies the NLP tasks and proposes a fine-tuning-free (though not training-free) method for training a language model, claiming better performance than GPT-3 on various tasks (a point I view with skepticism)."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper unified the NLP tasks, which could be helpful for research to evaluate LLM (but I believe [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) already did a good job now)\n- I like the figures."
            },
            "weaknesses": {
                "value": "Minor Weakness:\n- I take issue with the term \"fine-tuning-free\" for the language model, as it suggests a misconception that the proposed method requires no training. I acknowledge that the community lacks a clear definition of \"fine-tuning,\" commonly understood as \"continuous training\" on a pre-trained model. Nonetheless, the proposed language model also undergoes continuous training based on GPT-2, contradicting the \"fine-tuning-free\" label. Furthermore, even if you are starting training from scratch, calling it a \"fine-tuning-free\" language model is misleading if it indeed involves parameter updates.\n\nMajor Weaknesses:\n- The paper uses many terms to embellish the method, such as \"teacher signal\" and \"language signal,\" but in reality, the approach is a \"co-training\" scheme that trains on both target tasks and unsupervised English data. The paper argues that \"pre-training and fine-tuning is not an optimal paradigm\" and suggests \"training them together.\" I do not recognize the benefits here. Is it to prevent catastrophic forgetting? If that's the case, it requires verification within the paper. Overall, this paper lacks novelty and fails to demonstrate the effectiveness of the proposed method.\n- I assume one of the motivations behind \"co-training\" is to enhance generation ability. However, the authors did not substantiate this with perplexity scores. Some examples are unconvincing. More evidence is needed to support it, and if other methods are considered unreliable, human evaluation is required.\n- The paper asserts that their proposed LM achieved SOTA, surpassing GPT-3, which is not accurate. Even straightforward fine-tuning on these tasks with GPT-2 outperforms Free-LM. Would it then be plausible for the authors to assert that \"fine-tuning GPT-2 on these target data achieves SOTA and is superior to GPT-3\"? The authors should explicitly state the limitations in this section. By the way, the authors also need to show details how they design the prompt for GPT-3, an improper prompt also has big impact on the performance."
            },
            "questions": {
                "value": "The paper lacks novelty, fails to demonstrate the soundness of the proposed method, and makes overstatements in its evaluation, which also lacks details when they evaluate baselines. Hence, I strongly recommend rejecting this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698620743590,
        "cdate": 1698620743590,
        "tmdate": 1699636459498,
        "mdate": 1699636459498,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DLynXqH6Tc",
        "forum": "qgLyKwXVDs",
        "replyto": "qgLyKwXVDs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_nxBg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4774/Reviewer_nxBg"
        ],
        "content": {
            "summary": {
                "value": "The paper studies a special method that mixes various supervised finetuning (SFT) data together with the pretraining data for language models. This special method requires fitting existing SFT data from various tasks into a new binary classification format, where the supervision can be added as an extra term in the loss function. With this approach the pretraining and modified finetuning is conducted in iterations. \n\nThe authors claimed in the abstract that their method is \"finetuning free\" and a small GPT2 sized model trained with this approach \"out performs large models e.g. GPT-3 and InstructGPT\"."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The original motivation of this work is on the right general direction. It is important to study methods that can improve a general purpose LLM beyond what is possible with pretraining along, leveraging smaller but high quality datasets. Many fruitful methods has been proven along these lines, such as FLAN, Instruction finetuning, self-instruct, RLHF, etc. \n\nThe paper also studies whether the auto-regressive language modeling abilities has been impacted by adding SFT signals, and find that with their method this impact has been minimal. This is an encouraging sign.\n\nThe paper has set out to study cross-task generalizations, which is another important topic. They find some interesting generalization effect on COPA."
            },
            "weaknesses": {
                "value": "Although the paper has many good motivations, it also has serious flaws.\n\n1) The central claim is miss leading. They claims that their method is \"finetuning free\" and they compare their results to zero-shot and few-shot results. However, this is not the case. In particular, they still require gradient steps on the training split data of their eval tasks (or of a similar eval task). This is effectively finetuning, albeit with a slightly different loss function. In particular, using their method does not alleviate the user from collecting the finetuning set, which is importantly not the case for typical zero-shot and few-shot settings with a foundation LLM. Thus the claims involving comparison to GPT-3 and InstructGPT are unfair because those models did not explicitly have access to those finetuning training sets.\n\n2) The proposed method lacks novelty. It is essentially doing multi-task finetuning with the pretraining task mixed in. They did not show whether doing the same scheduling mixing pretraining tasks with vanilla finetuning tasks would have a similar effect, or whether multi-task parallel training with both the pretraining task and the vanilla finetuning tasks would have a similar effect. This is a very important baseline that is completely missing. It is possible that a much simpler approach - simply mixing in some pretraining data/task at finetuning stage would have all the same effect.\n\n3) The proposed formulation of the \"unified data\" seems narrow and classification heavy. It may not capture well generation heavy LLM use cases such as \"write me a poem\" that are very important and popular these days.\n\n4) The evals are too narrow. For comparison with GPT3 and InstructGPT, the author should test few-shot and zero-shot performances on unseen tasks."
            },
            "questions": {
                "value": "1, Could the author please compare the proposed method to the following methods?\n    1.1 Use the vanilla finetuning objective but the same scheduling of pretraining and finetuning, with and without reformatting proposed by the author. \n    1.2 Use a mixture of the vanilla finetuning objective and the pretraining objective, with and without reformatting proposed by the author. \n\n2, Could the author please focus the eval on out-of-domain tasks where the training split, or a training split of a similar task, is not available to the model, preferably in a few-shot / zero-shot setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4774/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802526474,
        "cdate": 1698802526474,
        "tmdate": 1699636459405,
        "mdate": 1699636459405,
        "license": "CC BY 4.0",
        "version": 2
    }
]