[
    {
        "id": "GGg7CqEjtS",
        "forum": "o7x0XVlCpX",
        "replyto": "o7x0XVlCpX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_VxR9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_VxR9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a multi-modal approach for image completion with LARGE missing regions. The different modalities, such as depth, edge, sketch, pose, provide complementary information for plausible completion. The approach does not require training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Dealing with LARGE missing regions is a critical task in image completion. This topic is of broad interest in the ML and image processing community.\n2. The idea of leveraging multiple resources is nice though not ground-breaking novel. Making is scalable and flexible is the key, which is solved by two stage approach: modality oriented conditional network and across-modality blending.\n3. The approach is integrated into the diffusion process neatly and training-free.\n3. The paper is very well written and easy to follow, with good illustrations. \n4. The results are convincing with well-planned experiments, which also demonstrate good image generation results beyond completion"
            },
            "weaknesses": {
                "value": "1. I'm not fully convinced that different image channels/features, such as depth, sketch, edge, could be called modality.\n2. The fair comparison is not easy since most SOTA are not considering multiple resources in the same time. It'd be nice to share some insight into this, and share failure cases."
            },
            "questions": {
                "value": "As in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791759848,
        "cdate": 1698791759848,
        "tmdate": 1699636457727,
        "mdate": 1699636457727,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vLkE1vQGhl",
        "forum": "o7x0XVlCpX",
        "replyto": "o7x0XVlCpX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg"
        ],
        "content": {
            "summary": {
                "value": "MaGIC or Multi-modality Guided Image Completion can merge text, canny edge, sketch, segmentation, depth, pose, or any arbitrary combination as guidance for image completion. The authors aim to design a framework that is scalable and flexible. MaGIC has two core components -- a modality-specific U-Net (MCU-Net), and a consistency modality blending (CMB). The MCU-Net will be individually fine-tuned under each single modality, in the first stage. Then, to achieve multi-modality guidance, the CMB algorithm flexibly aggregates guidance signals from any combination of previously learned MCU-Nets. The MCU-Net is similar to T2I-Adapter, composed of a standard U-Net denoiser from the pre-trained Stable Diffusion (SD) and an encoding network which injects a single modality guidance into the U-Net to attain single-modality guidance. The CMB leverages guidance loss to gradually narrow the distances between intermediate features from SD pre-trained U-Net and multiple MCU-Nets during denoising sample stage. This ensures that the SD U-Net features do not deviate too much from the original feature distribution during multi-modality guidance. CMB is training-free and allows for the flexible addition or removal of guidance modalities, avoiding cumbersome re-training and preserving the feature distribution of the original SD U-Net denoiser.\n\nTo verify MaGIC, the authors conduct experiments on image inpainting, outpainting, and editing using the COCO, Places2, and in-the-wild data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I like the extension of classifier guidance to multiple modalities that too training-free. Similar techniques has been explored in other single-modality context like in Sketch-Guided Text-to-Image Diffusion Models, but extending to multi-modal case is a nice extension.\n\nThe qualitative comparisons are very intuitive (especially with T2I-Adapter and ControlNet). The overall presentation is reasonable and easy to follow.\n\nThe authors included substantial appendix sections, detailing several architectural details like the design of MCU-Net."
            },
            "weaknesses": {
                "value": "While this is an interesting piece of work, I have some big gripes (please let me know if I understood it wrong):\n\nIn related work section (page 3 last paragraph), the authors give the impression that T2I-Adapter (and ControlNet) \"fails to simultaneously use multi-modality as guidance\". This is completely wrong. I understand that T2I-Adapter do not explicitly train jointly for multiple modalities, but they can combine multiple modalities (see section 4.3.2 in T2I-Adapter).\n\nSecond, the authors need to provide a solid justification why a simple \"feature-level addition\" mentioned in Page-6 paragraph-2 is not good. T2I-Adapter (in broad terms) does exactly that. I would need more detailed experiment and intuition comparing both T2I-Adapter and ControlNet for multi-modality case.\n\nThe argument given by authors \"denoiser is trained solely on the distribution of $$\\hat{F}_{c} = F_{enc} + F_{c}$$ \" makes even less sense when you consider ControlNet with its zero-convolutions. In ControlNet we get plausible generation from the starting iterations, thanks to zero-convolutions and with training the conditioning branch becomes good. Hence, the distribution mismatch should not really be an issue to begin with.\n\nApart from my major concerns above, there are some minor corrections/concerns:\n1. I think Eq. 4 should be $$ l (\\hat{F}_{C}, F_{*}) = \\frac{1}_{L} \\sum_{l=0}^{L} \\sum_{i=1}^{N} \\delta_{c} || \\hat{F}_{c}^{l} - F_{*}^{l} ||_{2}^{2} $$ (Note: subscript is C and not c?)\n\n2. Typos e.g., 2nd last paragraph just after equation 2 \"Denoising\"\n\n3. The MCU-Net is basically T2I-Adapter. I do not see any reason to have a new name for it (only to rebrand something and create more confusion). On the other hand, given MCU-Net is same as T2I-Adapter, the only merit of this paper is CMB."
            },
            "questions": {
                "value": "Since the only contribution of this paper is CMB, I would suggest to have a very detailed comparison with respect to T2I-Adapter, ControlNet, and many more (for multiple modalities).\n\nApart from just a few qualitative results and some incremental metrics improvement, why do you think Converse Amplification (or simply a variant of classifier guidance) a better approach than zero-convolutions with ControlNet?\n\nAlso, can you add some failure cases of CMB? This is important to give a better idea of where ControlNet lacks and where CMB lacks (I understand CMB can be coupled with ControlNet or T2I-Adapter)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4753/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg",
                    "ICLR.cc/2024/Conference/Submission4753/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900493722,
        "cdate": 1698900493722,
        "tmdate": 1700739686197,
        "mdate": 1700739686197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OLfDpGaEy0",
        "forum": "o7x0XVlCpX",
        "replyto": "o7x0XVlCpX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
        ],
        "content": {
            "summary": {
                "value": "The paper MaGIC: Multi-modality Guided Image Completion introduces a novel framework for image completion that supports various guidance modalities, such as text, edge, sketch, and more. The proposed method, MaGIC, enables flexible and scalable multi-modality guidance without the need for retraining the model. The paper demonstrates consistent improvements in image quality over existing approaches."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Innovative and Flexible Approach** The paper addresses the challenging problem of multi-modality-guided image completion. It proposes a new simple training-free procedure, allowing for various guidance modalities, such as text, edge, sketch, segmentation, depth, and pose. \n\n** Large Consistent Gains** The paper shows consistent and significant improvements over state-of-the-art approaches, particularly in image quality."
            },
            "weaknesses": {
                "value": "**Clarity and Typos** The paper is challenging to follow and contains multiple typos, which can impede understanding. Improved clarity in the presentation and thorough proofreading would enhance the paper's quality.\n\n**Non-standard Update Scheme** The update scheme presented in equation (5) appears inhomogeneous, as it involves gradient descent with respect to $z_t$ but updates $z'_{t-1}$. This choice could be a reasonable heuristic but is not discussed or justified, which leaves questions about its validity.\n\n**Lack of Quantitative Evaluation** The paper only qualitative results without quantitative evaluation metrics when compared with recent baselines such as ControlNet and T2I-adapter. In particular, the performances with respect to ControlNet should be carefully assessed.  \n\n**Inadequate Dataset and Modality Descriptions** The datasets used and the conditioning modalities are briefly presented. A more detailed description of the datasets, along with the rationale for their selection, would be beneficial. \n\n**Missing Ablations** A more in-depth exploration of the impact of the weights $\\delta_c$ in equation (4) would offer valuable insights. The stability of these parameters is critical as their tuning could rapidly be cumbersome.\n\n**Inconsistent Results** The results in Table 3.b appear to be inconsistent, with FID scores not following the expected pattern. This raises questions about the efficiency of the CMB method and its need for complex hyperparameter tuning. In particular, for a fixed P=30, FID(Q=1)>FID(Q=10)>FID(Q=5)."
            },
            "questions": {
                "value": "I wonder why the authors did not provide CLIP score evaluation as well as reconstruction performances."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4753/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF",
                    "ICLR.cc/2024/Conference/Submission4753/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699374216933,
        "cdate": 1699374216933,
        "tmdate": 1700737478197,
        "mdate": 1700737478197,
        "license": "CC BY 4.0",
        "version": 2
    }
]