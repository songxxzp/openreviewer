[
    {
        "id": "jaAguLQsSL",
        "forum": "mzyZ4wzKlM",
        "replyto": "mzyZ4wzKlM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_eo5p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_eo5p"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a scheme for convexly combining verified and adversarial loss functions to train verifiably robust models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper is generally well written and presented.\n\n2. The contribution is generally clear.\n\n3. The idea of expressive loss functions as interpolating between verified and adversarial losses is interesting."
            },
            "weaknesses": {
                "value": "1. My main concern is that the paper is not sufficiently novel to merit publication. Convexly combining loss functions is a rather obvious idea; indeed (8) is just a linear combination of two loss functions, something which has been around for ages."
            },
            "questions": {
                "value": "1. In table 3, the alpha parameter for the MTL-IBP method can get very low (e.g., $4 \\cdot 10^{-3}$ for CIFAR-10 $2/255$). Does this not mean that the loss essentially just reduces to the adversarial loss?\n\n2. Why do the optimal $alpha$'s vary so much between the $2/255$ and $8/255$ epsilons for CIFAR-10? As in Q1, the MTL-IBP loss boils down to just the adversarial loss for $2/255$, and changes to a $50/50$ split for $8/255$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Reviewer_eo5p"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697675526151,
        "cdate": 1697675526151,
        "tmdate": 1700673116627,
        "mdate": 1700673116627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ojHbO9Ut6K",
        "forum": "mzyZ4wzKlM",
        "replyto": "mzyZ4wzKlM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_oQBq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_oQBq"
        ],
        "content": {
            "summary": {
                "value": "The authors hypothesize that expressive loss functions yield better training for verified robustness. A family of loss functions $\\mathcal{L}_\\alpha$ is expressive if\n- $\\mathcal{L}(f(\\theta, x_\\text{adv}), y) \\leq \\mathcal{L}_\\alpha(\\theta, x, y) \\leq \\mathcal{L}_v (f(\\theta,x),y)$ for all $\\alpha \\in [0,1]$, where the left/right inequality become an equality if $\\alpha = 0/1$ respectively. \n- $\\mathcal{L}_\\alpha$ is monotonically increasing with $\\alpha$\n\nThey support their hypothesis that expressive loss functions yield better training for verified robustness by showing that trivial expressive losses obtained via convex combinations between adversarial attacks and IBP bounds yield sota results. They further hypothesize that the notion of expressivity is crucial to get sota and argue as follows:\n- They state that sota verified training algorithms rely on coupling adversarial attacks with over-approximations. They show that SABR is expressive - hence the good performance of SABR is inline with their explanation.\n- Other expressive losses can be trivially designed via convex combinations, i.e.\n\t1. CC-IBP: combine adversarial and over-approximated network outputs with in the loss\n\t2. MTL-IBP: combine adversarial and verified losses\n- Experimental evaluation of CC-IBP and MTL-IBP. Both attain sota, particularly on TinyImageNet and downscaled ImageNet. \nFurther, they analyze the parameter $\\alpha$ governing a robustness-accuracy trade-off. Better approximations of the worst case loss do not necessarily correspond to performance improvements. \n\nThe authors experimentally compare CC-IBP and MTL-IBP to prior work and find that the proposed methods match or outperform the literature. They also study the effect of the over-approximation coefficient on the performance profiles of expressive losses. The take away here is that better approximations of the branch-and-bound loss do not necessarily result in better performance. \n\nObserved that standard accuracy decreases with alpha and verified accuracy increases with alpha. The adversarial and verified robust accuracies unter tighter verifiers may first increase and then decrease with $\\alpha$, hence the need for careful tuning according to the desired robustness-accuracy trade-off. \n\nFinally, the assumption that better approximations of the worst-case loss results in better trade-offs between verified robustness and accuracy is investigated. The parameter $\\alpha$ is chosen based on the performance on a hold out set consisting of 20% of the training set. The worst case loss is approximated using a branch-and-bound loss. The authors report that sometimes it is better for the BaB error to be positive and sometimes for the BaB error to be negative."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation is mostly good.\n- Training certifiable networks is a relevant research problem. \n- The ideas are conceptually simple yet seem to be effective. \n- The work unifies and generalizes successfull approaches. \n- The authors provided code."
            },
            "weaknesses": {
                "value": "- It remains unclear how stable the results are (for example w.r.t. different seeds). \n- Writing could be improved in some parts of the paper, i.e. Section 6.3. \n- It remains unclear what \"tricks\" i.e. for regularization and initialization where specifically used."
            },
            "questions": {
                "value": "- What are the confidence intervals for the results in the paper with respect to different seeds? Are the trends consistent w.r.t. the randomness due to different seeds? \n- What specialized initialization and regularization techniques where used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766543556,
        "cdate": 1698766543556,
        "tmdate": 1699636990843,
        "mdate": 1699636990843,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TF3IVLwKNO",
        "forum": "mzyZ4wzKlM",
        "replyto": "mzyZ4wzKlM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_2HZC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_2HZC"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes \"expressive\" loss functions that interpolate between IBP and adversarial loss in a simple linear combinations and show good empirical performance on a variety of datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The empirical results seem strong, especially considering the fact that the proposed methods are simple interpolations."
            },
            "weaknesses": {
                "value": "The presentation and writing needs a lot of work and it seems the paper is hurriedly written. Specific concerns are below.\n\n\n\uf06e\tThe mathematical definition of property P in Eq 1 is given in section 2 without any discussion of what it means or entails and why is it interesting/useful. \n\n\uf06e\tYou could add atleast one example of how x_adv could possibly be generated in the background section.\n\n\uf06e\tExplicitly write down what \u201cverification\u201d means before using it in section 2.1. I don\u2019t know what the following statement means: \u201cHowever, formal verification methods fail to formally prove their robustness in a feasible time\u201d \n\n\uf06e\t \u201cAs seen from Eq 1, network is provably robust if \u2026logit differences \u2026 all positive\u201d \u2013 why and how before even defining what verification is.\n\n\uf06e\t\u201cIncomplete verifiers will only prove a subset of the properties\u201d \u2013 which properties ? only one is defined. \n\n\uf06e\tThe unlaballed equation with relationship of \\underline{z} and z. I would not write \\underline{z} as an inequation when saying it a lower bound without defining it first say using an example.  \n\n\uf06e\tCan you give an example when o and l are not equal ? the definition of z requires o to be atleast as big as l, and the definition of z also makes sense only if o and l are equal. Is o the size of the output before softmax evaluation or after? I am having a hard time reconciling dimensions of f( ) and z( ) for translation-invariance.\n\n\uf06e\tAre the other methods also grid-searched for best hyperparameters  for their respective methods ?\n\n\uf06e\tThe runtimes are a bit misleading? Does the runtime also include hyperameter search cost including the cost for best interpolating parameter ?"
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699332822314,
        "cdate": 1699332822314,
        "tmdate": 1699636990729,
        "mdate": 1699636990729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6W6sBaVb5J",
        "forum": "mzyZ4wzKlM",
        "replyto": "mzyZ4wzKlM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_tVkU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8030/Reviewer_tVkU"
        ],
        "content": {
            "summary": {
                "value": "This work studies the certified training with over-approximation for robustness certification. Specifically, the authors introduce the idea of expressivity of loss functions and show that it can range from worst-case loss to verified loss, based on which two forms of loss are proposed. The experiments show the performance of the new losses and some findings regarding robustness and accuracy are given."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation of the paper is sound, and the underlying theory regarding certified training remains unknown and challenging.\n- The paper is generally well-organized and easy to follow.\n- The experiments are comprehensive and different datasets and attack radii are used for the evaluation."
            },
            "weaknesses": {
                "value": "- My biggest concern is that the contribution and novelty of the paper are incremental and minor, which is about the expressivity of losses. However, it seems that it somehow borrows the idea of the previous work SABR, which gives an effective loss ranging from adversarial loss and verified loss.  The difference between this work and SABR is not that clear and significant as SABR can induce expressivity by letting $\\lambda=\\alpha$ as shown in Sec. 3.\n- Some key details are not given in the main text. E.g., it is not clear from the main text how the logit differences are associated with an adversarial attack for CC-IBP when it is compared to CROWN-IBP in Sec. 4.1, without which the contribution and novelty are further weakened in terms of the comparison.\n- The insight and intuition of the relationship between CC-IBP and MTL-IBP are not clear. For example, does any case exist where one can be degraded to the other? If so, either theoretical or empirical results are needed to show it.\n- For Table 1, the proposed method is with BaB as a complete method, I wonder if it is fair to compare with some incomplete baselines."
            },
            "questions": {
                "value": "See the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8030/Reviewer_tVkU"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8030/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699592608083,
        "cdate": 1699592608083,
        "tmdate": 1699636990628,
        "mdate": 1699636990628,
        "license": "CC BY 4.0",
        "version": 2
    }
]