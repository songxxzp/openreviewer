[
    {
        "id": "ibqCN1mgRD",
        "forum": "Pj52xO5ysY",
        "replyto": "Pj52xO5ysY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_7yoE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_7yoE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an attention-based multiple-instance classification model for sentiment analysis to add the interpretability of the model from word-level structure. The overall structure is well organized. The experiments show the interpretable ability of the proposed model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed model improves the interpretability from the word-level perspective and uses the MIC module. The overall structure is well organized, and the experimental results illustrate the effectiveness of the proposed model on two public SA datasets."
            },
            "weaknesses": {
                "value": "The novelty and contributions are limited. There are some existing methods using the MIC and self-attention mechanism on SA tasks. The authors should introduce them and explain the main differences as well as advantages. Chapter 4 in \"Sentiment Lexicon Induction and Interpretable Multiple-instance Learning in Financial Markets\". In addition, for the word level interpretability of the SA model, does the author consider combining other level information in the model as illustrated in \"A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification\" -- \"there is only word-level attribution but no high-level attribution such as those over phrases and clauses. Take sentiment analysis as an example, in addition to the ability to recognize the sentiment of sentences, an ideal interpretable model should be able to identify the sentiment and polarity reversal at the levels of words, phrases, and clauses.\"\n\nFor the AMIC architecture, the reason why using a three-layer feedforward operation should be introduced. And why not use a Transformer-based model and only use a self-attention mechanism? \n\nFor the word embedding, what are the differences between Glove-300-Wiki  and  word2vec? The author should add more comparative experiments to show the different performances and BERT embedding.\nFor Table 2, it is better to add more ablation studies on different datasets.\nMoreover, sentiment analysis is a common NLP task, so the authors should add other baselines.\n\nFor interpretability, do the authors consider an attention map to show the performance of the attention mechanism? to illustrate the effectiveness of the proposed model?\n\nMore sentence case studies could be added in the appendix."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698524421934,
        "cdate": 1698524421934,
        "tmdate": 1699636393093,
        "mdate": 1699636393093,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r7e9hspBA9",
        "forum": "Pj52xO5ysY",
        "replyto": "Pj52xO5ysY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_UzwB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_UzwB"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces AMIC, an attention-based multiple instance classification model for interpretable word-level sentiment analysis using document-level sentiment labels. AMIC offers enhanced interpretability compared to other models while maintaining competitive document-level performance. The model incorporates self-attention and positional encoded self-attention within a transparent architecture, providing insights into context-driven sentiment and decision-making."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tReasonable method.\n-\tEvaluation of two datasets with relatively detailed experimental analysis."
            },
            "weaknesses": {
                "value": "-\tThe techniques employed in this study are rather conventional, and in terms of text interpretability, while they do offer some assistance, their significance is limited. Furthermore, for longer sentences or sentence-level tasks (commonly addressed using BERT-based models for text encoding), they lack scalability.\n\n-\tThe work presented in this paper is quite mundane, and employing large models like ChatGPT for word-level interpretability might even yield better results, enhancing its overall scalability as well.\n\n**Text:** \\\nThe service of the restaurant is good, the overall experience is not bad\n**Prompt:** \\\nWhat is the emotion of this sentence, analyzed at the word level\n**ChatGPT 3.5:** \\\nAt the word level, the emotion of this sentence can be broken down as follows:\n\n\"good\" implies a positive emotion.\n\"not bad\" implies a somewhat positive or neutral emotion. While it contains a negation (\"not\"), it's a double negative, and when used colloquially, it often means \"quite good\" or \"satisfactory.\"\n\nSo, the overall emotion at the word level is generally positive, with a touch of reservation or neutrality.\n\n-\tThe related work should be updated with more recent related works.\n\n-\tFigure 1 appears quite blurry. I recommend redrawing the figure to ensure it meets the required resolution of 300 DPI.\n\n-\tThe formulas in the article appear quite unusual and are not conducive to understanding, especially when there are three formulas in a single line. It's not recommended.\n\n-\tThe format of the references in the paper needs to be consistent. If a referenced paper has already been published, it should not be cited in the arXiv format. This should be updated to reflect the appropriate citation style for the published version.\n\n-\tWriting errors are common across the overall paper. Examples could be found in \u201cTypos, Grammar, Style, and Presentation Improvements\u201d.\n\n**Typos, Grammar, Style, and Presentation Improvements:**\n\n-\tOn page 1, \u201c\u2026 weighting, indication of \u2026\u201d => \u201c\u2026 weighting, an indication of \u2026\u201d\n-\tOn page 1, \u201c\u2026 insights on how context influences \u2026\u201d => \u201c\u2026 insights into how context influences \u2026\u201d\n-\tOn page 1, \u201c\u2026 multiple instance classification model \u2026\u201d => \u201c\u2026 multiple instance classification models \u2026\u201d\n-\tOn page 1, \u201c\u2026 methods which focus \u2026\u201d => \u201c\u2026 methods that focus \u2026\u201d\n-\tOn page 2, \u201c\u2026 support vector machine \u2026\u201d => \u201c\u2026 support vector machines \u2026\u201d\n-\tOn page 2 \u201c\u2026 long-short term \u2026\u201d => \u201c\u2026 long-short-term \u2026\u201d\n-\tOn page 2 \u201c\u2026 shown BERT \u2026\u201d => \u201c\u2026 shown that BERT \u2026\u201d\n-\tOn page 2 \u201c\u2026 in text can \u2026\u201d => \u201c\u2026 in the text can \u2026\u201d\n-\tOn page 3 \u201c\u2026 sentiment score at \u2026\u201d => \u201c\u2026 sentiment scores at \u2026\u201d\n-\tOn page 3 \u201c\u2026 help a SA model \u2026\u201d => \u201c\u2026 help an SA model \u2026\u201d\n-\tOn page 3 \u201c\u2026 its ignorance to the \u2026\u201d => \u201c\u2026 its ignorance of the \u2026\u201d\n-\tOn page 6 \u201c\u2026 the sentiment of a wine \u2026\u201d => \u201c\u2026 the sentiment of wine \u2026\u201d\n-\tOn page 6 \u201c\u2026 sentiment and the other \u2026\u201d => \u201c\u2026 sentiment, and the other \u2026\u201d\n-\tOn page 6 \u201c\u2026 follows a 18:1:1 ratio \u2026\u201d => \u201c\u2026 follows an 18:1:1 ratio \u2026\u201d\n-\tOn page 7 \u201c\u2026 has similar performance \u2026\u201d => \u201c\u2026 has a similar performance \u2026\u201d\n-\tOn page 7 \u201c\u2026 is able to also capture \u2026\u201d => \u201c\u2026 is able to capture \u2026\u201d\n-\tOn page 7 \u201c\u2026 various impact of \u2026\u201d => \u201c\u2026 various impacts of \u2026\u201d\n-\tOn page 7 \u201c\u2026 local patterns recognition.\u201d => \u201c\u2026 local pattern recognition.\u201d\n-\tOn page 7 \u201c\u2026 aspect to the \u2026\u201d => \u201c\u2026 aspect of the \u2026\u201d\n-\tOn page 7 \u201c\u2026 delicate lingistic complexities \u2026\u201d => \u201c\u2026 delicate linguistic complexities \u2026\u201d\n-\tOn page 7 \u201c\u2026 clauses seperated by \u2026\u201d => \u201c\u2026 clauses separated by \u2026\u201d\n-\tOn page 8 \u201c\u2026 the capability AMIC in provid \u2026\u201d => \u201c\u2026 has a similar \n-\tOn page 9 \u201c\u2026 providing detailed interpretation \u2026\u201d => \u201c\u2026 providing a detailed interpretation \u2026\u201d\n-\tOn page 9 \u201c\u2026 of analysis process \u2026\u201d => \u201c\u2026 of the analysis process \u2026\u201d"
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Reviewer_UzwB"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762410980,
        "cdate": 1698762410980,
        "tmdate": 1699636392987,
        "mdate": 1699636392987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Qh9ZldEtAj",
        "forum": "Pj52xO5ysY",
        "replyto": "Pj52xO5ysY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_kPxy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4259/Reviewer_kPxy"
        ],
        "content": {
            "summary": {
                "value": "This paper presents AMIC, an innovative attention-based multiple-instance classification approach designed to conduct sentiment analysis at the word level while leveraging document-level sentiment labels for interpretability. The model's architecture employs both self-attention mechanisms and positional encodings, enhancing its transparency and allowing for a clear understanding of how word context influences sentiment determination."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The model's integration of the MIC module enhances word-level interpretability, and its efficacy is demonstrated through experiments conducted on two public sentiment analysis datasets."
            },
            "weaknesses": {
                "value": "The critique highlights the perceived lack of innovation and the limited contributions of the study, suggesting that the methods used are standard and offer minimal advancements in text interpretability. It points out a scalability issue with longer text sequences. It suggests that utilizing more advanced language models for sentence-level tasks or even LLMs (like ChatGPT or LLaMa) for word-level interpretability could improve performance and scalability significantly.\n\nIn addressing the use of different word embeddings, the question is about the rationale behind using GloVe-300-Wiki for the wine dataset and word2vec for the Sentiment140 dataset. The author is encouraged to explain the reasons for this choice and to provide a comparison between the two embeddings to elucidate their differences and justify their specific applications within the study. \n\nThe related work section is weak and does not cover SOTA models. It needs to be updated with more recent studies.\n\nFigure 1 is noticeably unclear and requires enhancement for better visibility.\n\nThe article has many writing errors throughout and needs to be thoroughly corrected."
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4259/Reviewer_kPxy"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699335922706,
        "cdate": 1699335922706,
        "tmdate": 1699636392912,
        "mdate": 1699636392912,
        "license": "CC BY 4.0",
        "version": 2
    }
]