[
    {
        "id": "prJEPNXkLT",
        "forum": "MZzlUyU2ih",
        "replyto": "MZzlUyU2ih",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_WGrL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_WGrL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework that empowers LLM-based planners with online failure detection and re-planning capabilities for robot tasks. The authors recruit a VLM (fine-tuned optionally) to continually monitor the satisfaction of a set of constraints (generated by an LLM), and identify a failure of action execution when any constraints are violated. Upon detecting a failure, the current action's low-level execution is halted and the LLM-based planner is invoked for re-planning. The proposed method is assessed in both a manipulation environment and a humanoid robot setting compared against various baselines. Quantitative results show that it has a higher success rate thanks to the accurate feedback from the VLM, and less execution time due to the immediate failure detection mechanism."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper presents a practical solution to grounding LLM-based planners for failure-aware re-planning. It proposes a novel way to acquire environmental feedback: it leverages an LLM to generate constraints, which are then examined by a VLM. This form of feedback is more precise than the open-ended scene captioning. \n\nThe benefits of the framework are sufficiently justified with comprehensive quantitative experiments."
            },
            "weaknesses": {
                "value": "The primary weakness of this paper lies in its presentation, which lacks clarity and hampers the comprehensibility of several sections, especially Section 3-5. I believe that the paper would greatly benefit from additional work on its writing and organization before it can be considered for publication.\n\nMy suggestions are as follows:\n\n1. Remove redundant and repeated paragraphs. \n    - Sentences introducing previous work in \u201cLLM-based planning with feedback\u201d appear three times in Section 2 (related works), Section 3 (problem statement), and Section 4.1 (method) respectively.\n    - In experiments, most of the baselines are shared in the two environments, but explained twice in Section 5.1 and 5.2.\n    - The first sentence in the abstract and introduction and very much similar. It would be nice to rewrite either one of them.\n2. Redistribute the content in the main body and appendix, to avoid having terms and numbers declared in the main sections but never explained until in the Appendix. \n    - In Section 4.2: \u201cLLM-generated constraints reach an admissible rate of 98%\u201d and \u201cthese two answers reach a consistent rate of 97%\u201d\n    - In Section 4.4: how are the \u201cpotential wasted time\u201d and \u201cfailure probability\u201d defined?\n    - In Section 5.2.2: \u201cthe average detection time from 2.5 seconds to 0.6 seconds\u201d, what is the average detection time?\n3. Please review the paper's grammar, including verb tense, singular/plural form, and article usage. A few examples in Section 3:\n    - assist LLM generate \u2192 assist LLMs to generate\n    - such oracle feedback is \u2192 such oracle feedback are\n    - the humans \u2192 humans\n    - which leverage powerful LLM \u2192 which leverage powerful LLMs\n    - high-level plan \u2192 high-level plans"
            },
            "questions": {
                "value": "1. I have doubts in the usage of the Saycan baseline. It seems that Saycan generates one action per step by jointly considering both visual affordance and LLM output, rather than \u201cdecomposing a task into actions and assumeing execution success\u201d. It might be more appropriate to use an alternative name for this baseline.\n2. In the \u201cAccuracy analysis\u201d section on page 18 of the Appendix, I noticed that the sum of TP and TN are the same before and after fine-tuning the VLM. Does it indicate that there is no performance improvement after the fine-tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4450/Reviewer_WGrL",
                    "ICLR.cc/2024/Conference/Submission4450/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697962798826,
        "cdate": 1697962798826,
        "tmdate": 1700362680350,
        "mdate": 1700362680350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hAoCFFMe5e",
        "forum": "MZzlUyU2ih",
        "replyto": "MZzlUyU2ih",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_nVnU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_nVnU"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed DoReMi, which enables frequent feedback into LLM planning. DoReMi uses LLM in a dual role of generating high-level plan as well as constraints for low-level skill execution; the constraints are then checked during skill execution by a second VLM. Detection of execution failure is formatted into a prompt for the LLM to facilitate re-planning. DoReMi"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is adequately original and significant. It introduces a VLM-based constraint detection to speed up LLM planning and recovery for robotics tasks. While prior work (Du et al., 2023) has considered using VLM for success detection, DoReMi automates the constraint criteria via common sense reasoning of LLM. I think this is a creative and useful combination of LLM and VLM together in a robust framework for task-and-motion planning. Given that large foundation models are increasingly deployed in decision-making, this work provides a conceptually appealing way to improve the robustness and safety of such integrated planning pipelines.\n\nThe paper is well-written, clear, and contains many useful diagrams and algorithm blocks that help convey the main messages."
            },
            "weaknesses": {
                "value": "There are several weaknesses in the paper, mostly lying in the strengths of the experimental results.\n\n1. IM-Oracle seems to outperform DoReMi in most cases in terms of Success Rates at the cost of a slightly higher Execution Time(s). This makes me wonder whether DoReMi's improvement comes just from the ability to prematurely stops low-level skill execution. This ability seems quite easy to add to any other method as well. \n\n2. The method appears to work well for low-level skills that immediately induce a state change in the interacted object (e.g., picking up an object); these skills are also precisely the ones that can be detected via Yes-No VQA queries. Relatedly, when the task becomes more complicated, DoReMi requires fine-tuning of the VLM to work well."
            },
            "questions": {
                "value": "1. Is the only difference between DoReMi and Inner-Monologue (Oracle) that DoReMi is able to utilize the VLM constraint feedback during skill execution while IM (Oracle) waits until the end of skill execution?\n\n2. Could the number of re-planning be reported in the results section? Relatedly, I wonder how would changing the constraint checking time impact the performance of DoReMi?\n\n3. A baseline that probabilistically calls for re-planning at fixed time interval should be included. This will help understand whether the advantages of DoReMi come from the ability of mid-execution replanning or the fact that the VLM constraint detection is accurately informing the replanning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698588155940,
        "cdate": 1698588155940,
        "tmdate": 1699636420130,
        "mdate": 1699636420130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XFq1rUH8U5",
        "forum": "MZzlUyU2ih",
        "replyto": "MZzlUyU2ih",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_fXV5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_fXV5"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces DoReMi, a system that uses large language models (LLMs) for high-level planning in robotics and to generate constraints indicating execution misalignments. Vision language models (VLMs) then continuously check for these misalignments. This allows for timely adjustments when there's a deviation from the plan, improving task success rates and completion times in robotic tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is articulately composed.\n2. The figures and the supplementary video enhance comprehension.\n3. The central idea is both compelling and clear-cut.\n4. Numerous experiments validate the benefits of the proposed approach."
            },
            "weaknesses": {
                "value": "1. In more intricate environments like \"prepare-food,\" the zero-shot performance of the VLM does seem to underperform, as evidenced by the low success rates in Tables 4-5 of the appendix. There's a concern about the potential for incorrect VLM results to misguide the LLM, leading it to continually replan and squander time. However, as noted in Table 2, DoReMi still surpasses the baselines. Could the authors provide insight into this discrepancy?\n\n2. Section 4.4's theoretical analyses raise some questions for me. Theorem 1 appears to be a direct consequence of the assumed Poisson distribution and doesn't particularly underscore the enhancements brought about by the proposed approach. I wonder if a more constructive approach would be for the authors to hypothesize about the VLM's accuracy, which might allow for an estimation of time savings."
            },
            "questions": {
                "value": "Please see the content in Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4450/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4450/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4450/Reviewer_fXV5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706019945,
        "cdate": 1698706019945,
        "tmdate": 1699636420053,
        "mdate": 1699636420053,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aM4JSraFkw",
        "forum": "MZzlUyU2ih",
        "replyto": "MZzlUyU2ih",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_VZHX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4450/Reviewer_VZHX"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to use LLMs with low-level skills to perform tasks. Specifically, this paper considers cases where the low-level skills fail to perform the sub-task successfully and a separate VLM is used to monitor success and failure of each skill. Once failure is detected (by repeatedly checking at some fixed frequency) the method calls the LLM for replanning from the given state."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The main contribution of this paper is to detect failures in skill execution using a VLM and using it to trigger LLM replanning. While previous methods do not trigger failure modes and thus have to wait for low-level skill execution to finish before replanning, this paper adds a VLM based failure detection for replanning."
            },
            "weaknesses": {
                "value": "I don\u2019t think the main contributions of the paper are very significant. Infact, it is a very simple extension of previous approaches which assume no replanning.   Moreover since VLMs have been used for success/failure detection previously, the use of VLMs to detect skill failures is not really novel and the only component that this paper adds is a very trivial LLM replanning step.\n\nOverall, this paper is basically an implementation of termination conditions (as defined/used in the options literature) with a VLM and a high-level LLM planner.  \n\nI also have some other concerns with the paper.\n\n**Using VLM for skill failures:** Why do we have to use a separate VLM (finetuned for skill failures). Since in many cases the skills are being learned why should we not use the skill value function to detect success or failures. This is useful because in the current setting success/failure is completely separate from policy execution. However, in many scenarios when we learn policies we can detect if we are very far from the skill distribution (atleast to some extent). \n\n**Disconnect between constraint generation and skills:** Ideally constraints should depend upon the skills in some way. For instance, as is used in classical symbolic planning literature. However, this approach takes skills as pure black boxes and the constraints are generated simply by the LLM without really considering for the skill. Infact, this implicit shared knowledge between skills and the high-level planner is actually quite important and in this work it is being provided through prompting but that is not really a scalable or general approach.\n\n\n**Frequency of VLM:** Since the paper uses VLMs for failure detection and proper execution it relies on the inference time of this VLM for fast execution. However, large VLM models (e.g. billions of parameters) will not be fast. \n\n**Assumption of recovery skills:** This work implicitly assumes that all recovery skills are available for the planner. However, this is an unrealistic assumption and simply due to the very toy setups used in this paper.\n\nOverall, replanning is very important. However, this paper takes an overly simplistic approach towards this problem. It doesn\u2019t tackle any of the more challenging and interesting cases and thus the overall contributions are really shallow and fail to provide a significant contribution."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4450/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809282816,
        "cdate": 1698809282816,
        "tmdate": 1699636419950,
        "mdate": 1699636419950,
        "license": "CC BY 4.0",
        "version": 2
    }
]