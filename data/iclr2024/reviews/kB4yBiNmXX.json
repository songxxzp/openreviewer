[
    {
        "id": "jb66dyETnt",
        "forum": "kB4yBiNmXX",
        "replyto": "kB4yBiNmXX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission467/Reviewer_oF4Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission467/Reviewer_oF4Y"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new vision transformer architecture, FasterViT, aiming to improve the GPU throughput for vision tasks. The key innovations of the paper include i) using convolutional blocks for the early stages of the model, which are typically memory bound; ii) a new hierarchical attention module (HAT) that interleaves global attention over carrier tokens and self-attention within local windows. Experimental results show that, compared to prior works, FasterViT achieves a better trade-off between accuracy and GPU throughput."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(S1) [Method] the idea of incorporating convolutional blocks for memory-bound stages is interesting, which is actually similar to the design of MobileViT.\n\n(S2) [Performance] Compared to the latest approaches, FasterViT presents improved throughputs on A100.\n\n(S3) [Ablation] Ablations show that the proposed HAT is a plug-and-play module that can also improve the performance of other models\n\n(S4) [Writing] The paper is easy to follow."
            },
            "weaknesses": {
                "value": "(W1) The proposed HAT looks similar to the hierarchical attention proposed in EdgeViT (Pan et al. 2022). While comparisons with EdgeViT is included, it would be beneficial if the author could further clarify the key difference between HAT and the attention block in EdgeViT (maybe just a small paragraph)\n\n(W2) If the reviewer understood correctly, FasterViT incorporated two new designs to improve the throughout i) conv-blocks in memory-bound stages ii) HAT. Clarifying how each element contributes to the throughput would be insightful. Here, the impact of ii) is shown in Table 5, an ablation study for i) would be a valuable addition.\n\n(W3) The authors are encouraged to expand the evaluation to include accelerators beyond A100, such as different GPUs, or even NPUs, so as to provide a broader performance perspective.\n\n\nThe reviewer appreciates the improved performance of FasterViT, but has some concerns about clarity and evaluation. At this moment, the reviewer would like to rate the paper as weak accept."
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776451026,
        "cdate": 1698776451026,
        "tmdate": 1699635973191,
        "mdate": 1699635973191,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QA63Wk2e6j",
        "forum": "kB4yBiNmXX",
        "replyto": "kB4yBiNmXX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission467/Reviewer_Wvdn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission467/Reviewer_Wvdn"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel family of hybrid CNN-ViT neural networks called FasterViT, designed for high image throughput in computer vision applications. FasterViT leverages the strengths of both CNNs and ViT by introducing a Hierarchical Attention (HAT) approach that decomposes global self-attention into a multi-level attention with reduced computational costs. This approach efficiently combines local and global representation learning. FasterViT consists of four stages, reducing input image resolution while doubling feature maps in the early stages and employing transformer blocks in later stages. The proposed HAT mechanism efficiently captures long-range spatial dependencies and cross-window interactions. Extensive validation on various computer vision tasks, including image classification, object detection, and segmentation, demonstrates that FasterViT achieves state-of-the-art performance in terms of accuracy and image throughput, especially for high-resolution images, outperforming competitive models like Swin Transformer V2."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- FasterViT is tailored for high-resolution input images and demonstrates faster image throughput compared to competitive models, particularly in handling images with higher resolutions.\n- The proposed HAT approach efficiently decomposes global self-attention into a multi-level attention mechanism, reducing computational complexity and enabling effective local and global representation learning. Overall, the idea of carrier tokens is novel and interesting.\n- The paper extensively validates FasterViT on various computer vision tasks, including image classification, object detection, and semantic segmentation, showcasing its state-of-the-art performance across a wide range of applications."
            },
            "weaknesses": {
                "value": "- The comparisons in Table 5 and 7 demonstrate minor improvements in terms of accuracy, while the throughput in Table 5 is reduced when using HAT. \n- Performance is compared on A100 GPUs. More platforms should be used to see if throughput results are consistent. At present results are not conclusive."
            },
            "questions": {
                "value": "- The authors should better explain the difference between the proposed attention and window local approaches line Swin."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848727253,
        "cdate": 1698848727253,
        "tmdate": 1699635973117,
        "mdate": 1699635973117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sREUNz6nHj",
        "forum": "kB4yBiNmXX",
        "replyto": "kB4yBiNmXX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission467/Reviewer_tsUS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission467/Reviewer_tsUS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fast ViT architecture using hierarchical attention module (HAT). The HAT module is a modified version of window attention by introducing a new carrier token that summarizes the information of a local window. In this way, the model can preserve a certain level of global information while being more efficient than conventional global attention. With such architectural changes, the proposed FasterViT is able to achieve fast inference on modern GPUs and produces good accuracy on classification and downstreaming tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is overall well written and organized. The motivation of proposing the HTA module is technically sound - it is a good practice to do more memory intensive operations in early stages while putting the computational intensive operations to the later stage. This is also verified in the experiments, where the proposed model is more GPU friendly compared to existing models.\n- The HAT module is not unnecessarily complex and intuitively easy to implement. It is also shown that it can be a plug-in replacement for conventional attention blocks, which makes it more flexible."
            },
            "weaknesses": {
                "value": "The major weakness of this work is experiment. The paper claims that with HAT and all the optimization regarding model architecture, the model has much less complexity compared to conventional attention and it compares the proposed model to a few efficient ViT. However, the comparison is on A100 GPU only and there is no comparison on any other platforms. Some recent works, such as EfficientFormer, FastViT and NextViT compared in Figure 1, all benchmarked their models on different mobile platforms. Therefore, it is unclear whether the proposed change applies similarly to other platforms, which could be very different from A100 GPu due to various software and hardware optimizations. In addition, the ablation study is not sufficient. To really understand the HAT module, a more detailed analysis on it should be provided."
            },
            "questions": {
                "value": "- While the proposed HAT is claimed to be more efficient, only benchmarks on A100 GPUs using a batch 128 are provided. Due to various software and hardware differences, conclusion drawn from the current benchmark may not generalize. For example, it is well known that depthwise conv is more efficient on mobile CPU than on GPU and some ops such as reshape/transpose on CPU is slower due to L2 cache limitation. Even on GPU only, the performance may also vary on different variants. Given that most recent works on efficient ViT have presented benchmark results on multiple platforms - GPU, CPU and DSP, the evaluation from this work is worse and incomplete. I would like to see more benchmarks otherwise the current results are not so convincing.\n- The comparison with existing models is not complete. Although in Figure 1, the proposed model is compared to many recent works in terms of image classification task, the comparison on other downstreaming tasks such as detection and segmentation is quite limited. For example, it is only compared with 3 other models (which are not even designed for efficiency) on the semantic segmentation task.\n- The ablation study only shows that the HAT module can be plugged into other model architecture and improves the performance marginally, but misses analysis on the module itself: how do you determine the architecture for the model variants from 0 to 6? Are there any guidelines or empirical results? How does the latency change with respect to changing the parameters in HAT, such as window size, conv kernel size, pooling size, etc? The current setting seems quite ad-hoc.\n- The parameter count and flops of the proposed model seems on the higher end. Under similar latency, it's much larger than most models. This may prevent it from being adopted in realistic scenarios, where there may be restrictions on model size and memory footprint. How do you aim to resolve this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698883319141,
        "cdate": 1698883319141,
        "tmdate": 1699635973056,
        "mdate": 1699635973056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QHfTJqGVug",
        "forum": "kB4yBiNmXX",
        "replyto": "kB4yBiNmXX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission467/Reviewer_B19F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission467/Reviewer_B19F"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a hybrid conv+transformer network, optimizing throughputs for high-resolution image processing.  It proposes a Hierarchical Attention to combine local window tokens and carrier tokens (one per window).  Results are mostly on ImageNet and ImageNet21k, and the results in Figure 1 are quite impressive."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Intuitive idea:  combing conv + attention is not new, and is often considered more efficient than pure conv or pure transformer for image processing. Window attention is also not new, but the hierarchical window attention is interesting.\n2. Impressive results:  results in Figure 1 are pretty impressive. FastViT outperforms other models by pretty good margin on ImageNet.\n3. Well written paper and easy to follow."
            },
            "weaknesses": {
                "value": "1. It is unclear how significant the proposed hierarchical attention (HAT) is.  Table 5 shows this HAT is better than Twins and EdgeViT; however, Table 7 show only marginal gains when comparing to the vanilla SwinTransformer if treating HAT as a plug-and-play module.\n2. The idea of HAT is not well motivated.  Though it shows good empirical results, it is unclear why simply adding a per-window CT token can significantly improve quality.  Would be nice to add a few more ablation studies or insights.\n3. All latencies are measured on A100 GPU. It would be nice to measure the latency on more diverse hardware platforms, such as different kinds of GPUs, and CPUs.\n4. Some image texts (e.g. in Figure 4 and 5) are too small to read. I recommend enlarging the text in these images."
            },
            "questions": {
                "value": "1. Could you provide more intuition and insights why simply adding one CT token per window will significantly improve model quality?\n2. Could you show the comparisons of FastViT with and without hierarchical attention for ImageNet, e.g., one is vanilla SwinTransformer-style attention and one is HAT?\n3. In Table 7, could you also add the throughput comparison?  I am curious about the overhead of HAT.\n4. Could you add latency for more hardwares (different GPUs and CPUs)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission467/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698894754633,
        "cdate": 1698894754633,
        "tmdate": 1699635972987,
        "mdate": 1699635972987,
        "license": "CC BY 4.0",
        "version": 2
    }
]