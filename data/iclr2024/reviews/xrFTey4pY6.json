[
    {
        "id": "cQLA6sT7dV",
        "forum": "xrFTey4pY6",
        "replyto": "xrFTey4pY6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_DRXt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_DRXt"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method called CLARIFY. In essence, it allows users to provide a short text description of a models repeated mistakes and uses these descriptions in a clip based weighting scheme to fix the spurious correlations during training. They find that a wide range of participants can provide descriptions that help model performance. In experimental eval, they find positive results applying their method in terms of worst group accuracy in a few datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Interesting and compelling method -- the idea of leveraging short text-based feedback on general error patterns to improve performance is potentially very useful\n- Strong results in improving performance in terms of worst-case group accuracy on a variety of datasets, particularly given the ease of use of the technique"
            },
            "weaknesses": {
                "value": "In general, the most significant difficulty I have interpreting the results of the paper is understanding the relationship between the text-based description, in terms of factors like complexity, length, nuance, etc, and the success of this training scheme. The presented results use quite simple descriptions---and it is a good thing such simple texts are useful for getting good results---but how much more complex can these descriptions be? Will CLIP fail to capture the nuance in a marginally longer and more challenging description of an error category? Moreover, if the description is not of a common phenomenon, like hair color, will this method fail? I understand perhaps not all of these questions can be answered in a single work, but I think some efforts should be made to provide guidance and clarity for which types of text descriptions the method can be successful on---this would be quite useful for readers."
            },
            "questions": {
                "value": "- How complex can the text descriptions be for the method to continue to improve results? What are the current constraints here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698125034877,
        "cdate": 1698125034877,
        "tmdate": 1699636792896,
        "mdate": 1699636792896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1TafP9Q2S7",
        "forum": "xrFTey4pY6",
        "replyto": "xrFTey4pY6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_9tpF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_9tpF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework named Clarify that enables non-expert model users to specify spurious attributes for a trained image classifier. Such information is used by a CLIP model to identify a group of related images, and reweight them during re-training. The resulting model is less prone to spurious correlations. \n\nEmpirical results:\n* On two image classification tasks (Waterbirds and CelebA), Clarify can improve worst-group accuracy by 7.3% on average.\n* By applying Clarify to ImageNet, 31 spurious correlations are identified and rectified. Minority accuracy was improved from 21.1% to 28.7%, with only a 0.2% drop in the overall accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This papers studies allowing non-experts to edit model behavior, which is an important problem, especially now that image classifiers can be used widely for various needs.\n* Visualization and method description is easy to understand."
            },
            "weaknesses": {
                "value": "* Paper may be improved from better organization. For example, I don't see a strong motivation that the related work section is between the method and the experiments.\n* More information is needed for the baselines and results. For example, how is \"class-balanced\"/\"wort-class\"/\"slice-balanced\"/\"worse-slice\" defined? Also what is DFR and how is Clarify different from them? Such information is necessary for me to evaluate the contributions of Clarify.\n* Comparison with automated methods is not convincing. Table 2 presents comparison with zero-shot/prompted models, while Clarify is a fine-tuning based method. Clarify outperforming RoboShot is not a convincing evidence. Comparison with fine-tuning based spurious correlation mitigation method is needed."
            },
            "questions": {
                "value": "* What are the design considerations behind the error score in equation 2?\n* In Figure 8, Clarify seems to help partridge and breastplate categories with a significantly larger gap. Is there anything special about these two categories?\n* Currently the method involves re-weighting the training set and re-training the model. Is it possible to used the initially trained model and fix the spurious correlations by further training it?\n* Sorry if I missed it somewhere, but how is the \"7.3% on average\" computed?\n\nMissing reference:\n* https://arxiv.org/abs/2210.00055\n* https://arxiv.org/abs/2103.10415"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698532914735,
        "cdate": 1698532914735,
        "tmdate": 1699636792777,
        "mdate": 1699636792777,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VCRf7hdl25",
        "forum": "xrFTey4pY6",
        "replyto": "xrFTey4pY6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_4msS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6844/Reviewer_4msS"
        ],
        "content": {
            "summary": {
                "value": "The paper described an interface called CLARIFY to elicit and rectify systematic failures of a model.\nUsers could spot and describe spurious correlations on two standard subpopulation shift datasets in about three minutes, which is then used to group examples to nullify the spotted spurious correlation.\nThe paper also demonstrated applicability at scale through evaluation on a subset of Imagenet.\n\nThis work is an interesting first step toward (much-needed) enhancement of standard annotation pipeline. \nI have a few concerns related to presentation, proposal and related work, which need to be resolved."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation and the problem are relevant and practical. Standard annotation pipeline requires rethinking to elicit additional information from the annotators.\n- Presentation is well motivated and mostly well-written."
            },
            "weaknesses": {
                "value": "**Error score**. Since the error score measures classification accuracy between two classes, the error score using random similarity values must be around 0.5. Yet, the error scores reported for genuine descriptions reported in Figure 3 are often less than 0.5.\nSince error score is intended to guide the user, how is the annotator expected to interpret the worse the random error scores?  \nTo further make my point, the error score for \"expert\" phrase on WaterBird (forest) and CelebA (man) is only 0.54 and 0.32 respectively.\nThe error score seems to be poorly designed.\n  \n**Role of initial starting point**. CLARIFY shows some keywords as an initial point for further human response.\nGiven the risk with random phrases (getting high error scores) as explained above, I believe the controlled language that the annotators are initially presented with has much role to play than what is emphasized in the paper. \nPlease address related questions to establish their role. \n\n**Elicitation is too simple even for the simple tasks.**\nThe expert phrases for WaterBird and CelebA are in combination of the label, i.e. forest $\\times$ water/land bird, man $\\times$ blonde/non-blonde for WaterBirds and CelebA dataset respectively.\nCLARIFY only elicits the keyword and not their combination, thereby missing on the true compositional phrase. This is somewhat of a minor issue and goes to only show the difficulty in describing failure modes using language. \n\n**Presentation issues**. The results section is very hurried. In Table 1, 2 are not well explained.\nWhat is LP, DFR, Group Prompt, Class prompt, worst-class, worst-slice?\nWhy is there (ours) marker for some methods?\nHow does CLARIFY work in a zero-shot setting presented in Table 2 because there are no longer examples that can be re-weighted using error description.\n\n**Role of humans**. The experiments did not paint a convincing role of humans in specifying the error pattern. Humans are not necessarily good at finding common patterns across many misclassified examples. Besides, no single pattern or keyword may explain the many miscalssfied examples. Please justify the required human skill and the potentially poor payoff for their effort (i.e. performance payoff for each keyword may not worth the effort). \n\nKim and Mo et.al. (Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation) that is mentioned in the paper proposed an automated discovery of keywords in the same setting as CLARIFY but without the need for human in the loop. They identified the expert keywords for both CelebA and Waterbirds and even demonstrated some results on ImageNet variants. More elaborate discussion and comparison with Kim and Mo et.al. is expected especially since identifying keywords is not easy for humans."
            },
            "questions": {
                "value": "- What are the keywords presented to the user in Fig. 3 results?\n- The top-6 phrases picked on CelebA are all outlier or random features such as *darker blonde, darker than..., any other...*.\nEven on WaterBirds dataset in Figure 3, we see somewhat random phrases such as *ducks, waterfowl* getting good error score. Why is that? On CelebA, *darker blonde*, *dirty blonde* resemble outlier features that may have been rare in the training dataset. How can CLARIFY prevent specification of outlier or random features over spurious features? \n- Please also present results or your comments on what happened when the user-study is conducted without the initial nudge of keywords?\n\nPlease answer other questions raised in weaknesses.\n\n----\n**Post-rebuttal comment**\n\nI thank the authors for their efforts in reporting additional experiments to compare head-on with Bias-to-Text. Good to see that the top keyword from Bias-to-Text does much worse than the best keyword of CLARIFY. I also appreciate that the Bias-to-Text and CLARIFY are complementary. Bias-to-Text can nudge a CLARIFY user with recommendations.\n\nAfter discussion with the authors, I gather that their contribution lies in proposing an interface for eliciting error explaining patterns and in demonstrating its utility. While I appreciate the value of empirical evaluation, CLARIFY is different from Bias-to-Text in only replacing the algorithmic lexicalization of bias with human elicitation. To my understanding, the interface of CLARIFY is a simple random rendering of correctly and incorrectly classified example. The lack of novelty prevents me from recommending an accept.\n\nBesides, I believe a stronger algorithm like Bias-to-Text can obviate or relegate the role of human oversight most likely requiring to rethink the interface."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6844/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6844/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6844/Reviewer_4msS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6844/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768646628,
        "cdate": 1698768646628,
        "tmdate": 1700756529555,
        "mdate": 1700756529555,
        "license": "CC BY 4.0",
        "version": 2
    }
]