[
    {
        "id": "ohZzRvIdaJ",
        "forum": "wAsjsSe0U6",
        "replyto": "wAsjsSe0U6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_xer1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_xer1"
        ],
        "content": {
            "summary": {
                "value": "This work adapted \u201cTotal Variation\u201d which has been extensively applied in areas such as image denoising for data preprocessing in neural networks, seeking to augment the network's ability to focus on low-frequency information. Through rigorous experimentation, it has been observed that this strategy markedly enhances the network's robustness, rendering it more adept at managing suboptimal input data that is either low-resolution, unclear, or highly noisy. Additionally, the approach improves the network's reliability against adversarial assaults."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper is well-written and clear, especially with its formulas and rules, making it easy for readers to get the theory and how things are done.\n2.\tThe paper introduces a brand new method using graph algorithms for sparse projections, which ensures the efficiency of the algorithm's execution. \n3.\tThis approach incorporates previously extensively studied Total Variation for data preprocessing in neural networks, seeking to augment the network's ability to focus on low-frequency information. \n4.\tExtensive experiments prove the proposed method in dealing with substandard data, such as those of low resolution, blurriness, high noise levels and adversarial attacks."
            },
            "weaknesses": {
                "value": "1.\tWhile this method serves as a preprocessing measure for input data, there is a notable absence of comparative validation against the performance of similar procedures (for instance, applying varying degrees of Gaussian blur or color perturbation to input images \u2014 a data augmentation approach that is, in fact, quite prevalent in network training, rather than solely introducing original clean images). The comparative analysis in Supplementary Material Fig.17 is overly subjective.\n2.\tNumerous studies, such as those found in reference [1,2], have delved into enhancing networks' capacity to process low-frequency information through a series of methods including image denoising and high-frequency noise injection. However, this paper lacks a comparative analysis with the findings of these studies or an exploration of whether there is room for further improvement building on their foundations.\n3.\tThe paper proposes three training methodologies utilizing Total Variation operations, all of which yield certain results, while differing in effectiveness (for example, the method \u201cFixed Training\u201d appears suitable for low-resolution scenarios, whereas \u201cFinetune\u201d is more applicable for adversarial attacks). However, the paper lacks an analysis explaining these variances, specifically a discussion on the applicable domains for each of the three methods. \n4.\tThe paper lacks a discussion concerning certain parameters, especially an analysis and empirical representation of \"early stopping,\" as mentioned in the title. Additionally, there is an absence of explanation as to why a sparsity of 0.8 was chosen for the Fixed training method, an aspect that could be elucidated with experimental demonstration and analysis\n5.\tThe image in Fig.7 (b) is blurred. The differences in Fig.5 are not very discernible except for the first column, suggesting there's no need for so many repetitive visuals.\n\n[1]Xie, Cihang, et al. \"Feature denoising for improving adversarial robustness.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.\n[2] He, Zhezhi, Adnan Siraj Rakin, and Deliang Fan. \"Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019."
            },
            "questions": {
                "value": "It is hoped that the aforementioned issues can be addressed as comprehensively as possible. Additionally, it would be beneficial to have a more clear and intuitive description of the graph constructed in the \"Sparse Projection via Graph Algorithm\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638561627,
        "cdate": 1698638561627,
        "tmdate": 1699636450939,
        "mdate": 1699636450939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8srQyiWCHX",
        "forum": "wAsjsSe0U6",
        "replyto": "wAsjsSe0U6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_NN23"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_NN23"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an image preprocessing approach where they disentangle the semantic structure from the details of the image to avoid textual bias in deep learning. The paper proposes to leverage the structure of the Total Variation (TV) regularization matrix in the Inverse Scale Space (ISS) to generate a regularized image path from large-scale with semantic information to fine-scale with detailed information. This approach also incorporates an early stopping mechanism for the generated image path, which is computed with high efficiency using Nesterov acceleration. They demonstrate their method on various image tasks, including robustness against noise, adversarial attacks, and low-resolution images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is reasonable and valuable to explore how to disentangle the large-scale semantic information from the fine-scale detailed information to conduct the high-level information from the image to the neural networks.\n\nThe idea of leveraging the graph algorithm to accelerate the sparse projection is interesting. \n\nThe experiments widely demonstrate their proposed algorithm on a variety of image tasks."
            },
            "weaknesses": {
                "value": "The writing and presentation are not clear enough. It is not easy to follow for the reader who is not familiar with the related theory [1][2]. A more detailed background introduction is recommended to add to the Appendix. Due to the presentation issues, the connection and difference between the existing theory and the method introduced in [1][2] is not clear. \n\nMany notations are not explained well. The role of \\beta, \\gamma is unclear. In Section 3.1, the claim \"with t playing a similar role as 1/\\lambda in Eq1.\" is confusing. And the definition of ||D\\beta|| seems has typo. \n\nIn experiments, the compared methods are not enough. The authors only compare with the vanilla and the TV layer methods. Some related preprocessing can filter out the high frequency, or the detailed contents should also be evaluated and compared.\n\n[1] Huang et al. \"Boosting with structural sparsity: A differential inclusion approach.\"\n\n\n[2] Fu et al. \"Exploring Structural Sparsity of Deep Networks via Inverse Scale Spaces\""
            },
            "questions": {
                "value": "I believe this paper needs to be refined to fix the above issues."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698992634730,
        "cdate": 1698992634730,
        "tmdate": 1699636450814,
        "mdate": 1699636450814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P4vHYdN7ez",
        "forum": "wAsjsSe0U6",
        "replyto": "wAsjsSe0U6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_5hKV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_5hKV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an instance smoothing algorithm that disentangles structural information from detailed ones via early stopping on generating a regularized image path from large-scale to fine-scale. Then different training procedures are proposed to incorporate this algorithm into the training process. Extensive experiments are conducted on robustness tasks to verify the effectiveness of the proposed model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper is the first to investigate the inverse-scale-space (ISS) property at the image level.\n2.\tThe experiments section provides convincing visualization and frequency analysis."
            },
            "weaknesses": {
                "value": "1.\tThe symbols in formulas need to be specified, e.g., the meaning of \u03b2.\n2.\tThe authors claim that they propose an efficient sparse projection method. In addition to superiority in computation and time complexity compared with SVD and LSQR, is there any advantage for performance improvement?\n3.\tThe choice of early stopping time is unclear. Since early stopping is an important operation to disentangle structural information from detailed ones, it\u2019s crucial to illustrate the choice of early stopping time.\n4.\tThe comparison between this method and existing methods is missing. The authors only compare their method with baseline ones, i.e., Vanilla Model and TV Layer."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699375175197,
        "cdate": 1699375175197,
        "tmdate": 1699636450742,
        "mdate": 1699636450742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hOB6v9RFxh",
        "forum": "wAsjsSe0U6",
        "replyto": "wAsjsSe0U6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_bFfP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4693/Reviewer_bFfP"
        ],
        "content": {
            "summary": {
                "value": "This article proposes a novel method for visual semantic learning based on early stopping in inverse scale space. The method can disentangle structural information from detailed information in images, and incorporate it into neural network training. The method improves the robustness and explainability of the models on various tasks, such as noisy images, adversarial attacks, and low-resolution images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1\u3001The paper proposes a novel instance smoothing algorithm that can disentangle semantic and detailed information in images using Total Variation regularization and Inverse Scale Space. This is a creative combination of existing ideas from image processing and sparse recovery. The paper also applies this algorithm to neural network training, which is a new domain for this kind of technique.\n\n2\u3001The paper provides theoretical analysis and empirical evidence to support the effectiveness and efficiency of the proposed algorithm. The paper also compares the algorithm with several baselines and demonstrates its advantages in various robustness tasks, such as noisy images, adversarial attacks, and low-resolution images.\n\n3\u3001The paper is well-written and organized, with clear definitions, notations, and explanations."
            },
            "weaknesses": {
                "value": "1\u3001The authors have not validated the effectiveness and scalability of their method on larger datasets, such as Imagenet.\n\n2\u3001The authors have not explored the possibility of applying TV regularization on feature maps, which may further improve the robustness and explainability of the models.\n\n3\u3001The authors have not compared with other structure-based methods, such as shape-biased models or edge detection-based models.\n\n4\u3001The authors have not conducted a sensitivity analysis on different TV regularization parameters, which may affect the performance and results of the instance smoothing algorithm."
            },
            "questions": {
                "value": "Here are some concerns and questions that I have for the authors:\n\n1\u3001How do you choose the optimal sparsity level for different tasks and datasets? Is there a general criterion or guideline for selecting the sparsity parameter?\n\n2\u3001How do you compare your method with other methods that also use TV regularization or other forms of regularization to enhance robustness and interpretability, such as TVM (Yeh et al., 2022b) or LRP (Bach et al., 2015)?\n\n3\u3001How do you evaluate the quality and diversity of the generated image path? Do you have any quantitative or qualitative measures to show the trade-off between structural and detailed information along the path?\n\n4\u3001How do you handle the cases where the structural information is not sufficient or reliable for the task, such as when the shape is distorted or occluded by noise or other objects? Do you have any strategies to incorporate other sources of information, such as texture or context, to improve the performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699434751996,
        "cdate": 1699434751996,
        "tmdate": 1699636450657,
        "mdate": 1699636450657,
        "license": "CC BY 4.0",
        "version": 2
    }
]