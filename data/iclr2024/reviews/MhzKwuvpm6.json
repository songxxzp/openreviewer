[
    {
        "id": "zh8X9qKy6C",
        "forum": "MhzKwuvpm6",
        "replyto": "MhzKwuvpm6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_3koB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_3koB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RILe, which is a teacher-student imitation learning architecture by introducing an intermediary teacher agent into the GAIL optimization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The framework seems interesting and new\n2. The algorithm is simple and seems to be better than previous baselines, but I have questions about those results, see below."
            },
            "weaknesses": {
                "value": "1. The motivation and advantage of this work compared with previous methods is not clear.\n2. Results are simply not good enough. For example, on Atari, most of the results do not solve the task (like -20 in Pong actually makes no difference with -21) and is similar to BC (or even worse); on Mujoco, the results are not consistent with those reported by the previous works.\n3. Writing is clear the most of time."
            },
            "questions": {
                "value": "1. What is the motivation of such a teacher-student framework? By adding the complexity to GAIL, what is the advantage compared with it? Under Eq (12) the authors said\n``This enables us to train the student agent in a standard RL setting where it receives rewards from the teacher to ensure that its policies mimic expert behavior. Thus, we can break the data-policy connection common to existing IL solutions and facilitate a less data-sensitive learning process that can generalize over the specific state-action pairs of expert trajectories.`` I do not see any `break the data-policy connection common` to GAIL, and I also do not understand why such a learning style `facilitate a less data-sensitive learning process`. GAIL also has such advantages.\n\n2. `This reward structure allows us to utilize any single-agent reinforcement learning algorithm, instead of using supervised learning to optimize over loss functions defined in Equations 11 and 12.` What is the difference to GAIL? GAIL also allows any single-agent reinforcement learning algorithm. And why should we conduct `supervised learning over loss functions defined in Equations 11 and 12`? What are you specifying?\n\n3. In Table 1, the Atari experiments\n    - (a) Why the BC behaves better for traj. num. = 1 than traj. num. = 100? Especially on Asteroids and Qbert.\n    - (b) Why the std for all baselines is 0? How many training/testing seeds are you using? How do you calculate the averaged results?\n\n4. In Figure 2, the Mujoco experiments\n    - (a) What is 2.5 expert trajectories?\n    - (b) Why the results of baselines, like GAIL and AIRL is not consistent with the report from their paper or many previous imitation learning papers? What implementation are you using? What kind of expert demo are you using? From my experience, GAIL can easily solve Hopper and Walker.\n    - (c) How many training/testing seeds are you using?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698358308743,
        "cdate": 1698358308743,
        "tmdate": 1699637154672,
        "mdate": 1699637154672,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vu3TZux2nA",
        "forum": "MhzKwuvpm6",
        "replyto": "MhzKwuvpm6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_RQBA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_RQBA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an interesting method to the imitation learning problem. It uses existing Adversarial Imitation Learning method to train a teach policy, and uses the action probability of the teach policy as a reward to train the (behavioral) student policy. Experiments are carried out on several Mujoco and Atari environments and outperform previous AIL/IRL methods like GAIL and AIRL."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed idea is simple and straightforward.\n\n2. The paper is easy to follow, and strong empirical performance is achieved on the Mujoco benchmark."
            },
            "weaknesses": {
                "value": "1. The major weakness of this paper is that is in its soundness. The teacher policy is trained to maximize the distribution matching between the student policy's state action visiting distribution and the expert's distribution, and its output probability is used as the reward for the student policy. It is unclear whether the student policy can still perform correct distribution matching. I would like to see some theoretical results on this. For the current version the method looks more like a heuristic but not a theoretically sound approach.\n\n2. It is unclear to me why adding a teacher policy as a reward processor to train the behavior policy would be better than using AIL/IRL reward directly. Besides theoretical results, I would also like to see some intuitive & in-depth discussions on this point. Also, it looks like the choice of AIL/IRL reward for training the teacher policy should not be limited to GAIL, but the authors only base their method on GAIL. The authors can try to use different choices to see if their proposed approach can lead to some improvement.\n\n3. I am also wondering the online sample efficiency of the proposed method. I think it would be necessary to add a return plot for comparison."
            },
            "questions": {
                "value": "See the weakness parts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698531739813,
        "cdate": 1698531739813,
        "tmdate": 1699637154572,
        "mdate": 1699637154572,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ei8SomCUTk",
        "forum": "MhzKwuvpm6",
        "replyto": "MhzKwuvpm6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_KLBr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9178/Reviewer_KLBr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to imitation learning that combines approaches from inverse reinforcement learning and more standard imiation learning methods. The approach involves training a discriminator to distinguish between a teacher's actions and the expert actions, then using the teacher to \"distill\" information into a student model by using the teacher's action as a reward function. The authors evaluate their method on Atari and MuJoCo tasks, where they show that their method outperforms GAIL, AIRL, and BC."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- I believe that the approach of combining GAN-like training of a teacher with distillation into a student model is novel. \n- The experimental results show that RILe has a substantial improvement over prior baselines on Atari and MuJoCo tasks."
            },
            "weaknesses": {
                "value": "- The presentation of this paper needs significant improvement, especially in the technical sections. It uses ambiguous and incorrect notation at times, and I am unsure what exactly the reward for the student is exactly. Several specific examples include:\n\n- Abstract: \"expert policy is trained directly from data in an efficient way, but requires vast amounts of data.\" -> This seems like a contradictory statement. Requiring vast amounts of data is normally not seen as \"efficient\".\n- Sec. 3: Pi* is overloaded between equation 1 and equation 3 and refer to the optimum for different objectives. \n- The notation IRL(t) should be defined before equation 3.\n- Sec 4: Thus, it evaluates the state action pair of the student agent s T = (sE, aS) and chooses an action aT that, in turn, becomes the reward of the student agent aT = rS. -> This is a syntax error - rewards are scalar values, which an action is generally not.\n- Eq 8 -> Same issue. What does it mean to minimize an action? Do you mean to minimize MSE between the action of the student and the expert?\n- \"the action of the teacher is the reward of thestudent: rS = \u03c0T ((sE, aS)\" -> missing a closing parentheses.\n\n- The performance of the baselines looks extremely poor (worse than performance on the same tasks presented in the original papers).This seems indicative of poorly tuned baselines. I do not completely trust the results until the questions I have raised in the questions section have been addressed."
            },
            "questions": {
                "value": "Sec 5.3: What does it mean to use 2.5 expert trajectories? What does half a trajectory mean?\n\nExperiments: What was the performance (total reward) of the expert trajectories used for each experiment? This would be a good number to show as an upper bound / oracle of performance.\n\nThe section mentions different amounts of expert trajectories used, but only one table is reported. Are all of the different # trajectories averaged into the same table?\n\nPlease report learning rates, other hyperparameters, and hyperparameter sweeping strategies in the experiments or appendix (esp for the different components: the student, teacher, and discriminator).\n\nWhy do the reported baselines (AIRL, GAIL) perform so poorly, even poorer than performance reported in the original papers? For example, GAIL reports expert performance on the Walker and Hopper tasks in their paper, yet in the baselines reported here the performance is very poor. Would appreciate if the authors shed some insight into the discrepancy in results (e.g. is it because of the reduced number of expert trajectories?)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9178/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698533566309,
        "cdate": 1698533566309,
        "tmdate": 1699637154454,
        "mdate": 1699637154454,
        "license": "CC BY 4.0",
        "version": 2
    }
]