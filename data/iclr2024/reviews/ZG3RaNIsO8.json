[
    {
        "id": "KbreIurx0g",
        "forum": "ZG3RaNIsO8",
        "replyto": "ZG3RaNIsO8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_hqfj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_hqfj"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel framework for automatic discrete prompt optimization, called EVOPROMPT, which connects Large Language Models (LLMs) with Evolutionary Algorithms (EAs). The authors demonstrate that this approach can generate better prompts compared to human-engineered prompts and existing methods for automatic prompt generation. The paper is well-structured, with clear explanations and thorough experiments conducted on 31 datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed EVOPROMPT framework is innovative, combining the language processing capabilities of LLMs with the optimization performance of EAs. This approach allows for the simultaneous leveraging of the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs.\n\n- The paper provides a comprehensive review of related works, including prompt engineering, discrete prompts, and LLMs with optimization algorithms. This review helps readers understand the context and contributions of the proposed method.\n\n- The experiments conducted on 31 datasets demonstrate the effectiveness of EVOPROMPT compared to crafted prompts and existing methods. The results show consistent performance gains over both manual instructions and existing methods."
            },
            "weaknesses": {
                "value": "- The paper could benefit from more detailed explanations of the implementation details for each type of EA (GA and DE) used in EVOPROMPT. Providing more information on the specific steps and instructions for each algorithm would help readers better understand the proposed method and its connection to LLMs.\n\n- The paper could include more ablation studies to better understand the impact of different components of the proposed method. For example, analyzing the effect of different population sizes, mutation rates, or selection strategies on the performance of EVOPROMPT would provide valuable insights into the method's robustness and generalizability."
            },
            "questions": {
                "value": "- Could you provide more detailed explanations of the implementation details for each type of EA (GA and DE) used in EVOPROMPT? Specifically, it would be helpful to understand how the algorithms are adapted to work with discrete prompts and how the LLMs are used to generate new candidate prompts.\n\n- How is the effect of different population sizes, mutation rates, or selection strategies on the performance of EVOPROMPT"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Reviewer_hqfj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759869543,
        "cdate": 1698759869543,
        "tmdate": 1699636444375,
        "mdate": 1699636444375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fFaWdYiZ7Q",
        "forum": "ZG3RaNIsO8",
        "replyto": "ZG3RaNIsO8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_HRoN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_HRoN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces EvoPrompt, a new framework that leverages evolutionary algorithms (EAs) for discrete prompt optimization in Large Language Models (LLMs) without requiring access to the gradient or parameters of the LLMs. Specifically, taking cues from EA concepts such as mutation and crossover, EvoPrompt uses the LLM itself to carry out crossovers and mutations on a collection of pre-defined prompts, leading to the generation of a fresh set of evolved prompts. The EvoPrompt has been instantiated to both Genetic Algorithm (GA) and Differential Evolution (DE) versions. Comprehensive experiments across 31 datasets have demonstrated the efficacy and good performance of EvoPrompt when tested on both closed-source and open-source LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The combination of LLMs with EAs as presented is novel to me, providing insights for future LLM research.\n* I liked the design of using the LLM itself to iteratively refine candidate prompts, which harnesses the natural linguistic strengths of LLMs. This ensures that the generated prompts are coherent, readable, and well-constructed sentences.\n* The authors have conducted extensive experiments on benchmark datasets and the discussions on different designs and hyperparameters are commendable.\n* The paper is well-written and easy to follow. It offers useful examples and LLM templates."
            },
            "weaknesses": {
                "value": "* EvoPrompt's performance appears to be influenced by the quality of the initial prompts (initial population), which may limit its applications. While the paper emphasized the DE version's superior exploration over the GA, a deeper analysis would be beneficial.\n* Further discussion and evaluation of the diversity of the prompts produced by EvoPrompt would be beneficial.\n* The EA optimization for a single prompt involves iterations. However, the search doesn't appear to converge in Figure 6. Is it possible to illustrate the effects of more iterations? It's unclear if these 10 iterations are conducted within a single LLM session or if each iteration starts with a new session. Besides, more details should be provided regarding the runtime, and resources for running the GA.\n* It would be beneficial if the authors could present negative examples where EvoPrompt failed, as this could inspire direction for future research. Moreover, the limitations of this approach should be discussed."
            },
            "questions": {
                "value": "1. In EvoPrompt, most of the initial populations were generated with a mix of manually-created and LLM-generated prompts. What is their ratio when DE works better than GA? What would be the performance of EvoPrompt if the initial prompts were all generated by LLM?\n2. How effectively can EvoPrompt discover novel prompts utilizing EA operators? (e.g., considering factors such as prompt length and the variance in approaches between new and prior prompts). \n3. When the LLM generates mutated new words, is there adequate diversity in the new words introduced, or is there a repetition of the same words?\n4. See other questions in the weakness part.\n5. Will there be any benefits to combining and alternating GA and DE updates during search?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Reviewer_HRoN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765275158,
        "cdate": 1698765275158,
        "tmdate": 1699636444256,
        "mdate": 1699636444256,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n0FgyX0C5A",
        "forum": "ZG3RaNIsO8",
        "replyto": "ZG3RaNIsO8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_LYv4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_LYv4"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present EvoPrompt, a framework to combine Evolution Strategies and a Language Model to engineer prompts to improve the performance of possibly another Language Model on a variety of instruction tasks. The authors mention that the prompt used to instruct an LLM strongly influences its performance and that human crafted prompts are often sub-optimal thus motivating the need to use optimization algorithms to generate prompts so as to maximize the performance of an LLM on a given task. In that context, the authors motivate the use of Evolution based algorithms as they typically perform great on discrete sequential data and offer a good way to balance exploration and exploitation. Evolution algorithms also have the benefit from performing black box optimization - they do not access to the function or its gradient - which allows users to use them with models that are only accessible through an API for instance. However, standard mutation and crossover functions  that typically invert / replace / mutate individual elements in the discrete sequences without maintaining global coherence which would lead these operators to quickly generate prompts that are unreadable and non interpretable. Therefore, the authors propose to use an LLM to perform crossover and mutation operations to always generate coherent and meaningful new samples. The authors compare to Evolution strategies - genetic algorithms and differential evolution - and benchmark their method on a variety of tasks including language understanding and summarization. The method is applied both to Alpaca (instruction-fine tuned Llama which is open-source) and to GPT-3.5 (proprietary model accessible through an API). They compare obtained to several baselines and propose some ablations to study the impact of mutation strategy used and the impact of the initial population."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "First of all, I find the paper well written. The motivation is clearly explained, the context is well introduced and the authors do a great job in the literature review part. The method is also clearly explained and detailed which makes it easy for the reader to understand what is going on and which might help to reproduce the work as well. I also really enjoyed Figure 1 and 2 which are very clean, colorful and do a great job at illustrating the method.\n\nThe method itself is sound and I agree that Evolution Strategies make a lot of sense for this problem for all the reasons mentioned. I mainly like the way crossover and mutation is implemented by querying an LLM. I find this idea very powerful as indeed, in most cases the main challenge in using EAs lies in defining operators that can stay in the data distribution when perturbing samples. \n\nI also find the experimental setup convincing as the authors performed a large number of experiments and compared to several relevant baselines. The fact that the method was illustrated both on an open-source and on a proprietary model also strengthened that part and I also like that the authors illustrated how to apply their method and LLM-based evo operator with two different types of Evolution Strategies."
            },
            "weaknesses": {
                "value": "While I really appreciate the evo operator that leverages LLMs, I have more doubts about the relevance of the considered application. I feel like there are more interesting applications for this method than prompt engineering which I believe will soon not be a thing anymore as we see the latest language models becoming more and more robust to the prompt that is used. However, I acknowledge that there are ongoing efforts in that field and while I personally don't find it very exciting or relevant, this work contributes efficiently to this research track.\n\nI also have some doubts about the relevance of this work to ICLR. In my opinion, the main contribution of this work is this LLM-based mutation/crossover operator which would make more sense in an evolutionary algorithms conference such as GECCO than at ICLR. \n\nFinally, I think that this work would strongly benefit from a study of the computational cost of the approach in the main core of the paper. I saw some elements that could help towards this end in the supplementary, but I would like to see an analysis of the computational cost / financial cost and time needed to perform this optimization. It would be interesting also to add similar analysis for the baselines. I think this would be very useful to the readers to evaluate to what extent they could apply that to their research if they have either to serve themselves Alpaca on an accelerator or to query the OpenAI API after connecting their credit card."
            },
            "questions": {
                "value": "All in all, I like the idea being introduced and I think the authors did a good job at introducing it and benchmarking it, therefore I would be happy to see this work published (it would be even better if the authors would demonstrate the method on a different use-case than prompt optimization however, I understand this would be a very different paper. It might be an idea for the next one though ;)). However, I have some doubts if ICLR is the best place for that as I think GECCO would be better suited. Therefore, I will say minor accept for now but I would be happy to increase my grade if the reviewers and other authors convince me that actually ICLR is the right place.\n\n\n**A few other general comments**:\n\n- As mentioned above, I think this would be very exciting to apply these LLM-based evo operators to other applications. For instance this could be used to generate game levels through textual description or coding exercises. With latest models such as DALLE-3, the same strategy could be applied to generate images.  \n\n- The authors mention that EA are good at balancing exploration and exploration, however it is not always the case and this is for instance often a limitation of GA methods. I would recommend the authors to have a glance at the Quality Diversity literature with algorithms such as MAP-Elites which are much better at balancing exploration. I think it would be very exciting to use EvoPrompt on top of a QD method.\n\n- In the conclusion, the authors mention looking at Simulated Annealing (SA) for future work. In my experience, GA and other more recent EAs always outperform SA by a strong margin (if correctly tuned and implemented). Thus, I would not advise going in that correction. QD methods are probably going to be more fun. Looking at multi-objectives methods such as NSGA-II might be interesting as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Reviewer_LYv4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853302439,
        "cdate": 1698853302439,
        "tmdate": 1700587192680,
        "mdate": 1700587192680,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9xBcn7NzUJ",
        "forum": "ZG3RaNIsO8",
        "replyto": "ZG3RaNIsO8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_ZLDk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4642/Reviewer_ZLDk"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces EVOPROMPT, a new framework that merges evolutionary algorithms (EAs) with Large Language Models (LLMs) to optimize human-readable prompts for language tasks. By using EAs to iteratively improve prompts for both open- and closed-source LLMs such as GPT-3.5 and Alpaca, the study reveals significant performance enhancements (up to 25%) over human-designed prompts, showcasing the potential synergy between LLMs and EAs for prompt optimization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well organized and clearly written."
            },
            "weaknesses": {
                "value": "The paper's attempt to introduce EVOPROMPT as a groundbreaking framework for discrete prompt optimization feels uninspired due to its reliance on established evolutionary algorithms (EAs) rather than innovating with novel approaches. While the study showcases the ability to optimize prompts for language models like GPT-3.5 and Alpaca, this contribution doesn't significantly advance the state of the art, particularly given the use of well-known evolutionary operators of DE and GA. Designing prompts for language models, although important, might not meet the level of substantial novelty expected at a conference like ICLR. This approach, while showing performance enhancements, might not present a substantial leap forward in the field of language model optimization, especially considering the expectations of innovation and original contributions at such academic conferences. I think this paper would be better suited in a conference like IEEE CEC or PPSN."
            },
            "questions": {
                "value": "Why out of so many available methods, you only chose DE and GA? Why you used only one particular offspring generation strategy of DE? How are you repairing infeasible set of offspring and how does that process affect your timing complexity?\n\nThe authors have provided extensive rebuttals and based on the discussions with the authors, I am revising my score to 6."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4642/Reviewer_ZLDk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4642/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699160333153,
        "cdate": 1699160333153,
        "tmdate": 1700681644069,
        "mdate": 1700681644069,
        "license": "CC BY 4.0",
        "version": 2
    }
]