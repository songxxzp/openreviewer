[
    {
        "id": "ubnGI7qXwE",
        "forum": "K9sVJ17zvB",
        "replyto": "K9sVJ17zvB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a diffusion-based method for video generation. The paper proposes the VersVideo model that leverages multi-excitation spatial-temporal convolution, a Mixture of Experts attention blocks, and a temporal compensated decoder to address the challenges associated with spatial-temporal dynamics. Additionally, it integrates the ControlNet model for versatile video generation, allowing for a wider range of visual conditions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-motivated and the proposal method is technically sound.\n2. The proposed method combines several existing powerful techniques to learn better spatiotemporal features.\n3. The proposed method achieves comparable performance on the MSR-VTT dataset."
            },
            "weaknesses": {
                "value": "1. The proposed method directly combines existing techniques, such as excitation network, MoE to enhance spatiotemporal feature representation, which makes the novelty a bit weak. The proposed Multi-Excitation Convolution applies excitation convolution multiple times. Original excitation convolution should be cited.\n\n2. From Table 1, the proposed fails to achieve better performance than Make-A-Video. Even compared with the methods trained on WebVid-10M, the performance improvement is marginal.\n\n3. From Table 2, the performance of the proposed method is a lot worse than the previous STOA method, e.g., VideoFusion. The author explains this by \"the VersVideo-L model has a significantly smaller parameter of 500M\". Why not use a larger model to verify the performance? Moreover, 1) Make-A-Video also reports performance on the UCF dataset, the results should be included; 2) the author should also provide performance under the same resolution as previous methods for better comparison.\n\n4. Results in Table 2 and Table 3 are from different training datasets. Better to keep it consistent for better comparison."
            },
            "questions": {
                "value": "see weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_Cov9",
                    "ICLR.cc/2024/Conference/Submission7134/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698572663774,
        "cdate": 1698572663774,
        "tmdate": 1700588804082,
        "mdate": 1700588804082,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bpd7g2Nkth",
        "forum": "K9sVJ17zvB",
        "replyto": "K9sVJ17zvB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_CWze"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of creating stable and controllable videos by proposing a versatile video generation model. In contrast to existing video diffusion models that often limit spatial-temporal performance due to oversimplification of standard 3D operations, VersVideo introduces multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes and multi-expert spatial-temporal attention blocks. This approach significantly improves the model's spatial-temporal performance without increasing training and inference costs. To address information loss during the transformation from pixel space to latent features and back, temporal modules are incorporated into the decoder to maintain inter-frame consistency. The paper also presents a unified ControlNet model suitable for various conditions, such as image, Canny, HED, depth, and style."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The introduction of multi-excitation paths and multi-expert spatial-temporal attention blocks enhances the model's spatial-temporal capabilities without significantly increasing computational cost. This is particularly impressive given the complexity of 3D operations in video generation. The solution proposed for minimizing the issue of information loss, mainly through the incorporation of the temporal module, helps achieve better inter-frame consistency, which is crucial for maintaining the temporal coherence of generated videos. The development of a unified ControlNet provides a more versatile approach to handle various conditions, leading to broad applicability.\n2. The quantitative improvements of the proposed method are notable. The related ablations offer reliable evidence about the effectiveness of the whole system."
            },
            "weaknesses": {
                "value": "1. Leveraging pre-trained models could limit the model's versatility and the ability to generalize across different datasets. Is it possible to validate the proposed designs to other SD models?\n2. The notable quantitative and qualitative improvements are appreciated. However, some common issues are not discussed in the paper, e.g., hand generation, multiple object generation, instruction following, and how to respond to relative location descriptions, etc. If these cases cannot be well-addressed, then they should be included in a limitation discussion."
            },
            "questions": {
                "value": "1. It might be beneficial to include a section discussing the limitations of your current approach and possible future work to address these limitations.\n2. If possible, consider releasing the code or at least providing more detailed implementation notes. This could help others replicate your results and build upon your work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786418222,
        "cdate": 1698786418222,
        "tmdate": 1699636844475,
        "mdate": 1699636844475,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GhkYjE0iHo",
        "forum": "K9sVJ17zvB",
        "replyto": "K9sVJ17zvB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_eX2J"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes VersVideo for video generation. To capture the temporal information, multi-excitation paths for spatial-temporal convolutions and multi-expert spatial-temporal attention blocks. Also, the decoder of VAE is also finetuned to maintain inter-frame consistency. The authors demonstrate the effectiveness of VersVideo in multiple video generation tasks, e.g., text-to-video generation and conditional video generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The paper studies video generation and proposes several blocks to enhance the temporal consistency of generated videos.\n\n2) The generated videos have plausible results.\n\n3) The authors demonstrate the capability of the proposed network in the controllable generation setting."
            },
            "weaknesses": {
                "value": "1) The details of the MoE attention are not explained very well. In section 2.2, the authors mentioned that they use MoE design to enhance attention. If my understanding is correct, the attention is performed on all channels, but the feedforward network after attention is applied to different groups. If the MoE attention is applied like this, how can the feature or attention be enhanced?\n\n2) Also, several details are missing for the temporal compensate decoder. In the paper, the authors do not mention what S1 stands for.\n\n3) For the Multi-excitation conv, the motivation for using four branches is not addressed clearly. The effectiveness of each branch is also not thoroughly ablated."
            },
            "questions": {
                "value": "Please see my concerns in the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698819682337,
        "cdate": 1698819682337,
        "tmdate": 1699636844352,
        "mdate": 1699636844352,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B80LXGDOhW",
        "forum": "K9sVJ17zvB",
        "replyto": "K9sVJ17zvB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a versatile video generation model named VersVideo, which enhances spatial-temporal capability using textual, visual, and stylistic conditions. The model incorporates multi-excitation paths for spatial-temporal convolutions and multi-expert spatial-temporal attention blocks to improve temporal consistency and generation performance without escalating costs. It also uses temporal modules in the decoder to maintain inter-frame consistency and mitigate information loss. Besides, they design a unified ControlNet model suitable for various visual conditions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well organized. The authors have well categorized and summarized previous methods. The authors explain the shortcomings of previous methods and the motivation for proposing the method very clearly.\n- The authors provide web demos for visualization. It is helpful for the readers to examine the effectiveness of the proposed method.\n- The authors provide a specially designed module for perceiving temporal information for video generation tasks. The design of the module is very innovative, and it shows improvement in quantitative indicators.\n- The decoder design and consistency loss are helpful for maintaining temporal consistency."
            },
            "weaknesses": {
                "value": "- The authors did not provide a visual comparison of the results. Providing a visual comparison would better demonstrate the effectiveness of the method. Besides, a user study is also needed to verify the effectiveness of the method.\n- Adding the impact of each module on visual effects in the ablation experiments would be more helpful in verifying what role each module plays.\n- The quality of the video-to-video editing results currently shown in the paper is not much better compared to other methods such as Render-A-Video and TokenFlow. The results generated in the current demo have severe color changes. Therefore, I doubt the effectiveness of this method in video fidelity.\n- Since many modules in the framework designed in the paper are related to temporal consistency, please provide indicators to measure temporal consistency. Currently, the paper only lists indicators of generation quality."
            },
            "questions": {
                "value": "Can the current framework handle the situation with multiple condition inputs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7134/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7134/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7134/Reviewer_1xNe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7134/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698859347995,
        "cdate": 1698859347995,
        "tmdate": 1699636844223,
        "mdate": 1699636844223,
        "license": "CC BY 4.0",
        "version": 2
    }
]