[
    {
        "id": "uPT4vavMQf",
        "forum": "Yx7TnC6AAp",
        "replyto": "Yx7TnC6AAp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_zdt3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_zdt3"
        ],
        "content": {
            "summary": {
                "value": "This paper studies imperfect information EFGs in the setting of partially observable Markov games. Due to the partial observability of the game, current techniques rely on either online learning with loss estimations or Monte Carlo counterfactual regret minimization to achieve convergence. However, the sample complexities depend on the cardinalities of the information set space and action space, which could be extremely large. This paper introduces efficient algorithms in the setting of linear function approximation, which circumvents the problem of dimensionality. In their setting, transitions between states are known and the current information set is also known to the players, and finally the reward functions admit a linear structure. With all this in place, the authors present two algorithms based on mirror descent and FTRL, which the authors call Least Squares OMD and Least Squares FTRL. Both these algorithms admit comparable regret guarantees to the existing state of the art algorithms, albeit with modified dependencies on exploration parameters and game tree structure respectively."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, the paper has a clear structure and the extension of prior work to the linear function approximation setting is a reasonable and interesting setting. Despite the complexity of the arguments required and the heavy notation, the technical work in the paper is impressive, and the authors do an admirable job with making the paper readable. The motivation for avoiding the curse of dimensionality in POMGs is also strong, and this paper is indeed an initial foray into understanding learning in IIEFGs with linear function approximation."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is that while the overarching setting is clear (POMG, linear function approximation etc), there are many other instances where the writing is less clear. For instance, the introduction describes the regret bounds of previous work without defining X or A. The definition of parameters $\\alpha$ and $\\lambda$ in the regret bounds of LSOMD and LSFTRL are similarly obfuscatory and not well explained - how the bound compares to prior work of Bai et al and Feigel et al is merely alluded to but not properly substantiated with experiments or examples. Along this line, Assumptions 3.2 and 4.1 are not clearly explained to the reader - how restrictive are these assumptions in practice? I feel that some experiments on games with large action spaces might have been helpful to express the relative advantages of using the approach in this paper. Overall, I believe if some explanatory remarks were added that could clear up some of the confusion, and a writing pass was done to make some of the explanations and contributions clearer, then the paper would be more worthy of acceptance."
            },
            "questions": {
                "value": "- How restrictive are Assumptions 3.2 and 4.1 in practice?\n- Is there a multiplayer variant of the algorithms proposed that can provide convergence guarantees in multiplayer, large-scale games? Does the need for each player to have access to the current information set make such a claim invalid?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Reviewer_zdt3",
                    "ICLR.cc/2024/Conference/Submission5362/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698123921745,
        "cdate": 1698123921745,
        "tmdate": 1700477568189,
        "mdate": 1700477568189,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vaSd6KrFJz",
        "forum": "Yx7TnC6AAp",
        "replyto": "Yx7TnC6AAp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_AopK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_AopK"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces two algorithms for online learning in Partially Observable Markov Games: Linear Stochastic Online Mirror Descent (LSOMD) and Least-Squares Follow-The-Regularized-Leader (LSFTRL). This research uniquely emphasizes learning in Imperfect Information Extensive-Form Games using linear function approximation, diverging from traditional settings. A significant contribution is the novel least-squares loss estimator that leverages composite reward features. For LSOMD, the research employs an exploration policy to derive its regret bound, denoted as $\\tilde{O}(\\sqrt{HX^2d\\alpha^{-1}T})$, contingent on a specific quantity $\\alpha$ related to the exploration policy. Furthermore, LSFTRL adopts a \"balanced transition\" methodology, previously used in several works, for its loss estimator. This results in regret bounds $\\tilde{O}(\\sqrt{H^2d\\lambda T})$ and $\\tilde{O}(\\sqrt{HXd T})$ ($\\lambda\\geq X/H$), which rely on another quantity $\\lambda$ linked to the game tree structure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper introduces two novel algorithms for online learning in POMGs, marking the first study of Imperfect Information Extensive-Form Games using linear function approximation. This linear approximation approach is notably more practical.\n- The paper clearly presents the problem setting, making it accessible to readers unfamiliar with the topic.\n- The algorithms are clearly explained, and the intuition behind them is also provided."
            },
            "weaknesses": {
                "value": "- For LSOMD, the parameter setting ($\\eta$) necessitates prior knowledge of $\\alpha$. A discussion on determining or estimating $\\alpha$ is required. An adaptive algorithm would be a preferable solution.\n- Similarly, the parameter setting ($\\eta$) of LSFTRL requires prior knowledge of $\\lambda$. Besides lack of discussion on $\\lambda$, it is not clear whether we should use the first or the second parameter initialization in practice. I think when $\\lambda\\geq X/H$, the second initialization should be adopted, but how to determine whether $\\lambda\\geq X/H$ and what regret we can get if $\\lambda< X/H$?\n- Discussion on lower bounds is needed, even if some necessary conditions on some of the factors in the regret bounds are helpful.\n- No experiment is provided in the main paper."
            },
            "questions": {
                "value": "- Is it possible to provide an adaptive algorithm that achieves a similar regret bound without knowing $\\alpha$ or $\\lambda$ in advance? Please refer to the weaknesses part.\n- Are the regret bounds provided by LSOMD and LSFTRL optimal? I guess that $\\sqrt{HdT}$ is unavoidable in the bound, but this requires a more rigorous proof.\n- Could you conduct empirical experiments to demonstrate the algorithms' effectiveness? Even numerical tests using toy examples would be insightful.\n- In Assumption 2.1, the paper assumes $\\left\\|{\\bf\\theta}\\right\\|_2\\leq \\sqrt{d}$ and $\\sup_{s_h,a_h,b_h}\\left\\|\\Phi(s_h,a_h,b_h)\\right\\|_2\\leq 1$. Can this pair of inequalities be substituted with $\\bar{r}_h(s_h,a_h,b_h)\\leq \\sqrt{d}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Reviewer_AopK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698204354844,
        "cdate": 1698204354844,
        "tmdate": 1699636540853,
        "mdate": 1699636540853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HBf4v60lG7",
        "forum": "Yx7TnC6AAp",
        "replyto": "Yx7TnC6AAp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_YLD6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_YLD6"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a solution to the problem of learning in two-player zero-sum imperfect information extensive-form games (IIEFGs) with linear functional approximation. The focus is on IIEFGs in the formulation of partially observable Markov games (POMGs) with known transitions and unknown rewards while admitting a linear structure over the reward functions. The challenge is that both players are unaware of the current underlying state, since only the current information set rather than the state is observable. This poses substantial difficulties in exploiting the linear structure of the reward functions, as the current feature corresponding to the current state is unknown.\n\nTo address this problem, the paper proposes a linear loss estimator based on the composite features of information set-action pairs. These composite reward features can be seen as features of corresponding information set-actions, and are weighted by the transitions and opponent's policy. The paper proves the unbiasedness of this estimator and derives regret bounds that depend on various game parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Disclaimer) I personally research on Markov Game, not on EFG. But I carefully read this paper, checked every proof. \n\nAs I try to find the literature of EFG regarding this, there are no linear approximation papers, and it is actually needed. In this sense, this is a good starting point I believe."
            },
            "weaknesses": {
                "value": "I have several concerns with assumptions. I will write it down in the questions section. My main concern is the tightness of this analysis, too strong assumptions, and also computational issues. If these are resolved, I am willing to change my score. I think this topic is extremely important for EFG literature, while this paper is somewhat weak because of the following questions."
            },
            "questions": {
                "value": "Q1) I think the assumption that is on page 7 (regarding exploration) is not used in Luo et al and Neu et al. As far as I understand, Assumption 3.2 provides a very strong assumption, basically saying that every policy covers any kind of x. Which is actually making no need for exploration. I think this is very strong, and it is not done in other \"recent\" works (a similar assumption was at very \"traditional\" papers) Can you clarify that? I think this is a kind of uniform-coverage assumption in offline RL literature. Or can you refer any specific examples that are used in EFG literature? \n\nQ2) Just want to understand: is this paper making an assumption that we know p(s|a.b)? or do we need to learn that? \n\nQ3) theorem 3.3: Still, that depends on X^2, so in the finite state action space case, it is not optimal. expected regret is not enough. Maybe for the general space, does this algorithm match with the lower bound? or can we prove (or have some clue) the lower bound? Also, can we eliminate H term as Balanced FTRL? \n\nQ4) page 8 : Still computation depends on A.. (O(XA)) which means that it does not scale with the linear representation. This is also related to Q1, as we want to cover the large action space or (maybe infinite size A). That means that alpha is at least smaller than  1/A. \n\nQ5) I am not an author of \nBreaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation \nand \nBreaking the curse of multiagents in a large state space: RL in Markov games with independent linear function approximation\nbut they are providing linear approximation scheme for Multi-agent RL. What is the relationship between this paper and these two approximations?\n\n**Nov 25) Still I do not think that the assumption is comparable, as this paper assumes a condition about \"exploration policy\"'s eigenvalue, so I want to re-evaluate.**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Reviewer_YLD6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716076259,
        "cdate": 1698716076259,
        "tmdate": 1700942134768,
        "mdate": 1700942134768,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KMOwSz9YMN",
        "forum": "Yx7TnC6AAp",
        "replyto": "Yx7TnC6AAp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_ahTG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5362/Reviewer_ahTG"
        ],
        "content": {
            "summary": {
                "value": "The paper studies algorithms for imperfect information extensive form games (IIEFGs) with linear function approximation, formulated as partially observable Markov games (POMGs) with known transition and bandit feedback. Least-squares estimators are proposed and incorporated into online mirror descent (OMD) and follow-the-regularized-leader (FTRL). Regret bounds are provided for both algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The setting studied in the paper is meaningful and interesting. While it seems like a natural generalization that stems from POMDPs and MGs, the results on linear POMGs in this paper are original and nontrivial. To my knowledge, these are the first algorithms and regret bounds for such linear POMGs."
            },
            "weaknesses": {
                "value": "I have some slight concern over the significance of the results: On one hand, the results (in particular, the regret bounds) at a high level do not seem surprising or particularly insightful (although there may be things I missed); on the other hand, this is clearly a rather generic framework for a POMGs, which in reality are usually highly complex (e.g., $H$ and $X$ can be large and $\\alpha$ tiny) but often come with specific structure that can be leveraged. Thus, I am not fully convinced that the algorithms and results in this paper can be useful in real world settings, although this by no means diminishes the theoretical value of the results. In addition, the specific assumptions on the structure (e.g., the linearity of rewards and access to the opponent's feature vectors) seem to make the results less general."
            },
            "questions": {
                "value": "A minor comment: I would appreciate if the author(s) define the relevant quantities upfront in the introduction (I see them in the abstract and later in the text, but think they deserve a note when first mentioned)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5362/Reviewer_ahTG"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5362/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729782144,
        "cdate": 1698729782144,
        "tmdate": 1699636540565,
        "mdate": 1699636540565,
        "license": "CC BY 4.0",
        "version": 2
    }
]