[
    {
        "id": "MrMKX2TXN9",
        "forum": "O0dW800ukz",
        "replyto": "O0dW800ukz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_G223"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_G223"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method for multimodal training of protein models based on a distillation method. The multimodal model incorporates Protein Sequence, Structure, Gene Ontology Annotation - named Protein SSA for short, which is tested on protein fold and enzyme commission tasks. \n\nThe paper first introduces the problem settings of modeling protein properties and behavior using machine learning with an additional focus on how multimodal data sources can enhance the modeling performance. This leads to the paper's key claims that prior work did not incorporate all possible modalities into their methods prompting the creation of Protein SSA which includes a broader set of modalities. The authors also claim that there is a shortage of protein modeling methods that do not require costly pretraining, leading them to propose a knowledge distillation based training for their multimodal setting. The paper then discusses related work in protein representation learning, domain adaptation and knowledge distillation method with a particular focus on graph-based knowledge distillation methods given that the paper trains GNN in their method.\n\nNext, the paper describes the problem setting and provides a preliminary exploration on whether multimodal embeddings can enhance performance on relevant protein tasks (GO, EC) with the evidence generally being supportive. The paper then describes the main method contribution in Protein SSA, including relevant formulation of message passing for the protein graph as well as the domain adaptation and knowledge distillation framework. The knowledge distillation framework mainly relies on minimizing the KL divergence between the embeddings of the teacher and student models, both of which are approximated by Gaussian distributions. \n\nIn Section 4, the paper describes the experiments Fold classification and enzyme reaction, as well as on GO and EC prediction tasks. Compared to the baselines presented in the paper, Protein SSA generally performs best across all tasks, including different types of methods that use a lower number of modalities. Section 4.5 of the experiments includes an ablation study where the paper investigates the importance of different components, including the presence of annotation in the teacher model, the presence of the teacher model itself and training without the KL loss."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper has the following strengths:\n* It proposes a new method (ProteinSSA) for multimodal protein modeling that includes a larger set of modalities that taken together improve modeling performance (originality, significance).\n* The problem definition and relevant related work are extensively discussed (quality, clarity).\n* The paper includes a relevant ablation study that investigates the effect of removing different components of ProteinSSA (quality, significance).\n* The experimental results are generally nicely presented with relevant analysis provided (quality, significance)"
            },
            "weaknesses": {
                "value": "The main weakness of the paper is clarify surrounding the training method used:\n* It is unclear whether ProteinSSA makes use of pretrained embedding model, especially for the teacher model. The paper mentions training ProteinBERT with additional modalities, but generally claims that ProteinSSA does not require large-scale pretraining. This appears inconsistent and further clarification would be helpful (significance, clarity).\n* The paper does not compare results against larger scale protein models for relevant tasks, including the ones mentioned in related work (e.g., ESM, KeAP, ProtST). It would be good to get a sense of much model size affects performance on the studied tasks (quality, significance).\n* The GNN architecture is not fully described (clarity)."
            },
            "questions": {
                "value": "* Can you describe how you obtain the embeddings for each modality? Do you use pretrained models for some or all modalities?\n* Can you describe how large your model ends up being in terms number of trainable parameters?\n* Can you describe your GNN architecture in more detail? How do you consolidate the graphs from the different graph modalities (sequence, structure) into joint embeddings?\n* Can you add the performance of the teacher model into your results tables?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Reviewer_G223"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698539475808,
        "cdate": 1698539475808,
        "tmdate": 1700558549940,
        "mdate": 1700558549940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nmTuWhZsyt",
        "forum": "O0dW800ukz",
        "replyto": "O0dW800ukz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_1uow"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_1uow"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn function enhanced protein representations by distilling knowledge from a teacher model with additional GO representation constraint. Here the teacher model is a ProteinBERT, while the GO is encoded by a fully-connected neural network. The combined representation will force the student model to learn meaningful and functional protein representations. The proposed model are evaluated on several understanding tasks, and the performance is pretty good."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed model performs well on the several protein understanding tasks."
            },
            "weaknesses": {
                "value": "1. **Lack of baselines:** The paper lacks some important baselines. For example, the paper didn't report the teacher model's performance and the performance of removing the KL divergence term.\n\n2. **The motivation is unclear:** Actually, I don't really get the reason why the author needed to train a student model, which seems redundant. In this paper, the student model is not smaller than the teacher model. Instead, the student model shares parameters with the teacher model. It seems the author just needs to finetune the ProteinBERT involving the additional GO information constraint.\n\n3. **The writing is confusing:** Many parts of the paper make me feel confused, especially the KL divergence part. For example, what do $P_S(G_S, A)$ and $P(Z_S|G_S, A)$ mean? Are these VAE model? If it's true, then expanding the $P_S(G)$ to $P(G|z)P(z)$, I don't think the assumption that \"$E_{p_S(G)}[KL[p_S(y|G)P_T()y|G]]$ does not depend on z\" holds. By the way, I don't really get what the source domain and target domain mean. It seems they are the same domain in the exception that source has an additional constraint on GO."
            },
            "questions": {
                "value": "I have already mentioned some questions in the weaknesses. Additional questions are provided as follows:\n\n1. In Equation 5, why the author directly add the $h_S$ to h_A without any transformation? It seems they are from different semantic spaces.\n\n2. Removing the AE-T doesn't influence the performance much. Does that mean this additional GO encoder didn't add to much benefit to the whole model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Reviewer_1uow"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698692593500,
        "cdate": 1698692593500,
        "tmdate": 1700733050504,
        "mdate": 1700733050504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y5It2vh6d4",
        "forum": "O0dW800ukz",
        "replyto": "O0dW800ukz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_KkPB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3403/Reviewer_KkPB"
        ],
        "content": {
            "summary": {
                "value": "This paper is concerned with how to compute embedded representations of proteins using a variety of data sources.  The authors note the imbalanced nature of protein data, where unannotated sequences are plentiful, with functional annotations an order of magnitude less so, and structures rarer by yet another order of magnitude.  Their solution is to fuse representations by distilling knowledge from a teacher network, for which structure, sequence, and annotations exist, and a student network that acts on sequence and structure alone.  While the student and teacher share a GNN architecture for encoding sequence and structure, the teacher additionally has function annotation information to enrich its' embeddings.  The teacher's richer embedding space regularizes the student's embedding space, thus imparting the student with extra information.  They go on to show favourable performance on tasks predicting fold classification, enzyme reaction, and GO-term predictions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors' attempt to circumvent the data imbalance for annotated protein data by using distillation is really interesting, and I think is well worth exploring here.  \n- In addition, combining the best of 1D, 3D (via a replacement for SE(3) tools via [Ingraham et al.]() and GNNs is really interesting too.\n- In particular, the result for ProteinSSA on fold classification is clearly an advance."
            },
            "weaknesses": {
                "value": "The largest weakness of this paper isn't in the ideas, but with the writing.  There are many examples where it's not quite clear from the text what the authors are trying to convey.  \n\n- The authors stress that ProteinSSA does not require pretraining, and does not make use of annotations.  This is only true of the *student* model, since the teacher does clearly make use of annotations where such exist. Table 1 is thus misleading.\n\n- The title of section 3.2 isn\u2019t very informative.  What problem does it address?  Or what part of the final architecture is being discussed here?  It\u2019s not clear, and would benefit from being rewritten.  For example, how does CDConv relate to KeAP and ESM-1b?  These are very briefly described, but not in sufficient detail to tell the reader why each was chosen, and how they relate to each other & to ProteinSSA as a whole. It\u2019s only when you read to the bottom of 3.2 that you discover that this section is all about establishing that pre-trained models are limited in different ways, and *that\u2019s* why ProteinSSA was made.  Please, lead with this, and then describe the limitations of other sequence-based models that require extensive pre-training afterwards.\n\n- Reading through the subsections of section 3, it\u2019s hard to put my finger on what the focus of this paper is.  \nThe different elements are well described, but what isn\u2019t clear exactly is how they will be synthesized into something new and exciting, as well as why the choices (e.g edge representation of 3.1, sequence function representation of 3.2) were made, and why (beyond them being SOTA at one point in time).  I think section 3 would be clearer, and benefit the reader if it had a summary of the subsections at the beginning, and for each subsection to describe one part of the whole model\n\n- Reading through to Section 3.3, it isn't yet explained why the authors think knowledge distillation is the best way to incorporate knowledge from annotations. Why not just use the teacher network directly? The answer is (I think from subsequent sections) that functional annotation are only sometimes available, so by instead aligning the latent space of the student with that of the teacher, the student derives the benefit of additional knowledge.  I have to stress this is not clear from reading section 3, but should be clearly spelled out somewhere within (or within the introduction).\n\n- Both the sentence preceding equation 8 and the sentence that follow are overly wordy, but without the benefit of clarity.  It\u2019s clear that the addition of a KL regularization term KL($P_{S}(z_{S})$, $P_{T}(z_{T})$) will force the student embedding distribution to become like the teacher distribution, thus affecting the student embedding state.  Words about \u201creduce the bound in the representation spaces\u201d or \u201cKL divergence matches distributions\u2026\u201d is a bit misleading; all that\u2019s intended here is an intention to regularize the parameters of the student  model indirectly through the distribution of its embeddings.\n\n- Table 3 reports only point estimates of max accuracy.  I find max accuracies very difficult to parse in a meaningful way.  I think the improvements of ProteinSSA would be better qualified if you report the distribution of accuracies from multiple runs, especially that of fold classification.  Even if you cannot re-run the alternatives, you can report ProteinSSA results more faithfully.\n\n\n**Minor points:**\n\nIn the introduction, the phrase \u2018grammar of life\u2019 isn\u2019t a helpful metaphor.  I realize this is a small point, but what these models learn are not always distillable into rules for compositional orientation of elements of protein language.\n\n- Equation 5 has a term $\\alpha$ that controls \"the isotropic of protein representations\".  What does this mean?\n- There are some grammtical errors in the first sentence on page 6\n- Page 6 in section 3.3 invokes the CLT.  I don\u2019t think you need to invoke the CLT here to model the distribution of the embeddings as Gaussian, you can just assume it to be true.  At any rate, it\u2019s not clear that the different batch derived embeddings are independent."
            },
            "questions": {
                "value": "- The ablation study of section 4.5 is welcome, but does not address one of the key choices of the paper (raised in the *Protein Domain Adaptation* paragraph of section 3.3), which is why the teacher embeddings are concatenated with a separate functional embedding rather than using function as an extra term in the loss function for classification.  How come?\n\n- Just prior to equation 10, there is an argument about reducing the generalization bound which seems a non-sequitur.  I do not understand why generalization bound arguments are being used here; it seems very disconnected from the rest of this section.  Could the authors please help me understand why?\n\n- Section 4.1 begins by describing ProteinBERT and how it is pre-trained.  Is this part of ProteinSSA?  If so, can ProteinSSA really claim (as in table 1) that it is not pre-trained?  If not, then is mentioning ProteinBERT here relevant?\n\n\nI want to stress to the authors that I think there is a good paper within here, but that its writing needs work, and that the authors need to think harder about ordering, motivating, and presenting their arguments.  I'm certainly willing to change my score if the post-rebuttal version of the paper takes my suggestions into account."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3403/Reviewer_KkPB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3403/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720049811,
        "cdate": 1698720049811,
        "tmdate": 1700589375159,
        "mdate": 1700589375159,
        "license": "CC BY 4.0",
        "version": 2
    }
]