[
    {
        "id": "6U4XfqmIPj",
        "forum": "v675Iyu0ta",
        "replyto": "v675Iyu0ta",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_5xMH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_5xMH"
        ],
        "content": {
            "summary": {
                "value": "The paper offers an analysis of the extrapolation performance of several simplified representation methods used to understand the information processing of deep architectures. The analysis is solely focused on Transformers trained on Dyck-k languages, which consists of strings of matched brackets of k different types. These models are tested in and out of distribution by manipulating the maximum hierarchical depth and other parameters. The study considers two simplified models: PCA and k-means clustering of the key and query embeddings of the transformer layers. The authors conclude that these simple methods offer a good description of the model in-sample, but fail to explain the behavior of the model out-of-sample."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is well written and it contains several interesting considerations on the nature and interpretation of transformers.\n- The problem area is vitally important to the implementation of AI in real-world applications, and the focus on out-of-sample performance is interesting and well motivated.\n- The experimental analysis is detailed and rigorous."
            },
            "weaknesses": {
                "value": "The focus of the experimental analysis is too narrow. The introduction section does a good job in outlining the research goals, but this aim is then overly specialized to a specific class of models trained on a toy problem. It is therefore very difficult to extrapolate the conclusions of the paper outside of its narrow domain, which in itself is not very useful for the broader literature. \n\nAll in all, in spite of the several interesting insights present in the text, the contribution and novelty of this work is very limited."
            },
            "questions": {
                "value": "- Can you include the analysis of other datasets and architectures? I do appreciate your work on toy-languages. However, I would like to see these insights to be applied  to models trained on naturalistic data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698403197167,
        "cdate": 1698403197167,
        "tmdate": 1699636748482,
        "mdate": 1699636748482,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Yc5Td1fjcy",
        "forum": "v675Iyu0ta",
        "replyto": "v675Iyu0ta",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_H66k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_H66k"
        ],
        "content": {
            "summary": {
                "value": "The paper describes how much the simplified transformer models represent the behaviour of the original ones.\n\nThe authors consider a use case of Dyck balanced-parenthesis languages and show that while the simplified proxies, using hard (one-hot) attention, show alignment with the behaviour of the original models, they do not match the behaviour out-of-distribution. They use the evaluation methodology as per Murthy et al (2023) which involves predicting closing brackets at least ten positions away from the corresponding opening brackets and evaluating the  highest-likelihood prediction accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Pros:\n- (Originality) The originality of the paper stems from analysing the claims of interpretability\n\n- (Significance) It is important to see the detailed analysis of limitations of simplified models on a simple example which would highlight the deficiencies of such models.\n\n- (Quality) The paper thoroughly addresses reproducibility\n\n- (Clarity) The paper is clearly written (however, see Q1-Q3)"
            },
            "weaknesses": {
                "value": "Cons:\n\n- (Elements of significance) Given that the analysis focuses on Dyck balanced-parenthesis languages, it is largely limited and arguably backs up the intuitive claim that the simplified models do not fully represent the behaviour of the original models; however, I still think that it  is still significant because we have such evidence described in detail as it helps inform large-scale model interpretability studies"
            },
            "questions": {
                "value": "1. In Figure 6, it seems like for the longer key depths, the predictions diverge more. Would the authors be able to emphasise more whether it is always the case and if there are any solid reasons behind this particular behaviour?\n\n2. In Figure 1 description it is stated that \u2018On the depth generalization split, the models achieve approximately 80% accuracy.\u2019 Is it 80% or around 75% as can be seen in the purple curve on the image? (It does not affect any conclusion, just found that I could not fully explain this discrepancy)\n\n3. \u201cHowever, the error patterns diverge on depths greater than ten, suggesting that the lower-dimension model can explain why the original model makes mistakes in some in-domain cases, but not out-of-domain\u201d To what extent does it  happen consistently across different training trajectories of the stochastic gradient descent and/or across different datasets? In other words, plausible it sounds, would the same tendency repeat if we change the data or train the model again?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695836311,
        "cdate": 1698695836311,
        "tmdate": 1699636748347,
        "mdate": 1699636748347,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CWkcXodLp5",
        "forum": "v675Iyu0ta",
        "replyto": "v675Iyu0ta",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_nnFq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_nnFq"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an analysis of transformer models trained on the Dyck languages. To do so, the models are simplified and analyzed with data-dependent and data-independent tools, highlighting a discrepancy between the behaviour of original models and simplified models on out-of-distribution data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper provides several methods to analyse transformers trained on the Dyck language, investigating whether simplified versions of the model are faithful to the original one on out-of-distribution test sets."
            },
            "weaknesses": {
                "value": "Being unfamiliar with the literature, it is hard for me to understand the point of the analysis, and it is hard to tell whether that is due to a poor presentation or due to my lack of understanding. However, what I find a weakness of the paper is the fact that the analysis is not paired with proposed improvements or solutions. For example, what do the results from the paper entail? Is it that transformer models are not suitable for learning language models? Or is it that using model simplifications, while facilitating the analysis of some properties of the model, leads to a mismatch with the original model on out-of-distribution samples? If so, what would be a better way to analyze transformer models, to avoid the found shortcomings of current methods?"
            },
            "questions": {
                "value": "In addition to the questions in Weaknesses:\n\n- What conclusions can be drawn from Deep Learning practitioners? Are transformers reliable for learning languages?\n- Does the analysis extend also to similar models or other datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Reviewer_nnFq",
                    "ICLR.cc/2024/Conference/Submission6585/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771023599,
        "cdate": 1698771023599,
        "tmdate": 1700740452898,
        "mdate": 1700740452898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KJfXcdH9t2",
        "forum": "v675Iyu0ta",
        "replyto": "v675Iyu0ta",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_oEsb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_oEsb"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a case study of using simplified model to interpret a trained transformer model on an algorithmic task. It's shown that using dimension reduction or clustering to simplify the model down to a proxy model may yield interpretability illusion. In particular, the simplified proxy model is not faithful in out-of-distribution settings, and cannot be used to reliably predict the original model's OOD error."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is focused on a classic formal language task (Dyck grammer) and provides a convincing case study of interpretability illusion in Transformer language model. I think the main observation from the paper is interesting and quite relevant. \n\nThe distributions are novel, as prior work in mechanistic interpretability mostly gives positive results. \n\nI think the main result is surprising, where the simplified model generalizes less well to OOD data than the full model, where intuitions from learning theory would suggest the opposite.\n\nThe paper is well-written and the illustrations are clear.  It also gives a good survey of related work."
            },
            "weaknesses": {
                "value": "While the paper delivers a strong conceptual message, at a technical level, it is a single case study on a somewhat toy algorithmic task. That is, the scope of the work is a bit limited. I personally would be interested in a broader study on similar formal language tasks (for example, on other languages expressed by finite-state automata https://arxiv.org/abs/2210.10749).\n\nThe paper would also be stronger if it looks into why the simplified model generalizes less well to the depth split. Figure 6 is an interesting observation. What is really being truncated by SVD (which plays a role for OOD generalization)? Is there any mechanistic story here?\n\nMinor suggestions\n---\n\nFigure 2(a)(b) should be accompanied by a color scale. Does yellow indicate 1 and green indicate something less than 1?\n\nAlso, for Figure 2(a) and related experiments, if you don\u2019t prepend the START symbol, what would the attention pattern look like? Does that affect any of your results here?\n\n\u201cSecond, the value embeddings encode more information than is strictly needed to compute depth, which might suggest that the model is using some other algorithm\u201d \u2014 Can you expand on this? What is the extra information, if you have looked into it all?"
            },
            "questions": {
                "value": "I have asked a few questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Reviewer_oEsb"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698809858826,
        "cdate": 1698809858826,
        "tmdate": 1700691832518,
        "mdate": 1700691832518,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ui3vNoVF0y",
        "forum": "v675Iyu0ta",
        "replyto": "v675Iyu0ta",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_iDw5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6585/Reviewer_iDw5"
        ],
        "content": {
            "summary": {
                "value": "This study focuses on the question whether a simplified model (e.g., models obtained from dimensionality reduction or clustering) can still faithfully mimic the behavior of the original model on out-of-distribution data. This study conducts experiments on synthetic datasets constructed using the Dyck balanced-parenthesis language and shows that simplified models are less faithful on out-of-distribution data compared to in-distribution data and can under- or overestimate the generalization ability of the original model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper focuses on a very important question, that is, whether the explanation model/proxy is faithful to the original model. Or more precisely, whether the explanation model can mimic the original model\u2019s behavior on different data distributions. Some existing explanation methods, such as distilling the target model into a decision tree, cannot guarantee the faithfulness on out-of-distribution data (e.g., masked input samples). Therefore, it is of significant value to delve into this issue."
            },
            "weaknesses": {
                "value": "1.\tI\u2019m not familiar with the Dyck balanced-parenthesis language used in this paper, so I feel a bit confused and overwhelmed reading Section 2.1. It would be a great help if the authors can give some toy examples when introducing the Dyck languages.\n2.\tThe phrase \u201csimplified models\u201d can be misleading in this paper\u2019s context. I was thinking of methods such as knowledge distillation or network pruning when I first see the phrase \u201csimplified models\u201d. However, what the paper mainly focuses on are dimensionality reduction and clustering methods. It is encouraged to use a more precise word other than \u201csimplified\u201d."
            },
            "questions": {
                "value": "I have several confusions when reading the paper, and hope these can be resolved by the authors\u2019 rebuttal.\n\n1.\tIn Section 2.1, the authors construct different testing datasets (named as **IID, Seen struct, Unseen struct (len <= 32), Unseen struct (len > 32), and Unseen depth**, respectively) to evaluate the model\u2019s generalization ability. I wonder which of these testing datasets are considered in-distribution datasets, and which are out-of-distribution datasets? It seems not clear from the paper.\n2.\tAfter reading Section 5.1, I\u2019m still confused about how to interpret the results in Figure 4. I only see as the number of components or the number of clusters increase, the simplified model becomes more similar to the original model (the JSD between the attentions decreases, while the ratio for predicting the same token increases). However, I\u2019m not sure how one can conclude that there is a generalization gap between the simplified model and the original model. I\u2019m also not sure how one can conclude that the simplified model underestimate/overestimate the generalization ability of the original model. Is it more appropriate to compare the prediction accuracy of the simplified model with that of the original model on both in-distribution and out-of-distribution data?\n\nFurthermore, combined with Question 1, if the testing set named **IID** is considered in-distribution, and the testing set named **Unseen struct (len>32)** is considered out-of-distribution, then why are the curves on these two testing sets so similar to each other?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6585/Reviewer_iDw5"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6585/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835052678,
        "cdate": 1698835052678,
        "tmdate": 1699636747982,
        "mdate": 1699636747982,
        "license": "CC BY 4.0",
        "version": 2
    }
]