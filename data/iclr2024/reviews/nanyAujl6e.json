[
    {
        "id": "PI1XxpT0f2",
        "forum": "nanyAujl6e",
        "replyto": "nanyAujl6e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_ayMs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_ayMs"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the out-of-distribution detection problem, which aims to precisely classify samples of known categories, and accurately discern samples of unknown categroies. To facilitate the recognition the out-of-distribution examples, a CLIP-based method is proposed, where a set of learnable negative prompts for each class are introduced. Promising results are obtained compared to existing out-of-distribution detection methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is clearly written and easy to follow. The weakness of the hand-crafted prompts is clearly interpreted and the motivation is reasonable.\n2. The proposed approach is simple and intuitive.\n3. Promising experimental results are achieved compared to existing OOD detection and prompt-based methods."
            },
            "weaknesses": {
                "value": "1. Meta-Net in Figure 6 is not introduced in the paper.\n2. The motivation and model design of this paper are similar to DualCoOp. The authors claim that the proposed method could learn negative prompts to capture negative features compared to DualCoOp, but there is no visual or quantitative evidence to verify this statement. Besides, the OOD detection performance of the DualCoOp design is not experimentally verified.\n3. Some existing OOD detection methods proposed in 2022 and 2023 are not compared or discussed[2][3][4].\n4. It seems that the authors have submitted this paper with an ICLR 2023 template.\n\n[1] Dualcoop: Fast adaptation to multi-label recognition with limited annotations.\n\n[2] Out-of-Distribution Detection with Deep Nearest Neighbors.\n\n[3] LINe: Out-of-Distribution Detection by Leveraging Important Neurons.\n\n[4] Decoupling MaxLogit for Out-of-Distribution Detection"
            },
            "questions": {
                "value": "In equation(9), the positive score and the negative score are obtained independently. If the obtained positive category is different from the negative category, could this lead to mistakenly recognizing a correct positive prediction as out-of-distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697964070664,
        "cdate": 1697964070664,
        "tmdate": 1699636112199,
        "mdate": 1699636112199,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MjGr3pVibz",
        "forum": "nanyAujl6e",
        "replyto": "nanyAujl6e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_ZZhi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_ZZhi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present a negative prompt tuning method with CLIP to improve the OOD performance. Specifically, the authors learn class-specific prompts for each category. A semantic orthogonality loss is also applied to encourage diverse negative prompts. The negative prompts are also considered in the evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed LSN method achieves good performance over baselines on various OOD benchmarks.\n2. The authors provide sufficient ablation studies to show the effectiveness of each proposed component.\n3. The proposed idea is simple and easy to understand."
            },
            "weaknesses": {
                "value": "1. This paper is related to negative learning or learning with complementary labels. The authors may consider adding some related discussion in the related work section.\n2. The proposed method may double the training and inference time with the negative prompts.\n3. I found a related work that the authors may add discussion in the related work section:\n\" How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models? , IJCV 2023.\""
            },
            "questions": {
                "value": "1. What's the ID dataset in Table 3?\n2. Why do CoOp/CoCoOp and CoOp/CoCoOp + LSN achieve the same ID results in Table 1 and Table 2?\n3. The ID results of CoOp/CoCoOp appear to be significantly lower than other baselines such as NPOS in Table 2. Can the authors explain some reasons?\n\nSome minor suggestions that do not affect my final rating:\n1. It is suggested to use `\\citep' rather than `\\cite' in the latex code\n2. Typo: 'we use we use' at the bottom of page 6"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698664466587,
        "cdate": 1698664466587,
        "tmdate": 1699636112123,
        "mdate": 1699636112123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k8iGBv0uZJ",
        "forum": "nanyAujl6e",
        "replyto": "nanyAujl6e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors perform OOD detection based on generic features learned from a large pre-trained language-vision model by matching the similarity between image features and features of learned positive prompts and negative prompts.\nThe core innovation of this paper is the proposed LSN module to learn a set of negative prompts for each ID category to help the network to comprehend the concept of \"not.\" They mine general negative features that are not present in a category but are present in all other categories by proposing a new loss in prompt learning. In the test, the MCM scores of the cosine similarity of positive prompts and negative prompts to image features are used as OOD detection metrics. Extensive experiments on various ood detection benchmarks have been conducted to demonstrate the effectiveness of the method proposed in this work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1\u3001This work utilizes the generic feature extraction capability of CLIP and does not need to finetune the image encoder and text encoder. It only needs to learn the appropriate positive and negative prompts by LSN for OOD detection. Therefore this method has high generality and low complexity.\n2\u3001SOTA performance is achieved in different benchmark experiments."
            },
            "weaknesses": {
                "value": "The overall prompt learning approach is still based on CoOp without a lot of innovation."
            },
            "questions": {
                "value": "1, This method is very dependent on the features learned by CLIP. If the features extracted by CLIP itself for some categories of images are not strongly discriminative, the effect of learning the prompts based on these features may be poor.\n2, The way to learn negative prompts is to mine the general negative features that each class of samples does not have but all other classes have, i.e., the negative classifier produces low activation values for that class and high activation values for other classes. Does this result in learning what are actually generic background features rather than general features that all other classes have?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1826/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_r91y"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698764342700,
        "cdate": 1698764342700,
        "tmdate": 1699636112043,
        "mdate": 1699636112043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NE4OI9xVNH",
        "forum": "nanyAujl6e",
        "replyto": "nanyAujl6e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes how to use CLIP for OOD detection with negated prompts that include the negation word 'not'. Using the learnable prompts embeddings, the method trains the model by freezing the CLIP encoders based on a contrastive loss. The proposed method shows improvement over the baseline CLIP for OOD detection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clearly written with descriptive visual figures.\n2. Experiments have been extensively conducted across a set of diverse datasets including small ones and large ones."
            },
            "weaknesses": {
                "value": "1. Comparison and related works to state-of-the-arts are missing (e.g., NNGuide [1], ASH [2], CLIPN [3], [4])\n2. The performance is too behind the state-of-the-art\n3. The main concept of the paper is too similar to CLIPN\n4. The performance improvement is very marginal\n\n[1]\n[2] \n[3] Wang, Hualiang, et al. \"Clipn for zero-shot ood detection: Teaching clip to say no.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n[4] Ming, Yifei, et al. \"Delving into out-of-distribution detection with vision-language representations.\" Advances in Neural Information Processing Systems 35 (2022): 35087-35102."
            },
            "questions": {
                "value": "I suggest the authors to properly address the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1826/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1826/Reviewer_Dyvx",
                    "ICLR.cc/2024/Conference/Submission1826/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1826/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835782256,
        "cdate": 1698835782256,
        "tmdate": 1700711584837,
        "mdate": 1700711584837,
        "license": "CC BY 4.0",
        "version": 2
    }
]