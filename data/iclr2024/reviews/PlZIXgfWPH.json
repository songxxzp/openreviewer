[
    {
        "id": "Ie8uVQmGsG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_NkWq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_NkWq"
        ],
        "forum": "PlZIXgfWPH",
        "replyto": "PlZIXgfWPH",
        "content": {
            "summary": {
                "value": "Hyperparameter Optimization (HPO) is a well-established approach to tuning ML algorithms on a given dataset. Although there has been more than a decade of research on HPO, the underlying optimization problem is still poorly understood, and only a few papers study the HPO landscape and its properties. The paper at hand sheds further light on HPO characteristics by applying fitness landscape analysis (FLA) and a visual inspection approach to many HPO problems. Besides providing a more thorough analysis than previous papers, the authors are the first to provide a better understanding of fidelity landscapes and how the landscape changes with respect to the dataset. Overall, the authors confirm that HPO landscapes are fairly benign (in particular in a local neighborhood around the optimum), are often stable with respect to fidelities (e.g., sub-sampled datasets and less epochs), and are also consistent across datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is a very valuable contribution to a better understanding of HPO. So far, many HPO approaches are developed based on rough intuitions, sometimes by visualization of 2 hyperparameter interactions, and thus a sound foundation for HPO is missing. The paper contributes to further closing this gap and will allow the development of even more tailored HPO approaches. \n\nThe authors propose an entire workflow on how to study HPO landscapes in a systematic and thorough manner. It encompasses the interpretation of the search space as a graph, the dimensionality reduction to allow for a 2 dim-visualization, the use of fitness landscape analysis (FLA) and landscape similarities between fidelities and datasets. \n\nA very important point is, furthermore, the thorough HPO data collection on 5 ML algorithms, each with more than ten thousand configurations and three / nine fidelity steps. This thorough data collection will allow further exploration and benchmarking of HPO. \n\nThe paper is overall very clear and easy to understand. (However, many typos are an issue \u2013 a spell checker is strongly recommended; also for the figures.)"
            },
            "weaknesses": {
                "value": "Although I like the main direction of the paper a lot, there are many problems in the details \u2013 partially I raised them as part of the questions below.\n\n1. Related work can be further improved. First, Xu et al. 2023 were not the first to treat HPO as a black box problem. I would be okay if you cite Bergstra and Bengio 2012 for that (although they might also not be the first ones). Second, Biedenkapp et al. [1] also proposed a 2-dim visualization of configuration spaces; a discussion on how the authors\u2019 new approach refers to that is missing. Third, I don\u2019t understand why Moosbauer et al. 2022 is cited for the nature of HP spaces. If the authors are unsure about what to cite for what, I recommend reading more surveys in detail.\n1. The workflow is inefficient in practice because it assumes that the entire (discretized) configuration was evaluated. It would be much more relevant if it could be directly applied to the output of any HPO approach.\n1. In view of the renowned paper by Bergstra and Bengio on random search and the low effective dimensionality of HPO spaces, I don\u2019t understand why the authors decided to use a grid sampling strategy nevertheless and thus do not efficiently sample all dimensions. The distance function could be a reason for that, but defining a distance function in a non-discretized space is not uncommon.\n1. The benchmarks heavily focus on tree-based algorithms. CNN is more of an outlier and the main paper discusses it also only very briefly (if at all). I would have hoped for a more diverse set of ML algorithms.\n1. HPO benchmark libraries such as HPOBench or YAHPO already provide ample data (also for multi-fidelity). I wonder why the authors decided to collect new HPO data and do not validate their findings on these established benchmarks.\n1. I wonder why the authors decided to use accuracy instead of balanced accuracy. I haven\u2019t checked all the datasets from this paper, but unbalanced datasets are fairly common and thus balanced accuracy (or similar metrics) would be the better choice.\n1. I\u2019m not convinced that the no free lunch theorem plays a role here since, as the authors argue themselves, the characteristics of HPO landscapes seem to be very similar s.t. in fact a single algorithm could be the best one. \n1. I\u2019m not convinced by the general multi-fidelity conclusion in the way the authors state it. In fact, it is easy to construct a benchmark based on DNNs with a constant learning rate as the only hyperparameter and show that multi-fidelity HPO can fail in this case (if not done very carefully). Overall, the wording of the conclusions needs to be much more careful.  \n1. The source code and the data are not part of the submission. The authors write in their paper that they will release the data. However, from my experience, it is of utmost importance to provide all of this for the reviews in an anonymized way.\n\n[1] Andre Biedenkapp, Joshua Marben, Marius Lindauer, Frank Hutter:\nCAVE: Configuration Assessment, Visualization and Evaluation. LION 2018: 115-130"
            },
            "questions": {
                "value": "* Portfolio approaches with complementary configurations (e.g., auto-sklearn 2.0 by Feurer at al.) are very famous and are constructed such that the configurations in the portfolio perform well on different datasets. If your conclusion is now that the area of well-performing configurations is consistent across datasets, I would also conclude that portfolio approaches should not be beneficial at all. One problem could be that the considered ML algorithms have this property, which might not be true for others; if that were true, it would partially invalidate your results. So, please comment on this contradiction between your results and previous results.\n* What is a unit of the color coding in Figure 1? If these are accuracy benchmarks, the values cannot go up to 6000 and beyond.\n* Why is the search space encoded as a ***directed*** graph? The neighborhood relationship goes in both directions.\n* UMAP has several assumptions on the underlying space. Do all these assumptions hold true in this case? \n* As you wrote, low effective dimensionality could have an impact on your evaluation. How would your results change if you first remove all unimportant hyperparameters?\n* At the AutoML conference 2023, there was a paper on studying HPO landscapes for AutoRL by Mohan et al. How does your paper relate to that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Reviewer_NkWq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3833/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697436033926,
        "cdate": 1697436033926,
        "tmdate": 1701010784253,
        "mdate": 1701010784253,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DVoiJAhizJ",
        "forum": "PlZIXgfWPH",
        "replyto": "PlZIXgfWPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_1UHx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_1UHx"
        ],
        "content": {
            "summary": {
                "value": "This paper performs a careful study on the HPs loss landscapes. It relies on modeling the hyperparameter configurations as a graph where every configuration is a node and an edge represents a similarity between two configurations. The authors conducted experiment on 5 search spaces and several datasets. Moreover, they compute several metrics such as autocorrelation, neutrality distance correlation, mean neutrality, no. local optima and mean basin size."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The topic addressed by this paper is extremely important and interesting, as HPO is a common task for practitioners and scientists. The insights gained in this work help to understand better the HP spaces and design better HP optimizers.\n- The method for evaluating the landscapes is interesting and uses helpful metrics.\n- The conclusions and findings are coherent with previous work assumptions, so it is unlikely that a procedural error happened in this research."
            },
            "weaknesses": {
                "value": "- The study focuses on simple models such as random forest and support vector machines, while there is only a neural network architecture type involved (CNN). Given the relevance of neural networks in the current world, this study should include more detailed comparisons on this matter.\n- Another common scenario is to create a broad space where both RF and SVM (algorithm and hyperparameter selection) are included. It is a missed opportunity to discuss how the loss structure is affected when optimizing both HP and algorithms.\n- There should be a stronger discussion on the impact of the findings. From my perspective, although the findings are interesting, they are not surprising. Aspects like the correlation of the loss among fidelities and datasets are well-known and they are exploited by common HPO methods. In other words, it is not clear how the findings in this work can improve the current methods for optimizing HPs."
            },
            "questions": {
                "value": "There are some interesting points that I would be very interested in understanding:\n1. How does the loss landscape change with the dataset size? Is there any available insight from this?\n\n2. How does the loss landscape look when I consider only HPs that affect the expressiveness/complexity of the model? My intuition is that these HPs built up a convex search space, with not local optima. Is this somehow supported in this study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Reviewer_1UHx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3833/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697902824826,
        "cdate": 1697902824826,
        "tmdate": 1699644285124,
        "mdate": 1699644285124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MgSsrpCYIV",
        "forum": "PlZIXgfWPH",
        "replyto": "PlZIXgfWPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_q8yg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_q8yg"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a hyperparameter optimization (HPO) loss landscape analysis pipeline. This pipeline encompasses visualization techniques, landscape characteristics like neutrality and smoothness, and introduced landscape similarity metrics.\nWith this pipeline they analyze five classical machine learning (ML) models across different datasets and fidelities. In particular they connect landscapes resulting from the train loss to the test loss.\nTheir main findings are that the hyperparameter (HP) loss landscapes are fairly smooth and  unimodal, training and test losses of an HP configuration are mostly consistent except for configurations with a low training loss and that low-fidelity landscapes are fairly similar to high-fidelity landscapes propertywise."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "### Originality\nThis paper is a significant leap in terms of hyperparameter loss landscapes analyses. While there are other existing papers and ideas (which the authors rightfully cite), this work represents a much broader, more detailed and nuanced study than the existing ones. Although they largely support the claims made in previous papers, having a broader study with a more extensive analysis pipeline certainly is an original contribution.\n\n### Quality\nThe quality of this work is mixed. While I appreciate the great description of the experimental results and that the authors provide an extensive appendix detailing most of the studies performed, the paper has some questionable parts (see weaknesses).\n\n### Clarity\nThe paper is generally structured in a clear and concise manner. Figures are easy to understand, but some important details are missing."
            },
            "weaknesses": {
                "value": "The work suffers from a set of problems regarding methodological choices in my opinion. \n\nIn particular, I see several weaknesses / questionable choices in the analysis pipeline: \n\n1) Page 4 / HPO Loss Landscape Construction: The assumption that the search space is a grid and thus countable / finite is quite a strong one that should at least be discussed by the authors, imo. Real-valued hyperparameters such as learning rate are simply represented by a set of values potentially hiding many local optima that are not found often in the analysis of the authors. Closely connected to this weakness is the problem I see in the distance function $d$ defined by the authors. Assigning a distance on the basis of the distance of two values in the search space grid instead of their true numerical distance bears the danger of completely losing track of their true numerical meaning. Moreover, if I were to rearrange the values in the grid such that they are no longer numerically ordered, the resulting neighborhood and thus graph would change - something that seems very odd to me. Similarly, I find it odd that the corresponding graph does not have improving edges between configurations differing only in one hyperparameter if their numerical values are not right next to each other in the search space grid. One may wonder whether it is sensible to have an edge between a configuration with a learning rate of 0.1 and 0.01 but not between 0.1 and 0.001, if both lead to improvements. The authors frame this encoding as a strength, which I disagree with. In particular, it is unclear to me to what degree this influences the final results of their findings as it might have an inherent smoothing effect caused by prior knowledge encoded via the search space. Most HPO tools do NOT follow such a discretization strategy such that I am not sure whether all of the insights gained here are really helpful for designing better HPO tools. Similarly, it is unclear what influence the hyperparameter of the framework defining when a neighbor is a neutral neighbor has on the results. What if this values was set to $2\\%$ or $5\\%$? Would the results look differently?\n\n2) Page 4 / Landscape Visualization: In the last sentence, the authors state that they apply a linear interpolation to generate a smooth surface. At the same time, they argue at the end of page 5 that the loss landscapes shown in Figure 3 (a) are \u201crelatively smooth\u201d. It is unclear whether this is an effect of the smoothing applied or would be the case anyway. Considering that the FLA metrics concerning smoothness show similar results, it will probably also apply without the smoothing step. Nevertheless, this seems a little questionable to me.\n\n3) Page 7/ Highly neutral; planar around the optimum: The authors state that they have been \u201cselective in the search space design\u201d, which is not further explained. They do list the search spaces in the appendix, but considering this comment, it seems that the authors took great care in choosing these search spaces which naturally greatly influences the results presented in the paper. Thus, the authors should comment on their approach for choosing the search space and on the consequences such as potential limitations of their analysis. They should also relate this to the setups of related analyses they cite. \n\n4) Limitations: In general I miss an elaboration on the limitations of the authors\u2019 analysis. For example\n    * Can conditional dependencies between hyperparameters be modeled in the framework? \n    * Do the embeddings account for the fact that for numerical hyperparameters (in principle) values between those given in the search grid can be chosen, but for categorical hyperparameters not? This is important as the analysis otherwise inherently assumes all hyperparameters to be categorical, (at least to a large extent). \n    * What hyperparameters for UMAP were set? This greatly influences the outcome of the analysis (see e.g. https://pair-code.github.io/understanding-umap/), but the concrete hyperparameters or how they were chosen are never mentioned. \n\nIt is also very unfortunate that no source code is provided since this makes the claim of the authors in the conclusion that their framework could be used to analyze other benchmarks a bit meaningless. \n\nFurthermore, the authors could highlight a little bit more that also other papers performed dimensionality reduction techniques to show how loss landscapes look like. I agree that the approach of using HOPE + UMAP for this has its pros and is novel, but easier approaches have been used before (see e.g. Q2 on page 5 in [A]).\n\nBesides the problems explained above, the paper suffers from some (easy to fix) inconsistencies and typographical errors. \n* Page 4 & 9: \u201ctunning\u201d -> tuning\n* Page 5: \u201c[...]under different scenarios, We apply[...] -> scenarios. We\n* Page 15: \u201cknoledge\u201d -> knowledge\n*Page 17: Paragraphs on HOPE and UMAP have quite some grammatical errors. \n* The references are largely inconsistent. For example, the first names of the first reference are not abbreviated, the ones of the second one are. This applies to many more.ArXiv papers are sometimes references as \u201carXiv preprent arXiv:...\u201d and sometimes as \u201cCoRR, abs\u2026\u201d. The paper cited for \u201cA survey of methods for automated algorithm configuration\u201d by Schede et al. is only the extended abstract, although a full version is published in JAIR: https://www.jair.org/index.php/jair/article/view/13676/26852\n\n[A] Sass, Ren\u00e9, et al. \"Deepcave: An interactive analysis tool for automated machine learning.\" arXiv preprint arXiv:2206.03493 (2022)."
            },
            "questions": {
                "value": "Since most of my questions are related to the problems I see in the methodological setup, I have listed them under weaknesses. However, I have some more questions here that are not connected to a weakness:\n* Can you comment about the consistency of landscapes across seeds?\n* From Figure 6 we can see that the correlation between test and train performance is very good for configurations with a bad training performance, but less so for those ones with good training performance. Even more importantly, it does not seem to be the case that performing some kind of early stopping would counter an overtuning effect. Do you have concrete suggestions how HPO tools should be changed based on these findings?\n* In the conclusion you mention that new HPO tools could be designed based on your findings. Do you have exemplary ideas? I wonder whether this is really the case since your findings largely are coherent with existing knowledge from smaller studies. \n\nI am more than happy to adjust my rating, if the authors do the following: \n* Clearly comment on the consequences of their design choices which I criticize under weaknesses in 1) - 3). In particular, I would at least like to know what limitations come with their design decisions, but ideally how that influences the results concretely. \n* Add a section on the limitations of their analysis (see critique point 4).\n\nOf course, I am also more than happy to increase my rating, if the authors or other reviewers point out factual mistakes in my review. To be very clear: I will raise my score to Accept, if I deem the problems 1) - 3) sufficiently addressed, as I think that the paper is solid except these problems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3833/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698397650483,
        "cdate": 1698397650483,
        "tmdate": 1699636341285,
        "mdate": 1699636341285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w7gtjWsU8I",
        "forum": "PlZIXgfWPH",
        "replyto": "PlZIXgfWPH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_N7RA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3833/Reviewer_N7RA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies HP loss landscape across several important scenarios: train v.s. test; fidelity; datasets and models. To conduct the analysis, the authors invented a new dimension reduction techniques for HP analysis: it first defines a DAG where the nodes are HPs and edges are \u201cimprovement edges\u201d so that an edge from v_i to is neighbor v_j (neighbor defined based on a distance function) means the loss of v_j is smaller than v_i. Then it applies the HOPE method to learn the node embedding and further uses UMAP to reduce the dimension to 2. Based on the resulting landscape, visual inspections and many metrics defined in the Fitness Landscape Analysis (FLA) literature can be used to characterize the landscape. Four observations, though many are not very surprising, are derived."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is mostly clearly written and the empirical analysis seems quite solid. It also proposed a new dimension reduction method for such analysis based on graph neural networks, which to the best of my knowledge, the first work doing so. They also use the metrics from the FLA field to characterize the landscape, which is also novel to me."
            },
            "weaknesses": {
                "value": "This work provides empirical evidence that many HPO practitioners already use, which limits the work\u2019s impact and significance. For example, the finding that HP landscapes are highly similar across datasets is the basis for hyperparameter transfer learning. The similarity across fidelity verifies again that the assumption of multi-fidelity HPO methods."
            },
            "questions": {
                "value": "What\u2019s the motivation of the proposed method in Section 2? What\u2019s the pros and cons compared to other related works? Why do the authors think the proposed method is better than the others such as Pushak & Hoos (2022)? Like mentioned in the section of \u201cNearly unimodal\u201d, the conclusion may indeed different based on what analysis method to use. If using the method from Pushak & Hoos (2022) in the same setting as in this paper, would the authors have the same conclusion? This may be a good experiment to cross verify the findings.\n\nI may have missed this. What is the space for quantifying the landscape characteristics? The low dimension embedding based on HOPE method, or the 2d after UMAP?\n\nIf many observations in Section 4.1 are based on the 2d landscape, then my following question will be relevant. In Figure 3, the landscape looks indeed smooth. But is it a result of using graph embeddings and UMAP, which smooth the landscape? Can it be different in the original higher dimensional space? Also can other properties of the landscape also due to the usage of graph embeddings and UMAP? I don\u2019t know much about high dimensional statistics and if there are some methods to quantify characteristics in the original HP space, that would be great. Otherwise, successful application of the observations derived from the paper to some HPO application could also in-directly verify the findings are relevant. For example, if the optimal tends to have a plateau, then the local search step in BO should not bring much improvement.\n\nSince the author mentioned \u201cHowever, when zooming into the top-10% regions, we find that the majority of our studied scenarios reveal low \u03b3-set similarities.\u201d, I think this should be highlighted in the paper because the prominent regions are the areas that people care about."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3833/Reviewer_N7RA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3833/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698604261966,
        "cdate": 1698604261966,
        "tmdate": 1700690471881,
        "mdate": 1700690471881,
        "license": "CC BY 4.0",
        "version": 2
    }
]