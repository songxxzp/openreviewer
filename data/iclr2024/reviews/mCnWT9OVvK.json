[
    {
        "id": "xoxsK0jCIS",
        "forum": "mCnWT9OVvK",
        "replyto": "mCnWT9OVvK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_kK3e"
        ],
        "content": {
            "summary": {
                "value": "This work analyzes 2 LLMs on the LFQA task using the RAG pattern. A superficial metric analysis reveals the RAG does change instrinsic text properties such as length and fluency but does not provided any sense of correctness. To investigate correctness and attribution the authors collected a small dataset of labeled question and answers with attributions. By labeling answer attributions the authors intend to evaluate how effective the LLMs are at attending to retrieved documents in the LFQA context. They arrive at conclusions that impact design choices for RAG-LLMs. However, some questions remain about the generality of the conclusions (due to a small dataset used and limited set of experiments). There are also potential shortcomings of the collected dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper performs some standard analyses comparing various RAG approaches with different search algorithms and LLMs. The conclusions point to some interesting properties of tThis paper performs some standard analyses comparing various RAG approaches with different search algorithms and LLMs. The conclusions point to some interesting properties of the investigated algorithms. Beyond this, collecting a dataset with attribution annotation and performing accompanying analysis on the resulting evaluation across the set of RAG algorithms provides potentially useful information to adopters of the proposed solutions of the investigated algorithms. Beyond this, collecting a dataset with attribution annotations allows them to perform a deeper analysis on how well the generated answers match up to the retrieved documents."
            },
            "weaknesses": {
                "value": "I do not think that the superficial level statistics provide much meaningful information about the RAG pattern in general or even these specific versions of it. It is self-evident that when provided with information to contextual the answer then generative language models have different linguistic properties. This has been studied before. \nAnother weakness is that, if I understand it correctly, then the dataset that was collected is specific to the algorithms used in this analysis. Since the answers are labeled this means that to apply this dataset to a new algorithm (or even a new generation/inference run) will require some machinery to transfer those labels. This can be challenging but I did not see a discussion of this process in the paper. So it is limited to only \"answer attribution\" methods. However, many approaches to this problem couple the answer and citation/attribution generation. Besides that it is rather on the small side of the datasets on this topic. \nI also find it somewhat surprising that the authors did not analysis the superficial statistics in light of the dataset (by filtering on attribution accuracy etc.)."
            },
            "questions": {
                "value": "What ways can the collected dataset be used in to improve RAG algorithms? It is not clear to me how to apply it to a specific algorithm but rather presents impressionistic suggestions about design patterns, some of which (ordering) are already common knowledge.\n\nWouldn't conditioning the SLS analysis on correct vs. incorrect results (possibly filtering with your dataset) provide more actionable information on the RAG for LFQA setup? In LFQA having the answers or some set of facts required to generate the answers should give you the ability to produce some normalized statistics, maybe this would be available through your dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698585962047,
        "cdate": 1698585962047,
        "tmdate": 1699637097942,
        "mdate": 1699637097942,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QWitOxq6A8",
        "forum": "mCnWT9OVvK",
        "replyto": "mCnWT9OVvK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
        ],
        "content": {
            "summary": {
                "value": "The paper studies how retrieval impacts answer generation for long-form question answering by presenting two controlled study settings: 1) fixing the LM and varying evidence documents; 2) fixing evidence documents and varying the LMs. Various attributes of generated answers and the attribution of generated answers to provided evidence documents are studied in this paper. A new dataset with human annotations to evaluate different answer attributions was created."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors provide an in-depth analysis of attribution with the newly annotated dataset. \n\n2. The story is well presented, and the motivation (Figure 1) is clear.\n\n3. The insights from attribution annotation results are pretty interesting."
            },
            "weaknesses": {
                "value": "1.\tWhile the paper demonstrates good motivation and understanding of the problem so-called long-form question answering, I have a different interpretation of the term \u201clong-form\u201d. I thought the problem is referring to \u201clong length/width form\u201d or \u201clong structured/semi-structured tables\u201d, which pose a greater challenge for current LLM-based retrieval systems. Therefore, I question whether \u201clong-form\u201d is an appropriate term to accurately define this problem. \n\n2.\tSince the tested dataset consists of a relatively small number of questions (271), it raises the question of why the entire dataset was not utilized for the experiments."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_o7Hh"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698587096564,
        "cdate": 1698587096564,
        "tmdate": 1699637097819,
        "mdate": 1699637097819,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z926pWABgB",
        "forum": "mCnWT9OVvK",
        "replyto": "mCnWT9OVvK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates how retrieval capabilities impact various models on long-form question answering tasks. It does so by:\n1. Investigating answer statistics of various (retrieval documents, models) pairs on ELI5.\n2. Collecting human annotations on the extent to which the answers are supported by retrieved evidence.\n3. Evaluating various methods for automatic attribution in the context of multi-document retrieval-augmented generation tasks. No method is at this time competitive with human annotation, but a T5-based attribution model shows the strongest scores among automated methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* Clarity: the paper states clearly its purpose and gives a wide overview of related work. It is easy to follow and it describes well its experimental setup.\n\n* Quality: the research is well executed, code and data are available in supplementary material. However, the paper does not seem to follow a strict scientific protocol: for instance, in section 4, the authors make a number of observations on the text generated by various experimental setups without connecting them to higher-level hypotheses that they could then test methodically.\n\n* Novelty: the annotated dataset as well as the evaluation of various models for multi-document attribution prediction are novel pieces of work.\n\n* Significance: as it stands, the paper does not seem to serve a well-identified purpose, and may not attract wide interest from the community as its insights are somewhat disconnected from what matters: the end-to-end human-perceived quality of these long-form question answering systems."
            },
            "weaknesses": {
                "value": "* The main findings of this work would deserve being stated more clearly. While the annotation of supporting sentences across multiple documents is of interest to the field, this is not the paper's main listed contribution. The paper makes a number of observations on how various retrieval-augmented LFQA systems behave, but without connecting them clearly to a consistent set of conclusions, or giving actionable guidance for researchers designing retrieval-augmented LFQA solutions."
            },
            "questions": {
                "value": "* Why is figure 4a a box plot? A common assumption for box plots is that it reflects independent, identically distributed samples. In this case, as each point reflects a different dataset, and the datasets are the same across models, this assumption does not seem to hold here.\n\n* What are the main actionable conclusions from your work that any researcher working on multi-document retrieval-augmented long-form question answering systems should know? For instance, \n  - what does Figure 3.(a) imply in terms of optimal ordering of documents presented to the LFQA system?\n  - does retrieving and using longer documents imply an improvement of end-to-end quality?\n  - how do various models handle different degrees of noise (irrelevant documents) in their context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8750/Reviewer_6bfG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740243137,
        "cdate": 1698740243137,
        "tmdate": 1699637097660,
        "mdate": 1699637097660,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Oy8csxWbX3",
        "forum": "mCnWT9OVvK",
        "replyto": "mCnWT9OVvK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_T5uy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8750/Reviewer_T5uy"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates retrieval-augmented language models (LMs) for long-form question answering (LFQA). By comparing answers from different LMs using the same evidence documents, the study analyzes the impact of retrieval augmentation. Emphasis is placed on how generated answers can be attributed to in-context evidence documents. The research provides insights into the behavior of LMs when using retrieval augmentation and reveals novel patterns in long text generation. The study uses questions from the ELI5 dataset and evaluates models like WebGPT, GPT-3, and Alpaca."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research evaluates off-the-shelf models for detecting attributions, offering a comparative perspective on their performance.\n- The research presents two controlled study settings to understand the impact of varying evidence documents and varying LMs, ensuring robustness in findings."
            },
            "weaknesses": {
                "value": "- While qualitative insights are valuable, an over-reliance on them without sufficient quantitative backing might be a weakness.\n- The off-the-shelf models that the authors compared are not comprehensive. I feel it's important to include GPT-4.\n- It's unclear what are the nontrivial takeaways from this empirical study."
            },
            "questions": {
                "value": "- We observed different behaviors across models like WebGPT, GPT-3, and Alpaca when provided with the same set of documents. What do you hypothesize as the underlying reasons for these differences?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8750/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698907826130,
        "cdate": 1698907826130,
        "tmdate": 1699637097545,
        "mdate": 1699637097545,
        "license": "CC BY 4.0",
        "version": 2
    }
]