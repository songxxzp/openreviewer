[
    {
        "id": "rPWtmAeooG",
        "forum": "3eFMnZ3N4J",
        "replyto": "3eFMnZ3N4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_TYx2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_TYx2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed Efficient-3DIM, a novel approach to single-image novel-view synthesis that aims to generate unseen perspectives of an object or scene from a limited set of input images. The proposed method reduces the training overhead to a manageable scale through several pragmatic strategies. Comprehensive experiments were conducted to demonstrate the efficiency and generalizability of the proposed method on common benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Efficient-3DIM differs from previous approaches by proposing a simple yet effective efficient training framework. This is achieved through several pragmatic strategies, including a crafted timestep sampling strategy, a superior 3D feature extractor, and an enhanced training scheme. \n\nThe crafted timestep sampling strategy reduces the number of timesteps required for training, while the superior 3D feature extractor improves the quality of the learned features. The enhanced training scheme includes a self-supervised Vision Transformer and a modified sampling strategy, which are evaluated and shown to be effective in generating photorealistic novel views. \n\nThe proposed strategies significantly accelerate the training process, reducing the total training time from 10 days to less than 1 day, while shown to be effective in generating photorealistic novel views."
            },
            "weaknesses": {
                "value": "The study employs the Objaverse dataset as the sole testbed, which contains 800k three-dimensional objects. While this is a substantial dataset, its diversity and representativeness could be a limitation. I wonder if the authors would be able to demonstrate similar results on ScanNet or MVImagenet as well. Additional results on real scenes would be valuable and welcome too.\n\nThe authors mentioned that each object in the dataset underwent a procedure where 12 viewpoints are sampled. Are all comparison methods adopting the same fixed number of viewpoints?\n\nThe work itself follows the line of works by (Watson et al., 2022) and (Liu et al., 2023b), and mainly studied training efficiency improvement. The contribution is solid yet not fully substantial."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632204016,
        "cdate": 1698632204016,
        "tmdate": 1699636331264,
        "mdate": 1699636331264,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BZkOdiGfOi",
        "forum": "3eFMnZ3N4J",
        "replyto": "3eFMnZ3N4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_FP6T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_FP6T"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the challenge of synthesizing novel views from a single image in computer vision. It introduces \"Efficient-3DiM,\" a framework designed to significantly reduce the training time and computational costs required to train such a model, achieving a training time reduction from 10 days to less than 1 day while maintaining efficiency and generalizability through innovative strategies like non-uniform timestep sampling and improved feature extraction."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This paper was developed to enhance the training efficiency of diffusion models for single-image novel view synthesis. Core Strategies include:\n- Revised Timestep Sampling: A novel strategy for selecting diffusion timesteps, and optimizing training.\n- Self-Supervised Vision Transformer: Integration of a self-supervised Vision Transformer to improve the incorporation of high-level 3D features better than CLIP.\n- Enhanced Training Paradigm: A refined training recipe that adopt low-precision training while addressing the numerical errors via extra layer normalization.\n\nAll strategies are grounded in motivating observations. When applied altogether, the speedup is quite significant: a 14x reduction in training time compared to the original zero 1-to-3 approach, enabling rapid iterations."
            },
            "weaknesses": {
                "value": "- Would this proposed approach be generalizable to accelerating training other image-to-3D models, such as Zero 1-to-3 and Syncdreamer? Why or why not?\n \n- The evaluation is solely conducted using the Objaverse dataset. Although this dataset is extensive and newly introduced, relying solely on a single dataset with potential biases and limited coverage could obscure any issues that the proposed method might have in the wild. It would be beneficial if the authors could also showcase results on additional datasets for a more comprehensive assessment."
            },
            "questions": {
                "value": "Please kindly refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750041448,
        "cdate": 1698750041448,
        "tmdate": 1699636331169,
        "mdate": 1699636331169,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dsPHMM7NZt",
        "forum": "3eFMnZ3N4J",
        "replyto": "3eFMnZ3N4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_fp2s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_fp2s"
        ],
        "content": {
            "summary": {
                "value": "Efficient-3DIM is an efficient framework for single-image novel view synthesis through diffusion models. The proposed method reduces the training time from 10 days to a single day while generating photorealistic and geometrically reasonable novel views."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n\nThe authors build their work on 3DIM and integrate three pivotal contributions: a modified sampling strategy departing from traditional uniform sampling, an integration of a self-supervised Vision Transformer replacing the conventional CLIP encoder, and an enhanced training paradigm. While all those steps taken are empirical, they each carry certain contextual novelty and together yield strong training performance:\nFor the modified time step sampling, although similar ideas were explored before, the authors take a new angle since the major phase of 3DIM\u2019s training is essentially characterized as a finetuning paradigm as the adopted novel-view synthesizer is initiated from a pre-trained text-to-image diffusion model.\nIncorporating multi-scale representations produced by the DINO-v2 encoder, in place of the CLIP encoder, significantly improves the dense prediction and correspondence.\nWhile the authors transit from full-precision to 16-bit mixed-precision training, they add another layer normalization before sending the DINO-v2 feature to the diffusion model, to mitigate the numerical errors."
            },
            "weaknesses": {
                "value": "Weaknesses:\n \n(1)   I do not fully understand why LN can mitigate numerical errors, in the third part of Efficient-3DIM\n \n(2)   The major goal of this work is to trim down the training time without spending more costs on the total training resources (e.g., taking large-batch via a distributed system). Could the authors elaborate on how the proposed method could be integrated with distributed training, and whether its speedup benefits may diminish in the (more scalable) distributed training setup?\n \n(3)   Figure 8: I need help seeing how Zero 1-to-3 falls short of producing multi-view consistent visual outputs. All displayed results have valid visual quality to me.\n \n(4)   Minor: \u201cneurallift-360\u201d paper was incorrectly cited twice in reference."
            },
            "questions": {
                "value": "Please refer to the Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Reviewer_fp2s"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798038759,
        "cdate": 1698798038759,
        "tmdate": 1700689325415,
        "mdate": 1700689325415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NMkQzpgk0L",
        "forum": "3eFMnZ3N4J",
        "replyto": "3eFMnZ3N4J",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_mro2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3749/Reviewer_mro2"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce Efficient-3DiM to accelerate diffusion models for single-image novel view synthesis, such as Zero123. Specifically, they employ a crafted timestep sampling strategy, a superior 3D feature extractor (DINO-v2), and an enhanced training scheme. Experimental results demonstrate that the proposed method could retain the performance metrics of the baseline but accomplish this with a remarkable 10x speed increase."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I really like the idea of integrating a self-supervised Vision Transformer for image conditions since the clip image feature only contains high-level semantic meaning. Figure 4 and Figure 5 also showcase the superiority of DINO-v2 over CLIP.\n2. An in-depth and smart analysis is provided for the denoising process, and then the author proposes to sample more for larger timesteps to learn geometry, which accelerates the training process."
            },
            "weaknesses": {
                "value": "1. I am not persuaded by the motivation from comparing training image classifiers and generative models, which are not comparable. Moreover, we usually treat Zero123 as a foundation model which does not need retraining. Under this circumstance, I think the value of the proposed Efficient-3DiM diminishes.\n2. The section \"ENHANCED TRAINING PARADIGM\" contains several well-known tricks, such as mix-precision training. I would like to suggest the authors should not emphasize this too much in their contribution.\n3. I expect the proposed method with a more advanced 3D feature extractor to achieve better performance than Zero123. It is suggested to apply 3D reconstruction (e.g., Neus) on the generated views to compare Efficient-3Dim and Zero123.\n4. Could you elaborate more on how to \"conduct several different spatial interpolation processing\"? And why do you only inject the features to the encoder of the UNet denoiser? I also suggest including more details on how to conduct feature amalgamation (e.g., feature shape and resolution)."
            },
            "questions": {
                "value": "1. It is suggested to include more related work for \"Novel View Synthesis from a Single Image\", such as [a-d]\n\n[a] Geometry-Free View Synthesis: Transformers and no 3D Priors. ICCV 2021. \n\n[b] Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image. CVPR 2022.\n\n[c] SynSin: End-to-end View Synthesis from a Single Image. CVPR 2020.\n\n[d] PixelSynth: Generating a 3D-Consistent Experience from a Single Image. ICCV 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no ethics concerns."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3749/Reviewer_mro2"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3749/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826017209,
        "cdate": 1698826017209,
        "tmdate": 1699636330952,
        "mdate": 1699636330952,
        "license": "CC BY 4.0",
        "version": 2
    }
]