[
    {
        "id": "bbKys3Jw8b",
        "forum": "O2jyuo89CK",
        "replyto": "O2jyuo89CK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_LS4E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_LS4E"
        ],
        "content": {
            "summary": {
                "value": "The idea is to model a vector drawing as a _set_ of strokes.\nDrawings are created by a diffusion process to create a \u201cstroke cloud\u201d.\nThe strokes are represented in a low-dimensional latent space, making them conditionally independent.\nThe paper tackles an interesting problem in a novel and principled way.\nIt produces reasonable results.\nIt is of publishable quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Introduction:\nDifferentiates between vector and pixel representations, and claims the former is pressed (somehow)\non creativity while the latter is based on perception. These claims I have weak sympathy with, but\nwhich I don\u2019t fully agree with (surely people draw with lines/vector because that is how they perceive\nthe world). The are made without any citation into either the perceptual or artistic / art-history literature.\nAnd they pervade the first part of the paper, making slightly uncomfortable reading for me - feeling that\nthe argument made may even mislead. I\u2019d like to see it changed, but given my greater sympathy towards\nthe use of vectors over pixels, it\u2019s not something I will insist upon other than \u201cthink again about the argument\u201d.\n\nRelated Works:\nThe paper touches upon only a very small fraction of the available literature, and fails to cite any paper prior to 2015 - even though there is a large body of work (see the non-photorealistic rendering literature).\nMore on this later.\n\nMethod\nThe method is to represent a drawing with a set of strokes, the exact number does not matter.\nI very much like the fact the authors acknowledge strokes are not independent, but use \nDe-Finetti\u2019s Theorem of Exchangeability - which I am not familiar with (so thanks for the introduction)\nso that they are conditionally independent.\n\nI am less convinced by the use of quadratic curves to represent strokes. Strokes can be long\nand very complex. Example - a drawing of curly hair may use long, looped strokes. And strokes vary \nin width, media density and much more. I understand this is a first step - but some text that acknowledges\nthe very severe limitations the method operates under is, i think necessary so that the contribution can\nbe more fairly understood.\n\nIt\u2019s not clear to me why the generator produces a face (in the same view angle, or its mirror, and\nwith the same crop window). This again is very limiting.\n\nResults:\nThe authors make no experiments at all. This is a significant weakness in the paper.\nRather they show some images that have been generated, and then claim the images look good.\nUnfortunately for the authors, others (like me) have a different position.\nI am not convinced the drawings are \u201chigh quality\u201d as claimed.\nEspecially when compared to the NPR work my view is that output produced in recent years is low quality.\nAnd no work I have seen in comparable in quality to human art.\n\nThe fact drawing are usually made of some actual thing, like a face or a dog.\nThe paper never mentions how to control the output to direct it to a particular noun class.\n(Does the noun class impact the embedding into latent space? I guess so - it depends on S)\nAll the images are of faces, and all in \u201cmanga\u201d style.\nThis means the paper is not clear on its generality.\nDo you have examples of other noun classes?\n\nConclusion:\nI found the conclusion rather limited. I certainly disagree with the claim that \n  \u201cthe primary limitation lies in the probabilistic nature of the reconstruction process\u201d.\nIn fact the primary limitation is much more likely to be that the system makes no use of\nsemantic information, other than possibly implicitly via training. It is this issue, rather than any\nother, that has constrained progress in this area.\n\nSummary:\nI enjoyed the paper, in part because of the controversial claims it makes, but mostly because it\ntakes on a difficult problem in an imaginative way. That said, I feel the authors would do themselves and\nthe field much great justice if they were to resist their claims. I strongly recommend looking at some\nreal drawings, ideally \u201cin the flesh\u201d - and not just manga images of 3/4 faces, heavily cropped, drawn\nwith short quadratic curves of uniform width and density, on a perfectly flat surface.\n\nThe paper lacks any experiment, which was once common but is far less so now. One obvious\nexperiment is to conduct some kind of Turing test, of which there are many variants in the NPR/NST\nliterature (especially the more recent). As it stands the paper lack rigour - a rigour which I suspect would\nlead the authors to question some of their more controversial (for me) claims.\n\nThe paper makes a contribution and is publishable."
            },
            "weaknesses": {
                "value": "Please see above."
            },
            "questions": {
                "value": "How well do you you expect the system to generalise to?\n* other objects\n* other points of view\n* other styles\n* the true complexity of real strokes\n\nWhat citable evidence do you have that vectors relate to creativity and pixels to perception?\n\nWhy did you conduct no experiment?\n\nWhy have you not cited any paper prior to 2015, when there is plenty of relevant work?\n\nWhat is your defence fot the claim that \"the primary limitation lies in the probabilistic nature of the reconstruction process\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Reviewer_LS4E"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682011738,
        "cdate": 1698682011738,
        "tmdate": 1699636056404,
        "mdate": 1699636056404,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E0rT9kFeXV",
        "forum": "O2jyuo89CK",
        "replyto": "O2jyuo89CK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_FQwM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_FQwM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method using Diffusion Models and  Set Transformer for generating sketchs of Anime girls in vector format.\nThe training data is obtained by transfering raster images into vector curves.\nThe generation module contain two parts, the Stroke cloud Representation Module (SRM) and the Latent Stroke cloud Generator (LSG). \nThe LSG module is a diffusion model, which serves to sample latent codes. Each latent code corresponds to a sketch result. \nThe SRM module contain two parts:\nThe first part is a Set Transformer, which serves as an encorder to encode training datas into latent codes.\nThe second part is a diffusion model, which serves as an decorder to generate sketch strokes conditioned on the latent code."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of applying the Set Transformer to encode the stroke data is the most significant advantage of this paper. The nature of the Set Transformer theoretically guarantees permutation-invariant, so that all the strokes in one sketch can be encoded into a latent code to represent the sketch without worrying about the order of the strokes. This encoder also works even if the stroke number varies among the sketches in the dataset.\n\n2. Also thanks to the Set Transformer, the decoder of the SRM module can sample arbitrary numbers of strokes to constitute a sketch.\n\n3.  Although this paper focuses on the topic of sketch generation. I see the potential of this method to be applied to other artistic styles such as oil painting, just needs to change the stroke model and the dataset."
            },
            "weaknesses": {
                "value": "1. I think the main weakness of this paper is the poor visual quality, which is far from artistic application. And the stroke design of the Bezier curve is too simple.  Could the Bezier curve contain more parameters such as width and transparency?\n\n2. Lack of comparison. I do recognize that there may be no methods similar to your technique route. But you should compare with at least one method related to the topic of sketch generation, even though there may be raster-based such as GANs. \n\n3. There seems to be a flaw in the experiment of section 4.4, figure 8. I can see the sketches corresponding to the interpolated results in Figure 8 (the middle ones) are obviously not in the correct domain. This may be because your interpolation function does not fit this distribution (and I didn't see your interpolation function), which leads to the interpolated latent codes are not on their true distribution\uff08you can analogy the manifold of Swiss Roll)."
            },
            "questions": {
                "value": "This question is just for discussion (doesn't affect my rating):\nHow do you critique the technique route that transfers raster images into vectors? Just as your method of establishing your dataset.\nI mean, people can use raster image generation tools such as Stable Diffusion to generate a raster image, and then transfer it into SVG format (the recent work of VectorFusion follows this idea). Compared with this technique route, what's the advantage of your technique route? Or in other words, do you think directly generating vectors is more prospoective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1294/Reviewer_FQwM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725633514,
        "cdate": 1698725633514,
        "tmdate": 1699636056333,
        "mdate": 1699636056333,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UOppnhceGF",
        "forum": "O2jyuo89CK",
        "replyto": "O2jyuo89CK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_DN8s"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1294/Reviewer_DN8s"
        ],
        "content": {
            "summary": {
                "value": "The paper purports to present a method for generating _vector representation of complex sketches_.\nThe problem is important and has the following challenges:\n1. most work on generative modeling focuses on images, so vector representation is underrepresented.\n2. the number of _strokes_ within each sketch is variable.\n3. autoregressive methods using a sequence representation fail to scale up.\n4. the sequence representation is sensitive to the ordering of strokes.\n\n---\n\nThe claimed contributions are:\n1. proposing the first method for generating _highly complex drawings_.\n2. using a _set representation_ instead of a sequence representation.\n\n---\n\nThe paper:  \n1. (MAJOR) argues that a set representation is is theoretically possible because  of  `De Finetti General Representation Theorem` which states that for an `exchangeable` set with a pdf for each of it's constituent sets (one realization of the permutation), there exists a pdf that describes all the permutations in the limit.\n2. (MAJOR) presents a practical way to construct this permutation-invariant pdf, so that we can sample sketches from this distribution tractably.\n3. (MINOR) presents a new dataset called `Anime-Vec10k` which which is significanlty more complex than existing datasets (notably QuickDraw!)\n\n---\n\nThe method used to achieve the goal of  \"generating _vector representation of complex sketches_\" is roughly the following:\n1. Consolidate image into sets of bezier-curves.\n2. Use a Set-Encoder to encode the stroke-set to get a latent.\n3. Use a VAE loss to regularize the space of latents.\n4. Use an MLP based conditioning denoising network to get from (500/1000) _random_ strokes to the actual strokes.\n5. The important point in point 4 is that **each stroke is considered iid**."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**The problem setting is valuable to the community.** \n\n---\n\nWhile diffusion models have been providing very high quality generative models in the raster world, they have not made as much of a splash in the vector world. So I commend the authors for tackling this problem.\n\n**Very good exposition.**\n\n---\n\nThe paper is very easy to follow. While the abstract could possibly use some more technical details. Everything from the introduction on flows logically. The introduction is superb, even though it spends a bit more time describing why `creational` representations matter than I would have liked. The introduction still sets up the problem well - the problem with sequences (ordering and non-scalability of the autoregressive approach) and the problems with using a set approach (variable and unknown cardinality). The related work section is quite comprehensive to me. I would still like to suggest two references which might be interesting to the reader in the weaknesses section.\n\n**The core idea is simple to implement**\n\n---\n\nThe whole paper is based on simple building blocks, so any (re-)implementation should be easy."
            },
            "weaknesses": {
                "value": "While I like the paper, in its current form the paper has many weaknesses:\n\n**(MAJOR): Sampling issues**:\n\n---\n\nThe authors clearly point out that the number of strokes is an issue, but simply brush it away by saying we use 1000 strokes.\nThis is a glaring flaw. What if the sketch is simpler? What happens to the other strokes? Are they duplicated? \n\nThe discussion in Appendix E is is simply not enough to provide a solution.\n\n**(MAJOR): Matching issues/ iid assumption of strokes **:\n\n---\n\nRelated to sampling is the issue of matching. In the diffusion model equation (3), you simply decompose the sketch into iid strokes. This is a **VERY** strong assumption. I do not see how during sampling, one cannot get a degenerate solution of just repetetively denoising into the same stroke. There is no question asked about how the iid sampling affects the proposed pipeline.\n\nIn the same way, how is the sequence $\\mathbf{s}$ generated at training time? Do you let the randomness of the random strokes take care of matching the proper final stroke? Why is there no hungarian matching? Assuming normalized coordinates in $[-1, 1] \\times [1, 1]$ , how does it make sense for the denoising network to take a random stroke at (-1, -1) and try to denoise it to (1,1), instead of just recognizing (with hungarian matching) or simple euclidean distance that it is much easier to denoise the stroke closer to the final location?\n\n**(MAJOR):  Sequence representation/Sequence lengths**\n\n---\n\nIt is not clear at a first glance (unless I am wrong) that the paper does the following:\n1. Takes in a **variable** number of input strokes\n2. Generates (denoises) a **fixed** (1000) number of output strokes.\n\nThe motivating factor of the paper, and the way the paper is currently written suggest that the model is capable of generating a variable number of strokes. It seems that is not possible\n\n**(MAJOR) No quantitative results**\n\n---\n\nI said previously that the paper has missing citations. I will mention the actual links later but describe them here:\n\nThere has been recent work (<1.5years) in the autoregressive generation area. The work is based on how to make generation permutation invariant which is exactly the problem being tackled here. There are two papers in this area: [Paschalidou] and [Para]. [Paschalidou] use a learnable query vector that looks at the previously generated sequence and predicts the next one. [Para] uses a set encoder and a sequence decoder with mask tokens to perform controled generation.\n\nThe authors do not cite these papers. The authors have no baselines as they (rightfully) claim that previous work does not scale - but there [Paschalidou] has code available and it would be easily adaptable to their current training regime\n\n[code](https://github.com/nv-tlabs/atiss)\n\nAll you have to do is make each token the sum of its control point embeddings! And then introduce some form of conditioning - either a single condition token or an encoder as done in [Para].\n\nThis should significantly strengthen the paper - we see exactly how slow and underwhelming the other methods are, how slow to sample, what the FID is, and qualitative results as well. I would really like to see those result\n\n**(MINOR)  Few qualitative results**\n\n---\n\nWhile there are a decent number of qualitative results in the paper already - the dataset itslef is small - 10k samples. I would request the authors to **check for overfitting** by also visualziing the closest training set example to each generated sample - this could be in the rgb space as well as some perceptual space - look at the sketch retrieval literature or just VGG features.\n\nAnother thing that will help both analyze the dataset quality and the generation quality is to have a big 10x10 grid from both sets (trainig and generation) somehwere in the paper\n\n**Missing citations**\n\nThe missing citations are \n1. @Inproceedings{Paschalidou2021NEURIPS,\n  author = {Despoina Paschalidou and Amlan Kar and Maria Shugrina and Karsten Kreis and Andreas Geiger and Sanja Fidler},\n  title = {ATISS: Autoregressive Transformers for Indoor Scene Synthesis},\n  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},\n  year = {2021}\n}\n2. @inproceedings{10.1145/3588432.3591561,\nauthor = {Para, Wamiq Reyaz and Guerrero, Paul and Mitra, Niloy and Wonka, Peter},\ntitle = {COFS: COntrollable Furniture Layout Synthesis},\nyear = {2023},\nseries = {SIGGRAPH '23}\n}"
            },
            "questions": {
                "value": "I already asked most questions.\n\n1. Figure 5: Why does the DDPM sampler seem to have more strokes than the DDIM sampler? or are the strokes just placed closer together in the DDIM sample?\n\n2. Do you plan to release the dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791620869,
        "cdate": 1698791620869,
        "tmdate": 1699636056228,
        "mdate": 1699636056228,
        "license": "CC BY 4.0",
        "version": 2
    }
]