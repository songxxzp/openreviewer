[
    {
        "id": "2tssLv83R5",
        "forum": "NkmJotfL42",
        "replyto": "NkmJotfL42",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_Hb6n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_Hb6n"
        ],
        "content": {
            "summary": {
                "value": "This paper presents several novel results regarding the non-existence of tight generalization bound in the over-parameterized setting. In particular, this paper proves that a tight generalization bound cannot exist without making assumption about the distribution of training data. In the case where no assumption is made about the learning algorithm, the authors show a much stronger version \"no free lunch\" that any generalization bound is not tight on a large number of (Algorithm, Distribution) pairs. And in the case where we are concerned with a specific algorithm family, then the authors show that there is a inherent trade-off between the tightness of generalization bound and the quality of the learned model."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I find the results of this paper be very impactful. Understanding generalization properties in the over-parameterized regime is of great interest in recent literature and this paper basically shows that such analysis is impossible (both theoretically and practically) without assuming specific properties about the data distribution. This in turn shows that great number of existing bounds in the literature are not useful and any future work should direct to data-dependent bounds.\n\nThe authors did a good job by putting their results into different cases that are intuitive on a high level. And I find Table 1 to be very clean.\n\nSection 2 is well-written and gets the main points across without being entangled with the details.\n\nThe proofs are generally clean and easy to understand"
            },
            "weaknesses": {
                "value": "I find the formal results stated in Sections 5 and 6 to be extremely difficult to follow. While the informal statements in Section 2 are understandably vague, Sections 5 and 6 failed to clarify my confusions from Section 2. I think this is due to two issues:\n\n1. Section 4 did a poor job at explaining the formal notation. I feel that Definitions 1 and 2 are not particularly well-motivated (more on this later). And shoving everything else into the Appendix does not help either.\n\n2. Certain points in the intro were not explained properly in later sections. a) The term \"vacuous\" was never formally defined, b) the connection between tightness of generalization bound (eq 1) and the notion of estimability is also not discussed in depth (why are they equivalent? I know the argument is not hard, but this is provides important contexts for the main theorems).\n\n3. The theorem statements are pretty mouthful themselves and except for Theorem 2, the authors did not offer discussions that helps with parsing the theorem statements.\n\nNext, some *major gaps* I found in the paper:\n\n4. The definition of over-parameterizaton (Definition 2) in this paper is not standard and my impression is that they are phrased to make the proofs simpler. While Definition 2 does intuitively fit the idea of over-parameterization, I feel strongly that the author should add: a) detailed discussions on why this definition is consistent with the standard setting in the literature, b) examples.\n\n5. Similar to the previous point, in Theorem 3, the condition on TV distance is unmotivated and seems to only exist to make the problem easier.\n\n6. In the proof of Theorem 1, the authors did not show the existence of Bayes-like Random ERM.\n\nA few minor comments regarding the proofs (I did not check Appendix G or H):\n\nPage 18: the theorem statement is about Theorem 2, *not* 1.\n\nPage 19: the final sentence should start with \"the second equality holds\"\n\nPage 20: the result \"Theorem 1 in Angel & Spinka (2021)\" is just a standard fact on the existence of coupling, so it is better to just say that directly.\n\nPage 20: the big equation block looks atrocious, please left align the lines and use indentation to make the + sign on the second line more visible. \n\nPage 20: Please define what event $B$ is before that big equation block. Also in the definition of $B$, the final inequality $L_{D_{I_1}}(A(S_1)) \\ge \\alpha$ is missing its RHS."
            },
            "questions": {
                "value": "In addition to all of my concerns above, how is over-parameterization used in the argument between Definition 3 and Theorem 1?\n\nOverall, I feel that some restructuring is necessary for Sections 4-6. The claims in this paper are very interesting and impactful, but I find them to be unnecessarily difficult to understand."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Reviewer_Hb6n",
                    "ICLR.cc/2024/Conference/Submission2576/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2576/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698126955567,
        "cdate": 1698126955567,
        "tmdate": 1700714887816,
        "mdate": 1700714887816,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "613vLvvTFC",
        "forum": "NkmJotfL42",
        "replyto": "NkmJotfL42",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_bL1w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_bL1w"
        ],
        "content": {
            "summary": {
                "value": "This paper provides various quantitative results that demonstrate that in an \"overparameterized setting\", generalization bounds cannot be tight unless they depend on the structure of the sampling distribution: if a generalization bound applies indiscriminately to all possible distributions, it cannot be tight for the result of a successful algorithm in the overparametrized setting (which is defined as a setting where there is not enough data for there to exist an algorithm which will perform ($\\alpha$-) well with high probability (1-$\\beta$) over a random draw of a realizable distribution. In a sense, the results can be interpreted as a quantitative version of the contrapositive of the fundamental lemma of statistical learning theory (which says that if a function class is estimable, it is learnable). \nSeveral results (including theorems 2 and 4) study specific toy settings where the overparmetrization can be more concretely described (e.g. the number of samples is less than VC dimension or the actual dimension of the space) and the statement holds thanks to an assumption that the marginal distribution over the input is uniform. These results are used to argue that the only type of generalization bounds that can correctly explain the success of Machine Learning models such as neural networks in the overparametrized setting must exhibit dependence on the sampling distribution. \n\n\n\nMore precisely\n\nTheorem 1 states that if we consider a function class associated to an classification problem with the 0-1 loss and a finite set of realizable distributions, as long as the problem is overparametrized (i.e., not reliably learnable over most distributions), then there exists an ERM algorithm whose error cannot be reliably estimated either.\n\nTheorem 2 shows that if the number of samples is less than the O(VC dimension) and the sampling distribution over the inputs is a uniform distribution over a finite set (which is shattered by the hypothesis set), then the class is not estimable. \n\nTheorem 3 shows that if a function class and a set of candidate ground truth distributions can be split into two parts, each realizable with respect to each other, and if the total variation distance between the resulting mixture distributions over the samples is not close to 1, then no estimator can work uniformly well for all algorithms with high probability. The proof heavily relies on the assumption that the total variation is not too high and a coupling argument from [2].\n\nTheorem 4 concerns the learning problem of learning linear maps over $F^d$ where $F$ is the finite field of size $q$. Here, similarly to Theorem 2, it is assumed that the inputs are drawn from a uniform distribution, which allows one to define the overparametrization more concretely ($n<<d$). One considers the class of algorithms which \"are biased towards a given set of subspaces\" (i.e. for all $n$, there exists a subspace of dimension $n$ such that the algorithm always outputs a consistent hypothesis from that space if there exists one).  This class is shown not to be estimable. The proof relies on several elegant combinatorial arguments together with existing results form [1] concerning the proportion of matrices over a finite field which have a given rank. \n\nTheorem 5 specifically improves the rate (in terms of the failure probabilities) for the case where $q=2$. \n\n\nReferences:\n\n[1] IAN F. BLAKE AND CHRIS STUDHOLME, PROPERTIES OF RANDOM MATRICES AND APPLICATIONS, 2006\n\n[2] Omer Angel and Yinon Spinka. Pairwise optimal coupling of multiple random variables, 2021. (Arxiv)"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Understanding generalization bounds at a fundamental level is an extremely important and popular topic, and this paper makes a nontrivial contribution in this direction. \n\n2. All the formal results and proofs are sound and reasonably well written. \n\n3. Some of the **proofs are nontrivial**, especially the **proof of Theorem 4**, which is quite impressive and relies on an ingenious splitting into many different situations depending on the rank of the design matrix and whether or not the first unit vector is in the span of its rows. **This is a solid maths paper.** \n\n4. The main claim of the paper, if appropriately toned down, is an interesting variation on known observations that may hitherto never been individually explained in such detail. This makes the paper highly worthy of publication. \n\n5. It is very nice that the relationship with existing works such as [5] is explained."
            },
            "weaknesses": {
                "value": "The paper makes sensational claims that lack nuance: in the main paper and in extensive discussions in the appendix, the paper uses the argument derived from Theorems 1 to 3 to explain that most existing generalization analyses are invalid or at least \"lacking\" since they \"do not take the distribution into account\". \n\nAs far as I am concerned, there are several serious problems with this claim which can be summarized as follows:\n\n1. The general argument is partially valid, but not as strongly as the authors claim:  it applies to any \"overparametrized bound that doesn't depend on the structure of the sampling distribution\". At an abstract level, this description very loosely \"can be argued or conjectured to apply to a large part of the existing literature. However, it doesn't strictly or objectively apply to almost any of the existing literature. \n\n2. The part of the paper's general argument which is valid is not completely novel. Although all the theoretical results appear novel and are indeed interesting, once they are compressed into the authors' high-level statement that \"a bound cannot be tight if it works uniformly over all distributions and the setting is overparametrized\", they no longer consist in a novel idea. Theorem 1 can be summarised as saying \"not learnable implies not estimable\", which is the contrapositive of the fundamental lemma of SLT which says that \"estimable function classes are learnable via empirical risk minimization\". The novel component of the results consists in turning this result into a quantitative version where a distribution over the set of realizable distributions is considered. \n\n3. There are plenty of papers that actively take the sampling distribution into account in their analysis and are not even cited here. The most obvious examples would be [3] and the NTK literature [4]. \n\n\n\n\nMore detailed explanation of point 1 above:\n\nAll theorems presented rely on the assumption that the problem is \"in the overparametrized setting\", which is defined in a very strict way that goes very far beyond saying that the number of parameters is greater than the number of samples and makes the abstract message of the theorems (theorems 1 and 3) approach a tautology. By definition, the authors assume that there is not enough data to find an algorithm that will reliably estimate most sampling distributions considered. This is only a short step from directly assuming that \"any generalization bound which only relies on the sampling distribution is vacuous\". All the concrete examples given in Theorems 2 and 4 make use of unrealistic assumptions about the uniformity of the sampling distribution. \n\nI understand that this does not fully invalidate the authors' point because it does show that no bound that relies on a \"fixed\" hypothesis class definition can be uniformly tight for all distributions: the authors' argument that \"for a bound to be nonvacuous in the overparametrized regime, it must be dependent on the sampling distribution\" is correct, but I feel like the authors catastrophically overstate the extent to which this directly invalidates any of the existing literature. \n\nWhich bounds in the existing literature are really \"in the overparametrized setting\"? Which of them really \"do not depend on the sampling distribution\"?\n\n\nThe only example of existing bounds that I can think of where it is genuinely known for certain that one is in the overparametrized setting would be parameter counting bounds (e.g. [6]). As far as I understand, even early norm-based uniform convergence bounds such as those of [7,12] cannot technically be proved to be \"in the overparametrized setting\" except by directly showing that they are vacuous, which makes the argument circular.  Likewise, although the idea that existing bounds such as margin bounds (e.g. [7]) \"do not take the distribution into account\" makes some sense, it is not technically true since the margin depends on the data: the idea that generalization bounds for neural networks in the setting where there are more parameters than samples must rely on the sampling distribution can arguably be placed as early as 2017 with the advent of norm-based bounds (this is reflected in the text of the introduction of [7]).  Data dependency is also given an increasingly important role in many more modern results such as those of [8,9], which replace the product of spectral norms in the bound by an empirical analog, and later in [10,11], which rely on empirical estimates of the norms of the activations in their analysis. Understandably, none of these bounds are traditionally considered to be in the category of \"data dependent\" bounds (because the data-dependency isn't \"strong\" enough), but none of the theorems of the present paper actually apply to any of them. This means the argument in the paper, although valid at an abstract/intuitive level, should not be presented as mathematically invalidating almost any existing works. Beyond the above works, **the results in [3,4] (including the NTK literature)** certainly take data dependency to the next level and arguably **provide a genuinely satisfying answer to the issue touched on in the present work**. They should be discussed. Note also that basically any bound that relies on the rank of the weight matrices or a similar quantity such as the sparsity of the trained connections would be non trivially using the sampling distribution (Cf also [15])\n\n\nIn the current version of the paper, on page 13, **margin bounds are presented as** part of examples of bounds that are **\"distribution independent\", which is quite disputable**.  The paper also similarly presents a **blanket dismissal of so-called \"Rademacher bounds\" on page 13**. Almost every bound, including actively distribution-dependent bounds such as [3,4] relies on Rademacher arguments in some part of the proof. Given the current nature of the reviewing process and the hype-driven personality of the community, publishing/highlighting the current version of the paper would put a lot of legitimate future research at high risk of being unfairly dismissed. Overall, the main paper and appendices B,C and D should be somewhat toned down.  \n\nIt would also be nice to describe the relationship between the current work and the benign overfitting literature [13,14]\n\n\n\n\n=====================Minor errors/issues of a mathematical/presentation nature=======\n\n\nStatement of Theorem 3 on Page 8: I think the authors meant to write $S_0~D^n_{I_{0}}$... before \"where $I_0=...$\" rather than the other way around. \n\nIn Lemma 1, the symbol represented by a small rhombus presumably refers to function composition but is not defined. \n\n\nIn the statement of Theorem 5, the fact that $q=2$ is not stated, though I believe it is an assumption in the Theorem.\n\n\nThe top of page 21 is not very well organized: the last component of the \"definition 8\", the definition of $A_{i}$ is problematic since it uses the words \"is biased towards selecting consistent hypotheses in $lin_{q,i}(d,n)$\" without explaining what it means (I know it can be inferred by analogy with definition 5 in the main paper, but it technically should still be defined more properly here). \n\nImportantly, the remark on page 21 only makes sense if one already has made an attempt at reading the proof of Theorem 4. It should be incorporated into the proof of Theorem 4 and explained more rigorously. \n\n\nThere are several minor issues with the proof of Lemma 3: firstly, in the first line, $f_a,f_b$ are defined as \"parity functions\", which seems to hint that we are working with the case $q=2$ (that is not the case). Furthermore, only $f_a$, not $f_b$, is used in that particular paragraph, though an arbitrary $f_b$ in the null space of $X^{-}$ is used in the next paragraph. The last sentence of the third paragraph takes some time to be digested: perhaps a quick mention of realizability can help the reader. \n\nIn the fourth paragraph of the proof of Lemma 3, I think writing \"there always exist **at least** $n+1-k$ canonical basis vectors which are not spanned...\" would be much better than the current \"there always exist in total $n+1-k$ canonical basis vectors which are not spanned...\", since this statement takes a bit of effort to mentally process and I think there can be more than $n+1-k$ vectors which are not spanned!\n\n\nIn the second line of Section G.4 on page 23, I think the authors meant to write $\\mathcal{H}_1$, not $\\mathcal{H}_0$. There should also be a better definition of $\\mathbb{A}_0$ in this part of the text (just a reminder would help, \"an algorithm is in $\\mathcal{A}_0$ if the following statement holds: \"if there exists a consistent hypothesis in $\\mathcal{H}_0$, the algorithm must chose one such hypothesis\".\n\n\nThe end of the proof of lemma 4 is badly organized: the last two paragraphs **should be swapped**, and the sentence \"the claim follows\" should follow as a separate last paragraph. \n\nOn page 24, in the paragraph that begins with the estimation of $\\mathbb{P}(E_3|E_{4,k})$, I think the authors meant $\\mathbb{P}(E_3|E_{4,k})=1-\\mathbb{P}(E_3^{c}|E_{4,k})$, not $\\mathbb{P}(E_3|E_{4,k})=1-\\mathbb{P}(E_3|E_{4,k}^{c})$.\n\n\nThe argument at the top of page 26 needs to be reformulated: \"which implies that $1-q^{-2} \\in 1-o(1/q)$ is not the right statement: the fact that $1-q^{-2} \\in 1-o(1/q)$ is not deduced from the previous statement, instead, it is the first statement in the induction case to show that the expression on the line above is in $1-o(1/q)$.\n\nOn page 26 (just like in the main paper), the statement of Theorem 5 should include the assumption that $q=2$. \n\n\nThe proof of Theorem 3 could be slightly improved: the sentence \"this yields what we wanted\" at the top of page 20 feels a bit abrupt since it has not yet been made clear that the statement below will be used to show that $\\mathcal{A}$ is not estimable. \n\nRight in the middle of page 20 in the proof of Theorem 3, the definition of the even $B$ is missing a $>\\alpha$ before the last curly bracket. \n\n===========Typos/very minor presentation comments=============\n\n\nThe table of notations should be substantially improved to include all the notations in the paper including $R_q(.,.,.)$, $Lin_q(d)$ etc.\n\n \nIn the appendix, the names of Theorems 1 and 2 have been interchanged (Theorem 2 in the appendix is actually Theorem 1), and Theorem 1 in the appendix is actually Theorem 2 (as can be seen from its introduction on page 18 which says, \"we now use Theorem 1 to show the following\") \n\n\nIn the Statement of Theorem 1 (referred to as \"theorem 2\") on page 16, there is an issue with the punctuation (,.)\n\n\nThere is a period missing at equation (8).\n\n\nIn page 22, proof of Lemma 2, the second part of the proof actually doesn't rely on the theorem from [1], so it is a shortcut. This is not clear from the formulation. At a minimum, the first sentence of the second paragraph could read \"the second statement can be inferred from the first, but also follows directly from a simple...\"\n\n\nIn the proof of Lemma 4 on page 23, first paragraph, there is an extra space after \"w.r.t.\". \nIn the proof of Lemma 4 on page 23,  \"be it deterministic of random\" should be \"be it deterministic or random\"\n\n\nThe paragraph starting with $\\mathbb{P}(E_1|E_{2,i}\\cup E^{c}_3\\cup E_{4,k})=1-q^{k-n-1}$ should be slightly reorganised to avoid the repeated use of \"since\". \n\nThere seems to be an extra space before equation (25) and there is a period missing at equation (30). \n\n\nAt the top of Page 25, there is an extra space between  $\\|$ and $P$ in equation (31).\n\n\nIn pages 26 and 27, there are plenty of equation numbers which are not in brackets. The corresponding \"\\ ref {}\" should be changed to \"~ \\ eqref { } \"\n\n\nIn the proof of Theorem 5 on page 27, there are plenty of \"$lin_{2,0}$ which should probably be $lin_{2,0}(d,n)$. \n\nThere is a period missing at the last equation of page 19.\n\nJust before the bullet points on page 20: \"at least one of the following items hold\" should be \"at least one of the following items holds\"\n\n\n=====================================\n\nReferences:\n\n[1] IAN F. BLAKE AND CHRIS STUDHOLME, PROPERTIES OF RANDOM MATRICES AND APPLICATIONS, 2006\n\n[2] Omer Angel and Yinon Spinka. Pairwise optimal coupling of multiple random variables, 2021. (Arxiv)\n\n[3] C Wei, T Ma, Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin, ICML 2019.\n\n[4] Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruosong Wang, Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks, ICML 2019. \n\n[5] Vaishnavh Nagarajan, J. Zico Kolter, Uniform convergence may be unable to explain generalization in deep learning, NeurIPS 2019\n\n[6] Philip M. Long, Hanie Sedghi, Generalization bounds for deep convolutional neural networks, ICLR 2020\n\n[7] Peter Bartlett, Dylan J. Foster, Matus Telgarsky. Spectrally-normalized margin bounds for neural networks, NeurIPS 2017.\n\n[8]  Wei and Ma, Data-dependent Sample Complexity of Deep Neural Networks via Lipschitz Augmentation, NeurIPS 2019\n\n[9] Vaishnavh Nagarajan and Zico Kolter. Deterministic PAC-bayesian generalization bounds for deep networks via generalizing noise-resilience, ICML 2019. \n\n[10] Antoine Ledent, Waleed Mustafa, Yunwen Lei, Marius Kloft, Norm-based generalisation bounds for multi-class convolutional neural networks, AAAI 2021.\n\n[11] Florian Graf, Sebastian Zeng, Bastian Rieck, Marc Niethammer, Roland Kwitt, On Measuring Excess Capacity in Neural Networks, NeurIPS 2023.\n\n[12] Size-Independent Sample Complexity of Neural Networks.  Noah Golowich, Alexander Rakhlin, Ohad Shamir, COLT 2018. \n\n[13] Peter L. Bartlett, Philip M. Long, G\u00e1bor Lugosi, Alexander Tsigler,  Benign Overfitting in Linear Regression\n\n[14] Shamir, The Implicit Bias of Benign Overfitting, COLT 2022. \n\n[15 ] Tomer Galanti, Mengjia Xu, Liane Galanti, Tomaso Poggio ,  Norm-based Generalization Bounds for Compositionally Sparse Neural Networks"
            },
            "questions": {
                "value": "1. Does Theorem 1 only apply to a discrete state space? In the proof towards the end of Page 16, you write down $P_1(S,h)=P_2(S,h)$, which only makes sense if we are in the discrete setting. This should be clearly stated as an assumption in the theorem. \n\n\n2. Could you explain the symmetry argument from the remark on Page 21 a bit more carefully? I am having trouble fully understanding it. \n\n\n3. Why does the argument you rely on in the proof of Theorem 5 not work in general for Theorem 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2576/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698495882998,
        "cdate": 1698495882998,
        "tmdate": 1699636194796,
        "mdate": 1699636194796,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ghPVTW54x0",
        "forum": "NkmJotfL42",
        "replyto": "NkmJotfL42",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_iLzH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_iLzH"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the question of the tightness of existing generalisation bounds to any learning problem of interest. Authors provably show that bounds being algorithm and data-distribution independent cannot reach uniform tightness in the overparametrised setting. They also show that, as modern generalisation bounds are often algorithm-dependent (while still independent of the data distribution), it is impossible for this type of bound to reach tightness uniformly for all data-distribution. Those limits suggest for future work to focus on generalisation bounds exhibiting how data-distribution and algorithmic performances intricate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This work, especially the introduction, is utterly well-written and organised, it has been a pleasure to read it.\n- The proposed results have the potential to be impacting for the whole generalisation field, as they suggest to give up the current shape of state-of-the-art generalisation bounds to direct future works on bounds focusing on the role of the data-distribution."
            },
            "weaknesses": {
                "value": "- I have no problems with results (although I did not read carefully the proofs). However, several paragraphs in this paper, not only provide unfair analysis of existing literature but also contains wrong claims concerning existing works. I strongly believe those paragraphs have to be re-written before acceptance, see the Questions section below for details."
            },
            "questions": {
                "value": "- I believe the notion of overparametrised setting should be explained more carefully and clarified. Indeed, in the introduction, authors precise that such a setting 'roughly means that the number of parameters in the networks is much larger than the number of examples in the training set' while Definition 2 does not even make intervene the notion of parametrised space. The link with neural nets should be explicited.\n- page 6 'analouge of Theorem 1' -> analogue\n- About the implications of Theorems 4 & 5, can you be more specific about the links with practical neural nets? I read Appendix D and I am wondering about the recommendation given at the end of the section: how is it possible for theoreticians to either 'exclude many distributions over linear functions' if one doesn't know which one to focus on? Second, how realistic is it, in terms of computational time, to verify that 'the neural network architecture with SGD cannot learn any large linear subspace of functions'? \n- From a broader perspective, I believe that some claims in the paper are more a matter of personal interpretation than scientific facts and should be reformulated. For instance, author claims in section 5.1 that 'Most published generalization bounds do not restrict the set of distributions or algorithms the bound should apply for. Hence, they should work in all scenarios.' This is not true, having a bound holding for any distribution does not imply that the bound will be tight on all situations (and this is precisely what your work is proving). On the contrary, there may be several interests to derive a generalisation bound, a practical one is deriving generalisation-driven learning algorithms who may have a practical performance tighter than the associated bound as in Dziugaite & Roy 2017, or again simply propose a certain measure of complexity as in Neyshabur et al. 2017 to check if, in a few concrete learning problems, this complexity measure is tailored to explain generalisation. To me, the sentence at the end of Appendix D: 'At the very least, for a generalization bound for a neural network architecture trained with SGD to be meaningful, it must satisfy either of the following items...' is misleading as you assimilate the notion of 'meaningful' to 'precisely explaining the tightness on a given situation'. From my understanding of your work, I would say that existing generalisation bounds are too generic to provide such a precise understanding and are only an intermediary step which has to be completed by a study of the intrications between the data distribution and a learning problem. \n   Similarly, the analysis of the bound of Dziugaite & Roy (2017) in section 2.3 would gain to be reworked. Indeed, the sentence 'the bound in Dziugaite & Roy (2017) relies on implicit assumptions about the algorithm and the population distribution' is simply not true as it relies on the McAllester's bound which does assume anything on the data distribution. However, the efficiency of their algorithm on MNIST is effectively not due to the PAC-Bayesian bound but to more subtle assumptions satisfied by the specific learning problem of interest and is not easily extendable to other learning problems. \n   Finally, claiming that 'explicitly stating the assumptions underlying generalization bounds is not only necessary for the bounds to be mathematically correct...' is utterly misleading as it suggests that existing bounds are mathematically false while they are only not able to ensure tightness of learning algorithms in many situations: a vacuous bound is not mathematically flawed!\n   To me it is necessary to rework those paragraphs in order for the paper to be published as they do not give a fair perspective on the existing literature.\n\n**Conclusion** My current score is linked to the writing of the few paragraphs I mentioned above, I believe they have to be re-written to give a fairer comparison with literature and to remove false claims. That being said, I would be happy to increase my score conditionally to such rewriting, as I think this work is of the highest interest for the generalisation literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Reviewer_iLzH",
                    "ICLR.cc/2024/Conference/Submission2576/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2576/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667873221,
        "cdate": 1698667873221,
        "tmdate": 1700730441363,
        "mdate": 1700730441363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JsozHtggWs",
        "forum": "NkmJotfL42",
        "replyto": "NkmJotfL42",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_jDS8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2576/Reviewer_jDS8"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of understanding the tightness of two general families of the generalization bounds in the literature. The first family is the class of complexity measures that depend on the output of the algorithm and the training set, the second class is the class of algorithms that depends on the \u201cdescription of learning algorithm\u201d and the training set. The main question of the paper is there a family of generalization bounds that are uniformly tight in the overparameterized setting?\n\nThe paper proposes some natural definitions such as \u201coverparameterized setting\u201d: They define the overparameterized setting as the setting that with a given number of samples, accuracy, and confidence, a realizable distribution with respect to a family of hypotheses is not learnable.\n\nThe main result of the paper is that for these two families of the generalization bounds we can't prove a generalization bound which provide \"uniformly\" tight bound."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I think the main message of the paper probably is the most interesting part:\n\nWe need to develop an understanding of the formal assumptions on the algorithms and distributions under which a generalization bound can be tight. Otherwise, we can develop lower bounds as shown in this paper."
            },
            "weaknesses": {
                "value": "-- The main issue I found in the paper is that in many places the discussions are not precise. As an example the claim about cross-validation can be misleading: The authors claim that cross-validation approaches do not lead to algorithm design principles. It is not completely correct. For instance, the well-known algorithm of the one-inclusion graph [Haussler et al 1988] is based on the cross-validation analysis. \n\n[Haussler et al 1988] Haussler, David, Nick Littlestone, and Manfred K. Warmuth. \"Predicting [0, 1]-functions on randomly drawn points.\" Annual Workshop on Computational Learning Theory: Proceedings of the first annual workshop on Computational learning theory. Vol. 3. No. 05. 1988.\n\n\n-- Proof of Theorem 2 and failure of the generalization bound: I checked the proof and it seems that the proof implies that the generalization gap is large for algorithms with large population error. I think it is unsatisfactory as we are interested to show a generalization gap is small for \"good\" learning algorithms\n\n\n-- There are generalization bounds based on conditional mutual information which proven to be \"tight\" in the realizable scenario (For instance Haghifam et al 2022, Thm. 3.3.) It seems this class of generalization bound has not been discussed in this paper.\n\nHaghifam, Mahdi, et al. \"Understanding generalization via leave-one-out conditional mutual information.\" 2022 IEEE International Symposium on Information Theory (ISIT). IEEE, 2022."
            },
            "questions": {
                "value": "-- Definition of tightness can be improved: In many cases, we are interested in correlation of complexity measure and the actual generalization gap. It is not clear to me that the definition of tightness is the best possible.\n\n\n-- In Page 14, there is this paragraph which clearly is not about Negrea et al. (2020).  This paragraph is unrelated to this paper.\n\n\u201cNegrea et al. (2020): The paper studies convex optimization, so the results can hold only for a single neuron. Nevertheless, although it gives matching lower and upper bounds, the bounds match only asymptotically when n is very large so the scenario is far away from the overparametrized regime (which is the focus of interest for neural networks).\u201d\n\n-- what are the challenges to extend the results to agnostic settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2576/Reviewer_jDS8"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2576/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699488312473,
        "cdate": 1699488312473,
        "tmdate": 1699636194565,
        "mdate": 1699636194565,
        "license": "CC BY 4.0",
        "version": 2
    }
]