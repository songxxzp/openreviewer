[
    {
        "id": "3YVRoHucFO",
        "forum": "lAhWGOkpSR",
        "replyto": "lAhWGOkpSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission104/Reviewer_L266"
        ],
        "content": {
            "summary": {
                "value": "The paper delves into the topic of multi-scale representations and identifies two primary challenges: \"scale inadequacy\" and \"field inactivation.\" The authors introduce the LWA model and, based on this, design the VWFormer as a decoder to enhance multi-scale representation capabilities. Through the effective LWA module, VWFormer outperforms most compute-friendly multi-scale decoders, such as FPN and MLP decoders."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Extensive experiments are conducted on Cityscapes, ADE20K, and COCOStuff-164K datasets. Despite reducing computational demands and the number of parameters, the proposed method demonstrates relative improvements compared to other approaches. \n\nThis work provides a thorough mathematical derivation of its method and showcases how to address the challenges of scale inadequacy and field inactivation.\n\nThis work presents a lot of ablation experiments to verify that the proposed methods outperform existing mechanisms.\n\nThe visualization results are interesting and provide many insights into multi-scale feature representations for semantic segmentation."
            },
            "weaknesses": {
                "value": "1. The manuscript requires structural refinement. Although the paper proposes a new methodology, the empirical validation predominantly resides in the appendix. The core content appears to be overly enmeshed in granular theoretical derivations, leading to a somewhat convoluted narrative structure.\n2. The delineation between the figures, tables, and the main text is ambiguous. It is advisable to render the captions in a font slightly smaller than the primary text to enhance clarity. The layout of the illustrations also warrants improvement. For instance, in Figure 2, it would be beneficial to incorporate descriptions of specific letters either within the caption or designated sections of the image. The term \"PE\" is presented here, yet a precise elucidation is deferred until Section 3.3.\n3. The visualization results do not substantiate the issues you've raised. The quantified outcomes displayed in Figure 5 merely illustrate the discriminative results of the proposed network in comparison to Segformer for a specific image region. If the method you presented exhibits a substantial improvement over Segformer, I believe it would be acceptable. However, given that the data only indicates approximately a 1% enhancement relative to Segformer. There is a lack of targeted visualization results addressing the issues of scale inadequacy and field inactivation.\n4. The author introduced \"varying window attention,\" but neglected a comparative analysis with other similar methods, such as the 'Shifted Window' in the Swin-Transformer.\n5. Some state-of-the-art segmentation methods like HRViT, SegViT-v2, ViT-adapter, Lawin Transformer, etc. could be compared in the experiments to better verify the superiority of your proposed architecture.\n\nSincerely,"
            },
            "questions": {
                "value": "1. Why not experiment with the same Encoder paired with different decoders (more than three)? This can help better show the consistent effectiveness of your approach. \n2. The computational load shown in Equation (2) appears highly reminiscent of the Swin Transformer. Could you possibly incorporate a comparison with the Swin Transformer?\n3. In Figure (3), you've employed an operation akin to that of Swin. However, after image concatenation in Swin, a mask is applied. As you concatenate features from different regions, how do you prevent interference or cross-effects between these diverse area features?\n4. SegFormer utilizes an MLP Decoder, which doesn't encompass an attention mechanism. Yet, in your VWA mechanism, you've incorporated an attention mechanism. How does this lead to a reduction in parameter count? This can be better analyzed.\n5. In the experimental section, could the results be organized based on the size of the modules in a systematic order?\n\nSincerely,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission104/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_L266",
                    "ICLR.cc/2024/Conference/Submission104/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697681298948,
        "cdate": 1697681298948,
        "tmdate": 1700110308716,
        "mdate": 1700110308716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6ROtelCuKO",
        "forum": "lAhWGOkpSR",
        "replyto": "lAhWGOkpSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
        ],
        "content": {
            "summary": {
                "value": "This paper points out two problems existing in the previous multi-scale semantic segmentation representation: scale inadequacy and field inactivation. To solve these two problems, this paper presents its method - VWA (varying window attention). It begins by splitting local window attention (LWA) into query window and context window, which are variable so that queries learn representations on a specific scale. Then, to reduce memory consumption, the pre-scaling principle, densely overlapping patch embedding(DOPE), and copy-shift padding mode are proposed. Finally, VWFormer is devised for semantic segmentation, which shows its effectiveness in terms of performance and reducing cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper makes full use of multi-scale features for fusion and proposes VMA to obtain multi-scale representation.\n+ The experiments and visual results show the problems that existed in the previous work, namely, scale inadequacy and field inactivation, which is insight.\n+ The VMFormer frame is more clearly drawn and is easy to follow."
            },
            "weaknesses": {
                "value": "- Too much space is spent to explain the memory usage problem, resulting in some experimental results, such as Table 9 and some ablation experiments, which can only be placed in the appendix.\n- Lack of ablation experiments on the copy-shift padding mode. It is mentioned in the paper that filling with zero will cause attention collapse, but there are no experimental results to explain it.\n- There is no comparison between VWA and LWA.\n- The pre-scaling principle is to reduce memory usage, and the impact of pre-scaling on performance is not reflected.\n- The method part is not abundant, such as the short path branch. Overall, it seems that the original is not high, mainly VWA.\n- Inference time is not reflected in the experiments."
            },
            "questions": {
                "value": "(1) For copy-shift padding mode(CSP), why not take the adjacent part to padding?\n(2) What is the short path branch?\n(3) What does pre. or post. in Table 7 refer to?  PE refers to padding with zero or CSP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission104/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission104/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission104/Reviewer_Vryj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847954190,
        "cdate": 1698847954190,
        "tmdate": 1699635935375,
        "mdate": 1699635935375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z492W6aaHo",
        "forum": "lAhWGOkpSR",
        "replyto": "lAhWGOkpSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission104/Reviewer_u8Vk"
        ],
        "content": {
            "summary": {
                "value": "This paper observes the limitation of local window attention and proposes an efficient modification to enlarge the receptive field while keeping the computation efficiency. Specifically, it designs a pre-scaling strategy, which cleverly reduces the extra computation of enlarging window size. This paper presents detailed computation comparisons with existing methods and verifies the effectiveness of the proposed methods on several benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper starts from the visual observations, then goes into detailed computation analysis, and further proposes modifications to enhance. The logic between the whole paper is very smooth and easy to follow.\n\n2. The detailed complexity analysis substance the efficiency of the proposed method. Besides, the experimental results further verify computation efficiency compared to existing methods.\n\n3. Quantitative analysis verifies the effectiveness of the proposed methods, highlighting the significance of varying window sizes."
            },
            "weaknesses": {
                "value": "1. The proposed copy-shift padding is a bit weird. It mimics the rolling operation in numpy and torch. I wonder why adopting this instead of symmetric or constant padding.\n\n2. Fig 2b and Sec 3.2 are not well-aligned. In Fig 2b there's a PE operation, which doesn't occur in Sec 3.2, which can confuse the readers."
            },
            "questions": {
                "value": "1. Are there any ablation studies indicating the effectiveness of copy-shift padding?\n\n2. From Table 7 in Supp, it seems avg_pool can be a substitution of PE, then I wonder what the performance will be like if the rescaling method \"DOPE --> avg_pool --> 1x1 conv\" is taken."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698928710453,
        "cdate": 1698928710453,
        "tmdate": 1699635935310,
        "mdate": 1699635935310,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aSXuhTnBLa",
        "forum": "lAhWGOkpSR",
        "replyto": "lAhWGOkpSR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission104/Reviewer_1jxp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission104/Reviewer_1jxp"
        ],
        "content": {
            "summary": {
                "value": "The submission introduces Varying Window Attention (VWA) to address scale inadequacy and field inactivation in semantic segmentation. VWA modifies the local window attention mechanism to dynamically adjust the receptive field, aiming to improve multi-scale representation learning. The paper also proposes a new multi-scale decoder, VWFormer, which incorporates VWA and demonstrates performance and efficiency gains on standard datasets. The work claims three main contributions: the VWA mechanism, the VWFormer decoder, and empirical improvements over state-of-the-art methods. Extensive experiments validate the effectiveness of the proposed methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Introduction of Varying Window Attention (VWA) to dynamically adjust receptive fields, addressing scale inadequacy and field inactivation.\n+ Development of new principles such as pre-scaling, densely overlapping patch embedding (DOPE), and copy-shift padding mode (CSP) to enhance efficiency in receptive field variation.\n+ Empirical improvements in mean Intersection over Union (mIoU) and reductions in computational cost (FLOPs)."
            },
            "weaknesses": {
                "value": "- The paper may not sufficiently differentiate VWA from existing local window attention mechanisms.\n- Potential concerns about the scalability and generalizability of the proposed method to different architectures or datasets."
            },
            "questions": {
                "value": "1. How does VWA fundamentally differ from existing attention mechanisms in handling multi-scale representations?\n2. Include a breakdown of performance gains across different classes or segments within the datasets used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission104/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699378949581,
        "cdate": 1699378949581,
        "tmdate": 1699635935241,
        "mdate": 1699635935241,
        "license": "CC BY 4.0",
        "version": 2
    }
]