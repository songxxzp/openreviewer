[
    {
        "id": "ztBGDe2cGj",
        "forum": "rO8QOHrCeA",
        "replyto": "rO8QOHrCeA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_pVfx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_pVfx"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to enhance the capability of LLM by instruction tuning on\ndatasets with I/O specifications. Namely, it adds more prompt contexts that\nspecify required I/O requirements and leverages that to instruct-tune a model to\ngenerate results that satisfy such results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Enhancing the generation of code models from natural languages is important and\na timely topic."
            },
            "weaknesses": {
                "value": "I am confused about the motivation of the work. It mentions that it wants to\nenhance the code model to generate more correct code (besides executable). From\nthe example given, i.e., figure 1, the so-called user intention seems to be the\ndescription of names of the column, etc., which are necessary information you\nneed to provide in the prompt to accomplish such tasks. Otherwise, the model\nsurely will not understand the input and cannot output the variables as needed.\n\nSo, I was thinking that the baseline methods are too naive, which does not even\nprovide such information to the LLM to generate code. But then, I found that\nthere are no baseline methods. Now, I am quite confused: where do you get such a\nmotivation? Is not that just poor prompting skills the paper assumes users have\n-- which is not realistic? Are there evidence showing that users do not want to\nprovide enough information to the LLM but want to get desired results?\n\nIf you are talking about understanding I/O constraints, e.g., relationships\namong input data, code generation from examples is a type of program synthesis,\nand there are many baseline works.\n\nIf the so called I/O specifications are necessary I/O descriptions, I do not see\nwhy users would want to exclude them. Also, various plugins try to infer data\ntype and descriptions from user prompts and given files. For the I/O\nspecifications shown in Table 1, I am very confused why users would not include\nthem in the prompt description if, for example, they want the output variable to\nbe named as df."
            },
            "questions": {
                "value": "What is your technical contribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772694475,
        "cdate": 1698772694475,
        "tmdate": 1699636750356,
        "mdate": 1699636750356,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bGxBQNs5Cs",
        "forum": "rO8QOHrCeA",
        "replyto": "rO8QOHrCeA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_ek5m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_ek5m"
        ],
        "content": {
            "summary": {
                "value": "Large language models (LLMs) have demonstrated significant potential in code generation. However, the code generated by these models occasionally deviates from the user\u2019s intended outcome, resulting in executable but incorrect code. To address this problem, the authors propose Gift4Code (grounded instruction fine-tuning for code). Gift4Code can access both a natural-language intent and an additional I/O specification. The experimental results show the effectiveness of Gift4Code."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors focus on an very important area.\n2. The improvements seem good."
            },
            "weaknesses": {
                "value": "1. The idea is not novel enough."
            },
            "questions": {
                "value": "The authors focus on an very important area and the improvements seem good. However, I have a concern.\n\nThe authors claim that the code generated by these models occasionally deviates from the user\u2019s intended outcome. However, CodeT also considers the test case, which is similar to the specification. Further, the overall idea of this paper is similar to CodeT, which also uses model to generate test cases. I know the differences between the specification and the test case. However, the overall framework of these two approaches are similar and the authors even fail to cite CodeT. If there is any difference, please add more details about it. \n\n[1] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J. G., & Chen, W. (2022, September). CodeT: Code Generation with Generated Tests. In The Eleventh International Conference on Learning Representations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827667781,
        "cdate": 1698827667781,
        "tmdate": 1699636750211,
        "mdate": 1699636750211,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z2qsZhGCXc",
        "forum": "rO8QOHrCeA",
        "replyto": "rO8QOHrCeA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_kH34"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_kH34"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces GIFT4Code, a novel approach designed to improve the accuracy of code generated by Large Language Models (LLMs) in the context of data science tasks. The problem addressed is that LLM-generated code can sometimes be executable but incorrect, not aligning with the user's intended outcome. GIFT4Code tackles this issue by fine-tuning LLMs specifically for code generation tasks with the natural-language-summarized input-output specification. It achieves this by using synthetic data generated by the LLM itself and providing execution-derived feedback and summarize the input-output with a \"generalist\" LLM. This more detailed specification is expected to concretize the user's intention so that the LLM better understands and aligns with the user's intentions when generating code. The paper evaluates the GIFT4Code approach on two data science benchmarks, Arcade and DS-1000, and the results indicate a significant improvement in the quality of generated code by appending the I/O summary."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "__The paper addresses an interesting and important problem in code generation__.\n\nThe ambiguity of human intent or instructions has been identified as one of the main reasons that hurt the code LLM's performance since the human intent is mostly high-level and always misses descriptions regarding corner cases. The input-output specification is one of the effective ways to concretize the requirements, and human developers also tend to refer to such specifications for accurate implementation. This paper targets guiding the code LLMs to efficiently capture such specified intent for better code generation, which could potentially increase the code LLMs performance in generating code better aligned with human intent. The problem, in general, is interesting and important.\n\n__The design is intuitive and smart, and the improvement over the base model is obvious__\n\nIn general, Gift4Code's design makes intuitive sense, and I specifically appreciate the core design of summarizing the input-output as a natural language specification. One potential difficulty that hinders the LLM from efficiently understanding both the execution results and the input-output constraints is the complicated data structure and the concrete arithmetic values that the LLM struggles to accurately perceive. Natural-language summarization as the input-output specification seems to bridge the gap between the constraints and the semantic space that LLM could efficiently understand. Such benefits are also successfully reflected from the evaluation, in both prompting and fine-tuning setup"
            },
            "weaknesses": {
                "value": "__Unclear description regarding how to get I/O summary in the realistic setup.__ \n\nWhen the model is deployed for inference in the realistic setup, especially when only human intent is provided, it is not clear where the I/O summary could potentially come from, and it is no more the same case as training data preparation that there are some ground-truth programs to be executed for the output and summarized by the \"generalist\" LLM. Will Gift4Code generate the I/O summary first, by only conditioned on the problem description, and then generate the code conditioned on its own generated I/O summary? Or the user has to provide a concrete I/O specification beyond the high-level intent? In the latter case, whether the user could generate a high-quality I/O specification as the training data remains unknown. Though the authors explain that they try to simulate noisy I/O specification at test time (Section 4.1), this is still some sort of leakage of the ground-truth code, not similar to the realistic setup when only human intent is available. I would urge the authors to explain how Gift4Code will be applied in such real-world scenario.\n\n__The evaluation is insufficient.__\n\nUnfortunately, while the design of Gift4Code seems effective, the evaluation of this paper is not sufficient. \n\nFirst, the baselines are not strong enough. For the model of similar size, i.e., 62B, the authors only compare with an unknown/anonymous code LLM which is used as the starting point of Gift4Code. Given the audience could not estimate the capacity of this 62B since it is not publicly available, using this model as the only baseline seems insufficient. In addition, the 15B baselines seem to perform comparably well to the vanilla 62B Code LLM, this makes the latter, as a baseline, even weaker. I would encourage the authors to compare with at least one more publicly available model that is widely recognized as \"capable\", such as Llama-2-70B. \n\nSecond, it is not clear how generalizable Gift4Code is. It seems that the implementation and training data of this paper is not publically available, so it could be difficult to estimate the generalizability of the method, i.e., whether the improvement is restricted to the unknown/anonymous code LLM or it will work similarly effectively when applied to other pre-trained code LLMs. I would ask the authors to conduct similar instruct-tuning using open-source code LLMs, such as code llama-7B, 13B, and 34B.\n\nThird, the claims that general instruct-tuning is not effective by simply comparing WizardCoder to StarCoder are not reasonable. The overall comparison is not well controlled. For example, the unknown/anonymous base code LLM, as explained in Section 4.1, is already fine-tuned on data science dataset, so the base model is already more capable in data science code generation than StarCoder. Also, Gift4Code is trained on the data science dataset again, so it is not clear whether its training strategy helps more or just the data does the job.\n\n__Lack of details in the model training, configuration, and architecture__\n\nWhile I am fine with the authors' choice of not releasing the artifact, the details for reproducibility are not sufficiently provided, questioning the work's reproducibility. For example, what is the architecture of the 62B base model, how many layers, hidden dimensions, types of positional encoding, etc. Also, the details of Gift4Code training, such as the learning rate, training for how long, what are the impacts of configuration, etc, are not revealed, either. Without the details mentioned above, it is nearly impossible to reproduce the results and leverage findings for other goals in the community, significantly weakening the contribution of this work. I would ask the authors to provide a detailed description of the training process, configuration, and model architecture to ensure the reproducibility of their results."
            },
            "questions": {
                "value": "- In the realistic development scenario, the expectation is that only the human intent is given. In such a case, the ground-truth code is unknown, so the summary of execution results relying on the \"generalist\" LLM is not possible. In this case, where is the I/O summary from? Will the user have to further indicate the specified I/O beyond the high-level intent? Is such a requirement practical?\n- Will the author release the artifact for reproducibility?\n- If the above is not possible, could the authors provide detailed description of the training process as a reproducibility statement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698878003729,
        "cdate": 1698878003729,
        "tmdate": 1699636750081,
        "mdate": 1699636750081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2gGJK2qrSY",
        "forum": "rO8QOHrCeA",
        "replyto": "rO8QOHrCeA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_Ns97"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6594/Reviewer_Ns97"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces GIFT4CODE, a method to create synthetic data for the instruction fine-tuning of LLMs in code generation. The authors use a general-purpose LLM to generate NL intents and feed them to a Code LLM to generate possible code solutions. They leverage program execution to post-process synthesized solutions and generate additional I/O specifications for grounding. The instruction-tuned LLM is evaluated on ARCADE and DS1000 datasets and compared with zero-shot and few-shot learning of the base model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper presents a pretty clear methodology for I/O specification grounded code generation and backs it with interesting results. For example, the performance between no context and full context is insightful.\n\nThe experiment shows fine-tuning with synthetic data gives a significant boost in the model performance on ARCADE. The performance gain is somewhat modest on DS-1000 even though 500 problems in this dataset were used for training."
            },
            "weaknesses": {
                "value": "Bootstrapping LLMs with synthetic data generated from strong LLMs is a pretty well-known technique. The paper claims that \u201cUnlike existing approaches, our key insight is to leverage program execution for synthetic data generation\u201d; however using execution feedback for synthetic data generation is also known in previous work, for example Code Llama.\n\nThis work lacks comparison with some strong open-source and proprietary models such as GPT-4 and Code Llama as a baseline.\n\nCode generation grounded with I/O specifications sounds interesting, but it is not clear if developers write I/O specifications similar to what being used in this work, especially in the generated I/O summary case.\n\nOne base model performs a little worse with TypeDesc spec than without."
            },
            "questions": {
                "value": "I wonder if the model regresses on standard benchmarks such as HumanEval and MBPP after the fine-tuning step. It would great if the authors could provide such results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6594/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699066354444,
        "cdate": 1699066354444,
        "tmdate": 1699636749945,
        "mdate": 1699636749945,
        "license": "CC BY 4.0",
        "version": 2
    }
]