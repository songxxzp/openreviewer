[
    {
        "id": "ZBJETs4Ijp",
        "forum": "jkonJu7ScD",
        "replyto": "jkonJu7ScD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_Hy4i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_Hy4i"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to use self-supervised tasks including reconstruction and next-state prediction based on masked input states, to improve deep reinforcement learning algorithm. The proposed method also combines Transformer to jointly model the short state sequence input. The proposed method is compared to multiple baselines and exhibits better performance in Atari and DMC benchmarks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is experimentally shown to be effective on two benchmarks.\n\n- Analysis on masking strategy is extensive and informative."
            },
            "weaknesses": {
                "value": "1. **Confusing diagram**. Figure 1 is very misleading to explain \"reconstruction\" and \"inverse dynamics modeling\". The reconstruction usually refers to predicting the original image, while the actual method is about minimizing error between the latent representations from two networks. This diagram also does not help explain action prediction (inverse dynamics modeling). The current diagram is showing a comparison of image embeddings.\n\n2. **Clarity**. Seciton 3 does not explain:\n\n   * Why a Transformer module is introduced to help modeling\n   * How action is predicted from the model, using current and next states\n   * How the online network performs state representation regression and action prediction simultaneously. Are there seperate heads for each auxiliary task?\n\n3. **Baselines**. Clearly there are missing baselines like RAD [1], DrQv2 [2] according to this study [3] on self-supervised learning with RL, and possibly [4] which uses Transformer for next frame prediciton, and vanilla MAE [5]. With the current baselines I can not evaluate the significance of proposed method, considering the novelty of the proposed method is limited. \n\n4. Due to extra architecture and parameter introduced in the Transformer module, are the baseline models using the same/similar architecture to ensure the fairness? Is the Transformer important in the modeling?\n\n[1] Laskin, Michael et al. \u201cReinforcement Learning with Augmented Data.\u201d ArXiv abs/2004.14990 (2020)\n\n[2] Yarats, Denis, et al. \"Mastering visual continuous control: Improved data-augmented reinforcement learning.\" arXiv preprint arXiv:2107.09645 (2021).\n\n[3] Li, Xiang et al. \u201cDoes Self-supervised Learning Really Improve Reinforcement Learning from Pixels?\u201d ArXiv abs/2206.05266 (2022)\n\n[4] Micheli, Vincent, Eloi Alonso, and Fran\u00e7ois Fleuret. \"Transformers are sample efficient world models.\" arXiv preprint arXiv:2209.00588 (2022).\n\n[5] He, Kaiming et al. \u201cMasked Autoencoders Are Scalable Vision Learners.\u201d 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021): 15979-15988."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723969916,
        "cdate": 1698723969916,
        "tmdate": 1699636038920,
        "mdate": 1699636038920,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "84fL0TZ8LX",
        "forum": "jkonJu7ScD",
        "replyto": "jkonJu7ScD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_t6qc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_t6qc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel representation learning objective MIND that integrates inverse dynamics modeling with the masked modeling of consecutive states. This dual approach enables the representation to encapsulate both the agent-controllable aspects as well as static features within the image. The effectiveness of this method is demonstrated through empirical results on the Atari 100K and DMControl-100K benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is very well-written. The empirical evaluation is comprehensive for tasks with discrete action spaces Atari-100K. Additionally, comprehensive ablation studies of the learning objective as well as hyperparameter choices are conducted."
            },
            "weaknesses": {
                "value": "The evaluation for continuous control tasks are very limited. DMControl 100K is originally proposed and evaluated in CURL, which consists of the simplest tasks in DMControl. Later works such as Dreamer-v2/v3, DrQ-v2, A-LIX[1], ATC[2], and TACO[3] mainly consider medium-difficulty tasks as well as the more challenging humanoid domain. Thus, more tasks and especially harder medium-difficulty tasks should be evaluated for the proposed method. I recommend the author to provide a comparison at 1M/2M of harder tasks instead of the six tasks presented in the paper. Also, A-LIX[1], ATC[2], and TACO[3] should be discussed and compared in the empirical evaluation."
            },
            "questions": {
                "value": "As shown by prior works such as [4, 5], the single-step inverse model is theoretically and empirically non-sufficient to capture the full agent-centric representation. For example, in an empty gridworld, a pair of positions separated by two or more spaces may be assigned an identical representation without incurring additional loss in a one-step inverse model.. In contrast, a multi-step inverse model is both theoretically sufficient and in practice achieves good performance. Would MIND benefit from multi-step inverse modeling instead of single-step inverse?\n\nI am willing to raise the score if my above two questions/concerns (a more comprehensive evaluation of continuous control tasks and multi-step inverse model instead of single-step) are addressed. I understand that given the time constraint of rebuttal, the authors are not able to address them fully. But it would be great at least to see the additional experiments on a few medium DMControl tasks.\n\n**Additional References that are not included in the paper**\n- [1] Edoardo Cetin, Philip J. Ball, Steve Roberts, Oya Celiktutan, Stabilizing Off-Policy Deep Reinforcement Learning from Pixels, ICML 2022\n- [2] Adam Stooke, Kimin Lee, Pieter Abbeel, Michael Laskin, Decoupling Representation Learning from Reinforcement Learning, ICML 2021\n- [3] Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daum\u00e9 III, Furong Huang. TACO: Temporal Latent Action-Driven Contrastive Loss for Visual Reinforcement Learning, NeurIPS 2023\n- [4] Riashat Islam, Manan Tomar, Alex Lamb, Yonathan Efroni, Hongyu Zang, Aniket Didolkar, Dipendra Misra, Xin Li, Harm van Seijen, Remi Tachet des Combes, John Langford, Principled Offline RL in the Presence of Rich Exogenous Information. ICML 2023\n- [5] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Didolkar, Dipendra Misra, Dylan Foster, Lekan Molu, Rajan Chari, Akshay Krishnamurthy, John Langford, Guaranteed Discovery of Control-Endogenous Latent States with Multi-Step Inverse Models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728582431,
        "cdate": 1698728582431,
        "tmdate": 1699636038820,
        "mdate": 1699636038820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ND7RjstoqA",
        "forum": "jkonJu7ScD",
        "replyto": "jkonJu7ScD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_RYme"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_RYme"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a hybrid objective of something very similar to BYOL (predicting representations of a target network) while the online network is masked, while combining this with a one-step inverse model.  This approach has nice empirical results and solid empirical analysis on Atari 100k.  The big downside I see to this paper is that the method is not extremely novel and the justifications are ad-hoc, whereas the field of representations for RL has seen rapid progress in theoretical analysis, so it should be possible to provide more detailed justifications for the technique.  Nonetheless, I see this paper as a solid empirical contribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The improvements over reasonable baselines like SPR and DrQ are considerable on the Atari 100k, which is a fairly difficult setting.  The improvements on DM control suite are also convincing.  \n  -The analysis of what the model is learning is reasonably thorough.  \n  -I appreciated the study of wall-clock time, showing that the method is practical to use on a single GPU and is cheaper than other methods."
            },
            "weaknesses": {
                "value": "-The justification in this paper for using inverse dynamics along with masked prediction is fairly ad-hoc and informal.  This isn't the end of the world, but the understanding of this area from theoretical RL is rapidly advancing (Efroni 2022, Lamb 2022, for example).  There have also been a few empirical papers with related ideas such as the InfoPower paper as well as the ACRO paper (Islam 2022).  One simple thing that might be worth trying is for the IT model, use the observation k steps in the future (with k sampled U(1,5) for example) and then predict the first action.  Some theoretical work has suggested the value of this approach.  \n  -The basic approach seems similar to combining BYOL (with masking as the augmentation) with a one step inverse model."
            },
            "questions": {
                "value": "-Why is there an IT network, rather than reusing the MT network for the IT network, perhaps with a distinct head for predicting inverse dynamics?  \n  -Why not also use masking of the inputs to the IT network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778171168,
        "cdate": 1698778171168,
        "tmdate": 1699636038755,
        "mdate": 1699636038755,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r6CGCsgzTB",
        "forum": "jkonJu7ScD",
        "replyto": "jkonJu7ScD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_uqBc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1128/Reviewer_uqBc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to learn representations for reinforcement learning  using a combination of inverse dynamics training and masked modeling."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The problem of learning representations from reinforcement learning makes sense\n- The paper appears to improve performance over baselines\n- The paper evaluates across a set of different environments, showing improved performance across them\n- The paper runs a comprehensive set of ablations"
            },
            "weaknesses": {
                "value": "- In equation one, p, c, q,  is not defined\n- It would be good to report confidence intervals in results, for instance in Table 1\n- The paper doesn't seem very novel -- it uses momentum contrast masked loss + a standard inverse dynamics representation learning objective"
            },
            "questions": {
                "value": "Can the authors provide some intuition why masking + momentum contrast outperforms other baseline methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1128/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698866411410,
        "cdate": 1698866411410,
        "tmdate": 1699636038681,
        "mdate": 1699636038681,
        "license": "CC BY 4.0",
        "version": 2
    }
]