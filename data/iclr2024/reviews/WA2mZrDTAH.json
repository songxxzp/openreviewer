[
    {
        "id": "KzhZp0iSOl",
        "forum": "WA2mZrDTAH",
        "replyto": "WA2mZrDTAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_41mH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method called ZegOT for zero-shot semantic segmentation, which transfers text-image aligned knowledge to pixel-level classification without the need for additional image encoders or retraining the CLIP module. ZegOT utilizes a Multiple Prompt Optimal Transport Solver (MPOT) to learn an optimal mapping between multiple text prompts and pixel embeddings of the frozen image encoder layers. This allows each text prompt to focus on distinct visual semantic attributes and diversify the learned knowledge to handle previously unseen categories effectively."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The experimental results lead the existing state-of-the-art (SOTA)  under the same settings on some datasets.\n\n\n+ The author modeled the optimal transport problem into the segmentation task of open vocabulary, providing a new approach, and this module can alleviate the problem of overfitting to the seed class."
            },
            "weaknesses": {
                "value": "-\tThe technical insight may not be enough The Deep Feature Alignment module proposed by the author is equivalent to extending the Relationship Descriptor based on Zegclip to multi level features, with the core still being the Relationship Descriptor. In addition, similar to CoOp's text prompt learning, would it be better to directly apply it to previous methods such as ZegClip?\n\n-\tThe experiment setting is not clear to me. For example, If the proposed method can effectively solve the problem of overfitting the network parameters to the seed class distribution after training, why not conduct a set of experiments under the setting of Inductive (unseen class names and images are not accessible during training.). In Table 2, the mIoU (U) and miou (S) of ZegCLIP are 87.3 and 92.3 respectively, but hiou should not be 91.1 and should be 89.7. The setting used in the experiment in table2 is Conductive. But does Tabel3 seem to be using Inductive settings? No explanation was provided, and would using Transductive settings be better than the previous method. In addition, the author claims to have obtained a sota, but it is 2.2 hiou lower on COCO-Stuff164K. For table4, I would like to know how much improvement can be achieved by using only MPOT compared to baseline.\n\n\n-\tFormula 17 is written incorrectly."
            },
            "questions": {
                "value": "seeing Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666716831,
        "cdate": 1698666716831,
        "tmdate": 1699636440184,
        "mdate": 1699636440184,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K6sqmPigBJ",
        "forum": "WA2mZrDTAH",
        "replyto": "WA2mZrDTAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_Dhw2"
        ],
        "content": {
            "summary": {
                "value": "This paper utilizes the large-scale CLIP model to solve the zero-shot semantic segmentation task. In this paper, the authors have proposed a novel Multiple Prompt Optimal Transport Solver (MPOT) module, which is designed  to learn an optimal mapping between multiple\ntext prompts and pixel embeddings of the frozen image encoder layers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method solves zero-shot segmentation from a new perspective. They propose optimal transport to enable the alignment between text and pixel space."
            },
            "weaknesses": {
                "value": "1. The authors include ZegCLIP to \" trainable image encoder-based approaches\". However, they fix the image encoder and train a new decoder. Such a statement is not accurate. \n2. In related work, the authors do not introduce open vocabulary semantic segmentation, which is highly related to this topic in this paper.\n3. The performance of COCO-Stuff does not outperform ZegCLIP.  It seems that the proposed method is more useful on simple images such as PASCAL VOC.\n4. Lack of inference speed comparison with previous methods. As the authors propose several blocks into the ZS3 framework,  it is quite essential to consider the inference speed.\n5. The optimal transport plan is just like a spatial attention map for each class and each prompt, I guess adding a self-attention layer or learnable spatial attention maps is also effective."
            },
            "questions": {
                "value": "It seems that the proposed Optimal Transport-based method can be plugged into other methods, such as ZegCLIP. Is it possible to use Optimal Transport in ZegCLIP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820883465,
        "cdate": 1698820883465,
        "tmdate": 1699636440102,
        "mdate": 1699636440102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8tDjiZFRhE",
        "forum": "WA2mZrDTAH",
        "replyto": "WA2mZrDTAH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_kYrk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4612/Reviewer_kYrk"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework that utilizes a frozen vision-language model (CLIP) for zero-shot semantic segmentation. The proposed method learns a set of text prompts that align with pixel embedding at different scales. Since the learn text prompts have coherence and usually have similar score maps with image features, they propose to refine the score map with optimal transport to make them more diverse. The method shows great performance on ZS3 under Pascal VOC, and Pascal Context."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea is interesting. They show that learning multiple prompts leads to similar score map for each prompt. Then they propose a method to make different prompts to have different score maps, which improves the diversity of the score maps. Optimal transport fits this purpose. An adequate comparison with other alignment methods such as bipartite matching and self-attention has been shown in the ablation study."
            },
            "weaknesses": {
                "value": "- What is the reason behind the similarity of the multiple learned prompts?\n- The performance of ZegCLIP is different from that in the original paper. \n- It seems the method is trained in a transductive setting; What is the performance in an inductive setting?\n- How does this work under different backbones? Previous works sometimes use Resnet for ZS3. Please consider a comparison.\n- The number of classes seems small. What are the results on datasets with larger numbers of classes, such as Pascal Context 459 and ADE-847?"
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4612/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699081859345,
        "cdate": 1699081859345,
        "tmdate": 1699636440029,
        "mdate": 1699636440029,
        "license": "CC BY 4.0",
        "version": 2
    }
]