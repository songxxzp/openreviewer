[
    {
        "id": "Eoakc2sjo4",
        "forum": "vNiI3aGcE6",
        "replyto": "vNiI3aGcE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
        ],
        "content": {
            "summary": {
                "value": "This paper studies provably efficient reinforcement learning in two-player zero-sum Markov games, an important special case of multi-agent RL. This paper improves existing results in the following directions: sample complexity, memory efficiency, Markov output policy, and burn-in cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper studies an important topic in MARL theory. The proposed algorithm simultaneously achieved state-of-the-art results in all the aspects it considers: It matches the best sample complexity bounds, reduces the burn-in cost, and improves the space complexity while still outputting a Markov policy. The theoretical analysis looks solid."
            },
            "weaknesses": {
                "value": "I reviewed this paper at NeurIPS 2023. My concern was about the technical novelty of the paper because the proposed algorithm follows the mature framework of Nash Q-learning. The improved sample complexity is achieved by also following an existing reference-advantage decomposition technique. \n\nIn terms of the bounds, the biggest improvements that this paper makes over existing works are regarding the space complexity and burn-in cost. In my opinion, these are less important metrics compared to sample complexity or time complexity, yet this work has to optimize these metrics at the cost of a much more complicated algorithm and proof procedure. \nWhile I still hold most of my previous opinions, I appreciate the authors\u2019 effort in improving their work and would like to increase my score compared to my NeurIPS evaluation. \n\nCompared to the NeurIPS submission, the new major results are Theorems 2 and 3. I found that the extension to multi-player general-sum games (Theorem 3) particularly interesting, but I was not able to find any algorithm or proof for this theorem. What is the learning target for general-sum games, Nash or correlated equilibria?"
            },
            "questions": {
                "value": "1.\tCould you please point me to the proofs of Theorem 3? Also what is the algorithm for this theorem (as I assume that Algorithm 1 only applies to two-player zero-sum games)? I do not think the extension from zero-sum to multi-player general-sum is straightforward and would hope to see a more detailed discussion.\n\n2.\tSince you now also consider multi-player general-sum games, it is probably helpful to include related works for learning in general-sum games, especially those using Nash V-learning (to name a few):\n\na. Song, Ziang, Song Mei, and Yu Bai. \"When can we learn general-sum Markov games with a large number of players sample-efficiently?.\" arXiv preprint arXiv:2110.04184 (2021).\n\nb. Mao, Weichao, and Tamer Ba\u015far. \"Provably Efficient Reinforcement Learning in Decentralized General-Sum Markov Games.\" arXiv preprint arXiv:2110.05682 (2021).\n\nc. Daskalakis, Constantinos, Noah Golowich, and Kaiqing Zhang. \"The complexity of markov equilibrium in stochastic games.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.\n\n3.\tFrom what I understsand, the new major results compared to the NeurIPS submission are Theorems 2 and 3. Could you please let me know if there are any other new results that I am missing?\n\n4.\tIn your future work, you mentioned the possibility of achieving A+B sample complexity instead of AB. Does the Nash V-Learning algorithm help with this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_nN5J"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697920188962,
        "cdate": 1697920188962,
        "tmdate": 1700243270588,
        "mdate": 1700243270588,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kaULO911yx",
        "forum": "vNiI3aGcE6",
        "replyto": "vNiI3aGcE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
        ],
        "content": {
            "summary": {
                "value": "This paper studies two-player zero-sum Markov games (TZMG). It proposes the model-free algorithm Memory-Efficient Nash Q-Learning (ME-Nash-QL), which achieves state-of-the-art space and computational complexity, nearly optimal sample complexity, and the best burn-in cost compared to previous results with the same sample complexity. Moreover, the proposed algorithm generates a single Markov and Nash policy rather than a nested mixture of Markov policies, by computing a relaxation of the Nash equilibrium instead, i.e. Coarse Correlated Equilibrium (CCE)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "# Originality\n- The related works are covered in detail.\n# Quality\n- The theoretical proofs seem to be rigorous. \n# Clarity\n- This paper is in general well-written and easy to follow. The design idea of the algorithm is clearly explained.\n# Significance\n- The theoretical results of this work are strong. It achieves state-of-the-art space and computational complexity, nearly optimal sample complexity, and the best burn-in cost compared to previous results with the same sample complexity.\n- TZMG is foundational and critically significant for MARL. This research has the potential to establish a new benchmark, providing a foundation for further studies in the related literature."
            },
            "weaknesses": {
                "value": "- Although the proposed algorithm is compared to Nash-VI (Liu et al., July 2021) and V-learning (Jin et al., 2022) in detail, the design idea of the proposed algorithm seems to share certain similarities with those from the two works. For example, they all compute a CCE policy and take the marginal policies; the choice of learning rate $\\frac{H+1}{H+N}$, the form of bonus terms, and the update of lower and upper bounds for Q-functions are similar. The originality of this paper could be significantly enhanced if the authors could discuss thoroughly the fundamental distinctions between the ideas of the proposed algorithm and the aforementioned Nash-VI and V-learning.\n- The theoretical findings are limited to the TZMG and CCE setting, which somewhat diminishes the overall contribution of this paper.\n- The auxiliary functions in Algorithm 2 are too nested, making it hard to read.\n### Minor:\n- There seems to be a blank section A.3.1 on page 14."
            },
            "questions": {
                "value": "- How is $\\operatorname{CCE}(\\bar{Q}, Q)$ compuated? I was anticipating a detailed introduction to its calculation to ensure the paper's comprehensiveness. An explicit explanation would greatly contribute to the paper's self-containment.\n- Is the achievement of the space complexity independent of $T$ attributed to the fact that the output policy is a single Markov policy? In this context, do the authors consider the CCE as an essential relaxation for realizing such space complexity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_M7D7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733640933,
        "cdate": 1698733640933,
        "tmdate": 1700612827715,
        "mdate": 1700612827715,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9Oiq9kvNwl",
        "forum": "vNiI3aGcE6",
        "replyto": "vNiI3aGcE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a model-free algorithm for two-player zero-sum Markov game, which enjoys low sample complexity and computational/space complexity. The resulting algorithm has optimal dependency on S and H but sub-optimal dependence on the number of actions. The algorithm design features the early-settlement method and the reference-advantage decomposition technique."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is well written and easy to follow. \n+ The proposed algorithm outperforms existing algorithms in terms of space complexity and computational complexity."
            },
            "weaknesses": {
                "value": "- My main concern is the technical novelty. The reference-advantage decomposition technique has already been incorporated in two-player zero-sum Markov game by Feng el al (2023) (not cited by this work), which achieves a regret in \\tilde{O}(\\sqrt{H^2SABT}) and matches with the regret bound in this work. The main novelty of the algorithm design thus lies in the early-settlement design in order to reduce the burn-in cost, which is not new in the literature.\n\nFeng, S., Yin, M., Wang, Y. X., Yang, J., & Liang, Y. (2023). Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. arXiv preprint arXiv:2308.08858."
            },
            "questions": {
                "value": "+ Regarding my point in weakness section, is there any other technical contributions besides reference-advantage decomposition and early-settlement design?\n\n+ Is it possible to obtain similar result for learning CCE in multi-agent general-sum Markov games?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_3yJL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771476977,
        "cdate": 1698771476977,
        "tmdate": 1700664665978,
        "mdate": 1700664665978,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5vLCXCTyJC",
        "forum": "vNiI3aGcE6",
        "replyto": "vNiI3aGcE6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model-free algorithm for learning Nash policy in Two-player Zero-sum Markov Game. The authors prove that this algorithm enjoy many benign properties, including outputting Markov policy, low computational/sample/space complexity in certain regime and low burn-in cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed algorithm enjoys several benign properties, as mentioned in the summary. In particular, the algorithm perform well when the horizon is very long while retaining other nice properties such as Markov output policy and low burn-in cost."
            },
            "weaknesses": {
                "value": "1. The proposed algorithm does not break the curse of multi-agent. Although the authors argue that there are many scenarios where horizon length is very long, I still feel that this is not general enough. I personally would still be more interested in algorithms that have $O(A+B)$ dependence in complexity.\n2. The algorithmic novelty is a bit unclear to me."
            },
            "questions": {
                "value": "1. There are many elements mentioned in the paper, such as complexity, burn-in cost, Nash policy, Markov policy etc. While I understand that no prior algorithm surpassing this algorithm in every aspect, I wonder what do the authors think is the most important aspect/what is the main focus?\n2. Can the authors explain what is the most salient algorithmic novelty to the newly proposed algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4956/Reviewer_Pgbr"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4956/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821785700,
        "cdate": 1698821785700,
        "tmdate": 1700481881121,
        "mdate": 1700481881121,
        "license": "CC BY 4.0",
        "version": 2
    }
]