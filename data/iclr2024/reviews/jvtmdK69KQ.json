[
    {
        "id": "4H3CnPdmwh",
        "forum": "jvtmdK69KQ",
        "replyto": "jvtmdK69KQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_dwg3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_dwg3"
        ],
        "content": {
            "summary": {
                "value": "Based on the Gaussian mixture expert model, the influence of top-K sparse softmax gating function on density estimation and parameter estimation was analyzed. Novel loss functions and Voronoi metrics were used to characterize the behavior of different regions in the input space."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The writing is clear and concise, making it easy to understand theory.\n\nS2. From the author's introduction, this paper is the first to perform convergence analysis for maximum likelihood estimation (MLE) under top-K sparse softmax gated Gaussian MoE. The results of this paper are innovative and helpful in inspiring new expert mixture system designs."
            },
            "weaknesses": {
                "value": "W1. The theoretical results of this paper lack experimental verification.\n\nW2. It only considers Gaussian mixture expert models and does not consider other types of models."
            },
            "questions": {
                "value": "Considering W2, I would like to know the difficulties in generalizing the results of this article to MoE methods without Gaussian assumption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Nan."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698667538076,
        "cdate": 1698667538076,
        "tmdate": 1699636805585,
        "mdate": 1699636805585,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vF4PpHqh8g",
        "forum": "jvtmdK69KQ",
        "replyto": "jvtmdK69KQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_X5io"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_X5io"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes Top-K sparse softmax gating mixture of experts. In particular, they focus on Gaussian mixture of experts and present theoretical results on the effect of MoE on density and parameter estimations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Analysis of MoE is a highly relevant and impactful direction of research.\n* The analysis seems thorough and rigorous at a high level and the presented results are interesting."
            },
            "weaknesses": {
                "value": "* The paper is quite dense, and it would have been helpful to outline high level ideas before delving into notation-heavy math.\n* Chen et al. [1] also analyze MoE, but in the deep learning setting. How does the analysis presented here differ from this work?\n\n[1] Towards Understanding Mixture of Experts in Deep Learning https://arxiv.org/pdf/2208.02813.pdf"
            },
            "questions": {
                "value": "1. What are the practical implications of the work, if any, for the use of Top-K sparse MoE as a means of conditional computation?\n2. How does this work compare to that of [1]?\n\n[1] Towards Understanding Mixture of Experts in Deep Learning https://arxiv.org/pdf/2208.02813.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865771816,
        "cdate": 1698865771816,
        "tmdate": 1699636805401,
        "mdate": 1699636805401,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gKfSIAwBML",
        "forum": "jvtmdK69KQ",
        "replyto": "jvtmdK69KQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
        ],
        "content": {
            "summary": {
                "value": "The paper provides theoretical analysis on the convergence properties of sparse softmax gating mixtures of Gaussian experts. The key contributions are proving convergence rates in two cases - when the number of experts is exactly-specified, and when there is an overspecified number of experts. \n\nFor the exact-specified case, the authors show a convergence rate of $O(n^{-1/2})$ to the true parameters under maximum likelihood estimation. This is an important theoretical result as it quantifies the sample complexity.\n\nFor the overspecified case, the convergence rate depends on the cardinality of the Voronoi cells induced by the gate activations"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The theoretical analysis of convergence rates for MoE models is novel and useful. Understanding sample complexity of different MoE architectures is valuable.\n- The paper is technically strong, with detailed proofs of the main results. The analysis for the overspecified case considering Voronoi cells is creative.\n- The assumptions are clearly laid out, making the results easy to interpret and apply."
            },
            "weaknesses": {
                "value": "- As noted, the paper is quite dense with heavy notation. More intuition and examples earlier on could make it more accessible.\n- The writing in the universal assumptions section is unclear, and should be revised for readability.\n- The implications of the theory for practitioners could be expanded on more in the discussion. Guidance on model design is lacking."
            },
            "questions": {
                "value": "- How do these convergence results compare to prior analysis on softmax vs sparse gating? Does this theory suggest one gating approach over the other?\n- Due to the instability of the EM algorithm, there is no error bar provided in Figure 3b. Can we still consider this experiment to be reliable to to justify the theoretical results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6919/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6919/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6919/Reviewer_qHcA"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6919/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699339565156,
        "cdate": 1699339565156,
        "tmdate": 1700108544057,
        "mdate": 1700108544057,
        "license": "CC BY 4.0",
        "version": 2
    }
]