[
    {
        "id": "8TTgD1TqzT",
        "forum": "NSBP7HzA5Z",
        "replyto": "NSBP7HzA5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_onZM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_onZM"
        ],
        "content": {
            "summary": {
                "value": "This work provides a re-interpretation of transformers as a probabilistic generative model of concepts and presents a new architecture, inductive transformers, that enables a stronger inductive bias towards more structured concepts. Tokens are encoded into latent variables with their own hierarchies/relations with other latent variables, and a decoder maps these latent variables back into tokens. Much machinery of the model is the same as vanilla transformers, but the operations are cleverly recast into this probabilistic framework, but there are also many additions are made to the model to accommodate this probabilistic framework. They trained their model on a toy dataset with two-token pairs and show that it can learn abstract concepts like modifiers (\"big cat\" or \"big dog\"), puts abstract concepts consistently in the same location within the model, and exhibits controlability in being able to delete concepts. This shows a potential improvement to transformers in terms of interpretability."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The re-casting of transformers into a probabilistic framework is interesting and clever. \n\n* The toy experiments shows the model has interesting properties (controllability, etc.) that can improve transformers more. \n\n* They tackle a well-motivated problem in how to build structured concepts into transformer networks."
            },
            "weaknesses": {
                "value": "* The paper is severely lacking clarity. Most of the details needed to understand the model are hidden in the appendix. Its not at all clear how this model works without digging through the appendix, and even then it takes a long time to figure out how the different components come together. This is especially puzzling given that the authors have so much extra space left in the main paper. The introduction, I feel, can be condensed a lot and the content in the supplement should be moved into the main paper such that readers of the main paper should understand how this architecture is implemented. \n\n* The model is only implemented on a very simple toy dataset (two word sentences). It would be better to see how this model can be used on a larger dataset that is more on the level of what transformers are trained on."
            },
            "questions": {
                "value": "* A lot of components of the model seem highly similar to clone structured cognitive graphs (George et al. 2021). What are the similarities/differences to their work?\n\nReferences:\n\nGeorge, D., Rikhye, R. V., Gothoskar, N., Guntupalli, J. S., Dedieu, A., & L\u00e1zaro-Gredilla, M. (2021). Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps. Nature communications, 12(1), 2392."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2683/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2683/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2683/Reviewer_onZM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774682575,
        "cdate": 1698774682575,
        "tmdate": 1699636209371,
        "mdate": 1699636209371,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lOsstp42e8",
        "forum": "NSBP7HzA5Z",
        "replyto": "NSBP7HzA5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_x6Rf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_x6Rf"
        ],
        "content": {
            "summary": {
                "value": "The paper present a new approach to inject additional inductive bias into transformers to enable tighter conceptual organization, greater conceptual control, and higher levels of conceptual abstraction. The approach is given an illustrative example simulation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper propose a generative statistical model such that recursive marginalization of athe model is in tight equivalence with the calculations performed by inference in a vanilla transformer. This idea can provide a foundation for the design of new inductive bias into transformers, which is inductive transformers."
            },
            "weaknesses": {
                "value": "It's very hard to understand the equivalence between the proposed generative model and the vanilla transformer."
            },
            "questions": {
                "value": "1. It's difficult to understand the equivalence between the generative model and vanilla transformer, is there more formal way to prove the equivalence?\n2. Can you provide more explanations on the \"concept\" meaning in both the generative model and vanilla transformer.\n3. What's the time complexity of the propose generative model's training/inference process?\n4. Is it possible to apply the proposed generative model on large scale dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806424834,
        "cdate": 1698806424834,
        "tmdate": 1699636209287,
        "mdate": 1699636209287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ao7bywtgjp",
        "forum": "NSBP7HzA5Z",
        "replyto": "NSBP7HzA5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_Yhkg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_Yhkg"
        ],
        "content": {
            "summary": {
                "value": "I had a hard time understanding the core contribution of the paper. As far as I can tell, the architecture is a sort of \"parallel grammar\" where abstract productions are learned and used to generate concrete tokens, in the same way that a PCFG might generate terminal symbols along with a complete abstract structure supporting it.  The vanilla transformer is viewed as the marginalization of this model. The hope is that by incorporating / learning abstract structure along with concrete productions, the model will have a more powerful inductive bias."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The high-level goal of the paper, which is to create transformers with strong, hierarchical and compositional inductive biases, is a great research direction.\n\nThe idea that vanilla transformers can be interpreted as the marginalization of a structured probabilistic model is intriguing; similar work has been done by Patel et al on CNNs."
            },
            "weaknesses": {
                "value": "While the high-level goals are laudable, the paper suffers from several weaknesses:\n\n- The paper does a poor job of explaining its ideas. While the introduction is reasonably accessible, I struggled to understand the probabilistic model, the production system, and the need for Bernoulli/categorical variables. I feel like this could have been framed much more clearly using terminology / frameworks that are more common in the machine learning community.  For example, this could have been framed in terms of probabilistic graphical models, or graph grammars, etc.\n\n- The empirical results were unconvincing. The training set was too simplistic, and the simple lesion done does not confirm the hypothesis of structured / compositional knowledge."
            },
            "questions": {
                "value": "none"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806494863,
        "cdate": 1698806494863,
        "tmdate": 1699636209203,
        "mdate": 1699636209203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "otxHObHSCx",
        "forum": "NSBP7HzA5Z",
        "replyto": "NSBP7HzA5Z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_XTyg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2683/Reviewer_XTyg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a way of incorporating inductive biases into the design of a vanilla transformer in a minimally invasive way: in the form of the activation functions and connections in the model. This is motivated by drawing comparisons with how humans are able to learn controllable abstract organised concepts and perform causal reasoning as a result. Successful variants of such a model might pave the way forward for models that are smaller and more capable than existing general models trained with large amounts of data and using massive computational resources."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- In my opinion, the key strength of the paper is the contribution of a straightforward model whose motivation is to achieve improved causal reasoning, iterative experimentation, long-range planning, and other cognitive abilities in language models, which are active research areas.\n- The illustrative example is intuitive and helpful to get a sense of all the advantages of the inductive transformer elicited in the paper.\n- The definitions for key ideas like identifiability, controllability are clear, increasing the readability of the paper."
            },
            "weaknesses": {
                "value": "- While the paper provides an illustrative example to demonstrate the inductive transformer's potential, it lacks empirical evaluation with large-scale experiments and/or diverse datasets, which reduces its impact in my opinion. A more comprehensive evaluation would provide a clearer understanding of its capabilities and limitations.\n- The introduction of additional inductive biases could potentially result in an increased model complexity and training cost. It would be helpful to discuss the trade-offs between model performance and computational resources, as well as practical scalability issues that may arise.\n- The paper focuses on concept learning, but does not discuss in detail the interpretability and explainability of the inductive transformer. Understanding how this model forms concepts and making its decision-making processes more interpretable is important in assessing its utility."
            },
            "questions": {
                "value": "- If we agree with the premise that inductive transformers enable learning of abstract concepts, how would we go about generalising these concepts to a wide range of tasks and domains? Each task/domain might warrant different inductive biases being incorporated into the model architecture, resulting in a complete loss of generality from one task to another. Would that be the case?\n- Could you discuss a potential merge of base models that have been trained to perform next token prediction and the minimal architectural changes to the transformer that result in the inductive transformer? Or would the proposed architecture entail completely getting rid of base models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2683/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699228227690,
        "cdate": 1699228227690,
        "tmdate": 1699636209124,
        "mdate": 1699636209124,
        "license": "CC BY 4.0",
        "version": 2
    }
]