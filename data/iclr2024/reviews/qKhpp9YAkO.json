[
    {
        "id": "4UjOxgZsgJ",
        "forum": "qKhpp9YAkO",
        "replyto": "qKhpp9YAkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_7hau"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_7hau"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the Associative Transformer (AiT) \u2014 a special Transformer block that supplements the normal (Attention, MLP) Transformer Blocks that augments the Transformer with associative memory. This special layer allows tokens from different samples in a batch to interact with each other via a modified attention operation. The model improves the baseline set by the original ViT at the cost of a few extra parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## Novel and reasonably well-defended\n- (+) The ablation study shows that we can expect a consistent ~2% performance gain on image classification tasks by including the GWT sub-layer into a Transformer Block.\n- (+) The computational complexity of using more tokens in the attention is mitigated by an intelligent use of \"hard attention\" to select only the most meaningful tokens in the squashed input.\n- (+) To my knowledge, this supplemental memory sub-block is novel."
            },
            "weaknesses": {
                "value": "## Paper lacks clarity in some areas\n\n1. (- -) Allowing samples across batches to compete via the bottleneck attention is inelegant -- inherently, this competition means that the ViT's prediction on an input is dependent on what samples are in the same batch. What is the performance if we use batch size = 1 during evaluation? What about different batch sizes? Unless I am misunderstanding something, it would be necessary to report standard deviation across different evaluation batch sizes when reporting metrics for this kind of network.\n2. (-) I do not find the attention maps in Fig 2 meaningful. Which layer's memory bank was used to display this attention map? How is the attention heatmap smooth if it operates on patches (not pixels)? These questions are not answered in the paper.\n3. (-) I suspect there are many more FLOPS needed for this network if the Hopfield Network component of the GWT sub-block employs recurrence. If true, this should be reported and quantified as a limitation of the model.\n4. (-) Fig. 1 is very unclear to me. See questions."
            },
            "questions": {
                "value": "I have a few confusions the authors could help clarify, many related to the display and caption of Fig. 1.\n\n1. Caption of Fig 1 says: \"...squashed into vectors $\\mathbb{R}^{(B \\times N) \\times E}$\". Is this not a matrix, where you combine the batch and token dimension? Perhaps this means a collection of vectors, but then the figure itself is confusing because we see only one vector passed into the bottleneck attention.\n2. \"MASK\" is not defined in Fig 1\n3. In Fig 1, there are two blocks of \"Explicit Memory\" -- do these represent the same matrix?\n4. In Fig 1, why are there three inputs to the Hopfield Network `(Explicit Memory, LT(explicit memory), state)`? Where is the update equation for the Hopfield Network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698850317711,
        "cdate": 1698850317711,
        "tmdate": 1699636043763,
        "mdate": 1699636043763,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4JTOuJlyBE",
        "forum": "qKhpp9YAkO",
        "replyto": "qKhpp9YAkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_vNUE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_vNUE"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a new neural network model called Associative Transformer (AiT) that uses a sparse attention mechanism to process input data based on biological principles. The model divides the input data into subsets and associates each subset with others to share information between them. The experimental results show that the AiT model outperforms traditional Transformer models and other methods in multiple vision tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The model is novel inspired by human brain's associative memory."
            },
            "weaknesses": {
                "value": "1. The paper does not include any ablation studies.\n2. The experimental evaluation of the model is limited to a few vision datasets. \n3. The work does not provide the available code."
            },
            "questions": {
                "value": "1. How does the model's performance vary when different types of priors are used in the workspace memory?\n2. How does the model's performance vary when different types of attention mechanisms are used?\n3. What are the limitations of the proposed model?\n4. The baseline seems not include SOTA models in vision tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1173/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1173/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1173/Reviewer_vNUE"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699086043116,
        "cdate": 1699086043116,
        "tmdate": 1699636043705,
        "mdate": 1699636043705,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wNXUUWvsk5",
        "forum": "qKhpp9YAkO",
        "replyto": "qKhpp9YAkO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_H5y7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1173/Reviewer_H5y7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the Associative Transformer (AiT), inspired by global workspace theory and (Modern) Hopfield networks.  They introduce a new module called a global workspace layer (GWL) that can be stacked after a standard Vision Transformer (ViT) module and demonstrate that their module can lead to modest improvements in image classification tasks compared to ViT and greater gains compared to other models, some of which however do better than AiT on relational reasoning tasks. They perform several ablations that help delineate which aspects of their model contribute the most to its success, and include one experiment that suggests their module can produce an enhancement of performance relative to a ViT without their added module."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed GWL introduces several rich properties into the computation of visual representations, and I found myself feeling that it was useful to explore the properties of these innovations.\n\nThe ablation experiments helped clarify the roles of different features of the GWL, and the experiment comparing AiT to more vanilla ViT modules suggested a possible advantage of the model that would be worth more fully understanding in future work.  The comparisons with the coordination method also introduced some potentially interesting advantages of the method on the CIFAR 10 dataset."
            },
            "weaknesses": {
                "value": "As a general reaction, this seems like exploratory work, and it's possible that the next iteration will be a big improvement.  I hope the following comments are food for thought in case others agree with my assessment that this iteration is not above threshold for acceptance.\n\nFor me the most important limitation of the work is the overall complexity of the GWL together with the relatively small advantage it offered compared to the standard ViT and even to compared to simple Feed-forward neural networks (called Dense in the ablation study).  Without a bigger advantage over these systems, and with the shortcomings of AiT on relational reasoning, it is hard to see clear evidence of an advantage for AiT over existing methods, especially given its complexity.  \n\nAlthough an experiment with the Pet dataset did suggest that with larger data sets the advantage of AiT could be larger, performance of both models was very poor, and if I understand correctly the AiT had far more parameters (there was no compensation for the added parameters in the GWL's added to the model).  The only place where I saw a consideration of parameters was in the exploration of integrating features of the GWL into variants of the coordination model.\n\nIn general, it seems to be a weakness that the GWL is an add, not a replacement, for the ViT module.  \n\nI also had difficulty understanding several aspects of the model, which I ask about under questions below."
            },
            "questions": {
                "value": "Responses to these questions or strong rebuttals to the weaknesses I've raised could potentially increase my rating.  In any case I'd be interested in seeing responses to make sure I've understood.\n\nI was uncertain about the functioning of the Hopfield net.  Do these equations describe an iteration that is applied some number of times?  If I'm correct, the t is not the iteration time step -- it is the batch time step -- but perhaps an iteration time step is implicit. In that case, the expression for \u02c6\u03bet in Eq (8) right, could be understood as the minimum of \u03bet over these implicit time steps.  Is all this correct?  If not, please explain.  If so, why not just use the final time value at the end of the settling process?  Are these values reaching their minimum at different times for different patches?  Is the global energy \u039et guaranteed to decrease over time steps?  If not, in what sense is this really an attractor network?\n\nIn introducing the GWL, the text states: \"The global workspace layer can be seen as an add-on component on the monolithic Vision Transformer, where the feed-forward layers process patches before they enter the workspace, facilitating abstract relation learning, and the self-attention learns the contextual relations for a specific sample. The global workspace layer learns spatial relations across various samples and time steps.\"  This text made me think that the initial hope was that the model would actually improve relational reasoning.  It did not, but this was attributed to 'the difficulty in accurately reconstructing question representations from the memory'.  This is testable by using the AiT and comparison models to derive and image representation for use as a component of a combined model that gets its embedding of the question from a separate language-processing module.  If you have results from such an experiment, it would be interesting to hear about them.\n\nIn the ablation studies, I was uncertain how to interpret the W/0 Hopfield ablation.  The text says \"W/O Hopfield evaluates the model performance when we replace the Hopfield network with another cross-attention mechanism.\"  I need a much more explicit understanding of this.  What exactly is being replaced with what.  Also, please clarify the meaning of 'W/O SA examines performance when the self-attention\ncomponent is excluded'.  Is the self-attention component the multi-head attention block in fig 1b?  This led to major decrements, but there was no mention of this fact or its meaning for our understanding of the proposed architecture.\n\nCan you comment on my sense that it is a weakness of the paper that the GWL is an add, rather than a replacement, to the ViT module?  Would one way to balance parameters be to reduce the total stack depth of the ViT when adding GWLs, as these are really added layers?  Have you tried that?\n\nDemonstrations of bigger advantages that might have been obtained since submission could potentially cause me to rate this paper higher.\n\nPlease address the issue of the lack of control for number of parameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1173/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699518932071,
        "cdate": 1699518932071,
        "tmdate": 1699636043607,
        "mdate": 1699636043607,
        "license": "CC BY 4.0",
        "version": 2
    }
]