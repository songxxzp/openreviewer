[
    {
        "id": "GcS5nItygB",
        "forum": "Nfu3bUkmdH",
        "replyto": "Nfu3bUkmdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests employing back-translation for few-shot, in-context learning of machine translation for low-resource languages. Initially, synthetic examples are generated by instructing the model to translate into English, utilizing few-shot, in-context learning with a variety of examples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea is simple to follow and the author tested the idea across diverse set of languages. They also show improvement compared to some baselines."
            },
            "weaknesses": {
                "value": "A major weakness in this work is that the author essentially reintroduces the concept of back-translation, a well-established technique in machine translation. Yet, this paper does not make any reference to the original, popular back-translation work by Sennrich in 2016, which raises questions about the author's familiarity with prior research and the potential reinvention of existing concepts. \n\nThe primary distinction is that it is now presented in the form of a few-shot in-context learning, rather than for training purposes. One of the proposed comparisons involves fine-tuning using synthetic data in the opposite direction, which basically is the original back-translation concept. The absence of this reference is significant because it represents the core idea of this paper. See also my 2nd question.\n\nThe synthetic back-translation data was generated by providing in-context examples across a diverse set of languages (Figure 1a). However, I believe that this crucial idea is not thoroughly explored, considering the potential variability in the types of languages, examples, diversity that could be explored. Conducting an ablation study involving different examples and languages would strengthen the paper's claims. Also, while the author comments that LDP method (Figure 1c) is superior to the standard few-shot approach (Figure 1b), there is a lack of experimental results to substantiate this claim."
            },
            "questions": {
                "value": "- Is there any specific reason on choosing BLOOM over BLOOMZ (instruct-tuned version of BLOOM)? I think it will be more fair comparison vs InstructGPT.\n\n- One of the strengths of the original back-translation approach lies in its ability to generate synthetic data at scale, and the size of the generated data can influence performance. However, I am uncertain about the data size used for your fine-tuning comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697536641243,
        "cdate": 1697536641243,
        "tmdate": 1700716314821,
        "mdate": 1700716314821,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OUTb9Fptgo",
        "forum": "Nfu3bUkmdH",
        "replyto": "Nfu3bUkmdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_y5rc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_y5rc"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \u201dlinguistically diverse prompting\u201d (LDP), a method aimed at improving prompt-based generative task performance in languages for which there are no available few-shot exemplars. In this method, few-shot in-context learning is enabled through leveraging in-context exemplars from various (higher-resource) languages to \u201dlocate the task\u201d. The authors find that their approach achieves at least comparable performance w.r.t. supervised methods for translation and multilingual summarization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The presented method is well-motivated by the observations from the literature and addresses a concrete problem in low-resource NLP (i.e. lack of in-context exemplars for some languages).\n\n2. Evaluation is rigorous and the analyses (section 4.4) provide valuable insights for this line of research."
            },
            "weaknesses": {
                "value": "Summary of Weaknesses\n\n1) Flawed linguistic diversity: this paper claims linguistic diversity mainly on the basis of selecting languages with different scripts: \u201dto ensure diversity \u2026 characters are used\u201d (page 1). However, this is not done systematically (the authors \u201dinclude various script types\u201d and later mention \u201ddissimilar lexical and regional characteristics\u201d but do not explain the exact selection process). Moreover, this misses important aspects of linguistic diversity that are captured by for instance taking into account phylogeny.\n\n2) This paper has reproducibility issues. The results in the paper cannot be replicated, as the approaches are evaluated on 200 randomly sampled sentences from the test set, while there is no explanation or source provided that details which sentences are included or how to reproduce this selection (e.g. which random seed). Random data selection is also used in one of the baselines, namely supervised prompting (A.2) without providing details."
            },
            "questions": {
                "value": "1. Is LDP truly an unsupervised prompting method, or are some aspects more like obtaining data for a kind of weak supervision?\n\n2. What are your criteria for distinguishing high-resource from low-resource languages?\n\n3. In what way does improving prompting performance for certain low-resource scenarios \u2019democratize\u2019 LLMs (title)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756301728,
        "cdate": 1698756301728,
        "tmdate": 1699636063021,
        "mdate": 1699636063021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jv85dyzNtv",
        "forum": "Nfu3bUkmdH",
        "replyto": "Nfu3bUkmdH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors tried to improve the LLMs performance on low-resource languages by creating synthetically diverse prompts in high resource languages. The authors show the effectiveness of their approach in translation and summarisation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "It is an interesting approach to get good performance on Low-Resource set up. The experiments are promising."
            },
            "weaknesses": {
                "value": "The results seems promising. The authors should provide more details about:\n\n(a) How the diverse language sets are selected? Do they observe any correlation on linguistically similar language selection vs a random set of languages?\n\n(b) Did they study the relation of number of languages to be selected and number of examples in the prompt?\n\n(c) Were the prompt set fixed for every test instance? \n\nAlso, it would be interesting to see the performance difference of selecting prompts from diverse languages vs creating synthetic prompts for just the pair of languages of interest."
            },
            "questions": {
                "value": "The paper would be sound if the authors can explain / provide experimental evidences on prompt selection as pointed out in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824071451,
        "cdate": 1698824071451,
        "tmdate": 1700716579415,
        "mdate": 1700716579415,
        "license": "CC BY 4.0",
        "version": 2
    }
]