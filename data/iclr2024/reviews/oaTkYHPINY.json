[
    {
        "id": "HJNWkEUmBh",
        "forum": "oaTkYHPINY",
        "replyto": "oaTkYHPINY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_t2HG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_t2HG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a \"mixture of prompts\" technique, which selects relevant prompts for a prediction task based on a pre-trained routing module. In experiments, they find their method outperforms a prompt tuning baseline in a standard and federated learning setup interms of perplexity on Dolly and Super natural instructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of routing relevant prompts is useful -- it could make learning a new task quite rapid and only requirese inputting new prompts \n- The presented results are promising in terms of improving performance. I think there are benefits of this method more generally in terms of ease of use -- it seems simpler to use than a prompt finetuning strategy."
            },
            "weaknesses": {
                "value": "There are some discussion points / experimental evaluations that would be useful to consider:\n- A conceivable alternative to this method is including all available prompts in the context window of the language model for few-shot learning -- what's the motivation for using something like this method over this? Does it mitigate the \"interference\" described in the paper? Are there additional memory + computational constraints?\n- Moreover, there are many peft techniques that have shown promise in learning from minimal labeled examples. I understand it may not be possible to baseline all of them, but including the most frequently used techniques, like LORA, would make the results more compelling to many readers to consider this method. I understand there are efficiency motivations here as well, but it would be useful for many readers in the centralization training I believe.\n- There are a few simple baselines to consider for this method. In general, it seems like the method upweights relevant prompts in the weighting module for use for the task. It's fairly common just to select prompts for a particular task using KNN on embeddings from a larger set. How would this compare to the current technique? Or, is it expected to be worse for some reason?"
            },
            "questions": {
                "value": "- Is this method sensitive to the ordering of prompts?\n- Why perplexity for supernatural instructions? Dolly makes sense, but I believe this benchmark typically uses rouge-l -- I didn't see this choice explained and am curious."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698119685859,
        "cdate": 1698119685859,
        "tmdate": 1699636730977,
        "mdate": 1699636730977,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K49X5omUEK",
        "forum": "oaTkYHPINY",
        "replyto": "oaTkYHPINY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_nzzG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_nzzG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a mixture-of-prompts (MoP) method for multi-task, multi-source prompting tuning in LLMs. The key innovation of the method is that instead of using a single prompt to tune all tasks, which the authors claim could lead to degraded performance due to interference between tasks, MoP prompt tuning trains multiple soft prompts and uses a dynamic, learnable gating functionality to scale the attention weights in a task-dependent manner among the different prompts. The authors show the effectiveness of their methods in compressed LLMs and both centralised and federated setups and show improvement over baseline prompt tuning methods in the setups."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper studies a specific problem setup that is both of great importance and is currently rather less extensively studied. While prompt tuning in different flavours has been extensively studied in the literature, multi-task, multi-source training in compressed and/or federated setup is not. Such a setup is both more challenging and more practical and thus is of great research and practical value to the community (some caveats to this point are stated in \u201cWeaknesses\u201d).\n- The research problem is of relevance, the hypothesis is reasonable, and the method proposed is sound. Inspired by the mixture of expert literature, addressing the prompt tuning problem with heterogeneity with MoP is natural and intuitive. The benefits of the key ideas, such as the use of smart gating and its pretraining, are evidenced well by the experiments (such as Fig 2)"
            },
            "weaknesses": {
                "value": "- While I appreciate the investigations in the setup as mentioned in \u201cStrengths\u201d, it seems that the method proposed is generic and not catered specifically to the federated or compressed setup, and these considerations are orthogonal. The experiments, however, exclusively test the MoP method in models aggressively compressed with pruning. I\u2019m not sure if this is due to computational constraints, but I wonder how the method will perform in a less aggressively compressed LLM. It seems that the extent of gain decreases as pruning moderates. The authors made a comment on Page 9 that \u201cif the pruning ratio is too low, there may not be enough room for improvement.\u201d \u2014 I think the paper would benefit if the authors could add more explanations as to why this is the case.\n- I also have some concerns regarding baseline comparisons and discussions w.r.t. the literature. The authors only consider a baseline proposed by Xu et al., 2023, which is standard prompt tuning on a compressed model after compression takes place. However, several other, arguably stronger baselines addressing the similar problem of prompt interference in multitask prompt tuning setup, as identified in Sec 3, are available, like [Wang 2023] (who seems to show that using a single prompt for many tasks works fine as long as we do low-rank task-specific updates afterwards) and [Vu 2022] \u2014 these works do not specifically target the compressed setup, but there\u2019s no reason why they can\u2019t work in it; it would be nice if the authors could discuss these works qualitatively and quantitatively. Additionally, without a comparison, I feel that the authors claim that their methods synergise with compression/federation learning setups particularly well (*\u201cVia experiments, we have observed an emerging ability of MoPs: they work out of the box, regardless of any reasonable model compression ratio or technique (i.e. pruning, quantization)\u201d* is a bit controversial, as the baseline prompt tuning methods may potentially work *generally* very well (note that Xu et al., 2023 that the authors compared against already shows such a trend) and the good performance is not due to the MoP design *specifically*. If it is indeed the case, the argument should be toned down a bit, in my opinion.\n    - Several works use the mixture of prompt/adapter ideas that are missing, such as [Wang 2022], although I agree that the motivation or the problem setup is not identical. In many of these works mentioned above and works like Attempt (Asai et al., 2022, which the authors cited but did not discuss in detail), mechanisms similar to the gating function that authors claimed as a key contribution were proposed and I encourage the authors to compare and contrast.\n- Better design ablations should be done: for example, the authors proposed to use the LLM intermediate embedding instead of a separate embedding for the gating function, and they decided to use the embedding after the 10th layer. These design decisions seem a bit ad-hoc; it\u2019d be nice if ablations were done to demonstrate the impact of different design aspects on the final performance.\n\n\n### Minor:\n- It is better to dedicate Sec 2 entirely to background and related work for clarity of writing, but the authors blended their own proposals, such as the injection of prompts in the middle layers in this section. Separating the two components would help with clarity and flow.\n\n\n## References\n[Wang 2023] Wang, Z., Panda, R., Karlinsky, L., Feris, R., Sun, H., & Kim, Y. Multitask prompt tuning enables parameter-efficient transfer learning. ICLR 2023.\n\n[Vu 2022] Vu, T., Lester, B., Constant, N., Al-Rfou, R., & Cer, D. Spot: Better frozen model adaptation through soft prompt transfer. ACL 2022\n\n[Wang 2022] Wang, Y., Agarwal, S., Mukherjee, S., Liu, X., Gao, J., Awadallah, A. H., & Gao, J. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. EMNLP 2022"
            },
            "questions": {
                "value": "Please address my concerns in *Weaknesses*.\n\nI will reconsider the rating upon a satisfactory response and reading the other reviews."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698554999045,
        "cdate": 1698554999045,
        "tmdate": 1699636730855,
        "mdate": 1699636730855,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cpJwjfKMQd",
        "forum": "oaTkYHPINY",
        "replyto": "oaTkYHPINY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_YeGh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_YeGh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed gating function is able to identify relevant skills for the current task and dynamically select and combine prompts accordingly. The authors claim that the proposed method can overcome prompt training interference from multi-tasks across centralized and federated learning scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper utilized a mixture of prompts to deal with heterogeneous tasks and data distributions. The performance seems to improved a lot compared with the baseline."
            },
            "weaknesses": {
                "value": "1. A mixture of prompts has been proposed in the NLP field in 2021 [1].\n\n[1]. Qin, Guanghui, and Jason Eisner. \"Learning how to ask: Querying LMs with mixtures of soft prompts.\" arXiv preprint arXiv:2104.06599 (2021).\n\n2. The presentation of this paper makes me confused. Is the \"Mixture of Prompts for LLM Task Adaptation\" designed specifically for federated learning? I mean whether the method is designed for multi-task, multi-source scenarios. The multitask prompt tuning method proposed in [2] also enables parameter-efficient transfer learning. Is the method in this paper an incremental setting based on [2].\n[2]. Zhen Wang, \"Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning\", ICLR 2023.\n\n3. How do you observe the \"aggressively compressed LLMs\"? It seems that the authors utilized SparseGPT to get the compressed LLM. If so, any contribution to this point?\n\n4. I think this paper combined many existing technologies into a good technique report."
            },
            "questions": {
                "value": "see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6509/Reviewer_YeGh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755374192,
        "cdate": 1698755374192,
        "tmdate": 1699636730721,
        "mdate": 1699636730721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "P1DYKBCnhB",
        "forum": "oaTkYHPINY",
        "replyto": "oaTkYHPINY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_g7Lw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6509/Reviewer_g7Lw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an innovative method for improving the efficiency and adaptability of large language models (LLMs) in handling diverse tasks and data distributions. This method, called Mixture of Prompts (MoPs), uses smart gating functionality to dynamically select and combine different groups of prompts, acting as 'experts,' based on the task at hand."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper introduces a novel approach to prompt tuning using Mixture of Prompts (MoPs) with a smart gating function. This is a significant contribution as it addresses the challenge of task and data heterogeneity, which is particularly relevant for real-world applications of LLMs.\n\n+ The paper demonstrates that the MoPs method is resilient to various model compression techniques, indicating that it can maintain performance even when computational resources are limited or when efficiency is a priority.\n\n+ The proposed method shows a substantial decrease in final perplexity compared to baselines in both federated and centralized scenarios. This empirical performance is well-documented through experiments, providing strong evidence for the effectiveness of MoPs."
            },
            "weaknesses": {
                "value": "- While the paper introduces an innovative approach, it does not thoroughly discuss the complexity involved in implementing the MoPs method. Practical application details are crucial for adoption, and this could be an area requiring further clarification.\n\n- The experiments conducted to demonstrate the efficacy of the MoPs method are limited to only two datasets. A broader range of datasets and scenarios would provide a more comprehensive understanding of the method's performance and generalizability.\n\n- Although the paper mentions the mitigation of model drift problems, there is a lack of in-depth analysis of how the MoPs approach ensures the model's stability over time, especially in federated learning scenarios where model drift is a significant concern."
            },
            "questions": {
                "value": "1. The gating function's ability to dynamically select and combine prompts is a key contribution. Could the authors elaborate on how this function can scale with increasingly complex tasks or larger sets of heterogeneous tasks?\n2. While the method shows resilience to model compression, could there be a threshold of model complexity below which the MoPs method starts to underperform? Is there a way to anticipate or calculate this threshold?\n3. Given that the paper showcases empirical results, is there a plan to evaluate the MoPs method in a live, real-world environment, where tasks are not predefined and data distribution can be unpredictable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699243454404,
        "cdate": 1699243454404,
        "tmdate": 1699636730608,
        "mdate": 1699636730608,
        "license": "CC BY 4.0",
        "version": 2
    }
]