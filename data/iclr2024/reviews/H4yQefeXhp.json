[
    {
        "id": "0uAwO5JwiM",
        "forum": "H4yQefeXhp",
        "replyto": "H4yQefeXhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes DMV3D, a 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. The reconstruction model incorporates a triplane NeRF representation and, functioning as a denoiser, can denoise noisy multi-view images via 3D NeRF reconstruction and rendering, achieving single-stage 3D generation in the 2D diffusion denoising process. The model is trained on large-scale multi-view image datasets of extremely diverse objects using only image reconstruction losses, without accessing 3D assets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The first contribution of this paper is to scale 3D diffusion generative models to very diverse categories and objects. Previous models such as DiffRF, etc. can only generalize within some shapenet like datasets with no more than 13 categories.\n\n2. The model demonstrates a novel method to conduct multiview diffusion. Instead of build attentions across views like mvdreamer or syncdreamer, they use attention to attend with learnable triplane tokens and with each other, therefore incorporating the 3D spatial prior in the process.\n\n3. The model shows good results of 3d generation, especially high quality geometry, which alwyas fail in SDS or nerf2nerf lines of works."
            },
            "weaknesses": {
                "value": "1.It seems the model learns from the objaverse and mvimagnet, which contain mostly single objects or separated objects. even the examples in out of domain results, in figure 6, the objects are not complicated as people use in SD-based models."
            },
            "questions": {
                "value": "1. As mentioned in weakness 1,  I would like to see some results of \"bunny seating on pancake\", this kind of generation. Even it is hard to do text to 3d, since the training set doesn't have compound objects, is it possible to do 2d conditioned 3d generation with this kind of prompt?\n\n2. The author mentioned in the 2d conditioned 3d generation task, they do not add noise to the reference view, however, some of other diffusion models usually also add noise to the reference view and each step, use the gt x0 of the that view and add new noise in ancestral sampling. The logic behind is the model is trained with noise images paired with the corresponding time step embedding, the clean image strategy will shock the model in inference. I wonder, in inference, if this clean ref image strategy can bring benefit over adding noise from x0."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_X5Dv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698048121950,
        "cdate": 1698048121950,
        "tmdate": 1700471632900,
        "mdate": 1700471632900,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fmP5tKmVP1",
        "forum": "H4yQefeXhp",
        "replyto": "H4yQefeXhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to generate novel 3D objects based on a diffusion model that encloses a large reconstruction model. To leverage the strong generative power of 2D models while improving the 3D consistency of the generated objects, the diffusion model operates on the domain of multi-view images and internally learns a transformer-based reconstruction model to build a 3D representation, which is later rendered into denoised output images. In order for the model to generalize across different categories, the reconstruction model uses the DINO features to bootstrap the features used for deriving tokens. Results show that by training jointly on the Objaverse dataset and MVImageNet dataset, the model is able to generate diverse shapes, conditioned either on images or texts. The usage of the transformer-based large reconstruction model improves the 3D consistency while maintaining a good generation quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and well presented. The notations are clear and the illustrations are informative. It's easy to read and understand most of the technical details and design choices.\n- Though conceptually similar to, e.g., MVDiffusion and RenderDiffusion (or diffusion with forward models), the method elegantly combines the advantages of both works with the help of a generalizable large reconstruction model using transformers, hence lifting the previous constraints within only one single category.\n- The method naturally enables conditioning over images by fixing the diffusion variables, leading to a new scheme for bridging 2D and 3D domains.\n- The generated shapes are of high quality and surpass the baselines by a considerable margin."
            },
            "weaknesses": {
                "value": "- In contrast to Image-based diffusion models, the runtime efficiency might be compromised since the reconstruction model operates during every iteration of sampling. Investigating whether the reconstruction can be repurposed or distributed over the intermediate denoising phases could be insightful, especially since the current intermediate reconstructed model is discarded (would be great if they could be visualized), leading to potential wastage.\n\n- The textures produced lack sharpness. Exploring the proposed framework's performance on higher-resolution images and 3D triplanes would be intriguing. Additionally, employing a hybrid representation that decouples geometry and textures could yield enhanced results."
            },
            "questions": {
                "value": "- How the camera viewpoints are sampled during the training process? Would the reconstruction model easily fall into a local minima where the 3D results become trivial by generating planes that are parallel to the image planes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_PYdg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698633163695,
        "cdate": 1698633163695,
        "tmdate": 1699636002527,
        "mdate": 1699636002527,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BJfGVhUBQy",
        "forum": "H4yQefeXhp",
        "replyto": "H4yQefeXhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an approach to 3D generation via a single-stage diffusion model. By denoising multi-view image diffusion, the authors aim to generate realistic 3D assets. Central to this methodology is a large transformer model that processes multi-view noisy images to reconstruct a clean triplane NeRF, subsequently yielding denoised images through neural rendering. The proposed method showcases flexibility, supporting both text- and image-conditioning inputs, and claims rapid 3D generation without requiring per-asset optimization. The approach is evaluated and shown to be superior to previous 3D diffusion models in certain domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(+) The paper showcases impressive results in 3D generation compared to prior methods. \n\n(+) The method's ability to accommodate text- and image-conditioning inputs augments its versatility, making it potentially suitable for diverse applications.\n\n(+) The paper is well-structured and clearly explains both the methodology and foundational design choices."
            },
            "weaknesses": {
                "value": "Although the paper showcases promising results and a solid methodology; however, its level of novelty is unclear:\n\n- It appears that the proposition combines techniques that have been used before. The 3D diffusion part of the proposal seems to have been influenced by \"Viewset Diffusion (ICCV 2023)\", while the design and training approach of the large-scale transformer model is similar to \"LRM: LARGE RECONSTRUCTION MODEL FOR SINGLE IMAGE TO 3D\", which was also submitted to ICLR 2024.\n- Concerns have been raised about potential overlap with the LRM manuscript, questioning submission singularity.\n\nBesides, a balanced perspective is lacking due to the absence of discussion on the paper's limitations, which could provide valuable insights for potential areas of improvement."
            },
            "questions": {
                "value": "- Given the inherent training characteristics of diffusion models, how does DMV3D achieve a training timeframe analogous to LRM? It would be helpful if the authors could provide an explanation.\n- It is important to clarify the extent of overlap between this work and the LRM submission in order to understand the distinctiveness of the contributions in this manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission753/Reviewer_nAvZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846627682,
        "cdate": 1698846627682,
        "tmdate": 1700484738472,
        "mdate": 1700484738472,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oL0ZLtFk2x",
        "forum": "H4yQefeXhp",
        "replyto": "H4yQefeXhp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission753/Reviewer_3SCV"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a 3D generation method that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. The proposed method supports both text- and image-conditioned 3D generation. Experimental results seem promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of directly denoising a triplane-based NeRF is interesting. The result of multi-view diffusion is promising."
            },
            "weaknesses": {
                "value": "1. Does the method use a pre-trained stable diffusion model or train the DDPM from scratch? If from scratch, how is the generalization ability guaranteed?\n\n2. For multi-view diffusion, what is the number of views for training and inference?"
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission753/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699137686233,
        "cdate": 1699137686233,
        "tmdate": 1699636002396,
        "mdate": 1699636002396,
        "license": "CC BY 4.0",
        "version": 2
    }
]