[
    {
        "id": "QbknFSrWFL",
        "forum": "r2Ji0Bzd4g",
        "replyto": "r2Ji0Bzd4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_pKAh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_pKAh"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors focus on the lightweight image super-resolution problem. The main contribution of this paper is exploring a flexible meta pruning technique which combines the structured and unstructured network pruning. Experimental results indicate the superiority of the proposed method as compared to existing lightweight image super-resolution methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe proposed flexible meta pruning technique combines structured and unstructured network pruning, enabling flexible pruning.\n2.\tThe proposed network achieves better performance than most existing lightweight image super-resolution methods,"
            },
            "weaknesses": {
                "value": "1.\tThe introduction of the proposed methods appears somewhat confusing. The proposed pruning method combines both structured and unstructured pruning, but it is difficult to distinguish their specific implementations and how they are integrated when reading this paper. Are channels being pruned and weights set to zero, or is it only utilizing sparse regularization for structural pruning.\n2.\tThe comparison with existing methods lacks a comparison of runtime. \n3.\tTable 6 is challenging to understand.\n4.\tThe advantages of this method in terms of flexibility compared to standalone structured and unstructured pruning need to be further emphasized.\n\n==============After rebuttal===========\nThe authors responses address most of my concerns, so I am willing to increase the score to 6"
            },
            "questions": {
                "value": "1.\tThe proposed pruning method combines both structured and unstructured pruning, but it is difficult to distinguish their specific implementations and how they are integrated when reading this paper. Are channels being pruned and weights set to zero, or is it only utilizing sparse regularization for structural pruning. \n2.\tAs a pruning method, how does it perform when applied to existing super-resolution networks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Reviewer_pKAh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1278/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698300812467,
        "cdate": 1698300812467,
        "tmdate": 1700639685058,
        "mdate": 1700639685058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fAQzftAsHr",
        "forum": "r2Ji0Bzd4g",
        "replyto": "r2Ji0Bzd4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_uZjY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_uZjY"
        ],
        "content": {
            "summary": {
                "value": "The manuscript introduces a novel approach, Flexible Meta Pruning (FMP), designed for lightweight image super-resolution. FMP entails the incorporation of network channel vectors and weight tensors into a hypernetwork, facilitating simultaneous pruning and optimization through proximal gradient and SGD. Additionally, the manuscript substitutes the residual block in RLFN with a simplified version and introduces the Lightweight SR Baseline (LSRB). The application of FMP to LSRB demonstrates competitive performances with relatively low computational complexity in several super-resolution experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1)\tThe manuscript is presented clearly and well-structured, complemented by easily understandable figures and charts.\n(2)\tThe concept of concurrently combining multiple pruning techniques is intuitive, and the proposed method is straightforward to comprehend and follow.\n(3)\tThe manuscript includes rich ablation studies, and experimental results effectively showcases the superior visual performance achieved by the proposed method."
            },
            "weaknesses": {
                "value": "(1)\tThe manuscript's contribution seems to be incremental, primarily involving the addition of kernel weight pruning based on DHP [1]. While the primary distinction between the proposed method and DHP lies in kernel pruning, the results shown in Table 6 indicate that simultaneous channel and kernel pruning may not significantly show effectiveness over channel pruning alone.\n(2)\tSome of the experimental comparisons may be perceived as lacking in both fairness and completeness when it comes to demonstrating the effectiveness of the method. Notably, in Figure 1, the analysis exclusively focuses on showcasing the influence of FMP. However, it's important to note that the baseline models utilized for evaluating FMP and other comparative methods differ. This dissimilarity in the baseline models pose challenges when attempting to assess the precise impact of FMP. Besides, in subsection 4.2, the experimental comparisons fail to cover some classic lightweight SR models including RLFN and ELAN, which are also mentioned in Section 1 and Section 2\n(3)\tThe manuscript does not sufficiently address the compatibility and generalizability of the proposed FMP method. Although FMP was developed for lightweight super-resolution, experiments have only been conducted on one model, LSRB. The suitability of applying FMP to other super-resolution models remains unexplained.\n\nReference\n[1] Li Y, Gu S, Zhang K, et al. Dhp: Differentiable meta pruning via hypernetworks[C]//Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VIII 16. Springer International Publishing, 2020: 608-624."
            },
            "questions": {
                "value": "(1)\tWhat optimization method was employed for the FMP method outlined in the manuscript? While the introduction section mentions SGD and proximal gradient descent, the experimental setup section suggests the use of Adam for FMP. This inconsistency is expected to be clarified.\n(2)\tLSFB represents a lightweight network achieved by replacing residual blocks in RLFN with simplified versions, and FMP is a lightweight method. Table 3 indicates that LSFB+FMP exhibits higher model parameters and computational complexity compared to RLFN. Could you provide an explanation for this discrepancy? Furthermore, LSFB+FMP demonstrates increased inference speed, despite the primary goal of parameter optimization not being fast adaptation in meta-learning. The reasons behind the improved inference speed is expected to be addressed and analyzed.\n(3)\tFMP introduces kernel weight pruning in addition to DHP, and the results in Table 4 is intended to show that FMP outperformances DHP. However, the combination of multiple pruning techniques may not always lead to an improved result. Besides, it seems that the higher the prune ratio, the better for the performances of both DHP and FMP. Both appear to be counterintuitive to some extents. Therefore, more detailed analysis and explanation are necessary here.\n(4)\tFormula (6) is similar to formula (7) because it needs to calculate the norm as well. However, why not trying to choose the suitable norm through experiments in the same way adopted to determine the norm in formula (7)?\n(5)\tIn ESA module of LSRB, spatial attention is used instead of convolution. Is this module involved in the proposed pruning method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1278/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698525552759,
        "cdate": 1698525552759,
        "tmdate": 1699636054524,
        "mdate": 1699636054524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5giDrpVGmB",
        "forum": "r2Ji0Bzd4g",
        "replyto": "r2Ji0Bzd4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_6gG5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_6gG5"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a flexible meta pruning (FMP) for lightweight image super-resolution (SR). The basic idea of FMP is interesting and consists of structured and unstructured pruning simultaneously during the SR network training. The authors propose channel vectors and weight indicators to control channel and weight sparsity of SR network. A simple yet effective SR backbone (LSRB) is also designed. Extensive ablations show the effect of several key structures and methods, like LSRB, flexible pruning, pruning method. The main comparisons with others also show that the proposed FMP achieves good performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The idea about flexible meta pruning (FMP) is straightforward but effective. FMP conduct structured and unstructured pruning simultaneously in the SR training, which makes the pruning and SR reconstruction more optimized.\n\nThe authors propose channel vectors and weight indicators to control network sparsity and optimize them with proximal gradient and SGD respectively. Such different optimization methods make the differentiable and flexible pruning easily.\n\nExtensive ablation studies are conducted to show the effect of each proposed key component, like effect of the baseline LSRB, pruning method, flexible vs. channel pruning.\n\nThe authors compare with recent related methods and achieve better performance with both quantitative and visual results. The authors provide PSNR/SSIM, visual results, and also model complexity analyses. Those results further support the claimed contributions.\n\nThe authors further discuss convergence criteria and compare with other model compression methods (e.g., NAS, KD, and channel pruning). In this comparison, the proposed FMP still show obvious gains over others.\n\nThe paper is well-written and easy to follow. The overall paper is well-organized.\n\nThe authors promise to release code, which makes the reproducibility easier and the experiments more solid."
            },
            "weaknesses": {
                "value": "The performance gains shown in Table 4 seem to be marginal. Did the results of DHP and FMP obtained by using same number of iterations or not? According to the paper, it seems that they may use different iterations based on the convergence criteria. If FMP uses less iterations and achieves similar or better results than DHP, then it makes sense. Please clarify this.\n\nThe authors mainly apply FMP to CNN based networks, like EDSR and LSRB. It is also important to apply this method to other structures, like Transformer-based methods. So that, we can see how well its generalization ability is.\n\nWhen applying the proposed FMP to other SR methods, do people have to modify too much parts of the new models? It seems that FMP can also be used for other image restoration methods. If so, it is better to discuss in the paper.\n\nMore recent methods can be compared. For example, ICLR-2022-SRPN can also be discussed and compared.\n[ICLR-2022-SRPN] Zhang et al., Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning, ICLR, 2022"
            },
            "questions": {
                "value": "The performance is obviously better than others. Did the authors used self-ensemble during the test phase?\n\nIn practice, does FMP save more running time than channel or weight pruning? Or does FMP need specific hardware to reach obvious reduction of resource (e.g., GPU memory and running time)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1278/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662022200,
        "cdate": 1698662022200,
        "tmdate": 1699636054439,
        "mdate": 1699636054439,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MeTyssYiWU",
        "forum": "r2Ji0Bzd4g",
        "replyto": "r2Ji0Bzd4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_znMB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_znMB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a flexible mera pruning method named FMP, which combines the merits of both structural and unstructured pruning, leading to better trade-off between accuracy and latency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The idea of combining the merits of structured and unstructured pruning is straightforward and promising.\n\n2. The experiments show that the proposed method can surpass the competitors with fewer parameters and less latency.\n\n3. The paper is well written and the idea is clearly illustrated."
            },
            "weaknesses": {
                "value": "1. The authors argue that unstructured pruning does not contribute to actual acceleration. However, the experiments in Table 1 do not include comparisons of latency.\n\n2. Some leading lightweight methods are missed in Table 1, e.g. MegSR[1], VapSR[2], Omni-SR[3]. The authors are suggested to provide comparisons over varied model sizes to verify the effectiveness of the proposed method, because we can not justify whether it can maintain its advantage among smaller models (~400 K params).\n\n3. Table 4 compares only one pruning method because other methods use pretrained SR backbones. However, to better justify the effectiveness of the proposed method, the authors can add more comparisons by also using a pretrained backbone. The pretrained backbone does not increase the inference cost and should not be viewed as a drawback of lightweight SR methods.\n\n4. According to Table 3, the backbone used by FMP already brings some improvements. Thus comparisons in Table 7 may be not fair. The authors are suggested to provide the results of applying ASSLN to  LSRB.\n\n[1] Yu et al., DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution, 2023\n\n[2] zhou et al., Efficient Image Super-Resolution using Vast-Receptive-Field Attention, 2022\n\n[3] wang et al.,  Omni Aggregation Networks for Lightweight Image Super-Resolution, 2023"
            },
            "questions": {
                "value": "See the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1278/Reviewer_znMB"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1278/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808362318,
        "cdate": 1698808362318,
        "tmdate": 1700013984753,
        "mdate": 1700013984753,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mRmah8bAEa",
        "forum": "r2Ji0Bzd4g",
        "replyto": "r2Ji0Bzd4g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_Jqxo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1278/Reviewer_Jqxo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the flexible meta pruning (FMP) for lightweight image SR, using a hypernetwork to perform channel pruning and weight pruning simultaneously. \nThe sparsity of the channels and weights controlled through the channel vectors and weight indicators\nChannel vectors and weight indicators are optimized with proximal gradient and SGD. \nFMP shows competitive experimental results against leading architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper combines the channel and weight pruning for model compression in the lightweight image SR and achieves competitive results across multiple datasets when compared to most leading approaches."
            },
            "weaknesses": {
                "value": "1. It's unclear how the compressed model would translate to the practical inference speed up, considering there are unstructured weight pruning. \n2. The ideas of channel and weight pruning have been around for a while. It is not clear what's new with the proposed approach?"
            },
            "questions": {
                "value": "1. What's the benefits of combining channeling pruning with unstructured weight pruning?\n2. In Table 3, instead of just the submodule, is it possible to show the inference speed up of the entire model?\n3. What's the motivation and benefits of applying proximal gradient for channels vectors and weight indicators?\n4. How much performance gain can FMP harvest if using self-ensemble?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1278/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698986476007,
        "cdate": 1698986476007,
        "tmdate": 1699636054307,
        "mdate": 1699636054307,
        "license": "CC BY 4.0",
        "version": 2
    }
]