[
    {
        "id": "HgWYKicHDD",
        "forum": "HOpQt44EzC",
        "replyto": "HOpQt44EzC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_aDje"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_aDje"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach for learning differentially private vision-foundation models via image captioning. The key innovation of the method is the use of an image captioning objective that enables the direct application of DP-SGD, since the loss is an additive function over the samples. Compared to the existing approach of using a masked autoencoder objective, the proposed image caption objective not only enables vision-language tasks, but allows for the use of much larger batch sizes during model training, without degrading the model performance, which greatly reduces the effective noise per training iteration, leading to better privacy-utility tradeoffs. In experimental evaluations on downstream vision and vision-language tasks, the method achieves SoTA performance relative to DP baselines and, on some tasks, even comparable performance to non-private models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Clarity**\n* The writing, figures, and tables are very clear.\n\n**Significance (Very Strong results)**\n* On the linear probing classification, the proposed method performs comparably or better than non-private AlexNet and SimCLR, and meaningfully outperforms the SoTA DP baseline ViP. \n* On few-shot learning, the proposed method more than doubles the accuracy of the SoTA baseline ViP.\n* On ARO, the proposed method performs comparably or better than non-private CLIP."
            },
            "weaknesses": {
                "value": "**Limited Novetly**\n* The paper lacks any meaningful theoretical or algorithmic contributions. The only insight of the paper is that image captioning is a suitable loss for DP training."
            },
            "questions": {
                "value": "* What is the reason for the *dilemma of large batch training in DP-SGD*, and how does image captioning solve this dilemma? Can you provide any insights?\n\n* Would combining the MAE and image captioning objectives lead to even better performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698699683359,
        "cdate": 1698699683359,
        "tmdate": 1699636242102,
        "mdate": 1699636242102,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WP80DGGS0L",
        "forum": "HOpQt44EzC",
        "replyto": "HOpQt44EzC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_nthH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_nthH"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on training data-safe foundation model under the differential privacy framework. Existing DP-based foundation models have difficulty in learning effective representations. To address this problem, the authors propose to take advantage of the large amount of paired image-text data to train multimodal foundation models. Specifically, the proposed model is trained to generate image captions under the DP-framework. Experiments on several image recognition benchmarks as well as some ARO benchmarks validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) This paper is well-written. The technical details are clearly presented and it is easy for the readers to follow the ideas of the proposed method.\n(2) The proposed method is effective as shown in the experiments. Specifically, the proposed DP-cap model significantly surpasses several existing DP-based methods and is on par with AlexNet and CLIP in some specific scenarios.\n(3) The authors have described some interesting findings in their experiments, e.g. the impact of batch size, which is different from the behavior of some existing methods (e.g. ViP). Such phenomenon might be inspiring for future researchers."
            },
            "weaknesses": {
                "value": "(1) The technical novelty of the proposed method is somehow limited. To be specific, the proposed method consists of two training stages. The first stage trains the model on texture images similarly as proposed in Yu er al. (2023). Next, the second stage further trains the model on image captioning task under the DP framework, which is quite straightforward.\n(2) I am not an expert in model safety, yet I am wondering that except for DP, is there any other frameworks/methods for overcoming the copyright/privacy issue? It is clear from the manuscript that DP-based models performs much inferior to non-DP-based models, yet it is not clear is there any alternative to DP. If so, why does the authors choose DP-based method at first place?"
            },
            "questions": {
                "value": "(1) As shown in Appendix A.1, the authors used 128 V100 GPUs for training the model. As a result of the huge amount of demanded GPUs, it would be difficult for most of the researchers in this area to follow this work. On the other hand, as shown in Figure 3(b), the proposed method has similar loss values across different batch sizes. Therefore, it would be interesting to see if the proposed method can achieve similar performances with smaller batch size. If so, more researchers would be able to follow and further improve this work.\n(2) As discussed in the \u201cweakness\u201d part, is there any other promising framework/method that could overcome the copyright/privacy issues of large foundation models? If so, why do the authors choose DP-based methods in this manuscript?\n(3) I would expect the authors to further clarify the technical novelty of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Reviewer_nthH"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755427434,
        "cdate": 1698755427434,
        "tmdate": 1699636242016,
        "mdate": 1699636242016,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YYgKFPRLLg",
        "forum": "HOpQt44EzC",
        "replyto": "HOpQt44EzC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_NLQ1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_NLQ1"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates the feasibility of differentially private (DP) pre-training for vision-language foundation models. Specifically, it illustrates that DP pre-training, when applied to both images and captions, learns more information for downstream tasks, compared to pre-training on images alone. Additionally, the paper introduces a model named DP-Cap, which outperforms other existing models across several benchmarks.  A comprehensive ablation study is conducted in experiments, which shows the impact of different components on DP-Cap. Furthermore, the paper provides discussions regarding the challenges, potential directions, and open questions of DP pre-training large-scale foundation models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper studies an important and interesting topic: security and privacy risks of foundation models.  \n\nIt proposes training foundation models by combining differential privacy with vision-language pre-training. This approach can help high-utility foundation models with DP training on both images and captions compared with training on images only.\n\n+ The paper introduces the DP-Cap model, which uses text supervision to learn image representations and exhibits multi-modal capabilities for DP training. The model achieves better performance on several benchmarks than several existing models.\n\n+ A lot of interesting and useful discussions are provided in Section 3.1 and Section 5. They can provide insights into DP pre-training of vision-language foundation models and may provide some potential research directions.\n\n+ The related work is well summarized. It can provide readers with a better understanding of the challenges and research status of this topic."
            },
            "weaknesses": {
                "value": "- There may be fairness issues in experimental comparisons.\n\n    For example, the paper states that DP-Cap and ViP were trained using the same training dataset and privacy budget. However, it is worth noting that ViP was trained without utilizing captions (If I understand correctly), while DP-Cap leveraged both images and captions. Consequently, the observed performance improvement of DP-Cap compared to ViP could be attributed to the additional information used during training. While this improved performance may be seen as a contribution of DP-Cap, it is important to consider that the privacy risk might have increased since the model learned caption information.\n\n\n- The effectiveness of the proposed method appears to lack a thorough analysis. \n\n   In Figure 1(a), the accuracies of DP-Cap are reported as 0.58 and 0.33 under 'random init' and 'syn init' on ImageNet. It is evident that 'syn init' significantly contributes to DP-Cap's performance improvement. However, it's worth considering that 'syn init' is derived from the paper proposing ViP. An important question arises: if both ViP and DP-Cap were not to utilize 'syn init,' what impact would it have on their final results?\"\n\n- Typos. For example\n\n  - The citation formats for some related papers appear to be incorrect.\n  - Is the double line in Table 1 positioned correctly?\""
            },
            "questions": {
                "value": "I am more curious whether using caption pre-training models increases privacy risks because the model sees more information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2977/Reviewer_NLQ1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821913617,
        "cdate": 1698821913617,
        "tmdate": 1700646511106,
        "mdate": 1700646511106,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tgFTReQfnt",
        "forum": "HOpQt44EzC",
        "replyto": "HOpQt44EzC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_fvPE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2977/Reviewer_fvPE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a DP mechanism for pertaining the Vision-Language foundation model under the image captioning task. The proposed method achieved some markable number, e.g., epsilon=8, the proposed image captioner attains 52.8% accuracy on CIFAR-10. This work shows a pathway for foundation models to be equipped with the DP guarantee."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well written and well organized with sufficient background introduction, and the related work analysis.\n\n2. The work shows promising results compared to some representative DP foundation model pretraining, e.g., ViP.\n\n3. There are extensive experiments compared to DP and Non-DP baselines."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper seems not fully justified. For example, in DP guarantee, what is the difference between DP-Cap and ViP? Under what architecture or updating mechanism design, DP-Cap can claim the novelty or difference?\n\n2. The paper lacks systematic experiment design. Or the motivation of the experiments is not clear. \n\nFor example, we observe the foundation model achieve better performance on some vision tasks compared to the original transformer based architecture, which has been observed on ImageNet-1K classification problem. Exactly on this dataset, it should firstly show the traditional classification task compared to other DP methods, and then go for few-shot setting.\n\nCurrently, only reporting the ImageNet-1K on few-shot setting with very low numbers (e.g., in Table 3), cannot lead to any positive conclusion.\n\n3. Some experiments comparison cannot draw any conclusion, e.g., Table 2 compared between DP-Cap and the non-DP based methods.\n\n4. One consideration is to replicate the experimental settings from ViP, e.g., DP fine-tuning evaluation on ImageNet-1K, experiments on Caltech-101 and CIFAR-100, which can provide direct comparison to ViP and other DP methods, as appeared in ViP paper. Otherwise, the paper will fail in setting consistent comparison to the literature."
            },
            "questions": {
                "value": "The main concern comes from the method novelty and the experimental settings. For details, please refer to weakness session."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2977/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699165706656,
        "cdate": 1699165706656,
        "tmdate": 1699636241859,
        "mdate": 1699636241859,
        "license": "CC BY 4.0",
        "version": 2
    }
]