[
    {
        "id": "tUuz83Bndo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission88/Reviewer_ezvM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission88/Reviewer_ezvM"
        ],
        "forum": "XM0fepbZE9",
        "replyto": "XM0fepbZE9",
        "content": {
            "summary": {
                "value": "This paper propose to late-fuse 2D RGB detectors and 3D LiDAR detections in the 2D image-plane. Experiments on  the established nuScenes LT3D benchmark show promising results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Experiments on  the established nuScenes LT3D benchmark show promising results."
            },
            "weaknesses": {
                "value": "see questions part."
            },
            "questions": {
                "value": "I am still surprised that this late fusion method can achieve such a big improvement in indicators. In fact, in addition to the Frustum PointNets you mentioned, articles such as SRDL[1] , MMF[2] also propose ways to integrate 2D detection results into 3D.\n\nIn terms of results, the original intention of this article is to solve the detection problem of rare categories, but there is no emphasis in this direction from the experimental level. From the perspective of the idea, this paper is more like a fusion of 2D and 3D detection results, so from the perspective of contribution, it seems a bit weak.\n\nAnd the experimental results of this paper were done on the nuScenes data set, so how does it perform on the KITTI data set and the larger Waymo data set?\n\nFinally, the first sentence of the second paragraph of Section 3.2 seems a bit ambiguous and seems to violate the principle of anonymity.\n\n\n\n[1] He Q, Wang Z, Zeng H, et al. Stereo RGB and Deeper LIDAR-Based Network for 3D Object Detection in Autonomous Driving[J]. IEEE Transactions on Intelligent Transportation Systems, 2022, 24(1): 152-162.\n\n[2] M. Liang, B. Yang, Y. Chen, R. Hu, and R. Urtasun, \u201cMulti-task multi-sensor fusion for 3D object detection,\u201d in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., Jun. 2019, pp. 7345\u20137353"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission88/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697187875283,
        "cdate": 1697187875283,
        "tmdate": 1699635933618,
        "mdate": 1699635933618,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qH2D9DZh6t",
        "forum": "XM0fepbZE9",
        "replyto": "XM0fepbZE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission88/Reviewer_zAdM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission88/Reviewer_zAdM"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the late-fusion of LiDAR and camera detection results is investigated for the long-tailed distribution problem in 3D object detection. By examining the different choices of 2D/3D RGB detectors, the choice of geometric fusion planes for LiDAR and image detection results, and the probabilistic fusion of LiDAR-camera multi-modal results, a significant improvement of the detection performance of long-tailed distribution 3D objects on the challenging dataset nuScenes is achieved. However, as far as the content of the article is concerned, I have the following questions and suggestions:\n1) The article is not innovative enough (although the results are impressive) and gives the impression of reading an experimental report rather than a scientific paper.\n2) The motivation of the fusion strategies used is not clearly described in the article.\n3) How to greedily tune the temperatures and the priors are not clear.\n4) Whether the way of late-fusion is limited by the upper limit of the capacity of the uni-modal detector itself. Would the conclusion be different if a different backbone is used?\n5) The fusion choice in the image plane requires LiDAR and camera to have sufficient field of view overlap. In the experiments, are the detection results of images from all viewpoints fused? If yes, how to solve the inconsistency of the detection results of images from different viewpoints in the overlap area?"
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1)\tThe overall detection results on nuScenes are impressive.\n2)\tThe ablation studies verify the effectiveness of each choice and design."
            },
            "weaknesses": {
                "value": "1) The article is not innovative enough and gives the impression of reading an experimental report rather than a scientific paper.\n2) The motivation of the fusion strategy used is not clearly described in the article.\n3) How to greedily tune temperatures and the priors are not clear."
            },
            "questions": {
                "value": "1) Whether the way of late-fusion is limited by the upper limit of the capacity of the uni-modal detector itself. Would the conclusion be different if a different backbone is used?\n2) The fusion method in the image plane requires LiDAR and camera to have sufficient field of view overlap. In the experiments, are the detection results of images from all viewpoints fused? If yes, how to solve the inconsistency of the detection results of images from different viewpoints in the overlap area?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Reviewer_zAdM"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission88/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698375895999,
        "cdate": 1698375895999,
        "tmdate": 1699635933530,
        "mdate": 1699635933530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qvpKMtB7tf",
        "forum": "XM0fepbZE9",
        "replyto": "XM0fepbZE9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission88/Reviewer_JxFc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission88/Reviewer_JxFc"
        ],
        "content": {
            "summary": {
                "value": "This work studies three design choices within the late-fusion framework for Long-Tailed 3D Detection (LT3D). The authors present a simple late-fusion approach that fuses 2D RGB-based detections and 3D LiDAR-based detections and achieves state-of-the-art results on LT3D benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Instead of fusing the RGB 3D detector with the LiDAR 3D detector, this work proposes using an RGB 2D detector to calibrate 3D detection classification and 3D score to help LT3D and achieve new SotA performance.\n- Extensive analyses of different choices of matching 2D and 3D, score calibration and different 2D and 3D detectors."
            },
            "weaknesses": {
                "value": "- Although the performance is quite good. It seems like this work is more like a good engineering work. The novelty is limited. \n- The 2D RGB detector works more like just a classifier. It looks like it is missing the most important baseline, e.g., using a strong classification network for the projected 3D boxes and still being able to perform semantic and score calibration."
            },
            "questions": {
                "value": "- Datasets like nuScenes [1] and Argoverse 2 [2] have multiple ring cameras. How do you deal with the projected 3D boxes divided into different camera scenarios?\n\n\n[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.\n\n[2] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse 2: Next generation datasets for self-driving perception and forecasting. arXiv preprint arXiv:2301.00493, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission88/Reviewer_JxFc"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission88/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698590654329,
        "cdate": 1698590654329,
        "tmdate": 1699635933433,
        "mdate": 1699635933433,
        "license": "CC BY 4.0",
        "version": 2
    }
]