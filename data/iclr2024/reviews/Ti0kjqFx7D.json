[
    {
        "id": "JBodkdw7v0",
        "forum": "Ti0kjqFx7D",
        "replyto": "Ti0kjqFx7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_7JjA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_7JjA"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the graph model editing problem by inserting an MLP in GNN architecture and freezing the GNN model during the model editing phase to alleviate the propagation of misinformation by the message-passing mechanism."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The problem setting is interesting.\n2. The motivation example in Section 3.1 is intuitive."
            },
            "weaknesses": {
                "value": "1. My major concern regarding this paper is the performance of the proposed method. Though Tables 2, 3, and 4 show great improvement compared with GD and ENN, the actual improvement is still limited. Based on the experimental results of GCN and GraphSAGE without any graph editing on these datasets (e.g., the performance of GCN reported in [SUGRL](https://ojs.aaai.org/index.php/AAAI/article/view/20748) Tables 1 and 2), GCN achieves 91.6\\% accuracy on A-Photo dataset and 84.5\\% accuracy on A-Computers dataset. However, in the proposed method, EGNN achieves 91.97\\% accuracy on the A-Photo dataset and 82.85\\% accuracy on the A-Computers dataset. Comparing these two results, if editing the graph model could not improve the performance, then why do we need it? Thus, the authors should also report the performance of GCN and GraphSAGE without any editing in the experiment. It's important to demonstrate that graph editing can indeed improve the performance of GNN. If EGNN is worse than the vanilla GCN and GraphSAGE, then the proposed method is meaningless.\n\n2. The novelty of this paper is somehow limited. The proposed method simply incorporates the MLP into the GNN architecture to address the issue of message-passing during the graph editing.\n\n3. In the related work, the period is missing for the sentence \"This assumption might not hold well for graph data, given the fundamental node interactions that occur during neighborhood propagation\".\n\n4. The evaluation regarding the metric DrawDown is questionable. See question 2."
            },
            "questions": {
                "value": "1. In the experiment, the authors evaluate EGNN under the inductive setting.  As many types of GNNs benefit from leveraging the feature and the graph topology of both labeled and unlabeled nodes, can EGNN work in the transductive setting?\n\n2. In Table 2, the authors report the DrawDown of EGNN, which is the **mean absolute difference** between test accuracy before and after performing an edit. Then, how can it be a negative value (e.g., -0.17 for EGNN with GCN and -0.01 for EGNN with GraphSAGE on the Coauthor-CS dataset)?\n\n3. In Algorithm 1 for the EGNN edit procedure, should the condition of the while loop ($\\hat{y}\\neq y_v$) be $\\hat{y}\\neq y_e$? Correct me if I misunderstand it. \n\n4. Figure 2 suggests that the more editing it conducts, the worse performance it achieves shown in the last three subfigures, e.g., Figures GCN (Reddit), GraphSAGE (ogb-arxiv), and GraphSAGE (Reddit). When the number of sequential edits increases from 10 to 40 or 20 to 40, we can observe the DrawDown consistently increases. In this case, why do we need to edit the model anymore?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Reviewer_7JjA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770777445,
        "cdate": 1698770777445,
        "tmdate": 1699636366444,
        "mdate": 1699636366444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m2GGGawqA3",
        "forum": "Ti0kjqFx7D",
        "replyto": "Ti0kjqFx7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_fgMm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_fgMm"
        ],
        "content": {
            "summary": {
                "value": "This paper explorea a and important problem, i.g., GNNs model editing for node classification. This paper empirically shows that the vanilla model editing method may not perform well due to node aggregation. Furthermore, this paper proposes EGNN to correct misclassified samples while preserving other intact nodes, via stitching a trainable MLP. In this way, the power of GNNs for prediction and the editing-friendly MLP can be integrated together in EGNN."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors can leverage the GNNs\u2019 structure learning meanwhile avoiding the spreading edition errors to guarantee the overall node classification task.\n\n2. The experimental results validate the solution which could address all the erroneous samples.\n\n3.  Via freezing GNNs\u2019 part, EGNN is scalable to address misclassified nodes in the million-size graphs."
            },
            "weaknesses": {
                "value": "1. The motivation is not clear. Since the authors trained the model on a subgraph containing only the training node, how node aggregation in GNNs will spread the editing effect throughout the whole graph?\n\n2. The experiment setting is not clear. The authors trained the model on a subgraph containing only the training node, then what graph is used for editing?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777200038,
        "cdate": 1698777200038,
        "tmdate": 1699636366363,
        "mdate": 1699636366363,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oTJO3Y4Dhc",
        "forum": "Ti0kjqFx7D",
        "replyto": "Ti0kjqFx7D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_5Af8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4034/Reviewer_5Af8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a work on tackling the conflict between message aggregation in GNNs and model editing by stitching a MLP as auxiliary; It also provides sufficient experiment results to support the effectiveness of their method."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Strength:\n\n\u2022\tThe paper claims to be the first work studying model editing on GNNs.\n\n\u2022\tThey choose to utilize loss landscape to demonstrate the reason of accuracy drops after editing, providing a visualization of the potential rationale; a theoretical analysis in appendix also offers more solid explanation.\n\n\u2022\tSeemingly good results on benchmarks."
            },
            "weaknesses": {
                "value": "Weakness:\n\n* Adding an additional MLP to compensate the original model is an interesting idea. However I have big concerns about the soundness of Algorithm 1. For example, if we require all parameters of the MLP to be zero, it is almost an optimum to make L_loc and L_task lowest, especially if the pretrained GNN model already has an MLP component (e.g. using skip connections of jumping knowledge). \n\n* The motivation of editing GNN models is not clear enough. The motivation starts from mitigating errors of GNNs in real world application; however, it does not present an example of such error or mention why the cost of wrong decision is high.\n\n* Lack of analysis of why there exists errors in GNNs\u2019 classification; are GNNs only making wrong prediction with certain type of inputs?\n \n* For section 5.3, it really confuses me how the described experiments help to answer R2: it corrects the flipping nodes and then test whether model makes right prediction on same class? If I get the point correctly, the flipped nodes only take 10% of the training node, so 90% of the time model should be able to learn the correct classification, then why it  can indicate the generalization ability of EGNN?"
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4034/Reviewer_5Af8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4034/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699122794907,
        "cdate": 1699122794907,
        "tmdate": 1700715792855,
        "mdate": 1700715792855,
        "license": "CC BY 4.0",
        "version": 2
    }
]