[
    {
        "id": "dRSxVii2Xh",
        "forum": "8iTpB4RNvP",
        "replyto": "8iTpB4RNvP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_GcXY"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to attack face deepfake detection models via backdoor attack, especially for blending artifact detection methods. To resolve the scalable problem, the paper builds a trigger generator to generate the trigger adaptively. To ensure the stealthiness of the trigger, a relative pixel-wise embedding ratio is incorporated in the generation of the poisoned sample. In general, the style of this paper is clear and logical. However, some experiments are missing and hence make readers confused about the effectiveness of some modules."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is the early exploration of backdoor attacks in face forgery detection and the description of the existing obstacles deployed trigger in face forgery detection is clear.\n2. Besides, the paper subtly simplifies the complex forgery problem as linear translation transformation and hence makes the backdoor attack in the translation-sensitive scenario.\n3. To make the trigger stealthy, the author further hid the generated noise by a simple weight parameter."
            },
            "weaknesses": {
                "value": "1.Some descriptions like formulation may be incorrect and may misunderstand the readers.\n2.Some ablation studies are missing and make readers confused about the effectiveness of the proposed modules. \n3.The method lacks generalization to some extent. The main gains derive from blending artifact detection. However, in the real scenario, not all evidence of determining forgery is based on blending traces in face forgery detection. Therefore, I would like to see the feedback about the questions listed below and decide to give a reasonable rating."
            },
            "questions": {
                "value": "The current rating is not my final decision, I hope the authors can explan the questions listed below in detail:\n\n1.In Eq. (2), as stated, the blending transformation needs two real samples, however, the variable in function T^b only has one sample instead of a pair of real samples. For example, in Face X-ray, the fake face image is generated via two different real ones. So I think the formulation should be adjusted to make it more reasonable.\n\n2.In the section on \u2018Stealthiness of Backdoor Attacks\u2019, Table 2 shows the qualitative results among different backdoor attack methods. However, the paper does not mention the number of samples for evaluation.\n\n3.The influence of the hyperparameter \u2018a\u2019 is ambiguous. Moreover, the comparison with/without \u2018alpha\u2019 is missing and hence makes readers confused about the effectiveness brought by the proposed relative embedding. It is better to show the qualitative results and the quantitative backdoor attack performance.\n\n4.This attack method is built on the generated image should show a relatively distinct facial boundary. However, in real scenarios, fake images are generated from various transformations. If some fake images are generated only via non-linear transformation, such as blur, this method may not be optimal. This may be the reason why the attack performance is lower than \u2018LC\u2019 because not all training samples contain linear translation transformation.\n\n5.Typo: In Resistance to Backdoor Defenses, \u2018Efficient-b4\u2019 should be \u2018EfficientNet-b4\u2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698463521257,
        "cdate": 1698463521257,
        "tmdate": 1699636151596,
        "mdate": 1699636151596,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T1onHBswxV",
        "forum": "8iTpB4RNvP",
        "replyto": "8iTpB4RNvP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_4qiC"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a novel backdoor attack strategy is introduced for the FACE FORGERY DETECTION task. This approach deceives face forgery detectors by embedding specific triggers within the training data. The authors delve deeply into the internal structure of current face forgery detectors and propose a transformation-sensitive triggering mechanism to facilitate effective backdoor attacks. Extensive experiments validate the efficacy of the proposed method and highlight potential vulnerabilities in face forgery detection systems."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper is the first to uncover the vulnerabilities during the training phase of face forgery detection, introducing a practical clean-label attack technique.\n\n2. The backdoor attack strategy presented is notably innovative, leveraging the unique architectural characteristics of face forgery methods.\n\n3. The trigger designed in this study is highly covert, making it challenging for the human eye to detect.\n\n4. The authors provide a comprehensive explanation of the motivations behind the proposed method, and the manuscript is articulated clearly."
            },
            "weaknesses": {
                "value": "1. The paper could provide a discussion on the limitations of the proposed method, ideally placed in the appendix.\n2. The choice of different kernel sizes for the trigger can impact the attack performance on various forgery detection models. The rationale behind such an impact remains unclear. Additionally, the authors' decision to finalize a size of 5 for the kernel lacks a clear justification.\n3. Table 4 indicates that existing defense mechanisms fail against the proposed attack. It remains ambiguous whether this failure is specific to the attack introduced by the authors or if it's a general shortcoming for other attacks as well. Furthermore, it would be valuable if the authors could analyze existing attacks to suggest potential defense strategies.\n4. In the section on trigger generation, the authors should provide a comprehensive description of the generator's overall loss function and its basic architecture."
            },
            "questions": {
                "value": "Please refer to [Weakness]."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639777178,
        "cdate": 1698639777178,
        "tmdate": 1699636151528,
        "mdate": 1699636151528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q1YBrZKwnF",
        "forum": "8iTpB4RNvP",
        "replyto": "8iTpB4RNvP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the Poisoned Forgery Face clean label backdoor attack framework utilizing a mask for the inner face region and a stealthy translation-sensitive trigger pattern, which is suitable for attacking facial forgery detectors. By poisoning a small portion of the clean training data, the attacker can flip the label of deep fake images or videos from fake to real. This approach can overcome the label conflict problem, which happens when the detectors are trained with self-blending techniques with only real data. Experimental results demonstrated a significant improvement in the attack success rate and the improvement of the trigger stealthiness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper addresses not only the traditional deepfake artifact detection, in which detectors are trained with deepfakes, but also the recent blending artifact detection, in which the detectors are trained with augmented self-blended real images. The proposed translation-sensitive trigger pattern is innovative and effective for dealing with the self-blending approach.\n \n+ The paper also provides a comprehensive benchmark for backdoor attacks, which is not available in the literature."
            },
            "weaknesses": {
                "value": "+ This paper is not the first in the literature to address backdoor attacks in face forgery detection. Cao et al. [A] have already investigated this problem. Therefore, the first contribution is invalid.\n \n+ The use of \"b\" in the equations in section 3 is confusing. b presents both \"blending\" and \"remaining clean.\"\n \n+ The benchmark should include some frequency-based backdoor attacks like [B].\n \n+ The reported performance of Face X-ray is lower than that in the original paper and in the SBI paper. I am wondering if there is something wrong with the experiments.\n \n+ The robustness of the proposed trigger patterns was not investigated. While sharing deepfake images or videos on social networks, these patterns may be destroyed by compression or some other image processing algorithms.\n \n+ The paper should include a paragraph of ethics statement\n \n+ The appendix section, which was referred from the main part, is missing.\n\nReferences:\n\n[A] Cao, Xiaoyu, and Neil Zhenqiang Gong. \"Understanding the security of deepfake detection.\" In International Conference on Digital Forensics and Cyber Crime, pp. 360-378. Cham: Springer International Publishing, 2021.\n\n[B] Wang, Tong, Yuan Yao, Feng Xu, Shengwei An, Hanghang Tong, and Ting Wang. \"An invisible black-box backdoor attack through frequency domain.\" In European Conference on Computer Vision, pp. 396-413. Cham: Springer Nature Switzerland, 2022."
            },
            "questions": {
                "value": "Please refer to the comments in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2182/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2182/Reviewer_uGDS",
                    "ICLR.cc/2024/Conference/Submission2182/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2182/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798554769,
        "cdate": 1698798554769,
        "tmdate": 1700536770660,
        "mdate": 1700536770660,
        "license": "CC BY 4.0",
        "version": 2
    }
]