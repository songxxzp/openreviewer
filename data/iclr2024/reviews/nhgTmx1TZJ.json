[
    {
        "id": "Ff1TtcnAsI",
        "forum": "nhgTmx1TZJ",
        "replyto": "nhgTmx1TZJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_gngw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_gngw"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors train a single multi-scale transformer to perform multiple different audio tasks. They demonstrate that, by learning these tasks together, they get a performance boost for this model, and achieve state of the art results on multiple tasks (against specialist models). They show that the existing model can be fine tuned to perform well on novel tasks. In addition, they describe how to translate multiple input modalities into a universal data space with a neural codec to allow this to happen, and discuss the merits and detriments of this system against other approaches."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* All data used is public, so results can be reproduced\n* Compute required is not out of the realms of most academic institutions (16 x MI200-64G GPUs ~ $64k) - particularly if, as is suggested, this model is taken and simply fine tuned upon\n* I am not aware of another model which has covered all of these tasks nor comprehensively demonstrated that combining different input modalities benefits each task; the authors show the benefit of training on multiple modalities by training their model on just the one task, and then again on all task, and by comparison with the current sota (generally single-task) model\n* Care and attention has been taken to compare with recent sota models in each domain, and to perform both objective and subjective evaluations. In addition, an anonymised website of audio generations was provided"
            },
            "weaknesses": {
                "value": "I would like to emphasise that all the following are suggestions for discussion / future work - IMHO this contribution is more than sufficient for publication in its current form.\n* Reliant on the input representation - any losses incurred by input representation cannot be modelled, and if a new representation were to be used, a new model must be trained\n* As noted in the limitation section, there's no demonstration of fine tuning to an unknown modality (which would require a new special modality token to be included in the vocabulary)\n* The evaluation is so wide ranging it's difficult to parse in tables. Perhaps a bar chart or plot could improve the interpretation of relative performance with sota benchmarks?\n* Only amazon turk was used for evaluation - a broader human evaluation with subject experts would be very interesting"
            },
            "questions": {
                "value": "* It is not explained exactly why the 4 tasks were selected for the fine tuning study, was there any reason?\n* Is there any reason that the process could not be end to end (i.e. must the neural codec be learned prior and fixed)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685958416,
        "cdate": 1698685958416,
        "tmdate": 1699637075241,
        "mdate": 1699637075241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R8nZGZXl50",
        "forum": "nhgTmx1TZJ",
        "replyto": "nhgTmx1TZJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_EAVx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_EAVx"
        ],
        "content": {
            "summary": {
                "value": "- The authors present a foundation model for audio generation and editing which encompasses 11 tasks spanning multiple modalities of audio such as speech, music, and general sounds. \n- The paper proposes a different approach to modeling audio tokenized into discrete codes using audio codecs. The proposed architecture is a single multi-scale transformer which involves a global transformer which operates on a summarized form of each audio frame, and a local transformer which performs autoregressive generation of codes within an audio frame. The design is proposed as a way to alleviate training transformers on very long sequences of flattened audio codes (SPEAR-TTS), or having train an autoregressive transformer on one level of codes and another non-autoregressive transformer on the remaining level of audio codes (VALL-E).\n- The training process involves 7 different tasks with various task indicator tokens being used. The remaining tasks are incorporated in a fine-tuning phase post training of the base model.\n- The authors perform evaluation using subjective and objective metrics and compare against various other models which may be specialized for each task. The results indicate that UniAudio is competitive against all the baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- As the authors show in the first table, this system is the first to tackle so many tasks in audio generation using a single backbone with only additional fine-tuning.\n- The proposed model architecture is different from those used in other audio generation models and shows some gains. The comparison is not necessarily fair because of differences in training data, but it does give the community a new option to consider while building audio generation models.\n- Results for speech data and denoising are decent."
            },
            "weaknesses": {
                "value": "- The paper is a very dense read with some details relegated to the appendix while some details difficult to glean from the text. For example, the details about the audio tokenizer is moved to the appendix and no specifics are mentioned in the main text. Also, the text and the figure in the model architecture section are a little difficult to understand. I think I got it after reading through a few times, but adding some annotations to the figure and its caption will greatly improve readability.\n- While the results for speech are decent, I found some issues w.r.t words being skipped. Also, the results on text to music/audio are pretty poor. The authors have not discussed any of the limitations in terms of quality of the generated audio despite the model being competitive with current state-of-the-art. \n- The paper does not offer too much in terms of insights. Not to take away from the effort it takes to setup such a large scale experiment, but my main takeaways from the paper are as follows:\n  1. Existing audio codecs are not suitable for wide range of tasks, so one needs to train them on more diverse data.\n  2. As long as we collect 100k+ hours of audio data we can achieve such performance using the multi-scale transformer. \n\n  The multi-scale transformer architecture is definitely interesting to the community but the architecture contribution is not well ablated, and in my opinion, that is the most important part of the paper.\n- Minor correction: In Table 3, subjective and objective metrics column headers should be interchanged."
            },
            "questions": {
                "value": "- Wonder if the authors can add any ablations that answer questions related to the effect of data quantity?\n- Did the authors consider using DAC as the audio codec? Those models are trained on a more diverse dataset and also have other improvements over EnCodec."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698863140905,
        "cdate": 1698863140905,
        "tmdate": 1699637075135,
        "mdate": 1699637075135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ih66iSoj8e",
        "forum": "nhgTmx1TZJ",
        "replyto": "nhgTmx1TZJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_kfiC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_kfiC"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a LM-like audio model, that is able to conduct multiple (11) audio generation tasks. Audio is tokenized with a EnCodec-like quantizer to fit for LM. Multi-scale Transformer is used for handling multiple tokens per audio frames. Experiment was conducted by training on 165K hours audio data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The overall approach is sound. It's great to see a single model can handle so many different audio generation tasks."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is very limited. All the components used in the paper are previously existing. The main contribution of this paper is to train a multi-task model with a mixture of multiple datasets. \n2. The overall contribution to the research community seems very limited. There is no much insights can be drawn from this paper to benefit the community. There is no ablation study conducted. It looks more of an engineering work than a research work.\n3. The presentation of this paper needs improvement. It seems presented in an eye-catching, but misleading way. For example, it's improper to put Table 1 as the first thing in the content of this paper, because it lacks context and is confusing. Even worse, it's not a fair comparison -- the cited papers didn't include experiments on specific tasks doesn't mean that the methods presented in those papers are incapable of those tasks."
            },
            "questions": {
                "value": "None."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "See the 3rd item in \"weaknesses\"."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699226223299,
        "cdate": 1699226223299,
        "tmdate": 1699637075030,
        "mdate": 1699637075030,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FemtiwPq0R",
        "forum": "nhgTmx1TZJ",
        "replyto": "nhgTmx1TZJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_dsre"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8597/Reviewer_dsre"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a foundation model for audio generation capable of handling a number of different tasks, all of which output audio (including speech, environmental sounds, and music) conditioned on input from multiple modalities including text, audio, and MIDI.  \n\nThe paper also introduces a two-level Transformer architecture where the top-level attention operates across audio *frames* (where each frame consists of multiple discrete tokens) and the bottom-level attention operates on the tokens within a frame."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I am very much in favor of the goal of the paper: to introduce a multitask foundation model for audio generation.  I am also totally on board with the proposed architecture; decomposing the quadratic attention into frame-level and token-level is an excellent idea for handling the longer sequence lengths that are necessary for freeform audio generation."
            },
            "weaknesses": {
                "value": "Two main areas of weakness:\n\n1) Most of the evaluation tasks do not seem ideal for demonstrating the effectiveness of an architecture designed to enable long sequence lengths; other than text-to-music, the tasks are all quite local, with no real dependencies longer than a few seconds.  Notably, text-to-music is the task on which the model in the paper performs worst compared to baseline.  And the experimental evaluation of the architecture vs others consumes only a small section of the paper and is fairly inconclusive.\n\nEssentially, the architecture seems overkill for most of these tasks, and for the task for which one might expect it to help most (text-to-music), possibly the smaller training data size prevents the model from taking advantage of its capability.\n\n2) The paper does not convincingly demonstrate that training a single model on speech-, music-, and sound-generating tasks exhibits synergy across domains.  While for most of the evaluation tasks the multitask model outperforms the single task model, a) the difference in performance is small and b) it's not clear that the model really benefits from training on all of speech, music, and sound compared to a separate speech model, music model, and sound model as this experiment was not performed.  It's also not clear how much of the benefit depends on specifics of the datasets and tasks used here vs a more general principle; e.g. the overall amount of speech data here is much larger than the other domains and so a music task might benefit from training jointly with speech more than would be the case if the amount of music data were larger.\n\nOverall, I'm just not satisfied that the main hypothesis of the paper -- training a model on all audio generation domains at once is better than training separate models for each domain -- is sufficiently backed up with experimental evidence.  The fact that the joint model outperforms state of the art on about half the tasks (and *not* the music tasks) corroborates my dissatisfaction here.\n\nOne other possible reason for training on multiple domains could be: only a small quantity of training data exists for certain domains.  However, for the case of foundation models this is certainly not true; it is usually *labels* that are in short supply, and raw audio -- speech, music, and sound -- exists in enormous quantity."
            },
            "questions": {
                "value": "1) Basically, convince me that training on speech helps with music, even if one has access to enormous unlabeled music datasets.  I'm totally willing to believe that training on all the speech tasks at once is helpful.\n\n2) I do believe that the architecture proposed is going to outperform the comparison architectures for audio generation.  But the experiment demonstrating this isn't especially compelling; did you perform other experiments on the different architectures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8597/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699231815743,
        "cdate": 1699231815743,
        "tmdate": 1699637074927,
        "mdate": 1699637074927,
        "license": "CC BY 4.0",
        "version": 2
    }
]