[
    {
        "id": "w595MejieA",
        "forum": "k2a2aPOA4b",
        "replyto": "k2a2aPOA4b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Universal Entropy Optimization (UEO), a method for unsupervised universal fine-tuning of vision-language models (VLMs) like CLIP. UEO aims to enhance the model's performance in two key aspects: accurate classification of samples from known classes and effective identification of samples from classes not present in the predefined classes. It does this by leveraging sample-level confidence and entropy optimization to handle out-of-distribution (OOD) samples. The paper presents results from experiments conducted across 15 domains, demonstrating that UEO outperforms baseline methods in terms of both generalization and OOD detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**\n\nThe paper proposes a simple yet efficient solution to address a unique and realistic setting of unsupervised universal fine-tuning. According to the authors, this is the first paper to tackle this practical setting. While the key principle of the method is similar to DANCE, unlike DANCE, the proposed approach does not require hyper-parameter selections, which can be challenging in the unsupervised setting.\n\n**Quality**\n\nThis paper exhibits notable strengths in its hyper-parameter-free approach, Universal Entropy Optimization (UEO), which addresses the challenging task of unsupervised fine-tuning of vision-language models (VLMs) under real-world conditions, including potential out-of-distribution (OOD) samples in unlabeled data. Through comprehensive experiments conducted across diverse domains and the introduction of novel evaluation metrics like the AUC score, the paper showcases the effectiveness of UEO in both in-distribution classification and OOD detection. UEO's parameter-efficient methodology and emphasis on real-world scenarios make it a valuable contribution, marking its quality in the field of VLMs and unsupervised fine-tuning.\n\n\n**Clarity**\n\nThe paper is well-written and easy to follow. The motivation of the paper is clear. The authors build upon previous works and cite them appropriately. \n\n**Significance**\n\nThe proposed setting is practical. The paper tackles the problem where unknown classes can be present in the unlabeled data, replicating real-world scenarios. The analysis is thorough and the experiments across multiple settings show consistent improvements."
            },
            "weaknesses": {
                "value": "One potential weakness of the paper is that it relies on sample-level confidence weights to approximate entropy minimization and maximization. While this approach is innovative, it may be sensitive to the distribution of confidences within the unlabeled data. If the confidences are not well-calibrated or vary significantly across samples, it could affect the effectiveness of UEO. The performance of UEO might be influenced by the quality and reliability of the confidence estimates, and if the confidence estimates are noisy or inaccurate, it could lead to suboptimal results.\n\nOne experiment that is missing would involve applying the method to VLMs that are not well-calibrated and observing its impact on the performance of the fine-tuned model.\n\nAnother potential drawback is that, when compared to InfoMax, there does not appear to be a significant improvement in accuracy on certain datasets. For instance, on the DomainNet dataset, there is no noticeable difference in performance in accuracy."
            },
            "questions": {
                "value": "1. Can the universal entropy optimization loss be potentially incorporated during the contrastive pre-training phase as well? \n\n2. Apart from being hyper-parameter free, could the author elaborate more on the benefit of using the proposed universal entropy optimization instead of the entropy separation loss in DANCE? \n\n3. Since the method relies on the confidence of the model, could the authors discuss the effect of calibration of predicted probabilities on the performance of the method? \n\nI would be willing to raise my score if the authors could address my concerns and answer the questions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_spW5",
                    "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697788588895,
        "cdate": 1697788588895,
        "tmdate": 1700386485530,
        "mdate": 1700386485530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D2mV4YQOSc",
        "forum": "k2a2aPOA4b",
        "replyto": "k2a2aPOA4b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel task setting for unsupervised CLIP fine-tuning, where the label spaces of unlabeled data and predefined text classes are partially overlapped. As a result, the trained model is required to concurrently detect out-of-distribution categories while recognizing samples within the predefined classes. To address this challenge, the paper proposes a straightforward approach that aims to minimize the conditional entropy of confident samples and maximize the marginal entropy of less confident ones. Experiments are performed on benchmark datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. To the best of my knowledge, the proposed task setting is novel. I believe it offers a valuable direction for unsupervised CLIP adaptation.\n2. The proposed approach is straightforward and results in a general improvement.\n3. Experiments are carried out on widely accepted DA benchmarks."
            },
            "weaknesses": {
                "value": "1. As highlighted in the introduction and method sections, the paper's primary focus is the class discrepancy between unlabeled data and the predefined label space. However, the principal experiments are based on domain adaptation datasets, characterized predominantly by distributional differences between domains. I believe more general classification datasets (e.g., ImageNet and SUN397 as in CoOp) should be employed to define the task setting and verify the efficacy of the proposed method.\n2. The introduced method bears a significant resemblance to existing mutual information maximization losses. The sole distinction appears to be the instance weight, which is based on maximum prediction probability. Additionally, the performance gains seem rather marginal when compared with its peers.\n3. Missing ablation studies. The methodology encompasses two critical components: firstly, the entropy-related training objective, and secondly, the parameter-efficient tuning. I'm uncertain if the competing methods in Tab. 1 also implement the same parameter-efficient tuning. Furthermore, the prompt and affine parameters of the parameter-efficient tuning should be dissected and examined individually.\n4. Missing references. Some related studies delve into various task settings, such as black-box DA in [1], partial DA in [2], and universal DA in [3].\n\n[1] Unsupervised Domain Adaptation of Black-Box Source Models, BMVC21\n[2] Universal domain adaptation, CVPR19\n[3] Partial adversarial domain adaptation, ECCV18"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_pTa8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698389746641,
        "cdate": 1698389746641,
        "tmdate": 1700449967765,
        "mdate": 1700449967765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NXdAj12Gcw",
        "forum": "k2a2aPOA4b",
        "replyto": "k2a2aPOA4b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel setting called \"unsupervised universal fine-tuning,\" which involves both in-distribution prediction and out-of-distribution detection. To tackle this problem, the authors presented an approach called \"universal entropy optimization.\" It utilizes the confidence of each sample to minimize the entropy of confident samples but maximize the entropy of confident samples. These combined lead to improvement for both generalization and out-of-distribution detection on benchmarks like DomainNet, VISDA-V, Office-OF, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The problem setup of \"unsupervised universal finetuning\" seems reasonable and is grounded in the disadvantages of previous settings.\n* I think the approach of \"universal entropy optimization\" (UEO), especially Eqn. 3, is interesting in achieving maximization and minimization at the same time. I don't directly work in this field and am not sure whether Eqn. 3 has been used by other people. Nonetheless, I think the UEO approach in the paper is intriguing.\n* The performance demonstrated in the experiment section supports the effectiveness of the approach."
            },
            "weaknesses": {
                "value": "(Details in the questions section) I think the authors might need to clarify several questions to fully illustrate their novel explorations, including the role of vision-language models and the significance of the new setting. Additionally, the numbers in Table 1-4 are quite close in some datasets (though I admire and appreciate the exhaustive evaluation from the authors), so it is also better to make more clarifications."
            },
            "questions": {
                "value": "1. What is the special role of the \"vision-language model\" in the paper or the investigated problem? It seems to me the approach and problem-setting are applicable to models beyond CLIP?\n\n2. Following the above question, I think the authors need to better clarify how their experiment setting differs from previous works. Specifically, the authors mentioned \"unsupervised universal fine-tuning\" as a novel fine-tuning setup, but it seems the evaluation directly adopted the previous datasets without special curation. Therefore, I am wondering if this is a new setting, or some previous setting adapted to CLIP, or some other cases?\n\n3. The numbers in the tables are quite close for some datasets, and the performance for UEO is not the best on some datasets, such as  the avg numbers. Therefore, I think clarifications on the following questions would be helpful:\n* Is there a clear baseline of the UEO approach, e.g., some simple modification or fine-tuning strategy to CLIP for this setting?\n* What is the variance of these numbers?\n* Which is the largest and hardest dataset?\n* State-of-the-art is not necessary for me, but the authors might need to investigate more into the difference in performance and offer some insights. Let's take Table 1 for example, the gap between UEO and UPL on OH (Avg.) in quite significant, what might be the cause?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_tymK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698702590511,
        "cdate": 1698702590511,
        "tmdate": 1699636184381,
        "mdate": 1699636184381,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hOSy93bANH",
        "forum": "k2a2aPOA4b",
        "replyto": "k2a2aPOA4b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
        ],
        "content": {
            "summary": {
                "value": "This study tackles the problem of finetuning a vision-language model like CLIP on new unlabeled data with samples of unknown classes. To this end, a new approach called universal entropy optimization (UEO) is proposed. UEO utilizes the CLIP output score with known classes to determine whether a sample is an out-of-distribution (OOD) one. Then, the in-distribution samples are optimized following the standard entropy minimization strategy whereas the OOD samples are forced to maximize their prediction entropy. This finetuning process is parameter efficient as only the text prompts are involved during finetuning. Results using various methods across different open-set finetuning scenarios are evaluated, and the proposed strategy is validated to be effective."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The attacked problem of the side effect caused by out-of-distribution samples during unsupervised finetuning is interesting, as it is encountered for many downstream applications of a large pretrained model.\n    \n- The effort of trying to adopt a unified adaptive loss function for both ID and OOD samples are appreciated, even though this goal is not quite accomplished in this study as would be later discussed.\n    \n- This method is validated on both ResNet and Vit-B backbones across various domain adaptation datasets, and comparisons with previous studies indicate a superior performance of the current method."
            },
            "weaknesses": {
                "value": "- The strategy of entropy minimization for ID samples and entropy maximization for OOD samples have been a popular method[1, 2, 3]. This study applies the principle to the field of vision-language models. Despite its effectiveness on different benchmarks, the core idea resembles traditional ones, which would compromise the novelty of this study.\n    \n- I understand that the authors contribute in a generalized form as in Eq. (3) & (4) for the loss function of both ID and OOD samples. However, a similar principle of maximizing Mutual Information ID samples and penalizing the mutual information of OOD samples has been also proposed in [4].\n    \n- The theoretical derivation of Eq. (3) & (4) could be more explicit and detailed. The current version appears to be intuitive and lack thorough theoretical analysis. Eq. (3) is proposed just to satisfy the rule that minimize the entropy ID instances and maximization the entropy of OOD samples\u201d. However, no theoretical guarantee is provided so that Eq. (3) & (4) would always satisfy the above principle. The explanation is also missing of how Eq. (3) & (4) would be more suitable than a simple stepwise function, e.g. $L_{ID}=H(p(x)$ and $L_{OOD}=-H(p(x))$, and the determination of OOD samples follows the common practice as introduced in Sec. 3.1.\n    \n- As for the scope of application of the proposed method, it appears to be a general OOD method that can be also applied to traditional classification networks. I wonder why this method is applied to only CLIP method instead of extending it to other pretrained backbones.\n    \n\nReferences\n\n1.Chan, R., Rottmann, M. & Gottschalk, H. Entropy Maximization and Meta Classification for Out-of-Distribution Detection in Semantic Segmentation. in *2021 IEEE/CVF International Conference on Computer Vision (ICCV)* 5108\u20135117 (IEEE, 2021).\n\n2.Mac\u00eado, D., Ren, T. I., Zanchettin, C., Oliveira, A. L. I. & Ludermir, T. Entropic Out-of-Distribution Detection. in *2021 International Joint Conference on Neural Networks (IJCNN)* 1\u20138 (2021).\n\n3.Lee, K., Lee, H., Lee, K. & Shin, J. Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. Preprint at [http://arxiv.org/abs/1711.09325](http://arxiv.org/abs/1711.09325) (2018).\n\n4.Nimah, I., Fang, M., Menkovski, V. & Pechenizkiy, M. ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. in *Findings of the Association for Computational Linguistics: EMNLP 2021* (eds. Moens, M.-F., Huang, X., Specia, L. & Yih, S. W.) 1606\u20131617 (Association for Computational Linguistics, 2021)."
            },
            "questions": {
                "value": "From my point of view, Eq. (3) can also be viewed as an implicit threshold strategy to determine OOD samples. Specifically, assume of the max softmax probability $w$ follows a uniform assumption $w\\sim \\mathcal U(\\frac{1}{C},1)$ , where $C$ denotes the total number of ID classes. The expectation $\\mathbb E(w)=\\frac{1}{2}(1-\\frac{1}{C^2})$ and $\\mathbb E(\\frac 1 w)=log(C)$. Therefore, $\\tilde w(x) - \\tilde \\Phi(w(x))\\approx \\frac{w}{\\mathcal B_t \\mathbb E(w)} - \\frac{1/w}{\\mathcal B_t \\mathbb E(1/w)}$, and the thereshold for determining whether a sample is OOD now becomes $\\lambda=\\frac{1}{2} (1-\\frac{1}{c^2})\\log(C)$. In other words, Eq. (3) could be also one implicit form of thresholding strategy. I think the author should state the explicit benefit brought by the unified form as in Eq. (3) and (4) compared to a hard thresholding one. For example, we can observe in the form of $\\lambda$ that $\\lambda$ increases with the number of classes $C$ , yet I could not understand the rationale of this property."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2480/Reviewer_eDVD"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2480/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761793021,
        "cdate": 1698761793021,
        "tmdate": 1699707480991,
        "mdate": 1699707480991,
        "license": "CC BY 4.0",
        "version": 2
    }
]