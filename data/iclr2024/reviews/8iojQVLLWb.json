[
    {
        "id": "sCxmoh8aiF",
        "forum": "8iojQVLLWb",
        "replyto": "8iojQVLLWb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_kgXL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method based on Bayesian knowledge distillation for online action detection. A teacher-student framework is proposed. By distilling the mutual information and distributions of a Bayesian teacher model to an evidential probabilistic student model. The student model can not only make fast and accurate inference, but also efficiently quantify the prediction uncertainty. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Pros:\n1. A method based on Bayesian knowledge distillation is proposed, which makes inference more efficient.\n\n2. Experimental results verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "Cons:\n1. This paper seems to simply combine existing knowledge distillation and uncertainty techniques. Knowledge distillation is already proposed in PKD (Zhao et al. (2020)), and uncertainty technique have been used in Uncertainty-OAD (Guo et al. (2022)) for online action detection.\n\n2. More details about the teacher model should be included. The results of the teacher model are also missing.\n\n3. More visual analysis should be included instead of all numerical analysis.\n\n4. Can the performance of the student model boost by increasing the number of model parameters? I wonder when the performance of the student model can exceed that of the existing state-of-the-art methods when the number of parameters is increased."
            },
            "questions": {
                "value": "See Weaknesses for more details, and limitations should also be included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698245865909,
        "cdate": 1698245865909,
        "tmdate": 1699636073494,
        "mdate": 1699636073494,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "I3OuCrEVh2",
        "forum": "8iojQVLLWb",
        "replyto": "8iojQVLLWb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
        ],
        "content": {
            "summary": {
                "value": "This paper explores a Bayesian knowledge distillation (BKD) framework for the online action detection task, which aims at identifying the ongoing action in a streaming video without seeing the future. Specifically, the teacher model is a Bayesian neural network which outputs the mutual information between historical features and ongoing action and predicts the detection uncertainty. The student model learns useful information from the teacher model and constructs a Dirichlet distribution for uncertainty quantification. Competitive performances are achieved on multiple benchmarks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ As far as I am concerned, this is the first work which applies distillation on the online action detection task, which may provide some inspiration for the community."
            },
            "weaknesses": {
                "value": "- Lack of novelty. Although it may be the first work which applies the distillation architecture on online action detection, there is little innovation on components design and theoretical analysis. Teacher-student architecture, Bayesian neural network, and attention network are all very common tools.\n- Misleading/Inappropriate use of evidential deep learning (EDL). EDL is based on the Subjective Logic theory, which is implemented by its unique optimization objective and is accompanied by its own uncertainty calculation method. In the work, the authors construct a simple Dirichlet distribution and then claim they adopt EDL, which is not true.\n- Unfair (or at least incomplete) comparison of computation efficiency and model complexity. Authors claim that the proposed method BKD achieves competitive performance with less model complexity and computational cost, and provide comparison results on Table 2. However, the other methods did not adopt a teacher-student distillation manner as BKD, and the Bayesian neural network which is used as the teacher model by BKD is quite computationally heavy. It is a very natural result for BKD to achieve fast inference at the cost of much larger computation in the training phase via distillation, and the comparison of training speed is not provided. \n- Careless writing. For example, there are two very obvious citation mistakes on the first page of supplementary material."
            },
            "questions": {
                "value": "1. More discussion about the unique novelty of this work may be provided.\n2. Why do the authors use CE loss and Eq.(10) for model optimization and uncertainty quantification, instead of using the EDL loss and the EDL uncertainty estimation method?\n3. A comparison of training time is necessary for the completeness of experiments.\nFor others please refer to the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_i6Lf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698725430958,
        "cdate": 1698725430958,
        "tmdate": 1699636073385,
        "mdate": 1699636073385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bJhf7qSgdt",
        "forum": "8iojQVLLWb",
        "replyto": "8iojQVLLWb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
        ],
        "content": {
            "summary": {
                "value": "This paper tried to handle a practical video detetion problem, online action detection without seeing the future frames. The authors introduced Bayesian knowledge distillation as a teacher network and evidential probabilistic neural network as a student network. The proposed method is evaluated on three benchmark datasets including THUMOS\u201914, TVSeries, and HDD and shows the efficiency. Ablation studies are conducted to prove the efficiency of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Online action detection aims at identifying the ongoing action in a streaming video without seeing the future and it is a practical problem for the video analysis. The authors tried to resolve a real problem with the motivation behind, to make the whole network inference efficient. They tried the teacher and student architecture and introduced Bayesian knowledge distillation as a teacher network and evidential probabilistic neural network as a student network. Comprehensive experimental results on several benchmark datasets are provided."
            },
            "weaknesses": {
                "value": "The paper appears to be more of an attempt at work rather than containing a substantial amount of insights or analysis. The paper introduced the Bayesian knowledge distillation, (for the first time in knowledge distillation?), however the authors did not provide much insights, such as why it will make the learning efficient etc. The same issue happend to the student network, the authors just introduced the network into the paper without much explainations. From my point of view, I did not understand why this paper should stand out due to two important proposals. Meanwhile, the experimental results shown in the paper are pretty cherry picked, such as MAT (Wang et.al 2023) is compared in different tables. However, in the Figure 4, the authors did not show MAT, on the contrary, the authors showed papers published before 2022. \n\nThe performance in the paper is not impressive, whatever in mAP or FPS."
            },
            "questions": {
                "value": "The questions are listed in the weakness section. Please address these questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1447/Reviewer_37DN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698874068456,
        "cdate": 1698874068456,
        "tmdate": 1699636073293,
        "mdate": 1699636073293,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ALFcm92MAc",
        "forum": "8iojQVLLWb",
        "replyto": "8iojQVLLWb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1447/Reviewer_f1zi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present Bayesian knowledge distillation (BKD), a framework for online action detection that is both efficient and generalizable. The authors utilize a teacher-student architecture to improve efficiency. A key aspect of the proposed method is the introduction of a student model based on the evidential neural network. This student model learns feature mutual information and predictive uncertainties from the teacher model. With this design, the student model can not only select important features and make fast inferences, but also accurately quantify prediction uncertainty with a single forward pass. The proposed method was evaluated on three benchmark datasets: THUMOS\u201914, TVSeries, and HDD."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Scope: Online action detection is a crucial task for various applications, including autonomous driving, visual surveillance, and human-robot interaction. This paper addresses challenges such as incomplete action observations and computational efficiency. The focus of the paper is how a model can generalize to unseen environments. Overall, the work is relevant to the ICLR community.\n2. The authors propose Bayesian knowledge distillation (BKD) as a solution for efficient and generalizable online action detection. They utilize a teacher-student architecture for knowledge distillation and leverage the student model to enable efficient inference."
            },
            "weaknesses": {
                "value": "1. Contribution #1: The authors assert that the proposed Bayesian deep learning model contributes to the task of active online action detection. However, the reviewer has concerns regarding this claim for the following reasons:\n    a. Firstly, the reviewer agrees that inference speed is an important consideration and acknowledges the adoption of a teacher-student model. However, the authors have not provided justification for the limitations of existing teacher-student architectures for online action detection. It remains unclear why the authors chose to leverage evidential deep learning for this purpose. Additionally, there is no comparison of different teacher-student architectures, making it difficult for the reviewer to understand the rationale behind this design choice.\n    b. Secondly, the motivation behind incorporating uncertainty prediction into the design is unclear. While the reviewer acknowledges the need to consider uncertainty due to the inherent unpredictability of the future, the authors have not highlighted the limitations of not modeling uncertainty. For example, a deterministic prediction of the current action may not be necessary, and predictions of potential opportunities for different actions may suffice.\n    c. The authors present an experiment in Figure 6 to demonstrate the use of uncertainty quantification for abnormal action detection, specifically in the context of THUMOS. However, the experiment appears to be relatively simple and may be considered cherry-picked. A comprehensive evaluation of the proposed framework is necessary to validate its effectiveness.\n2. The second claim is that the proposed framework can perform feature selection using mutual information and output Bayesian predictive uncertainties. The reviewer was expecting experimental evidence to support this claim. However, the reviewer did not find sufficient evidence regarding the effectiveness of the architectural design. To substantiate this claim, the reviewer requests experiments that can validate its effectiveness.\n3. Experiments: As mentioned in point 1 (a), the authors primarily focus on demonstrating that the proposed framework performs on par with existing methods on benchmarks for online action detection. However, the reviewer believes that there is a lack of insights regarding the ablation of teacher-student architectures. To establish the value of the proposed evidential deep learning approach, the reviewer requests experiments that can validate this claim."
            },
            "questions": {
                "value": "The reviewer identified several major concerns in the Weakness section and would like to know the authors' thoughts on these points. Please answer each concern in the rebuttal stage. The reviewer will respond according to the authors' rebuttal in the discussion phase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1447/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698917454150,
        "cdate": 1698917454150,
        "tmdate": 1699636073219,
        "mdate": 1699636073219,
        "license": "CC BY 4.0",
        "version": 2
    }
]