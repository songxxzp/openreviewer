[
    {
        "id": "jbfwEpmJNf",
        "forum": "7VPTUWkiDQ",
        "replyto": "7VPTUWkiDQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_JA2G"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_JA2G"
        ],
        "content": {
            "summary": {
                "value": "The authors theoretically and empirically show that compositional generalization can be achieved through:\n1. Structural constraints on the decoder (each data dimension is rendered as the sum of functions operating on slots separately), which ensures that the decoder compositionally generalizes.\n2. An encoder-decoder consistency loss (reconstruction loss for the representations) on slot-shuffled representations from the encoder output, which encourages the encoder to compositionally generalize with the additive decoder.\n\nThe paper provides a joint encoder-decoder framework for compositional generalization for autoencoders, where previous work has mostly focused on specific aspects of the setting."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is very well-written.\n- The theory is sound and significant for the community.\n- The joint encoder-decoder framework for compositional generalization in autoencoders is quite elegant.\n- The limitations of the framework and the additivity constraint on the decoder are adequately stated."
            },
            "weaknesses": {
                "value": "Although they support the theory, the experiments are quite limited. For instance, these are all with only two slots with 16 dimensions each. See the questions section for additional information that would be interesting to see from experimentation."
            },
            "questions": {
                "value": "- How does the effect of the consistency loss scale with the number of slots?\n- What is the impact of how slot-supported the training data is? i.e. in Figure 2 (1), what is the impact of the width of the blue band on empirical effectivity?\n- How does the method hold up on non-synthetic data, especially if you slightly relax some constraints? For instance, what if you have expressive slot-wise decoding, but allow for a low-expressivity non-linear combination at the end for rendering?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612735636,
        "cdate": 1698612735636,
        "tmdate": 1699637046243,
        "mdate": 1699637046243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ClvWuLVjqS",
        "forum": "7VPTUWkiDQ",
        "replyto": "7VPTUWkiDQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_wPGp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_wPGp"
        ],
        "content": {
            "summary": {
                "value": "This paper presents conditions where compositional generalization is theoretically guaranteed for object-centric learning. \n\nSpecifically, they first extend the identifiability theory of object-centric representations to handle partial joint distribution supports, with an additional assumption/constraint on the decoder to be compositional. This ensures that slots are identifiable in the training distribution. They then ensure the generalizability of decoders (which, e.g., generate images given slot representations) with another assumption/constraint as the decoder being additive. The theoretical analysis is similar to those proving the compositional generalizability of any additive inference models.\n\nThe novel step is to enforce the compositional generalizability of encoders by learning with the synthesized data in new compositions of latent slots/symbols given the generalizable decoder. So, in order to learn an encoder that can generalize to unseen combinations of objects, they first build a dataset with new compositions of latent symbols/slots by permutating learned latent symbols/slots in the training distribution. They then generate the fake images using the \"supposedly generalizable\" decoders on new combinations. The encoder is trained to learn the inverse mapping of the decoder. This process is formulated as a compositional consistency regularization loss in practice.\n\nThe experimental results are aligned with the theories in a simple two-object synthetic image environment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper discusses an important problem: learning compositionally generalizable object-centric representations. The paper is well-written and easy to read. The connections with related works are also interesting and inspiring. \n\nThe reviewer especially appreciates the theoretical guarantees and analysis. Even though the assumptions are strong on both the functions to be approximated as well as the parameterization of learned functions, they are still aligned with the image object-centric representation learning setting, and the methods can be relaxed and realized using modern object discovery methods such as slot attentions. \n\nThe proposed regularization loss to enforce the compositional generalizability of encoders is interesting and seems easy to use. \n\nThe ablation study on the additive decoder (softmax v.s. sigmoid in slot attentions) is interesting and inspiring."
            },
            "weaknesses": {
                "value": "It would be great if the assumptions could be relaxed, e.g., to handle occluded objects or to handle general latent variable learning domains other than the image objects. \n\nThe \"contemporary\" work [1] discussed most parts of this paper except for the generalizable encoder. \n\nThe experimental environment is simple with two-object synthetic images. It would be more convincing to see results on multi-object real images. \n\n[1] S \u0301 ebastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. arXiv preprint arXiv:2307.02598, 2023."
            },
            "questions": {
                "value": "Are there results in more complex environments? \n\nCan the theories be generalized to more general settings with weaker assumptions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634403846,
        "cdate": 1698634403846,
        "tmdate": 1699637046123,
        "mdate": 1699637046123,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Cqyjwk0Hak",
        "forum": "7VPTUWkiDQ",
        "replyto": "7VPTUWkiDQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_KGuC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8400/Reviewer_KGuC"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the problem of compositional generalization in object-centric autoencoders.\nThe authors formalize this as requiring the model to identify the ground-truth object latents not just on the training distribution, but also on out-of-distribution combinations.\nThey make two key assumptions to achieve this: (1) The generative process satisfies compositionality, meaning each pixel depends on one object, and irreducibility, preventing objects from being decomposed.\n(2) The decoder is additive, decoding each object slot independently.\nUnder these assumptions, the authors prove autoencoders can identify objects in-distribution by minimizing reconstruction error.\nThe additive decoder then guarantees generalization out-of-distribution.\nHowever, the encoder may still fail to generalize.\nTo address this, the authors propose compositional consistency regularization.\nThis trains the encoder to invert the decoder on recombined object slots, enabling the full autoencoder to generalize.\nBy combining in-distribution identifiability and compositional consistency regularization, the authors prove autoencoders satisfying their assumptions will generalize compositionally.\nThrough synthetic experiments, they provide empirical evidence supporting their theoretical results.\nIn particular, they demonstrate the importance of additivity and compositional consistency for generalization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper made contributions for \n- Formalizing compositional generalization as an identifiability problem\n- Theoretical guarantees for in-distribution identifiability\n- Showing an additive decoder enables out-of-distribution generalization\n- Introducing compositional consistency regularization\n- Providing overall theoretical guarantees for compositional generalization\n\nThe work makes theoretical progress on understanding compositional generalization in object-centric representation learning."
            },
            "weaknesses": {
                "value": "- The assumptions of compositionality and irreducibility are quite restrictive. Most real-world datasets likely violate these. \n- The additive decoder limits modeling of complex object interactions and relations.\n- The consistency regularization implementation requires sampling implausible object combinations. More principled schemes could improve results in complex environments.\n- Experiments only validate the theory on simple synthetic datasets. Testing on more diverse and realistic data would better demonstrate applicability, though the evaluation would also be more challenging.\n- The proposed methods, especially when ensuring encoder-decoder consistency and handling latent slots, might pose scalability issues for very large datasets or more complex models. A discussion on the scalability, computational costs, and potential solutions would make the paper more robust."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8400/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698730754058,
        "cdate": 1698730754058,
        "tmdate": 1699637046010,
        "mdate": 1699637046010,
        "license": "CC BY 4.0",
        "version": 2
    }
]