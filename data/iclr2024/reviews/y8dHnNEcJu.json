[
    {
        "id": "VrWc5irLKw",
        "forum": "y8dHnNEcJu",
        "replyto": "y8dHnNEcJu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a novel method for weakly-supervised semantic segmentation (WSSS) by exploiting the large vision-language model, CLIP. By adopting a pre-trained WSSS model as a mask generator, they update the parameters of the generator using contrastive learning between the image-text triplet. After that, the learnable prompts are trained by contrastive prompt learning. Then, the mask generator is further updated using the proposed class-associated semantic refinement. By leveraging the CLIP-based image-text aligning, they improve the strong baseline, WeakTr, and achieve state-of-the-art performances on VOC 2012 and COCO 2014 benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, and the description of the proposed methods is clear with well-illustrated figures.\n\n2. The proposed method (segment-label matching, contrastive prompt learning, class-associated semantic refinement) is intuitive and convincing.\n\n3. The experiment is somewhat complete and the proposed methods are well-ablated."
            },
            "weaknesses": {
                "value": "## 1. complex training pipeline.\n\nAs I understand, this method is a refinement method for existing WSSS methods using knowledge of CLIP.\nNamely, this paper adopts WeakTr as a strong baseline WSSS method and refines it using CLIP-based contrastive learning.\nFrom a from-scratch training perspective, WeakTr requires a two-step training pipeline (CAM generation and online retraining) and SemPLeS requires a three-step training pipeline (segment-label matching, contrastive prompt learning, and class-associated semantic refinement).\n\nI wonder if the mask generator could be replaced by attention maps of CLIP or activation maps from CLIP (as done in CLIP-ES).\n\n## 2. dependency of the mask generator.\nI have a concern that the performance of the proposed method may largely depend on the mask generator. If the mask generator fails to generate proper segmentation masks, the WSSS performance is largely dropped.\nI guess the proposed (refinement) method can be adapted to other WSSS methods, but there is only one experiment using WeakTr.\n\n## 3. more meaningful comparison.\nThe methods in Table 3 can be categorized into two groups, CLIP-based methods (CLIMS and CLIP-ES) and CLIP-free methods.\nConsidering that CLIMS achieved 68.7% and CLIP-ES achieved 71.4% in VOC 2012 testset, the baseline method, WeakTr, already shows a much higher performance of 74.0%.\nAlthough SemPLeS achieved a 74.8% performance, I think this outstanding performance is mainly from WeakTr.\nIt would be great if there were any attempts to compare with CLIP-based methods more meaningfully (e.g., using the same seg model or the same baseline model).\n\nAlso, even though the SemPLeS used the large-scale vision-language model (CLIP) with the complex three-step refinement training, it is questionable that the 0.5% improvement in COCO 2014 validation set (Table 3) is promising."
            },
            "questions": {
                "value": "Overall, I think the proposed method is interesting and convincing.\nHowever, I have some concerns related to the complex training pipeline and dependency of the mask generator model.\n\nTherefore, my initial rating is weak reject (4), and I finalize the rating after rebuttals."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I conclude that there are no ethics concerns in this paper."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission925/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission925/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission925/Reviewer_LBiv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586261309,
        "cdate": 1698586261309,
        "tmdate": 1699636019360,
        "mdate": 1699636019360,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fOyt0IhPon",
        "forum": "y8dHnNEcJu",
        "replyto": "y8dHnNEcJu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission925/Reviewer_ZuKn"
        ],
        "content": {
            "summary": {
                "value": "This paper propose a prompt-learning method to enhance weakly-supervised semantic segmentation prediction.  Existing works often utilize pre-trained CLIP models to guide the class-specific foreground mask predictions, however, they often fail to separate co-occurring background categories from foreground (e.g. train vs. railroad, horse vs. grass).  Prior works address such issues by manually design background prompts for each category, hoping to refine the predicted pseudo mask.  Such methods require human efforts to manually annotate the background prompts.  This paper propose a stage-learning technique, by first training object mask predictors and then background prompts with image-text contrastive learning.  This paper demonstrates SOTA performance on standard benchmakrs, such as VOC and MSCOCO."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of separating co-occurring background from foreground for each class makes a lot of sense.  In particular, the examples of the train and co-occurring railroad is convincing.\n\n2. The results are SOTA on both benchmarks.\n\n3. The paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "1. There are some citations missing:\n    * ***intra-class foreground-background discrimination***: Learning Integral Objects With Intra-Class Discriminator for Weakly-Supervised Semantic Segmentation.  Fan et al. CVPR 2020.\n    * ***pixel-wise contrastive learning for WSSS***: Universal Weakly Supervised Segmentation by Pixel-to-Segment Contrastive Learning. Ke et al. ICLR 2021.\n\n2. The efficacy of class-specific background prompt is not clear.  In Table 4, training with $L_{prompt}^f$ seems to be more effective than $L_{prompt}^b$ and $L_{refine}$.\n\n3. Segment Anything (SAM) is a strong framework for mask proposals.  Without fine-tuning on VOC / MSCOCO, I believe SAM can still provide high-quality segmentation on out-of-distribution imagery.  Reasonable baselines are: 1) classifying mask proposals with SAM using OVSeg[1]/CLIP/MaskCLIP+[2] features, and 2) instead of learning background prompts, one can learn to refine per-category mask proposals using dense segmentations derived from SAM.  We can vote the foreground confidence within each SAM segment (very like refine binary masks with CRF).  However, this paper does not provide any comparison to SAM (see questions for details).\n\n\n[1] Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. Liang et al.  CVPR 2023.\n[2] Extract Free Dense Labels from CLIP. Zhou et al. ECCV 2022."
            },
            "questions": {
                "value": "1. My first concern is the idea of using CLIP to guide mask predictions.  CLIP is notoriously know to perform poorly on segmentation.  May works have been proposed to address such issues (e.g. OVSeg[1] and MaskCLILP+[2]).  My question is why the authors choose to use CLIP but not MaskClip+ features?  In fact, probably we don't even need the proposes background prompt learning when using mask-sensitive CLIP features. The authors should provide analysis on OVSeg/MaskCLIP+ features.\n\n2. I'm not sure how necessary it is to train the Mask Predictor. SAM is an existing strong segmentation framework.  Why not apply OVSeg[2] (with intra-class background prompt tuning) on mask proposals from SAM?  Or even refine the mask predictions with SAM.\n\n3. In Table 4, what's the performance if you train with only $L_{match}$ and $L_{prompt}^f$?  It seems to me that training with $L_{prompt}^f$ brings the most performance gain, meaning the background prompts might not be as effective as the paper claims."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771853777,
        "cdate": 1698771853777,
        "tmdate": 1699636019290,
        "mdate": 1699636019290,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8u19XhPj13",
        "forum": "y8dHnNEcJu",
        "replyto": "y8dHnNEcJu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission925/Reviewer_WM2o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission925/Reviewer_WM2o"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a Semantic Prompt Learning for WSSS (SemPLeS) framework. The author proposes contrastive prompt learning to acquire class-associated background prompts and further proposes a Class-associated Semantic Refinement module to suppress erroneous associations of co-occurring backgrounds. This method shows better performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors propose a novel CLIP prompt method that was automatically learned rather than manually designed, which effectively promotes the alignment of the semantic space.\n\n2. The authors provide a detailed explanation of the modulation process, demonstrating the effectiveness of learnable prompts.\n\n3. The logic of this paper is clear and easy to read."
            },
            "weaknesses": {
                "value": "1. In Sec. 2.1, the author briefly introduces the current research status of WSSS three-stage learning. However, this method is only a research branch of WSSS, and the end-to-end method should be supplemented. Furthermore, as far as we know, the recent WSSS research is basically based on CLIP. The author's innovation lies in the non-manual design of prompts rather than establishing vision-language associations, thus there is no need to emphasize the contribution of using CLIP.\n\n2. In Figure 2 (a), symbol abbreviations X_k^f and X_k^b are given for the foreground and background of the image, and the text prompt t_k should also be indicated in this figure.\n\n3. In Sec. 3.2.2, it writes \u201clearnable prompts p_k as the input of the text encoder E_t\u201d. It is necessary to illustrate how to initialize it and its shape. The motivation for learning prompts is not new, there exist many works that learn prompts in the CLIP community, such as COOP. More differences between these methods should be discussed.\n\n4. Missed comparison with other methods, such as the work titled \"MARS: Model-agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation \" from ICCV 2023. To ensure a comprehensive research evaluation and establish a fair assessment of the proposed method's performance, the authors should include extensive analysis and comparison with more SOTAs.\n\n5. Figures 3 and 4 can be augmented by incorporating a dedicated column on the left-hand side to present image labels, rather than embedding them within the figure itself. This graphical modification not only improves visual clarity but also aligns with the model's separate processing of images and labels.\n\n6. What does \"All BG Prompts\" in picture 4 mean?\n\n7. Result of L_match + L_prompt^b + L_prompt^f  should be added in Table 4."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698926122615,
        "cdate": 1698926122615,
        "tmdate": 1699636019214,
        "mdate": 1699636019214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WIWUXKn7gI",
        "forum": "y8dHnNEcJu",
        "replyto": "y8dHnNEcJu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission925/Reviewer_BYUr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission925/Reviewer_BYUr"
        ],
        "content": {
            "summary": {
                "value": "A new Weakly-Supervised Semantic Segmentation (WSSS)  method, called SemPLeS, is proposed in this paper. In SemPLeS, Contrastive Prompt Learning and Class-associated Semantic Refinement are used to learn the prompts that adequately describe and suppress the image backgrounds associated with each target object category. The authors tested SemPLeS, and it outperformed the existing SOTA on popular benchmarks like PASCAL VOC and MS COCO."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Using contrastive learning and CLIP text/visual encoders is an interesting idea. Especially optimizing learnable negative prompts and applying it to contrastive learning with positive image regions is very interesting. The novelty of the proposed method seems to be high.\n\nBy the experiments, the effectiveness of the proposed method was clearly shown. The proposed method seems to outperform the current SOTA method, WeakTr."
            },
            "weaknesses": {
                "value": "According to Paper-with-code, SOTAs for val and test are 78.4 and 79.0 by WaekTr, respectively. Why the results of the baselines shown in Table 2 are less than that ? The authors should present results of the proposed method under the same settings as Paper-with-code SOTA.\nhttps://paperswithcode.com/sota/weakly-supervised-semantic-segmentation-on-1\nhttps://paperswithcode.com/sota/weakly-supervised-semantic-segmentation-on"
            },
            "questions": {
                "value": "I'm wondering if the proposed method is also effective for panatomic segmentation tasks such as Pascal Panatomic and CityScapes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission925/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698952286669,
        "cdate": 1698952286669,
        "tmdate": 1699636019122,
        "mdate": 1699636019122,
        "license": "CC BY 4.0",
        "version": 2
    }
]