[
    {
        "id": "eYQQuAJgIe",
        "forum": "cVUOnF7iVp",
        "replyto": "cVUOnF7iVp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
        ],
        "content": {
            "summary": {
                "value": "The paper studies sparse linear regression in the different local differential privacy models (LDP). \n\nFor non-interactive LDP they propose an algorithm with estimation error $\\tilde{O}(\\frac{d\\sqrt{k}}{\\epsilon\\sqrt{n}})$, and show a lower bound $\\Omega(\\frac{\\sqrt{dk\\log d}}{\\epsilon \\sqrt{n}})$ (for sub-Gaussian covariates). In addition, they show that it is possible to improve the upper bound by a factor $\\sqrt{d}$ given public unlabeled covariates.\n\nFor interactive LDP they propose an algorithm with estimation error $\\tilde{O}(\\frac{k\\sqrt{d}}{\\epsilon\\sqrt{n}})$, and show a lower bound $\\Omega(\\frac{{\\sqrt{dk}}}{\\epsilon \\sqrt{n}})$ (for sub-Gaussian covariates)"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "There are a few new non-trivial results that improve over the state of the art. The paper is well-written, I really enjoyed reading it. The contribution and comparison with prior works is clear. The idea behind Algorithm 1 is very nice and seems to be new (though I'm not an expert in the field, so I'm not 100% sure).\n\nIn addition, they found and fixed a bug in one of the results of prior work [1] on the iterative LDP settings that implied an incorrect upper bound. I briefly checked it, and indeed Hoelder's inequality in the proof of Theorem 9 is used incorrectly there, so it is good that this mistake was found and fixed."
            },
            "weaknesses": {
                "value": "I didn't find major weaknesses. However there is one thing (which I formulate in the Questions below) that is confusing to me."
            },
            "questions": {
                "value": "Your proof of Theorem 7 looks very similar to the proof of Theorem 9 from [1] (and you mention that). Could you please explain what are important differences, assuming linear regression settings? (I didn't check the details, so maybe I missed something). From the first glance it looks like the proof from [1] works not only for the uniform distribution, but also for 1-sub-Guaussian distributions (modulo their wrong bound in the very beginning), and if it is the case, it should also work for you settings, or did I miss anything important?\n\nAnd one minor thing: I suggest to move Table 1 to the introduction.\n\n[1] Di Wang and Jinhui Xu. On Sparse Linear Regression in the Local Differential Privacy Model.\nIEEE Transactions on Information Theory 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5338/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5338/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5338/Reviewer_ZWvr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792144108,
        "cdate": 1698792144108,
        "tmdate": 1699636536655,
        "mdate": 1699636536655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O16uFRHQN5",
        "forum": "cVUOnF7iVp",
        "replyto": "cVUOnF7iVp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_UbUb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_UbUb"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of sparse linear regression under the local differential privacy setting. The authors provide new lower bound results for this problem with a $k$-sparse underlying model parameter. In addition, the authors develop efficient upper bound algorithms for the same problem."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of the current paper are summarized as follow:\n1. The authors provide new lower bound results for sparse linear regression under local differential private model.\n2. The authors develop new efficient algorithm for solving the same problem."
            },
            "weaknesses": {
                "value": "The weaknesses of the current paper:\n1. It is unclear the dimension dependence in the lower bound is due to the hardness of the LDP setting or the norm of the data.\n2. It is unclear why the authors need the $\\ell_1$ norm bound in their results.\n3. It is unclear why Assumptions 1 and 2 are both required in the upper bound results.\n4. Why it is reasonable to consider the sparse model in the classical setting?\n5. The sample complexity requirement seems to be very bad in terms of $d$."
            },
            "questions": {
                "value": "Here are some additional questions I have for the current paper:\n1. For the Remark 2, why the authors claim that the sparse linear models in the non-interactive LDP setting are ill-suited? It seems to me that the dimension dependence in Theorem 1 comes from the norm of the data, what will the result look like if you assume the data vector to be $\\ell_2$ norm bounded? In addition, the results in Raskutti et al. 2011 and Cai et al. 2021 seem to assume the data vector to be $\\ell_2$ norm bounded.\n2. For Theorem 3, why do you assume Assumptions 1 and 2 holds simultaneously? In Assumption 1, you assume $x$ with covariance $\\Sigma$, and the transformed data to be Sub Gaussian. In Assumption 2, you further assume $x_i$ has variance $\\sigma^2$. In addition, what is the assumption on $\\zeta$?\n3. If the lower bound has nothing to do with $\\ell_1$ norm bound, you should give the results in terms of the $\\ell_2$ norm bound. \n4. Whether the upper bound results can be extended to the $\\ell_2$ norm bound case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698793758150,
        "cdate": 1698793758150,
        "tmdate": 1699636536555,
        "mdate": 1699636536555,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VCB68tqTzO",
        "forum": "cVUOnF7iVp",
        "replyto": "cVUOnF7iVp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_qQ5H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5338/Reviewer_qQ5H"
        ],
        "content": {
            "summary": {
                "value": "This paper studies sparse linear regression under local differential privacy. Firstly, it establishes a lower bound under a non-interactive LDP protocol for sub-gaussian data. Secondly, it proposes the first upper bound that has a $\\sqrt{d}$ gap compared to the aforementioned lower bound. It also demonstrates that this gap can be closed if public unlabeled data is available. Lastly, in the case of sequentially interactive protocol, this paper presents a lower bound and corrects the results of the iterative hard thresholding algorithm from prior work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is thorough and clearly written. \n2. The problem is well-defined and important."
            },
            "weaknesses": {
                "value": "The upperbound and lowerbound do not match. It is unclear which bound is tight. Also $n$ has to be greater than $O(d^4)$ to achieve a rate of $O(d)$ in Theorem 3."
            },
            "questions": {
                "value": "1. Is the l2 norm the right metric for linear regression? For example, Cai at el (2021) consider $\\|\\theta^{priv}-\\theta^*\\|_\\Sigma$, which corresponds to minimal emprical risk. Do the results also hold under this normalized metric?\n2. Is k used in Algorithm 1?\n3. Regarding Remark 3, is it necessary to release the covariance matrix privately in LDP model? Can you privatize the two terms in OLS solution together?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5338/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806391684,
        "cdate": 1698806391684,
        "tmdate": 1699636536460,
        "mdate": 1699636536460,
        "license": "CC BY 4.0",
        "version": 2
    }
]