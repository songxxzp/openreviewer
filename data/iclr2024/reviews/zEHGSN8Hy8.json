[
    {
        "id": "TDT0xBIy0x",
        "forum": "zEHGSN8Hy8",
        "replyto": "zEHGSN8Hy8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_wrMM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_wrMM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method to learn the sentence embedding from existing pre-trained sentence embedding to distinguish different semantics and define similarity-based set operations, including intersection and difference and their combinations. The paper evaluates the method on artificial data and shows several case studies of applications, including semantic search, data annotation, and topic discovery."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes a novel and simple method to fine-tune existing pre-trained embeddings to be fit to set-based operations. \n- The results in the artificial setting show the feasibility of the method for several different sentence embedding\n- The case studies show interesting results that present the method's potential."
            },
            "weaknesses": {
                "value": "- Representing set operation with embedding is not novel, but the comparison and discussion compared with existing methods are missing. \n  - Vitalii Zhelezniak, Aleksandar Savkov, April Shen, Francesco Moramarco, Jack Flann, Nils Y. Hammerla Don't Settle for Average, Go for the Max: Fuzzy Sets and Max-Pooled Word Vectors. ICLR 2019.\n  - Siddharth Bhat, Alok Debnath, Souvik Banerjee, and Manish Shrivastava. Word Embeddings as Tuples of Feature Probabilities. RepL4NLP. 2020.\n  - Shib Dasgupta, Michael Boratko, Siddhartha Mishra, Shriya Atmakuri, Dhruvesh Patel, Xiang Li, and Andrew McCallum. Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings. ACL2022.\n- The set operation presented in the paper does not satisfy the commutative law, and it orders the elements in the first set. This is not the usual set theory, and the users may be confused if they use the method that supports usual set operations, but the limitations are not discussed in detail.\n- The quantitative evaluation is performed only in artificial settings, and there are only case studies for the application results. It is unclear how the method can be stably used for the application."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3036/Reviewer_wrMM",
                    "ICLR.cc/2024/Conference/Submission3036/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698150126242,
        "cdate": 1698150126242,
        "tmdate": 1700697419483,
        "mdate": 1700697419483,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t3JC5HiNpK",
        "forum": "zEHGSN8Hy8",
        "replyto": "zEHGSN8Hy8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_fKmY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_fKmY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new information retrieval framework, SetCSE, based on set operation and sentence-level contrastive learning. The whole framework includes two steps: 1. fine-tuning sentence embedding model by minimizing inter-set loss. 2. ranking sentences in the set based on definition 2. The paper then defines two set operations, interactions and differences, based on the sentence-set semantic similarity. Furthermore, the paper conducts experiments for both set intersection and difference. Additionally, the paper shows three real-world applications, including semantic search, data annotation, and new topic discovery. The paper also provides quantitative analysis by comparing SetCSE with supervised learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper formulates the sentence retrieval problem as a combination of sentence-set similarity and set operations, which is novel for the community. \n2. The paper provides a comprehensive set of experiments, introducing two new settings: set intersection and set differences. It uses multiple baselines to demonstrate the robustness of the framework. The paper also offers sentence embedding visualizations to illustrate the improvement in sentence representations. Additionally, the paper presents detailed hyperparameters and offers quantitative justification for the proposed framework\n3. The paper provides three different downstream applications. Each is paired with background papers and results to show the effectiveness of the proposed methods."
            },
            "weaknesses": {
                "value": "1. The paper would benefit from an additional experiment on sentence retrieval, similar to the one in Section 6.1. In this setting, the paper could compare its performance against traditional retrieval-based methods such as DPR and BM25 to better illustrate the model's improvements. Furthermore, in Table 2, certain baseline models do not show significant improvements with SetCSE. For instance, the improvements for SGPT are marginal when compared to other baselines. The paper should include an analysis explaining the variations in improvements among different models\n2. The core concept is to create clusters of sentences with semantic meaning, and this can limit the generalization ability of the proposed framework. In comparison to other baselines, the incorporation of semantic meaning within sets naturally provides additional information for training.\n3. The paper fails to provide code."
            },
            "questions": {
                "value": "How to adapt the proposed method to a situation where sentences are not clustered with semantic meaning or where the clusters do not exist?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3036/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3036/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3036/Reviewer_fKmY"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698465362166,
        "cdate": 1698465362166,
        "tmdate": 1700696970266,
        "mdate": 1700696970266,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OecP3CIS1V",
        "forum": "zEHGSN8Hy8",
        "replyto": "zEHGSN8Hy8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_BQxn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_BQxn"
        ],
        "content": {
            "summary": {
                "value": "The current research piece builds on top of existing sentence embedders based on contrastive learning by defining set operations that can be applied to sentences. Concretely, the concept of semantic similarity between sentences is extended to be defined between individual ones and Sets of sentences, by taking the average similarity. \nTwo new set operations are defined. The operations build upon the order relationship between the elements of the first set. The intersection of A and B is the set of elements in A that are closer to B (formally all the elements in the intersection are more similar to B than any other element not in the intersection). And the difference A minus C, corresponding to all the elements in A that are less similar to C (analogously to intersection, all the elements in the different are *less* similar to C tan the elements not in the difference).\nThe framework can be applied to any language models that measure sentence similarity. Experiments are carried out taking baselines as TDIDF, BERT, RoBERTa, Contriever SimCSE, DiffCSE MCSE and SGPT on the AG News, Financial PhraseBank, Banking77 and MTOD datasets.\nBaselines are compared without and with the contrastive training that makes the model aware of set operations, showing an improvement in their perception of these set operations.\nThe work closes with a use case application. The set operations can be used to search related sentences using a set of sentences as positive or negative filtering criteria."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The idea of this paper is really clever, simple and straightforward.\nThe motivation is there, provide a LLM with notions of set theory to improve it in terms of search capabilities. \nExperimentation seems reasonable and enough. Proves the point authors want to provid"
            },
            "weaknesses": {
                "value": "The authors make a good point to show the capabilities brought by the new training regime. However, there is no analysis on capabilities that are lost because of it. Do the models train with this regime underperform on sentence similarity or information retrieval datasets."
            },
            "questions": {
                "value": "How do the models perform on sentence similarity tasks after applying the SetCSE training?\n\nHow about other general NLU tasks such as sentence classification, sequence tagging, extractive QA or multiple choice QA? \n\nThe largest model where this approach was applied was a RoBERTa-like model. Does the approach escalate to bigger models? \n\nCan it be applied while using LoRA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767987036,
        "cdate": 1698767987036,
        "tmdate": 1699636248729,
        "mdate": 1699636248729,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NduXqt9Rba",
        "forum": "zEHGSN8Hy8",
        "replyto": "zEHGSN8Hy8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_3g7o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3036/Reviewer_3g7o"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Set Operations using Contrastive Learning of Sentence Embeddings (SetCSE), a framework that incorporates set theory into semantic search to handle complex and multifaceted queries. Recognizing that intricate semantics often arise from clusters of sentences rather than isolated ones, the authors present a method where sets of sentences collectively represent semantics. The inter-set contrastive learning component is designed to fine-tune language models to capture contextual nuances and discern between different semantic sets. The SetCSE operations - intersection, difference, and series - are employed to structure queries effectively, allowing for a granular and nuanced retrieval of sentences from large corpora. An illustrative use case of analyzing S&P 500 companies\u2019 stances on ESG issues demonstrates the framework's practical utility in complex information retrieval scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Provides a well-defined and practical framework that enables complex information retrieval tasks which are not possible with current search methodologies.\n\n2. Applies contrastive learning to sentence embeddings in a unique way, emphasizing contextual differentiation between sets of sentences.\n\n3. Offers compelling real-world applications, such as parsing nuanced topics like ESG stances from earnings calls, highlighting the framework\u2019s potential for practical deployment."
            },
            "weaknesses": {
                "value": "1. There is no mention of an error analysis which would be beneficial in understanding the limitations of SetCSE in certain scenarios.\n\n2. The applications of complex semantic search, data annotation, and new topic discovery are very cool with the detailed examples, but there is not quantification here or comparison with others with existing set methods from the literature (same with Table 1 and 2 as well). Do you have comparisons with other methods from the literature on this topic?\n\nTypos:\nSection 7 \"DISUCSSION\""
            },
            "questions": {
                "value": "1. Can the authors discuss any observed limitations or frequent error patterns during SetCSE operations?\n\n2. How does SetCSE scale with the size of the dataset and the complexity of the query semantics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3036/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699045901517,
        "cdate": 1699045901517,
        "tmdate": 1699636248654,
        "mdate": 1699636248654,
        "license": "CC BY 4.0",
        "version": 2
    }
]