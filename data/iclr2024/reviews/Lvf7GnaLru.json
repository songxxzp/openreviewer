[
    {
        "id": "C7e5KXjYyK",
        "forum": "Lvf7GnaLru",
        "replyto": "Lvf7GnaLru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_WG9i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_WG9i"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the class of  \u201cdiversification\u201d methods that address OOD problem, and identify the key components contributing to their performance. Some findings are provided through research\uff0cthat can help guide the development of approaches. Some experiments are conducted to support their results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper investigates the \u201cdiversification\u201d methods from theoretically from different aspects. Some key results are provided.\n2) Valid experiments have proved the rationality of their views. \n3) Some showcase are provided to validate the meaning of the findings."
            },
            "weaknesses": {
                "value": "1) What are the unique advantages of \u201cdiversification\u201d methods to solve OOD problems\uff1fDoes it contribute much to OOD community to study the components of \u201cdiversification\u201d? More discussion of motivation and related works are needed.\n2) Despite the limitation talked in paper, I think experimental setup of the paper is still simple. More complicated diversification loss, approaches, need to be included."
            },
            "questions": {
                "value": "In addition to the methods used in the paper, does it need to consider more models to support the conclusions of the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "nan"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Reviewer_WG9i"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746736646,
        "cdate": 1698746736646,
        "tmdate": 1699636550258,
        "mdate": 1699636550258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j2xyF3QlfC",
        "forum": "Lvf7GnaLru",
        "replyto": "Lvf7GnaLru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_MHwE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_MHwE"
        ],
        "content": {
            "summary": {
                "value": "The authors study 2 recently proposed supervised learning \"diversification\" methods (DivDis and D-Bat) which aim to achieve high accuracy out-of-distribution (OOD) by generating a diversity of hypotheses each of which fit the training data but which disagree on additional unlabelled data from a different distribution  and then picking one final hypothesis from the list . The process is intended to reduce the odds of yielding a model which relies on spurious correlations which would not persist under distributional shift.  Through a mix of theory , toy experiments and real-world-data experiments, the authors arrive at a number of findings which warn that neither DivDis nor D-Bat (nor any particular diversification method ) is likely to work well in all situations. The performance of diversification methods is shown to be highly sensitive to the distribution of the unlabelled data on which diversity is measured.  There is an interaction between the learning algorithm and the unlabelled data distribution such that each affects the optimality of the other. The appropriateness of the inductive bias of the learning algorithm is shown to be critical to the success of diversification methods. Increasing the number of hypotheses generated by the diversification methods does not necessarily fix these concerns...the interplay between learning algorithm and unlabelled distribution and the importance of inductive bias persist."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work is original to the best of my knowledge.  \n\nThe paper is well written and clear. I have a few suggested typo-style edits in the weaknesses section, but the writing is certainly strong. \n\nI did not notice any errors or incorrect conclusions in the findings. Although I didn't have time to go through every result in extreme detail, I have a reasonable amount of confidence in the correctness of the results, generally.\n\nGeneralization which is robust to out-of-distribution shifts is certainly a worthy topic."
            },
            "weaknesses": {
                "value": "My main concern with the paper is whether its results are significant enough to merit acceptance at ICLR. I'm open to being persuaded that the paper is significant enough, but that's not clear to me, for a few different reasons.  I offer these concerns with only moderate confidence, since I am not an expert specifically on distributional shift literature.\n\nThe paper focuses largely on the pitfalls and limitations of 2 papers from ICLR 2023, the Lee DivDis and the Pagliardini D-Bat approaches. I don't doubt that these papers are high-quality and significant, but they simply haven't been around long enough to know for sure how significant it is to critique them and scrutinize their flaws. I'm not saying that critiquing them is unworthy- it's just hard to tell whether it's a highly significant contribution. \n\nAlso, although the authors do use real-world data to an extent, I find the applications to still be a bit contrived and not illustrative of a clear real-world situation where distributional shift needs to be handled and cannot be avoided.  The MNIST-CIFAR concatenations and Waterbirds-CC are both contrived, i.e., constructed to have spurious correlations, rather than spurious correlations arising naturally.  Office-Home with Art, Product, Clipart, Real-World is somewhat better, but nonetheless, the authors of that dataset did manage to assemble all four sources and so the sensible thing to do would seem to be to train on a dataset with all 4 sources mixed together. I can see how you could have a real-life situation where e.g. only Art, Product, Clipart are available and then you have to design a system which peforms well on Real-World, but the paper would be stronger if a problem domain was studied where that was a real constraint that made distributional shift unavoidable. The most obvious scenario would be distribution shift over time, e.g. , we had to collect images during sunny weather in the summer and then had to deploy the system in darker weather when it was raining...something along those lines. \n\nMy other significance concern is the findings (while common-sense) seem a bit unsurprising to me. We've  known since the 1990s from the Wolpert No Free Lunch theorems, the bias-variance tradeoff, etc  that the right inductive bias is crucial to supervised learning success, in general. While it's worthwhile to illustrate a version of this principle for these recently-developed diversification methods, the finding that the principle holds seems more like common sense to me than like a surprising finding that advances the field in a major way. Maybe I'm wrong, though, and maybe I\"m expecting too much from a paper worthy of ICLR acceptance.\n\nAnother significance concern I have comes from a reaction I had to a sentence on page 4: \"We, therefore, focus on studying the first stage and assume access to the oracle that chooses the best available hypothesis in the second stage\". While I can see that it's true that the first stage is crucial to the success of the whole diversification approach, as someone who is not that familiar with diversification methods or OOD-robust algorithms, it's not that clear to me that we can assume that the second stage will succeed if the first stage succeeds. I guess the idea is that if K is small, then we only need a modest amount of supervised holdout data to choose from among the K diverse hypotheses generated, e.g. the in-sample bias of picking from among a low-K list using validation data is modest? I suppose that could be true, but I would appreciate more background on that topic. \n\nGiven what I wrote in the previous paragraphs and given the focus on 2 papers just published in 2023, the whole paper leans a little bit too far in the direction of assuming that what the authors are studying is very important rather than making the case for the importance of the topic to someone who isn't already deeply involved in OOD and diversification.\n\nTypos:\n\npage 5 function. And the training distribution -> function, and the training distribution (don't start a new sentence here)\npage 9 \"This is in consistence with\" -> This is consistent with"
            },
            "questions": {
                "value": "Can you add some more background for how OOD concerns can arise in industry? Even though the experiments use real world data in some sense, the tweaking of the real world data to introduce spurious correlations makes the experiments a bit less compelling in terms of real-world evidence. \n\nCan you explain a bit about why I can assume that stage 2 (disambiguation ) probably is doable? I need to be reasonably confident that stage 2 is also doable in order to care about the success of stage 1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Reviewer_MHwE",
                    "ICLR.cc/2024/Conference/Submission5419/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784464623,
        "cdate": 1698784464623,
        "tmdate": 1700683567853,
        "mdate": 1700683567853,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7ZE4TiweU4",
        "forum": "Lvf7GnaLru",
        "replyto": "Lvf7GnaLru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_xY3D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_xY3D"
        ],
        "content": {
            "summary": {
                "value": "The paper critiques recent trend of building diverse models and then selecting one at test time, as a way of enhancing OOD generalization. It presents empirical and theoretical results on why the methods may not always work."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I really like this paper. Given so many methods being proposed for OOD generalization, it is important to take a step back and analyze which ones are likely to work and under what conditions. \n\nThis paper finds that the literature on diversification of hypotheses may not be conceptually well-motivated. The key result is that the success of this technique depends on inductive bias of the model architecture and the same architecture may not work well for different kinds of test set. \n\nIn hindsight, many of the observations seem obvious. for example, given that the methods use 2-3 different hypotheses, they obviously are relying on the training procedure's inductive bias (otherwise how can 2-3 samples explore the full space of \"good\" hypotheses?). But still, the authors do a good job of articulating multiple such issues in a single paper."
            },
            "weaknesses": {
                "value": "While the analysis is compelling, I'm wondering whether these limitations matter in practice. What if we do model selection over multiple architectures and multiple diversity algorithms? Is the risk that the results we get on a cross-validation set may not generalize to the test set?\n\nIf so, what are the summary statistics of the test set that the above procedure would need to know? For example, if the spurious ratio of the (unseen) test set is known, can that be used to simulate a pseudo-test set and then do model selection over it?\n\nOverall, I'm unclear of main takeaway of this paper. Should we not use diversification algorithms? What is the better alternative?\nPersonally, I feel a better message from the paper can be that it identifies the axes on which model selection should be done, if some summary statistics of the test set can be provided in advance."
            },
            "questions": {
                "value": "See the weaknesses above. \nIn particular, can the authors setup a model selection expt where the spurious ratio of the unseen test set is known and the test set is simulated. Would it always lead to the correct model? What else do we need to know about the test set? Can that be summarized, or is that not possible (and hence there is no way to know the best model apriori)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909260704,
        "cdate": 1698909260704,
        "tmdate": 1699636550008,
        "mdate": 1699636550008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T3cpMbGWwJ",
        "forum": "Lvf7GnaLru",
        "replyto": "Lvf7GnaLru",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_shsk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5419/Reviewer_shsk"
        ],
        "content": {
            "summary": {
                "value": "This paper examines two recently proposed algorithms for improving out-of-domain generalization through diverse hypothesis selection with respect to both a labeled and unlabeled dataset. The algorithms try to find hypotheses that agree with the labels on the labeled data but disagree with one another on the unlabeled data.\n\nThe authors essentially find that there is \"no free lunch\" for improving out of domain generalization in that the set of diverse hypotheses selected will depend on which unlabeled data was used to find the diverse hypotheses, which underlying model class was used, and which metric is used to quantify the diverseness of a set of hypotheses.\n\nThe diverse hypothesis selection algorithms examined in the paper are:\n1. diversify and disambiguate aka \"DivDis\" (Lee et al. 2023)\n2. diverisity by disagreement training aka \"DBAT\" (Pagliardini et al., 2023)\n\nDivDis and DBAT use different metrics to assess the diversity of a set of hypotheses.\n\nThe main contributions of the paper are:\n1. Proposition #1 which states that DivDis and DBAT will select different second hypotheses w.r.t. an unlabeled dataset depending on the extent to which the first hypothesis selected (i.e., the ERM) agrees with the true hypothesis on the unlabeled data.\n2. Proposition #2 which states that the number of diverse hypotheses generated by an algorithm like DivDis or DBAT needs to be super-linear in the number of unlabeled datapoints in order to ensure that at least one has greater than 50% accuracy with respect to the true hypothesis.\n3. Experiments on synthetic and real datasets to support propositions #1 and #2, and which also show that the success of a diverse hypothesis generation algorithm jointly depends on the underlying model class and the unlabeled set of data selected."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper really dives into the intricacies of the diverse hypothesis generation problem and does a wonderful job illustrating how complex the problem truly it is; that is, success simultaneously depends on all variables. In my opinion, this message should be communicated more frequently in conference proceedings.\n\nIn particular, proposition #1 is quite illuminating in that it shows how DivDis and DBAT select different diverse hypotheses from one another, and there are different regimes defined in terms of the agreement with the true hypothesis on the unlabeled data where each is superior. And here, it was nice to see how the experiments on the synthetically constructed datasets (e.g., MNIST/CIFAR) supported the theoretical findings."
            },
            "weaknesses": {
                "value": "This paper has a number of weaknesses. I found the presentation more confusing and dense than it could be:\n\n  1. In particular, there is some terminology and notation that can be improved for greater understanding. The \"spurious ratio\" index is a poorly named quantity because it's literally the accuracy of the selected hypothesis with respect to the true hypothesis h*. Namely, h* has the maximum spurious ratio value of 1.0, but it's definitely NOT spurious as it's the true hypotheis. Another name, like the \"agreement ratio\" would be much clearer.\n\n  2. Similarly, the plots on the left side of figure 2 do not seem to agree with the description of the synthetic problem in the first paragraph of section 4.1, which made it very hard to understand what was being communicated (I elaborate on my confusion below).\n\nAnd though a good message to repeat, the no-free-lunch findings discussed in the paper are known in the supervised learning setting, so they definitely need to hold for this harder setting with labeled + unlabeled data. Could this research direction become even more constructive by making certain statistical assumptions? In the paper, D_u can be any \"out of distrubtion\" distribution; and the capacity of the learning algorithm is assumed to be infinite in proposition 2, even though in other parts of the paper, inductive biases of different model classes are highlighted (which suggests that there the model class has less than infinite capacity in practice).\n\nThe results for DivDis in table 1, section 5.2 for increasing K are not convincing because alpha needs to increase as the square of the number of hypotheses in the set (i.e., O(K^2)). Otherwise the regularization becomes too strong and dwarfs the empirical risk. But the text makes it seem like a fixed alpha was used, which would needlessly harm DivDis' performance as K gets larger."
            },
            "questions": {
                "value": "Regarding figure 2, h* is defined in the text such that instances with x1 > 0 should be labeled as positive examples. I assume that x1 is the horizontal axis such that points to the right are positive. But the plot and its legend shows points to the left are positive (Class 1). Is my interpretation correct here?\n\nAssuming it is correct, hyperplanes are typically defined by their normal, in which case is should be that h*(x) = h(x; 0), not h*(x) = h(x; pi/4), since theta=0 radians points in the positive horizontal direction to the right, whereas theta=pi/4 radians points up to the top of the page."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5419/Reviewer_shsk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5419/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699051981537,
        "cdate": 1699051981537,
        "tmdate": 1700719438479,
        "mdate": 1700719438479,
        "license": "CC BY 4.0",
        "version": 2
    }
]