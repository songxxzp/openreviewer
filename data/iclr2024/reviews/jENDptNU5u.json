[
    {
        "id": "obgVmWbn4q",
        "forum": "jENDptNU5u",
        "replyto": "jENDptNU5u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_tHfv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_tHfv"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to edit an image by synthesizing an object-interactive human in the image. To do this, their framework consists of two stages: the first stage generates object-interactive skeleton using diffusion-based module, while the second stage outputs a Human and Object Interaction (HOI) image based on skeleton and text guidance. However, their only contribution is to train a network that predicts skeleton based on the diffusion model."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Good writing. This paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "- Limited innovation and contribution. The only contribution of this paper is to train a network that predicts skeleton based on the diffusion model, which is not novel.\n- Insufficient experiments. Lack of comparison with HumanSD [1].\n\n  [1] Ju, Xuan, et al. \u2018HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation\u2019. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 15988\u201315998.\n\n- Incorrect statements and unfair comparisons. For example, there are ablation experiments on diffusion model vs. ResNet101 that do not provide comparisons of parameters and inference times, which is clearly unfair; also, both quantitative and qualitative experiments in comparisons with other methods illustrate precisely the power of ControlNet rather than this method.\n\nTherefore, the paper is not at all up to the standard of ICLR and I recommend the authors to choose another conference."
            },
            "questions": {
                "value": "As HumanSD found out, ControlNet still has all four types of problems, and you only generate conditional inputs for ControlNet-Inpainting, so why do you dare to claim that you have solved all four types of problems? Besides, it has nothing to do with your contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2394/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2394/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2394/Reviewer_tHfv"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698582748843,
        "cdate": 1698582748843,
        "tmdate": 1699636174764,
        "mdate": 1699636174764,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zc0HJjzu91",
        "forum": "jENDptNU5u",
        "replyto": "jENDptNU5u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_rWaG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_rWaG"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the first method for the generation of Human-Object-Interaction (HOI) images. Specifically, they introduce EditHOI composed of two stages. In the first stage, they design a diffusion-based module to generate an object-interactive skeleton based on the pre-defined bounding box. Next, the authors will have the second stage to generate the HOI images based on skeleton and text prompts. Beyond the methods, the paper provides two novel evaluation metrics for better evaluations. Both qualitative and quantitative results showcase its capability to successfully generate HOI images with reasonable human actions, surpassing the existing state-of-the-arts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strengths of the proposed paper can be summarized as:\n1. The authors propose a novel skeleton and text-guided local editing framework that can generate Human-Object-Interaction images based on a single input text prompt.\n2. Both qualitative and quantitative evaluations demonstrate the capability and superiority of the proposed method."
            },
            "weaknesses": {
                "value": "The weaknesses of the proposed paper can be summarized as:\n1. The qualitative results presented in Figure 1 and 3 are not obvious and good enough, although it indeed surpass the existing methods.\n2. There is no visualization for the generated skeleton and analysis on this part. More visualization and explanation would be beneficial.\n3. What the results will look like if we remove the bounding-box conditions?"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772895283,
        "cdate": 1698772895283,
        "tmdate": 1699636174658,
        "mdate": 1699636174658,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H7VSFHdYcB",
        "forum": "jENDptNU5u",
        "replyto": "jENDptNU5u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_jZPv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_jZPv"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"EditHOI,\" a novel framework for Human and Object Interaction (HOI) image editing, addressing the complex and ill-posed nature of image editing tasks. EditHOI stands out in its ability to synthesize object-interactive humans into images, a task that has been problematic for existing image editing models due to issues like the flawed generation of humans, absence of interaction, and incomplete multi-human generation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "EditHOI introduces a novel approach to the task of HOI image editing, an area that has not been extensively explored. The framework's two-stage process, which involves generating an object-interactive skeleton and using it for image inpainting, is a creative solution to the problem of synthesizing object-interacting humans in images. This represents an inventive combination of diffusion-based models and text guidance for image editing.\n\nThe framework is methodologically sound, employing a diffusion-based module for skeleton generation that can handle diverse and unstructured in-the-wild images. The use of text prompts in the second stage to guide the image synthesis process demonstrates a thoughtful integration of multimodal inputs (skeletal structure and textual description) for image editing."
            },
            "weaknesses": {
                "value": "Technical contribution: the technical contribution of this work is limited. The main part involves generating an object-interactive skeleton with diffusion models and using it for image inpainting. \n\nScalability: It aims to handle a very specific editing task, i.e., Human and Object Interaction(HOI) image editing. The narrow scope of this topic significantly limits its generalization across different tasks.\n\nFailure Analysis: While the paper discusses the framework's success in overcoming certain problems in HOI image editing, a detailed analysis of failure cases would be beneficial."
            },
            "questions": {
                "value": "see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777902797,
        "cdate": 1698777902797,
        "tmdate": 1699636174586,
        "mdate": 1699636174586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bVqXHGbWBz",
        "forum": "jENDptNU5u",
        "replyto": "jENDptNU5u",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_BnY7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2394/Reviewer_BnY7"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to edit an image by synthesizing an object-interaction human in the image, which first generates skeleton using the diffusion-based module, and second outputs a human and object interaction image based on skeleton and text guidance"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper proposes a technical pipeline dedicated to addressing the issue of poor performance of HOI in the community of text-to-image."
            },
            "weaknesses": {
                "value": "The proposed solution appears technically rigorous, yet lacks novelty. \n- The resolution to the four issues raised by the authors: absence of human, incompleteness of human, absence and incompleteness of interaction, and incomplete multi-person generation problem, seems to be significantly attributed to ControlNet. \n- The skeleton generation network based on the diffusion model lacks innovative elements. Utilizing various embeddings to guide diffusion models is a common technique, such as the employment of text embedding, time embedding, and image embedding to guide the text-to-image model."
            },
            "questions": {
                "value": "- Generating HOI is a challenging task. Is there a more fundamental solution to enhance the Text-to-image model\u2019s ability to generate interaction, rather than simply combining existing algorithmic techniques, such as ControlNet and SD-Inpainting?\n- Control generation based on skeletons is a common solution for images with high dependence on structure and layout, typically involving initial skeleton generation followed by image generation. Given that skeletons can be correctly generated based on interactive objects, why can\u2019t the subject be directly generated from interactive objects? Is it feasible to not use skeletons as an intermediate bridge? It would be beneficial if this could be elucidated through experimentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2394/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699337841703,
        "cdate": 1699337841703,
        "tmdate": 1699636174521,
        "mdate": 1699636174521,
        "license": "CC BY 4.0",
        "version": 2
    }
]