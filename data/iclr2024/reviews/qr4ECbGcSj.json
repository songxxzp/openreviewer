[
    {
        "id": "xYBLnORF5j",
        "forum": "qr4ECbGcSj",
        "replyto": "qr4ECbGcSj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_EiAB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_EiAB"
        ],
        "content": {
            "summary": {
                "value": "The paper aims to provide a comprehensive picture of the expressivity of a pletora of objective specifications in RL, which include reward machines, trajectory feedbacks, multi-objective formulations among the others. The expressive power of an objective specification is measured in terms of the orderings it can induce over stationary Markovian policies in every possible environment, which is formally defined through a set of states, a set of actions, a transition kernel, and an initial state distribution."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- (Significance) The paper tackles a very important question on how to specify the objective in RL. Given the recent results showing the limitations of Markovian rewards (e.g., Abel et al., 2022) and the growing popularity of alternative feedback models, such as RL from human feedback, regularized RL, and convex RL to name a few, I think this is a very relevant research line.\n- (Originality) I am not aware of previous effort in exploring the expressivity of such a comprehensive set of objective specifications.\n- (Limitations) The paper is upfront in reporting the limitations of the contribution (see Section 4.1)."
            },
            "weaknesses": {
                "value": "- (Arbitrary analysis) The works makes a series of arbitrary choices in defining the expressivity of an objective specification (e.g., limiting the analysis to ordering of stationary policies) that are not sufficiently motivated in my opinion.\n- (Implications) While the paper provide lots of results in terms of expressivity (as defined in the paper) of the objective specifications, most of the implications of their analysis are left as future work.\n- (Related work) Given the nature of the work, which is in part a survey of objective specifications in RL, the discussion of the related literature is insufficient.\n- (Clarity) While the main messages of the paper are clear and easily accessible, some notation choices are somewhat confusing, and they also depart the objective specifications from how they are formulated in the literature in some cases.\n\nGENERAL COMMENT\n\nThe paper tackles a very important research problem, which can be coarsely rephrased as \"how can we specify the objective in RL to obtain the behaviour that we want as output of the learning process\"? However, it falls somewhat short of answering this questions, providing an analysis of a set of objective specifications in a very limited setting (full ordering of stationary policies), and without addressing the implications of their results. Some of the reported results can still be valuable, although it is not fully clear what is novel and what is already known, but I believe that a better motivation of the choices on how to evaluate expressivity is necessary to clear the bar for acceptance. Thus, I am currently providing a negative evaluation, but I will consider updating my score if the authors can convince me on why comparing objective specifications in terms of full ordering of stationary policies matters.\n\nDEFINITION OF EXPRESSIVITY\n\n(C1. Policy ordering) The paper makes clear that the expressivity of the objective specifications is evaluated only in terms of policy ordering, but I am wondering whether this is the most natural notion of task, and how can be motivated in practice. Arguably, in most of the real-world problems, what really matters is the outcome of the learning process, i.e., what policy is ultimately learned. In this view, the SOAP looks more natural (terminology from Abel et al., 2022). I think that a soft-SOAP task notion would make for a stronger case in the expressivity evaluation. Anyway, the choice of task notion has a strong impact on the expressivity overview and it shall be motivated carefully in my opinion.\n\n(C2. Stationary policies) Another weak point I see in the analysis is restricting the policy class to stationary Markovian policies. It is well-known that a Markovian reward is expressive enough to specify objectives over deterministic stationary Markovian policies, which leaves only the set of stochastic policies to the other specifications. I am wondering why one would want a stochastic policy in the first place. Typical motivations are robustness to model misspecifications and adversaries, or perhaps to make the learning problem more efficient (e.g., regularized objectives). For some of the reported objective specifications, I think that the main selling point is that they cannot be optimized by Markovian policies. For instance, FTR allows to extract optimal history-dependent policies, which can be valuable, although in the analysis turns out to be less expressive than FOMR, which does not look that useful.\n\nIMPLICATIONS\n\n(C3. Expressivity against tractability) Whereas the Section 4.1 clarifies that tractability is not considered in the analysis, this may defeat the purpose of the analysis itself. Very general models have been proposed in the past, but they intractability makes them less appealing for practitioners. For instance, the POMDP model arguably subsumes all of the objective specifications reported in the paper, but it is intractable in general.\n\nRELATED WORK\n\n(C4. Missing references) This paper is surveying a lot of objective specifications but does little to relate those with pre-existing literature. Various previous works considered extension to the standard RL model (based on Markovian rewards), such as:\n- RL with general utilities (e.g., Zhang et al., Variational policy gradient method for reinforcement learning with general utilities, 2020)\n- Convex RL (e.g., Hazan et al., Provably efficient maximum entropy exploration, 2019; Zahavy et al., Reward is enough for convex MDPs, 2021; Geist et al., Concave utility reinforcement learning: the mean-field game viewpoint, 2021; Mutti et al., Challenging common assumptions in convex reinforcement learning, 2022; Mutti et al., Convex reinforcement learning in finite trials, 2023)\n- Vectorial rewards (e.g., Cheung, Regret minimization for reinforcement learning with vectorial feedback and complex objectives, 2019)\n- Trajectory feedback (e.g., Chatterji et al., On the theory of reinforcement learning with once-per-episode feedback, 2021)\n- Duelling feedback (e.g., Saha et al., Dueling RL: Reinforcement learning with trajectory preferences, 2023)\n- Preference feedback (e.g., Xu et al., Preference-based reinforcement learning with finite-time guarantees, 2020)"
            },
            "questions": {
                "value": "- Can the authors address the comment reported above?\n\n- Can the authors clarify which of the reported result is new and not directly implied by previous works?\n\n- How can be OMO more expressive than FOMR? I am probably missing something, but why we cannot take for any ordering over occupancy measures $m_1 > m_2 > ... > m_n$ a function $f: M \\to \\mathbb{R}$ such that $f(m_1) > f (m_2) > ... > f (m_3)$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6003/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698352810379,
        "cdate": 1698352810379,
        "tmdate": 1699636643537,
        "mdate": 1699636643537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9VxrF14nyj",
        "forum": "qr4ECbGcSj",
        "replyto": "qr4ECbGcSj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_Re36"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_Re36"
        ],
        "content": {
            "summary": {
                "value": "This paper surveys a wide range of objective-specification formalisms in reinforcement learning setting and establishes the theoretical expressive relationship among these formalisms. These results facilitate the choice of objective formalism for RL practitioners."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work connects 17 objective-specification formalisms, compares their expressivity, and present results using a Hasse diagram. This contribution is clear.\n2. The discussion and review of each formalism is helpful for RL practitioners to choose specifications."
            },
            "weaknesses": {
                "value": "1. There are some other formal language-based specification formalisms for RL, such as Signal Temporal Logic, which is more powerful than LTL because of its robustness property, and there are a wide range of literature in this direction, see e.g., [1], [2]. The authors are encouraged to study STL formalism with others to make the comparison more thorough.\n\n2. As mentioned in Sec. 4.1, there is no tractability comparison among various formalisms, which is however very important for researchers/practitioners to decide which one they want to use.\n\n3. The writing can be further improved. For instance, some notations can be be more precise (e.g., in Def. 2.7, vector in R^|S||A||S| should be replaced by R^|S| \\times R^|A| \\times R^|S|).\n\n[1] Balakrishnan, A. and Deshmukh, J.V., 2019, November. Structured reward shaping using signal temporal logic specifications. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 3481-3486). IEEE.\n[2] Wang, J., Yang, S., An, Z., Han, S., Zhang, Z., Mangharam, R., Ma, M. and Miao, F., 2023. Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications. arXiv preprint arXiv:2306.06808."
            },
            "questions": {
                "value": "I have no question.`"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6003/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6003/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6003/Reviewer_Re36"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6003/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624506424,
        "cdate": 1698624506424,
        "tmdate": 1700432486212,
        "mdate": 1700432486212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vkbF37UtNF",
        "forum": "qr4ECbGcSj",
        "replyto": "qr4ECbGcSj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_XNit"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_XNit"
        ],
        "content": {
            "summary": {
                "value": "This paper compares the relative expressivity of different task specification mechanisms for reinforcement learning agents. The authors set out to compare the relative expressivity of Markov rewards, limit average rewards, reward machines, linear temporal logic, regularized RL, and outer non-linear Markov rewards, and prove that each of these formalisms have a non-zero intersection, but also a non-zero exclusive component. In doing so the authors define 17 different task specification formalisms, and posit that a complete ordering over all possible policies of an agent is the most expressive (if impractical) tasks specification mechanism available for RL agents. \n\nThe authors primarily provide expressivity result as a Hasse diagram where a directed edge represents subsumption of the task specification formalism."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. **Timely and major significance**: The authors assertion that each of the popular reward mechanisms do not completely express all aspects of other reward formalisms is an important result, and the conclusions are supported by extensive background materials, and proofs. I also believe that the recommendation of the authors that researchers be more aware of reward specification frameworks, and algorithms catering to them is also well received. \n\n2. **Comprehensive survey of reward mechanisms**: The authors were systematic in their coverage of task specification formalisms, and have exhaustively documented each pairwise dependency. Although for presentational clarity, the proofs had to be relegated to the supplementary materials. The paper seems well indexed."
            },
            "weaknesses": {
                "value": "1. The authors center the discussion around ordering of stationary policies, and while it is justified in the context of the paper, and where the research effort has been directed historically, there are many instances (especially with partial observability), where non-stationary policy over observations must be implemented."
            },
            "questions": {
                "value": "1. I am curious if the authors only considered a fragment of LTL, or the entirety of LTL that allows for accpetability definitions over infinitely long trajectories\n2. I am also uncertain if ordering over stationary policies is enough to describe an LTL task that requires a memory dependent policy (usually implemented through a cross product MDP)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6003/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729571632,
        "cdate": 1698729571632,
        "tmdate": 1699636643250,
        "mdate": 1699636643250,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jVf3IiYPvA",
        "forum": "qr4ECbGcSj",
        "replyto": "qr4ECbGcSj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_mz4B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6003/Reviewer_mz4B"
        ],
        "content": {
            "summary": {
                "value": "Um. Wow.\n\nThis paper is remarkable in many ways. The paper contributes a comprehensive set of proofs designed to show which reward formalisms in RL are distinctive and which can be subsumed by others; the central contribution of the paper is a lattice that diagrams these relationships. One conclusion from the work is that many formalisms offer distinctive capabilities, and that no single formalism is dominant.\n\nThe paper really does seem to be an incredible, detailed set of work. It is is highly theoretical, and is likely to be useful only to a select subset of researchers who are deeply steeped in RL research.\n\nI am not capable of evaluating the proofs in any sort of detail, even though I have spent many years in RL. However, the way that the paper is written, combined with the parts that I was able to evaluate, suggests a serious contribution to the literature.\n\nThe question is: is ICLR the right venue for this work? While this is probably the magnum opus of this sort of work, it's unclear how much the general ICLR community would benefit from including it in the conference proceedings.\n\nWould a journal be a better outlet?"
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ Comprehensive evaluation\n+ Clearly summarized results\n+ Interesting resulting insights\n+ Well-written, despite extensive technical detail"
            },
            "weaknesses": {
                "value": "- Likely to be useful to a very small subset of people\n- Unclear if ICLR is the right venue\n- Dense notation / writing"
            },
            "questions": {
                "value": "Why do you think that ICLR is the right venue for this work, as opposed to an RL-specific conference or a journal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6003/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811965619,
        "cdate": 1698811965619,
        "tmdate": 1699636643116,
        "mdate": 1699636643116,
        "license": "CC BY 4.0",
        "version": 2
    }
]