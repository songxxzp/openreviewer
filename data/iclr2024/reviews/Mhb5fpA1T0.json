[
    {
        "id": "b1Gl2d3GzV",
        "forum": "Mhb5fpA1T0",
        "replyto": "Mhb5fpA1T0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_Hwk5"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method called Actions from Video Dense Correspondences (AVDC) that learns to perform robotic tasks without ever accessing labels of robot actions. The method first trains a text-conditioned diffusion video prediction model on videos of robot data. It then computes optical flows for adjacent frames of the prediction, and by segmenting out the object of interest and finding a rigid body transformation that best fits the corresponding points, computes an object (manipulation) or robot (navigation) transformation that enables the robot to follow the plan. Experiments are conducted on several environments: simulated meta-world and iTHOR tasks, visual pusher for evaluating learning from a human embodiment, and bridge data/a real Panda arm."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Learning from action-free videos is an important and challenging problem, which could enable data-driven robotics to access scale through sources like Youtube.\n- The proposed method makes sensible design decisions, including the video model and correspondence computation strategy. \n- The evaluation is very thorough in terms of the number of environments considered, and AVDC appears to yield consistent performance gains.\n- The authors have committed to making their video model, which requires significantly fewer resources than prior works, open source. I think this is also a valuable contribution (although less related to the main message of the work).\n- The presentation is quite good, the writing is clear, and main ideas are communicated directly."
            },
            "weaknesses": {
                "value": "- The most apparent weakness of this work is that not all tasks that a robot might want to solve can be solved by a trajectory of target object poses. For example, it is unclear how to plan a task that would require the robot to use another object as a tool. It is also unclear how deformable objects could be handled. There also may be tasks such as pressing a button on a microwave, which do not involve the microwave moving, but require a particular amount of force to be applied, for which this may not be applicable.\n- The work \u201cZero-Shot Robot Manipulation from Passive Human Videos\u201d by Bharadhwaj et al. presents very similar (although not identical) ideas and is not discussed in the prior work or cited. Could the authors discuss and ideally perform a comparison to some of the ideas in that work?\n- Related to the previous point, the ideas presented in this paper are not entirely novel. However, I believe that this particular combination/instantiation of them, as well as the evaluation and exploration of them that is provided, is a valuable contribution to the community."
            },
            "questions": {
                "value": "- The video generation performance is seemingly quite good even when very few training trajectories are provided (just 165 videos for Meta-World). Can you comment about how overfitting can be avoided or provide some intuition?\n- The performance of the UniPi baseline is surprisingly poor. Could you please provide an explanation for the common failure modes or visualizations? Same goes for the BC baselines. Is this due to the low amount of data provided, thus causing action prediction models to overfit? If so, would it be possible to report results with greater number of demonstrations (like 50, or even 15, rather than 5)?\n- Are the camera poses that are used for evaluating the policies the same as the ones in the training data? I assume that they are but it would be good to have confirmation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736029395,
        "cdate": 1698736029395,
        "tmdate": 1699636067198,
        "mdate": 1699636067198,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SwtG9uXKJ6",
        "forum": "Mhb5fpA1T0",
        "replyto": "Mhb5fpA1T0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_itAQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_itAQ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method for constructing a video-based robot policy, capable of performing diverse tasks across different environments. This approach doesn't require action annotations but uses images for a task-agnostic representation. Text is employed for specifying robot goals. By synthesizing videos to predict robot actions and employing dense correspondences between frames, the model infers actions without explicit training labels. It can leverage the large-scale RGB videos on the internet for training, and use this knowledge for robotic manipulation. The paper showcases the effectiveness of this approach in tabletop manipulation and navigation tasks and also provides an open-source framework for efficient video modeling."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposed a new correspondence based method to obtain robot action in forecasted robot videos. It proves that a latent dynamic model is not needed if the forecasted video has good quality.\n2. The authors proposed a new method to generate future videos using a diffusion model, which achieves efficient training. It provides a promising toolbox for the community.\n3. The method is evaluated on two tasks, table-top manipulaion and in-door navigation, demonstrating its effectiveness in different domains.\n4. The paper is well-written and solid."
            },
            "weaknesses": {
                "value": "1. The selected robot tasks are relatively toy, and the potential of such kind of video prediction method is not evaluated. However, this is not the weakness of this paper, but a common practice for video prediction based robot control."
            },
            "questions": {
                "value": "1. In the appendix H.1.2, the authors say \"We calculate such direction by extrapolating the line between the grasp point and the\nfirst subgoal more than 10cm away from the grasp\" Should it be push point?\n2. In Sec. H.1.3, \"For the push mode, we re-initialize the gripper as described above \" is not clear. What does the re-initialization refer to?\n3. When learning the diffusion model for the IThor environment, why not apply the adaptable frame sampling technique  in this case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820594210,
        "cdate": 1698820594210,
        "tmdate": 1699636067126,
        "mdate": 1699636067126,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WWTeOBq2O3",
        "forum": "Mhb5fpA1T0",
        "replyto": "Mhb5fpA1T0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach for learning video-based policies in robot manipulation settings. The key benefit of the approach is training on actionless video data across human and robot embodiments. The method, termed Actions from Video Dense Correspondences (AVDC), consists of three stages: (1) diffusion-based video prediction given a text-based goal and starting image, (2) optical flow prediction creating dense correspondences, and (3) executed on a robot platform using off-the-shelf inverse kinematics and motion planners. AVDC uses the ability to project a 3D point onto the image plane both from depth and optical flow to compute the transformation of rigid objects across the predicted video frames. These transformations allow AVDC to infer actions in the environment. Then off-the-shelf robotics primitives can be used to enact the planned trajectory. The approach is benchmarked on the Meta-World and iTHOR simulation platforms and on a real-world robot platform, outperforming the considered baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "* The general problem of making use of actionless human video data is of interest and importance to the research community.\n* The problem is well-motivated and the literature review does a good job of contextualizing the paper in prior work.\n* The paper is strong, well-written and easy to follow.\n* The use of geometry to reconstruct the transformation of the predicted objects (stationary camera) or embodiment (moving camera) which can be derived simultaneously from the optical flow and depth camera during deployment is clever. This allows the training data for the video prediction and optical flow not to require depth, with depth only being necessary during deployment. The transformations of either the objects or the embodiment then can be used in conjunction with off-the-shelf inverse kinematics, motion planners, grasp point predictors, etc. This also allows for learning from human videos and then zero-shot deploying to the robot, which is very impressive.\n* The figures are informative and effectively illustrate the benefits of the proposed approach.\n* The experiments consider both simulation and real robot evaluation, as well as an ablation study, demonstrating AVDC's superior performance as compared to the considered baselines and support for AVDC's design choices. In particular, I appreciated the discussion and later the results for why not to directly predict the optical flow without the intermediate step of video prediction.\n* The discussion did a good job of describing the weaknesses and failure modes of the proposed method."
            },
            "weaknesses": {
                "value": "* The literature review is missing a number of relevant works.\n  * V-PTR: similar high-level motivation of using video-based, prediction-focused pre-training and then action-based finetuning. This should have likely served as a baseline for the proposed method. \n  * [A] Bhateja, Chethan, et al. \"Robotic Offline RL from Internet Videos via Value-Function Pre-Training.\" arXiv preprint arXiv:2309.13041 (2023).\n  * Diffusion policy: diffusion policy has shown very good results in terms of multi-task, low-data regime performance. \n    * [B] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" arXiv preprint arXiv:2303.04137 (2023). \n    * [C] Ha, Huy, Pete Florence, and Shuran Song. \"Scaling up and distilling down: Language-guided robot skill acquisition.\" arXiv preprint arXiv:2307.14535 (2023).\n* In particular, my biggest concern with the paper is the lack of comparison to a strong BC baseline. As the AVDC method uses diffusion to predict images, it seems natural to baseline against a diffusion policy (e.g., [B, C]). R3M is a fairly old representation at this point (e.g., Voltron [D] would be a better representation) and the simple MLP-based BC policy would strongly underperform diffusion policy. This is confirmed by, for example, the very poor baseline performance in Tables 1 and 2. In Sec. 4.3, it is mentioned that since 'R3M is pretrained on robot manipulation tasks ... it might not be suitable for visual navigation tasks'. Something like [E] could be a better baseline here.\n  * [D] Karamcheti, Siddharth, et al. \"Language-driven representation learning for robotics.\" arXiv preprint arXiv:2302.12766 (2023).\n  * [E] Shah, Dhruv, et al. \"ViNT: A Foundation Model for Visual Navigation.\" arXiv preprint arXiv:2306.14846 (2023).\n* In Sec. 4.5 'Results', the paper states that Fig. 10 presents screenshots of robot trajectories, but I believe that is Fig. 9? Fig. 10 shows human predicted trajectories.\n\nSome typos and points of confusion are listed below:\n1. Page 3 - 'Unipi'.\n2. Sec. 4.1:  'compare AVDC to its [variants] that also predict dense correspondence'.\n3. Sec. 4.2: 'maximum number of planning affects' -> 'maximum number of replanning steps affects'.\n\n**Post-rebuttal: Most of my concerns have been addressed! I am raising my score as such."
            },
            "questions": {
                "value": "1. In the related work, you mention that RL based methods often have to interact with the environment. However, offline RL-based methods avoid this issue (e.g., [A]). What is the downside of such approaches compared to the proposed method?\n2. Was the choice of the factorized spatial-temporal ResNet block ablated?\n3. I did not quite understand in Sec. 3.3 'Predict object-centric motion', what happens to achieve subsequent subgoals after the first grasp-contact point is reached. Do you pick the next one in the subsequent predicted video frame?\n4. In the replanning strategy, why would a smaller robot movement necessarily be indicative of failure? What if the inaccuracy in compounding error results in large, but inaccurate robot movements?\n5. Is there a reason not to use a receding horizon-style replanning strategy as in [B]?\n6. Do you have a sense as to why AVDC (Full) underperformed in the 'btn-press-top' task in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_cxfp"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1395/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698985521342,
        "cdate": 1698985521342,
        "tmdate": 1700500130830,
        "mdate": 1700500130830,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pk855MWulF",
        "forum": "Mhb5fpA1T0",
        "replyto": "Mhb5fpA1T0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to learn robot policies from action-free video data. The motivation is that there exists a lot of video data, but very little action data. Video prediction methods are often over-dependent on actions but have the benefit of being task agnostic. AVDC aims to solve this challenge by learning a video generation model (via diffusion) on the robot video data. From the generated sequence of images, the optical flow is estimated, which conditioned on some 3D knowledge as well as masks/segmentations of different objects gives an idea of the actions that are taken. The robot actions are then taken. To avoid accumulating errors, AVDC allows for replanning after a few actions. The approach is tested on manipulation (Meta-World) and navigation (iThor) setups, as well as qualitative results on a cross-embodiment visual pusher dataset and some robot arm data. Experiments and ablations find that (1) AVDC outperforms inverse dynamics+video prediction and BC baselines (2) all individual components are important."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper tackles an important problem of learning from action-free videos \n- The method, to my knowledge is novel\n- The approach significantly outperforms baselines on many different tasks \n- The ablations are well analyzed \n- The paper is easy to follow and well written"
            },
            "weaknesses": {
                "value": "- I think one of the main limitations is the setting: AVDC needs videos of robots performing the task. I believe this is a contrived setting as it is very likely that if video + 3D information is available, then this was a robot demonstration, and one can just collect action data. To me, it is unclear how this approach will scale beyond robot data. \n\n- I am concerned by the reported results for the BC baseline. Due to action data being available, as well as the robot data being in-domain for the task a simple BC or kNN baseline should work very well. There are many cases where the results are < 5% success. This should be addressed. I would be willing to increase my score if this weakness is addressed. \n\n- AVDC relies on object/robot masks - a simple baseline would be to use those as a proxy for the actions. One could get pseudo action labels from the videos and train a policy. \n\n- AVDC assumes that all objects are going to be directly manipulated by the robot directly but this is not the when one object as a tool. \n\n- Navigation approaches have many action free baselines which should be explored as well\n\n- It would be good to see real world experiments\n\n- It would be good to have more of an analysis on the quality of the video prediction model. I suspect it has a"
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1395/Reviewer_QCWJ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1395/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699642738055,
        "cdate": 1699642738055,
        "tmdate": 1700675180031,
        "mdate": 1700675180031,
        "license": "CC BY 4.0",
        "version": 2
    }
]