[
    {
        "id": "0B8IUhQ6eH",
        "forum": "KJYIgEteHX",
        "replyto": "KJYIgEteHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_Eeae"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_Eeae"
        ],
        "content": {
            "summary": {
                "value": "This manuscript discusses the distributional robustness of deep learning based MRI reconstruction (solving an ill-posed inverse problem and recovering an underlying sub-Nyquist sampled image). The authors experimented with U-Net-based MRI reconstruction under multiple subtypes of distribution shifts and analyzed their effects on the performance. The authors also argue that more diverse data leads to more robust models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem of identifying and mitigating distributional shifts for deep learning accelerated MRI is of significant real-world relevance. It is critical for building a trustworthy deep learning driven MRI reconstruction system. \n\nThe experiments are performed on a large range of MRI reconstruction datasets with multiple real-world types of distributional shifts (imbalanced data, anatomical shifts, diverse magnetic fields, images from healthy subjects or from patient with health conditions, etc.)."
            },
            "weaknesses": {
                "value": "The scientific contributions of the manuscript is limited: despite the detailed analysis and discussion, the distributional robustness of deep learning MRI reconstruction has been discussed by a series of prior works [1-6]. Despite more detailed experiments (imbalanced data and healthy versus disease images), the major conclusions do not go beyond those of early works [1-3]. The authors failed to make significant theoretical and methodological contributions either (while most of [1-6] proposed either theoretical insights and/or methodological contributions). \n\nThe writing needs improvement: The paper is poorly structured, and it does not follow quite well the conventions of ICLR. It is difficult to identify the key arguments and contributions from the text. It is also difficult to grasp the chain of arguments and evidences.\n\nSec. 2: There is a lack of a brief introduction of essential key concepts: coils and sensitivity maps, sampling masks and accelerations, the signal-processing interpretation of MRI acquisition, problem settings for MRI reconstruction, etc. Missing these key concepts would bring difficulties for readers who are not familiar with MRI reconstruction.\n\nThe choice of using a U-Net is over-simplistic, given that the mainstream reconstructions works are based on unrolled proximal gradients with deep cascade networks, variational networks, as well as probabilistic diffusion models, which may also bring stronger distributional robustness due to better inductive biases compared with a plain U-Net. \n\n[1] https://onlinelibrary.wiley.com/doi/10.1002/mrm.27355 \n[2] https://arxiv.org/abs/1902.10815 \n[3] https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28148 \n[4] https://arxiv.org/pdf/2011.00070.pdf \n[5] https://link.springer.com/chapter/10.1007/978-3-030-87231-1_21 \n[6] https://www.sciencedirect.com/science/article/pii/S0730725X21002526"
            },
            "questions": {
                "value": "The authors are encouraged to improve the clarity of the paper: talking about the problem background, existing works and their drawbacks, then the key contributions, in the introduction section. Then, in the following sections, the authors are encouraged to make their arguments clear, and then demonstrate how the experiments support their arguments. \n\nThe authors are also encouraged to bring more theoretical insight behind the observational results, given that distributional shifts in MRI are not a newly identified problem. \n\nThe authors are also encouraged to take the effect of different reconstruction technique into consideration: despite diverse implementation details, these methods can be generally categorized as 1. plain feed-forward networks; 2. unrolled cascaded networks; 3. variational networks, as well as 4. probabilistic diffusion models. The authors may want to consider the effects of inductive biases on distributional robustness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This is a retrospective study based on public datasets. The authors need to adhere to terms and conditions specified by their owners."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8202/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8202/Reviewer_Eeae",
                    "ICLR.cc/2024/Conference/Submission8202/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698425740768,
        "cdate": 1698425740768,
        "tmdate": 1700600585068,
        "mdate": 1700600585068,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4vpUd5vndG",
        "forum": "KJYIgEteHX",
        "replyto": "KJYIgEteHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_JsA6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_JsA6"
        ],
        "content": {
            "summary": {
                "value": "The paper examines the effect of diverse training data on the performance of MRI reconstruction models. To perform this experiment, the paper considers a wide suite of datasets with fully-sampled raw data, including fastMRI, Stanford 3D, the 7T database, CC-359, and others. For most of these datasets, a U-Net is trained and then evaluated on data that would be considered out-of-domain from the training distribution. The paper has a series of conclusions based on empirical evidence:\n\n1. Having separate models for each distribution is not better than having one model for all distributions. This includes skewed data situations.\n2. Data diversity improves robustness to distribution shift.\n3. Pathology can be reconstructed from healthy subjects.\n4. Hold-out-sets with out-of-domain data can be used to assess overfitting.\n5. A model trained on all data is most robust."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is one of the largest in terms of experiments on the effects of data for MRI reconstruction that I have seen so far.\n- The experiments consider varying classes of models beyond the U-Net used for most experiments.\n- The goals of the paper are clearly presented and examined in targeted experiments.\n- The findings on out-of-distribution hold-out performance as a surrogate for early stopping could be useful to practitioners."
            },
            "weaknesses": {
                "value": "I am currently learning towards rejection because there are some issues with several of the conclusions.\n\n1. For the first conclusion, no statistical tests or confidence intervals are used to qualify the statement that P+Q gives similar performance to P alone. I also think that a more rigorous analysis should have been done on a case level to compare the methods, as average SSIM scores can obscure what is going on at the tails. Medicine is inherently risk averse, so the tails are critical for this application.\n2. For data diversity, this effect seems to somewhat rehash the results of (Knoll, 2019), but with larger quantities of data. As with (1), a deeper analysis of edge cases could have been useful. However, in general I don't have major issues with this section.\n3. The analysis of pathology could be particularly problematic, as SSIM average scores obscure what is going on in small regions of the image. For example, in (Radmanesh et al., 2022) the SSIM for T1 images at a 6X acceleration is still quite high, but only 80% of the cases were accepted by radiologists for being clinically acceptable. In the same paper, Figure 1 shows that for very low SSIMs, some large and prominent pathologies can still be seen, whereas more modest pathologies such as MS lesions are erased at low accelerations. All this is to say that the SSIM metric is not indicative of performance for pathology cases, and a deeper analysis is needed to substantiate the claim that models trained on healthy subjects can reconstruct pathology.\n\nSo in summary, the paper's analysis is lacking in a few areas that probably need to be improved, including most importantly a deeper statistical analysis and looking at pathology beyond global SSIM numbers. Beyond that, the paper is submitted to the datasets and benchmarks track, but I did not identify what it considered to be its core contribution to that area.\n\nRadmanesh, Alireza, et al. \"Exploring the Acceleration Limits of Deep Learning Variational Network\u2013based Two-dimensional Brain MRI.\" Radiology: Artificial Intelligence 4.6 (2022): e210313."
            },
            "questions": {
                "value": "1. Why did you elect to use the U-Net for most experiments rather than a VarNet, which is generally more popular in the field?\n2. Could you provide more detail on the 7T database? The 7T database paper does not say much about a data release, and it only mentions 24 volunteers. Noting this, it might be good to list the number of subjects for each dataset in Table 1.\n3. Did you consider transfer learning as an alternative path to gaining the benefits of diverse training, as originally proposed in (Knoll, 2019)? Using full data from the beginning could take substantially more compute and would be a limitation.\n4. Did you consider the effect of model size on robustness?\n5. The current paper considers discriminative models, but more many approaches have been proposed based on generative priors that might be robust and can be trained on non-fully-sampled data (e.g., Jalal, 2021). Did you consider looking at these generative models?\n\nJalal, Ajil, et al. \"Robust compressed sensing mri with deep generative priors.\" Advances in Neural Information Processing Systems 34 (2021): 14938-14954."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698551599761,
        "cdate": 1698551599761,
        "tmdate": 1699637017841,
        "mdate": 1699637017841,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yGY5ismg4p",
        "forum": "KJYIgEteHX",
        "replyto": "KJYIgEteHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_xWg5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_xWg5"
        ],
        "content": {
            "summary": {
                "value": "This paper performs a thorough empirical evaluation of the effect of variability in the acquired training data on the in- and out-of-distribution performance of deep learning models trained for MRI reconstruction. The paper contains experiments supporting several points:\n\n- Training a single model on two different distributions yields similar in-distribution performance as two models trained separately on the different datasets.\n- Training a single model on multiple distributions improves out-of-distribution performance (though it does not match performance of a model trained specifically for the new distribution).\n- A model trained on healthy subjects can generalize well to subjects with pathologies, even when the model has never seen pathologies during training.\n- \u201cDistributional overfitting\u201d can occur where out-of-distribution begins to decrease while in-distribution performance continues to increase.\n\nBased on the observed trends, the paper demonstrates that a single model trained on a large collection of datasets provides better out-of-distribution performance and comparable in-distribution performance to networks trained solely on FastMRI."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper contains extensive experiments across various different data splits (based on anatomy, contrast, field strength, pathology, and data source) and convincingly demonstrates consistent trends in in-/out-of-distribution performance across many of these splits and across many architectures. The experiments are carefully done (for example, in Section 3/Figure 2, reporting results not just on the conglomerate dataset, but also on a subset of the data whose size roughly matches the sizes of P/Q).\n- The questions studied in this paper regarding generalization across scan parameters and pathology state are very important in the context of medical imaging, where data is hard to come by and varies significantly from site-to-site. The paper provides actionable insights for deep learning practitioners (for example, when training on FastMRI, it is good to include data both with and without fat suppression).\n- The paper is clearly written and the graphics are largely well-designed to distill the conclusions for the reader. For example, figure 7 is a somewhat unconventional data visualization but makes the point very clearly."
            },
            "weaknesses": {
                "value": "I will stress that I think this is a very strong paper and that I don\u2019t believe that the weaknesses below are reasons to reject the paper.\n- **Limited tuning of hyperparameters.** From Appendix D, it appears that all models are trained with the exact same hyper parameters. A more thorough version of this experiment would tune these hyperparameters for each architecture and dataset. However, I recognize that, given the number of models trained in the paper, this would be exceptionally computationally expensive. The chosen hyperparameters seem to be fairly standard values that a deep learning practitioner might use to initialize a model, and the observed trends are largely consistent across architecture, dataset split, etc, so I am okay with the current experimental setup \u2014 but it may be good to include a note about this in the Limitations section.\u2028\n- **Limited discussion of model capacity.** Related to the above point, I am particularly curious whether the observed trends hold for smaller models. This is motivated by some informal experiments I did on FastMRI data with/without fat suppression with much smaller models than those in this paper, where I observed that including both data splits did decrease model performance compared to models trained separately on each split. One hypothesis could be that models with lower capacity are less able to capture the variability in multiple data splits, leading to this effect. The bigger models in this paper definitely achieve better performance overall and thus are the right ones to report results for here, so I don\u2019t expect the current experiments to be expanded to cover this point and think the paper is strong enough as is. But if the authors have already conducted some related experiments, they may be useful to include in the appendix, and/or this may be another point for the discussion.\n- **Limited utility of the early stopping criteria for distributional overfitting.** Unlike traditional overfitting, where the early stopping epoch can be chosen by looking at a held-out subset of the current dataset, it seems to be much harder to monitor early stopping for the distributional overfitting case. Because we are interested in performance on out-of-distribution data, in a realistic scenario, we likely don\u2019t have the out-of-distribution data to monitor in the first place (if we did, we\u2019d want to include it in the training dataset). I see in section 7 that early stopping was performed by tracking validation loss on the fastMRI knee validation set, which seems like an \u201cin-distribution\u201d loss to me, because the fastMRI knee training set was included in the training data. So it is not clear to me to what extent distributional overfitting is being properly avoided here. I don\u2019t think this is a major limitation; even as is, the learned model outperforms the baselines on the out-of-distribution tests. But this would be a good point to discuss in section 6.\n- **It is hard to track the same model across some figures.** In Figures 4 and 5, multiple versions of the same architecture are reported within the same figure, trained on different splits of the data. In the current data visualization in Fig 5 for example, it is hard to track which pair of pentagons correspond to the same model configuration. I wonder if it would be useful to have an additional graphic plotting the *difference* in SSIM on Q for each *difference* in SSIM on P, with one datapoint for each model. This would partition the points into four quadrants showing relative improvement/performance decrease on P and Q, and a compelling result would be to show that no datapoint shows a dramatic decrease in performance on Q when trained on just P, as opposed to being trained on P+Q.\n- **Confidence intervals are missing in Figs 2-3.** I understand these would clutter some of the other visualizations, but in Figs 2-3 it would be great to see confidence intervals for the bar plots to contextualize the differences between the bars."
            },
            "questions": {
                "value": "- How do different model hyperparameters (especially pertaining to model size/model capacity) affect these trends?\n- How can one effectively choose an epoch for early stopping to avoid *distributional* overfitting?\n- Please see the data visualization suggestions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818694097,
        "cdate": 1698818694097,
        "tmdate": 1699637017737,
        "mdate": 1699637017737,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "26q6InSWH3",
        "forum": "KJYIgEteHX",
        "replyto": "KJYIgEteHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_YEQb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_YEQb"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the impact of the training data on the model\u2019s performance and robustness for image reconstruction of accelerated MRI by conducting diverse experiments. First, they split dataset into two different pairs of distributions including image contrasts (FLAIR, T1 etc.), magnetic fields (1.5T or 3T), and different anatomies (brain or knee), and compared the in-distribution performance of a single model trained on both against individual models trained on each dataset. Also, they evaluated the models on out-of-distribution data regarding image contrasts, magnetic fields, anatomies, and presence of malignancy (training on data from healthy patients and testing on data from non-healthy patients). Also, they found that \u201cdistributional overfitting\u201d occurs when training for long, performance on in-distribution data continues to improve marginally while performance on out-of-distribution data abruptly drops. Based on the experiments, the authors claim that using various distributions of training data provides a more robust model compared to developing separate models for individual distributions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tThe topic they investigated should be simple and interesting to researchers like the reviewer in medical imaging machine learning. This is because they have been always curious about whether models trained on a variety of datasets perform better on out-of-distribution test sets than models trained on individual datasets. \n-\tDiverse and well-designed experiments were conducted to strengthen their claims. \n-\tWhen reading through the experiments, the reviewer thought about the following questions based on the previous experiments, but the authors conducted the experiments that answer to my questions. It seems they spent much time on designing experiments for demonstrating their claims. \n-\tBeyond the explanations of the empirical results, they leveraged the findings to show the benefit of using early stopping onto reducing \u201cdistributional overfitting\u201d and improving model\u2019s robustness without compromising in-distribution performance."
            },
            "weaknesses": {
                "value": "-\tWhat the authors want to claim should be interesting to researchers in this field. However, it would be much better to show another medical imaging application in addition to image reconstruction, like cancer detection, lesion segmentation etc.\n-\tAccording to Table 1, the datasets seem diverse in terms of \u201cView\u201d and \u201cVendor\u201d as well. So, it would be much more interesting to conduct experiments considering the two factors. \n-\tAlso, it seems they synthesized accelerated MRI by under-sampling the fully sampled k-space from the original MRI images. Then, it would be interesting to compare models trained on k-space data sampled with different frequency (eg. 4-fold vs 8-fold)"
            },
            "questions": {
                "value": "-\tAll experiments the authors did are well described and their results make sense to the reviewer. So, I\u2019d like to give the authors suggestions about the experiments like listed in \u201cWeaknesses\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8202/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837079828,
        "cdate": 1698837079828,
        "tmdate": 1699637017606,
        "mdate": 1699637017606,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eQKAONBIOx",
        "forum": "KJYIgEteHX",
        "replyto": "KJYIgEteHX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_2hGx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8202/Reviewer_2hGx"
        ],
        "content": {
            "summary": {
                "value": "Deep learning models tend to overfit the input data distribution, which has been shown and studied by several papers in the literature before. This paper studies the effects of input distributional shifts (due to anatomy brain vs knee, contrast FLAIR vs T1, and Magnetic field 3T vs 1.5T) on deep learning models' performance for the task of accelerated MRI reconstruction. The authors propose training on diverse datasets as a solution to increase the robustness of deep learning models to the aforementioned shifts. The authors designed the study problem methodically by first showing that training on diverse datasets isn't worse than training on a single dataset. Then they extend the idea to multiple datasets and show the results on out-of-distribution (held out datasets, not shown during training) datasets. The results indicate that training on diverse datasets is better for out-of-distribution generalization for Accelerated MRI reconstruction."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and easy to read.\n- The experimental design is reasonable and logical for studying input distribution shifts.\n- Reporting results on multiple shifts (due to anatomy brain vs knee, contrast FLAIR vs T1, and Magnetic field 3T vs 1.5T), shows that problems exist for several variations of input distribution shifts. Also puts the proposed solution in a better standing.\n- Reporting results on diverse datasets (16 used in the paper) shows the general applicability of the inferences made."
            },
            "weaknesses": {
                "value": "- The paper does not explore vendor distribution shifts, raising questions about the model's generalizability across different MRI vendors.\n\n- The paper focuses on input-distribution shift in visual aspects of MRI images but does not delve into how these shifts manifest in k-space data, the raw data format for MRI. \n\n- The comparison between Figures 1 and 2 suggests that increased in-distribution data size correlates with better out-of-distribution performance. However, the paper does not explicitly study the impact of dataset size on model performance with out-of-distribution data. \n\n- The study utilizes full datasets from different sources for training, but it doesn't explore whether using a subset of these datasets could be equally effective. \n\n- The paper does not adequately discuss its limitations."
            },
            "questions": {
                "value": "- Is there a reason vendor distribution shift was not studied? Will a model trained using data for GE vendors work for Siemens?\n\n- The premise of the paper is based on input-distribution shift. While visually different contrast images, anatomy, etc have distribution shift in the data. However, the input to MRI reconstruction networks is k-space data (subsampled MRI). How much of the data shift is actually present in k-space representation? Is the k-space representation for different contrast/anatomy/Magnetic-field images actually different distributions not clear in the paper? Studying histograms of amplitude and phase information might give an answer to this question. The distribution shift in the visual domain vs in the Fourier(k-space) domain might be completely different.\n\n- Comparing results from Figures 1 and 2, It looks like if the in-distribution data size increases (1.3k vs 20k, 400 vs 6k), it also shows an increase in out-of-distribution performance (SSIM on Q is always higher for a larger dataset in Figure 1). A study of the impact of dataset size increase vs out-of-distribution performance is also warranted. This will support the idea that add datasets from diverse resources is better than increasing the dataset size of a single source.\n\n- The study has used full datasets from different resources while training. Is there a need to use all samples from different datasets? Maybe you only need a few to inform the model of variation in input distribution with a few samples. Maybe P+Q is not necessary. P+0.005Q is sufficient for good model training ?\n\n- The paper lacks a discussion on the limitations of the results. For example, this is only applicable in cases where data from different resources are available at training time. Other limitations might also include the amount of training time required to train the model with a large dataset size. Comparison of training times for different models might also be a relevant metric to the study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8202/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8202/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8202/Reviewer_2hGx"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8202/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699719885622,
        "cdate": 1699719885622,
        "tmdate": 1699719885622,
        "mdate": 1699719885622,
        "license": "CC BY 4.0",
        "version": 2
    }
]