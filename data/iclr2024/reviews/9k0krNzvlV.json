[
    {
        "id": "92Wx46QaEz",
        "forum": "9k0krNzvlV",
        "replyto": "9k0krNzvlV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_GhKC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_GhKC"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the learnability of watermarks for language models. The authors use watermark distillation, a method that trains a student model to behave like a teacher model that uses decoding-based watermarking. They test their approach on three decoding-based watermarking strategies and find that models can learn to generate watermarked text with high detectability. However, they also find limitations to learnability, such as the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper finds an approach for learning watermarks in language models through knowledge distillation.\n2. The paper presents empirical findings on the learnability of watermarks, including the success of the proposed distillation method on three distinct decoding-based watermarking strategies. The results show that models can learn to generate watermarked text with high detectability.\n3. The paper discusses the limitations of learnability, including the loss of watermarking capabilities under fine-tuning and the high sample complexity for learning low-distortion watermarks. These limitations provide valuable insights and highlight the challenges in implementing watermarking in language models."
            },
            "weaknesses": {
                "value": "1. The method presented is neither novel nor new. Knowledge distillation is a well-established concept, and this paper simply tests existing watermarking methods in conjunction with it. The paper appears more evaluative in nature.\n\n2. There's a body of prior works focused on watermark distillation. These works show that if the student model distills the watermarked teacher model, it offers protection against model extraction attacks. Although the context may differ, it is better for the author to acknowledge this in the related works section. Key references include:\n\n- He et al., \"Protecting Intellectual Property of Language Generation APIs with Lexical Watermark,\" AAAI 2022.\n- Zhao et al., \"Distillation-Resistant Watermarking for Model Protection in NLP,\" EMNLP 2022.\n- He et al., \"CATER: Intellectual Property Protection on Text Generation APIs via Conditional Watermarks,\" NeurIPS 2022.\n- Zhao et al., \"Protecting Language Generation Models via Invisible Watermarking,\" ICML 2023.\n\n3. Although the p-value is utilized for watermark detection, I also recommend that the author present quantitative metrics such as AUC, FPR, etc., for a more comprehensive understanding of watermark detection."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698468320999,
        "cdate": 1698468320999,
        "tmdate": 1699637173744,
        "mdate": 1699637173744,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Uh4nTHQKRg",
        "forum": "9k0krNzvlV",
        "replyto": "9k0krNzvlV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_hxrQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_hxrQ"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates whether watermarks for language models, which allow for the detecting model-generated text, can be directly learned by another model without applying the same watermarking decoding algorithm, termed weights-based watermarking. This has implications for using watermarks in open-source models and for potential spoofing attacks.\n\nThe authors propose two methods for learning weights-based watermarks without specialized decoding - logits-based and sampling-based watermark distillation. This transfers watermarks from a teacher model to a student model.\n\nExperiments are run with three different decoding-based watermarking techniques. The results show that the proposed distillation methods can learn watermarks, but learnability varies across schemes and hyperparameters. Lower distortion watermarks are harder to learn.\n\nThe authors demonstrate two applications of the findings. For open models, learned watermarks will be removed by further fine-tuning, and they leave the learning weights-based watermarking that is robust to fine-tuning for future work. For spoofing attacks, sampling-based distillation can replicate watermarks. This suggests limitations of using watermarks for provenance.\n\nIn summary, this paper provides an interesting empirical analysis of the learnability of current decoding-based textual watermarks. It highlights the potential limitations of text watermarks for provenance. Their findings suggest the need for investigation on watermarks that are difficult to learn as a prevention of the discussed spoofing attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n- Investigates the novel problem of whether language models can learn to generate watermarks themselves. This question has important implications but was previously unexplored.\n\nQuality:\n- Provides extensive empirical results across three watermarking schemes and hyperparameters\u2014thorough, reproducible experiments.\n- Uses appropriate metrics to assess watermark detection and text quality\u2014rigorous quantitative evaluation.\n- Clear methodology and training details\u2014enable replicability.\n\nClarity:\n- Structured logically to build understanding incrementally. Smooth flow between sections.\n\nSignificance:\n- Foundational insights into watermark learnability advance knowledge in an important emerging area.\n- Learnability findings have direct implications for applications like watermarking open models.\n- Identifies risks around spoofing that impact watermark security. Informs responsible development.\n- Benchmark results across schemes and hyperparameters enable future work. Valuable analysis."
            },
            "weaknesses": {
                "value": "The experimental methodology could be strengthened by\n 1. controlling for model architecture (use the Llama-2 as the student model for both logits-based and sample-based experiments for a more intuitive comparison of the results)). This would better isolate the distillation techniques themselves.\n 2. increasing the dataset diversity (currently, only one dataset is used).\n 3. somehow, seq-rep-3 is not evaluated on the sample-based distillation.\n\nThe technical writing is condensed in some areas, making it harder to follow for non-experts. Expanding explanations around key concepts like the distillation objectives could improve readability.\n\nThe related work could be expanded to provide a broader context. Watermarking has been studied for other modalities like images, which could give a useful perspective. Meanwhile, model-specific watermark leveraging the learnability of distribution difference has also been discussed in the literature [He, Xuanli, et al. ].\n\nThe spoofing attack evaluation is limited to a simple harmful/innocuous categorization. More nuanced human evaluations could better characterize risks.\n\nThe limitations around fine-tuning robustness are acknowledged but not thoroughly investigated. Exploring techniques to improve robustness could be valuable.\n\nHe, Xuanli, et al. \"Cater: Intellectual property protection on text generation apis via conditional watermarks.\" Advances in Neural Information Processing Systems 35 (2022): 5431-5445."
            },
            "questions": {
                "value": "I would like to request an evaluation using Llama-2 for both distillation techniques, rather than different models for each. Doing so would allow for a more direct comparison of the distillation methods themselves, better isolating the impact of the techniques. This could strengthen the technical contributions by providing improved analysis of the relative efficacy of logits-based vs sampling-based distillation.\n\nAdditionally, conducting more in-depth studies on spoofing attacks across diverse topics and datasets could further increase the impact of the work's findings. Expanding the spoofing experiments to include varied contexts beyond the current proof of concept could substantiate the risks posed. Rigorously demonstrating successful spoofing in different settings would validate concerns around watermark security and attribution. This expanded analysis could strengthen the evaluative assessment by demonstrating comprehensive consideration of the practical implications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Reviewer_hxrQ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695790638,
        "cdate": 1698695790638,
        "tmdate": 1700605982720,
        "mdate": 1700605982720,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xC9FBcVELy",
        "forum": "9k0krNzvlV",
        "replyto": "9k0krNzvlV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_metd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_metd"
        ],
        "content": {
            "summary": {
                "value": "The authors study whether language watermarks can be spoofed by training a student language model (LM) on the watermarked outputs of a teacher LM (the process is known as distillation). The experiments show that all three surveyed watermarking methods can be spoofed when the attacker has 640k (watermarked) samples, each comprised of 256 tokens, and that the distilled watermark remains effective under different decoding strategies and has similar robustness properties as the teacher's watermark."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Overall, I liked the paper. The problem of watermarking open models and spoofing attacks against watermarks is timely and important. \n\n* The authors show that three watermarking methods are vulnerable to spoofing through distillation. \n\n* The authors convince me their distillation approach works for many different generation parameters against all three watermarking methods when sufficiently many samples are available to the attacker.\n\n* The authors show experiments with many state-of-the-art models."
            },
            "weaknesses": {
                "value": "**Contribution to Open Model Watermarking**. As the authors show, open-model watermarking using distillation is not robust against fine-tuning. I am unclear about the contribution of the authors to open model watermarking. No prior work has used distillation to watermark LMs, hence there is no security threat. What is the use of a watermark that (i) lacks robustness and (ii) can be spoofed by design? I would love to hear the author's thoughts on this. \n\n**Limited novelty.** One would expect that watermarking can be spoofed if the attacker can access sufficiently many watermarked samples. If I understood correctly, the authors use **163 million tokens** ($640$k samples times 256 tokens each) of watermarked text with the _same_ message. How realistic is this assumption? Is the attack effective when a provider uses a different message after generating a set number of tokens? \n\nSpoofing is possible when sufficiently many samples are revealed. I believe a more interesting approach is to measure the number of samples that enable spoofing or to derive a theoretical bound for the spoofer's sample efficiency. Insights into that question could guide the party that injects the watermark into deploying defenses, such as switching to a different key or message after this number of samples has been revealed. Unfortunately, the paper does not investigate this problem. Do the authors have any insights into that problem?"
            },
            "questions": {
                "value": "* What are your contributions towards open model watermarking?\n\n* How sample-efficient is the spoofing attack?\n\n* Can the distilled and original watermarked distributions be distinguished? I assume the spoofed watermarked does not learn rare watermarked sequences, but instead focuses first on the most often occurring ones. If a defender can detect and ignore those during detection, can they evade your spoofing attack while effectively detecting watermarked text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I believe there are no ethical concerns in this paper."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706246079,
        "cdate": 1698706246079,
        "tmdate": 1699637173488,
        "mdate": 1699637173488,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UNGdgOHu7b",
        "forum": "9k0krNzvlV",
        "replyto": "9k0krNzvlV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_2p7U"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9328/Reviewer_2p7U"
        ],
        "content": {
            "summary": {
                "value": "This paper raises a concern about forgery attacks on current LLM watermarking methods. To achieve the attack goal, a distillation-based strategy is introduced. Experiments on three existing LLM watermarking methods demonstrate the threat of such an attack."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "It is interesting to introduce forgery/spoofing attacks into the recent popular area, namely, LLM."
            },
            "weaknesses": {
                "value": "1. This paper directly extends distillation strategies on decoding-based watermarking and then poses the limitation of weight-based watermarking. However, no solution is provided. It seems like an experimental report, and it would be better to introduce the specific solution.\n\n2.  The motivation of this paper is there is a limitation of decoding-based watermarking, namely, replacing it with a normal decoder. Is the assumption practical? In practice, for an LLM API, how do we conduct such an operation?\n\n3. For wright-based LLM watermarking, more attacks (besides fine-tuning) shall be considered, such as pruning and further distillation.\n\n4. The paper exceeds 9 pages."
            },
            "questions": {
                "value": "1. Re-clarify the contribution of this paper, and provide strong results or explanations to support the claim. Furthermore, re-clarify the scenario for better understanding. \n\n2. For decoding-based watermarking, if different users are assigned different keys, will the distillation or spoofing attacks fail?\n\n3. What is the additional computational cost? Evaluate or compare it.\n\n4. Besides decoding-base LLM watermarking, there exist some backdoor-based LLM watermarking methods. Are the findings still suitable for them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9328/Reviewer_2p7U"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771310497,
        "cdate": 1698771310497,
        "tmdate": 1699637173333,
        "mdate": 1699637173333,
        "license": "CC BY 4.0",
        "version": 2
    }
]