[
    {
        "id": "kYq70q0i78",
        "forum": "Vw24wtSddM",
        "replyto": "Vw24wtSddM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_rpWY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_rpWY"
        ],
        "content": {
            "summary": {
                "value": "1. The authors introduce a new module called Tree Cross Attention (TCA) that reduces the number of tokens required for efficient inference while maintaining comparable performance to Cross Attention. \n\nTCA organizes data in a tree structure and performs a tree search at inference time to retrieve relevant tokens for prediction. Specifically, TCA only retrieves information from a logarithmic O(log(N)) number of tokens for performing inference, while Cross Attention scans the full set of O(N) tokens. \n\n2. The authors also present ReTreever, a flexible architecture for token-efficient inference that incorporates TCA. \n\n3. Empirically, the paper demonstrates the effectiveness of TCA and ReTreever on various classification and uncertainty regression tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and straightforward to understand.\n\n2. The empirical results that the authors show are impressive. Specifically, the authors show that Perceiver IO's performance drops significantly as the length of the sequence increases, while ReTreever is able to maintain high accuracy across a range of sequence lengths.\n\n3. The proposed method, ReTreever, is able to perform token-efficient inference while achieving better performance than Perceiver IO for the same number of tokens. ReTreever does this by using Tree Cross Attention to retrieve the necessary tokens, only needing a logarithmic number of tokens log(N) << N, making it efficient regardless of the encoder used."
            },
            "weaknesses": {
                "value": "1. In the evaluation, the authors focus on %tokens metric. How does that translate to wall clock speed up? Or does the tree structure introduce operations that are hard to take advantage of by modern hardware?"
            },
            "questions": {
                "value": "1. My main question regarding the evaluation is in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698264549162,
        "cdate": 1698264549162,
        "tmdate": 1699636147413,
        "mdate": 1699636147413,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WkyYgWXIPD",
        "forum": "Vw24wtSddM",
        "replyto": "Vw24wtSddM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_Wmtm"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a tree cross attention (TCA) architecture to effectively encode context into a tree structure when performing attention, such that the amount of attended tokens will be less than the actual number of context tokens. The architecture is co-trained with the objective to effectively retrieve context through graph-based searching, and utilize context for performing the task. Experiments on a specific set of tasks (i.e., copy task, gp regression on image completion and time series on human activities) show that TCA is able to achieve a similar performance as full cross attention while attending to much fewer tokens. \n\nThe architecture proposed is generally applicable to a lot of settings broadly, yet the evaluation is too specific and does not adequately proves the generality of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea to construct context as a tree is interesting and could have broad implications in constructing context for language models use cases including agent trajectories, in-context learning examples and retrieved documents and more."
            },
            "weaknesses": {
                "value": "- **More context on the baseline IO Perceiver**: The authors need a background section for IO perceiver so the work is self-contained. With the current version, IO perceiver, though being a famous and well-cited paper, is not clearly stated. \n- **Speedup by attending to fewer context tokens**: One claimed benefit of the method is that it attends to fewer tokens to context when performing the task, which I assume would result in an inference speedup. But the work does not explicitly measure if TCA runs faster than CA. On the other hand, TCA does in most of the cases leads to a slight performance degradation, and it will be important to justify the design with proper inference wall-clock time measurements. \n- **More general evaluation**: The authors claim to have proposed this general architecture, but the evaluating tasks are specific and not as general as expected. The tasks evaluated in the baseline work \u2014 IO perceiver (e.g., pretraining MLM, optical flow and multimodal encoding) seem harder and more general than the ones performed in this paper, (i.e., copy task, gp regression on image completion and time series on human activities). It would be nice to see experimental setups with more significant implications like pre-training, yet there seems to be significant amount of work to be put into actually scale it to these real settings."
            },
            "questions": {
                "value": "- Does the algorithm run faster than full attention in terms of wall-clock time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698621306733,
        "cdate": 1698621306733,
        "tmdate": 1699636147320,
        "mdate": 1699636147320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ne4PbK1WHW",
        "forum": "Vw24wtSddM",
        "replyto": "Vw24wtSddM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_779Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_779Z"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for reducing the test-time computational cost of attention. Namely, TCA walks down a tree to attend to a single set of sibling leaves, while only attending to a compressed version of the other leaves. This allows the complexity of attention to be logarithmic in the total number of leaves $N$ (the sequence length). This comes at the cost of training a policy to traverse the tree, which must be trained via REINFORCE, an aggregator that compresses and composes leaf representations, as well as defining the tree itself.\n\nExperiments on a copy task, GP regression, image completion, human activity classification show that the method is efficient and performant. Additional analysis highlights the method's memory efficiency compared to full attention."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I enjoyed reading the paper. The paper is well-written and easy to follow. The idea is simple and clever."
            },
            "weaknesses": {
                "value": "Overall, I believe the paper is pretty complete. I am mostly curious about how to make this method work for self-attention and (masked) autoregressive modeling.\n\nLarger-scale experiments would be appreciated, as the current experiments are quite small-scale. Presumedly the challenges of training the tree expansion policy would increase with harder datasets.\n\nOne suggestion for a larger-scale experiment would be training a translation or summarization model and replacing the encoder attention with tree cross attention."
            },
            "questions": {
                "value": "## Questions\n1. Would TCA work out of the box for masked language modeling, e.g. BERT?\n2. Did you try using Gumbel-softmax for training the tree expansion policy?\n3. What are the barriers to applying TCA to self-attention? Would aggregation become the most expensive operation?\n\n## Suggestions\n1. In the last paragraph of 3.1, I was a little confused about why k-d trees were needed as I was only thinking about 1D sequences. Having a picture of an image tree and some more prose about different domains would be really nice for motivating and showing the generality of the method.\n2. While including cross attention in the model name pragmatically implies the method is not intended directly for self-attention, it would be nice to add a footnote that the focus is not on autoregressive modeling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698984543023,
        "cdate": 1698984543023,
        "tmdate": 1699636147260,
        "mdate": 1699636147260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eq3NhPbWih",
        "forum": "Vw24wtSddM",
        "replyto": "Vw24wtSddM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes TCA, a tree-based cross attention module to reduce the complexity of cross attention from $O(N)$ to $O(\\log(N))$, where $N$ is the number of tokens used for cross attention. Given $N$ tokens, TCA first constructs a balanced binary tree representation using standard methods like K-D tree, where the leaf nodes are the token embeddings, and the internal node representations are aggregated using the two children of the internal node. TCA uses reinforcement learning to learn good internal node representations. This construction is only performed once for a set of context tokens. Now, for a given query vector, a tree search is performed to select a subset of nodes ($O(\\log(N))$ size) of the tree for cross attention, resulting in $O(\\log(N))$ complexity for retrieval. Using TCA, the paper further proposes ReTreever, a general-purpose retrieval model that achieves token-efficient inference. The paper compares the ReTreever models with other token-efficient retrieval models like Perceiver IO and show impressive gains over the baseline - little to no drop in performance while leveraging only a small subset of tokens for cross attention."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper is well written and builds the theory coherently.\n- The proposed cross-attention architecture, TCA, along with the general purpose retrieval model, ReTreever is novel.\n- Because ReTreever uses reinforcement learning to learn the internal node representations, the reward used for optimization can be non-differentiable like accuracy, which improves performance over a reward based on cross entropy because the reward model is simpler in case of accuracy.\n- The reasoning behind each of the loss terms in $\\mathcal{L_{ReTreever}}$ is well-explained and it also uses leaf-level cross attention loss to make the training faster.\n- The empirical results on various tasks like copying, uncertainty estimation are impressive using ReTreever, and the paper also has good ablation studies to test the various components of the proposed approach."
            },
            "weaknesses": {
                "value": "- It would be good if a similar row (as given in Table 2) can be added to Table 1 for Perceiver IO with increased latent tokens that matches the performance of TCA on the copy task.\n- Theoretical complexity is fine, but the paper should also report wall-clock time for ReTreever and compare it with the full Transformer+Cross Attention and Perceiver IO models. I am guessing the tree approach is not parallelizable on accelerated devices like GPUs, but it would be good to see if there's considerable decrease in latency on CPUs.\n- Building on the previous point, wall-clock times for the tree construction and bottom-up aggregation should be reported too.\n- Using ReTreever-full does not make sense and it only confuses the understanding of the reader in my opinion. Either remove it, or add more details like why there is a performance gap between the full cross-attention and ReTreever-full given both are using 100% of the tokens."
            },
            "questions": {
                "value": "I have asked most of my questions in the weakness section. If the authors can address my questions and add the relevant latency benchmarks too, I am willing to increase my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2145/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2145/Reviewer_RnYk",
                    "ICLR.cc/2024/Conference/Submission2145/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2145/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699281655002,
        "cdate": 1699281655002,
        "tmdate": 1700686496163,
        "mdate": 1700686496163,
        "license": "CC BY 4.0",
        "version": 2
    }
]