[
    {
        "id": "NOnOROBmZd",
        "forum": "4sGoA7Eih8",
        "replyto": "4sGoA7Eih8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_wffn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_wffn"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a theoretical approach for reversing input data using attention weights and model outputs. The study explores the mathematical foundations of the attention mechanism to assess if knowing attention weights and model outputs can be used to reconstruct sensitive information from input data. The research aims to enhance our comprehension of this aspect and promote the creation of more secure and reliable transformer models. Ultimately, the paper seeks to contribute to responsible and ethical progress in the field of deep learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper develops a theoretical approach to reverse input data using attention weights and model outputs. By investigating the mathematical foundations of the attention mechanism, the study explores the potential for reconstructing sensitive information from input data. This investigation can enhance our understanding of this process and support the creation of more secure and dependable transformer models, fostering responsible and ethical advancements in the field of deep learning."
            },
            "weaknesses": {
                "value": "1. The authors do not provide any empirical results to support the effectiveness of the attack method. We respect works that focus on theoretical contributions. However, for the deep learning attack topics, it is better to evaluate the effectiveness of the proposed attack via experiment since it is feasible to do experiments.\n\n2. It is unclear in which scenario the attacker can access the attention weights and the output of the attention layer.\n\n3. This paper does not involve a non-linear activation layer (e.g., ReLU), which is widely applied to transformers into discussion, which should introduce challenges to invert the input perfectly."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698616740031,
        "cdate": 1698616740031,
        "tmdate": 1699636160496,
        "mdate": 1699636160496,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4MGPskcdm8",
        "forum": "4sGoA7Eih8",
        "replyto": "4sGoA7Eih8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_SERj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_SERj"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new model inversion attack that recovers private training data from attention weights and outputs. Particularly, this paper takes a theoretical approach and shows that, given the trained attention weights and output, the adversary can update the private data X by minimizing the loss function."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors perform a comprehensive theoretical analysis of attention inversion attacks."
            },
            "weaknesses": {
                "value": "The paper has the following weaknesses: \n- There is no empirical evaluation of the proposed inversion attack method. Although the authors explain that they focus on theoretical analysis, the efficacy of the data recovery method shall be justified with empirical results. There are no quantitative results that can prove the efficacy of attention inversion attacks.  \n- The paper does not discuss the overhead of the proposed attack. Particularly, the computation overhead of recovering data given attention weights and outputs is not clear."
            },
            "questions": {
                "value": "Please consider addressing the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Reviewer_SERj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698631637832,
        "cdate": 1698631637832,
        "tmdate": 1699636160396,
        "mdate": 1699636160396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Kvhjj7vWAq",
        "forum": "4sGoA7Eih8",
        "replyto": "4sGoA7Eih8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_vHCT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_vHCT"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a theoretical approach for inverting input data using weights and outputs for transformer architecture."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The question of whether transformer architecture is secure against data reconstruction is important."
            },
            "weaknesses": {
                "value": "I honestly don't quite understand the contribution of this paper. There is no support/evaluation of the \"main result\" in Section 1.\n\nPresentation of the paper (the arrangement of content in sections) looks a bit unusual."
            },
            "questions": {
                "value": "I don't understand what this paper tries to do."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807060691,
        "cdate": 1698807060691,
        "tmdate": 1699636160317,
        "mdate": 1699636160317,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lrYKsaouYh",
        "forum": "4sGoA7Eih8",
        "replyto": "4sGoA7Eih8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_K9uu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2274/Reviewer_K9uu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an algorithm to recover the input data from given attention weights and output by minimizing the loss function capturing the discrepancy between the expected output and the actual output. The proposed method addresses the importance of understanding and safeguarding the transformers' intermediate outputs to protect the input data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper targets at a timely problem given privacy concerns around transformers and attention mechanisms."
            },
            "weaknesses": {
                "value": "1. Lacks any empirical evaluations to complement the theoretical results.\n\n2. The equations in the paper are not numbered.\n\n3. Some symbols are not explained. For example, in the second equation, what it A1_n?\n\n4. Based on my understanding, the core of this paper is to find an optimal X*=argminL(f_\\theta(X),Y). This seems an easy optimization problem which can be done by gradient decent. This paper does not explain what are the challenges of solving this optimization problem and how to overcome these challenges.\n\n5. Given the weight and output, the input can also be found by black-box attacks where a network is constructed to do model inversion. This paper lacks comparing with such black-box attacks."
            },
            "questions": {
                "value": "Please see the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2274/Reviewer_K9uu"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823524681,
        "cdate": 1698823524681,
        "tmdate": 1699636160240,
        "mdate": 1699636160240,
        "license": "CC BY 4.0",
        "version": 2
    }
]