[
    {
        "id": "gE6DCeMMkm",
        "forum": "228XQpErvW",
        "replyto": "228XQpErvW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_g2Zo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_g2Zo"
        ],
        "content": {
            "summary": {
                "value": "The paper deals with offline to online RL. A simple algorithmic novelty is presented and experiments are conducted to show that compared to alternative methods, especially at the beginning of online training, severe performance drops can be reduced."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper is very carefully prepared and well written.\n* The proposed algorithm is simple perhaps even elegant\n* The results are promising"
            },
            "weaknesses": {
                "value": "* The performance of the algorithm has not been studied on benchmarks with stochastic MDPs."
            },
            "questions": {
                "value": "No questions, but further comments:\n* At \u201cBatch or offline reinforcement learning methods\u201d, I would think it would be good to also mention one of the older papers on batch/offline RL, e.g. [1], so that it is clear that the topic did not just come up in 2020. But I consider this a matter of taste.\n\n* The term \"policy collapse\" remains too vague. It should be clarified what is meant by it.\n\n* At \u201cMany previous methods Zheng et al. (2023b); Lee et al. (2022); Zhang et al. (2023) could achieve better policies than their pre-trained offline models but suffer from policy collapse at the beginning of the transition from offline to online\u201d, the reader might ask whether safe RL should solve the problem, and thus be mentioned, e.g., [2][3][4] and explained why these techniques cannot be used here.\n\n* Furthermore, it seems useful to distinguish the current work from [5] and {6], which are also robust against performance losses in the online phase.\n\n[1] Lange et al, Batch Reinforcement Learning, 2012\\\n[2] Laroche et al, Safe Policy Improvement with Baseline Bootstrapping, 2017\\\n[3] Nadjahi et al.: Safe policy improvement with soft baseline bootstrapping. 2019\\\n[4] Scholl et al.: Safe Policy Improvement Approaches and their Limitations, 2022\\\n[5] Swazinna et al., User-Interactive Offline Reinforcement Learning, 2022\\\n[6] Hong et al., Confidence-conditioned value functions for offline reinforcement learning, 2022\n\n\n* In \u201cWe use a bootstrapped ensemble Q-network with an outlier filtering technique for more accurate value estimation to reduce the uncertainty encountered during the distribution drift\u201d, it remains unclear which of this existed before and which is an innovation.\n\n* The discussion of offline RL in Section 2.1 does not address model-based offline RL. I think it should be clarified that only model-free offline RL is considered in the paper.\n\n* In Table 1, there is no mention of what the numbers behind the $\\pm$ are. If nothing is mentioned, then they should be estimates of statistical uncertainty, e.g., the standard error. The standard deviation should never be used after a $\\pm$ because it cannot serve as a measure of the uncertainty of the mean preceding the $\\pm$. Finally, the uncertainty of the mean becomes smaller as the number of experiments increases, while the standard deviation does not. The authors should ensure that the standard error is used at this point, or another measure of uncertainty, such as the 95% confidence interval (but in this case it should be mentioned in the caption).\n\n* In Table 1 there are some format errors, like \"90.7 $\\pm$ 2.00\", where the number of decimal places of the uncertainty (2.00) does not match the number of decimal places of the measured value (90.7). So it must be 90.7 $\\pm$ 2.0 or 91 $\\pm$ 2.\n\n* In the text \"fine-tune\" is used mostly, but in one place \"finetune\" is used.\n\n* The following references are duplicated:\n\nPhilip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning\nwith offline data. arXiv preprint arXiv:2302.02948, 2023a.\\\nPhilip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning\nwith offline data. arXiv preprint arXiv:2302.02948, 2023b.\n\nHan Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy\nlearning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023a.\\\nHan Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive policy\nlearning for offline-to-online reinforcement learning. arXiv preprint arXiv:2303.07693, 2023b."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Reviewer_g2Zo"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4209/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698254811588,
        "cdate": 1698254811588,
        "tmdate": 1700433782564,
        "mdate": 1700433782564,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "saO2D9Til2",
        "forum": "228XQpErvW",
        "replyto": "228XQpErvW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_AHJ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_AHJ2"
        ],
        "content": {
            "summary": {
                "value": "This manuscript proposes a new method \u2013 Increased Simple Moving Average of Q-value (ISMAQ) for the fine-tuning problem in offline-to-online RL. Although the proposed formulation seems interesting and novel, the reviewer believes this manuscript still has room for improvement before publishing. See details below."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The observation and insight in Figure 1 seem interesting \u2013 where the Q-mean of non-expert data increases while the Q-mean of expert decreases.\n2. The proposed method in equation 2 seems to be novel."
            },
            "weaknesses": {
                "value": "1. [Minor] The citation format needs to be updated. Please use `\\citep` instead of `\\cite`, when the papers being cited are not used as nouns. For example, in the second line of the introduction, Silver et al. (2017), and the other citations should appear as (Silver et al., 2017). There are many other citations that use the wrong format, which inevitably affects the readability of the manuscript.\n2. [Minor] The citation in Section 2.2.1 seems weird \u2013 it contains two papers by Ball et al (2023a;b), which appear to be the same paper in the reference.\n3. [Major] While the observation in Figure 1 of Section 4.1.1 seems interesting, the reviewer does not fine the conclusions convincing enough, since it only has experimented with `halfcheetah`. The reviewer is expecting more environments from D4RL (such as `walker`, `hopper`, `ant-maze`) and even other environments such as `adroit-hand`, as adopted by the Cal-QL [1] paper.\n4. [Major] The Lemma in Section 4.1.2 is not informative enough \u2013 the reviewer does not understand what is the Lemma proving. The reviewer is expecting a rigorous lemma to be written with math notations, not text descriptions. Based on the current presentation of the Lemma, the reviewer cannot tell the correctness of the lemma.\n5. [Major] The experiments in 3 are also not conclusive enough using only the `walker2d` environments. The reviewer understands the authors\u2019 motivation is to provide a justification for the ReLU operator, but the reviewer is expecting more experiments in other environments as suggested in 3.\n6. [Minor] The original paper of REDQ [2] is by Chen et al., not Zhao et al. The reviewer understands that Zhao et al., proposed a new method for offline-to-online that is built on top of [2], but it might be better for the authors to clarify the origins of REDQ [2] in Section 5.1.\n7. [Major] For the experimental evaluations in Section 5, only conducting experiments in D4RL locomotion is not enough. The reviewer is expecting more environments (see e.g., [1])."
            },
            "questions": {
                "value": "[Major] At the bottom of Section 4.2, the authors claim that \n\n> These two techniques both could be implemented with minimal changes with only $\\mathcal{O}(1)$ time complexity\u2026\n\nWhat does \u201cThese two techniques\u201d refer to? Since the author also mentioned $\\mathcal{O}(1)$ complexity in the abstract, the reviewer would expect more discussion on where the $\\mathcal{O}(1)$ complexity comes from and how it is achieved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4209/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698283432692,
        "cdate": 1698283432692,
        "tmdate": 1699636387840,
        "mdate": 1699636387840,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H8wALBvIov",
        "forum": "228XQpErvW",
        "replyto": "228XQpErvW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_Thik"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_Thik"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel regularization strategy that should help to efficiently adapt offline pre-trained policies with additional online data in an offline-to-online setting. Specifically, the authors propose an exploration bonus to add on top of previously existing offline RL algorithm TD3+BC in order to not remain too conservative when moving from offline to online training. The new term is based on the difference between the average Q-value of the current and a reference time step. Additionally, low-complexity buffer techniques are incorporated into the method in order to adapt to distribution drift due to the changing policy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Many offline RL algorithms as well as exploration techniques focus in one way or the other on a measure of uncertainty for their regularization - offline RL approaches penalize it to remain within data support, while exploration schemes explicitly seek out uncertainty for information gain. The proposed technique however uses nothing of the sort, instead it simply measures the difference in average episodic Q-values over training time. Based on the observation that these values increase only for sub-optimal agents & decrease over time when the buffer only contains expert data, it is used as an additional loss term to improve the policy in the online training part of the algorithm. The method is very simple and does not require training of additional models like most other regularization schemes in this context. At the same time it appears to work well and to the best of my knowledge it can be considered novel. Also, the outlier filtering appears to be an innovative concept to improve the stability of the method.\nFurthermore, the empirical performance on last-10 appears to match the prior SotA, while it outperforms the prior best on first-10."
            },
            "weaknesses": {
                "value": "It is a little unclear to me what exactly first-10 & last-10 performance means (I may have missed it) - if it refers to the average return of the policy during the first and last 10 gradient updates, I am wondering whether the comparison for the first-10 case is meaningful: From the appendix I gather that f=2, i.e. the policy is updated every 2 steps, so you only have really 5 different policies. Also, the algorithm starts with the offline pre-trained policies, which we know perform well since TD3+BC is known to work on the presented datasets. Is it possible that 5 policy updates is just too little to move far away from this & that is the reason why it is that good? I know that prior O2O approaches had trouble to even maintain the offline performance when they moved to the online phase, however it seems odd that others (like e.g. TD3+BC to TD3) basically drop immediately by a huge margin. Do they all start with the same pre-trained policy performance? What do you attribute this difference especially during the first few updates too? I would suggest to extend the plots towards the left so that one can also see the offline training phase and directly inspect what happens when you move from offline to online. The last-10 performance isn't really better than the prior SotA by REDQ, so since the main contributions are novelty and first-10 performance, I think it is important to examine the latter more closely.\n\nI believe some other prior works should also be considered in the related work section:\n\n[1] Ghosh, D., Ajay, A., Agrawal, P., & Levine, S. (2022). Offline rl policies should be trained to be adaptive. ICML 2022\n\n[2] Hong, J., Kumar, A., & Levine, S. (2022). Confidence-Conditioned Value Functions for Offline Reinforcement Learning. ICLR 2023\n\n[3] Swazinna, P., Udluft, S., & Runkler, T. (2022). User-Interactive Offline Reinforcement Learning. ICLR 2023\n\nThey are also concerned with offline to online learning, just that their online phase is a little shorter and their adaptations thus look a little different than the one you consider. Still, when thinking about O2O they are closely related and should be considered."
            },
            "questions": {
                "value": "I do not understand figures 7/8:\n- what is the middle figure showing - there is no legend so it's unclear which of the other two legends is active here?\n- since the colours are the same in each graph, it is a bit misleading what this means\n--> e.g. is the blue one a combination of the legends (ISMAQ weight=1 AND K=5)?\n\nwhat does no_ismaq mean in fig.10? The text says something about plainly using Eq. 5, but there the ISMAQ weight is already contained...\n\nin fig 9 you evaluate different choices for d - have you tried really small ones as well, like 1? I mean at some point it has to collapse right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Reviewer_Thik"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4209/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740843627,
        "cdate": 1698740843627,
        "tmdate": 1699636387759,
        "mdate": 1699636387759,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QsFQwBhwYT",
        "forum": "228XQpErvW",
        "replyto": "228XQpErvW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_74i3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4209/Reviewer_74i3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an offline-to-online RL algorithm named ISMAQ (Increased Simple Moving Average of Q-value). This method extends TD3+BC and introduces a new loss term into the actor loss, designed to selectively raise the average Q-values based on convergence. Additionally, it incorporates various techniques, including critic ensemble, outlier filtering, combined experience replay, and the removal of the oldest transition in the buffer. In the experiment, ISMAQ outperformed several previous methods on the D4RL locomotion benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method builds upon TD3+BC and improves over it on both offline-to-online setting and online from scratch setting.\n\n2. The ablation studies testing the sensitivity of each component in Sections 5.3 and 5.4 are informative.\n\n3. The paper studies an interesting and important problem."
            },
            "weaknesses": {
                "value": "1. The comparisons with several other offline-to-online RL algorithms are missing [1, 2, 3, 4]. Several of them are missing in the related work as well.\n\n2. The method is only evaluated on D4RL locomotion tasks. It would be beneficial to include results on the D4RL Antmaze tasks as in [2,3,4] and the Adroit binary task as in [1, 4] which require higher sample efficiency than the locomotion tasks.\n\n3. I don\u2019t think the following sentence is true. Does AWAC need either of the requirements?\n>Unfortunately, the aforementioned offline-to-online methods need at least one of the following requirements that makes them resource-consuming Yu & Zhang; Zhao et al. (2022); Lee et al. (2022); Nair et al. (2020); Luo et al. (2023): Changing the offline training processes (requires re-training of the offline models), introducing additional models other than existing ones, and maintaining multiple buffers.\n\n4. The REDQ (Zhao et al. 2022) baseline is confusing to me. The original REDQ should be the paper [5]. I would suggest changing the name of that baseline.\n\n5. Is the following statement in  Section 4.3 correct? I don\u2019t think any of [1, 2, 3, 4] is using ensemble.\n> almost all previous O2O studies take advantage of certain kinds of ensemble learning \n\n5. Many citations are styled incorrectly and are difficult to read \u2013 \\citep{} should be used instead of \\citet{}. See the official formatting instructions below. Additionally, several papers are cited multiple times, such as Ball et al. (2023a and 2023b) and Zheng et al. (2023a and 2023b).\n>When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \\citet{} (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis using \\citep{} (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d)\n\n6. The style of Figures 7 and 8 is broken. They override the text above, and it's also confusing that there are three figures accompanied by only two captions.\n\n\n\n[1] Nair et al., AWAC: Accelerating Online Reinforcement Learning with Offline Datasets, 2020\n\n[2] Zheng et al., Online Decision Transformer, 2022\n\n[3] Wu et al., Supported Policy Optimization for Offline Reinforcement Learning, 2022\n\n[4] Nakamoto et al., Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning, 2023\n\n[5] Chen et al., Randomized Ensembled Double Q-Learning: Learning Fast Without a Model, 2021"
            },
            "questions": {
                "value": "1. How is the offline pre-training phase performed? Is the proposed method simply pre-trained using TD3+BC, or is the ISMAQ loss also combined?\n\n2. In Figure 12, it appears that the offline pre-trained performance of ISMAQ and TD3+BC should be comparable. However, in Figure 5, there is a significant difference in the initial performance between ISMAQ and TD3+BC_to_TD3. What\u2019s the reason for this discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4209/Reviewer_74i3"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4209/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828014306,
        "cdate": 1698828014306,
        "tmdate": 1699636387692,
        "mdate": 1699636387692,
        "license": "CC BY 4.0",
        "version": 2
    }
]