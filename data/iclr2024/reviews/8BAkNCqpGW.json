[
    {
        "id": "y1bUJP7cgh",
        "forum": "8BAkNCqpGW",
        "replyto": "8BAkNCqpGW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_4mko"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_4mko"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically studies the problem of offline policy optimization in POMDPs from a confounded offline data dataset, following the line of previous works on confounded POMDPs and policy gradient in MDPs. The core contribution is a deconfounded policy gradient ascent algorithm (Algorithm 1) based on proximal causal inference with theoretical guarantees. There are also numerical demonstrations to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The confounded offline POMDP setting itself is a well-motivated problem, and based on existing works it is natural to ask how to learn the optimal history dependent policy with computational efficiency. The authors answer this question by looking into the policy gradient style method which has not been explored in this literature.\n2. The idea of using bridge functions (originated from proximal causal inference) to identify not only the policy value $\\mathcal{V}(\\pi_{\\theta})$ but also the policy gradient $\\nabla_{\\theta}\\mathcal{V}(\\pi_{\\theta})$ is new.\n3. The theoretical derivations of the policy gradient identification is novel and is of independent interest to future research in confounded offline RL area.\n4. The theoretical results are self-content and sound."
            },
            "weaknesses": {
                "value": "1. From my viewpoint, by looking into the previous line of works, the main theory (Sections 6.1 & 6.2) of this work is mostly based on **(i)** the theoretical understanding of statistical analysis for using minimax estimator to solve bridge functions in confounded offline POMDP settings, e.g., [1, 2, 3]; **(ii)** the analysis of global convergence of policy gradient ascent methods in standard MDP settings, e.g., [4, 5]. So the technical contributions of the main theory part are somehow weakened given these prior works.\n2. As stated in 1., a consequence is that the theoretical assumptions regarding the policy gradient analysis (Section 6.2) are mostly adapted directly from those for MDP setups. How to understand these assumptions in POMDP settings with a history dependent policy class is less discussed.\n\n**References:**\n\n[1] Bennett, Andrew, and Nathan Kallus. \"Proximal Reinforcement Learning: Efficient Off-policy Evaluation in Partially Observed Markov Decision Processes.\" *Operations Research* (2023).\n\n[2] Shi, Chengchun, et al. \"A Minimax Learning Approach to Off-policy Evaluation in Confounded Partially Observable Markov Decision Processes.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n[3] Lu, Miao, et al. \"Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes.\" *The Eleventh International Conference on Learning Representations*, 2023.\n\n[4] Agarwal, Alekh, et al. \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift.\" *The Journal of Machine Learning Research* 22.1 (2021): 4431-4506.\n\n[5] Liu, Yanli, et al. \"An Improved Analysis of (Variance-reduced) Policy Gradient and Natural Policy Gradient Methods.\" *Advances in Neural Information Processing Systems* 33 (2020): 7624-7636."
            },
            "questions": {
                "value": "1. Continued from Weakness 1., I would appreciate it if the authors could highlight more on the technical contributions behind the main theory, especially when compared with the previous line of works listed.\n2. Continued from Weakness 2., it seems that the paper does not contain any discussion of a concrete policy class example. I think this is important since we are now dealing with history-dependent policy class which is different from previous MDP policy gradient problems. How does the dependence on history change (or not change) the difficulty of doing policy gradient and why? It would be great if such discussions can be included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Reviewer_4mko"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698560080693,
        "cdate": 1698560080693,
        "tmdate": 1700683229570,
        "mdate": 1700683229570,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s5bvUaY00d",
        "forum": "8BAkNCqpGW",
        "replyto": "8BAkNCqpGW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_KDFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_KDFK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a policy gradient method tailored for confounded POMDPs with continuous state and observation spaces in the offline learning context. The authors present a novel method for non-parametrically estimating any history-dependent policy gradient in POMDPs using offline data. They employ a min-max learning procedure with general function approximation to estimate the policy gradient through solving a sequence of conditional moment restrictions.\n\nThe authors provide a finite-sample, non-asymptotic bound for the gradient estimation. Using the proposed gradient estimation method within a gradient ascent algorithm, the paper demonstrates the global convergence of the algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses a challenging problem on policy gradient methods for POMDPs in the offline setting with continuous state and observation spaces. This is a novel contribution to the best of my knowledge; most existing work on policy gradient methods has been centered around fully observable environments. \n\n- The paper\u2019s contributions seem to be significant with the global convergence result of the algorithm to find the history-dependent optimal policy. Additionally, the identification result for non-parametrically estimating any history-dependent policy gradient under POMDPs using offline data is a unique contribution."
            },
            "weaknesses": {
                "value": "- The paper introduces a significant number of symbols and notations, which might overwhelm readers, especially those who are less familiar with the topic. To improve clarity, the authors could consider providing a table or appendix that lists all the symbols used along with their definitions. Additionally, they might simplify the notation where possible and ensure that each symbol is clearly defined upon its first use. For example, the notation seems to be quite heavy at the end of Page 5 where $\\mathcal{Z_t}$ and $\\mathcal{W_t}$ are introduced.\n\n- The full coverage assumptions stated in Assumption 4(a) and Assumption 5(a) are indeed common in offline RL literature, but they bring about challenges and potential limitations to the proposed method in the paper. \n\n1. **Full Coverage Assumption (Assumption 4(a))**: This assumption, which requires that the offline distribution $P_{\\pi_b}$ can calibrate the distribution $P_{\\pi_\\theta}$ induced by $\\pi_\\theta$ for all $\\theta$, is strong and might not always be satisfied in practical scenarios. In real-world applications, especially in domains like healthcare or finance, obtaining an offline dataset that sufficiently covers all possible actions and states can be impractical due to ethical, logistical, or financial constraints. The paper could improve by discussing the potential implications of this assumption, providing guidance on how to assess whether this assumption is reasonable in a given setting, or suggesting alternative approaches if the assumption is not met.\n\n2. **Optimal Policy Coverage (Assumption 5(a))**: This assumption requires that the optimal policy $\\pi^*$ is covered by all the policies in the class. While this condition ensures that the policy class is rich enough to contain the optimal policy, it might be too restrictive in practice."
            },
            "questions": {
                "value": "How might the proposed method be adapted or extended to accommodate partial coverage assumptions, and what would be the technical challenges associated with such an adaptation? Could you provide insights or discuss potential ways for relaxing the full coverage assumptions while maintaining the theoretical guarantees of the method?\n\n\n================================================\n\n**After Rebuttal:**\n\nThank you for the detailed response. I feel like my concerns have been addressed by the authors' response, and I would like to raise my score to 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Reviewer_KDFK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596130819,
        "cdate": 1698596130819,
        "tmdate": 1700605723505,
        "mdate": 1700605723505,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T6WBMk4qkN",
        "forum": "8BAkNCqpGW",
        "replyto": "8BAkNCqpGW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_YGPH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_YGPH"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a policy gradient method for confounded partially observable Markov decision processes (POMDPs) in the offline setting with novel gradient identification and estimation. Also, a theoretical analysis of the suboptimality of the proposed method is provided. Finally, numerical experiments are conducted to evaluate the performance of the algorithm."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The gradient identification proposed is new to confounded POMDPs, and the policy gradient based on that is complimented by strong theoretical guarantees. The statistical error of the gradient estimation and the suboptimal of the obtained policy are both discussed with a comprehensive analysis. Also, The theoretical results are also complimented by experimental results."
            },
            "weaknesses": {
                "value": "While I typically do not complain about the empirical results of a theory paper, I do expect that the authors could show how to implement such estimation and algorithm on real practical RL problems."
            },
            "questions": {
                "value": "1. In the third paragraph of Section 3, does the definition of history $\\mathcal{H}_{t}$ lack the subscript on $\\mathcal{O}$ and $\\mathcal{A}$?\n2. The notation $Z_{t}$ has already been defined in Section 3; however, another $\\mathcal{z}$ is used in Section 5 as additional notation. Are they the same things?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Reviewer_YGPH",
                    "ICLR.cc/2024/Conference/Submission7564/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698724661402,
        "cdate": 1698724661402,
        "tmdate": 1700794282400,
        "mdate": 1700794282400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8jGlXbwRYV",
        "forum": "8BAkNCqpGW",
        "replyto": "8BAkNCqpGW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_5Pwh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7564/Reviewer_5Pwh"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel policy gradient method for confounded partially observable Markov decision processes (POMDPs), and proves it converges to global optimal under certain assumptions. Their method estimates the policy gradient using bridge functions calculated from offline data, which is adopted from min-max estimator of [Dikkala et al. (2020)]. The paper provides a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class. Additionally, the authors show the global convergence of the proposed algorithm in finding the history-dependent optimal policy under certain technical conditions. This work is claimed to be the first to study the policy gradient method for POMDPs under the offline setting.\n\nThe paper also discusses the challenges in studying policy gradient methods in confounded POMDPs under the offline setting, such as the bias in estimation due to unobserved state variables, the need for function approximation in continuous spaces, and the challenge in achieving global convergence for finding the optimal policy.\n\nThe authors contribute by proposing a policy gradient method with both statistical and computational guarantees, establishing a non-asymptotic error bound for estimating the policy gradient, and providing a solution for global convergence in POMDPs."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "**Dsiclaimer:** I should first note that the results of this paper are super technical. A proper review of this article needs a full working week of my time, which clearly I couldn't put. I tried going through Appendix A and B, but even then I cannot say I understood completely.\n\nDespite these challenges, it is evident that the results derived in this paper solid. Establishing any form of convergence for the Policy Gradient (PG) algorithms within the context of POMDPs is immensely valuable to the community. Furthermore, their method for gradient estimation in this study presents itself as a potentially advantageous tool in its own right."
            },
            "weaknesses": {
                "value": "Given the technical nature of this paper, I am compelled to express my reservations about the suitability of the ICLR conference as the platform for its publication. I believe that this work might find a more fitting home in a scholarly journal, where reviewers are afforded ample time to thoroughly validate the results presented. Additionally, the attempt to condense the material into a 9-page format has significantly hindered its readability. In particular, the contents of Appendix A are critical enough that they warrant inclusion (or partial inclusion) in the main body of the text."
            },
            "questions": {
                "value": "[Vlassis et. al., 2012] have proved that finding a stochastic controller of polynomial size that achieves a certain target, is an NP-hard problem. Optimizing policy for confounded POMDPs is even more challenging. While [Vlassis et al., 2012] does not directly contradict this paper, it does raise questions about the real-world applicability of the assumptions made herein. Is there a possibility to provide examples that provide lower bounds? [Agrawal et. al.] provided an example demonstrating the necessity of distribution mismatch coefficient (which I presume it isn't very difficult to have a similar one for POMDPs). With this work, I also love to see more about computational complexity (and polynomial / NP-hardness / ...) of specific classes of POMDPs.\n\nI must note that I am unable to specifically identify any of the assumptions as unreasonable, but I guess Assumption 2 is the most critical assumption. \n\nReferences:\n- Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy\ngradient methods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research, 22(1):4431\u20134506, 2021.\n- Vlassis, Nikos, Michael L. Littman, and David Barber. \"On the computational complexity of stochastic controller optimization in POMDPs.\" ACM Transactions on Computation Theory (TOCT) 4.4 (2012): 1-8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7564/Reviewer_5Pwh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7564/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698893293081,
        "cdate": 1698893293081,
        "tmdate": 1699636915438,
        "mdate": 1699636915438,
        "license": "CC BY 4.0",
        "version": 2
    }
]