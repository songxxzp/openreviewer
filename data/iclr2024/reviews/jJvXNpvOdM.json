[
    {
        "id": "tOIAP0Xck5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_eRZf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_eRZf"
        ],
        "forum": "jJvXNpvOdM",
        "replyto": "jJvXNpvOdM",
        "content": {
            "summary": {
                "value": "The paper proposes a hierarchical task planner equipped with several proposed components for room rearrangement for user-defined goal states.\nSearch Network exploits LLMs to query possible receptacles where unseen objects may be present.\nThe graph-based state representation encodes the objects' spatial relationships and their distances for the current and goal states in the form of graphs.\nThis is later used for Deep RL based Planner trained by the proposed cluster-biased return reward decomposition.\nFor the evaluation, the paper introduces a new benchmark, RoPOR, for room rearrangement that addresses blocked goals and swap cases, with newly introduced metrics that mainly measure agents' efficiency.\nThe proposed method outperforms the baselines in their empirical validations by noticeable margins."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Tackling the blocked goals and swap cases is well-motivated and sounds sensible. Addressing them seems to be an important problem.\n- Exploiting prior knowledge encoded in LLMs for possible target receptacles looks reasonable.\n- Exploring an end-to-end framework for room rearrangement for user-defined goal states is intriguing."
            },
            "weaknesses": {
                "value": "- In Search Network, it is unclear why we need the \"two-staged\" approach: 1) filter out some implausible receptacles and 2) obtain the most plausible one. Why not just use SCN alone to get the most plausible receptacle, as the implausible receptacles should result in low scores and thereby be not chosen, consequently?\n- The graph-based state representation requires shortest-path computation for all fully connected edges, but this seems quite computationally heavy, especially when we have a large number of nodes, leading to a drastically increasing number of edges.\n- The comparison with some baselines seems unfair. For example, Weihs et al. and Gadre et al. do not use depth maps as input while the proposed method does, but they are compared in a single table. In addition, the authors utilize additional training datasets (Sec. 3.1).\n- Some new metrics are introduced to measure agents' efficiency but they look a bit similar to SPL in navigation literature, which basically penalizes an agent's success rate by the length of trajectories it took so far. Similarly to the introduced metrics, as agents take more steps for rearrangement or search, SPL penalizes the success rates more, accordingly.\n- The environments used in the proposed benchmark look quite \"clean.\" It seems that we have objects only related to rearrangement tasks, as illustrated in Figure 16 and the supplementary video. This looks quite far from practical scenarios as we usually have many objects inside rooms.\n\n\\* Minor\n - It might be better to divide a result table (e.g., Table 1) into one for the main results and the other for the ablation study for better readability."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Reviewer_eRZf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697388424651,
        "cdate": 1697388424651,
        "tmdate": 1699636108938,
        "mdate": 1699636108938,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XMZD1oGcQ4",
        "forum": "jJvXNpvOdM",
        "replyto": "jJvXNpvOdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_JNsv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_JNsv"
        ],
        "content": {
            "summary": {
                "value": "The authors tackle the problem of object re-arrangement. To define the goal state, the agent is able to explore the room in its goal state and build up its own (graph-based) internal representation of the room. Then, at test time, the objects in the room are shuffled around such that they can be occluded or hidden in receptacles. The job of the agent is to put the room in the desired goal state as efficiently as possible. To do this, the proposed method keeps track of which objects have been seen and which haven't. A search network predicts probable locations of unseen objects. Both the current scene and goal scene are encoded as graphs, with objects as nodes and geodesic distances between objects as edges, and this scene is encoded with graph networks. Lastly, they use Q-learning with a proxy reward network to train the planner. To this end, the authors also build a dataset to train the search network and the graph network, in addition to contributing a new benchmark in Ai2Thor (RoPOR). The proposed approach significantly outperforms the baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors provide a new benchmark for object re-arrangement in AI2Thor. The new dataset supports swap cases (objects' positions that are swapped compared to the goal scenario) and blocked goals (ie. there is an object occupying the goal position of another object).\n* Extensive supplementary video and supplementary material\n* Modified RL training approach seems to outperform other approaches in that it converges substantially faster.\n* The approach significantly outperforms the baselines."
            },
            "weaknesses": {
                "value": "* Convoluted approach that is a bit hard to follow, especially because many details are omitted in the main paper. It would also be helpful to more clearly define the task setup (inputs and outputs and their explicit representations).\n* Not obvious why the SRTN needs to be learned instead of manually defined.\n* Manual heuristics (where objects are unlikely to be found) and GT information (e.g. odometry, collision vector, etc.) is required.\n* More details on the reward design in the main paper would be appreciated\n* Critical ablations are not in the main paper.\n* There is already an existing dataset (and associated metrics) for the same setup in Sarch et al. (Sec. 4.5 of their paper). While there are some deviations between the proposed dataset and the existing dataset, it still makes sense to benchmark the proposed approach against the already existing dataset that has baselines already benchmarked against it. Is there a reason this wasn't done? The setup of the existing seems like a subset of the proposed dataset's setup, so it seems doable.\n* Somewhat similar to Sarch et al.\n    * Both use 2D and 3D representations of the environment\n    * Both use object detector + a search network to guide exploration\n    * Both represent the scene as a graph on top of which they perform inference\n\nMinor comments:\n* Some strange wording such as contributing to the \"research fraternity\", misspelling such as \"detetector, \"Paramater\", etc.\n* Assumes perfect motion and manipulation\n* Goal definition of having the goal scene already set up and utilizing an exploration stage is a bit impractical, as it requires setting up the goal scene every time it is changed. Also the robot needing to explore the goal state takes time compared to, say, language defined goals."
            },
            "questions": {
                "value": "* What are the failure cases? Does the agent ever get stuck in a loop?\n* Are duplicate objects handled (e.g. multiple sponges in a scene)? Are these included in the test data, and how does the agent perform under these conditions?\n* SRTN requires manually defined rules and heuristics about where objects are unlikely to be found (e.g. cup in bathtub). Is the network unable to learn these probabilities automatically?\n* Does the SRTN need to be learned via the MLP? It seems like the probabilities can be pulled directly from the data without learning. We could simply build a hardcoded table of probabilities. For example, the probablity of finding a sponge in the sink can be hardcoded to 0.80. Is there a reason why that number would change? What is the benefit of learning here?\n    * It is odd that we are trying to learn where objects are likely to be, but the training dataset is designed such that the authors \"ensure a random distribution of object placements\" (Appendix C.1).\n* How does the agent go from the output of the planner to discrete motions (rotate, move forward, pick up, place)?\n    * How is navigation performed when going from point A to point B? Is it assumed that this problem is solved?\n* There is already an existing dataset (and associated metrics) for the same setup in Sarch et al. (Sec. 4.5 of their paper). While there are some deviations between the proposed dataset and the existing dataset, it still makes sense to benchmark the proposed approach against the already existing dataset that has baselines already benchmarked against it. Is there a reason this wasn't done? The setup of the existing seems like a subset of the proposed dataset's setup, so it seems doable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Reviewer_JNsv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698715464645,
        "cdate": 1698715464645,
        "tmdate": 1699636108858,
        "mdate": 1699636108858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6jJwMQzmsZ",
        "forum": "jJvXNpvOdM",
        "replyto": "jJvXNpvOdM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_YNy7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1796/Reviewer_YNy7"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the task of visual room rearrangement where an agent must first explore a 3D scene autonomously to map its content before objects in the scene are moved around. The agent must then, in a second phase, re-organize objects whose position has been changed compared with the initial scene. The authors propose a series of contributions to both evaluate methods, with a new benchmark and metrics, and improve autonomous agents\u2019 overall performance and efficiency, with different architectural and training contributions.\n\nFirst, the paper introduces a new Search Network whose goal is to predict the position of unseen objects after the room has been untidied. The claim is that such a search process could leverage prior common sense to find candidate receptacles more efficiently. Thus, the Search Network is composed of a large language model (LLM) that incorporates prior knowledge about the relationships between objects and receptacles.\n\nAnother contribution is a hybrid action space Deep RL agent that tackles both object search and rearrangement. The state space of this Deep RL agent is a graph representation of both the initial scene (known as goal state) and the current scene (known as current state) to provide information about the position of objects. Finally, the paper proposes a proxy reward network predicting a dense reward signal to facilitate the RL training of the policy.\n\nThe method is compared with different baselines on a new benchmark, RoPOR, and additional metrics, evaluating the efficiency of taken paths, are considered."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* **S1**: The paper tackles an important and challenging task, i.e. visual room rearrangement, and motivates its different contributions based on important considerations about the task.\n\n* **S2**: This work proposes many different contributions, both from an evaluation point of view (benchmark, metrics) and from an architectural and training points of view.\n\n* **S3**: The method is compared against different relevant baselines and shows a promising gain in performance.\n\n* **S4**: Many qualitative videos are presented in the Supplementary Material, helping to better compare methods by visualizing their behavior."
            },
            "weaknesses": {
                "value": "* **W1**: **[Major]** How can the Search Network leverage prior knowledge as the room is untied? More specifically, if object shuffling/placement is done randomly, shouldn\u2019t there be no prior remaining? Indeed, in phase 2, objects are placed in locations were there shouldn\u2019t be (so that the agent can re-organize them). It is hard for me to understand how, in this case, the Search Network can learn anything meaningful. A result in the ablation study (Table 2) seems to confirm this intuition: the performance of Ours-RS, where the Search Network is replaced with a uniform random sampling of the next receptacle to visit, is very close (1p Success Rate) to the performance of Ours. The difference does not seem to be significant enough to claim the Search Network does more than random search. In order to claim a gain, authors should report the mean and standard deviation over a few training runs (random seeds). The following could also be done by authors:\n    * **W1.1**: Reporting the performance of Ours-RS on the introduced RoPOR benchmark.\n    * **W1.2**: Comparison of the Search Network with another simple baseline: selection of the closest receptacle (with reported performance on both RoPOR and RoomR).\n    * **W1.3**: Provide more details about how objects are moved in phase 2: there should not be any prior remaining, and if there is, it might mean that the comparison is unfair with other methods because the authors\u2019 search model might have been trained to learn those \u201cshuffling priors\u201d while it is probably not the case of previous work.\n\n* **W2**: **[Major]** This comment is quite related to the previous one: as mentioned in the paper, the Search Network is finetuned to incorporate prior knowledge about object-receptacle relationships (see W1 about why I am not convinced any such relationship can be learned in the untidy scenario). Authors should still show the pre-training of the LLM brings a performance gain. What about the same LLM architecture initialized with random weights and trained as done in the paper?\n\n* **W3**: **[Major]** It is not clear to me how the ground-truth data to train the Sorting Network (SRTN) is generated. Could authors elaborate on this?\n\n* **W4**: **[Major]** One might argue that the Sorting Network only could be enough to predict the most likely object-receptacle pairs. Authors should provide an ablation study showing the impact of the additional Scoring Network.\n\n* **W5**: **[Major]** I would like authors to clarify the following points regarding the Proxy Reward network:\n    * **W5.1**: What is the interest of this Proxy Reward network? The paper mentions it is a way to predict a dense reward to train the RL agent. However, given a simulator, couldn\u2019t we simply compute a dense reward from privileged simulator information at training time?\n    * **W5.2**: What is the training ground truth for the Proxy Reward network?\n    * **W5.3**: What is the average return on the y-axis of Figure 3? Such return is indeed associated with a specific reward function: what are the terms of this reward function? Moreover, I would like the authors to provide more details about the comparison done in Figure 3 and thus the conclusions we can draw from it.\n\n* **W6**: **[Major]** The paper mentions the introduced method \u201cassumes the availability of perfect motion planning and manipulation capabilities\u201d. While this is a strong assumption, it does not outweigh the contributions in this work. However, an important question is: Are all the baselines this method is compared against also benefiting from the same assumption? Otherwise, this could be considered as an unfair comparison.\n\n* **W7**: **[Minor]** When introducing their SNS metrics, authors should cite *Anderson et al., On Evaluation of Embodied Navigation Agents* that introduced quite similar metrics such as SPL.\n\n* **W8**: **[Minor]** Paper citations are not properly inserted in the text. Authors should use parentheses (\\citep{} in Latex) when needed, and remove double citations (e.g. \u201cSarch et al. Sarch et al. (2022)\u201d)."
            },
            "questions": {
                "value": "All questions and suggestions are already mentioned in the \u201cWeaknesses\u201d section as a list of numbered points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1796/Reviewer_YNy7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1796/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771236904,
        "cdate": 1698771236904,
        "tmdate": 1700575063347,
        "mdate": 1700575063347,
        "license": "CC BY 4.0",
        "version": 2
    }
]