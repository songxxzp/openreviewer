[
    {
        "id": "bprylPlM8U",
        "forum": "CMzF2aOfqp",
        "replyto": "CMzF2aOfqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG"
        ],
        "content": {
            "summary": {
                "value": "Early stopping is one of the most prevalent approaches to select model. However, it requires additional validation data. This paper proposes a new method to early stop without using the validation data empirically."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It does not require additional validation data for model selection.\n- Experiments on various structures and settings."
            },
            "weaknesses": {
                "value": "- What authors proposed is only supported by the empirical result.\n- Following the question 2, more discussions may be needed for the related previous researches.\n- Since the metic is moveing average over k epochs, sensitivity analysis over k is needed."
            },
            "questions": {
                "value": "- It may not work well for extreme cases, e.g. class imbalanced dataset. Any solution for those settings? If not, some assumptions could be specified for the setting.\n- What is the difference between the previouse studies'[1,2] findings and what authors propose in section 3.2 (fitting\nmislabeled examples impairs the overall model\u2019s fitting performance)?\n- Will PC monotonically decrease before its local minima? In other words, are there no fluctuations? or should we need some threshold?\n- How will this pattern change when utilized with additional regularizations e.g. data augmentation? Will it be consistent or will it flutuate before it goes to local minima?\n- For Table 1 and Table 2, which algorithm is utilized (just Cross Entropy?)? If utilized with several algorithm managing noisy labels, how much different between best and label wave?\n- Want to see result on more noise condition for table 3 and 4.\n- How about on real noise, e.g. Clothing1M?\n- Will this criterion fit to another task, e.g. semantic segmentation?\n\n[1] Wei, J., Liu, H., Liu, T., Niu, G., Sugiyama, M., & Liu, Y. (2022, June). To Smooth or Not? When Label Smoothing Meets Noisy Labels. In International Conference on Machine Learning (pp. 23589-23614). PMLR.\n\n[2] Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., & Liu, Y. (2020, October). Learning with Instance-Dependent Label Noise: A Sample Sieve Approach. In International Conference on Learning Representations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_D1rG",
                    "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697548541379,
        "cdate": 1697548541379,
        "tmdate": 1700726513457,
        "mdate": 1700726513457,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g5N1fHyRD5",
        "forum": "CMzF2aOfqp",
        "replyto": "CMzF2aOfqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an interesting method to perform early stopping in case of label noise without needing a validation set. The writing is good and the method provides some insights. However, there are concerns on the applicability of the method and the experiment set up is insufficient to evaluate the effectiveness of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The method is interesting and has some insights.\n2. The paper is well written, and everything is presented nicely. \n3. Good results are shown on certain noise datasets."
            },
            "weaknesses": {
                "value": "1. My biggest concern is the applicability of the method. Currently it\u2019s not clear the method works well on what scenarios. \n2. Experiment evaluation is far from sufficient and the setup can be improved."
            },
            "questions": {
                "value": "1. Regarding applicability, my intuition is that the method only works well when there is \u201csignificant\u201d amount of \u201crandom\u201d label noise in the training set. \u201cRandom\u201d is because the method relies on that the model fits simple patterns first and then learn random patterns from the noise. What if the label noise also only include simple patterns? E.g. black donkeys are mostly labeled as horses? The method also requires significant amount of label noise so that the model prediction fluctuate to a degree to be detected by the method. This is also reflected by the fact that experiments only considers >20% noise.  Can the authors provide more insights into this through discussion or experiments?\n2. Only datasets with synthetic noise are considered. How does the method work on real datasets with real-world noise?\n3. The baselines seem to use a noisy validation set, and evaluation is done on a clean test set. It makes more sense split the clean test set to create a clean validation set or to create a clean validation set from the training set. This is because we want to ensure the validation set has a same distribution as the test set, i.e. to be clean.\n4. What happens if the amount of label noise is less than 20%? Including label noise from 0-20% can help better understand the method.\n5. In the motivation of not using a validation set is that using a validation set reduces training set size and thus decreases performce, by this motivation, the method is targeting domains with limited amount of training set. How small the dataset size should be when it\u2019s preferred to consider this method? Could you provide some analysis on this?\n6. In order to show the method\u2019s effectiveness, more datasets from diverse domains should be considered."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "see questions"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_R2TX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698335332055,
        "cdate": 1698335332055,
        "tmdate": 1700605128175,
        "mdate": 1700605128175,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "daVtRF6F72",
        "forum": "CMzF2aOfqp",
        "replyto": "CMzF2aOfqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies an important topic of learning against noisy labels. Specifically, the authors mainly focus on how to automatical detect the transitioning point for early stopping, i.e., from fitting to clean to fitting to noise. There are two proposed key metrics so-called \"stability\" and \"variability\", and and the method uses \"prediction change\" as the mean of detecting early-stop point. The results show that the method can detect the point accurately by showing the test accuracy difference compared to that obtained from the global maximum point."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I felt there are multiple strengths of this work.\n* n-depth analysis: This work provides phase 1 to phase 3 analysis to understand how the DNN learns knowledge from noisy data.\n* Well-defined metric: Several useful metrics are proposed: prediction changes, stability, and variability.\n* The paper is well organized and easy to read."
            },
            "weaknesses": {
                "value": "There are several weaknesses in this work, which may be useful to polish the paper.\n* **Missing important reference:** I know references that are highly related to this work but unfortunately not mentioned and compared. These two papers also tackled exactly the same point and mentioned similar intuitions on what the best early stopping point is. These papers are worth mentioning and comparing. \n[1] How does early stopping help generalization against label noise, arXiv 2019\n[2] Robust learning by self-transition for handling noisy labels, KDD 2021\n\n* **Unclear setup for practicality:** Detecting an early stop point is very important in the industry, especially when using strong regularization techniques together. For example, in the computer vision domain, using Mixup (or Cutmix, etc), Batch Norm, Dropout, and other architecture-specific regularization (e.g., stochastic depth for Vision Transformers) is a must-need. These kinds of strong regularization obviously change the learning behavior of DNNs like the training and testing curves. For the complete study toward practical methods, these recipes of training should be considered altogether, or theoretical support is needed.\n\nThese two major issues contribute the most when determining my review score."
            },
            "questions": {
                "value": "Please address the two major weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concern was detected."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission32/Reviewer_LTbz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698739707556,
        "cdate": 1698739707556,
        "tmdate": 1700552130852,
        "mdate": 1700552130852,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QTioTnlsUJ",
        "forum": "CMzF2aOfqp",
        "replyto": "CMzF2aOfqp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission32/Reviewer_8gHt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission32/Reviewer_8gHt"
        ],
        "content": {
            "summary": {
                "value": "In the face of label noise, this publication presented an early halting technique using the Label Wave approach. Although the finding in this publication is intriguing, it only makes a little contribution to label noise bias."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This publication presented learning perplexing patterns, a transitional stage in learning with noisy labels."
            },
            "weaknesses": {
                "value": "The cifar10/100 dataset is utilized in this studies; however, real-world datasets such as webvision and food101 should be employed to confirm the efficacy of the suggested approach. I'm interested in seeing these outcomes.\n\nIn order to determine whether the suggested method chooses the best classifier, I would like to examine the maximum test accuracy during the training phase. \n\nGiven that the focus of this research is label noise, studies should compare the state-of-the-art techniques currently used for label noise learning, like DivideMix[1], ELR[2], AugDesc[3] and so on.\n\n[1] Li, Junnan, Richard Socher, and Steven CH Hoi. \"Dividemix: Learning with noisy labels as semi-supervised learning.\" arXiv\npreprint arXiv:2002.07394 (2020).\n\n[2] Liu, Sheng, et al. \"Early-learning regularization prevents memorization of noisy labels.\" Advances in neural information\nprocessing systems 33 (2020): 20331-20342.\n\n[3] Nishi, Kento, et al. \"Augmentation strategies for learning with noisy labels.\" Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2021."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission32/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698770541747,
        "cdate": 1698770541747,
        "tmdate": 1699635926338,
        "mdate": 1699635926338,
        "license": "CC BY 4.0",
        "version": 2
    }
]