[
    {
        "id": "PUamtgv6WM",
        "forum": "Sgvb61ZM2x",
        "replyto": "Sgvb61ZM2x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_PZS3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_PZS3"
        ],
        "content": {
            "summary": {
                "value": "The authors combine the node perturbation (NP) algorithm with an activity decorrelation method and better gradient estimation to improve NP's performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper does a good job introducing note perturbation (NP), along with its motivation and caveates. \n2. The combination of the decorrelation method and NP is novel and speeds up training in some cases.\n3. These ideas might be applied in other settings where learning is easier; e.g. weight perturbation was applied to finetuning LLMs https://arxiv.org/pdf/2305.17333.pdf"
            },
            "weaknesses": {
                "value": "My main issue with the paper is that NP still performs very poorly, even with decorrelation (NP-D), and doesn't match backprop. The results do not justify the last line of the abstract, which says\n> significantly enhances performance of NP learning making it competitive with BP.\n\nFig. 3 shows that with early stopping NP-D can slightly outperform NP, but the gap between the best performance for those two methods is smaller than the gap between BP and NP-D. Fig. 5 looks a bit more favourable, but NP-D still performs much worse than BP. Both figures show experiments on shallow networks, so it's reasonable to expect the gap will widen for deeper networks due to NP incorrectly estimating gradients.\n\nIn addition, both Fig. 3 and Fig. 5 show that NP-D results in a very large, much larger than for ND, generalization gap.\n\nThe experiments are also rather limited: it's only two very similar datasets, and only shallow architectures. Convnets are also not used for cifar10 -- a few conv layers trained with BP can perform much better than ~56% with small MLPs, so seeing how NP-D performs there would be helpful."
            },
            "questions": {
                "value": "Why is everything trained for so many epochs? 2000 epochs is a lot, and backprop should normally converge to very good performance on cifar10 within the first few dozen epochs. But here it takes hundreds. \n\nBeginning of page 2:  weight propagation (WP) -> perturbation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698428900310,
        "cdate": 1698428900310,
        "tmdate": 1699636308927,
        "mdate": 1699636308927,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DO8BDDLBoF",
        "forum": "Sgvb61ZM2x",
        "replyto": "Sgvb61ZM2x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_W31p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_W31p"
        ],
        "content": {
            "summary": {
                "value": "This paper is about the node-perturbation approach to training artificial neural networks. The main claimed contributions of the paper are 1) reformulating the node perturbation (NP) methods in terms of directional derivatives (NP-iterative, section 2.1.2), and providing a simpler implementation (NP-activity, section 2.1.3) and 2) a decorrelation method (sec 2.2) applied on the input of each layer to speed up the convergence of training of node perturbation methods. The theory is then tested on classification tasks (CIFAR-10 and 100) with multi-layer perceptrons of three hidden layers (Sec 3.1, 3.2), and on convolutional networks (sec 3.3). An extension is also proposed based on two noisy forward passes instead of one clean and one perturbed pass (sec 3.4)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Strengths:\n\n- **Clarity**. The paper is clearly written, and easy to follow. The concepts are well-explained, and the paper hierarchy is clear.\n\n- **Ambition**. The paper is ambitious about the proposed method and test it only on more difficult benchmarks, CIFAR-10 etc, but it turns out to be also a weakness in this case (see weaknesses).\n\n- **Importance**. The problem of finding alternatives to BP is important to reduce the energy cost of deep learning."
            },
            "weaknesses": {
                "value": "I see two main weaknesses:\n\n- **Awareness of prior work**. The contribution of linking perturbations of forward passes to directional derivatives is not really a new contribution, it has been done before in e.g. [1], which I expected to see cited since it is extremely related. The activity-based updates can be thought of as a finite difference implementation of this approach. More generally, linking forward pass to directional derivatives means dealing with forward-mode differentiation, while BP is reverse-mode. Reverse mode is efficient because of the regime many-parameters/scalar loss, and forward mode is all the more inefficient if done exactly, hence the use of noise to probe multiple directions at once and make it more efficient, at the cost of more variance. It would be nice if the authors place their work clearly in this picture, and clarify their contributions after better literature review.\n\n\n- **Overfitting**. The decorrelation approach proposed in this work does help for training accuracy, but leads to massive overfitting compared to BP (Fig 3 and 5). I think this is very concerning since achieving small training error is not the hard part of training neural networks, what really matters is the held-out error. It is known e.g. that BP with SGD has an implicit bias that enables the network to generalize [2], and somehow the decorrelation method suppresses this bias, which is interesting. I think this important point should be addressed/solved by this submission. Maybe the authors should try to vary the amount of decorrelation to see whether it affects overfitting. For now, the claim in the abstract that decorrelation makes NP learning competitive with BP is not backed by the data given the overfitting.\n\n[1] Baydin, At\u0131l\u0131m G\u00fcne\u015f, et al. \"Gradients without backpropagation.\" arXiv preprint arXiv:2202.08587 (2022).\n\n[2] Chizat, Lenaic, and Francis Bach. \"Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.\" Conference on Learning Theory. PMLR, 2020."
            },
            "questions": {
                "value": "Questions:\n\n- How do the angles reported in Fig 2 evolve during learning? Do they become more aligned or less aligned?\n\n- Why use the squared error loss when doing classification? Why not the cross-entropy loss? I can understand why the authors wanted to only use CIFAR-10/100 because these tasks are more difficult than MNIST or Fashion MNIST, but in this case it could be valuable to add experiments on MNIST to see whether the findings are the same regarding overfitting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Reviewer_W31p"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698576909796,
        "cdate": 1698576909796,
        "tmdate": 1699636308845,
        "mdate": 1699636308845,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JUcB1XaQLX",
        "forum": "Sgvb61ZM2x",
        "replyto": "Sgvb61ZM2x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_G9as"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_G9as"
        ],
        "content": {
            "summary": {
                "value": "The paper studies node perturbation (NP), i.e. a category of learning algorithms that are different from BP, potentially more biologically-plausible, and possibly more suitable for efficient learning hardware, especially for noisy electronics. The work here formulates NP into a version that approximates directional derivatives with respect to a layer's activation, and also combines it with a method that decorrelates the activity among the neurons of a layer, resulting in faster and more stable learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work is in an interesting field, namely that of alternative learning algorithms to backpropagation.\n\nThe paper is quite nicely written, making the reasoning for the design choices easy to follow.\n\nThe results seem potentially very useful to NP, stabilising and speeding up the learning process."
            },
            "weaknesses": {
                "value": "The work is only relevant to the niche of node perturbation, as the paper fails to contextualize the work more broadly, with literature review, experimental comparisons or otherwise.\n\nThe paper does not cite the state of the art (SOTA) within the NP subfield (Mengye Ren et al., ICLR 2023).\n\nThe methods that were used in that NP SOTA have not been incorporated in the experiments here, so it is hard to evaluate whether the advantages seen here could be combined with the previously seen progress in NP.\n\nThe authors do not cite any of the multiple other bio-plausible alternatives to BP than NP. The SOTA in such alternatives actually outperforms NP and is arguably more plausible and suitable for hardware (Journ\u00e9 et al., ICLR 2023).\n\nThe test accuracies reported here are significantly lower than that SOTA, and tests in more advanced datasets than CIFAR 100 have not been performed.\n\nThe paper's abstract concludes \"making it competitive with BP\", but this seems heavily exaggerated based on the actual demonstrations."
            },
            "questions": {
                "value": "Could the authors comment on the efficiency and the biological plausibility of the decorrelation method?\n\nSimilarly, how could the other aspects of the method be implemented in the brain?\n\nFor example, how could the \"clean\" (sic) version of the output be obtained in the brain, as it assumes the absence of noise?\n\nDoes the claim of the paper about suitability of noisy hardware hold, under the assumption of a clean forward pass?\n\nHow plausible is it biologically that for each training example, multiple forward iterations are needed before the full update? It seems that hundreds of such iterations were necessary in the simulation.\n\nHow energy efficient would this be in a hardware implementation? It seems to me that it would also increase the energy consumption by orders of magnitude.\n\nThe paper mentions that the convolutional architecture was very shallow. Why couldn't it be scaled up?\n\nI could not find an experiment showing the impact of the decorrelation method on the results. Was it implemented in all versions of NP experiments?\nAlso, was it (or any other decorrelation method) used in experiments with backprop? Otherwise, could the authors comment on whether comparisons are fair?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698962997975,
        "cdate": 1698962997975,
        "tmdate": 1699636308785,
        "mdate": 1699636308785,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FObzgWr6FW",
        "forum": "Sgvb61ZM2x",
        "replyto": "Sgvb61ZM2x",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_hNrk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3548/Reviewer_hNrk"
        ],
        "content": {
            "summary": {
                "value": "The authors present an approach to improve the efficiency and stability of node perturbation (NP) as an alternative, biologically plausible method to backpropagation (BP) for credit assignment in deep neural networks. Their main contributions include reframing NP in terms of computing directional derivatives and proposing a decorrelation procedure to mitigate input bias. Specifically, they suggest using a decorrelation matrix initialized as the identity matrix and subsequently updated to reduce correlations. For gradient approximation, they perturb one layer at a time using a noise vector, measuring the resultant change in loss, and thereby estimating the gradient respective to each layer."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**Novelty in Approach**: The manuscript's primary strength lies in its enhancement to NP by directional derivatives calculated from neural activity measurements. This offers a fresh lens to potentially harness NP more effectively in deep learning contexts -- perhaps also spur methods utilizing a hybrid NP_noise and NP_activity update.\n\n**Clarity in Gradient Approximation**: The iterative node perturbation method introduced offers a transparent and logical pathway to approximate gradients. By perturbing one layer at a time and measuring the resultant loss changes, the approach provides both intuition and efficacy in gradient estimation."
            },
            "weaknesses": {
                "value": "**Learning Rate Optimization**: The approach to learning rate optimization might introduce inadvertent biases. By optimizing the learning rate specifically for NP and then using this optimized rate for BP without independent optimization, the results might not reflect the accurate comparison with BP. This is further underscored by references like Hiratani et al., which hint at nuances between NP and BP at different learning rates.\n\n**Concerns Over Biological Plausibility**: The introduction of the decorrelation mechanism, while effective, does raise eyebrows regarding its biological realism. If the overarching goal of the research is to be in harmony with biologically plausible learning paradigms, then not sure how the decorrelation step fits in.\n\n**Ambiguous Visual Interpretation**: Interpretation of Figure 2 seemed ambiguous to me. With training curves of three methods appearing quite similar without decorrelations, the clarity of conclusions derived becomes muddied.\n\n**Scope Limitation with Network Types**: The research seems to overlook certain types of networks, notably the absence of comparisons for NP iterative in convolutional networks on established benchmarks like CIFAR100.\n\n**Formatting and Implementation Details**: The paper contains formatting discrepancies, notably in equation numbering. Such inconsistencies can be distracting and hinder smooth comprehension. Additionally, the omission of granular details about the implementation, such as the underlying framework, time metrics, and codebase accessibility."
            },
            "questions": {
                "value": "1. Do the authors plan to release the code base? If so, could you please share the link to an anonymized repository?\n\n2. What exactly does figure 2 convey? The authors say \u201cranking\u201d is important here \u2013 for layer 2 NP iterative is slightly better, whereas for output NP activity is slightly better. Overall it\u2019s difficult to see what should be the conclusion since the training curves for these 3 methods without decorrelations is very similar. \n\n3. Grid search for learning rate optimization \u2013 lr is optimized for NP, and then BP is trained with this lr. Hiratani et al. have shown that NP approximates BP in low lr \u2013 for a fair comparison of the learning curves, BP curve should be for LR optimal for that? Also, the variance in performance with multiple seeds, across learning rates will be good to see. How does this scale with the network architectural parameters, such as the depth and width of a fully connected network?\n\n4. Hiratani et al. have also shown that NP is unstable in higher learning rates, did you observe \u201ccrashes\u201d in training \u2013 where the accuracy suddenly falls to chance? It would be interesting to see whether decorrelations help with this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3548/Reviewer_hNrk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3548/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699036378227,
        "cdate": 1699036378227,
        "tmdate": 1699636308690,
        "mdate": 1699636308690,
        "license": "CC BY 4.0",
        "version": 2
    }
]