[
    {
        "id": "NEaQd2FLiQ",
        "forum": "II0OENWgi8",
        "replyto": "II0OENWgi8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_XD7J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_XD7J"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an HRL algorithm for discovering skills based on the successor features. The paper claims to extend the traditional concept of goal to trajectory-based skill. The proposed algorithm relies on constrained optimization to balance maximizing the environment rewards and the skills' intrinsic rewards. The authors present result for continuous control task, showing both qualitative and quantitative results on the Brax physics engine."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors rely on constrained optimization to balance the extrinsic rewards and the intrinsic rewards. In practice this is implemented through a Lagrangian relaxation.\n- The experiments follow a rigorous protocol using multiple seeds (10)\n- The authors provide both quantitative and qualitative results\n\nUsually methods in HRL simply add the extrinsic reward to the intrinsic rewards without considerations to the stability and converge of the method. Constrained optimization offers a more rigorous way to do so. Moreover, the paper shows good scientific standards in the presentation of the empirical results, which has been a weakness for many recent papers in the field."
            },
            "weaknesses": {
                "value": "- One of the main claims of the paper is that SCOPE \"extends the traditional concept of goal to trajectory-based skill.\" This is simply wrong and clearly misrepresents a large body of literature on goal-conditioned RL and HRL in general\n- The authors compare to a few HRL algorithms that learn skills, these algorithms have their rewards defined through DIAYN. This is an outdated baseline that has been beaten numerous times in the literature\n- The qualitative results do not convey very effectively the diversity of the learned skills  \n\nThe claim that the proposed method extends the traditional concept of goal to trajectory level skill seems to me like a fundamental misunderstanding of what goals can represent in HRL. The idea that goals are associated with a particular state (meaning x,y,z positions) is a very narrow point of view of the numerous ways they are instantiated in the literature. Goals essentially depend on the goal space, which can encode any kind information we could imagine, including temporally extended features such as speed or feet contact rate.\n\nThe algorithm is compared to a variety of algorithms, but the HRL baselines essentially build their reward on DIAYN, which is not a good algorithm anymore. A recent state-of-the-art algorithm building on a quantity closely connected to the successor features is the idea of Laplacian based options [1]. A comparison to such a strong algorithm would be necessary to really attest to the algorithm's performance.\n\nThe qualitative experiments do not convince me that the algorithm learns diverse skills. Figure 2 is very poorly presented, how is the distance exactly calculated? What is the x axis? Why use a task dependent d_eval? It is very unclear how this exactly relates to diversity. Previous work [2] presents diversity in a better way and such visualization would help the paper a lot."
            },
            "questions": {
                "value": "How are the skills chosen exactly? Do they use the hand-defined features from Section 5.1 (feet contact, angle, ...)? How does the method avoid learning multiple times the same skill?\n\nWhy refer to DOMiNO as Reverse SMERL? DOMiNO presents strong results using methods that do not rely on DIAYN, this would be a much better comparison.\n\nMore details on Figure 5 would be needed.\n\n\"Solving Problem P1 amounts to maximizing the return while executing a desired skill\" This is in essence the main goal of this paper [3].\n\n\"SF captures the expected future states visited under a given policy\" This is not right, it is the expected future feature occupancy, not the state.\n\n\"DOMiNO learns policies that are not skill-conditioned, hence the skills are not controllable.\" This is not right. Conditioning a policy on a N possible goal is the same as to learning N different discrete skills. It is simply a question of architecture.\n\nDespite building skills on the SF, no mention of [4] is made throughout the paper, which is closely related.\n\n====================================================================================\n\n[1] Deep Laplacian-based Options for Temporally-Extended Exploration. Klissarov and Machado, 2023\n\n[2] Eigenoption Discovery through the Deep Successor Representation. Machado et al. 2018\n\n[3] Reward-respecting subtasks for model-based reinforcement learning. Sutton et al. 2023\n\n[4] Lipschitz-constrained Unsupervised Skill Discovery. Park et al. 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721267392,
        "cdate": 1698721267392,
        "tmdate": 1699636106681,
        "mdate": 1699636106681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YkDP6pdGxu",
        "forum": "II0OENWgi8",
        "replyto": "II0OENWgi8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_a9mP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_a9mP"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for simultaneously learning to solve a given task while also learning a diverse set of skills. Precisely, given the relevant features per state, the paper defines skills as the average of the set of features encountered by a policy. They then define a constrained optimisation problem where for each skill, an agent needs to find a policy that maximises the reward returns and whose average trajectory features converge to the given skill. They then solve a Lagrangian relaxation of the problem by combining several methods from prior works. They use universal value function approximations (UVFAs) to learn the skill-conditioned policies, universal successor features approximations (USFAs) to learn the skill-conditioned successor features, and model-based RL (DreamerV3) to improve sample efficiency. They theoretically show that a solution to the formulated problem exists under some assumptions, and empirically show that their practical algorithm (SCOPA) outperforms prior works in various continuous control tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is mostly well-written and investigates an important problem. The specific proposed approach for learning to solve a given task while learning diverse skills is novel and interesting.\n\n- The authors provide theory showing that under strong assumptions about the environment (mainly the existence of a policy that satisfies hypotheses 1 and 2 simultaneously), their formulated constrained optimisation problem is solvable.\n\n- I really liked the figures visualising the number and quality of skills successfully learned (like fig 3). They are a nice way of visualising the diversity and quality of learned skills. They also show that SCOPA outperforms the baselines in both diversity and quality.\n\n- The paper shows numerous empirical results comparing SCOPA with many relevant baselines in several continuous control tasks. Impressively, SCOPA is shown to outperform all these baselines in terms of the number/quality of skills learned and usefulness for downstream tasks."
            },
            "weaknesses": {
                "value": "**MAJOR**\n\n1- **Theory**:\n\n- It is not clear if the MDPs are assumed to be continuous, discrete, or finite for the theoretical results. This should be made explicit.\n\n- The motivation for the given definition of a skill is not given. It is not clear why it is an average instead of a sum or any other function over the sequence of features. It is also not clear why the average gives the same probability to each feature in the trajectory. Shouldn't it be an expectation over the sum of features? \n\n- The explanations for Proposition 1 and Algorithm 1 are scattered. This made hypothesis 1 of Proposition 1 especially hard to understand.\n\n2- **Baselines and empirical significance**:\n\n  - It is not clear if the 10 replications/runs used for each plot are different runs of each algorithm (e.g Algorithm 1 for SCOPA), or if they are using the same models trained from a single run. This should be clarified, as it affects the significance of the empirical results.\n \n  - The authors use MBRL (DreamerV3) to speed up the learning of their actors and critics, but do not do the same for the main baselines (SMERL, REVERSE SMERL, and DCG-ME). This makes the experiments extremely unfair towards the baselines. If the authors did not want to go through the additional effort of augmenting the baselines with DreamerV3, then I think they also shouldn't have augmented SCOPA with DreamerV3.\n  \n  - The authors used actor/critic networks with hidden layers of size 512 for SCOPA (and its variants), but only 256 for the main baselines (SMERL, REVERSE SMERL, and DCG-ME). The authors did not explain why they made this decision. Additionally, the number of iterations used for the main baselines (or possibly function evaluations for DCG-ME) was not stated. \n\n  - These make it hard to evaluate the significance of the empirical results, and I think it also puts into question the claims and conclusions based on them.\n\n3- **Evaluation Metrics**:\n  \n  - The *distance to skill* and *performance* metrics used are specific to the type of skills learned by SCOPA. Despite this, the authors spend a lot of time comparing their approach to the baselines using these metrics, which is extremely unfair to the baselines. Naturally, this made the baselines look extremely terrible, inconsistent with the great results shown in those prior works.\n\n  - Just like prior works, I think the authors should have focused more on the \"Using skills for adaptation\" experiments for the comparison with the baselines. These experiments demonstrate how useful the diverse skills learned by each method are for downstream tasks. The authors should include such experiments for a variety of downstream tasks (similar to prior works).   \n\n  - While the authors have attempted to make their evaluation metrics fair by modifying the main baselines (SMERL, REVERSE SMERL, and DCG-ME) to learn the same skills that SCOPA is trying to learn, I think this is extremely wrong. While giving the baselines the same additional priors as SCOPA is good, the authors shouldn't have modified the actual optimisation objectives or skills being learned them.  \n\n\n**MINOR**\n\n- The proposed approach relies on the state features being predefined/known.\n- The abstract and introduction could do a better job of explaining the contributions and what SCOPA is since I couldn't tell the difference between it and UVFAs or USFAs until Sec 3. \n- Some of the figures in the experiments section are too far from where they are referenced for the first time. E.g Fig 2 and 3.\n- There are a couple of typos or grammar errors throughout the paper. I have listed some examples here\n  - Page 6: There is no fullstop after the definitions of $r_{SMERL}$ and $r_{REVERSE}$.\n  - Page 5 \"critic training\": There is a \"where\" or \"and\" missing between the two equations."
            },
            "questions": {
                "value": "It would be great if the authors could address the major weaknesses I outlined above. I am happy to increase my score if they are properly addressed, as I may have misunderstood pieces of paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Reviewer_a9mP"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796121484,
        "cdate": 1698796121484,
        "tmdate": 1699636106613,
        "mdate": 1699636106613,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b0CnxNyd0h",
        "forum": "II0OENWgi8",
        "replyto": "II0OENWgi8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_9NVL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_9NVL"
        ],
        "content": {
            "summary": {
                "value": "This work presents SCOPA, an algorithm to solve tasks given by the task reward while learning skills based on successor features. In this work, a skill is represented as a mean vector of the features (for states or cumulants). The objective (P1) is to learn a skill-conditioned policy to maximize the task return w.r.t such that the mean of the future-occupant features becomes z, which is reformulated as the constraint optimization problem (P2). The authors show that this constraint can be modeled as learning successor features."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The problem is well formulated, and is of a very high significance: This work generalizes and extends goal-conditioned learning in combination with skill learning, and unifies the idea with a very novel, clear and efficient method. Unlike many prior works in (unsupervised) skill learning, this work considers maximization of task rewards and skill learning simultaneously which can result in learning useful skills for fulfilling tasks, which is often hard in unsupervised GCRL.\n\n\n- The method is very principled and beautifully designed without much hack or ad-hoc techniques. I find the use of SF (successor features) very interesting, and the flow of derivation gave me a lot of joy to read and learn about it. I haven't gone through the details of proof very thoroughly, but at a high level the idea of proof on the Proposition (Problem P1 -> P2) makes sense.\n\n\n- The algorithm also features intriguing and useful merits and controllability, including diversity and quality trade-off using a single hyperparameter.\n\n\n- SCOPA works very well on challenging locomotion tasks (including Ant and Humanoid) and their adaptation tasks (e.g. with hurdles, obstacles, and broken body) \u2014 prior methods usually have difficulties in learning meaningful skills in these domains. The qualitative results also look impressive.\n\n\n- The analyses and experiments provided in the paper are very comprehensive and of high quality."
            },
            "weaknesses": {
                "value": "- The skill space is a linear combination (span) of the feature space, i.e., a skill vector can be represented as only a linear combination of state observations. While this might be limiting in terms of how expressive or arbitrarily complex skill representation can be learned, it's not a major concern because the assumption is quite mild and also a crucial part to make the use of SF possible; in fact, rich skill spaces would be actually more difficult to learn than having such inductive biases.\n\n\n- To define a skill, so-called \"feature engineering\" is still needed. In practice, the feature seems to be designed carefully as a low-dimensional subspace of the observation (as low as 2~3 dimensional) in order to be useful. As noted in many unsupervised skill learning papers (namely, Gu et al., 2021, etc.), the method would not work very well if no domain knowledge about which feature spaces would be useful (e.g. velocity for locomotion tasks). This would be a minor limitation, but often in the robotics domain it can be seen as a reasonable assumption.\n\n\n- While the learning of SF is in principle model-free (although more technically speaking, it's somewhere between model-free and model-based learning), the method requires learning of a world model.\n\n- Figure 2: the x-axis of the plots lacks labels, difficult to figure out what they would exactly mean.\n- Proposition: it'd be good to mention what \\Psi(st, z) denotes here -- the successor feature."
            },
            "questions": {
                "value": "Sample efficiency -- how many samples is needed to learn successful skills and solve the task?  Would the method still work if the entire algorithm is trained without the use of model, in a model-free fashion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822703792,
        "cdate": 1698822703792,
        "tmdate": 1699636106541,
        "mdate": 1699636106541,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uEL7MxTDxv",
        "forum": "II0OENWgi8",
        "replyto": "II0OENWgi8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_SA5X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1773/Reviewer_SA5X"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Skill-Conditioned OPtimal Agent (SCOPA), which is a method for learning policies that are conditioned on also executing a skill using successor features. Skills are defined as achieving a mean feature vector across a trajectory. SCOPA takes the constrained optimization problem of maximizing reward while achieving a skill and casts it as an optimization problem with a Lagrange multiplier parameter lambda_L that trades off between performance and diversity.\n\nTo train policies that skill-conditioned (recall that skills are mean skills), which can be quite costly given the large space of state-skill pairs, a world model is trained using an extension of Dreamer-v3 which can generate training data for the actor-critic training.\n\nThe experiments compare SCOPA to related baselines in Walker, Ant, and Humanoid (using Google Brax). The experimental findings suggest that SCOPA is able to achieve skills (including those contrary to the main task), achieves better performance than the baselines, and are more adaptive to downstream tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "They use multiple baselines for the experiments. \n\nGiven their definition of skill, the introduced method seems like a reasonable approach to tackle the problem of achieving reward while learning skills.\n \nTheir method does seem to do better than the baselines and achieve desirable characteristics of a skill-learner (good performance, ability to learn skills, adaptability, etc.)"
            },
            "weaknesses": {
                "value": "There are several concerns I have with the paper. In particular, the lack of addressing the limitations of this paper is making me confused. I hope to gain some clarity on this.\n\nConcern - Definition of skill: I don't know if the authors introduced this notion of a skill in this paper, or if there is a precedent in the literature for this definition of skill. If the latter, it would be helpful to include a citation. If the former, I think there needs to be more explanation or justification as this is not intuitive to me and seems to be lacking in several ways. It seems the definition of skill is quite narrow, it defines skills as feature accumulation over an entire trajectory, which does not seem like an intuitive way to capture the notion of a skill in a general sense, and seems useful for a specific type of task (e.g., locomotion tasks as tested in this paper). For example, if the agent must move between points, and the climb a ladder, it already becomes unclear how this impacts how one should define skills.\n\nConcern - Reproducibility: Hyperparameter details are offered in the appendix, which may be enough to reproduce the work. There are indeed many moving parts, such as the actor/critic/world model/SF network, and plenty of hyperparameters that suggest this may not be easy to reproduce. Any thoughts on the authors' behalf regarding reproducibility would be appreciated.\n\nThe method seems to hinge on the availability of feature functions to extract relevant limitation. Not much time is spent discussing where this might come from. Is this a limitation? Can these feature functions be learned?\n\nBreadth of experiments: All of the experimental tasks are locomotion related, where there is some task uniformity (i.e., the agent is moving across some plane) that is aligned with the definition of skills. For example, if the agent's task was to walk and then climb a ladder, what skills would be defined/be appropriate? How would SCOPA perform?\n\nI like aspects of this paper, and my concerns may be from confusion or missing the point. However, as I have many questions, I am concerned about the clarity of the paper. If my questions and concerns can be addressed or clarified I may well change my score.\n\nMinor nits:\ntypo: \"dependant\" -> \"dependent\" in figure 2.\ntypo: \"the agent needs multiple timestep\" -> \"timesteps\"\ntypo: missing a period before \"For a fair comparison...\"\ntypo: \"extrinsinc\" -> \"extrinsic\""
            },
            "questions": {
                "value": "1. Will the code be released? While the hyperparameters are available, I do have reproducibility concerns. There are many moving parts (e.g., learning the model, the constrained optimization, etc.) that to the naked eye seems sensitive to implementation details and hyperparameters.\n\n2. Is this definition of skill commonly used? As I understand it is not. What was the reasoning behind this definition? If there is a citation for this definition of skill, that would be appreciated and helpful to the paper.\n\n3. I am confused about this description of distance to skill: \"To evaluate the ability of a policy to achieve a given skill z, we estimate the expected distance to skill, denoted d(z), with n = 10 rollouts while following skill z.\" What is the mathematical definition of distance to skill? Is this just Euclidean distance? If I'm missing the mathematical definition somewhere, I apologize.\n\n4. In the experiment section, we state: \"we combine with four different feature functions that we call feet contact, jump, velocity and angle.\" Where do these feature functions come from? \n\n5. Where does the delta in P2 come from? When you reformulate P2 into (1), where does the delta go?\n\n6.  In (2), which is used to train the network, delta' is needed to compute the targets for the cross-entropy loss. I checked the SCOPA hyperparameters in the Appendix and I couldn't find the values for delta'. Is the need for delta' lost somewhere?\n\n7. Typically in goal-conditioned RL, you have a goal, and the reward function that you optimize is associated to the goal. This paper states that the P1 generalizes goal-conditioned RL, but to me it is just a different objective. I.e., you still have a main task reward augmented by the skill/goal objective. Thoughts on this? Am I misunderstanding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1773/Reviewer_SA5X"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1773/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832511865,
        "cdate": 1698832511865,
        "tmdate": 1699636106451,
        "mdate": 1699636106451,
        "license": "CC BY 4.0",
        "version": 2
    }
]