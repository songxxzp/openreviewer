[
    {
        "id": "u1kNFzyZLB",
        "forum": "Ns8SXMJ2ic",
        "replyto": "Ns8SXMJ2ic",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_jfx3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_jfx3"
        ],
        "content": {
            "summary": {
                "value": "This paper benchmarks a variety of SPSA (and SPSA-esque) optimization algorithms on a suite of noiseless quantum machine learning tasks. The authors find that vanilla SPSA can work very well, although hyperparamter tuning is important (for SPSA as well as all optimizers)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- QML papers often go about selecting optimizers blindly, and it is always important to have a more rigorous and empirically informed approach to the problem\n- The paper is generally well written and understandable"
            },
            "weaknesses": {
                "value": "- Two sentences start with \u201cbeyond\u201d in the second paragraph (a minor point on readability)\n- Figure 1 should have the exact solutions as a horizontal line. This makes it easier to understand the scale of the error.\n- There are a few citations that this would benefit from using [1-3], some of which include randomised hamiltonians as a benchmark. \n- The core of my issue with this paper is that it doesn\u2019t do enough to build on previous work. The results are not new, SPSA as a good non-parameter shift gradient optimizer and the importance of hyperparameter tuning are both mentioned in [2]. Although more verification and more problem evaluation is always good, the results and methods are generally similar to a growing lineage of QML optimization benchmarks. \n- SPSA would benefit from more mathematical background and explanation when first presented.\n- This paper does not seem geared towards (a decent knowledge of QML is necessary for appreciation of the paper, and it does not make much of an attempt to familiarize people with it. This is not a slight against the paper a priori, just a specific concern with this venue), nor necessarily of great interest to, the ICLR community (which is largely dominated by classical ML researchers). \n- Given the known dependence of optimization difficulty of QML systems on circuit width and depth (even for non gradient based optimizers [4]), this paper would benefit from more consideration and analysis of these (since the size and widths seem picked somewhat arbitrarily). \n\nReferences:\n\n[1] Bonet-Monroig, X., Wang, H., Vermetten, D., Senjean, B., Moussa, C., B\u00e4ck, T., ... & O'Brien, T. E. (2023). Performance comparison of optimization methods on variational quantum algorithms. Physical Review A, 107(3), 032407.\n\n[2] Lockwood, O. (2022). An empirical review of optimization techniques for quantum variational circuits. arXiv preprint arXiv:2202.01389.\n\n[3] Lavrijsen, W., Tudor, A., M\u00fcller, J., Iancu, C., & De Jong, W. (2020, October). Classical optimizers for noisy intermediate-scale quantum devices. In 2020 IEEE international conference on quantum computing and engineering (QCE) (pp. 267-277). IEEE.\n\n[4] Arrasmith, A., Cerezo, M., Czarnik, P., Cincio, L., & Coles, P. J. (2021). Effect of barren plateaus on gradient-free optimization. Quantum, 5, 558."
            },
            "questions": {
                "value": "- Would the code be available if the publication is accepted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Reviewer_jfx3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698535953799,
        "cdate": 1698535953799,
        "tmdate": 1700630761821,
        "mdate": 1700630761821,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ytrpSP9CD3",
        "forum": "Ns8SXMJ2ic",
        "replyto": "Ns8SXMJ2ic",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_McZo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_McZo"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on benchmarking local zeroth-order optimizers for variational quantum algorithms. The motivation behind this paper is to provide benchmarks and insights for understanding the performance of different optimizers in variational quantum learning problems. In addition to random parameter initialization, the authors also randomize the parameterized circuit/ansatz and objective to create a more diverse and realistic benchmark. By conducting experiments based on randomized tasks, the study provides findings and insights into the behavior of various optimizers."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper investigates an important issue in quantum machine learning, exploring the performance of classical optimizers for variational quantum systems. It has done a number of numerical experiments to present the findings and observations."
            },
            "weaknesses": {
                "value": "- Lacks theoretical analysis and solid results to support the performance claims made for different optimizers.\n- The experiments conducted in this study are limited in scope and scale. The small scale of the experiments (few qubits) limits the generalizability of the results and makes it challenging to draw robust conclusions.\n- Lacks clarity in presentation. It is not easy to follow and understand the experimental setups (e.g., algorithm, number of qubits, which kind of simulation).\n- The tasks considered in this paper are not general enough.\n- Does not provide significant conceptual or technical contributions to quantum machine learning."
            },
            "questions": {
                "value": "- Considering the barren plateaus issue in many variational quantum algorithms, do the main results in this paper still hold? \n- What is the theory to guarantee that the randomized quantum circuit is random?\n- What are the parameterized circuits used in this paper?\n- What is the scalable analysis for the problem that the paper focuses on?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Reviewer_McZo"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698574397404,
        "cdate": 1698574397404,
        "tmdate": 1699636997717,
        "mdate": 1699636997717,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jDaqYaEoQv",
        "forum": "Ns8SXMJ2ic",
        "replyto": "Ns8SXMJ2ic",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_tcpu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_tcpu"
        ],
        "content": {
            "summary": {
                "value": "This paper benchmarks several SPSA-like (i.e., zeroth-order, local) optimizers on a randomized set of parametrized quantum learning problems (including Hamiltonian minimization and generative modeling). In each problem, the loss function is the expected value of a Hamiltonian. The authors focus on the noise-free setting and use the exact expected value in their experiments. Their findings are: (1) these optimizers are sensitive to hyperparameter tuning; (2) more elaborative optimizers are not generally better (the vanilla SPSA performs very well in most cases); (3) accelerated methods usually have a faster convergence rate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Compared with prior arts, this work focuses on the performance of zeroth-order local minimizes across a series of partially randomized tasks in quantum learning. This methodology provides new insights into translating existing knowledge to new application scenarios. Details of the experiment setting are discussed and they look reasonable to me. The experiment results are illustrated in two ways: (1) convergence/loss plots and (2) box plots showing the statistics of the end-results. Overall, the manuscript is organized and well-written. The technical claims and numerical findings are plausible."
            },
            "weaknesses": {
                "value": "I feel this work is way too empirical with limited theoretical insights. In Section 5, some numerical findings are interesting (even counter-intuitive). It would be better to motivate and explain a bit more in terms of optimization theory. For example, in the 1D Ising & 2D Heisenberg experiments (and also the two generative models), adamSPSA has a much slower convergence than the vanilla SPSA. This is a bit surprising because ADAM often has faster convergence compared to the vanilla gradient descent. Is this because of the noisy gradient estimation (SPSA uses a zeroth-order oracle to estimate gradient), or a unique behavior only for quantum Hamiltonian minimization problems? \n\nI also find that the 95% confidence interval in Figures 1 & 3 looks very thin. This is also a bit surprising because the optimization landscape for these problems should be highly nonconvex, and SPSA is a stochastic optimization algorithm. One explanation I could imagine is that the initial guesses are chosen in a small neighborhood in the parameter space so the loss curves have significant overlap. Or because the hyperparameter tuning only involves 3 random keys? Can you elaborate a bit more on this? Does this numerical methodology reflect the general landscape of quantum learning?\n\nThis work only studies the noiseless setting, while I feel the noisy setting is more relevant. Due to the Heisenberg limit, the expected value of the Hamiltonian can not be computed very accurately on a real quantum computer, unless the number of samples is large. On the other hand, SPSA is a zeroth-order optimization algorithm that is sensitive to random fluctuations in the function value. It would be more relevant to consider a benchmark with a reasonable noise model (or at least, an imperfect loss evaluation)."
            },
            "questions": {
                "value": "A few questions can be found in the \"Weakness\" section. I suggest the authors add some discussions (or experiments) regarding the noise setting. Some previous results suggest that SPSA-like optimizers are still robust in the presence of noise. How are these results connected with the current work? Also, it would be appreciated for the authors to interpret their numerical results in the context of the landscape of these quantum learning problems. In practice, the knowledge of the target learning problem usually sheds light on what optimizers we want to choose."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8066/Reviewer_tcpu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698610622714,
        "cdate": 1698610622714,
        "tmdate": 1699636997589,
        "mdate": 1699636997589,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l4Q7BbYlMo",
        "forum": "Ns8SXMJ2ic",
        "replyto": "Ns8SXMJ2ic",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_amNq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8066/Reviewer_amNq"
        ],
        "content": {
            "summary": {
                "value": "The paper conducts a benchmark of local zeroth-order optimizers in the realm of quantum information, encompassing a range of partially randomized tasks. Through the comparative analysis of seven optimizers, the authors reveal a noteworthy trend: simpler heuristic methods, with SPSA at the forefront, frequently outperform their more intricate counterparts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The submission conducts a systematic evaluation of local zeroth-order optimizers in quantum information, offering valuable insights that extend beyond specific quantum learning tasks. By conducting a comprehensive benchmark of seven optimizers across partially randomized scenarios, the authors provide practical guidance for using simpler heuristic methods like SPSA, to train variational quantum algorithms. Besides, the submission effectively communicates its motivation, ensuring clarity and comprehension for readers."
            },
            "weaknesses": {
                "value": "The submission appears to align with the benchmark paper category; however, it lacks key components that are typically expected in such a context. Benchmark papers typically aim to promote scalable, robust, and reproducible research. They often involve well-processed datasets, methods, and accessible results. Many benchmark papers also provide a dedicated website, offering documentation, example scripts, and a public leaderboard. Unfortunately, this submission falls short of meeting these essential criteria, as even the source code remains unreleased."
            },
            "questions": {
                "value": "The authors are encouraged to perform a comprehensive revision of their submission to ensure alignment with the formal benchmark paper format."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8066/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737043259,
        "cdate": 1698737043259,
        "tmdate": 1699636997456,
        "mdate": 1699636997456,
        "license": "CC BY 4.0",
        "version": 2
    }
]