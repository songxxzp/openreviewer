[
    {
        "id": "HJuvKCjWuv",
        "forum": "RBqowcUwFP",
        "replyto": "RBqowcUwFP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
        ],
        "content": {
            "summary": {
                "value": "In this work, the Authors propose a novel algorithm for discerning and characterizing multiple intents underlying natural behaviors. To this end, they combine the inverse Q-learning (a version of IRL) with the expectation-maximization algorithm used to delineate the intents. The Authors consider two types of dynamics in their framework: the Bernoulli process and the Markov process; they provide theoretical derivations and describe implementations for both. They then test their framework on a simulated task (a gridworld with 2 underlying intentions) where they show the framework\u2019s ability to recover the ground truth, and on an existing mouse dataset (a bandit reversal learning task) where they reconstruct and describe mouse intents."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A definite strength of this work is that it has been anticipated in the field. As a continuous-time model for multiple intent reconstruction has been developed by Ashwood et al (NeurIPS 2022; referenced in current manuscript), a question that has been repeatedly raised was about the discrete version of the framework. This question has become especially relevant after another work by Ashwood et al (Nat Neurosci 2022; also referenced in current manuscript) that used large-scale IBL data to propose that natural behaviors can be represented via an MDP featuring rapid progression of discrete states. This ICLR submission delivers on that expectation.\n\nThe model in the paper is well-founded; it features reasonable choices of the constituting algorithms (e.g., the Baum-Welch algorithm for discerning the intents).\n\nHaving the analysis for both simulated and animal data is a plus."
            },
            "weaknesses": {
                "value": "Along the same line with the strengths, I see the main weakness here in high similarity to Ashwood et al (NeurIPS 2022) work. The Authors mention in the Appendix that the aforementioned approach \u201cis limited to capturing continuous intra-episode variation of reward functions during navigation behavior, and difficult to adopt to other environments\u201d but, should that be true, that requires further substantiation. The data analyses offered in this paper seem to mainly serve as a proof of principle for the proposed model.\n\nOverall, the work is nicely done and well-timed; my only concern is that the high similarity to prior literature determines the work\u2019s novelty which may be insufficient for ICLR. With that said, I\u2019m open to comments by the Authors, other Reviewers, and the Area Chair in that regard."
            },
            "questions": {
                "value": "-Introduction: wouldn\u2019t it make more sense to introduce your work via Ashwood et al, 2022a and 2022b papers? I feel like this way the reason for the development of your model and the comparison to the existing state-of-the-art would be more transparent.\n\n-Page 5 under Equation 10: what does Delta Z mean? Is it supposed to reflect the available transitions?\n\n-Page 6 under Figure 3: why is it necessary to punish the types of reward irrelevant to the intentions? A more natural way seemingly would be to set them equal to zero. I assume this natural way hasn\u2019t worked out for some reason?\n\n-Figure 4A: why does the LL in the training curve drop? That is unlikely to be explained by overfitting as suggested in the text.\n\n-Page 7 bottom line: \u201cAlthough model performance continued to improve slightly with more latent states, we will focus [\u2026] on [\u2026] 3 states\u201d. Wouldn\u2019t it be easier to make this argument by using the Bayesian Information Criterion instead of the pure NLL to choose K? This way one can arrive at a principled number of intents that very well may turn out to be equal to 2.\n\n-Figure 5C. Following up on my previous point, this figure leaves me with the impression that the third intent is just spurious (not stable; immediately reverses to the first intent). Could you please comment on why you consider it important?\n\nMinor comments:\n\n-Figure 3: the overlap of the crosses (x) and dots ( . ) is hard to follow. Could you please use an alternative way to represent this data?\n\nI\u2019d also suggest tuning down a couple of literature-related claims:\n\n-Page 2: \u201c[IRL\u2019s] adoption as mathematical behavior models in neuroscience research has been relatively limited\u201d. I had another impression \u2013 it seems to be an up-and-coming tool, as exemplified by some awesome works from Jon Pillow\u2019s and Xaq Pitkow\u2019s groups.\n\n-Page 2: \u201c[our method presents a [\u2026] framework for characterizing the delicate balance between exploration and exploitation [\u2026] which constitutes a [\u2026] comparatively understudied aspect within the realm of neuroscience.\u201d I\u2019d say that, first, there\u2019s a huge spillover from the machine learning field of intrinsic motivation (a.k.a. an internal reward for exploration); many of these works claim biological plausibility. There are some other nice works, e.g. Pisupati et al (eLife 2021) and references therein that directly address the issue. There\u2019s also lots of work on Bayesian optimality that study the deviations from optimal exploitation to account for the environmental dynamics, e.g. Yu and Cohen (NeurIPS 2008).\n\n-Page 4: \u201cIn behavioral neuroscience, it is commonly considered that animals alternate between multiple intentions under the Markov property\u201d. The entire reason why the Ashwood et al (Nat Neurosci 2022) paper cited there emerged is because that\u2019s _not_ how people used to characterize natural behaviors. This is reflected literally in the first sentence of the said paper. While this new work has gotten substantial traction in the field, I wouldn\u2019t say that that new way to model data has completely wiped out the conventional approach.\n\n________________________________________________________________________________\npost-rebuttal:\n\nI would like to thank the Authors for their clarifications. I appreciated the fast, detailed responses.\nPosting my final response here as, at this time, I cannot otherwise make it visible to the Authors.\n\nI believe that the updated manuscript is a more solid, transparent, and substantiated work.\nThe most interesting finding to me is that new priors here allowing for abrupt changes of goal maps, enabled by the novel problem formulation, optimization objective, and solver, were more consistent with the mouse decision-making data in the maze experiment than DIRL (the previous SOTA), rendering the proposed model important. I would also like to thank the Authors for the clarification that the choice of different smoothness prior in DIRL would not necessarily be able to recover the same dynamics, necessitating the formulation of the problem in the way proposed here.\n\nDespite the similarity to prior work, this is an important and interesting result. I increased my score to reflect it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_yMx7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698438021056,
        "cdate": 1698438021056,
        "tmdate": 1700763910844,
        "mdate": 1700763910844,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ah3boUB4Ps",
        "forum": "RBqowcUwFP",
        "replyto": "RBqowcUwFP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
        ],
        "content": {
            "summary": {
                "value": "This work considers an inverse reinforcement learning model (IRL) with latent discrete intention variables. Using an inverse Q-learning and EM-based approach, they perform inference on these latent variables at each time based on either generalized Bernoulli or Markovian dynamics, learning both the transition dynamics between intentions (in the Markov case) and their corresponding reward functions. Experiments involve recovery on simulated data and from behavior in mice performing a two-alternative forced choice task with randomly changing reward structure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Intriguing generalization of inverse RL methods to neuroscience.\n- Well-motivated incorporation of latent drives."
            },
            "weaknesses": {
                "value": "- The fitted model is somewhat simplistic. Latent states are assumed to be multinomial or Markov, but the most plausible biological assumption would be that transitions between drives also interact with reward/satiety/recent history.\n- There are only two experiments: one on simulated data (where it is) compared to IAVI and IQL but not the Ashwood et al. or other similar models that might be applicable. Similarly, the mouse behavior is quite limited in terms of the need for RL. Again, the Ashwood Nature Neuro paper or the Ebitz, Albarran, and Moore (2018) provide fairly flexible models that are likely to capture the data as well. Given the synthetic data, one would have expected a more challenging task here as a target for IRL."
            },
            "questions": {
                "value": "- Is it possible to incorporate some recent reward history into the transition structure? Since the inference algorithm is EM, will any EM-compatible latent variable model work, in principle?\n- Where are the bottlenecks for the method in terms of inferential complexity? Is the limiting factor the IAVI regression (i.e., the size of the tabular problem) or the EM complexity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_o7yT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671475715,
        "cdate": 1698671475715,
        "tmdate": 1699636863866,
        "mdate": 1699636863866,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fg1GL3MGlr",
        "forum": "RBqowcUwFP",
        "replyto": "RBqowcUwFP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_bHzC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_bHzC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an expectation-maximization approach for multi-goal IRL based on the inverse Q-learning IRL method. The approach involves clustering trajectories into multiple intentions and independently solving the IRL problem for each intention. The authors evaluate their algorithm using both simulated experiments and real-world mice data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The problem of multi-goal IRL is highly relevant and its applications to cognitive science have been sparse in the past. I therefore particularly appreciate the application to the cognitive science domain to interpret real mice data.\n\nThe introduction of a multi-goal approach based on the inverse Q-learning algorithm seems novel and holds the promise of potentially outperforming previous multi-goal IRL methods.\n\nThe paper is well-structured and easy to follow, contributing to its readability and comprehensibility.\n\nThe algorithm's application to real mice behavioral data demonstrates its practical applicability in real-world scenarios."
            },
            "weaknesses": {
                "value": "The most significant weakness of the paper is the lack of discussion regarding related work on multi-goal IRL. Despite the existence of numerous prior works in this field (e.g., [1-6]), the paper does not reference or discuss any of them. The absence of a comparison with existing methods raises questions about the true novelty and contribution of the proposed approach. The paper should explicitly highlight what sets its method apart and how it compares to the existing literature. Especially works [4-6] previously approached the multi-goal IRL problem with an expectation-maximization approach, even though inverse Q-learning was not used as backbone algorithm.\n\n[1] Dimitrakakis, C., & Rothkopf, C. A. (2012). Bayesian multitask inverse reinforcement learning. In Recent Advances in Reinforcement Learning: 9th European Workshop (EWRL), pp. 273-284, Springer\n\n[2] Gleave, A., & Habryka, O. (2018). Multi-task maximum entropy inverse reinforcement learning. arXiv preprint arXiv:1805.08882.\n\n[3] Babes, M., Marivate, V., Subramanian, K., & Littman, M. L. (2011). Apprenticeship learning about multiple intentions. In Proceedings of the 28th international conference on machine learning, pp. 897-904\n\n[4] Choi, J., & Kim, K. E. (2012). Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Advances in neural information processing systems, vol. 25., pp. 305-313\n\n[5] Michini, B., & How, J. P. (2012). Bayesian nonparametric inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases: European Conference, (ECML PKDD), pp. 148-163, Springer\n\n[6] Bighashdel, A., Meletis, P., Jancura, P., & Dubbelman, G. (2021). Deep adaptive multi-intention inverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD), pp. 206-221, Springer"
            },
            "questions": {
                "value": "How does your approach compare to previous multi-goal IRL methods, especially those mentioned in the references [1-6]? It is crucial to provide a detailed comparison to establish the uniqueness and advantages of your proposed method in light of the existing literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779476880,
        "cdate": 1698779476880,
        "tmdate": 1699636863690,
        "mdate": 1699636863690,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NPjDl6dhKk",
        "forum": "RBqowcUwFP",
        "replyto": "RBqowcUwFP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1"
        ],
        "content": {
            "summary": {
                "value": "This method, LMV-IQL, seeks to extend a class of IRL algorithms to the case of multiple intrinsic rewards, applied to behavioral modeling in neuroscience. They first identify each intention / reward and then solve for each. They demonstrate their method on both simulated and experimental datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The text is well written, particularly when defining theorems and the algorithmic steps. It would be a definite strength to extend IRL approaches to the regime of multiple unknown (intrinsic) reward functions or internal motivation states."
            },
            "weaknesses": {
                "value": "Some of the figures need additional details/components. E.g., Figure 1, 2 need color scalebars, Figure 3 would benefit from some explanation of the legend (where are the red and blue squares?), the colors on the state labels in Figure 4C are unnecessary and uncorrelated with the colors in the legend, ... \n\nDefinitions of the comparison methods were weak. For example, 'IAVI was further extended to the sampling-based model-free Inverse Q-learning (IQL) algorithm' with no citation or explanation of how the authors of this paper implemented those algorithms, is insufficient. \n\nSimilarly, the primary metric, EVD, is cited but not defined. \n\nThe authors only show an improvement over IAVI and IQL, and do not compare these other methods (including LV-IQL, which performed the same on the simulated dataset) in the experimental dataset."
            },
            "questions": {
                "value": "The authors motivate their method as extending beyond a single reward function, but then apply it only to the case of 2-3 rewards/intentions/states. Can this be extended easily to more than a small number?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7249/Reviewer_M3j1",
                    "ICLR.cc/2024/Conference/Submission7249/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801635049,
        "cdate": 1698801635049,
        "tmdate": 1700711023895,
        "mdate": 1700711023895,
        "license": "CC BY 4.0",
        "version": 2
    }
]