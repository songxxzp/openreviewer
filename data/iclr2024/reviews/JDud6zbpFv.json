[
    {
        "id": "24aLWJYwqc",
        "forum": "JDud6zbpFv",
        "replyto": "JDud6zbpFv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_VT9L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_VT9L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the cooperative coevolution for QD methods on reinforcement learning problems. The major contribution is claimed to be the first time use of cooperative coevolution, and this can help improve the sample efficiency."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The experiment setup is considerate, and the results are promising. The main idea of using cooperative coevolution into reinforcement learning is easy to understand for addressing large-scale search space."
            },
            "weaknesses": {
                "value": "This work is not the first time of applying cooperative coevolution to reinforcement learning, even restricted in the quality-diversity like evolutionary algorithms, see the following reference:\n[1] Evolutionary reinforcement learning via cooperative coevolutionary negatively correlated search.\nSo the related claims should be revised.\nGiven the above work, the contribution of this work may be limited."
            },
            "questions": {
                "value": "What is the relationship between large-scale search space and the sample efficiency? Why address the large-scale space with cooperative coevolution can help improve the sample efficiency of QD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Reviewer_VT9L"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678226432,
        "cdate": 1698678226432,
        "tmdate": 1700579974934,
        "mdate": 1700579974934,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AIvFYASkHS",
        "forum": "JDud6zbpFv",
        "replyto": "JDud6zbpFv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_CbDL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_CbDL"
        ],
        "content": {
            "summary": {
                "value": "In that paper, the authors introduce Cooperative Coevolution QD (CCQD), a novel method to evolve populations of diverse and high-performing RL agents, the so-called QDRL problem. The paper aims to improve the sample efficiency of current methods by disentangling the feature extraction learning, also called representation learning part and the decision learning part of the policies during training. This done by introducing two separate populations that are co-evolved with the representation population being much smaller than the decision one. This is motivated by the intuition that most of the representation knowledge can be re-used by all the agents while the diversity and performance is more likely to emerge from the decision parts. The authors propose to split the policy networks layers to define these populations where the first group of layer and the last group of layers would respectively define the representation and decision populations. \n\nThe framework is general and most of the design choices used by the authors are common in that literature including the type of archives and variation operators. The authors introduced two changes compared to others such as PGA-MAP-Elites in the way the critics are handled.\n\nTheir method is then benchmarked on the QDax suite against several state-of-the-art methods (PGA-ME, QDPG, ME and PBT-ME (SAC))."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall I find that this paper addresses an interesting topic in the QDRL literature which is well motivated. The approach is sound and I share the same intuition that these methods would strongly benefit from different treatments of the representation learning and decision learning parts. Using co-evolution to do so is novel and I find it also elegant.\n\nThe paper is overall well written and does a good job at setting up the context, the problem and presents a good overview of the literature on this topic. The technical aspects are also well explained and I found it easy to understand how the algorithm works and is implemented.\n\nRegarding the experimental section, I find the ablation study informative and I like the introduction of the Atari Pong environment which is not very common in that field but I think very useful to show the versatility of the approaches."
            },
            "weaknesses": {
                "value": "However, I have strong doubts about the larger benchmark where CCQD is compared on the QDax suite to QDRL competitors. While I appreciate that the authors considered a large number of environments and repeated each experiment five times, I have serious doubts about the showed result as they don't match the ones reported in recent works such as PBT-ME (ICLR last year) produced on the exact same benchmark and for which the code is available. \n\n- My main concern is the number of time steps considered and the maximum fitness obtained in several environments.  For instance in Ant-Uni, the authors show training evolution over 5e7 time steps for a maximum fitness reached of 1500. This fitness corresponds to the robot barely moving at low speed. In that environment, asymptotic fitnesses are more in the scale of 6000-7000, see for instance PBT-ME, which corresponds to the ant robot actually running. Same observation in HalfCheetah where the authors report asymptotic performance of 1500 while usual values are around 6000-7000 too. It is even worse in Walker2d-Uni where reported performance is 800 compared to 4000 in state-of-the-art works. This indicates that the shown results correspond to very early stages of training, before actual convergence, and thus cannot allow to draw any meaningful conclusion.\n\n- The authors claim \"PBT-ME is capable of adjusting the algorithm\u2019s hyperparameters while optimizing, resulting in the need of a significant amount of computational resources\" while of the main point and contribution of the paper is to show that tuning the hyperparameters can be done with the same budget, and even in some cases improve the efficiency. The reported conclusions and performance ranking to other methods are not in the line with the one published while both the code and benchmark are open. I think this might come from the fact that all the runs were stopped too early and I would expect the authors to find back the original paper claim if they wait long enough to see the beginning of training convergence. \n\n- Moreover, despite these limitations, in most cases the sample efficiency gain that is shown remains limited and I would expect more from an efficient representation/decision decoupling. In that context it is also hard to say if the small obtained gains compared to PGA-ME for instance are due to the co-evolution strategy being proposed or to some other minor changes like the PeVFA update of the critic or the fact that the population critic has been replaced by one critic per agent.\n\nAll in all, I am happy to see such works submitted at ICLR and hope there will be more in the future as I believe QDRL is a very exciting field and might play an important role in the future of AI methods. I also like the motivation and find the method original. The paper is also pleasant to read. However, as per my points above, I think that the main experiments of the paper show strong weaknesses which makes it unconvincing. I would be happy if the authors could mitigate these points by notably running all methods longer, until observing the beginning of asymptotic convergence, and also by confirming findings of previous works. In case the authors find different results of previous works, I would expect a strong discussion about what in their opinion could trigger these changes. If the authors were able to strengthen their experimental setup I would be happy to see this work published at ICLR but in the current state I think the work is not ready."
            },
            "questions": {
                "value": "**Minor points**:\n\n- (manon et al.) ---> (flageat et al.) page 7\n\n- It might be interesting to cite \"The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers\" as the work is recent and was published at GECCO and here is also this idea of compressing the feature extraction within a single model, even though both works share different final motivations.\n\n- I would have liked to see more discussion about the co-evolution aspect of the method, the different existing strategies and the motivation behind the design choices (e.g. the split of the policy layers). Maybe it would be worth it to move part of the other technical details that are common to most QDRL works to the supplementary to focus more in the main core on the aspects that make this work unique."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4340/Reviewer_CbDL"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778423960,
        "cdate": 1698778423960,
        "tmdate": 1700584149575,
        "mdate": 1700584149575,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mc5ZDoR88k",
        "forum": "JDud6zbpFv",
        "replyto": "JDud6zbpFv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_zwMj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4340/Reviewer_zwMj"
        ],
        "content": {
            "summary": {
                "value": "**Problem Setting**\n\nThis work aims to tackle the methods of quality diversity, where we desire to find a diverse set of solutions to an optimization problem. Specifically, we try to make QD methods more computationally efficient.\n\n**Novel Idea**\n\nThe idea is to factor the population space of all neural network parameters into two spaces, the first layers and the later layers. These correspond to a representation layer and a decision layer.\n\nThe representation part has a smaller population and is implemented as a list. The decision portion contains a much larger population and is impllemented as a grid.\n\nParent selection is performed uniformly. Parts are selected independently from the decision and representation portions, then evaluted jointly. The mutation operator is implemented through a combination of an evolutionary operator and direct optimization through a critic. There is a unique critic for each representation part. Survivors are selected through the vanilla MAP Elites style approach where parts are first segmented into cells, and then chosen based on fitness.\n\nExperiments are presented on the QDax suite. Comparisons are presented against map elites, along with modern versions. Experiments show strong results and CCQD consistently outperforms the prior methodology."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a simple method to reduce the sample complexity of quality diversity methods applied to neural networks. The idea is to partition the parameter space into a representation layer, which consists of the first neural network layers, and the decision layer, which is the later laters. This is a clean formulation and is communicated in an understandable manner. The significance of this work is in its further exploration as a method to factorize evolutionary methods over neural networks.\n\nAblations provide answers to questions such as what the qualitative impact of the representation vs the decision parts are."
            },
            "weaknesses": {
                "value": "The setting provided places an emphasis on sample complexity, without considering the asymptotic performance of the methods. It would be strengthening to confirm that the CCQD method reaches the same or better asymptotic performance as the other methods. \n\nIn addition, a description of the diversity criteria used to separate the symbols in the QD algorithm would improve clarity."
            },
            "questions": {
                "value": "What is the motivation between using both an evolutionary operator as well as a learned actor critic update?\n\nIt would be interesting to see if there is a structured way to decompose the neural network into factors to be used for QD. It would also be informative to see the affect of QD on the parameters themselves, i.e. are decision policies trained via QD more robust to noisy inputs because they are trained on a variety of representations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4340/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698787657936,
        "cdate": 1698787657936,
        "tmdate": 1699636404054,
        "mdate": 1699636404054,
        "license": "CC BY 4.0",
        "version": 2
    }
]