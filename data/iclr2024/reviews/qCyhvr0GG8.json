[
    {
        "id": "FsgRKp3nFU",
        "forum": "qCyhvr0GG8",
        "replyto": "qCyhvr0GG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_751B"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_751B"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an unsupervised video object representation learning framework, namely VONet. Taking an image-based method as per-frame baseline, the proposed VONet builds temporal attention to learn correspondence in high-level space, resulting in significant improvement against previous video-based methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The results look good with large improvement against previous methods."
            },
            "weaknesses": {
                "value": "1. The current title is too broad, which makes the readers hard to understand the specific contributions of this paper.\n\n2. The motivation of each contribution of the proposed method is not clearly clarified in Introduction, especially the difference or new insights w.r.t previous  methods.\n\n3. It is hard to refer to the expression of the symbols in Figure 3."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698480458648,
        "cdate": 1698480458648,
        "tmdate": 1699636087008,
        "mdate": 1699636087008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fgxbFsdXwn",
        "forum": "qCyhvr0GG8",
        "replyto": "qCyhvr0GG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_x2B3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_x2B3"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for unsupervised video object learning, VONet. The uniqueness of VONet is primarily the parallel attention process that is capable of generating attention masks for all slots with the consideration of temporal consistency which utilizes context propagation across time and object-wise sequential VAE framework. Their results on 5 MOVI datasets show that the proposed method significantly outperforms previous methods as measured by two popular metrics namely FG-ARI and mIoU."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method addresses unsupervised video object learning which can be paralleled and temporally consistent given the slot numbers. Both temporal consistency and parallel segmentation instead of a sequential learning are crucial for learning objects in a video. This paper solves these very important first steps."
            },
            "weaknesses": {
                "value": "As already mentioned in the paper when the predefined slot numbers is larger than the actual number of objects, an unwanted side-effect if over-segmentation. Can the authors provide any insights on how to potentially combine multiple slots to prevent such overfitting?"
            },
            "questions": {
                "value": "Can the authors provide any insights on how to potentially combine multiple slots to prevent such overfitting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Reviewer_x2B3"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650637241,
        "cdate": 1698650637241,
        "tmdate": 1700872401663,
        "mdate": 1700872401663,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KpBK9zt4aP",
        "forum": "qCyhvr0GG8",
        "replyto": "qCyhvr0GG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_9HJm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_9HJm"
        ],
        "content": {
            "summary": {
                "value": "VONet introduces two key innovations: 1) a parallel attention network that employs the same U-Net architecture simultaneously on K context inputs, and 2) an object-wise sequential VAE framework, aimed at improving the temporal consistency in unsupervised video object learning. Notably, VONet significantly outperforms the baseline by a substantial margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Within this paper, the \"object-wise sequential VAE\" is introduced, which is a novel and highly effective representation for exploring temporal dependencies in video frames. \n2. The experimental results in this paper are impressive, surpassing previous methods by a large margin."
            },
            "weaknesses": {
                "value": "1. In order to show the advantages of parallel processing, it would be beneficial that a comprehensive latency/accuracy comparison with MONet could be provided in the single-frame scenario.\n\n2. It would be beneficial to have a comparative analysis between the object-wise sequential VAE and other temporal dependency networks, particularly the memory network mentioned in reference [1]. These methods excel in modeling temporal information and such a comparison would greatly enhance the paper's overall comprehensiveness.\n\n[1] XMem: Long-Term Video Object Segmentation with an Atkinson-Shiffrin Memory Model ECCV 2022"
            },
            "questions": {
                "value": "Please see Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1582/Reviewer_9HJm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699059436918,
        "cdate": 1699059436918,
        "tmdate": 1699636086874,
        "mdate": 1699636086874,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8AA5UFQJkT",
        "forum": "qCyhvr0GG8",
        "replyto": "qCyhvr0GG8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_v4jo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1582/Reviewer_v4jo"
        ],
        "content": {
            "summary": {
                "value": "This paper presents VONet, an innovative approach to unsupervised video object learning inspired by Monet. Specifically, Monet implements an efficient and effective parallel attention inference process that simultaneously generates attention masks from U-Net for all slots. In addition, the temporal consistency necessary to track objects across video frames is achieved by integrating an object-wise sequential VAE framework. Experiments demonstrate that the approach achieves competitive performance on several challenging object-centric video prediction benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.The parallel attention inference process proposed in this paper greatly improves the slot generation efficiency of MoNet and creates conditions for its further application.\n\n2.This paper proposes the KLD loss, which utilizes the principle that \"only slot representations that exhibit temporal consistency can exhibit predictability\" to cleverly achieve temporal consistency in unsupervised video object learning.\n\n3.The paper is relatively easy to follow, with good mathematical formulations and diagrams."
            },
            "weaknesses": {
                "value": "1.VoNet is a work based on MoNet and is highly similar to ViMON, a comparison of results with MoNet and ViMON should be reported in the experimental phase to demonstrate the advantages of VoNet.\n\n2.Can the method in this paper correctly handle objects appearing in the middle of the video or reappearing after being occluded and maintain temporal consistency? The MOVI dataset does not seem to be able to model this situation, consider adding experiments in natural scenes.\n\n3.Existing video object-centric learning methods based on slot attention are developing rapidly, e.g.DINOSAUR(https://arxiv.org/abs/2209.14860), VideoSAUR(https://arxiv.org/pdf/2306.04829.pdf), and perform well on natural scene datasets such as YouTube-VIS, please explain the advantages of the method in this paper compared to methods based on slot attention."
            },
            "questions": {
                "value": "For writng, the introduction section is short. It is hard to understand the main idea of the whole work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1582/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699103336698,
        "cdate": 1699103336698,
        "tmdate": 1699636086783,
        "mdate": 1699636086783,
        "license": "CC BY 4.0",
        "version": 2
    }
]