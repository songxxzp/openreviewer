[
    {
        "id": "32orOKi2H0",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_fuRV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_fuRV"
        ],
        "content": {
            "summary": {
                "value": "The paper studies a constrained MARL problem, where a group of agents operate independently in an MDP and jointly optimize a reward function. There are safty constraints that must be satisfied by their joint policy as well. The first focus of the paper is on the hardness of the problem, where the authors showed that strong duality does not hold. The authors then propose a decentralized primal approach to solve the problem. Via examples, it is illustrated either the decentralized primal algorithm or the primal-dual algorithm outperform the other in all cases."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem studied is interesting and relevant to the topics of ICLR. The authors made efforts to derive rigorous analyses of the problem."
            },
            "weaknesses": {
                "value": "From a computational perspective, the statements regarding the hardness of the studied problem are rather loose. Theorem 1 shows that the problem can be reduced to an optimization problem with quadratic constraints, which is in general hard. But nevertheless this doesn't imply the constrained MARL problem is also hard (for that requires a reduction in the other direction, i.e., quadratic optimization can be reduced to the studied MARL problem). Statements such as \"some studies argued that it is **probably** an NP-complete problem\" and  \"Thus, constrained cooperative MARL is a hard problem due to the presence of safety and product policy constraints\" are very loose. It also unclear whether the hardness comes from the safety constraints or the product constraints. Results in the subsequent sections do not seem to provide any clear message regarding the computational complexity of the problem, nor the time complexity of the algorithms discussed. Overall, the insights provided are a bit limited."
            },
            "questions": {
                "value": "Is the problem known to be hard without the safety constraints (with only the product constraints)? How much is known about this in the literature?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698595170089,
        "cdate": 1698595170089,
        "tmdate": 1699636367309,
        "mdate": 1699636367309,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5wBH0Kqb73",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_2Wr4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_2Wr4"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the hardness of constrained cooperative multi-agent reinforcement learning. In particular, it argues that there is a strictly positive duality gap. In addition, neither primal-dual or primal algorithms are strictly better than the other one."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper addresses an important question. \n+ The paper is well-written. \n+ The results seem correct and the explanations and theorems make intuitive sense."
            },
            "weaknesses": {
                "value": "- The bound $\\Delta$ seems to be trivial? It just comes from the geometric sum and assuming that the risk at each step is less than 1?\n- I'm not sure I see the only if part of Theorem 7. \n- It would be good to offer some advice on when to use which type of algorithm. \n- Some simulation would be helpful to illustrate the results."
            },
            "questions": {
                "value": "The paper seems to want to say that the problem is NP-hard but stops short. It seems that the quadratic equality is more of an analogy rather than equivalence to standard optimization problems. It would be good to make this more precise."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Reviewer_2Wr4"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698688061878,
        "cdate": 1698688061878,
        "tmdate": 1699636367236,
        "mdate": 1699636367236,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tcBbqFcVUV",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_MmRm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_MmRm"
        ],
        "content": {
            "summary": {
                "value": "The paper studies cooperative multi-agent RL problems in which all agents aim to maximize the average reward value function subject to a constraint on the average safety value functions. The authors provide a counter-example to show that the strong duality fails. Nevertheless, the authors extend two existing constrained policy search methods in the single-agent setting to cooperative multi-agent RL under constraints and prove their non-asymptotic optimality gap and constrained violation error bounds. The authors also investigate the pros and cons of the two methods in numerical examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well written, and statements are supported by justifications. \n\n- The authors show the existence of a strict duality gap in a simple example of cooperative constrained MDPs. This structural property reveals the limitations of methods in the single-agent case, which is useful for developing new algorithms.\n\n- The authors present a primal-dual algorithm and investigate its limitation by revealing the dependence of error bounds on the duality gap.      \n\n- The authors also present a primal algorithm that works in a decentralized way, and provide finite-time error bounds on the optimality gap and constraint violation. \n\n- Numerical examples are provided to show that either one can perform better than the other."
            },
            "weaknesses": {
                "value": "- For the duality, the authors didn't discuss the connection to the duality of constrained Markov potential games: Provably Learning Nash Policies in Constrained Markov Potential Games. Since constrained cooperative Markov games are a particular case, a non-zero duality gap directly follows. \n\n- Due to the non-zero duality gap, the proposed two algorithms suffer some gaps caused by the multi-agent and constraint coupling. It is more expected that the primal-dual algorithm suffers a duality gap. The primal algorithm has a dependence on an advantage gap that is less expected. It is not clear if these gaps are necessary and which one is better. \n\n- It is interesting to check the policy iterate convergence of two algorithms in simple examples. If this can be proved for the algorithms under certain conditions, it would be more beneficial to guide the practice.\n\n- Experiments are done with artificial examples. It is favorable to check the performance of the two actor-critic algorithms for solving real constrained tasks, and compare it with existing methods as mentioned."
            },
            "questions": {
                "value": "- Is the non-product form of optimal policy the only reason for the duality gap? Does there exist a more fundamental metric that can characterize the cause of the duality gap?   \n\n- As shown in convergence analysis, the effect of the duality gap in different algorithms can be different. Does this suggest a better way to design algorithms? Is it possible to remove such gap dependence?  \n\n- The advantages of the two algorithms are discussed in terms of policy iterates, which are stronger than the output policy in algorithms. Is it possible to show them in convergence theory?  \n\n- As mentioned, prior works also studied constrained cooperative Markov games and the authors have improved the analysis. Can the authors illustrate the analysis differences due to the lack of zero duality gap? It is useful if the authors could compare assumptions and results with them in a table."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Reviewer_MmRm"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698967782205,
        "cdate": 1698967782205,
        "tmdate": 1699636367168,
        "mdate": 1699636367168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NAWglhvJjg",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_Ycbe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_Ycbe"
        ],
        "content": {
            "summary": {
                "value": "The paper provides a comprehensive analysis of the strong duality condition in constrained cooperative Multi-Agent Reinforcement Learning (MARL), (for the first time) revealing its failure to hold and its impact on convergence rates of primal-dual algorithms. Then the authors presents/proposes a new decentralized primal algorithm to avoid the duality gap in constrained cooperative MARL. But their analysis shows that the convergence of this new algorithm is hindered by another gap induced by the advantage functions.The authors contribute to the understanding of the complexity of the constrained cooperative MARL problem by comparing it to cooperative MARL and single-agent constrained RL, and It is rigorously showed in this paper that constrained cooperative MARL is fundamentally harder than its special cases of cooperative MARL and constrained RL. Note that, before this work, strong duality has not been formally validated in constrained cooperative MARL, and therefore leaving convergence of the existing primal-dual type algorithms obscure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Strengths:\n\nThe problem studied in this paper is important and interesting, and the valuable findings here could be beneficial to MARL research community.\n\nThe paper provides a comprehensive analysis of the strong duality condition in constrained cooperative Multi-Agent Reinforcement Learning (MARL), (for the first time) revealing its failure to hold and its impact on convergence rates of primal-dual algorithms.And then the authors present/propose a new decentralized primal algorithm to avoid the duality gap in constrained cooperative MARL.\n\nThe authors compare the primal-dual algorithm with the primal algorithm and show that neither of them always outperforms the other in constrained cooperative MARL, both theoretically and experimentally. Such theoretical and empirical  analysis are valuable to better understand hardness of constrained cooperative MARL and the performances of different algorithms.\n\nThe authors contribute to better understanding of the complexity of the constrained cooperative MARL problem by comparing it to cooperative MARL and single-agent constrained RL."
            },
            "weaknesses": {
                "value": "The authors identify and reveal the issue about the previous primal-dual algorithms for constrained cooperative MARL, which is valuable, but the contribution would be much more significant if the authors could also propose a solution to successfully solve this identified problem . \n\nThe authors did attempt to propose a new decentralized primal algorithm to resolve the detected issue/challenge, but it seems no much success of the proposed solution. The proposed decentralized primal algorithm's convergence is hindered by a gap induced by the advantage functions, which can be seen as a major limitation. The comparison of the proposed decentralized primal-dual algorithm with the existing primal-dual algorithm doesn't seem to clearly indicate that the proposed new approach is a consistently superior approach, which makes the paper's contribution less significant.\n\nThe paper is highly theoretical, and comprehensive empirical validation/comparison of the proposed solutions are mostly missing. It would be great to also see some more comprehensive empirical experiment analysis on broad representative tasks (rather than just some very limited extreme case examples in current manuscript).\n\n(Though I have to admit that, although the authors are not able to successfully propose a solution to solve the issue, (for the first time) identifying this important problem/issue about  primal-dual algorithms for constrained cooperative MARL itself might be already quite valuable, and its contribution might possibly enough for publication on ICLR. Though if they are able to also provide a successful solution (in addition to identifying the problem), it would be a much stronger paper.)"
            },
            "questions": {
                "value": "see weakness section comments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699248000603,
        "cdate": 1699248000603,
        "tmdate": 1699636367099,
        "mdate": 1699636367099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uo2OA59fta",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_WbB4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_WbB4"
        ],
        "content": {
            "summary": {
                "value": "This work concerns a problem of MARL known as Constrained Cooperative MARL (CC-MARL). In such a setting, multiple agents share a common reward function that depends on joint action and the transitions of the underlying MDP depend on that joint action as well; further, apart from striving to maximize the expected discounted cumulative rewards they will get, the agents attempt to minimize a set of constraints. Those constraints in fact concern the expected discounted cumulative costs.\n\nExisting literature has offered results for correlated policies -- rather than product ones. The authors demonstrate that the problem of CC-MARL admits a formulation as a mathematical program with nonconvex (bilinear) constraints which is known to be NP-hard, in the general case. In general, in such a case, a solution of the CC-MARL is a product policy that will have an approximately zero single-agent optimality gap and constraint violation. The optimality gap is just the gain a single agent can have by unilaterally deviating from the joint output policy and the constraint violation is the amount by which a constraint is violated.\n\nThen, the authors demonstrate how the existing primal-dual algorithmic framework only manages to provide a bound on the constraint violation that depends on the *duality gap* of the underlying lagrangian function. This means that existing art can only offer solutions that could potentially have a constraint violation as large as the maximum possible discounted cumulative reward.\n\nFinally, the authors design an algorithm based on the single-agent RL CRPO algorithm. Convergence is proven using a potential/Lyapunov function argument. The optimality and constraint violation bounds depend on a quantity known as the *advantage gap*. The advantage gap in turn is zero if and only if the q-functions can be decomposed in a sum of functions that only depends on single-agent actions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors extend previous work that existed only for the case of correlated policy optimization to a product policy setting. They even improve single-agent RL bounds for the CRPO algorithm.\n\nThe paper offers a rather rich exposition of previous theoretical results in the literature of (constrained) cooperative RL."
            },
            "weaknesses": {
                "value": "A weakness of the authors' result is the lack of a definitive answer as to the hardness of the problem of Constrained Cooperative MARL. Indeed, the fact that the optimization program corresponds to a mathematical program with bilinear constraint functions is an indication of its potential hardness, yet there is a multitude of refined computational complexity classes that it could belong to. Is the problem *total*, i.e., is the problem guaranteed to have a solution? If so, it will belong to the TFNP complexity class. Then, does it belong to some known classes such as PPAD, PPA, CLS, PLS?\n\nI believe the paper has merit in extending previous results that considered correlated policies to quantifying the effect of being restricted to product policies. That being said, the narrative and even the title of the introductory text could benefit by stressing this fact rather than putting the focus on the hardness, since there is no definitive answer of the computational complexity."
            },
            "questions": {
                "value": "What were the main challenges you faced in proving a definitive refined hardness result?\n\nWhat would be the advantage gap if the reward functions admitted a network-separable structure and the transitions were additive?\n\nDo you think that the dependence on the advantage gap is tight? Can it be improved, or is it yet another indication of the hardness of approximation of solutions of constrained cooperative MARL problem solutions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Reviewer_WbB4"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699400445167,
        "cdate": 1699400445167,
        "tmdate": 1699636367027,
        "mdate": 1699636367027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IAeIWq4btR",
        "forum": "wFWuX1Fhtj",
        "replyto": "wFWuX1Fhtj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_ditR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4041/Reviewer_ditR"
        ],
        "content": {
            "summary": {
                "value": "The paper first examines whether strong duality holds for the constrained cooperative MARL setting (a problem which has been studied in previous works). In contrast to the constrained single agent case where strong duality holds, the authors reformulate the constrained cooperative MARL problem as a constrained optimization problem on the occupation measure associated with the agents\u2019 product policy, and prove that strong duality does not hold for constrained MARL case, because of the existence of non convex constraints (related to the product joint policy). They establish the first convergence rate result that characterizes the impact of duality gap on the constraint violation and optimality of the output policy of the Primal-Dual algorithm. In particular, both the optimality gap and the constraint violation converge at a similar sub-linear rate, but the latter up to a convergence error that depends on the duality gap of the problem. Furthermore, the paper proposes a primal-based algorithm for constrained cooperative MARL with the convergence not involving the duality gap, based on decentralized NPG policy updates. In particular, the authors show that both the optimality gap and the constraint violation converge at the sublinear rate, up to certain convergence errors that depend on defined advantage gaps. The authors also show that these advantage gaps vanish if and only if the Q-function has a certain factorization scheme. Last but not least, the paper explicitly compares the two algorithms and proves that each of the two algorithms can be better than the other in certain scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-motivated, since strong duality has not been validated in previous constrained cooperative MARL works and the duality gap is crucial for the convergence of the Primal-Dual Algorithm.\n- The paper is very well-written and easy-to-follow with concrete examples and good intuition of the examined algorithms.\n- The paper introduces some novel technical elements, such as the upper bounds in inequalities (17) and (19) on page 8."
            },
            "weaknesses": {
                "value": "- The assumption of the decomposition schema of the Q-function, that the advantage gaps depend on, seems limiting. Is the assumption necessary to ensure the computational tractability of the problem? Moreover, in practice, similar assumptions (on the linear decomposition of the Q-function) can harm performance, even in the unconstrained setting (e.g. see the comparison between VDN and QMIX (Rashid et al. 2020)).\n- It is not clear by the authors if the paper can be compared with other state-of-the-art algorithms (if exist) in terms of the optimality gap (in the constrained cooperative MARL setting)."
            },
            "questions": {
                "value": "I have the following questions regarding some technical details of the paper:\n- In the proof of Theorem 3, the authors have assigned $\\lambda_k$ with a value larger than $\\lambda_{k,{max}}$ (page 19). Can the authors be more explicit about why they are able to assign $\\lambda_k$ with such a value?\n- In the proof of Theorem 7, can the authors be more explicit about the proof steps and explain what $\\pi_{\\omega}$ and $\\omega$ are?\n- In the proof of Lemma 3, can the authors be more explicit about the last inequality of page 33?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4041/Reviewer_ditR"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4041/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699548386072,
        "cdate": 1699548386072,
        "tmdate": 1699636366943,
        "mdate": 1699636366943,
        "license": "CC BY 4.0",
        "version": 2
    }
]