[
    {
        "id": "op456rwOF7",
        "forum": "5Vh0XqOTGi",
        "replyto": "5Vh0XqOTGi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_KHth"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_KHth"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel approach called the Generative Adversarial Federated Model (GAFM) to mitigate label leakage attacks from gradients in binary classification tasks in vertical federated learning. GAFM combines the vanilla splitNN architecture with Generative Adversarial Networks (GANs) to indirectly incorporate labels into the model training process. The GAN discriminator within GAFM allows federated participants to learn a prediction distribution that closely aligns with the label distribution, effectively preventing direct use of labels. Additionally, GAFM incorporates an additional randomized cross-entropy loss to enhance model utility. The paper evaluates the effectiveness of GAFM on various datasets and demonstrates its ability to mitigate label leakage without significant degradation in model utility."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper combines GAN and splitNN to mitigate label leakage in vertical federated learning and conducts ablation experiments to investigate the role of each part.\n- The paper is well organized and easy to follow."
            },
            "weaknesses": {
                "value": "- The author needs to provide a sufficient explanation for the ability of GAFM to mitigate label leakage. In section 3.4, the author claimed that GAFM has more mixed intermediate gradients but does not provide further explanation of the reasons for this phenomenon and the motivation for combining the GAN method.\n- In Figure 3, both the GAN method and the CE method cause a decrease in model utility due to less label information leakage. However, their combination increases model AUC and leak AUC compared to the GAN-only model. It needs to be fully explained.\n- GAFM does not provide sufficient privacy protection, as the attack AUC is mostly above 0.6, while the baseline Marvell in its original article can reduce the normal attack to 0.5.\n- The paper needs to provide information on how to partition the features in its experiment. Different feature partitions will result in very different experimental results."
            },
            "questions": {
                "value": "- In this paper, why does Marvell perform much worse than the original paper? In the original Marvell paper, Norm Attack AUC can be reduced to 0.5 by Marvell on the same dataset, e.g., Criteo. Since the experimental results are not clearly explained, it is hard to tell the contributions made in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Reviewer_KHth"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659254982,
        "cdate": 1698659254982,
        "tmdate": 1699636262654,
        "mdate": 1699636262654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hh8U2qQE6K",
        "forum": "5Vh0XqOTGi",
        "replyto": "5Vh0XqOTGi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_ph6x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_ph6x"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the label leakage and protection in split learning (vertical federated learning) binary classification where the label-owning party wants to protect the label information from the other parties while allowing the joint learning of different parts of a predictive model. The authors propose a new approach by incorporating Generative Adversarial Networks in the model formulation: the label party learns a generator as the classifier and indirectly use the label information by matching the predicted label distribution rather than predicting the explicit labels (through the help of a learned discriminator). To further improve the performance, the author propose an additional cross-entropy loss to use the cut layer features to predict the example\u2019s randomly perturbed labels to further boost performance. Experimentally, the authors show that the proposed method GAFM achieves a better model utility and privacy compared to existing baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper focuses on solving an important practical problem of Label leakage in split learning."
            },
            "weaknesses": {
                "value": "- **Applying GAN to a binary classification seems unprincipled**. By using a GAN formulation which matches the predicted label distribution and the ground truth label distribution, the correspondence between an example\u2019s feature $x$ and its label $y$ is completely lost. Suppose there are 50% positive examples and 50% negative examples. Then the target label distribution under the proposed gaussian additive perturbation would be an equal mixture of two Gassians $\\mathcal{N}(0, \\sigma^2)$ and $\\mathcal{N}(1, \\sigma^2)$. If the GAN is learned perfectly, we can only guarantee that the generator $G_\\theta$ will also output this exact same Gaussian mixture (where the randomness is induced by the randomness over the example $x$). However, this cannot guarantee that all the positive $x$ (with $y=1$) will be mapped to $\\mathcal{N}(1, \\sigma^2)$ \u2014 instead, it is possible that the opposite can also happen, where all the positive examples are actually incorrectly mapped to $\\mathcal{N}(0, \\sigma^2)$ while the negative examples are all mapped to $\\mathcal{N}(1, \\sigma^2)$. In this case, the classifier would achieve an accuracy of $0$. Thus this current GAN formulation can\u2019t guarantee the proper learning of a predictive generator classifier. Even if the authors additionally use the $L_{\\textrm{CE}}$ to improve the performance, this additional loss cannot change the property of the generator $\\theta_g$. Thus I don't believe this issue can be theoretically prevented.\n\n- **Reliability of the experimental results**. The results presented in Table 2 makes me unsure whether the implementation and hyperparameters are tuned correctly for the experiments, thus making me question the reliability of the results:\n    - **Too much variation in the AUC performance measured for Marvell**. In Table 2, the authors show that model utility (test AUC) for the method Marvel over 10 random seeds can have a range as wide as > 0.2 (on Spambase min=0.59, max=0.82). This is likely a result of incorrect code implementation or inappropriately-tuned learning rate. The authors explain this away by pointing out that Marvell adds Gaussian noise perturbation to intermediate gradients which can lead to fluctuating leak AUC values. However, this fluctuation is over the privacy metric (leak AUC) instead of the utility metric, and the fluctuation results from it being computed over a random training batch. However, the authors do not claim there is any noticeable variation over the utility metric (which is averaged over all test data) over different random seeds. This wide performance range of Marvell shown in this paper makes me concerned that the paper\u2019s experimental results are not carefully checked and tuned.\n    - **norm leak AUC of max_norm**. In the Marvell paper, the leak AUC under the norm attack of the method Max Norm is < 0.6 on Criteo (Figure 4) and around 0.7 on ISIC (Figure 9). However, in Table 2, the authors report a leak AUC number of 0.92 and 0.99 for Max Norm on Criteo and ISIC respectively. Considering max norm is a heuristic approach proposed specifically to protect against the norm attack, this much worse (higher is worse) result compared to that reported in the original paper makes me concerned the experimental evaluation are not fully correct.\n\n- **Lack of privacy utility tradeoff visualization**. In Li et al (2022), the proposed method Marvell has a tuneable hyperparameter $s$ which controls the privacy-utility tradeoff: using higher values of $s$ allows more privacy but lower utility and this tradeoff frontier is visualized by Li et al. Thus it is expected for the authors to discuss whether their methods can achieve a privacy-utility tradeoff and if so present a complete tradeoff curve instead of showing only an individual point on the curve in Table 2.\n\n- **Choice of attack methods**.\n    - The authors mention on page 6 that the relationship between the sign and the cluster of gradients maybe not be as straightforward (and thus haven\u2019t experimented with the cosine attack). However, this explanation is not clear and deserves more elaboration. Besides, regardless of the exact formulation of GAFM, the cosine leak AUC can still be evaluated and should be reported.\n    - **Practicality of mean attack and median attack**. These two new attacks introduced by the authors assume the attackers know the gradient centers of class 0 and class 1. However, it\u2019s not clear how the attackers can obtain this information in the first place. Computing the class specific mean requires taking the average over each corresponding class, thus requiring the knowledge of the class identities in the first place. Hence, the logic becomes circuitous. The authors should explain more clearly whether they believe these two attacks are realistic, and if so, how they can be achieved in practice.\n\n- Typo: at the end of page 4, $\\hat{y} = D_{\\theta_g}(f(x))$ should be $\\hat{y} = G_{\\theta_g}(f(x))$."
            },
            "questions": {
                "value": "- Figure 2 is confusing. According to the caption, it is expected to be a visualization of the intermediate gradients colored by their ground truth label. However, each intermediate gradient should be a high-dimensional vector but is shown as a single number in Figure 2. Besides, it\u2019s also not clear to me why the predictions should be shown on the horizontal axis for this figure. Can the authors provide an explanation of how to interpret the Figure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Reviewer_ph6x"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698785916546,
        "cdate": 1698785916546,
        "tmdate": 1699636262571,
        "mdate": 1699636262571,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PpAUIpjVyp",
        "forum": "5Vh0XqOTGi",
        "replyto": "5Vh0XqOTGi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_wbsE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_wbsE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a GAN-based approach to transform the upload embedings for prediction from non-label party. In this way, the backpropagated gradients are perturbated. This prevents label leakage from gradients during model training. The empirical results demonstrate the effectiveness of GAFM among four public datasets, Spambase, IMDB, Criteo, ISIC. Furthermore, the authors also investigate which component (e.g., GAN or cross-entropy) is necessary to make GAFM work."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The empirical results are promising.\n2. The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. This paper only investigates a specific kind of vertical federated learning. In the setting, the authors assume that all the participants without labels share the same model architecture, which is not so practical. Besides, they only investigate on the binary tasks.\n2. The GAN-based approach to protect labels seems similar to [1]. They seem to prevent the attacks by transforming the uploaded embedding. This paper may be improved by adding some comparative experiments or some analysis in related work.\n\n[1] Scheliga, Daniel, Patrick M\u00e4der, and Marco Seeland. \"Precode-a generic model extension to prevent deep gradient leakage.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2022."
            },
            "questions": {
                "value": "1. what is the model architecture of the label party? What would be the experiment comparison between the different approaches in the context of the label party owning many layers of a neural network?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836245894,
        "cdate": 1698836245894,
        "tmdate": 1699636262459,
        "mdate": 1699636262459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pF7nIiwnvR",
        "forum": "5Vh0XqOTGi",
        "replyto": "5Vh0XqOTGi",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_vM2y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3149/Reviewer_vM2y"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a GAN-based Federated Model, GAFM, to address the binary label leakage problem in VFL. GAFM employs GANs as a surrogate to indirectly utilize label information, thereby mitigating LLG. To reduce its negative impact on model performance, an additional cross-entropy loss is further adopted to enhance prediction accuracy. Empirical results on various datasets demonstrate that GAFM achieves a better balance between model utility and privacy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Tackling an important problem in VFL**: LLG is a crucial security issue in VFL, impacting many significant applications. Achieving an optimal balance between utility and privacy is the ultimate goal in this field.\n\n- **Effective Method with Insightful Explaination**:  Backpropagating the GAFM loss results in a blend of two gradients whose distribution centers diverge in opposite directions. Coupled with Proposition 3.1, the paper effectively explains this mutual gradient perturbation of GAN and CE losses as the reason for GAFM's success. Empirical demonstration in Figure 2 and theoretical analysis in Appendix B provide a comprehensive understanding of the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "**Unclarities in Experiment**\n- The value of the **hyperparameter** $s$ significantly affects the performance **of Marvell**, yet it is not mentioned in this paper. The details of training and evaluating Marvell should be provided.\n- It would be more insightful to display a **utility-privacy trade-off curve** for a more intuitive performance comparison. (e.g., varying $s$ for Marvel to generate its dot-curve and varying $\\Delta$ to produce GAFM's dot-curve, as shown in Figure 4 of the Marvell paper)\n- As depicted in Figure 3, 'GAN only' demonstrates extremely poor performance on Criteo and IMDB, with AUC values close to 0.5. It appears that the GAN fails to learn the label distribution, thereby losing its purpose as a surrogate, which contradicts its initial motivation . From my perspective, the generator should, to some extent, successfully simulate the distribution, but it now seems that the performance of the generator is not crucial. **This leads me to question the necessity of GAN**. Perhaps any other method that provides the opposite gradient center with respect to CE loss could replace the GAN module. I hope the authors can provide a reasonable explanation for this.\n\n**Unclarity in Method Design**\n- What is the underlying intuition behind designing the CE loss to measure the distance between $f(x)$ and the randomized response $\\tilde y$? Given that $f(x)\\in R^d$ is a hidden vector and $\\tilde y \\in R$ is a noisy label, why is it reasonable or necessary to reduce the dimension of $f(x)$ to 1 by averaging? Would using a single FC layer be an alternative to achieve this purpose? A clear explanation is needed here.\n\n**Presentation need improvement**\n- Highlighting the best performance in the table would help readers grasp the key point quickly.\n- Figure 1 is quite misleading when it comes to understanding the dimension of $\\hat y$ and the structure of the discriminator. In fact, for the binary classification problem, its structure is extremely simple as the input is merely a scalar value. Since the primary real-world applications of VFL are in tabular data, using MLP for model structure demonstration would be smoother."
            },
            "questions": {
                "value": "As stated in the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3149/Reviewer_vM2y",
                    "ICLR.cc/2024/Conference/Submission3149/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3149/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840675495,
        "cdate": 1698840675495,
        "tmdate": 1700809336649,
        "mdate": 1700809336649,
        "license": "CC BY 4.0",
        "version": 2
    }
]