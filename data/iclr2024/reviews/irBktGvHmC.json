[
    {
        "id": "UZ0pdfgfrJ",
        "forum": "irBktGvHmC",
        "replyto": "irBktGvHmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_fxuS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_fxuS"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method, SYFLOW, for discovering subgroups of a dataset where the distribution of a target variable deviates significantly from its distribution in the population as a whole. Their method utilizes normalizing flows to approximate the distribution of the target variable, and therefore isn't limited to detecting only distributions of specific parametric types, like existing previous methods. SYFLOW also uses an end-to-end differentiable quality score, so the thresholds used to define the subgroup do not need to be pre-defined discretizations of the features, nor is there a need for computationally expensive combinatorial optimization procedures. Experiments on synthetic and real data show that in terms of the deviation of the discovered subgroup from the population, their method achieves improved performance over competitor methods, or similar performance with vastly reduced runtime."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem under consideration is important and has widespread practical utility. As the authors mentioned, existing methods for subgroup discovery usually require pre-specified discretizations of continuous variables, and in such cases either suffer from intractable runtime or poor accuracy. Using an end-to-end differentiable learning framework to this problem is a novel and exciting application of ML techniques, and using normalizing flows to deal with arbitrary target distributions while preserving useful gradients is a clever solution to a tricky problem.\n\nSome of the empirical results are also promising. SYFLOW obtains large gains in either performance or runtime compared to previous baselines, and it appears to be more widely applicable across difference problems, as compared to other methods which may have excellent performance on certain problems (e.g. where some of their distributional assumptions about the target variable are justified) but suffer greatly when their assumptions aren't met. In contrast, SYFLOW's performance is consistently high."
            },
            "weaknesses": {
                "value": "My main concern is with the technical clarity/correctness of Section 3.3. This is significant because this section provides a mathematical justification for the proposed method, but there are a number of mistakes, undefined terminology, and unclear logical reasoning which cast serious doubt on the theoretical soundness.\n\n**Undefined or unclear terminology.** Many of the quantities used in the derivation are not defined. The quantities $p_Y$, $p_{Y|S=1}$, $p_{Y|S=1, \\mathbf{x}}$, etc. are presumably densities and conditional densities of these random variables with respect to the Lebesgue measure, but these are never defined explicitly. They are sometimes used interchangeably with the probability laws themselves, e.g., the authors also refer to $D_{\\mathrm{KL}}(p_{Y|S=1}\\|p_Y)$.\n\nMost confusing are the random variable $S$ and related quantities. What, precisely, is the definition of the random variable $S$? Based on the initial motivation, it seems that $S(\\mathbf{x}) = \\sigma(\\mathbf{x})$ is just the indicator of whether or not a point belongs to the correct subgroup. But my initial understanding was that the subgroup is a deterministic quantity, in which case $p_{S=1|\\mathbf{x}}(\\mathbf{x})$ is either 0 or 1 and is in fact equal to $S(\\mathbf{x})$. If this is incorrect, what is the underlying probabilistic process? It seems like the authors may be confusing a soft approximation for a deterministic quantity with a true probability. If there is some true underlying probabilistic mechanism for $S$, it should be defined explicitly. If there is not an underlying probabilistic mechanism, then most of the derivation in Section 3.3 does not make sense.\n\n**Typos.** There are several technical typos which make the paper harder to understand, including:\n1. Pg. 3, definition of $\\mathcal{M}(x)$: The numerator should be $p$, not $D$.\n2. Pg. 3, definition of $s(x;\\alpha,\\beta,a,t)$: The summand in the denominator should be $a_i\\hat{\\pi}(x_i;\\alpha_i,\\beta_i,t)^{-1}$.\n3. Pg. 4, equation (2): I think this should be $D_{\\mathrm{KL}}(p_{Y|S=1}\\| p_Y)$.\n4. Pg. 4, equation (5): Based on the motivation, I think equation (5) should be the integral over $\\mathbb{R}^p_{\\not\\in \\subset}$. It's also not clear why $\\mathbb{R}^p_{\\not\\in}$ is being broken up into two regions, since the intuitive justification that $\\epsilon_1$ and $\\epsilon_2$ both approach 0 could just be directly applied to the whole region $\\mathbb{R}^p_{\\not\\in}$.\n5. Pg. 4, last inequality: The first integral also has a factor of $p_{\\mathbf{x}}(\\mathbf{x})$, so I believe this should also have a factor of $C_{\\mathbf{X}}$ in the bound.\n5. Pg. 5, equation (7): It seems the approximation is being used in the first equality, not the second.\n\n**Experimental results.** It is suspicious that the number of features which actually need to be thresholded $f_p$ is so small compared to the ambient dimension. What happens when a more significant fraction of the features need to be thresholded? Also, I assume the information about which features must be thresholded is not being supplied to the algorithm, but this isn't stated explicitly.\n\nFor the real-world data experiments, reporting the size-corrected KL divergence is basically reporting the value of the objective that SYFLOW optimizes, so it is unsurprising that SYFLOW has the best performance. Including some other metrics like deviations of common statistics (mean, standard deviation, etc.) from their population levels would be a more convincing comparison.\n\nIn Fig. 2(c), the results of SYFLOW look fairly unstable. Can the authors comment on this?\n\nIn Figs. 3(b) and 3(c), it is not obvious that the distribution of the y-values for the blue points is significantly different from the distribution for the overall population.\n\nAlmost none of the results have any measures of uncertainty/error bars. The authors should run multiple trials and report the variability of the methods; at present, it's unclear if the differences in performance are statistically significant.\n\nThe authors also don't describe or give explicit reference to the baseline methods.\n\n**Minor: Missing references.** Lastly, there are also some missing references that should be discussed, including:\n1. Christopher Sutton, Mario Boley, Luca M Ghiringhelli, Matthias Rupp, Jilles Vreeken, and Matthias Scheffler. Identifying domains of applicability of machine learning models for materials science. Nature communications, 11 (1):1\u20139, 2020. (Relevant for the COVID example.)\n2. Zachary Izzo, Ruishan Liu, and James Zou. Data-driven subgroup identification for linear regression. ICML, 2023. (Another method which does subgroup discovery without pre-specified discretizations.)\n3. What is the citation for the \"most related work of its kind\" described in the last paragraph of Section 4.1?"
            },
            "questions": {
                "value": "1. What is the precise definition of the random variable $S$?\n\n2. None of the functions in Algorithm 1 (Appendix C) are defined. Can the authors provide a complete, precise definition of their algorithm? It's especially unclear to me how normalizing flows will be used to learn $p_{Y|S=1}$ in a way that keeps nonzero gradients with respect to the learned boundaries. Also, what is the regularization term being used in the algorithm?\n\n3. How does the method handle discrete features, as it does in the COVID-19 experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3608/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3608/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3608/Reviewer_fxuS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697657467997,
        "cdate": 1697657467997,
        "tmdate": 1699636316407,
        "mdate": 1699636316407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hiU94Ei4Or",
        "forum": "irBktGvHmC",
        "replyto": "irBktGvHmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_MKfq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_MKfq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for extracting interpretable rules that describe regions in which the distribution of $Y$ differs within the joint distribution $(X, Y)$.\nTraditionally, this problem of subgroup discovery has been approached using combinatorial methods. \nIn this study, the authors proposeed solving this problem as a differentiable problem.\nSpecifically, the authors approximate the rules defining the regions of $X$ using differentiable soft-binning functions and estimate the distribution of $Y$ using Normalizing Flow.\nThe objective function is defined as the KL divergence between the distribution of $Y$ and the distribution of $Y$ within the subgroup.\nFinally, the subgroup is estimated by miximizing this KL divergence using gradient-based methods.\nIn the experiments, the authors reported that the proposed method achieved comparable or superior performance with shorter computation times compared to existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strength of this paper is the performance of the proposed method.\nIn particular, its superior performance and shorter computation times would be a significant practical advantage.\n\n**Originality, Quality**\n\nAn original aspect of the proposed method is its capability to handle multi-dimensional outputs for $Y$ by using Normalizing Flow.\n\n**Clarity**\n\nThere appears to be considerable room for improvement in terms of the clarity of this paper. For more details, please refer to the \"Weaknesses\" below.\n\n**Significance**\n\nThe superior performance and shorter computation times of the proposed method would be a significant practical advantage."
            },
            "weaknesses": {
                "value": "The weaknesses of the paper include \"Ambiguity in Problem Definition,\" \"Lack of Explanation for Existing Methods,\" and \"Insufficient Explanation of Experimental Setup.\"\n\n**Ambiguity in Problem Definition**\n\nThe paper uses a latent variable $S$ to represent subgroups without providing a clear definition of it.\nAn unclear aspect of this problem definition is how the latent variable $S$ is determined.\nWhile the paper treats the latent variable $S$ as a variable related to $X$ and $Y$, the assumption regarding this relationship is not explicitly stated.\nHowever, estimating the latent variable without assumptions is evidently impossible.\nConsequently, it is possible that there are implicit assumptions in the problem definition of this paper that have not been explicitly mentioned.\n\nAlso, in the paper, functions such as $s$ and $\\sigma$ are introduced to estimate the undefined $S$.\nHowever, it appears that the proposed method is estimating not the true undefined $S$ but simply $\\sigma$ that maximizes (8).\nThis seems to diverge from the problem definition that assumes the existence of the true latent variable $S$.\n\n**Lack of Explanation for Existing Methods**\n\nIn the experiments, several methods such as SD-$\\mu$, SD-KL, RSD, and BumpHunting are introduced as the baseline methods to be compared with.\nHowever, these baseline methods are introduced without references, making it impossible to determine what these methods do and whether they are valid as baseline methods.\nAdditionally, the absence of supplemental code makes the results irreproducible.\n\n**Insufficient Explanation of Experimental Setup**\n\nDetails of the experimental setup for the proposed method are not provided in the paper.\nFor instance, how the temperature parameter $t$ was set and decayed, and how the size-corrected KL's $\\gamma$ was configured, are not described.\nThese parameters are crucial for achieving favorable results with the proposed method, both in this experiment and for practical applications.\nDescribing the methods for setting these parameters, as well as other settings such as optimization algorithms, their parameters, and the number of training epochs, is essential to ensure the reproducibility of the results."
            },
            "questions": {
                "value": "* What is the definition of the latent varianbel $S$? How it is related to $X$ and $Y$?\n* What are SD-$\\mu$, SD-KL, RSD, and BumpHunting? Please provide their reference, and also discuss how and why they are relevant as the baseline methods to be compared with.\n* Please describe the details of the experimental setup for the proposed method. For instance, how the temperature parameter $t$ was set and decayed, and how the size-corrected KL's $\\gamma$ was configured? What optimization algorithms are used, and how their parameters and the number of training epochs are determined?."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698388055249,
        "cdate": 1698388055249,
        "tmdate": 1699636316320,
        "mdate": 1699636316320,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VU4b19zwFM",
        "forum": "irBktGvHmC",
        "replyto": "irBktGvHmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_mMzX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_mMzX"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce Syflow that contains components to the task of learning distributions over subgroups in a data set, particularly with an emphasis on interpretability and scalability. It combines the use of normalising flows for flexibility in modelling arbitrary distributions with a differentiable objective function for optimisation. The authors perform experiments to evaluate Syflow on a number of datasets and compare it to baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The combination of normalising flows, differentiable KL-divergence objectives and learnable feature bounds for subgroup discovery their integration for this specific application offers a new approach.\n- Importance and Applications: The problem tackled is particularly relevant for healthcare, aiming to identify interpretable subgroups in medical data. The approach has the potential to inform clinical decision-making and has high utility in healthcare applications.\n- Relevance to ICLR Community: The method encompasses topics of interest to the ICLR community, including normalising flows, differentiable objectives, and scalability. The focus on both theoretical and practical aspects of machine learning makes it a suitable contribution for the conference."
            },
            "weaknesses": {
                "value": "- The writing could be sharpened in places, namely the introduction. In particular, it is lacking in citations to back up the statements and help guide the reader. The last three paragraphs in the introduction make many statements about previous methods and the state of the field yet there are no references, with the exception of two citations in the first sentence. I believe many of these references appear in the related works section, but it is confusing for the reader to have to cross reference the sections and find the relevant citation.\n- This also applies to parts of the derivations, e.g. moving from eq. 2 to eq. 3 using \"rules of marginal probability and then that of Bayes\" would be better written out more explicitly to help the reader.\n- The results are promising but they could be more convincing. There seems to be from single runs which makes it difficult to estimate significance. There should be error bars on the figures and errors in the table would be a very helpful addition."
            },
            "questions": {
                "value": "- There are two limit parameters two set the range for the subsets, what if the subsets are multi-modal? Are multiple ranges applied to each feature? I.e. to capture both a subset with young adults and another with older individuals?\n- Cutting the abstract into three paragraphs makes it easier to read since it is so long but I suggest to the authors that they solve it instead by making the abstract more concise, in one paragraph\n- This paper uses the style guide for the 2023 conference, not 2024\n- The figure captions could be more descriptive to help the reader understand the context better, also, the captions could be a *little* bit larger \n- Figure 3a should be a table and not a figure, also, is it supposed to be \"quantitative\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Although a specific ethics review is not required, it would be a very welcome addition from the authors to have them add a discussion on the potential ethical concerns. This is particularly important in light of the potential application in health. What are the potential failure modes of the method? Are there any specific groups or types of groups that might be disproportionally affected? What could a practitioner do to mitigate and understand potential biases? The authors are well placed to shed some light on this issue."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790140768,
        "cdate": 1698790140768,
        "tmdate": 1699636316227,
        "mdate": 1699636316227,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "27v3ZbnKMM",
        "forum": "irBktGvHmC",
        "replyto": "irBktGvHmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_SrBh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_SrBh"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed SYFLOW, a method to discover locally optimal subgroups with exceptional distributions from a dataset using neural rule extraction."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Originality: the paper demonstrates creative combinations of existing ideas for subgroup detection \n- Clarity:The paper is well-organized with clear description of problem formulation, related work, experiment design and results comparison. Proposed method is evaluated on both synthetic data and public benchmark datasets."
            },
            "weaknesses": {
                "value": "1. Identification of a single subgroup versus the rest of population might of low practical utility, with comparison to identify multiple subgroups that are different from each other. \n2. The proposed method will generate rule-based subgroup characteristics based on one or more individual raw features, which might miss the potential hypotheses such as weighted combination of different features to distinguish and characterize the potential subgroup, e.g. 0.5*Age + I(Heart Failure), etc.   \n3. Identify distribution-wise separable groups might be meaningful from statistical perspective, but might not be meaningful from real-world view (e.g. rule of ages might be of highest KL-divergence in identification of subgroup A, but the subgroup of the 2nd highest KL based on gender might matter most in clinical perspective). Also, the quantification measure of divergence might result in potential over-fitting on existing data and cannot be generalized to unseen data. How to ensure robustness of the proposed method? \n4. Not sure whether experiment results are cross-validated in Figure 3. \n5. It's not clear how the characteristics of subgroups identified in different models differ from each other in Figure 3. It will be helpful to review the rules similar to Figure 4, but compared across different models. \n6. The paper lacks the description of limitations in final conclusion part."
            },
            "questions": {
                "value": "1. How to validate the robustness of the proposed approach?\n2. Will the proposed approach guarantee best classification if subgroup is known?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802049235,
        "cdate": 1698802049235,
        "tmdate": 1699636316158,
        "mdate": 1699636316158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xbAM4l52Wk",
        "forum": "irBktGvHmC",
        "replyto": "irBktGvHmC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_eY6J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3608/Reviewer_eY6J"
        ],
        "content": {
            "summary": {
                "value": "The work presents a method for identifying a salient subset given a labeled dataset. This subset is defined by a logical conjunction of an interval over each feature, and it is salient in that the distribution of the label/target differs significantly from that of the population. The paper described a differentiable method of learning both the saliency and the optimal subset (i.e. the feature intervals). Specifically, it defines soft thresholding to approximate each conjunctive clause, and thereafter employs normalizing flows for density estimation for this subset and the population; the gap between these is maximized to identify the optimal subset using SGD. Empirical evaluation over synthetic and real dataset shows the method is effective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work does a good job of presenting the problem and defines a promising method to solve it."
            },
            "weaknesses": {
                "value": "- In Section 3.3, the divergence (eq 2) is first approximated using eq 7, and subsequently eq 7 is approximated via sampling. Why couldn't the eq 2 be directly approximated via sampling?\n- The evaluations are somewhat inadequate. First, what is SD-\\mu in Section 5.1? Second, what are the cutpoints there? Finally, what are the various columns in Table/Figure 3 (i.e. where are these methods defined)?\n- The method as presented is primarily for identifying a single salient subset. It would be much more useful if it could be used for multiple salient subsets, that are further distinct from each other (to get diversity).\n- The paper mentions on page 2 that `As we show, this quantisation can greatly reduce the quality of the results` -- where is this shown?\n- Please cite the relevant literature the first time you use/define terms (e.g. normalizing flows in the abstract).\n- In Section 2 \"Preliminaries\" first para, I suppose in \\pi(x;\\alpha_i,\\beta_i) you meant to use x_i (and not x)."
            },
            "questions": {
                "value": "Please address the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3608/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699169819703,
        "cdate": 1699169819703,
        "tmdate": 1699636316097,
        "mdate": 1699636316097,
        "license": "CC BY 4.0",
        "version": 2
    }
]