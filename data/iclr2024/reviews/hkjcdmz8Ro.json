[
    {
        "id": "dxi9xoZx4s",
        "forum": "hkjcdmz8Ro",
        "replyto": "hkjcdmz8Ro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_PADE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_PADE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method named Prompt Automatic Iterative Refinement (PAIR), which can perform black box jailbreak attack to large language models. Experiments and results have shown that PAIR can successfully attack GPT 3.5 and GPT 4 less than twenty queries. Such threats raise the further research in better safety alignment for large language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper gives good presentations about their method and details about the prompt design process.\n2. Compared to previous jailbreak method GCG, jailbreak prompts generated by PAIR can maintain semantic meanings and successfully attack black box models in only a few quires.\n3. The authors provide detailed ablation study about their system prompt design.\n4. The design of the attack method uses the idea of agent interaction. PAIR use multiple language models to revise the prompt and attack the black box models."
            },
            "weaknesses": {
                "value": "1. Not convincing results in Table 1. PAIR can only successfully attack parts of the models presented in your papers. This method is almost useless toward both open source Llama2 and close source Claude. Such inconsistency of results among different models makes the results and methodology less convincing to me.\n2. Too naive loss function. The Jailbreak Scoring part is too naive as an optimization method. The score is just a True or False score. I don't think such naive loss function design can be helpful for performing successfully attack, which makes the entire method seems like random searching. That maybe the reason why PAIR cannot jailbreak models like Llama2 or Claude.\n3. The necessary usage of complex system design in Table 7. Though authors have implemented ablation studies toward the usage of example and 'improvement' in attacker system prompt design, I still have concerns about the necessity of such a long and complex system prompt designing."
            },
            "questions": {
                "value": "What are the versions of the close source models used in your experiments. As far as I know, many close source models like ChatGPT would update their weights regularly. For better reproducibility, can you show in detail the specific version of the close source models. Some versions of GPT3.5 and GPT4 do not have safety alignment while others have. You may get totally different results with different versions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Reviewer_PADE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698514894084,
        "cdate": 1698514894084,
        "tmdate": 1699636954587,
        "mdate": 1699636954587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uCNZepZ4cE",
        "forum": "hkjcdmz8Ro",
        "replyto": "hkjcdmz8Ro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_HLa7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_HLa7"
        ],
        "content": {
            "summary": {
                "value": "A novel and query-efficient LLM-agent-based red-teaming tool for testing jailbreaking behaviors of black-box LLMs. The core invention includes an attacker (an LLM) that reads the response from a target model (another LLM), and it uses the response and feedback to improve the jailbreak prompts. Empirical results show comparable (or better) jailbreak performance compared to the GCG method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is novel and leverages the chain of thoughts and in-context learning capability of LLMs for red-teaming.\n2. The ability to perform query-efficient red-teaming for black-box LLMs is important\n3. The jailbreak results (both direct queries and transfer) on black-box LLMs (GPT-3.5, GPT-4, Claude-1, Claude-2, PaLM-2) are quite remarkable."
            },
            "weaknesses": {
                "value": "While I enjoyed reading the paper and found the proposed method quite neat and novel, I have several major concerns that prevented me from recommending acceptance in the current form. I look forward to the authors' rebuttal to clarify my concerns.\n\n1. In the evaluation, it is stated that \"For each of these target models, we use a temperature of zero for deterministic generation\". I do not find this setting convincing, as this is not the default setting for LLMs. Moreover, a recent study <https://arxiv.org/abs/2310.06987> actually shows that merely changing these generation hyperparameters from the default value can weaken the safety alignment, without further optimization or tweaks. I would like to see the results on the default generation parameters (admittedly there will be randomness, and more runs are needed to make the result statistically meaningful), and would like to know why considering the unusual setting of making the temperature =0  is of practical importance.\n\n2. If I understand the evaluation setting correctly, I do not think the authors did justice to the GCG method in terms of query analysis. \n- First of all, the proposed method is prompt-specific, which means the total number of queries actually grows linearly with the number of test prompts. On the other hand, GCG is prompt-agnostic, which finds a \"universal\" suffix for every test prompt. When a new test prompt comes in, GCG does not require any query, while the proposed method does. If there is a large number of test prompts to be evaluated, perhaps GCG can be more query-efficient.\n- The authors mentioned they used 50 queries from AdvBench for evaluation, while \"when the maximum number of iterations is reached. For GCG, we use the default implementation and parameters, which uses a batch size of 512 and 500 steps for a total of 256,000 queries.\" This is confusing to me. Why shouldn't we just run GCG on the same 50 queries for a fair comparison? I would like to see a jailbreak success rate v.s. query count plot comparing GCG and the proposed method in this case.\n\n3. The related work only covers recent works on jailbreaking LLMs. However, given the similarity in adversarial testing/red-teaming, I suggest the authors include the discussion on query-based black-box attacks as related work, especially in the discussion of query efficiency. Of course, due to the special text interface, attackers may not need to explicitly estimate gradient information for updating the attack vectors, as opposed to standard ML models."
            },
            "questions": {
                "value": "- Referring to Weakness #1, please justify the selection of an unusual generating hyperparameter setting. Why didn't the authors try jailbreak attacks using the default settings (in which the alignment is mostly effective)\n- Referring to Weakness #2, is the query analysis fair to GCG? Given a small set of test prompts, perhaps the difference in query efficiency can be minor. If the set of test prompts is large, the growing total query complexity of the proposed method versus GCG can be an issue."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7805/Reviewer_HLa7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701067986,
        "cdate": 1698701067986,
        "tmdate": 1700621885046,
        "mdate": 1700621885046,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6PS88SG8ck",
        "forum": "hkjcdmz8Ro",
        "replyto": "hkjcdmz8Ro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_9YSw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_9YSw"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the safety of LLMs. In particular, the authors proposed to utilize an LLM to craft jailbreaking prompts to break the safety of a target LLM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The safety of LLMs is an active research area. The proposed method could help to red team LLMs.\n\n2. The idea of utilizing LLMs to generate jailbreaking prompts is novel."
            },
            "weaknesses": {
                "value": "1. The technique contribution is week. The proposed method utilizes the LLM to refine the prompt. Thus, the performance of the proposed method heavily relies on the designed system prompt and LLMs. Moreover, the proposed method is based on heuristics, i.e., there is no insight for the proposed approach. But I understand those two points could be very challenging for LLM research.\n\n2. The evaluation is not systematic. For instance, only 50 questions are used in the evaluation. Thus, it is unclear whether the proposed approach is generalizable. More importantly, is the judge model the same for the proposed algorithm and evaluation? If this is the case, it is hard to see whether the reported results are reliable as LLMs could be inaccurate in their predictions. It would be better if other metrics could be used for cross-validation, e.g., manually check and the word list used by Zou et al. 2023. The proposed method is only compared with GCG. There are also many other baselines, e.g., handcrafted methods (https://www.jailbreakchat.com/). \n\n3. In GCG, authors showed that their approach could be transferred to other LLMs. Thus, GCG could craft adversarial prompts and transfer them to other LLMs. It would be good if such a comparison could be included. \n\nA minor point: The jailbreaking percentage is low for certain LLMs."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801319961,
        "cdate": 1698801319961,
        "tmdate": 1699636954194,
        "mdate": 1699636954194,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "l6aXwkpyyS",
        "forum": "hkjcdmz8Ro",
        "replyto": "hkjcdmz8Ro",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_2SyA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7805/Reviewer_2SyA"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an automated red teaming method that uses an attacker LLM to iteratively propose refinements to a jailbreak until it works. Experiments confirm that this method is able to jailbreak various open- and closed-source models, including ChatGPT. The method outperforms GCG at a much lower compute budget, and ablations to the system prompt used by the attacker LLM demonstrate the importance of different parts of the method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This is a sensible automated red teaming method, and it obtains comparable or better results than GCG while more closely mirroring human jailbreaks\n\n- Ablation studies that show the importance of a few different components of the method"
            },
            "weaknesses": {
                "value": "- Ultimately, this is a fairly simple method, and there isn't much technical innovation. One could say that this paper is mainly about the system prompt in appendix B, and various experiments measuring its efficacy. The paper would benefit from more analysis into different strategies taken by the attacker LLM, whether these mirror what human red teamers try, whether smarter attacker LLMs work better (disentangled from how unfiltered they are), etc.\n\n- The only baseline is GCG. The baselines in Perez et al would be especially useful to include. E.g., the few-shot method in that work also uses iterative queries to an attacker LLM.\n\n- GPT-4 is used as a judge, which makes the results in the paper hard to compare to, since the underlying model in the GPT-4 API isn't guaranteed to always be available\n\n- In Section 3.3, the authors state, \"modifying the system prompt is a feature only available on open-source LLMs, limiting the available choices for A\". This isn't true at all. OpenAI introduced the notion of a system prompt, and their API allows editing system prompts. This also contradicts the experiments a few paragraphs later on GPT-3.5\n\n- Why aren't there experiments with GPT-4? It's odd that GPT-3.5 performs less well than Vicuna 1.5, since GPT-3.5 gets much higher scores on MMLU and is generally smarter. It would also be interesting to know if GPT-4 performs less well than Vicuna 1.5, and it would be an easy experiment to run.\n\n- In principle, the attacker LLM could violate the JSON format, since no methods for guaranteeing JSON formatting are used. How often does the attacker LLM violate the JSON format? How does your method handle cases where this happens?\n\n- In Figure 4, it's unclear whether the left plot is showing the effect of width or depth. The caption makes it seem like the x-axis is depth, but this doesn't make sense in context."
            },
            "questions": {
                "value": "See weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7805/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699306387266,
        "cdate": 1699306387266,
        "tmdate": 1699636953967,
        "mdate": 1699636953967,
        "license": "CC BY 4.0",
        "version": 2
    }
]