[
    {
        "id": "nMxOXLuqev",
        "forum": "EAT7gmyIH2",
        "replyto": "EAT7gmyIH2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX"
        ],
        "content": {
            "summary": {
                "value": "Explaining a deep neural network decision on a data point by using linear models as approximators in the locality of the data point has become a common practice. This paper argues that local linear approximation is inapt as the black boxes under investigation are often highly nonlinear. They propose a novel local attribution methods Distillation Approach for Model-agnostic Explainability (DAME) which does not use a linear model as local approximator. The method consist of training a student network to copy the prediction of the original DNN on the perturbated version of the data point along with a Mask-generator network that masks those perturbated samples. After training, this Mask-generator will be used to generate an explanation for the original DNN. DAME is evaluated on computer vision datasets using (a) IoU between the explanation and human annotation, (b) human subjective rating of the quality of the explanation, and (c) the drop in accuracy of the original DNN when the important pixels are removed. They also evaluate it using an audio dataset and a medical dataset, on which both use an IoU metric."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is clear and the work is well contextualized regarding prior works (although it could refer to more recent perturbation-based attribution methods).\n- Using distillation methods for explaining a model is an interesting idea"
            },
            "weaknesses": {
                "value": "- The main weakness of this paper is in the evaluation. \n\t- It is because we do not know the reasoning behind DNN decisions --i.e. we do not what a good explanation of its decision is-- that we carefully develop methods for that purpose. In that sense, a subjective human evaluation of the quality of the explanation (b) is not actually informative of the quality of the explanation\n\t- The decisions of a DNN do not necessarily rely on the same features humans rely on (the opposite has previously been shown [1-2]). Hence an explanation that accurately depicts that the DNN does not use human-like features will be wrongly penalized by IoU metrics (a)\n\t- On the other hand, a standard way to evaluate attribution methods is using fidelity measure, Deletion and Insertion --introduced in RISE-- being the most widely used ones, which the paper does to compare DAME with RISE and LIME (it is not exactly clear if pixels are progressively removed as in Deletion or if all important pixels are removed at once). If Deletion is indeed used, the results of the 2 baseline are slightly surprising as RISE has been shown consistently to be better than LIME in previous work [3-4], which is not the case here.\n- Also, the motivation of the paper comes from the claim that linear models are inapt to accurately approximate non-linear models locally. An instantiation of the proposed framework with linear models is missing to make the claim more concrete.\n\n\n[1] Geirhos et al. Shortcut learning in deep neural networks. Nature Machine Intelligence. 2020.\n\n[2] Fel et al. Harmonizing the object recognition strategies of deep neural networks with humans. NeurIPS. 2022.\n\n[3] Petsiuk et al. RISE: Randomized input sampling for explanation of black-box models. BMVC. 2018\n\n[4] Novello et al. Making Sense of Dependence: Efficient Black-box Explanations Using Dependence Measure. NeurIPS. 2023"
            },
            "questions": {
                "value": "- I was wondering if the authors have thought about running standard attribution methods on the original and student models as a sanity check that they do seem to have similar decisions for similar reasons?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_7xDX",
                    "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698611565705,
        "cdate": 1698611565705,
        "tmdate": 1700603841438,
        "mdate": 1700603841438,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "efT8le4iYy",
        "forum": "EAT7gmyIH2",
        "replyto": "EAT7gmyIH2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model-agnostic, gradient-free, saliency-based method to understand local behavior of black-box models. They establish the shortcomings of previous works like LIME that use a locally linear model to approximate the behavior of a neural network in a given sample\u2019s neighborhood. They use ideas from the MLX (Machine Learning from Explanations) area and propose to address this via distilling the black-box model into a smaller student model only in the sample\u2019s neighborhood. Concretely, they generate perturbations of a sample and then learn saliency masks (explanations) such that a perturbed sample masked by the saliency when passed through the student model has the same target class softmax score as the teacher. These two models, the one that learns the masks and the student that distills the black-box in a sample\u2019s neighborhood, are chained and trained together using a distillation+explanation loss (with 2 more loss terms to avoid identity learning and preserve class distributions between student and teacher). They share results of their approach on 2 vision datasets and 2 audio datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis method works in input space and not in the binary mask space like LIME does\n2.\tThe paper establishes the shortcoming of locally linear approximations with a small toy experiment.\n3.\tThey share results from many varied experiments with both quantitative metrics and qualitative samples. To quantify the quality of their explanations, they compute IoU with human annotations for samples that are classified correctly by the model \u2013 since that is the class that human annotations would be explaining.\n4.\tIt is an intuitive approach. The paper is well written and easy to follow\n5.\tThey compare with LIME, RISE, GRADCam and other gradient based methods from Integrated Gradients family.\n6.\tThe appendix is very thorough and quite informative"
            },
            "weaknesses": {
                "value": "1.\tThe biggest bottleneck to using this approach would be having to train a whole new model to understand the behavior of the model for one single input sample.\n2.\tResults from RISE are often quite competitive in tables 1 and 2. Smooth Grad is also quite competitive.\n3.\tThis approach is akin to a gradient-based approach in the guise of gradient-free. If one was to distill the whole black-box into another model (not just in the sample\u2019s neighborhood) and then apply any gradient-based method, I believe that that would be much simpler since one won\u2019t have to train a smaller model to get an explanation for each sample and I believe it would perform competitively seeing the numbers in tables 1 and 2. So, I have doubts about why the authors have taken this round-about route. It is at least worth it to compare this with works that use distillation to understand models.\n4.\tI might have missed something here but the audio experiment results don\u2019t seem too convincing:\na.\tIn task 2, padding noise on two sides is an easy noise pattern to learn/catch. \nb.\tIn task 3, cough data says that it was manually annotated. Are there going to be any plans to release this to enable discussion/reproducibility?\n5.\tSome language in the paper such as \u201cmildly vs strongly non-linear\u201d is non-standard. This is a small nitpick."
            },
            "questions": {
                "value": "1.\tHave the authors considered using this method on well-known spurious feature detection image datasets like Decoy-MNIST and ISIC?\n2.\tIf one was to distill the whole black-box into another model (not just in the sample\u2019s neighborhood) and then apply any gradient-based method, I believe that that would be much simpler since one won\u2019t have to train a smaller model to get an explanation for each sample and I believe it would perform competitively seeing the numbers in tables 1 and 2. So, I have doubts about why the authors have taken this round-about route. It is at least worth it to compare this with works that use distillation to understand models. I would like to get the authors thoughts on these points.\n3.\tCan the authors clarify if I have incorrectly interpreted the audio experiments setup or results? Are there going to be any plans to release manually annotated cough data to enable discussion/reproducibility?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_RyMT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777227052,
        "cdate": 1698777227052,
        "tmdate": 1699637121516,
        "mdate": 1699637121516,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UvLeUcG7Uw",
        "forum": "EAT7gmyIH2",
        "replyto": "EAT7gmyIH2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for generating a learnable saliency-based explanations model, which is model-agnostic and requires only black box query access to the model. The framework consists of two models: a mask-generation module that generates the saliency maps and a student network to distill the black-box model's predictions by approximating the black-box model's local behavior near the input sample. The parameters of these two networks are learned by generating perturbations in the neighborhood of a given sample."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses an important question on generating saliency map-based explanations with only black-box\naccess to a model.\n- Besides traditional tasks from Computer Vision, the paper also reports results on audio processing tasks.\n-  The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "One of the critical issues with the paper is how they evaluate & the choice of baselines. Even though they consider a\na diverse set of tasks, the authors must add additional experiments to strengthen the paper.\n\nIt is hard to see whether the proposed framework offers a clear advantage over the baselines (as explanations are typically subjective).\n\nIt would also be essential to understand how architectural changes affect the results.\n\n- Does the architecture of the map generation & student network affect the performance? Does it need to be shallow\nor deeper? What are the design considerations for these networks?\n- How does the proposed method compare to, say, just distilling a smaller model from the black-box model & then\nusing the distilled network to generate saliency maps (and use these as explanations for the black-box model?)? This should be a baseline.\n- What's the need for a map-generation network in the framework? Can't we distill the black-box model through a\nstudent network exposed to the perturbations?\n- The authors should add the above two setups as baselines."
            },
            "questions": {
                "value": "The proposed framework incurs an additional computation cost but performs worse than a simpler technique like RISE, and the improvement seems marginal.\n\nHow important are the perturbations? The mask-generation network seems to be trainable without the perturbations of inputs. It would be better to investigate the impact of the number of perturbations on explanation performance to evaluate the effectiveness of the perturbations.\n\nI also encourage the authors to consider benchmarks like CUB & AwA2 (and other benchmarks where concepts are annotated), which contain annotations of salient parts of the image; this helps them compare against some gold standards\n\nRefer to Weaknesses for additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8912/Reviewer_hRgW"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847060110,
        "cdate": 1698847060110,
        "tmdate": 1700631484803,
        "mdate": 1700631484803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nb49J50EYR",
        "forum": "EAT7gmyIH2",
        "replyto": "EAT7gmyIH2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8912/Reviewer_kHxm"
        ],
        "content": {
            "summary": {
                "value": "This work proposes Distillation Approach for Model-agnostic Explainability (DAME), an approach which fits a non-linear model is fit in the vicinity of an input sample to to explained. The model is fit to obtain a saliency map explanation based on a teacher-student distillation approach which uses a combination of 3 loss functions. The proposed method is comprehensively evaluated on image and audio datasets using a number of evaluation techniques and shows improvement over existing local explainability methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall when the decision boundary is wiggly and inputs are high dimentsional, sparse linear models may not mimic the source model's behaviour around a sample and it makes sense to use a non-linear approach which might provide a better approximation. The approach proposed to generate the local saliency explanation is novel. The evaluation is quite comprehensive including fidelity-based, subjective and qualitative evaluations and comparison with 9 XAI methods."
            },
            "weaknesses": {
                "value": "Based on the 3 loss functions that need to be handled, it seems likely that the method may not work out of the box (like LIME) and users will probably need to customize/tune hyper-parameters etc. to get the explanations right."
            },
            "questions": {
                "value": "- How is local vicinity and distance between the given sample and perturbations defined in DAME - is this same as LIME?\n- In case of DAME, can the authors comment on local invariance of explanations (do similar inputs yield similar explanations)?\n- Would DAME be impacted by correlated featured?\n- Instead of using a masking approach to generate perturbations (e.g. LIME), if we have a realistic distribution of perturbed images (e.g. MeLIME), can the DAME pipeline still be used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8912/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699290837401,
        "cdate": 1699290837401,
        "tmdate": 1699637121261,
        "mdate": 1699637121261,
        "license": "CC BY 4.0",
        "version": 2
    }
]