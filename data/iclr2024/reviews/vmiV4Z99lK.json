[
    {
        "id": "zKHyGkpjsa",
        "forum": "vmiV4Z99lK",
        "replyto": "vmiV4Z99lK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission378/Reviewer_dz6f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission378/Reviewer_dz6f"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a fast stochastic approach for quantizing the weights of neural networks with full-network error bounds."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors for the first time propose error bounds for quantizing an entire L-layer neural network under minimal assumptions on weights and input data in the infinite alphabet case."
            },
            "weaknesses": {
                "value": "If I understand correctly, in Figure 1 and Table 1, the authors quantize only the weights of neural networks, which cannot bring out the benefits of quantization. What this means is that to fully exploit quantization for convolutional networks, it is necessary to quantize both weights and activations of neural networks because running convolutional networks is compute-bound.\n\u00a0\n\nFurthermore, more experiments on lightweight models such as MobileNetV2 are required due to the fact that it is well known that performance degradation is severe when quantizing MobileNetV2. To validate whether SPFQ is really powerful or not, the experiment on MobileNetV2 is necessary."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Reviewer_dz6f"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648074028,
        "cdate": 1698648074028,
        "tmdate": 1699635964908,
        "mdate": 1699635964908,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nYD4lPp6OP",
        "forum": "vmiV4Z99lK",
        "replyto": "vmiV4Z99lK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission378/Reviewer_xzB6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission378/Reviewer_xzB6"
        ],
        "content": {
            "summary": {
                "value": "The paper studies model compression through weight quantization. It introduces a new stochastic quantization framework, called (SPFQ), by employing randomness in the quantizer. SPFQ is consisted of a data-alignment phase and a quantization phase. SPFQ with approximate data alignment has a computational complexity that scales linearly in the number of parameters of the neural network. The paper also obtained a first error bounds for quantizing an L-layer neural network, under an infinite alphabet condition and limited assumptions on the weights and input data. Through experiments, the authors applied their quantization to the weights of several neural network architectures that are trained for classification tasks on the ImageNet dataset. The simulations indicated a small loss of accuracy compared to unquantized models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-stochastic quantization appears to be a novel idea that has not been tried for the model compression although It was used in stochastic gradient(or weight parameter) quantization for communication in both distributed training and federated learning.\n\n-the analysis of quantization error appears to be rigorous and correct, despite having some limiting but mild assumptions.\n\n-it appears that the analysis is applicable with any non-linearities in activation function as long as it is a Lipschitz function, after properly changing the values of constants.\n\n-it\u2019s data alignment phase has a linear complexity which is improvement on deterministic quantization algorithms that require solving optimization problems, and hence usually having higher order complexity in the number of parameters."
            },
            "weaknesses": {
                "value": "-the error analysis was not validated by the simulations. Simulations only reflect the accuracy of the quantizer compare to unquantized method but it does not say anything about how good the error analysis is.\n\n-experiments does not use any baseline method for comparison with the proposed method. Only unquantized model accuracy was compared. This is unacceptable."
            },
            "questions": {
                "value": "-please provide simulation results confirming how tight the error bounds. \n-please provide discussion as to how the error analysis can help with the design of better quantizer. Is there a lesson to be learned from error bound that one can leverage to improve the quantizer design.\n-please provide simulation results on imagenet dataset comparing the performance of the proposed method with the state of art model quantizers.\n\n-the error analysis implies that the error upper bound increases with batch size. However, simulations suggest otherwise. Can you explain the contradiction?\n\n-would fine tuning improve the accuracy. Perhaps simulation results with fine tuning would be helpful too."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719908514,
        "cdate": 1698719908514,
        "tmdate": 1699635964813,
        "mdate": 1699635964813,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Lz9AvkQgXD",
        "forum": "vmiV4Z99lK",
        "replyto": "vmiV4Z99lK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission378/Reviewer_nVek"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission378/Reviewer_nVek"
        ],
        "content": {
            "summary": {
                "value": "The authors propose stochastic path-following quantization (SPFQ), a greedy algorithm that sequentially quantizes the rows of a neural network's weight matrices starting from the input layer. SPFQ quantizes weights \"keeping previously quantized weights in mind\": it keeps track of the quantization error it introduces after each step, and to quantize the next row of weights, it selects a weight setting that minimizes the total network quantization error. This way, at each step, it corrects for a \"bad\" setting for the quantized weights.\n\nThe basic version of SPFQ greedily minimizes the 2-norm of the quantization error induced for the pre-activations of the current hidden layer. The authors show their algorithm admits a decomposition into two stages: the first step finds unquantized weight settings that would correct for the total quantization error as best as possible, and the second step quantizes these weight settings.\n\nBased on this decomposition, the authors propose two variants of SPFQ. The first variant completely eliminates the total quantization in the first step. Hence, the only remaining error arises from quantizing the new weight settings; however, it is more involved computationally. Thus, the second proposed variant is an iterative scheme that eliminates the total quantization error only approximately but is cheaper computationally.\n\nFinally, the authors derive full-network error bounds for these two SPFQ variants."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "SPFQ is a simple and elegant algorithm, and its motivation in section 2.2 and decomposition in section 2.3 are particularly nice. The authors' theoretical results are rigorously stated, and having checked appendices A and B in detail and skimmed over C-G, I am reasonably confident that their proofs are correct.\n\nI also found both the main text and the appendix to be well-written and reasonably easy to follow."
            },
            "weaknesses": {
                "value": "My biggest concern regarding this paper is that, in its current form, it is a poor fit for ICLR as the authors do not showcase how their contributions are relevant to the ICLR community in any way. The paper would be a better fit for a more specialized venue, e.g., COLT or a statistics conference/journal.\n\nI hold this opinion because, under the current narrative, the purpose of SPFQ is to allow the authors to derive the bounds in sections 3 and 4 and not to be a practical scheme (though I should emphasize that this criticism is in no way meant to imply that I don't find the bounds interesting). This issue appears at multiple levels in the paper:\n- The original version of SPFQ is motivated by minimizing the 2-norm of the pre-activation's quantization error, from which authors then deviate with their two proposed variants (though the second variant contains the original as a special case). Since the variants are technically no longer controlling the 2-norm of the quantization error, and the authors do not comment on the benefits of deviating from the original formulation, this appears to indicate that their only purpose is that the error bounds in sections 3 and 4 can be derived for them.\n- The authors do not numerically evaluate any of their bounds. Hence, also in part due to their complexity, it is impossible to gauge the bounds' tightness in any scenario.\n- The usefulness of the relative error bound in Corollary 3.2 and related results seems very limited, and I am not convinced that they bear out their feature as one of the premier contributions in Eq (2). This is because the authors need to assume that the network weights follow a standard Gaussian distribution for their results to hold. This assumption usually holds before training (as a standard, but at least isotropic Gaussian is a commonly used prior for neural network weights) and certainly does not hold after. Thus, the authors' results regarding the number of bits required to quantize a network seem to only hold for untrained networks in practice.\n- The authors do not state what SPFQ's intended purpose is: is it to compress a network or to speed up inference? This question is important because it determines the relevant benchmarks the method should be compared with.\n- The experiments are relegated to Appendix H and are limited to evaluating SPFQ on a few small architectures and comparing it with only one other related work. In particular, there are no comparisons with state-of-the-art post-training quantization methods.\n- Moreover, there are no ablation studies between the different SPFQ variants.\n\nWriting-wise, while the paper is well-written, it is incomplete, as it is missing a conclusion/discussion section.\n\nFinally, a related work I think the authors should mention is that of [2], which also explores the idea of sequentially correcting for poor stochastic quantization in a greedy/myopic way, though in the context of variational autoencoders.\n\n## References \n - [1] Eric Lybrand and Rayan Saab. A greedy algorithm for quantizing neural networks. J. Mach. Learn. Res., 22:156\u20131, 2021\n - [2] Gergely Flamich, Marton Havasi and Jos\u00e9 Miguel Hern\u00e1ndez Lobato. Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding. In Advances of Neural Information Processing Systems, 2020."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698845072536,
        "cdate": 1698845072536,
        "tmdate": 1699635964739,
        "mdate": 1699635964739,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lxPZNUP5fg",
        "forum": "vmiV4Z99lK",
        "replyto": "vmiV4Z99lK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission378/Reviewer_sBRp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission378/Reviewer_sBRp"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a quantization algorithm for compressing pre-trained neural networks. The proposed scheme is a greedy algorithm that   quantizes weights sequentially. Given a set of quantized weights, the neural network activations are then found with those quantized weights, which is then weighted / aligned with the unquantized output via scaling. The effect of this scaling is incorporated in the weight, which gives the effective parameter value to be quantized. Since this is sequential, and at every step the algorithm tries to minimize the $\\ell_2$-error between the outputs of the quantized weights and the original weights. this algorithm is referred to as \"greedy\".\n\nMoreover, the quantizer used in this paper is a stochastic quantizer, wherein the quantization levels are decided, and the output of the quantizer is one of the two quantization points closest to the scalar input. The output is probabilisitic with a probability that is inversely proportional to the distance of the input to these points. Such a quantizer ensures that the output is unbiased, which ensures analytical tractability. Furthermore, when the quantizer is unsaturated, i.e., when the number of quantization levels in infinite, or the quantizer input lies in a pre-defined dynamic range, the variance the the quantization error is also bounded. \n\nThe authors use these two ideas in this work to derive error bounds for a multi-layer (deep) feed-forward neural neural network."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Quantization of neural networks is an important problem, provided the significant research impetus that is going into this domain in order to  make modern large neural networks (which can easily consist of billions of parameters) deployable on memory-constrained devices.\n\nOne of the strengths of this work is the detailed theoretical analysis that has been done in order to derive the error bounds on the relative quantization error. The authors have put commendable effort in stating the relevant preliminaries and building up to the theorems with appropriate lemmata which can be quite useful in general.\n\nThe paper is generally well-written."
            },
            "weaknesses": {
                "value": "I have some concerns in general, and would be happy to revise my score if these are satisfactorily addressed:\n\n1. It is not clear what the novelty of this work is. It seemed to me that the core framework regarding the greedy quantization algorithm was already proposed in the prior work Lybrand & Saab (2021). In terms of proposed algorithm, is the only contribution of this work the replacement of the deterministic quantizer in Lybrand & Saab (2021) by a stochastic quantizer in order to make the analysis feasible for deeper neural networks? If so, the authors should clarify the fact that stochastic quantizer is NOT actually proposed first in this paper, since I could not find any relevant references to the same where stochastic quantization is introduced. Eq. (6) in this work is a well-known quantizer design. See for example:\n\n(a) Non-subtractive dither (https://ieeexplore.ieee.org/document/823976) -- This is exactly the same, or for instance, see\n\n(b) Eq. (4) of (https://ieeexplore.ieee.org/abstract/document/4542554)\n\nThe idea may date even earlier, but stochastic quantization is quite widely used for quantization in distributed optimization, etc. Keeping this in mind, it seems like the idea of replacing the deterministic quantizer of GPFQ with a stochastic quantization is straightforward. Although, the analysis of this and the conclusions is not trivial.\n\n2. It is very important to keep in mind that stochastic quantization cannot actually be implemented in hardware as it is not easy to quantize to values that are not allowed by current hardware primitives provided by the GPUs (eg., 4-bit Floating point). This is important to noted because although the theory is pretty comprehensive with stochastic quantization, modifications must be made to the algorithm to account for hardware limitations. For instance, the stochastic quantizer ensures that the quantizer out put is unbiased, and this gives a nice upper bound on the expected quantization error. However, since this problem highly inspired from practical purposes, it is important to show some numerical simulation results for the non-deterministic version, especially since it might be likely that the bias in quantization errors from deterministic quantizers may propagate and accumulate in the greedy algorithm. For example, parallel research directions such as this: https://github.com/TimDettmers/bitsandbytes also work on neural network quantization, and assume that the weights are Gaussian, but translate to actual hardware benefits when deployed. \n\n3. The work only considers fully connected neural networks and subsequently, uses results from vector quantization. In order neural network models like attention, parameters can be matrices instead, where low rank compression becomes important in addition to quantization. It is important that the authors highlight this limitations.\n\n4. Please move the numerical simulations to the main paper. A problem which is inspired by a practical issue as this, should be justified with numerical simulations. Also, most of the times, GPFQ performs better than SPFQ. Why should SPFQ be preferred in that case? Expect for the theoretical guarantees?\n\n5. Since the weights in this greedy algorithm are quantized sequentially, how is this sequence actually determined? Different weights in a networks have different sensitivities to the loss function -- is that taken into account while determining the sequence of quantization?\n\n6. Please add a conclusions section."
            },
            "questions": {
                "value": "In addition to the concerns mentioned in the weakness section above, I have a few more questions. I'd really appreciate it if the authros could address them:\n\n1. Comparing the assumptions in Thm 3.1 (infinite quantization points) and Thm. 4.1 (finite quantization points), it looks like the Gaussian weight assumption is only used in the latter. If I understand correctly (please correct me if I'm mistaken), then this assumption allows from a finite bit requirement because of strong concentration properties of the Gaussian distribution. Analysis of the expected quantization error for the Gaussian weights assumption yields a bit-requirement of log log N per parameter.\n\nA similar result can be obtained without the Gaussian weight assumption in the worst-case analysis. This is done by side-stepping the Gaussian assumption by utilizing a Randomized Hadamard transform (which has similar concentration properties that can be obtained using Chernoff bounds). This was done in this recent work (https://ieeexplore.ieee.org/abstract/document/10095529) for linear regression and classification models. See Table 1. Since the error bounds essentially depend of the vector quantization error for Lipschitz activations, can similar results be derived utilizing the Hadamard transform based randomized quantizer as proposed in this work to sidestep the Gaussian weights assumption ,and without changing the final conclusion of the results?\n\nI would be more than happy to revise my score post-rebuttal if I am mistaken anywhere and/or if the concerns are adequately addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission378/Reviewer_sBRp"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission378/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699092855440,
        "cdate": 1699092855440,
        "tmdate": 1699635964676,
        "mdate": 1699635964676,
        "license": "CC BY 4.0",
        "version": 2
    }
]