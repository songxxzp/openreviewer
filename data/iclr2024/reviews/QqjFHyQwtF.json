[
    {
        "id": "7GoZLS3BVH",
        "forum": "QqjFHyQwtF",
        "replyto": "QqjFHyQwtF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
        ],
        "content": {
            "summary": {
                "value": "To integrate acoustic information into the speech recognition output error correction using large language models (LLMs), the authors of this paper compare three fusion methods: early, mid, and late fusions. To further improve late fusion, this paper investigates adding uncertainty and proposes a new approach, Uncertainty-Aware Dynamic Fusion (UADF), introduced to integrate acoustic information, significantly enhancing word error rate (WER) and showing potential in audio-visual speech recognition."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Overall, this paper's structure is easy to follow.\nIt reminds me of previous milestone works about RNN decoders: Shallow fusion, Cold fusion, and Deep fusion. And this paper also has the potential to be a milestone. \n\nThe UADF is novel in this paper, and the improvements are guaranteed shown in Tables 1 and 2."
            },
            "weaknesses": {
                "value": "ICLR conference is less specialized for speech researchers than ICASSP and INTERSPEECH. A brief and precise background introduction to the speech recognition framework is needed.\n\nFor the three frameworks in Figure 1, it needs to be clarified: where is N-best from? The N-best and the X_tok, X_enc, and X_dec are from different types of models according to the descriptions. It is better to mention these differences in the figures.\n\n\"language models have been widely utilized in ASR tasks over the past two decades,\" maybe there is a better way to express it; some earlier efforts are ignored.\n\nAdditionally, I believe you want to demonstrate that the proposed method is also applicable to audio-visual tasks. This requires a more detailed description; merely using a single paragraph here is insufficient."
            },
            "questions": {
                "value": "The N-best are from WavLM and Whisper in GER-based H2T neurlPS2023. This paper is actually implementing a system combination, which, of course, will bring accuracy improvement. So, this work can be extended to more wider tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698596295748,
        "cdate": 1698596295748,
        "tmdate": 1699636327373,
        "mdate": 1699636327373,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vlfmncpXsh",
        "forum": "QqjFHyQwtF",
        "replyto": "QqjFHyQwtF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
        ],
        "content": {
            "summary": {
                "value": "This paper builds upon the recently proposed \u201cgenerative error correction\u201d (GEC) paradigm for speech recognition (ASR). The previous GEC work used a text-only LLM to map from a list of ASR hypotheses to a single transcription. In this paper, the authors propose to add acoustic information to this mapping process by including speech representation in the GEC. They experiment with several such \u201cfusion\u201d strategies (so-called early, mid, and late fusion), and finally show that late fusion with uncertainty awareness works best. Evaluations are conducted on a number of ASR benchmarks, showing significant WER improvements compared to GEC or ASR-only baselines, and the method is also shown to generalize to audio-visual ASR."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Using LLMs for ASR error correction is an interesting idea and has shown strong results in the past. ASR models often perform badly on rare words such as named entities, and LLMs can be a useful component to improve recognition of such words. In the earlier GEC work, only the top hypotheses of the ASR model was used for the final transcription. Since these hypotheses may not always contain information from the correct word, it is natural to also use the ASR information more directly. With these considerations, the ideas in this paper are relevant and a logical continuation in this line of work.\n\n2. Incorporating modality-specific uncertainty using the UADF approach is a novel and interesting technique, and seems to provide reasonable improvements compared to static weighting of modalities. For example, in Table 4, we see that particularly on very low SNR conditions, there is a large improvement in WER when using UADF instead of static weights. I also appreciate that the authors show ablation results and analysis for the UADF strategy in Section 5.2.\n\n3. In addition to traditional ASR benchmarks, the authors show that their UADF strategy can be used for audio-visual ASR, where it gives a small improvement over AV-HuBERT."
            },
            "weaknesses": {
                "value": "My main concerns about the paper are about the presentation (in terms of relation with prior work) and comparison with baselines in the experiments, which I will detail below.\n\n### This model in the context of prior work\n\nIn order to make this point clear, let me briefly summarize some key events in the history of the use of language models (LMs) in ASR.\n- ASR was formulated as a noisy channel model using the Bayes rule P(W|X) = P(X|W)P(W). The two distributions were named an acoustic model and a language model, respectively. The LM was trained separately on source text, and only used for decoding [1].\nWFST-based decoding provided a way to create a incorporate n-gram LMs into the decoding graph for efficient **first-pass decoding** [2].\n- Incorporating larger n-gram LMs was hard (since the decoding graph explodes), so researchers used them instead in **second-pass rescoring** in both offline and on-the-fly settings [3, 4], by subtracting the original LM scores and adding back the larger LM\u2019s scores.\n- The Bayesian formulation still made sense in the era of hybrid HMM-DNN model, even when the acoustic models were discriminatively trained, since the scores could be interpreted as pseudo-likelihoods by subtracting an appropriate prior, and so the same decoding/rescoring framework carried over.\n- In the era of \u201cend-to-end\u201d ASR, the models are trained to directly estimate P(W|X), and so log-linear interpolation with an external LM is not interpretable in the Bayesian sense anymore. Yet, practitioners still do this and call the process \u201cfusion\u201d (e.g. shallow fusion, cold fusion, etc. [5, 6]) \u2014 this is analogous to using LMs in the first-pass decoding.\n- Analogous to the second-pass rescoring, we can obtain candidates using beam search on an ASR model, and then re-rank them with an externally trained LM (e.g. as done in the original LAS paper). So far, this \u201crescoring\u201d step only manipulates the hypotheses, without considering the acoustic information. Let\u2019s call this model (A).\n- More recently, a two-pass E2E ASR model was proposed which has an encoder shared between a streaming RNN-T model and a full-context LAS decoder [7]. The idea was to perform a first-pass decoding with the RNN-T head, and use the decoded hypotheses along with the encoder representations to rescore using the LAS head in a *acoustic-guided rescoring* technique.\n- In [8], the authors further built upon this two-pass model by proposing a **deliberation network**, which is an LAS decoder that attends to both the acoustic representations as well as the first-pass hypotheses, to generate the output. Let\u2019s call this model (B). \n\nThe Generative Error Correction (GER) model (which this paper extends) is analogous to model (A), with the exception that it formulates the rescoring problem as a many-to-one sequence transduction task, as opposed to a simple re-ranking task. \n\nThe proposed model is similar to model (B), as is clear from equation (2). Its goal is to generate a new sequence given acoustic representations and an N-best list. In fact, the \u201cearly\u201d and \u201cmid\u201d fusion methods in the paper are, in my opinion, exactly the same as a deliberation model, with the exception that the decoder is frozen instead of being jointly trained. \n\nSituating late fusion is a little more complicated since the fusion only happens at the scoring stage, and it is quite confusing to me exactly how to think about it. This \u201clate fusion\u201d is, in a way, similar to shallow fusion methods in E2E ASR, with the exception that a GER model is used in place of the external LM. The other view is how the authors present it: GER with additional acoustic information.\n\nWhile it is completely acceptable to use new terminology that is more in keeping with broader advances in LLMs, I think it would be beneficial to situate the proposed method in the context of the above ASR+LM strategies to make the paper more widely accessible. In summary, the use of LMs in ASR can be through either first-pass decoding or second-pass rescoring (this paper falls into the latter). Second pass rescoring may be modeled as re-ranking or sequence generation, and methods may or may not use the original acoustic information. This kind of taxonomy and categorization can be used to clarify the contribution of this work. \n\n[1] Jelinek, Frederick. \u201cContinuous speech recognition by statistical methods.\u201d Proceedings of the IEEE 64 (1976): 532-556.\n\n[2] Mohri, Mehryar et al. \u201cSpeech Recognition with Weighted Finite-State Transducers.\u201d (2008).\n\n[3] Ljolje, Andrej et al. \u201cEfficient general lattice generation and rescoring.\u201d EUROSPEECH (1999).\n\n[4] Sak, Hasim et al. \u201cOn-the-fly lattice rescoring for real-time automatic speech recognition.\u201d Interspeech (2010).\n\n[5] Chorowski, Jan and Navdeep Jaitly. \u201cTowards Better Decoding and Language Model Integration in Sequence to Sequence Models.\u201d Interspeech (2016).\n\n[6] Sriram, Anuroop et al. \u201cCold Fusion: Training Seq2Seq Models Together with Language Models.\u201d Interspeech (2017).\n\n[7] Sainath, Tara N. et al. \u201cTwo-Pass End-to-End Speech Recognition.\u201d ArXiv abs/1908.10992 (2019): n. pag.\n\n### Evaluation problems\n\nIf we accept the above categorization of models, the paper can be viewed in 2 ways: (i) shallow fusion of ASR and GER, and (ii) GER with acoustic information. As such, the proposed method should be compared against the following baselines: (1) end-to-end ASR, (2) shallow fusion of ASR and LLM, and (3) GER-only. The authors have compared against (1) and (3), but not against (2), which may be important to show how using GER improves compared to simply using a LLM with shallow fusion.\n\nThis is important because training the GER itself requires generating N-best hypotheses for the whole training data, which is computationally expensive, and it can only be used in an offline manner, while simple shallow fusion can be done on-the-fly. As such, we expect to see significant WER gains when using the GER module.\n\nAnother concern, which is a common issue when using LLMs, is the following: *How can we ensure that the test data was not seen by LLAMA?* In particular, the authors show in Table 3 that we get a large improvement on CHiME-4 when using the proposed method. CHiME-4 is essentially WSJ read outdoors, and WSJ consists of text from newspapers, which are in the public domain. It would be more useful to conduct experiments on a closed dataset, such as CHiME-5, which is less likely to have been memorized by LLAMA.\n\n### Other minor comments\n\n1. Some sections of the paper use a lot of flowery language which can be avoided. For example, in the last paragraph in Section 1, extraneous terms such as \u201ca pioneering UADF technique\u201d, \u201cadroitly allocates\u201d, and \u201cconspicuously outperforming\u201d can be made concise. In general, authors should stick to reporting rather than embellishing.\n2. In Section 2 (paragraph 1), the authors mention that language models have been used in ASR for two decades, but the oldest citation is only from 2019. I think they should conduct a more thorough literature review in order to perform correct credit attribution.\n3. The use of the terms \u201cearly\u201d, \u201cmid\u201d, and \u201clate\u201d fusion in the paper may be confusing for readers familiar with ASR methods such as shallow, cold, and deep fusion. The authors should consider either changing *fusion* to another word, or making the difference explicit in the paper."
            },
            "questions": {
                "value": "Some of the proposed methods are specific to particular model choices. For example, (i) \u201cearly fusion\u201d assumes that we are using Wav2Vec 2.0 as the encoder, and (ii) late fusion would only work with an encoder-decoder style model. The dominant ASR modeling strategy in the industry is conformer-transducers, which do not have such speech tokens, and their logit space is 3-dimensional (where vocabulary dimension also includes a blank token). Have the authors considered how the proposed method would work with such models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698623996716,
        "cdate": 1698623996716,
        "tmdate": 1700502205510,
        "mdate": 1700502205510,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fuF73OSo5o",
        "forum": "QqjFHyQwtF",
        "replyto": "QqjFHyQwtF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a late fusion strategy called \"UADF\" that combines the modalities of LLM and ASR to enhance generative error correction. The research demonstrates promising results across multiple datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Integrating acoustic information with LLM is a promising strategy that enhances the capabilities and potential of LLMs in ASR tasks. This approach leverages the strengths of both acoustic processing and advanced language modeling, providing a more robust framework for GER.\n2. The paper conducts a comprehensive examination of different fusion strategies, delving into detailed analyses of each approach.\n3. The proposed UASF yields promising results across datasets with varying conditions, including ASR and VASR."
            },
            "weaknesses": {
                "value": "I believe it's not entirely fair to make comparisons with GER given the use of acoustic information.  It would be better to add some comparisons of the results with LLM scoring."
            },
            "questions": {
                "value": "1. What distinguishes the proposed method from shallow fusion using LLM?\n2. In equation 5, do $f^{llm}$ and $f^{asr}$ need to be of the same length? If they do, how to ensure this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw",
                    "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628645109,
        "cdate": 1698628645109,
        "tmdate": 1699648252414,
        "mdate": 1699648252414,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ec1qg2gFIH",
        "forum": "QqjFHyQwtF",
        "replyto": "QqjFHyQwtF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
        ],
        "content": {
            "summary": {
                "value": "Recent studies showed that large language models (LLMs) can be successfully used to map the N-best hypotheses list generated by an ASR system to the predicted output transcription to improve ASR accuracy. But LLM is not trained with acoustic information. This paper proposed a method named Uncertainty Aware Dynamic Fusion (UADF) to infuse acoustic information to the auto-regressive decoding process with ASR decoder and LLM predictions. Specifically, it uses late fusion to combine the probability output by ASR decoder and LLM to predict the recognition results. In this process, temperature scales are applied to the logits of ASR decoder and LLM to match up the model confidence and recognition accuracy. Besides, the ASR and LLM combination weights are decided by LLM entropy so when LLM\u2019s uncertainty is larger, more compensation from the ASR model will be used to decide the recognition results. The proposed method showed obvious WER reduction for several ASR tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper proposed several methods applied to the fusion of ASR decoder and LLM output to improve performance. The paper compared the proposed method with several existing methods and did some ablation study to analyze the effectiveness of the proposed method. It also did experiments to prove the generalization of the proposed method. All the results showed the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "\u2022\tSome contents are not accurate or clear. \no\tIn eq. (9), the entropy of LLM output is not accurate since for a discrete distribution, the entropy should be the sum of -p(y_t,i)logp(y_t,i). \no\tIn figure 2, it\u2019s clear the probability of \u201call\u201d from ASR is high. But it didn\u2019t show where is the probability of \u201call\u201d from LLM ? And what\u2019s the meaning to show the blue cross in the left bottom part and the green cross in the right bottom part? \no\tIn the experiments part, for all late fusion experiments, the ASR model needs to be retrained with the training data. But it\u2019s not clear whether all the training data are used to get a unified model or for each task (WSJ, ATIS, Chime-4), the model is updated only with the training data from this task. \no\tThe results in table 2 showed that without calibration, the proposed UADF did not perform better than the static method (WER 1.39 vs. 1.36). This means calibration is crucial for UADF, so the tuning of temperature t1 and t2 becomes a key to make it work. But the paper didn\u2019t give enough information on these two parameters, such as what\u2019s the optimal value, should we tune them for different tasks and so on."
            },
            "questions": {
                "value": "1.\tDo we have the WER of the top 1 hypothesis in HyPoradise dataset for different tasks? These values may not be useful to validate the proposed method. But it will make the readers understand better why we need to use LLM to refine the N Hypotheses.  \n2.\tAs noted above, tuning of temperature t1 and t2 becomes a key point to make the proposed method work. The questions about this are: \na.\tDid the author observe the same conclusion for other tasks (WSJ,  Chime-4, LRS3)? \nb.\tDo we need to tune these values for different tasks, or we could get them with one task\u2019s data and applied them to other tasks.\nc.\tHow much minimum data should we use to tune these values? \nd.\tWhat\u2019s the optimal value for these two parameters for the tasks in this paper? \n3.\tIn eq(10), the weight of LM and ASR didn\u2019t sum up to 1.0. Will it matter since this means for different t, the range of P(y_t) will be different. \n4.\tIt\u2019s noted that \u201cbeta\u201d in eq. (10) is set to 0.5 for the experiments. It didn\u2019t show what\u2019s the performance is if we change it to other values. Will the results be sensitive to this value?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238760036,
        "cdate": 1699238760036,
        "tmdate": 1699636327062,
        "mdate": 1699636327062,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ibhuxq04ll",
        "forum": "QqjFHyQwtF",
        "replyto": "QqjFHyQwtF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
        ],
        "content": {
            "summary": {
                "value": "The authors present an approach they describe as Uncertainty Aware Dynamic Fusion (UADF) that 1. calibrates the LLM score at the token level accounting for the over-confidence of the neural network when doing auto-regressive decoding, 2. a time-changing uncertainty uncertainty that dynamically adjusts the decision-level fusion of text-based LLM and ASR scores. The results show significant improvement over strong baseline models such as Whisper and wav2vec or Hubert on a variety of tasks. Further the authors demonstrate that the results can be applied in an AVSR task with the dynamic uncertainty improving results in noise over a static fusion."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall the work is well motivated and demonstrates strong empirical results."
            },
            "weaknesses": {
                "value": "The point about calibration in using LLMs for rescoring of ASR hypotheses may be novel, however both calibration of hybrid or neural network ASR model scores and the use of strong language models combined with ASR scores are not novel approaches and have been widely used within speech. \n\nThe paper seems to desire establishing the term \"modality laziness\" as a technical way of describing when multimodal system underachieve compared to a unimodal system; in this case I would describe this as a simply a poorly designed system. There are multiple papers in the literature that describe effective ways of combining audio and visual modalities to yield an improve AVSR system over any single modality. Look for the term \"modality drop out\" and work on audio-visual conformers or AV-hubert.\n\nApplying the entropy of the LLM predictive posterior eqn (9)  as a weighting factor for the dynamic fusion neatly correlates with the desired property of: when the LLM is more certain, then the ASR weight of the ASR can be less, and when the LLM is uncertain, then the ASR weight is higher.  However, it doesn't stem from the statistical approach of ASR where \nY_T = argmax Y_n p(X|Y_n; M_am) P(Y_n; M_lm ) where the first term in the argmax is the acoustic score and the second term the text-based LLM score, which could be computed with a LLM. In this light, without a Bayesian motivation, one might consider UADF as a hueristic method of combining LLM scores with ASR.\n\nLastly, the results here are all conducted on speech recognition tasks. Its unclear if such an approach can be applied to non-speech tasks or is of interest to the broader ICLR community."
            },
            "questions": {
                "value": "Can you re-frame UADF in a Bayesian framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699333901717,
        "cdate": 1699333901717,
        "tmdate": 1699636326971,
        "mdate": 1699636326971,
        "license": "CC BY 4.0",
        "version": 2
    }
]