[
    {
        "id": "Vp2lILktrE",
        "forum": "VmGRoNDQgJ",
        "replyto": "VmGRoNDQgJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission751/Reviewer_6EGX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission751/Reviewer_6EGX"
        ],
        "content": {
            "summary": {
                "value": "This paper explores how to backdoor semantic segmentation in the real world. The proposed method is called influencer backdoor attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and mislead classifications of all victim pixels in every single inference whenever the adversary-specified backdoor trigger appears. In particular, the authors propose nearest neighbor injection (NNI) and pixel random labeling (PRL) to further improve attack effectiveness based on their understanding of the mechanism of semantic segmentation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Semantic segmentation has been widely used in self-driving system. Accordingly, its research topic is realistic and of great significance.\n2. In general, I enjoy the design of the proposed method. In particular, NNI and PRL are designed based on the mechanism of semantic segmentation. Accordingly, the proposed method is not a trivial extension of BadNets against image classification.\n3. The paper is well-written and the proposed method is easy to follow to a large extent.\n4. The experiments are comprehensive to a large extent."
            },
            "weaknesses": {
                "value": "1. It would be better if the authors can provide more details about why you only consider patch-based attack. (More details about semantic segmentation in the real world)\n2. The authors should provide more details in the pipeline figure. For example, the authors should at least highlight the trigger area.\n3. I think it would be better to provide the performance of NNI+PRL in main results since your goal is to design a strong attack.\n4. Please provide more results about the resistance to potential defenses (e.g., image pre-processing).\n5. It would be better if the author can conduct physical experiments (Stamp the trigger patch and use your camera to take a video and send it to the attacked model). \n6. It would be better if the authors can also discuss potential limitations of this work."
            },
            "questions": {
                "value": "Please refer to the 'Weaknesses' part. I will increase my score if the authors can address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Reviewer_6EGX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650815509,
        "cdate": 1698650815509,
        "tmdate": 1700554732815,
        "mdate": 1700554732815,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ujoqOAI5VL",
        "forum": "VmGRoNDQgJ",
        "replyto": "VmGRoNDQgJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission751/Reviewer_dGyx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission751/Reviewer_dGyx"
        ],
        "content": {
            "summary": {
                "value": "The aim of this paper is to design an effective backdoor attack method for image segmentation models, which explores how specific triggers can be injected into non-victim pixels to mislead the recognition of pixels in the victim category. Specifically, the authors propose an effective nearest-neighbor departure injection strategy by considering the contextual relationships of the segmentation model. The authors demonstrate on a large number of experiments that the predictions of the segmentation model may be affected by both near-backdoor and far-backdoor attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The attack scenario has some practicality. The authors propose a novel attack task and reveal the impact of trigger proximity on the attack of the segmentation model.\n2. the related work is presented exhaustively. The article provides an exhaustive review of related work and provides the reader with a historical background of research in this area."
            },
            "weaknesses": {
                "value": "1. The authors claim to be the first backdoor attack work on segmentation models, but in my opinion this is not the case. In fact, there have been some discussions about backdoors for segmentation models, e.g., [1], [2], and the authors should differentiate and compare with the above methods and demonstrate the advantages of the method.\n2. poisoning triggers are not realistic and require extremely high poisoning rates for effective backdoor attacks. Firstly, the presentation of the trigger in Fig. 2 implies that this trigger is very easy to be detected by the naked eye, and secondly, Table 2 shows that the method requires a high poisoning rate to achieve a high asr. Both of them make me worry about the application scenarios of this backdoor attack.\n3. Results of other defense experiments. Although the authors compare many fine-tuning-based defense methods to prove the effectiveness of the proposed backdoor, I am still concerned about whether the existing attack methods are able to overcome the existing backdoor defense methods, such as data cleansing methods, model modification methods, and model validation methods.\n\n[1]Hidden Backdoor Attack against Semantic Segmentation Models.\n[2]Object-free Backdoor Attack and Defense on Semantic Segmentation"
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Reviewer_dGyx"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759019996,
        "cdate": 1698759019996,
        "tmdate": 1700584516058,
        "mdate": 1700584516058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p8Wnt4JJSI",
        "forum": "VmGRoNDQgJ",
        "replyto": "VmGRoNDQgJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission751/Reviewer_tjpw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission751/Reviewer_tjpw"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach to executing backdoor attacks on semantic segmentation models. It begins with a detailed formulation of backdoor attacks specific to semantic segmentation tasks. In addition, the authors develop a foundational baseline for executing such attacks and refine this approach by introducing advanced techniques, namely nearest neighbor injection and pixel random labeling. The effectiveness of these proposed methods is evidenced by strong experimental results, showcasing the robust performance of the attack strategies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper introduces backdoor attacks in the context of semantic segmentation, a topic more closely related to AI applications than previous backdoor endeavors. \n- The authors provide a robust formulation of backdoor attacks for semantic segmentations.\n- The experimental results are striking, with a 95% attack success rate after poisoning only 10% of the VOC training set, which is quite remarkable."
            },
            "weaknesses": {
                "value": "- The paper did not provide experiments in the real-world. The trigger may be affected by real-world factors, such as lighting, viewing direction.\n- The trigger employed in this paper is sizable and conspicuous. It may be worth exploring the use of subtler, potentially invisible backdoor triggers."
            },
            "questions": {
                "value": "- The method necessitates alterations to the labels in the training set, which could be readily identified by some pre-trained semantic segmentation models.\n- Sometimes we only finetune a pretrained large model in a small downstream dataset, typically requiring only a small number of epochs. This might not be adequate for the model to sufficiently learn dependency on the backdoor triggers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission751/Reviewer_tjpw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760542805,
        "cdate": 1698760542805,
        "tmdate": 1700647106377,
        "mdate": 1700647106377,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j4Nou4rX8f",
        "forum": "VmGRoNDQgJ",
        "replyto": "VmGRoNDQgJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission751/Reviewer_DVe6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission751/Reviewer_DVe6"
        ],
        "content": {
            "summary": {
                "value": "The paper studies backdoor attacks on semantic segmentation models, such that when a given trigger is inserted in test images the pixels of a victim class are classified instead into a different target class. A baseline method to create poisoned data, Influencer Backdoor Attack (IBA), is introduced, together with two improvements of it, Nearest-Neighbor Injection (NNI) and Pixel Random Labeling (PRL). In the experiments, the attacks, in particular PRL, is shown to achieve high success rate when the trigger is added to test images, while preserving clean performance (i.e. on non-victim pixels and images without the trigger) very close to the one of clean models. Finally, the found attacks are even tested effective in real-world scenarios."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Backdoor attacks for semantic segmentation models are an interesting threat model, apparently underexplored in prior works, and the paper fills this gap.\n\n- The proposed methods are effective in the experimental evaluation on several architectures and datasets, and even in the real-world scenes. In particular, PRL improves the poisoning rate necessary to achieve high success rate.\n\n- The paper provides extensive ablation studies on the parameters of the proposed attacks to support the design choices."
            },
            "weaknesses": {
                "value": "- It is not clear why, by default, the triggers are constrained to overlap with pixels of a single class only (if I understand it correctly, this happens both at training and test time): this seems a less natural choice than using a random position regardless of the class of the covered pixels. App. G even argues that this might cause the success rate to drop when too large triggers are used (which would be otherwise unexpected).\n\n- Testing the proposed attacks on more recent and effective backbones than ResNet-50 might enrich the experimental results."
            },
            "questions": {
                "value": "As minor suggestion, I think the real-world scenario results are of particular interest, and could be discussed in more details (and maybe with more images) in the main part of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission751/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839878431,
        "cdate": 1698839878431,
        "tmdate": 1699636002149,
        "mdate": 1699636002149,
        "license": "CC BY 4.0",
        "version": 2
    }
]