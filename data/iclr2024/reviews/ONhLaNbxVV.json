[
    {
        "id": "kOdZYr2xzK",
        "forum": "ONhLaNbxVV",
        "replyto": "ONhLaNbxVV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_KfuC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_KfuC"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new method called R3-ProtoPNet to improve the interpretability and performance of the prototypical part network (ProtoPNet). ProtoPNet is an interpretable image classifier that makes predictions based on prototypical parts of images. However, it can learn spurious or inconsistent prototypes. R3-ProtoPNet collects human feedback on prototype quality to train a reward model. The reward model predicts human preferences between prototypes. R3-ProtoPNet has 3 stages - reward reweighting, prototype reselection, and retraining. Reweighting uses the reward model to update prototypes to be more similar to highly rewarded patches according to human preference. Reselection replaces low reward prototypes with random high reward patch candidates. Retraining realigns the network features and classification layer with the improved prototypes. Experiments were done on CUB-200-2011 birds dataset using VGG, ResNet, and DenseNet architectures. Human ratings of prototype-image pairs were collected via Amazon Mechanical Turk.\nThe reward model achieved over 90% accuracy in predicting human prototype preferences. R3-ProtoPNet improved average prototype reward by 31.65% and activation precision by 46.85% over ProtoPNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Advantages of the approach:\nInspired by the RLHF, the authors use a flexible human feedback mechanism via learned reward model. Reward model also provides a quantifiable metric for prototype quality. Reweighting and reselection improve prototype interpretability. Retraining maintains or improves predictive performance.\nThis approach can be applied to different base architectures like VGG, ResNet, etc.\nMost importantly, reward-guided training aligns prototypes to human preferences.\nThe results are quite compelling: it achieves high reward model accuracy in predicting human preferences -- this indicates it captures prototype quality well. Increased average reward shows prototypes are more meaningful after R3 training.\nImproved activation precision verifies R3 prototypes have better overlap with birds. R3 training maintains or improves accuracy, showing no loss of predictive power. R3 ensemble outperforms ProtoPNet ensemble, demonstrating improved performance.\nExamples show reduced dependence on background and other spurious features after R3 training.\nThus, retraining enables improved prototypes to be utilized for better prediction."
            },
            "weaknesses": {
                "value": "Some potential weaknesses include:\nThe approach strikes as overly hand-crafted. The beauty of ProtoPNet was exactly in that it learned parts from coarse labels. Compared to that, the proposed approach is overly reliant on guidance by manually collected labels.\nLimited evaluation on just one dataset (CUB birds) - needs more diverse evaluation.\nDuplicate prototypes still occur after R3 training.\nReward model may not sufficiently capture cross-image consistency.\nPotential for reward model to penalize useful but non-obvious features."
            },
            "questions": {
                "value": "You showed results on the CUB birds dataset. How does R3-ProtoPNet perform on more complex, diverse datasets like ImageNet? Does it still improve interpretability and accuracy?\nCan you provide some analysis or examples showing that R3 training does not lose focus on useful non-obvious features like texture or contrast?\nHow sensitive is R3-ProtoPNet to the amount and quality of human feedback data for the reward model? How little data can you use and still see benefits?\nYou retrain the full network - have you tried only retraining the classifier layers? This could improve efficiency.\nHow does R3-ProtoPNet scale to larger datasets and models? Is the computational overhead of R3 prohibitive?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712689568,
        "cdate": 1698712689568,
        "tmdate": 1699636723305,
        "mdate": 1699636723305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NWOvmXCTJD",
        "forum": "ONhLaNbxVV",
        "replyto": "ONhLaNbxVV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_Tqnf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_Tqnf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a reward reweighed, reselected, and retrained prototypical part network (R3-ProtoPNet) that improves upon the original prototypical part network (ProtoPNet) by Chen et al. (2019). Given a trained ProtoPNet, the proposed R3-ProtoPNet involves: (1) collecting human feedback regarding the quality of learned prototypes; (2) learning a reward function that takes as input an image and its prototype activation map from a particular prototype, and outputs a reward that represents how good the prototype activation map is for the given image; (3) using the learned reward function to reweigh the inverse distance to a prototype and to improve the prototype; (4) reselecting prototypes whose rewards are below a threshold by replacing them with random patch representations whose rewards are above the acceptance threshold; (5) retraining the model to improve the prediction accuracy. The authors performed extensive experiments using several ProtoPNet models with various base architectures, and concluded that their R3-ProtoPNet has higher test accuracy, higher average rewards (based on the learned reward function), and higher activation precision over the original ProtoPNet."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors proposed a way to improve the prediction accuracy of ProtoPNet, which is an important interpretable deep classifier.\n- The proposed method is generally sound.\n- The paper is easy to read."
            },
            "weaknesses": {
                "value": "- There is no evaluation of the learned reward function.\n- While the proposed R3-ProtoPNet does empirically improve the prediction accuracy over the original ProtoPNet, there is little theoretical insight as to what makes R3-ProtoPNet work.\n- Is it necessary to have both reward reweighing and prototype reselection? More specifically, while prototype reselection is more necessary (it is a way to move away from badly learned/low-reward prototypes), reward reweighing seems optional to me. An ablation study on this will be helpful.\n- There are limited visualizations. While the authors did include visualizations of prototypes (in the appendix), they did not include enough examples of how prototypes are used in R3-ProtoPNet, the closest prototypes to a given image from a trained R3-ProtoPNet, and the closest image patches to a given prototype learned by an R3-ProtoPNet. These are needed to convince readers that an R3-ProtoPNet uses high-quality prototypes in reasoning, and the learned prototypes are semantically meaningful."
            },
            "questions": {
                "value": "- An R3-ProtoPNet can be thought of as initializing a ProtoPNet in a smart way (by reward reweighing and prototype reselection), and then training it again. As mentioned earlier, is there any explanation for why this empirically improves the prediction performance?\n- As mentioned before, is it necessary to have both reward reweighing and prototype reselection? Please include an ablation study on this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792266863,
        "cdate": 1698792266863,
        "tmdate": 1699636723172,
        "mdate": 1699636723172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y1B1ZJ3gin",
        "forum": "ONhLaNbxVV",
        "replyto": "ONhLaNbxVV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_cp6j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_cp6j"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a debugging procedure for Part-prototype Networks based on human feedback.  Rather than using the feedback as-is, as done by prior work, the proposed method (R3-PPNet) uses it to train a reward function, which generalizes said feedback, and use the latter to drive the model away from bad prototypes.  The model refinement step uses a couple of heuristics to improve the model.  Experiments are carried out on the CUB-200 dataset augmented with prototype rating feedback collected with Mechanical Turk."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality**: To the best of my knowledge, the idea of combining RLHF mechanics and concept debugging (also, I like it a lot).\n\n**Quality*:  The proposed technique is sensible (all three stages), and makes good use of existing techniques.  The coverage of related work is good.  The experimental setup is mostly satisfactory (for instance, the authors consider several backbones and good evaluation metrics), but see below.  I also appreciate how the authors were upfront about limitations of their technique.\n\n**Clarity**: The text is very readable.  Ideas are conveyed clearly.\n\n**Significance**:  This work tackles an important problem in concept-based models.  The key contribution is, in my opinion, showing that RLHF can be used for debugging learned concepts (or steering the model towards using better concepts).  The specific algorithm itself is not core, and it could be improved, I think.  Regardless, I believe the main contribution will have some impact on interactive debugging techniques."
            },
            "weaknesses": {
                "value": "**Originality**:  This work combines existing ideas.  The degree of novelty, from a technical perspective, is limited (but still sufficient, in my opinion).\n\n**Quality*:  [Q1] One issue with the experiments is that they consider a single data set (CUB-200).  Bontempelli et al. (cited by the authors) do evaluate their approach on three data sets (CUB-200, a synthetic data set, and an X-ray data set).  The choice of focusing on CUB-200 only is not exactly justified.\n\n[Q2] It is also not clear why R3-PPNet was not compared to the work of Bontempelli et al. -- it should be trivial to convert tratings into binary lables (say, ratings below 3 could be converted to a \"bad\" label, and 4 and above to \"good\").\n\n[Q3] One clear downside of the approach is that the reward model is pre-trained on a large number (in terms of annotation cost) of ratings.  While the cost of collecting ratings is generally compensated for when dealing with LLMs (as these models can be used for a variety of tasks, so you'd want them to be as good as possible), the cost-benefit ration for ProtoPNets is not as clear.  I think this should, at the bare minimum, be discussed in the limitations.\n\n\n**Minor issues**:\n\n- In Section 5.1, you wrote \"R3-ProtoPNet requires two datasets\", but that's not true.  It needs one dataset with additional annotations:  now new *inputs* are added by this second \"dataset\".  I'd prefer if the text was changed accordingly."
            },
            "questions": {
                "value": "Please see Q1-Q3 above.\n\nQ4.  It seems to me R3-PPNet is designed for passive learning only.  The reason is that, if I understand correctly, the reward function (which depends on the learned model) becomes obsolete after fine-tuning/debugging the model.  If the debugged model is still buggy, the old reward function cannot be used again.  Is this correct?\n\nQ5.  How have alpha, beta, and gamma been chosen in the experiments?  How should users of your system choose them?  How sensitive is the quality of the resulting model to the choice of thresholds?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Reviewer_cp6j"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6468/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698843674696,
        "cdate": 1698843674696,
        "tmdate": 1699636723021,
        "mdate": 1699636723021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "etyJhkZuEc",
        "forum": "ONhLaNbxVV",
        "replyto": "ONhLaNbxVV",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_Fuan"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6468/Reviewer_Fuan"
        ],
        "content": {
            "summary": {
                "value": "The authors propose three modifications to the training of prototypical part networks, inspired by RLHF (i..e using lay human feedback on the quality of prototypes). The authors demonstrate that this approach improves the interpretability of the learned prototypes according to three metrics, and provide qualitative support for the improvements conferred by their approach."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Clear relationship with differentiation from/discussion of improvements from protopnet. \nOverall well written, specially the section on 4.2. Very good integration of notes,\u00a0explanation, and notation/math (this is surprisingly rare, so good job!)."
            },
            "weaknesses": {
                "value": "While it's great that your scope is clear, it ends up feeling a bit like a lab report, where your assignment was to apply RLHF to ProtoPNet and report the results. This isn't bad per-se; science for the sake of doing it can be great, but ideally for a scholarly paper I would want to see some more interpretation, insight, and high-level thinking.\u00a0\n\nIt would also be nice to see some more illustrative/selective qualitative results.\n\nFinally, it would be ideal to have an evaluation done by expert human evaluators (whatever that group might be depending on the intended goal of the paper, if any) -- ie. most likely, ornithologists or birders. Given that the focus is on interpretability, how useful are the prototypes actually, to humans who might use them? Do they find the same characteristic features people use for ID? Any novel and interesting ones?\n\nCaptions of tables could be improved to be more descriptive."
            },
            "questions": {
                "value": "The \"parts\" highlighted by the original or your modified version tend to be very soft-edged and blobby. In some sense I would \"like\" to see prototypes that crisply highlight e.g. tails, feet, beaks, etc. Do you think this is desirable and/or possible? Why or why not? E.g. the pixel-level segmentation maps used in AP seem like they could provide very good supervisory signal for this.\nI also feel there could be a more nuanced discussion of what makes a feature spurious or not. E.g. it's repeatedly stated that the background is spurious, but (as an amateur birder) I can tell you habitat (especially if it includes something as characteristic as a nest) can often be incredibly useful for identification. Even the presence of sky vs. grass or sea or something could be informative (e.g. woodland thrushes would virtually never be photographed against open sky, unlike a swallow). And even at a more general level, the shape/contour of e.g. tailfeathers or the bird's overall silhouette (i.e. the edge pixels which would include some bg) are also often characteristic.\nThe main stated high-level goal of the work is to improve interpretability ... for whom, and for what purpose? If everything worked \"perfectly\", what would this system be able to do/be used for? Or if it doesn't have a goal in mind (which again, IMO is totally fine and great for science), what do we understand better about RLHF or Prototype nets or deep nets in general as a result of your work?\n\nsmall things:- protopnet should be cited in abs- briefly state what results/evidence you found supports the claims about improvement (second to last sentence)\u00a0- suggest renaming the section \"limitations\" to \"limitations of ProtoPNet\" to avoid confusion with typical \"limitations\" sections.\u00a0- interpretability ...leads to a .... suggest rephrasing as \"interpretabilty\u00a0... is useful for RLHF\" or something like that.\u00a0\u00a0- explain what is the Bradley Terry Model and why you want to use it"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6468/Reviewer_Fuan"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6468/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699657812316,
        "cdate": 1699657812316,
        "tmdate": 1700961869854,
        "mdate": 1700961869854,
        "license": "CC BY 4.0",
        "version": 2
    }
]