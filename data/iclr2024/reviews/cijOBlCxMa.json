[
    {
        "id": "6RgIYIUd3p",
        "forum": "cijOBlCxMa",
        "replyto": "cijOBlCxMa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_h7FN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_h7FN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a CustomNet, an innovative approach to customizing objects in image generation based on SDs. It overcomes the limitations of existing methods, including slow optimization, identity preservation issues, and copy-pasting effects. It incorporates 3D novel view synthesis, allowing for diverse and identity-preserving outputs. It provides precise location and background control, surpassing existing techniques. This method enables zero-shot object customization without test-time optimization, offering better control and enhanced visual results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Incorporating 3D novel view synthesis differentiates the proposed CustomNet from prior approaches, improving identity preservation and varied output generation.\n\n2. This method's intricate designs address the limitations of existing techniques, allowing for precise object location and flexible background control.\n\n3. The proposed pipeline handles real-world objects and complex backgrounds effectively.\n\n4. The proposed zero-shot object customization is achieved without test-time optimization, allowing for control over the location, viewpoints, and background."
            },
            "weaknesses": {
                "value": "1. The proposed CustomNet focuses on the limitation of 256 \u00d7 256 resolution, which may affect the quality of generated images.\n\n2. Non-rigid transformations and object style changes are not supported, limiting flexibility."
            },
            "questions": {
                "value": "1. How is CustomNet different from past methods for object customization? How does it achieve better identity preservation and varied output generation?\n\n2. Can you explain the detailed designs mentioned in the text that allow for location and background control in CustomNet?\n\n3. How does CustomNet handle real-world objects and complex backgrounds in its dataset construction pipeline, and what's the motivation?\n\n4. Can you explain zero-shot object customization without test-time optimization and its multi-aspect control?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698041785953,
        "cdate": 1698041785953,
        "tmdate": 1699636322533,
        "mdate": 1699636322533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qGYJnJOzkz",
        "forum": "cijOBlCxMa",
        "replyto": "cijOBlCxMa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an method for inserting objects into scenes. The main contribution is a unified framework that allows users to specify the background, the object, and its location. The background can be specified by text description (generation) or actual image pixels (composition), while the object is specified by its background-subtracted image, the desired relative camera view, and its bounding box within the final image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "There are two main strengths of the paper:\n\n1. The presented method is an end-to-end solution for the task of placing objects into scenes, which produces harmonious outputs easier than a multi-stage pipeline.\n\n2. The presented results look convincing in both preserving the object's identity a and harmonizing the final composition."
            },
            "weaknesses": {
                "value": "The novelty of the approach is somewhat limited, as it essentially plugs background generation/compositing into the Zero-1-2 architecture. Placing the object into a localized bounding box as a guide instead of keeping it centralized would be straightforward to do in Zero-1-2.\n\nAn interesting component of the system is the synthetic dataset of objects composed onto backgrounds. Publishing that dataset for future research would add to the contribution."
            },
            "questions": {
                "value": "How come you need to specify both the camera translation and image location? Wouldn't it be enough to only encode camera rotation and the bounding box to place the object anywhere in the image?\n\nYou mention that Zero-1-to-3 method cannot produce non-centered objects. But they allow users to specify full 3D camera rotation and translation, which is a component that this paper inherits. Why wouldn't translation enable users to place objects off-center?\n\nIf I understood the section 3.2 correctly, to make Figure 2 more clear, you could show 2 branches as the input to the text encoder: \"sitting on the beach\" for the Generation branch and NULL for the Composition branch.  Although I may be wrong, since the model could possibly take in both  the background image and the textual description. What happens in that case in terms of the output?\n\nPhrases like \"delicate designs\" and \"intricate designs\" seem out of place in this paper. The network design, if that's what this refers to, seems reasonable and straightforward.\n\nIt may be helpful to specify that R is a 3x3 matrix and T is a 3-vector. If that really is the case (as it is in Zero-1-2), then the rotation is not just a rotation, but also non-uniform scaling and sheer and possibly even reflection. Is that too many degrees of freedom to specify as input for a human user? Why not just specify the viewing direction with something like azimuth+altitude?\n\nYou did not define d in equation 2.\n\nA figure visualizing some of the synthetic dataset images would be helpful.\n\nPaper should discuss some remaining artifacts in the limitation section, such as:\n  * Minor change of identity in Figure 1 Row 3 (look at the number 1 on the toy car)\n  * Bad aspect ratio of the dog in Figure 1 Row 4 (it's too squished)\n  * The white pixels around the character in Figure 4 Row 1 Last Column\nWhere do these come from and how would one go about improving them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3663/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3663/Reviewer_vmn1",
                    "ICLR.cc/2024/Conference/Submission3663/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593128018,
        "cdate": 1698593128018,
        "tmdate": 1700495645090,
        "mdate": 1700495645090,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QE4PFRi5kP",
        "forum": "cijOBlCxMa",
        "replyto": "cijOBlCxMa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_96bJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3663/Reviewer_96bJ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach to text-to-image generation that integrates 3D novel view synthesis for enhanced object customization. Unlike traditional methods that have challenges with identity preservation and limited customization, CustomNet offers explicit viewpoint control, location adjustments, and flexible background generation. It leverages a new dataset construction pipeline using both synthetic multiview data and natural images. The result is a model that achieves zero-shot object customization with improved identity preservation, diverse viewpoints, and harmonious outputs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The authors present a comprehensive solution that seamlessly integrates background inpainting into the Zero-1-to-3 pipeline, addressing its potential limitation in altering the background or location of the object.\n3. The paper utilizes the latest vision foundation models, such as SAM and BLIP, in conjunction with \"Zero-1-to-3,\" to construct a dataset that supports the training of the proposed unified framework. The inverse data generation pipeline which decompose the netural image into the training component is interesting."
            },
            "weaknesses": {
                "value": "1. A significant concern with this paper is its limited novelty. It leans heavily on the \"Zero-1-to-3\" model as its foundation. While the addition of background inpainting offers an enhancement, the core mechanism\u2014explicit 3D novel view synthesis\u2014remains unchanged. Moreover, it inherits constraints from \"Zero-1-to-3\", particularly the resolution limitation of 256 \u00d7 256, which might not be practical for real-world scenarios.\n2. The approach to decompose foreground objects using SAM and augmenting them using \"Zero-1-to-3\" is intriguing. However, there are lingering questions about the methodology. For instance, considering that SAM segments every element in a scene, how does the paper specifically determine the foreground object? Furthermore, if the selected foreground object falls outside the \"Zero-1-to-3\" training domain, is the efficacy of the method compromised?\n3. The paper mostly compares CustomNet with \"Zero-1-to-3\" and a few other models. A broader discussion with state-of-the-art models in the domain would have given a better quality assessment of CustomNet's performance. For example, [1] also employs stable diffusion for background inpainting while preserving the appearance of foreground objects through a dual-branch composition.\n\n[1] Li, Siyuan, et al. \"OVTrack: Open-Vocabulary Multiple Object Tracking.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "Please see above weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800559901,
        "cdate": 1698800559901,
        "tmdate": 1699636322359,
        "mdate": 1699636322359,
        "license": "CC BY 4.0",
        "version": 2
    }
]