[
    {
        "id": "JvRypeyEoW",
        "forum": "OwHAzbkk5z",
        "replyto": "OwHAzbkk5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_YacP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_YacP"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an efficient automatic sampler search algorithm, SS, designed to optimize data selection for deep learning model training. While previous methods relied on heuristic guidelines or labor-intensive trials, SS offers a streamlined approach by mapping samplers to a reduced set of hyper-parameters. It quickly gauges sampler quality using an approximated local minimum, making it computationally economical and ideal for vast datasets. Extensive testing shows SS-enhanced sampling delivers notable performance gains, such as a 1.5% improvement on ImageNet, and offers transferability across different neural network architectures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Data sampling strategy is indeed an important topic and could benefit the community.\n2. The paper is well written and there are rigorous motivations for the choice of the SS method and the algorithm is well presented."
            },
            "weaknesses": {
                "value": "Despite the good presentation and mathematical formulations, the results of the proposed method are not convincing enough. The reported baseline methods seem to be significantly lower than the common baselines. For example, MobileNetV2 is known to be able to achieve 72% - 74% top-1 accuracy on the ImageNet-1K dataset. The reported value in the paper is 70.4%. In fact, even the improved results of MBV2 with SS is still significantly lower than the common baseline. The same trends hold for all the models reported in the paper. It would be more convincing if the authors could verify the effectiveness of the proposed method on baseline models with \"normal\" training."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2232/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2232/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2232/Reviewer_YacP"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2232/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737681497,
        "cdate": 1698737681497,
        "tmdate": 1699636156468,
        "mdate": 1699636156468,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oBoMQuzrQ7",
        "forum": "OwHAzbkk5z",
        "replyto": "OwHAzbkk5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_BnW4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_BnW4"
        ],
        "content": {
            "summary": {
                "value": "The authors propose an automatic swift sampler search algorithm to explore automatically learning effective samplers. They then examine the quality of a sampler at a low computational expense by mapping a sampler to a low dimension of hyper-parameters and use an approximated local minimum."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel formulation to map a sampler to a low dimension of hyper-parameters\nThe paper uses an approximated local minimum to quickly examine the quality of a sampler rather than training from scratch, which is efficient and rarely done in previous works.\nThe paper also designs a transform function to smooth the objective function of the sampler search problem and uses Bayesian optimization as the agent for the search process\nThe paper demonstrates the transferability of the learned samplers"
            },
            "weaknesses": {
                "value": "The paper does not provide enough theoretical analysis or justification for its proposed formulation, transform function, and approximation method. \n\n             Can the author provide more profound  justification ?\n\nThe paper does not explain why its method can generalize to different tasks and datasets, or  provide any experiment on how it compares with other sampler search methods in terms of complexity and scalability.\n             \nCan the author complete the relevant comparative experiments ? \n\nThe paper does not conduct ablation studies or sensitivity analysis to show the impact of different components or hyper-parameters of its method.\n\nCan the author explain the effectiveness of components separately?\n\nThe paper does not report the standard deviation or confidence interval of its experimental results, which makes it hard to assess the statistical significance and robustness of its method."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2232/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829072489,
        "cdate": 1698829072489,
        "tmdate": 1699636156386,
        "mdate": 1699636156386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xTkMgHN11W",
        "forum": "OwHAzbkk5z",
        "replyto": "OwHAzbkk5z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_UJut"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2232/Reviewer_UJut"
        ],
        "content": {
            "summary": {
                "value": "The purpose of this paper is to create a sampler that can assign appropriate sampling probabilities to training data in order to improve performance. Unlike previous approaches that relied on heuristic rules or expensive learning methods, this paper proposes an automatic and efficient sampler search algorithm called SS. Specifically, SS employs a new formulation to map a sampler to a lower-dimensional space of hyper-parameters and uses an approximated local minimum to quickly evaluate the quality of a sampler. SS can be applied on large-scale data sets with high efficiency and leads to performance gains on various datasets, e.g., CIFAR10, CIFAR100, ImageNet-1k, and YTF."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The motivation of this paper is clearly illustrated, and it is convincing. How to efficiently and effectively search for a proper data sampling policy is important. \n\n2. The solution is reasonable. The proposed inner loop and outer loop pipeline with Dimension Reduction, Smooth the Objective Function, and Local Minima Approximation designs are innovative."
            },
            "weaknesses": {
                "value": "1. The outer loop searches for the sampler that has the best score on the validation set. It seems the final performance is also reported on the validation set. Should this lead to the model overfitting the validation set? This paper should also report the performance of test sets.\n\n2. The experimental results of the proposed SS is not impressive. The performance of reported models is far behind the SOTA methods, e.g., Swin Transformer and ConvNext. The SS should be tried on SOTA methods to make the performance gains more convincing.\n\n3. More recently proposed works, e.g., automated loss function search e.g., Li et al. (2019a) and augmentation policy search methods Lin et al. (2019); Tian et al. (2020), should be compared in performance."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2232/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841305878,
        "cdate": 1698841305878,
        "tmdate": 1699636156320,
        "mdate": 1699636156320,
        "license": "CC BY 4.0",
        "version": 2
    }
]