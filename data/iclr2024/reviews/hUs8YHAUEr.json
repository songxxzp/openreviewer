[
    {
        "id": "Pev718ZXIq",
        "forum": "hUs8YHAUEr",
        "replyto": "hUs8YHAUEr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_F87r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_F87r"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to improve language models' code generation via multi-perspective self-consistency. \n\nSpecifically, the multi-perspective is defined as sampling generations from 1) solutions; 2) specifications; 3) test cases. Then the authors propose to construct a graph based on those samples, and use inter-consistency and intra-consistency (via solving an optimization problem) to improve the final generations.\n\nThe authors did experiments on a few language models and show their method outperform several baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors did a thorough exploration on what factors could potentially affect code generation performances in LLMs, and it's interesting to see that all three perspectives, solution, specification, and test cases all play important roles in improving the final performance. \n\n- The experiments are done on a fairly comprehensive set of code generation models, including GPT, Code Llama and WizardCoder.\n\n- For intra-consistency, the authors also did a comprehensive study over what kind of measure functions can help the most."
            },
            "weaknesses": {
                "value": "- Novelty: [1] already proposed to construct a graph and explore more fine-grained consistency via MAX-SAT solving (although for natural language based reasoning tasks). The similarities and differences should be better discussed in the current paper.\n\n[1] Jung et al. Maieutic prompting: Logically consistent reasoning with recursive explanations. EMNLP 2022.\n\n- Generalizability: although framed as \"multi-perspectives\", this current paper only explores a single use-case of code generation, and with very specific perspectives: solution, specification, and test cases. It would be interesting to show if this method can be generalized to other tasks (e.g., math tasks, commonsense reasoning, or symbolic reasoning).\n\n- Added complexity and ad-hoc design choices: The current framing adds a lot of complexity to the existing baselines, and many of the design choices are not well justified. In practice it would be difficult to deploy such a method to ensure optimal performance. E.g., \n1) designing the perspectives: for each task, how much manual effort is needed to design those perspectives? (and writing the prompts for each perspective?) How sensitive is the final performance w.r.t. the prompts written for each perspective?\n2) constructing the graph: each edge needs to be specifically designed (section 3.2), why those choices and how much difference does it make if designed differently? For intra-consistency, similarly the authors designed many different measures, and based on the experiment results the best measure varies depending on the task. How would one pick which measure to use in practice?\n3) solving the optimization: depending on the number of nodes and edges, the solving part could be very expensive; even with the iterative algorithm, it might take many rounds to reach convergence. This point needs to be better discussed.\n4) choice of parameters: what is the value of \\alpha in experiments? From Figure 4, on human-eval the performance varies a lot depending on \\alpha. The final reported performance seems to be the \\alpha that achieves the best performance. But without a training/dev set, how could one pick \\alpha in an unsupervised way?\n\n- Fair evaluation: Table 2 shows the proposed method yields some gains over several baselines. But digging deeper, a fair comparison should be made between methods that use *the same number of generated samples*. MPSC uses many more samples (200 solutions, 100 specifications, 500 test cases) while most of the baselines only use solutions (200 samples only). In addition, MPSC-label is not a fair comparison given it uses human labels.\n1) if given the same number of samples, i.e., baselines with 800 samples vs MPSC, how does the performance compare?\n2) what is the variance of the proposed method, given that more samples from different perspectives are drawn?\n3) how does the proposed method compare with [2]? [2] also uses test cases to improve language models' code generation.\n4) Table 4 shows the test cases give the most gains, so maybe a simple baseline could be added: use the generated test cases to filter out incorrect generated solutions, and then apply self-consistency on the filtered set.\n\n[2] Chen et al. Teaching Large Language Models to Self-Debug. 2023."
            },
            "questions": {
                "value": "- How is the value of \\alpha chosen for each task? Is it chosen based on the final performance (which means the test data is known when you pick \\alpha)?\n\n- How does the proposed method work on the best model experimented (GPT-4)? Does it still give gains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1679/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698463079589,
        "cdate": 1698463079589,
        "tmdate": 1699636096179,
        "mdate": 1699636096179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yIlPVyh9AM",
        "forum": "hUs8YHAUEr",
        "replyto": "hUs8YHAUEr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_cqa4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_cqa4"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the intra-consistency and inter-consistency of LLMs through the code generation tasks. A multi-perspective self-consistency (MPSC) framework is proposed to enhance the decoding process of LLMs by introducing a multipartite graph. MPSC achieve noticeable improvement in Pass@1 on four code generation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work investigates both intra-consistency and inter-consistency of LLMs\n2. The proposed MPSC achieves significant improvement on code generation\n3. The proposed MPSC can act like a plug-in to enhance other LLMs"
            },
            "weaknesses": {
                "value": "1. The MPSC is complicate and unstable which limits the reproducibility\n2. The cost of MPSC is very high which limits its application\n3. The narrative of the paper is not clear"
            },
            "questions": {
                "value": "Comments\n1. There have been some works investigate the inter-consistency issue [1]. I think the inter-consistency issue should come from different LLMs other than a single LLM. Therefore, the inconsistency among solutions, specifications, and test cases seems like a kind of intra-consistency within a single LLM.\n\n[1] Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate\n\n2. MPSC needs to sample 200 solutions for a problem, a very high temperature should be used to ensure diversity. I am a little worried about its reproducibility and stability. Moreover, it is costly to implement MPSC due to we should sample 200 solutions for each problem.\n\n3. I have spent a lot of time trying to understand the details of the MPSC framework. A real example of the multipartite graph will be helpful to lower the threshold of understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1679/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742339464,
        "cdate": 1698742339464,
        "tmdate": 1699636096107,
        "mdate": 1699636096107,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "daZ9VTBkql",
        "forum": "hUs8YHAUEr",
        "replyto": "hUs8YHAUEr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_F8qr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_F8qr"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a framework based on multiple perspectives for further researching and enhancing the previous self-consistency method. The idea is to divide self-consistency into consistency among various perspectives. The authors have validated the effectiveness of this method on different models and programming datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The improvement over the baselines is significant. However, it's unclear whether this improvement is due to the allowance of code execution.\n\n2. The paper is easy to read.\n\n3. The method proposed in this paper can be applied to both closed-source large models (like ChatGPT) and open-source large models."
            },
            "weaknesses": {
                "value": "1. A key issue with the methodology and experiments of this paper is that the proposed method actually requires the execution of code, rather than merely generating it. For instance, in Listing 3 of Appendix 3, one needs to execute the solution to complete the validation with test cases. The authors need to emphasize this distinction from other methods. Clearly, when code execution is allowed, the task becomes easier. This issue makes the main experimental results of the paper, as shown in Table 2, unfair, as other methods do not require code execution.\n\n2. Can the method proposed in this paper be applied under conditions where code execution is not allowed? If so, what are the results? The authors have not demonstrated this.\n\n3. Is the learning approach designed in this paper overly complicated? Is it possible to avoid using w(vi,vj) and f(vi) and directly employ an MLP or other ensemble methods to obtain the answer? For instance, self-consistency actually uses max-vote directly. Overly complex optimization algorithms make the methodological contributions of this paper ambiguous.\n\n4. The specific \"perspectives\" used in this paper do not entirely align with intuition. For example, in Figure 2, the paper views the solution, specification, and test case as three perspectives, whereas conceptually, they are three answer components."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1679/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832980038,
        "cdate": 1698832980038,
        "tmdate": 1699636096031,
        "mdate": 1699636096031,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h2IKpFA0Y6",
        "forum": "hUs8YHAUEr",
        "replyto": "hUs8YHAUEr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_myx4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_myx4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Multi-Perspective Self-Consistency (MPSC), which extends the original self-consistency framework to consider both inter-consistency and intra-consistency. For code generation, they consider 3 perspectives: solutions, specifications and test cases. MPSC generates multiple samples for each perspective, then constructs a multipartite graph and learns a scoring function to select the final answers. They compare MPSC to other baselines including self-consistency, MBR-Exec, CodeT and self-collaboration. They demonstrate that MPSC consistently outperforms the baselines by a significant margin on multiple coding benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. MPSC is a natural extension of self-consistency for code generation, where the consistency among the solution, test cases and specifications can be precisely verified by code execution.\n\n2. The experiments show remarkable performance improvement compared to strong baselines that utilize multiple samples and/or test cases."
            },
            "weaknesses": {
                "value": "The main weaknesses of this work are: (1) the implementation details are unclear; and (2) some ablation studies are missing. Specifically, I have the following questions:\n\n1. How is MBR-Exec implemented? I do not understand why MBR-Exec can perform worse than self-consistency. To my understanding, self-consistency selects the final program based on the exact match; i.e., selecting the most frequently appeared code in all samples. On the other hand, MBR-Exec selects programs based on the frequency of execution results. Does MBR-Exec utilize the given test cases as in the original paper?\n\n2. For MPSC-Label, how are the golden test cases utilized? Do you directly filter out those programs that do not pass the test cases? In general I do not understand why MPSC-Weighted Cardinality can sometimes outperform MPSC-Label.\n\n3. It is interesting to see that GPT-3.5 with MPSC can outperform GPT-4. However, sampling 200 solutions is still very expensive. Do you have results with fewer number of samples, e.g., 10 or 100? What is pass@200, which should be the upper bound of the performance?\n\n4. It is helpful to add discussion on the quality of generated test cases and specifications. For example, what are the true positive and false negative rates?\n\nAlso, I think MPSC is well-suited for code generation, but how to extend it to other domains remains unclear."
            },
            "questions": {
                "value": "1. How is MBR-Exec implemented? Does MBR-Exec utilize the given test cases as in the original paper?\n\n2. For MPSC-Label, how are the golden test cases utilized? Do you directly filter out those programs that do not pass the test cases?\n\n3. Do you have results with fewer number of solutions, e.g., 10 or 100 instead of 200? What is pass@200, which should be the upper bound of the performance?\n\n4. It is helpful to add discussion on the quality of generated test cases and specifications. For example, what are the true positive and false negative rates?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1679/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699244235299,
        "cdate": 1699244235299,
        "tmdate": 1699636095945,
        "mdate": 1699636095945,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5ADLi1fzbC",
        "forum": "hUs8YHAUEr",
        "replyto": "hUs8YHAUEr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_q6tm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1679/Reviewer_q6tm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents Multi-Perspective Self-Consistency (MPSC), a novel framework aiming at improving the performance of LLMs at decoding time in complex reasoning tasks such as code generation. Extending from previous work on self consistency (e.g., Wang et al., 2022), the authors introduce multiple perspectives and a formulation of inter-consistency, which captures the agreement between generated outputs from diverse perspectives. The authors conduct experiments in code generation using various code competition benchmarks and three perspectives: solution, specification, and test cases. Empiricial results show a good amount of improvement over the baseline method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The multi-perspective method is well-motivated and well-suited for tasks like code generation.\n- The authors conduct comprehensive evaluation and show significant performance improvement over various baselines.\n- The paper is well-written."
            },
            "weaknesses": {
                "value": "- The main limitaiton of the work is the useability on a broader range of tasks. Despite MPSC is claimed to be task-agnostic, only code generation was presented in the paper, which greatly limits the impact of this work. On one hand, the authors only study code competition task, and it is unknown whether the framework can work in code generation in the wild. On the other hand, the authors should consider including at least one more NL task to demonstrate the extensibility of the framework.\n\n- It is unclear whether and how well the framework can generalize towards more perspectives. In code generation, there are only three perspectives, which is quite limited. It would be great to think about and demonstrate that MPSC can work with arbitary number of perspectives.\n\n- The perspectives are manually curated now, which can be a limitation for tasks with vague perspective definitions. It would be great to discuss whether manual curation of perspectives is required and if not how that would impact the end performance."
            },
            "questions": {
                "value": "- What would be the reason for degradation in MBPP?  \n\n- The improvement is diminishing when using a higher k in pass@k. Can the authors perform experiments with a much higher k (e.g. 100) and see the gap there?\n\n- For Bayes Risk, why would using BLEU metrics preferred especially given the code generation task?\n\n- Can the authors discuss the overhead added due to the graph introduced by MPSC?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1679/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699465708103,
        "cdate": 1699465708103,
        "tmdate": 1699636095866,
        "mdate": 1699636095866,
        "license": "CC BY 4.0",
        "version": 2
    }
]