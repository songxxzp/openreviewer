[
    {
        "id": "NIg6Vq2o0k",
        "forum": "AL4tS0HhJT",
        "replyto": "AL4tS0HhJT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_2myu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_2myu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a post-prediction diagnosis method, which involves training with a penalty that encourages learning class-wise feature and contrastive feature (T and C channel), which can then be plotted and decide whether a prediction a high/low quality. If the features align with the prediction, this is high quality, otherwise low quality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Examining the consistency of the feature as a confidence measure is intuitive and a common approach to similar problems."
            },
            "weaknesses": {
                "value": "1. This paper is missing a lot of important related works and does not seem to follow recent research. For example, \"However, to the best of our knowledge, successful applications of these methods, including inductive conformal prediction, to deep CNN on large image datasets have not been reported.\" This is clearly wrong... as early as in 2020 there was https://openreview.net/forum?id=eNdiU_DbM9 on ImageNet.\n\ta. The Related Work is mostly Conformal Prediction when this is clearly about model confidence/classification with rejection.\n\n2. Insufficient experiments: No baseline. A naive baseline would be if we just set some threshold on the maximum class probability we can also separate HQP and LQP. Also the paper suggested \"large image dataset\" yet only CIFAR-100 is used (and it seems like there were some difficulty applying the proposed method to 100 classes as well).\n \n\n3. Writing is very bad - not just in terms of presentation but the entire draft is incoherent. For example, \n\ta. captions are uninformative; \n\tb. The part about Yo-Yo plate is extremely terse despite the importance. Also, is point 1 in 3.3 a typo? I don't see \"t>c\" (lower or upper case) in the two equations. Similar typo in point 4 as well for $C_min$\n\tc. No introduction leading to section 3.3 at all.\n\td. the 1st contribution is \"pinpointing three sources of uncertainty\" including model imperfectness, yet \"imperfect\" never appeared after the introduction section.\n\nIn general, I could see how this paper could be improved to an acceptable paper, but the current form does not look like a complete version actually."
            },
            "questions": {
                "value": "1. What message does Fig 4 convey? I think I understand but there's little explanation and I'd like to confirm. Also the font is unreadable, and \"Plate 0\" should be replaced with \"Airplane\" (also I assume 0=airplane as the index does not match).\n\n2. What does 1b convey in this paper? There's no mention of calibration in this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Reviewer_2myu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698353652932,
        "cdate": 1698353652932,
        "tmdate": 1699636838877,
        "mdate": 1699636838877,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZkNyt2yuQG",
        "forum": "AL4tS0HhJT",
        "replyto": "AL4tS0HhJT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_c9j6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_c9j6"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the problem of getting confidence estimation of a CNN, where the usual softmax output does not represent the true probability, resulting in calibration error. The paper then proposes a method of augmenting the baseline CNN and use contrastive learning to obtain the quality of a prediction."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper\u2019s idea seem interesting and the paper is relatively easy to read."
            },
            "weaknesses": {
                "value": "**Overall comment**\n\nI had a hard time understanding what is being discussed in the paper. The authors seem to have missed a lot of relevant literature/even the problem setup, and have weak evaluations of their claims.\n\n**Do not cite/compare to calibration literature**\n\n>  But even a prediction with 95% probability concentrated on one class may still turn out wrong many times more often than the anticipated rate of 5%.\n\n> Among all test cases falling between .95 and .96, less than 60% cases are correctly predicted. Compared to the anticipated error rate of 5%, the actual error rate is more than 8 times higher.\n\nThis is the calibration error problem. Calibration is defined as the difference between true probability given a model\u2019s prediction, and the model\u2019s predicted probability. There exists numerous prior work in trying to solve the calibration problem, I do not see any of them cited by this paper: Platt scaling [1], Isotonic Regression [2], scaling-binning calibrator [3] to name a few.\n\nNo comparison/discussion of any of these methods is given.\n\n**Poor experimental evaluation**\n\nThe quality and depth of experimental evaluation is poor due to the lack of appropriate baselines, lack of ablation studies. The experiments only use very small image datasets, and no experiments are done on large datasets like ImageNet [4] / WILDS [5].\n\n**No relationship between claims in introduction and experiments done, claims are not formal**\n\n> We pinpoint three sources that may cause prediction uncertainty in deep CNN: distributional non-conformability, population heterogeneity, model imperfectness\n\nWhere is this pinpointing happening? What is the definition of each of these terms? E.g., what is model imperfectness here? Which experiments are done to establish these three sources?\n\n**Typos**\n\nThe paper also has some typos:\n1. Sever -> Severe (page 1)\n2. CIFAR1 -> CIFAR10 (page 7)\n\netc."
            },
            "questions": {
                "value": "1. (**Compressed class label representation**) How is the class compression being done? There is an official superclass clustering available for CIFAR-100, see [6]\n2. Also what is the final confidence value you get for a prediction? How is that number compared to the baseline CNN? Could you provide a reliability diagram [7][8] of your method? Also there are other metrics to measure calibration error, like ECE [3], could you report some of these?\n\n[1] Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. https://home.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf\n\n[2] Transforming classifier scores into accurate multiclass probability estimates, https://dl.acm.org/doi/10.1145/775047.775151\n\n[3] Verified Uncertainty Calibration, https://arxiv.org/abs/1706.04599\n\n[4] ImageNet: A large-scale hierarchical image database, https://ieeexplore.ieee.org/document/5206848/authors#authors\n\n[5] WILDS: A Benchmark of in-the-Wild Distribution Shifts, https://arxiv.org/abs/2012.07421\n\n[6] Coarse CIFAR-100: https://github.com/ryanchankh/cifar100coarse\n\n[7] The comparison and evaluation of forecasters. Journal of the Royal Statistical Society. Series D (The Statistician), 32:12\u201322, 1983.\n\n[8]  Increasing the reliability of reliability diagrams. Weather and Forecasting, 22(3):651\u2013661, 2007"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Reviewer_c9j6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698592629449,
        "cdate": 1698592629449,
        "tmdate": 1699636838765,
        "mdate": 1699636838765,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NDXFayy9Ua",
        "forum": "AL4tS0HhJT",
        "replyto": "AL4tS0HhJT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_xsnZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_xsnZ"
        ],
        "content": {
            "summary": {
                "value": "The paper deals with the important issue of prediction uncertainty in Convolutional Neural Networks (CNNs). It introduces a novel method called post-prediction confidence training (PPCT) to guide users in distinguishing between high and low-quality predictions by considering three sources of uncertainty: distributional non-conformability, population heterogeneity, and model imperfectness.\n\nThe authors rightly point out that while the overall accuracy of a machine learning model is a critical concern for developers, the individual users are more interested in the trustworthiness of specific predictions. The paper delves into the gaps in confidence measures and presents a novel approach, PPCT, to address these concerns. Notably, it introduces modifications to the network's training architecture and provides a comprehensive post-prediction diagnostic summary for users."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors introduce a new method (PPCT) that focuses on post-prediction confidence, a significant and under-researched issue in the deep learning community.\nThe paper does an excellent job of identifying the various sources of prediction uncertainty, making it a comprehensive study on the topic.\nThe proposed modifications to the network architecture, dual-channel representation, and the introduction of the Yo-Yo molding plate, are interesting and innovative.\nThe post-prediction diagnostic summary offers real practical value, enabling users to evaluate predictions from multiple perspectives."
            },
            "weaknesses": {
                "value": "The paper doesn't adequately compare PPCT with existing conformal inference methods, which would be crucial for establishing its superiority or identifying its unique benefits. Like Uncertainty sets for image classifiers using conformal prediction\nby Angelopoulos et al. (2020).\nThe methods developed lack rigorous theoretical guarantees, making it challenging to ascertain their reliability and robustness while conformal inference methods provide precise guarantees that hold in finite sample and under minimum assumptions.\nThe paper doesn't seem fully updated on the latest advancements in conformal inference literature, which is crucial for a comprehensive review of the subject. For example, besides using conformal inference as post processing step, there have been developments in tuning hyperparameters using conformal inference, like in Optimizing Hyperparameters with Conformal Quantile Regression by Salinas et al. (ICML 2023).\nThe numerous grammatical errors and typos make the paper difficult to read and comprehend in some sections, detracting from its quality and clarity.\nThe list of weaknesses provided by the reviewer seems incomplete, and a more thorough critique would be beneficial.\n\nThe authors should conduct extensive comparisons with existing conformal inference methods, providing both quantitative and qualitative analyses.\n It would be valuable if the authors can offer some theoretical guarantees or bounds for their method, even if under certain conditions or assumptions or at least carry out more extensive experiments on additional datasets highlighting the nuances of the method and providing deeper intuition on the varying degree of effectiveness of the method.\n A more comprehensive literature review on conformal inference would enrich the paper's content and context.\nThe paper should be thoroughly proofread to eliminate grammatical and typographical errors, ensuring clearer communication of ideas."
            },
            "questions": {
                "value": "How does this work differentiate or improve upon previously published methods on the same topic? A clearer distinction could help underline the significance of this study.\n\nHave the authors considered testing their methodology across other datasets or domains? This would help gauge the robustness and versatility of their approach.\n\nSome sections of the paper could benefit from visual representations or flow diagrams to help the reader grasp the concept more intuitively."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7103/Reviewer_xsnZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634619044,
        "cdate": 1698634619044,
        "tmdate": 1699636838651,
        "mdate": 1699636838651,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lmGCu1HT0V",
        "forum": "AL4tS0HhJT",
        "replyto": "AL4tS0HhJT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_9DJD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7103/Reviewer_9DJD"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors focus on analyzing the causes of prediction uncertainty in the context of supervised learning, including distribution shift, subpopulation shift across classes, and imperfect prediction model. While previous works focused on distribution shift, the authors propose a new approach called post-prediction confidence training to explore the other two reasons. They also suggest a blueprint, which uses T channel and C channel in CNN configuration. Experiments on several image datasets are conducted to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem explored in this work is crucial to the development of modern deep learning. Understanding the quality of predictions is a critical task in real-world applications.\n\n2. The method provided in this work can address three sources of prediction uncertainty. The idea is novel as previous method focused on distribution shift only."
            },
            "weaknesses": {
                "value": "1. The writing can be improved. The authors directly describe their method in the main part, instead of introducing the problem setting. This might increase the difficulty of readers to touch the background.\n\n2. The motivation of the proposed method is not clear. Although the authors introduce the limitation of CNN training for post-prediction evaluation in subsection 3.1, the description is not clear so that the reviewer is still confused about what the limitation is.\n\n3. There are too many hyperparameters in the proposed method. As introduced in Subsection 3.4, There are at least ten hyperparameters that are chosen by human, which I think is impracticable.\n\n4. It can be better if the authors can provide empirical evidences by large-scale datasets, like Imagenet.\n\n\nTypo: Section2 \"Matiz & Barner (2019) applied ***IPC*** to active learning.\", should be \"ICP\""
            },
            "questions": {
                "value": "1. In subsection3.1, The authors introduce the problem for complex models and list CNN as an example. I am confused why the authors describe the limitation as an issue of CNN training, instead of all complex models. Can we implement the proposed method for other network architectures, like transformer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832632320,
        "cdate": 1698832632320,
        "tmdate": 1699636838541,
        "mdate": 1699636838541,
        "license": "CC BY 4.0",
        "version": 2
    }
]