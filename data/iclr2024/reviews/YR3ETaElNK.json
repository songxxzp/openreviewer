[
    {
        "id": "82yNHi0Gu8",
        "forum": "YR3ETaElNK",
        "replyto": "YR3ETaElNK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission472/Reviewer_6WJ1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission472/Reviewer_6WJ1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an interesting idea of tuning LayerNorm for PEFT, yielding impressive results compared with LoRA and tuning other parts in LLMs. Five multi-modal tasks were tested to show the effectiveness of their proposed method. 20% better performances, 41.9% reduction of trainable parameters and 17.6% decreasing of GPU memory usage were reported. The contributions of this paper include, (1) tuning LayerNorm with a simple and efficient direction, and (2) significantly better results and resource/cost reduction on 5 tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. simple and efficient tuning of LayerNorm for LLMs;\n2. strong results reported for 5 multi-modal tasks."
            },
            "weaknesses": {
                "value": "1. simple layernorm finetuning is reported to be including less trainable parameters and sensitive to a list of hyperparameters such as learning rate and training epoch, making it difficult to find the optimal performances in a quick way;\n2. not clear yet of how other researchers combine LayerNorm with other PEFT methods - are the results better or not?"
            },
            "questions": {
                "value": "1. any further detailed results on combining LayerNorm tuning with other types of PEFT? such as those listed in https://github.com/huggingface/peft?\n2. so how exactly do you select the hypermeters during tuning? learning rate and training epoch, any detailed hints on applying your method to new tasks and new datasets?\n3. can you show some examples that tent to be better or worse when applying LayerNorm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Reviewer_6WJ1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698450243709,
        "cdate": 1698450243709,
        "tmdate": 1699635973787,
        "mdate": 1699635973787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lwYc7OBZAe",
        "forum": "YR3ETaElNK",
        "replyto": "YR3ETaElNK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission472/Reviewer_itTB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission472/Reviewer_itTB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an efficient approach to finetuning Large Language Models (LLMs) for multi-modal tasks, resulting in Multi-modal Large Language Models (MLLMs). By finetuning only the LayerNorm's weights within attention blocks and making LayerNorm the sole trainable component, the model achieves superior performance, especially when finetuned with conversational data. This LayerNorm-focused strategy, combined with the right data type, leads to significant performance improvements on benchmarks while using fewer resources, highlighting the potential of LayerNorm tuning for multi-modal tasks. The experiments are thorough and well-executed, yielding insightful results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper introduces an efficient way for transforming LLMs into MLLMs by only finetuning the LayerNorm within each attention block.\n2. The paper provides empirical results showing that the proposed LayerNorm tuning method can yield competitive performance, often surpassing other tuning approaches like full parameter finetuning or LoRA.\n3. The LayerNorm tuning method offers substantial benefits in terms of efficiency, reducing trainable parameters and GPU memory usage.\n4. Beyond empirical results, the paper explores deep into understanding the role of LayerNorm in adapting LLMs to the multi-modal domain, enhancing the model's expressive power."
            },
            "weaknesses": {
                "value": "1. Inconsistency Across Different LLMs: The experimental results appear to vary depending on the specific LLM used. While the authors highlight that human-aligned LLMs tend to yield better results, there seems to be inconsistency, especially when comparing the human-aligned 7B model with the 13B one. This raises questions about the robustness of the proposed method across different model sizes and configurations, making the claim less convincing.\n2. Lack of Exploration of Combined Training Strategies: The paper primarily focuses on the LayerNorm tuning strategy. A more comprehensive exploration, combining this with other training strategies, would provide a richer understanding of the method's potential and its interaction with other techniques. Such an exploration could offer more actionable insights for practitioners looking to adopt the proposed method.\n3. Empirical and Intuitive Explanations: The paper's explanations for the observed results lean heavily on empirical evidence and intuitive reasoning. A deeper theoretical analysis or a more rigorous exploration of the underlying mechanisms would strengthen the paper's claims and provide a more solid foundation for the proposed method."
            },
            "questions": {
                "value": "1. In the experiments, there seems to be a performance variance between different LLMs, especially between the human-aligned 7B and 13B models. Could the authors elaborate on the potential reasons for this inconsistency?\n2. While the paper provides empirical and intuitive explanations, could the authors shed light on any theoretical insights or hypotheses they might have about why tuning only the LayerNorm yields such significant results?\n3. Beyond LoRA, are there other recent advancements or techniques that have shown promise in multi-modal learning or the finetuning of Large Language Models? It would be beneficial to compare your approach with these newer baselines to provide a more current context."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698745164090,
        "cdate": 1698745164090,
        "tmdate": 1699635973708,
        "mdate": 1699635973708,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XQU84K0Jjl",
        "forum": "YR3ETaElNK",
        "replyto": "YR3ETaElNK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission472/Reviewer_qMBg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission472/Reviewer_qMBg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a strategy for transforming Large Language Models (LLMs) into Multi-Modal Large Language Models (MMLMs) by tuning LayerNorm in attention. The authors argue that this approach can improve the performance and efficiency of multi-modal tasks, such as image captioning and visual question answering. The paper presents empirical evidence to support this claim, showing that the proposed method outperforms existing approaches on several benchmark datasets. The authors also discuss the role of LayerNorm in adapting LLMs to the multi-modal domain and improving the expressive power of the model. Overall, the paper provides a novel and effective solution to the challenge of multi-modal learning, which has important implications for natural language processing and computer vision."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper introduces a novel method to transform large language models into multi-modal language models by tuning LayerNorm only, which yields strong performance. The idea is straightforward but effective.\n2. The experiments are solid and the results support the idea of the authors.\n3. The paper is well-written and well-organized, and the comprehensive analysis is interesting and attractive."
            },
            "weaknesses": {
                "value": "1. It is better to detail the LayerNorm tuning way in the paper and compare it with other methods.\n2. Although existing experiments have demonstrated the effectiveness of the method, I suggest evaluating the model's capabilities comprehensively on more data, such as VSR, HM..."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission472/Reviewer_qMBg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828885198,
        "cdate": 1698828885198,
        "tmdate": 1699635973642,
        "mdate": 1699635973642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2iegoKq8zE",
        "forum": "YR3ETaElNK",
        "replyto": "YR3ETaElNK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission472/Reviewer_rJin"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission472/Reviewer_rJin"
        ],
        "content": {
            "summary": {
                "value": "Transforming LLMs, to Multi-Modal LLMs requires further finetuning on multi-modal data which can be pretty expensive.\n\nThere exist parameter efficient alternatives to fully finetuning models, and most popular ones are prefix tuning and LoRA. These methods a small amount of trainable parameters to the model.\n\nThe paper presents an alternative, which is not only parameter efficient, but also doesn't add new parameters to the model. The method simply trains the layer norm weights of the model associated with attention. The authors also demonstrate through multiple experiments that this is a strong alternatives that perform pretty well for multi-modal setup. There are two alternatives discussed: 1) only LayerNorm, and 2) LayerNorm along with vision language connector, word embeddings and output head. While #1 is more efficient, #2 is more performant."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Strong results while being much more parameter efficient compared to other similar techniques like LoRA, Attn QV Proj, and Attn MLP.\n- The two variants present a tradeoff between parameter efficiency and quality. This could also be useful when the finetuning dataset is very small and using LayerNorm only variant could provide a less overfitting alternative.\n- While LoRA and other methods add additional parameters (adapters), this method does not need it and hence does not require handling them separately during training/inference to merge them with the original parameters."
            },
            "weaknesses": {
                "value": "- The method is claimed to be useful only for MultiModal tuning, and might not be comparable to LoRA or other methods for LLM only finetuning."
            },
            "questions": {
                "value": "Do we have any results comparing it with LoRA for non-multimodal tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission472/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699040987751,
        "cdate": 1699040987751,
        "tmdate": 1699635973570,
        "mdate": 1699635973570,
        "license": "CC BY 4.0",
        "version": 2
    }
]