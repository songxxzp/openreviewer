[
    {
        "id": "pIC7FJOE4n",
        "forum": "5Nn2BLV7SB",
        "replyto": "5Nn2BLV7SB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_8MGB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_8MGB"
        ],
        "content": {
            "summary": {
                "value": "- This paper raises a pivotal question: How can we design a local LLM-based evaluator that safeguards privacy and minimizes potential data leakage?\n- In this study, evaluations from GPT-3.5 are distilled into a model named PandaLM, which utilizes the capabilities of LLaMA, achieving performance on par with both GPT-3.5 and GPT-4.\n- Furthermore, the researchers have curated a robust human-annotated dataset, which is indispensable for verifying PandaLM's effectiveness and paving the way for future studies.\n- Additionally, PandaLM has been employed to fine-tune the hyperparameters of various open-source LLMs, leading to significant performance improvements compared to default configurations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This research innovatively introduces a local LLM-based evaluator, positioning it as a viable alternative to ChatGPT and GPT-4. The emphasis on local models highlights their intrinsic advantage in minimizing the risks associated with data leakage.\n- PandaLM, as presented in the paper, exhibits a commendable performance that stands shoulder-to-shoulder with both GPT-3.5 and GPT-4. Notably, the resilience of PandaLM is evident as it continues to demonstrate robustness even when transitioning to the specialized domain of medicine."
            },
            "weaknesses": {
                "value": "- While the authors utilize PandaLM to optimize hyperparameters, there's an evident absence of evaluation results concerning different hyperparameter settings. Specifically, insights into how performance varies with adjustments in learning rate or epochs would be beneficial.\n- The presented evaluation results aren't juxtaposed with learning metrics like loss and perplexity. Despite the authors' assertion that perplexity might offer optimization directions, this comparison is conspicuously missing.\n- The hyperparameter settings presented appear incomplete, particularly the omission of early stopping.\n- There seems to be a gap in the generalized and robustness evaluation of PandaLM. Although the authors have assessed PandaLM in the context of legal and biological domains (indicative of domain shift), a broader spectrum of evaluations, especially with unseen models like LLaMA-2 (indicative of model shift), would enhance its credibility.\n- While evaluating LLMs is undeniably vital, solely focusing on outcome evaluation might be resource-intensive. A deeper dive into behavior prediction could provide a more comprehensive understanding and potentially be more resource-efficient."
            },
            "questions": {
                "value": "1. How does a model optimized by loss or perplexity fare in evaluation results? Can you shed light on the advantages of PandaLM over automatic metrics in hyperparameter optimization? The statement, \"... Perplexity ... does not effectively guide the choice of hyperparameters\", prompts questions regarding the mechanism by which PandaLM aids in hyperparameter selection. Is there a direct correlation between PandaLM's results and the choice of hyperparameters? Additionally, could you share the results for models selected based on optimal perplexity?\n2. Is there an implementation of the early stopping strategy? If so, how does this impact overall performance?\n3. How well does PandaLM generalize in the face of model shifts? There's a latent concern regarding potential overfitting with PandaLM, which might explain its superior performance over GPT-3.5 and GPT-4. It would be valuable to see results on unseen models, i.e., models whose generations weren't part of the training set. For instance, while the dataset includes responses from models like LLaMA-7B, Bloom-7B, and others (deemed as 'seen' models), results on LLaMA-2-7B (an 'unseen' model) are anticipated.\n4. Could you clarify the contents and implications of Table-3? How do 'alpaca' and 'vicuna', mentioned in this section, correlate? Moreover, where can we find the results for GPT-3.5-turbo and GPT-4 in the legal and biological domains?\n5. Building on the topic of scalable oversight, how does PandaLM perform with larger models, particularly LLaMA-13B and LLaMA-70B? A deeper insight into scalable oversight would be highly appreciated.\n6. Given the substantial evaluation costs associated with fine-tuned models, how might one conduct more granular evaluations prior to the fine-tuning process? I'm eager to hear the authors' take on behavior prediction as a potentially promising avenue beyond the scope of PandaLM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698804877641,
        "cdate": 1698804877641,
        "tmdate": 1699636762405,
        "mdate": 1699636762405,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BkhoLeZNNT",
        "forum": "5Nn2BLV7SB",
        "replyto": "5Nn2BLV7SB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_FMz2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_FMz2"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce PandaLM, a language model that can be used to evaluate and compare the response of other language models. The evaluation criteria are supposed to centre around important but subjective factors such as conciseness, adherence to instructions, and comprehensiveness. This is important because in contrast to tasks like classification, the responses of a language model cannot be justly evaluated using a static set. The alternative methods, e.g. human evaluation or using expensive cloud LLMs through APIs, are also less than ideal.\n\nThe authors use this trained model to not only evaluate other instruction tuned language models from the Alpaca family, but also use it to find better hyperparameters for training them. To validate their hypothesis, they show that 1) the evaluations from PandaLM match human evaluations to a greater degree than GPT-3.5 and GPT-4; and 2) they also show that the resulting hyper-parameter-optimized models' performance are superior to their predecessors as evaluated by human evaluators, GPT-3.5, and GPT-4.\n\nI believe they also plan to make the dataset used to evaluate PandaLM itself available to the open source community as well."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem being addressed is certainly an important one, and the introduction of a repeatable process with reduced cost (as opposed to human evaluation and cloud LLMs) will definitely help the community and the research of LLMs\n- Open sourcing the model and the dataset (I assume)\n- The 70B model beating GPT-4, definitely makes it a worthwhile method\n- The majority of the results seem to point in the same direction, which I find to be rare and makes it easier to draw conclusions from the work\n\n- Originality: I do not know of a similar attempt to train such a model, and hence assume the work to be original, even though there isn't a lack of people who have tried to use LMs to evaluate other LMs\n- Clarity: the description seemed easy to understand and without any unnecessary math / jargon to complicate things\n- Significance: though I expect the proposed models to start becoming obsolete rather soon due to the speed of innovation, I still believe the model could be a strongly positive force in solving the problem of LLM evaluation"
            },
            "weaknesses": {
                "value": "- Even though this model is still better than GPT-4 which is great, still the low accuracy/F1 scores leave one desiring a better model. Can the authors propose a higher-bound on the expected F1/accuracy scores? For example, can we compare this to the percentage of human evaluators usually that agree with each other on the same task?\n\n- In Table 3, PandaLM-7B is shown to have dangerously low numbers: 47% for LSAT and 51% for BioASQ. Assuming that chance accuracy is 50%, this highly questions the validity of this version of the model. I understand that the judge for these results has been GPT-4, however, that doesn't resolve the issue. I highly suggest to not simply dismiss the results, and further validate why. If it turns out that human evaluators actually do prefer this model, then all the better. Otherwise, the 7B version of the model might not be fit to the task of evaluating results outside the domain in which it was trained on."
            },
            "questions": {
                "value": "I have already asked two questions in the previous section, but I'll include a few minor areas of improvement here.\n\nMinor issues:\n- In Table 2, I believe GPT-4's Recall should be highlighted as it's higher than PandaLM-70B's (68% vs 66%)\n- In the conclusion, the authors mention that \"based on PandaLM evaluations\", the superior models are the ones that have been trained using PandaLM. Then they claim that \"this order emphasizes the efficacy of PandaLM ...\". I believe this claim is incorrect since by definition a model that the PandaLM preferred in hyperparameter tuning, is already preferred to the base model. Hence, one cannot make any claim based on the results. If I misunderstood, then please clarify. Otherwise I believe this sentence should be ommitted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806096678,
        "cdate": 1698806096678,
        "tmdate": 1699636762285,
        "mdate": 1699636762285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VWJYtywW0t",
        "forum": "5Nn2BLV7SB",
        "replyto": "5Nn2BLV7SB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_Vuch"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6662/Reviewer_Vuch"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel framework for the evaluation of instruction-tuned large language models (LLMs) named PandaLM, which can be used to select the superior one given several LLMs. Rather than focusing on the traditional metrics, like F1, precision or recalls, PandaLM addresses important subjective factors, such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality.  Further, the authors also collect a human-annotated test dataset to evaluate the reliability of PandaLM. Finally, the authors use PandaLM to tune several state-of-arts LLMs and yield better performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1, Build an open-source LLM, PandaLM, which can help to do evaluation of the performance of LLMs on several subjective factors and the performance of PandaLM on a human-annotated dataset. In detail, PandaLM stands out by assessing performance based on a range of subjective factors such as conciseness, clarity, instruction adherence, comprehensiveness, and formality. These aspects are often neglected in traditional benchmarks that focus predominantly on objective correctness.\n\n2, The paper performs extensive experiments to show the effectiveness of PandaLM.  The paper conducts thorough experiments to validate the effectiveness of PandaLM. By benchmarking against industry leaders like GPT-3.5 and GPT-4, the authors provide compelling evidence of PandaLM's capability to serve as a reliable evaluation tool.\n\n\n3, The paper is well-organized and well-written."
            },
            "weaknesses": {
                "value": "1, In the abstract and introduction sections,  the authors should make clear the claim point and avoid overclaiming the contricution. For example, in the abstract, rather than simply claiming that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance, the authors should point out it is on the evaluation of the dataset collected by the authors. In the introduction section, instead of claiming that Tuning models with PandaLM-selected hyperparameters yields more substantial performance enhancements, it should point out specifically the performance compared to the LLM searched by Alpaca\n\n2, In Table 1, it is better to add the performance of LLaMA as a judge. By doing so, it can rule out the possibility of the performance of PandLM is caused by the backbone model, rather than the proposed LLM tuning framework."
            },
            "questions": {
                "value": "see the questions in  Weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6662/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699146862946,
        "cdate": 1699146862946,
        "tmdate": 1699636762146,
        "mdate": 1699636762146,
        "license": "CC BY 4.0",
        "version": 2
    }
]