[
    {
        "id": "qeE3uoeXeA",
        "forum": "b8hRudcKQ3",
        "replyto": "b8hRudcKQ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_ijEK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_ijEK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a FL framework embedded with an incentive mechanism. It balances the preferences of clients (double roles: model trainer and model user) to model performance gain and monetary reward by designing the aggregation weight $\\alpha$ for local models. The aggregation weight $\\alpha$ consists of MBS and monetary quotations. The MBS defines one client\u2019s contributions on model performance to others. And the monetary quotations indicate that one client\u2019s will to pay for model performance gains. Clients with low-quality data may provide a higher monetary quotation to make its aggregation weight $\\alpha_i$ be larger so to get more model gains. Clients with more data (data seller) do not care about model performances but hope to receive monetary rewards through contributes better model performances based on more local data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis paper is well organized and easy to follow.\n2.\tThis paper considers the double roles of FL clients and propose an incentive mechanism to balance the preferences of clients to model performance gain and monetary reward, which is an important and interesting topic in federated learning.\n3.\tThe proposed algorithm is comprehensive and practical, since it designs the model aggregation weights based on model performance and monetary quotations and consider the adjustments of hyper-parameters through typical FedHPO methods."
            },
            "weaknesses": {
                "value": "1.\tThe proposed two metrics (MBS and monetary quotations) are commonly used and not novel, especially the latter is commonly used in prior auction-based studies, as the authors stated in related work (page 2).\n2.\tThere are some notations confusing. Such as:\n-\tPage 4: \u201cwhich means involving client j deteriorates the model performance on client i\u2019s validation set\u201d, i and j may be exchanged.\n-\tPage4: what does $N_C$ mean? Does it equal to $N$? \n3.\tIn page 4, the simplified MBS calculation uses the derivative of evaluation metric, what is computed and denoted in experiments? I can not find the illustration for this clarification.\n4.\tIn page 4, Section 3.2 Monetary quotation, the second sentence confused me. The quotation $b_i$ should be the highest willing payment of client $i$ to the FL system, not the profits received from others. The authors are suggested to re-organize the description.\n5.\tThe theoretical analysis for convergence is necessary for guaranteeing the effectiveness of FL algorithm. \n6.\tExperiments are not enough to verify the advance of the proposed method due to the following reasons:\n-\tOnly one small CIFAR-10 dataset is used for experiments. The more and larger dataset should also be used for testing.\n-\tThere are only two or three clients allocated in the FL system. As we all know, a cross-device FL system often many participants, so the effectiveness of the proposed method in a FL system with more clients should be evaluated.\n-\tNo prior baselines are compared, so the advance of the proposed method is unknow.\n-\tFigure 2 (a), the last two figures in the bottom line are reversed.\n-\tFigure 2 (b), client bid: 0:10, why tesy_acc improves far less than test_f1?"
            },
            "questions": {
                "value": "Please see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697597184621,
        "cdate": 1697597184621,
        "tmdate": 1699636817212,
        "mdate": 1699636817212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "twiaqdnXGj",
        "forum": "b8hRudcKQ3",
        "replyto": "b8hRudcKQ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_Eroa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_Eroa"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a federated learning framework, Alpha-Tuning, that motivates from client incentives and adaptively adjusts client model performances based on both data and monetary contributions. The paper formulates the client incentivization problem into a federated hyper-parameter optimization (FedHPO) problem where the optimal aggregation weights are updated based on the client contributions. As such, previous works on FedHPO are directly adopted here. The method also includes the distribution of monetary rewards back to all clients. The authors are able to demonstrate the efficacy of the proposed mechanism/framework through experiments with 3 federated clients possessing CIFAR-10 data."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The incentive-related motivation in FL is important and interesting.\n2. The recognition of the two types of federated clients with different objectives in monetary compensation and model performance improvements is valid."
            },
            "weaknesses": {
                "value": "1. The denomination between monetary quotation and model improvement is not designed and analyzed carefully. \n2. The incentives for clients to join the federated effort were not carefully discussed.\n3. The design choices for SHA and FedEx need to be better justified.\n4. The empirical experiments and evaluations are not comprehensive.\n\nMore details about the weaknesses are elaborated below in the Questions."
            },
            "questions": {
                "value": "1. The denomination between monetary quotation and model improvement is not designed and analyzed carefully. As a high-level question, how much money should correspond to the model performance increment from 90% to 91% for a specific client\u2019s validation data?\n2. From Eq. (7), I can see that monetary quotations from all clients are distributed back to clients depending on the contributions $B$. However, this monetary reward is not linked to performance guarantees and may make clients lose incentive. To elaborate, even if the monetary quotation is high for a client (i.e., the client is willing to spend much money for an improved performance), but it is entirely possible that the model for the client cannot be further improved (e.g., one possibility is that other clients\u2019 data is not helpful). In this case, the money quotation from the specific client still gets distributed to others, when there is no model improvement. How do you justify this case under your design?\n3. Firstly, I believe there is a typo for the green legend in Figure 4 (left): The green label should be for \u201c10:0\u201d right? From this figure, we observe that client 1 gets a lower F1 score when the quotation is 10 (green circle) as compared to when the quotation is 5 (blue circle). I.e., client 1 gives more money but gets a worse model. This reinforces my Question 2 about having no guarantee for model performance with respect to monetary quotation. Could you justify this case? Could you explain the client\u2019s incentives and behaviors, too?\n4. In the paper, the training aggregation weights $\\beta_i$ are treated as hyper-parameters. However, they are closely linked to the final model trained as well. Why can\u2019t they be considered as designs to favor specific clients, just like what $\\alpha$ does?\n5. There is no convergence result for the algorithm proposed. In fact, the authors wrote in section 3.3 that \u201c$\\alpha_i^{(t)}$ can be changed in different iterations\u201d. How can you assure convergence when the loss (or, objective) is constantly changing? Does the objective converge as well? If no theoretical convergence can be given, then the paper becomes a lot less convincing.\n6. The design choices for SHA and FedEx need to be better justified. Could you elaborate on why these methods fit your specific incentive mechanism? Or, any other FedHPO method could possibly work too?\n7. Wouldn\u2019t a client benefit a lot more by giving a very difficult, or potentially adversarial (e.g., wrong label) validation points? In this case, the other clients are unlikely to improve on this client\u2019s validation dataset. Therefore, this client would not need to pay other clients (or pay very little) and obtain more money back. Even worse, when every client is now incentivized to state adversarial validation datasets, the federated training process can be adversely affected.\n8. The assumptions 2) and 3) in Property 2 sound very stringent to me, it might make the property much less useful in practice.\n9. The empirical experiments are not comprehensive enough. The results are not convincing with only 3 clients, could you produce results for at least 50 federated clients with data splits following standard heterogenous FL baselines/benchmarks? The dataset used (CIFAR-10) is also very well-behaved. Can you show experiments with larger-scale datasets like ImageNet and Sentiment-140?\n10. I see in Table 3 of Appendix B.2 that the aggregation weights candidates are a discrete set of potential values. Could you optimize a continuous vector for it instead?\n11. There are existing works that have looked into model incentives (instead of monetary incentives), but none of them are cited or discussed in the paper. One example would be [1].\n\n[Minor]\n\n12. It should be made clear that (3) is not a direct reduction of (2). Please correct me if I am wrong.\n13. In the fourth line of the \u201cComputation of MBS.\u201d paragraph, there is a typo. It should be $w_{\\mathcal{C} \\setminus i}^{(t)}$ instead of $w_{\\mathcal{C} / i}^{(t)}$.\n14. What is the reason for creating an influence score matrix $\\mathbf{B}$ after Eq. (4)? You are going to add them over the clients anyway, i.e., in Eq. (5).\n\n**References**\n\n[1] Gradient-Driven Rewards to Guarantee Fairness in Collaborative Machine Learning. NeurIPS, 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698395914186,
        "cdate": 1698395914186,
        "tmdate": 1699636817095,
        "mdate": 1699636817095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GJZdH1py0O",
        "forum": "b8hRudcKQ3",
        "replyto": "b8hRudcKQ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_C98P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_C98P"
        ],
        "content": {
            "summary": {
                "value": "This work analyzes clients' incentives in federated learning where clients have cost to participate in FL and get corresponding monetary benefits from the final global model. The work proposes to give different weights to clients depending on their local validation loss which can translate to the client's performance contribution in the given training round. Gaining intuition from the leave-on-out method, the work proposes to train $N+1$ model where one model is trained with all clients and all the other models are trained with updates from the $N-1$ clients' update. They show that such method can satisfy different clients' participation FL with different quotation settings and costs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work investigates a relevant problem in FL where clients' incentives and their corresponding participation cost and compensation are considered. \n\n- The work incorporates an interesting approach of hyper-parameter tuning + leave-one-out method to modulate the performance monetary quotation and compensation for the clients based on their validation losses.\n\n- The work provides detailed explanation of their method for the readers to understand including ideas in which the work was originated from with relevant references."
            },
            "weaknesses": {
                "value": "- The main issue I have with the work is its feasibility to be implemented in realistic FL settings. For instance, how likely is that the monetary quotation to be fixed throughout the training rounds for the clients in the FL setting and also how likely is it that it is feasible to train $N+1$ models in FL? With increasingly high computation and communication cost of training models via FL increasing the number of modes seem to be infeasible and rather unrealistic. \n\n- Another weakness I can see from the FedHPO method is that the payment happens at the end of the FL training, depending on how a client contributes to another clients' performance. However, such performance is not deterministic, and may change throughout the course of training. In this case, how would the clients know in advance of whether their quotations are met or the monetary contribution is positive? If the clients don't have such guarantees, it will be easier for them to simply opt-out of the training process. \n\n- Lastly, the empirical validation of the work seems weak. Since the work is mainly empirical, I think a more thorough evaluation is needed including more clients and different tasks. In the current version, only limited number of clients are considered with the CIFAR10 dataset."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698872263205,
        "cdate": 1698872263205,
        "tmdate": 1699636816943,
        "mdate": 1699636816943,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SCLsSVgAl5",
        "forum": "b8hRudcKQ3",
        "replyto": "b8hRudcKQ3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_1CDp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6984/Reviewer_1CDp"
        ],
        "content": {
            "summary": {
                "value": "This work introduces an Alpha-Tuning framework, which addresses the challenge of optimizing federated learning (FL) processes to align model performance and monetary rewards with varying client preferences. \nAt the core of Alpha-Tuning is a mechanism for dynamically determining the weights assigned to clients' local validation losses. \nThese weights are based on each client's performance contribution in the training round and its monetary quotation, allowing for a personalized adjustment of FL course bias.\nIn addition, this submission includes a payment rule designed to compensate clients according to their data contribution. \nThis ensures that clients are rewarded in a way that aligns with their level of participation in the FL process.\nThe effectiveness of the Alpha-Tuning framework is demonstrated through experiments on CIFAR10."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation is clearly articulated and compelling.\n\nThe discussion and illustrations are generally accessible, but there is room for improvement in enhancing clarity and comprehensibility."
            },
            "weaknesses": {
                "value": "The primary concern with this submission pertains to the experimental comparison:\n- There appears to be a lack of comparison between the proposed solution and prior works.\n- The experiments rely solely on a single dataset, CIFAR-10, making it challenging to assess the effectiveness of the proposed solution comprehensively.\n- Additionally, the parameter settings in the experiments raise some questions. For instance, the number of clients seems relatively small. In prior works, using 20 or 100 nodes is a more common and reasonable choice for evaluating federated learning methods."
            },
            "questions": {
                "value": "Please refer to \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6984/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699328632542,
        "cdate": 1699328632542,
        "tmdate": 1699636816827,
        "mdate": 1699636816827,
        "license": "CC BY 4.0",
        "version": 2
    }
]