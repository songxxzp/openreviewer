[
    {
        "id": "o50OdUbUdT",
        "forum": "JlSyXwCEIQ",
        "replyto": "JlSyXwCEIQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
        ],
        "content": {
            "summary": {
                "value": "This paper describes a new program synthesis approach for solving the Abstraction and Reasoning Corpus (ARC). The authors take a 220m parameter T5 code pretrained language model, fine-tune it on handwritten solutions to the 400 training set tasks using a DSL designed for ARC as well as randomly mutated solutions to these tasks, and then train a policy by iteratively attempting to solve tasks and adding hindsight-relabelled solutions to a buffer of tasks to train the policy on. Their approach solves 40/400 tasks on the evaluation split, and improves over time."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The approach is excellent: well-designed, simple in principle yet careful in the details, and seems to be tuned well. The design decisions are carefully explained and justified, and the whole procedure is written out well in a way that is easy to follow. \n\nThis is a natural approach for ARC that has not been tried yet, and it is really exciting to see the results the authors have generated. Many ARC papers, as the authors note, are not good enough to use as a baseline, so the fact that this work succeeds in attempting to solve tasks from the evaluation set is a noteworthy accomplishment. Moreover, the approach is not simply brute-force search, and has the potential for much better performance if various parts of the system are tweaked and improved.\n\nThe related work is good. The ablation experiments are good. The discussion section is good. Really, it is a very nicely written paper."
            },
            "weaknesses": {
                "value": "- Overall, the writing could use general revisions for clarity, mainly on little details of wordings rather than high level changes.\n- There is no mention of code being available or made open source.\n- The authors only evaluate their approach on ARC, even though it could be in principle compared to synthesis approaches in other domains. Due to the difficulty and uniqueness of ARC, I think only evaluating on ARC is more than sufficient, but showing performance on another domain would help elucidate what about the algorithm is working and not working as expected.\n- There is limited insight into understanding the capabilities of the model. For example, the paper does not convey well what exactly the model is improving at over time, or how competent the model is at generating syntactically correct code or using the full range of DSL operators once the fine-tuning stage is completed. \n\nOverall, despite the well-written nature of the paper and good design of the approach. I find it hard to have a good grasp of how well the approach actually worked. 40/400 tasks seems a bit low given the technique, and there's not much space in the paper devoted to understanding what exactly is going on with the model. The ablations help a bit. I think the best way to get a sense of this would be to have a link to a page which shows the tasks and generated solutions the model discovers over the course of training, and when each one was found. This would really help understand how well the approach is working. In addition, seeing some of the tasks it fails to solve, which we might expect it to solve (e.g. if the DSL can solve it in a few operators), would help too. While this might make the approach look less impressive, it would really increase the quality of the paper."
            },
            "questions": {
                "value": "Questions\n- Why don't you evaluate on the hidden test set? Even if you do not beat the state of the art, it would be good to know the performance.\n- I don't understand how hindsight relabelling works with the mutation baseline in section 3.1 / appendix A.2 \u2014 for searching via mutation, it seems like you just need to store the program and the inputs into the buffer, no need to store the outputs?\n- Your main approach that combines mutation and policy just uses mutation for generating the data during fine-tuning, is that right? There's none of the iterative random mutation search to try to solve tasks?\n- Do you have any evidence that the pretrained model is comfortable using the DSL after pretraining? Or does it still struggle to use the correct input args, etc? not sure how best to convey this, but any information on this could help understand the model's capabilities better.\n- I'm wondering if a grid array number encoding works better for training, because it's more like how T5 has seen things encoded before, instead of a brand new encoding it doesn't really understand. did you try this at all, or just stick to the computationally cheaper representation?\n- You mention that including the mutated programs is a form of regularization, but I would prefer to call it data augmentation. Both imply preventing overfitting, but based on my understanding of the term, adding more data doesn't really regularize a model per se. \n- What % of the time do the sampled programs execute successfully?\n- 40/400 eval is not the state of the art on eval, is it? I think brute force approaches have solved a high fraction of the eval set \u2014 can you clarify? For example, I recall the original ARC kaggle competition winner saying they solved at least a hundred on the evaluation set using their DSL: https://github.com/top-quarks/ARC-solution/tree/master\n\nSuggestions (feel free to ignore if you feel otherwise, and no need to discuss in rebuttal)\n- I think the ARC example in figure 1 should be more complicated. This would both help audiences unfamiliar with ARC better understand the nature of the dataset, and would make the object encoding scheme easier to understand when looking at how the input is encoded \u2014 right now, the object encoding for Figure 1 has a lot of 0's, 1's, and 2's, which makes it hard to quickly understand how the example maps to the input.\n- My suggested improved title: Solving the Abstraction and Reasoning Corpus with Iterative policy-guided program synthesis. Justification: you're only evaluating on ARC, and calling your approach \"abstract reasoning\" is only true in the sense that it's applied to the abstraction and reasoning corpus. I don't think your LLM is doing much abstraction or reasoning, in the sense of forming new abstractions itself, or chaining multiple steps of thinking together to arrive at its solution (debatable though depending on your NN philosophy)\n- It might be good to include more examples of the DSL operators, so readers can have a rough sense of what your LM is generating without having to fully read Hodel's DSL explanation. Figure 4 is useful for this, but maybe you can show an example earlier on for this.\n- Some details to clarify in the main text: \n    - the mutated programs are evaluated on the inputs of the program it mutated off of to generate outputs\n    - some more details on fine-tuning: how many mutated programs, how many epochs of training. \n    - you mention weighing training on the handwritten solutions and solved tasks more often, so that you don't forget them, but is this explained further, or in the pseudocode in the appendix? if not, they should be included (maybe I missed it)\n    - I would rewrite the last sentence of the \"sampling stage\" paragraph in section 2.2 to be clearer that you sample n_p times total. \n- I would be careful not to use the word \"test\", as it might be mistaken for ARC's hidden test set. for example, in Table 2, I would call it Eval performance, not Test performance.\n- You have a \"policy only\" baseline in Figure 3, which should be described or at least listed in the \"baselines\" section. In particular, I am confused whether \"policy only\" means that the initial fine-tuning doesn't have the mutated tasks, or if the \"policy+mutation\" means that you're searching for new tasks solutions at each iteration with both with the policy and via random mutations. \n- Clarify that the solution programs were also written by Hodel \u2014 they deserve credit for that!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5606/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5606/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_NHnw"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698078868858,
        "cdate": 1698078868858,
        "tmdate": 1699636578154,
        "mdate": 1699636578154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Z0pQIkZ6ud",
        "forum": "JlSyXwCEIQ",
        "replyto": "JlSyXwCEIQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_xY18"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes CodeIt, a program synthesis method that leverages learned prior for sampling and learning for ARC problem solving. To solve an ARC instance, the method uses a code-based pretrained network to sample program variants and used the augmented data to retrain the network for final program prediction. With good implementation, the method achieves good performance compared to previous state of the art, and the ablation studies show the contribution of proposed components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method is essentially a simplified version of DreamCoder, the dream part in particular. The pretrained code-based network serves as the learned prior, and the sampling stage basically tries to augment the input-output-program triplets such that they could be used for fortifying the network, providing locally diversified data for additional training. This is a pretty intuitive way of attempting the problem. However, some questions still persist, see below."
            },
            "weaknesses": {
                "value": "As the method still largely relies on the data it samples, I wouldn't be surprised to see it becomes better than previous methods. However, I'm also interested in hearing where the limit of the simple mutation baseline is: if you more extensively sample mutations and in the extreme case, the mutations cover all your newly sampled data, would your method becomes inferior? From my perspective, your method might only be better than the mutation baseline, because your pretrained policy network serves as some smart prior, and to get exactly the programs your policy samples, random mutation might simply take more than (less efficient). However, using a unverified prior would also risk data coverage, meaning that it might not cover as much data as the random mutation method. In this sense, the random mutation, while inefficient, when taken to the extreme, could possibly be better.\n\nAs the method is basically data-driven, but smartly, I would be expecting to see ablation on the amount of sampled data, rather than the context length. How would the model perform if you reduce the number of samples, or even better, can you show the curve of perf vs. num of samples per task? If you decrease that number, your performance might not be better than the mutation baseline.\n\nAnother problem regarding ARC in general is evaluation. There has been no equal footing as for the number of data one could use. I note that in your method, you have incorporated quite a lot of sampled programs, and I seriously doubt what would happen when the newly sampled programs are used by other methods. Also a pretrained CodeT5+ is used, which already sees quite a lot of data. In the extreme, one would like to sample the space as much as possible and feed them to a model, saving all the trouble in modeling.\n\nExperiments only on ARC are not necessarily sufficient to show the superior of the method. I knew of the Raven matrices that also stress abstract reasoning, and the RAVEN dataset has similarly structured data and a much simplified program structure. Would it be possible to show similarly improved performance on this task?\n\nOne thing I've been thinking about ARC is that the community has been doing program search on a fixed DSL for a while. Would it be possible to jointly search over the DSL space, maybe starting simple such as using a mutation method for the DSL space?"
            },
            "questions": {
                "value": "See weaknesses. A lot of the questions may not be properly answered under the current climate, but please try to."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698743333939,
        "cdate": 1698743333939,
        "tmdate": 1699636578045,
        "mdate": 1699636578045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GvIzis5M5i",
        "forum": "JlSyXwCEIQ",
        "replyto": "JlSyXwCEIQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_JPeY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_JPeY"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an iterative program search method to tackle the challenging benchmark, ARC, designed to measure AI skill acquisition and generalization ability. The main idea is similar to iterative policy improvement approaches, where at each iteration,1)  a set of solutions are sampled from current policy, 2) local search is performed around the sampled solutions (e.g., program mutation), 3) policy is improved via learning. The model is finetuned from the pretrained Code-T5 model, and the entire work is implemented based on the DSL manually designed for ARC. The experiment shows that the proposed method, CodeIt, outperforms previous SOTA method by large margin. Also, authors conduct various ablation studies to analyze policy\u2019s capacity to understand input-output grids (i.e., few-shot program inference ability), and the effect of policy update and pretrained weights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed idea is simple and reasonable\n* The empirical study clearly demonstrates the effectiveness of the proposed method\n* The paper reads well (but there are few missing details. Please refer to the Questions section)\n* The ablation experiments are adequately designed to analyze the effect of each component"
            },
            "weaknesses": {
                "value": "* Limited contribution and novelty\n\n As described in the related work section, the proposed idea shares the main idea with the iterative policy improvement works: iterating between policy-guided search followed by imitation learning to improve the policy. It is indeed somewhat beneficial to the readers to show that applying existing idea to the new challenging domain works well. However, it would be more helpful to provide more intuition beyond that, given that the idea is mostly inspired by the existing work in other domains. For example, analyzing whether there is any unique challenge/benefit in applying iterative policy improvement ideas to the ARC domain compared to conventional RL domains could be an interesting contribution.\n\n* Scalability is limited\n\n In the discussion section, authors mention that \u201c..via program mutation and hindsight relabeling, thus requiring only a small amount of expert programs to start training\u201d. Although this is much better than requiring a large number of programs as data, the program mutation is only possible and effective if the DSL is efficiently designed by domain experts and also the program mutation algorithm is carefully designed by domain experts. Also for filtering, the execution engine for DSL is required as well. Overall, these requirements limit the scalability of the proposed approach, and it is important to study how well the proposed method will perform depending on the quality/availability of these prerequisites.\n\n* Comparison with baselines in common settings\n\n Although I recognize that there seems no widely accepted common experiment  setting exists, when it comes to comparing with other approaches, it would be more convincing if the proposed method and the compared methods are compared in a common setting. I do agree that the proposed DSL, grid representation, program mutation, and DSL execution engines are the important contributions of this work. However, still for evaluating the effectiveness of the proposed *learning framework* alone, including an apple-to-apple comparison result would greatly improve the paper\u2019s significance. Also, comparing with other learning-based baselines would be great.\n\n* Intuition behind recency-based sampling of input-output pairs\n\n It would be helpful to provide intuition behind the proposed design choice such as recency-based sampling of input-output pairs."
            },
            "questions": {
                "value": "* Comparison with previous iterative policy improvement methods\n\n It is not clear how the proposed method differs with iterative policy improvement. Sampling from policy and performing local search (e.g., program mutation in CodeIt) around the sampled action is a common approach in iterative policy improvement. The related work only explicitly compares with ExIt and ReST among iterative policy improvement methods. \n\n* First paragraph of Section 3.1: the meaning of $n_\\rho$ and $n_{\\text{tasks}}$ are not defined\n\n* In Figure 3, the meaning of x-axis is undefined and unclear. Section 3.3 mentions \u201cacross meta-iterations in Figure 3.\u201d. Does it mean \u201cnumber of sampled programs\u201d is the same as meta-iterations?\n\n* The agents \u201cCodeIt: mutated+policy\u201d and \u201cCodeIt: policy only\u201d are not defined\n\n* Suggestion: population-based policy searching\n\nAs indicated by the ablation A2 experiment, policy improvement plays an important role in program searching. It would be an interesting future direction to try population-based policy searching approaches; i.e., maintaining multiple population of policy networks Q to enable more efficient exploration in program search space."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802000572,
        "cdate": 1698802000572,
        "tmdate": 1699636577911,
        "mdate": 1699636577911,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HO3mfW4m50",
        "forum": "JlSyXwCEIQ",
        "replyto": "JlSyXwCEIQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the task of programming by examples. The authors propose a method where a language-model-based policy network generates DSL programs given the input examples. The policy network is first pretrained on human-annotated examples-program pairs, and then iteratively trained on programs and input examples generated from the last iteration of the network. Program mutation and hindsight relabeling increase the diversity of the training data. The proposed method has achieved SOTA performance on the evaluation split of the ARC benchmark."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method is simple and straightforward.\n- The proposed method achieves SOTA performance on the evaluation split of ARC. The ARC benchmark being tackled is known to be challenging. Most works do not evaluate the full evaluation dataset but instead focus on a simpler subset.\n- Ablation studies are also conducted to demonstrate the contribution of each component in the model."
            },
            "weaknesses": {
                "value": "- Technical novelty is limited. As the related work mentions, iterative policy improvement and hindsight relabeling are not new ideas. In the regime of program synthesis, the idea sampling from the policy, filtering by execution, and then retraining is also explored in [1].\n- The effectiveness of the proposed pipeline is not convincing:\n    - From Table 1, we can see that the full version of CodeIt solves only 1% more programs compared with the \u201cmutation only\u201d baseline (40/400 vs 36/400). This naive baseline simply samples programs from the training dataset perturbed by changing one line of code. This indicates the DSL and the original reference program for the training dataset play a major role in achieving SOTA performance. The author also mentions this point in the discussion. But I believe it is important to expand on this.\n    - To better illustrate the performance of CodeIt, I strongly encourage the authors to provide the following results: 1. how many solved tasks are in common for CodeIt and the mutation baseline? 2. For tasks that both methods solve, are there any differences between the solutions of each method? Will CodeIt generate a more succinct program?\n\nOverall, I believe the technical novelty and the effectiveness of the method need more justification. I am willing to raise my score if my concern is addressed.\n\n[1] Language Models Can Teach Themselves to Program Better.  Patrick Haluptzok, et al. ICLR 2023"
            },
            "questions": {
                "value": "- It would be helpful if the authors could provide quantitative results on how well CodeIt compresses programs like the example in Figure 4. How many correct programs are shorted throughout the iteration? Are there programs that become longer?\n- The task mutation procedure plays a crucial part in the procedure, I believe it\u2019s important to expand the pseudocode to explain more details about it. There is no explanation about functions in the pseudocode, though some can be inferred from the names. How is this mutation procedure determined? How sensitive are the results w.r.t to the hyperparameter $\\phi$?\n- How do authors determine the number of training epochs during continual learning? Will training for more than 1 epoch for each iteration degrade the performance?\n\nOther Comments:\n- Figure 4, it\u2019s hard to interpret the figure because the task being considered is not shown and the meaning of each function is unknown.\n- why the example input and output are named S_0 and S_N?\n- It is encouraged for authors to include the DSL in the paper, the original write-up of the DSL is not straightforward to find.\n\nTypo\n- The first sentence of Section 3.3: \u201cWe report performance of CodeIt after [sampling] 500,000 programs,\u201d\n- Second paragraph of Section 2.2: \u201cWe [finetune] the policy network on the resulting set, and initialize our replay buffer with it\u201d. \u201cfinetune\u201d should be \u201cpre-train\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5606/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5606/Reviewer_WdyM",
                    "ICLR.cc/2024/Conference/Submission5606/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5606/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814144408,
        "cdate": 1698814144408,
        "tmdate": 1700732414152,
        "mdate": 1700732414152,
        "license": "CC BY 4.0",
        "version": 2
    }
]