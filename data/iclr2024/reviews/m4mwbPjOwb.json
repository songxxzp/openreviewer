[
    {
        "id": "4tsi80o0Lk",
        "forum": "m4mwbPjOwb",
        "replyto": "m4mwbPjOwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_nfMt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_nfMt"
        ],
        "content": {
            "summary": {
                "value": "They proposed a text-to-speech model which utilizes a latent diffusion model. They introduce a U-ViT for latent diffusion based speech synthesis, and they do not require text-speech alignment for speech synthesis."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This work utilizes a pre-trained language model for text encoder, and they can generate a speech without text and speech alignment. This facilitates training pipeline efficiently."
            },
            "weaknesses": {
                "value": "Although this work proposed a simple method for text-to-speech without text and speech alignment, I have many questions about this and others. \n\n1. They fixed the maximum length of speech by 20 seconds during training. This may make a training pipeline simple, but I think it is not efficient for GPU. In addition, this framework could not control the duration of speech.\n\n2. It would be better if you could add an additional experiment according to text length. Because you train your model with a fixed length, you should demonstrate the robustness according to text length.\n\n3. The authors may not know the definition of end-to-end. This model is not the end-to-end speech synthesis model. They need the pre-trained audio autoencoder and language model for the text encoder. They have overclaimed it. \n\n4. The audio quality is too bad. I think it is because the audio autoencoder has a lower quality. You should have trained an audio autoencoder with speech data or replaced it with a high-quality audio autoencoder. I recommend to use different audio autoencoder such as DAC or HiFi-Codec or utilizes a pre-trained codec-based vocoder such as Vocos for high-quality waveform audio. It is well known that re-training the codec-based vocoder could generate a better quality of audio.\n\n5. I think NaturalSpeech 2 already introduced this kind of method for speech synthesis. The difference with NaturalSpeech 2 is only the necessity of duration predictor. However, I think removing the duration predictor decreased the controllability of the model so I hope the authors address this issue. I think that the authors should have listened to the demo samples of NaturalSpeech 2 and compared the audio quality with yours. In addition, although they propose new architecture, there is no ablation study for model architecture.\n\n6. They only compared the model with YourTTS. YourTTS has a very low audio quality. You should have trained the VITS with same dataset and speaker prompt."
            },
            "questions": {
                "value": "I sincerely have a question about the audio quality. How do the authors think the audio quality of your model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6296/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6296/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6296/Reviewer_nfMt"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6296/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698488071842,
        "cdate": 1698488071842,
        "tmdate": 1699639947719,
        "mdate": 1699639947719,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QW0W4cROdG",
        "forum": "m4mwbPjOwb",
        "replyto": "m4mwbPjOwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_Smvk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_Smvk"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a TTS model, the simple-TTS, that uses latent diffusion models and U-Audio Transformer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The usage of U-Audio Transformer is potentially useful for speech generation.\n2. The results reported in the experimental section are encouraging."
            },
            "weaknesses": {
                "value": "1. The authors claim that the proposed model is an end-to-end model, however, it contains at least 3 separately training stages. Obviously, it is a not end-to-end model.\n\n2. The authors call this model is a 'simple' model, however, it's not so simple. Taking the baseline model YourTTS for comparison, YourTTS is trained without using language model pre-training and En-Codec model, thus appears to be more simple.\n\n3. In sections 1 and 2, the authors claim that simple-TTS is much simpler than NaturalSpeech2 and VoiceBox. However, in the experimental section, they do not provide a direct comparison to these models. \n\n4. The experiments are far from sufficient.\n   1). Why not present the MOS results for Text-only TTS?  which is very important to evaluate this work. \n   2). Many important details are missing. For example, the sample rate of the audio samples. The synthesis speed is not presented.\n   3). From my subjective evaluation, the audios in the supplementary Material are not as good as some SOTA models such as VITS and NatrualSpeech."
            },
            "questions": {
                "value": "Why not present the MOS results for Text-only TTS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6296/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744399086,
        "cdate": 1698744399086,
        "tmdate": 1699636691081,
        "mdate": 1699636691081,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CufJkXzEyd",
        "forum": "m4mwbPjOwb",
        "replyto": "m4mwbPjOwb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_etZW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6296/Reviewer_etZW"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a TTS model in the form of Latent diffusion called Simple-TTS. It simplifies the training process of the TTS model by using a pre-trained text encoder and EnCodec and training only the weight of latent diffusion model. It outperforms the open-source zero-shot TTS model, YourTTS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. As the title of the paper, the training process of TTS has been greatly simplified. By utilizing a pre-trained Text Encoder (ByT5), the need to train the text encoder has been eliminated, and by aligning speech with text through cross-attention, the need for a duration predictor has been removed. Through this, the model is trained using only a simple v-prediction for diffusion model.\n\n2. Listening to the samples provided in the Supplementary material, the generated samples sound expressive."
            },
            "weaknesses": {
                "value": "1. In simplifying the model training, there is a suspicion of potential issues in the process of learning monotonic alignment between text and speech through cross attention. Additionally, padding all sentences to a fixed length during training and allowing the diffusion model to learn on its own is presumed to be heavily influenced by the length of the speech data in the dataset. It seems that this model may also have the robustness issues that were present in autoregressive TTS models using cross-attention for alignment like Tacotron or TransformerTTS.\n\n2. Sample quality in supplementary material is too bad.\n\n3. Despite the proposed model has lower speaker adaptation performance compared to recent papers such as VALL-E, SPEAR-TTS, and VoiceBox, claiming to release the strongest publicly available system by showing performance improvements over an easily beatable baseline like YourTTS seems like an overstatement in abstract."
            },
            "questions": {
                "value": "* Despite using pre-trained models, why do you refer to Simple-TTS as an end-to-end TTS model?\n\n* Regarding Weakness 1, does Simple-TTS have no robust issues in finding alignments even with the introduction of cross-attention? If not, it would be beneficial to provide the ASR metrics for the Hard sentences found in Appendix B of the FastSpeech paper as well.\n\n* How can Simple-TTS generate speech that exceeds the predetermined length during training? For example, generating 30+ seconds of speech given a few long sentences.\n\n* Regarding Weakness 2, the sample quality is too bad compared to existing zero-shot TTS models. NaturalSpeech 2 also models continuous latent representation similarly to the proposed paper, but the sample quality of the proposed method is relatively poor in comparison. It would be beneficial if this could be improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6296/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823371841,
        "cdate": 1698823371841,
        "tmdate": 1699636690971,
        "mdate": 1699636690971,
        "license": "CC BY 4.0",
        "version": 2
    }
]