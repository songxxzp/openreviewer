[
    {
        "id": "48u9U7wpJL",
        "forum": "D8DAQhpznu",
        "replyto": "D8DAQhpznu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_LeSy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_LeSy"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the role of confidence in improving the LLM performances in QA tasks. Some LLMs do not output confidences, so a new method to elicit the confidence is necessary.\n\nThis paper first considers \u201clinguistic confidence\u201d: prompt the model to output a notion of confidence. However, for the models that provides accesses to probabilities, the performances from the linguistic confidence are worse than the performances from the probability confidence.\n\nThis paper proposes using surrogate models (i.e., some models where we do have access to their probabilities) to estimate the confidence. On 10 out of 12 datasets, the method has a higher AUC than the \u201clinguistic confidence\u201d method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed confidence estimation approach is elegant. It\u2019s simple, and it works well.\n- There are extensive experiments showing that the proposed approach (and its variants) work on a wide range of problems.\n- There are also extensive ablation studies showing the different combinations of the surrogate models. (Some studies are not covered though \u2014 please refer to my comment below.)"
            },
            "weaknesses": {
                "value": "- The proposed algorithm seems to have limitations. The proposed algorithm 1 still requires the main model to output linguistic confidences (unless alpha=1), which is a confidence score less as good as the probability scores.\n- The evaluation can be more rigorous. I have been looking for the evaluations for the validity of the linguistic confidence. Specifically, how well do they correlate to the probabilities directly outputted by the models? The evaluation scores presented in this paper focused on the utility of these confidence scores though.\n- The value of a crucial hyperparameter is not reported. Alpha, the scaling factor between the two confidence scores, seems very important for the overall AUC / AUROC performances. The actual values for the optimal settings, or the approaches to reach the optimal values, are not reported.\n    - A related note, the heading of the second paragraph in 5.1 (\u201dEpsilon is all you need\u201d) seems to indicate that very small alpha values are sufficient, which is obviously not the case, considering that \u201cTiebreak\u201d and \u201cSurrogate\u201d settings have quite different results. In general, a claim like \u201cXYZ is all you need\u201d usually leave me with a (perhaps wrong here) impression that the paper is a social media post rather than a scientific paper.\n- An intuitive extension of the algorithm could have been explored. Since using one surrogate works well, does combining two surrogate models work?"
            },
            "questions": {
                "value": "What do the \u201c+\u201d symbols at the end of many methods in tables 2 and 3 mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8590/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609069105,
        "cdate": 1698609069105,
        "tmdate": 1699637074681,
        "mdate": 1699637074681,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rbRGRUtBFT",
        "forum": "D8DAQhpznu",
        "replyto": "D8DAQhpznu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_RfxS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_RfxS"
        ],
        "content": {
            "summary": {
                "value": "This paper leverages the open-source LLM known as \"llama 2\" to assess the uncertainty or confidence of outputs from the black-box LLM model, GPT-4. The authors demonstrate that by using the llama 2 confidence scorer, one can achieve a higher AUC. Moreover, the paper introduces a novel mixture function designed to combine outputs from multiple confidence-scorer models, ultimately resulting in an optimized scorer."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Assessing the uncertainty of black-box language models represents a significant and intriguing research direction.\n- Leveraging the probability metrics from an open-sourced language model is an intuitive approach.\n- The authors provide comprehensive AUC results from a variety of confidence-scorer models and policy models. These findings will be valuable for future researchers when choosing a confidence scorer."
            },
            "weaknesses": {
                "value": "- Soundness: The methodological soundness of this study appears somewhat lacking. There's a noticeable lack of a baseline comparison in the work.\n  - While the paper primarily focuses on uncertainty or confidence, it doesn't compare with established certainty scorers. It would be beneficial to discuss relevant works such as [1] and [2] and incorporate them in the experimental section.\n  - The study also touches on the critiquability of LLMs and LLM evaluation. Including references [3], [4], and [5] in the related work and experiments would provide more depth and context to the discussions.\n\n- Novelty and Contribution:\n  - The approach of using surrogate models to interpret black-box models isn't novel.\n  - The introduced mixture function, essentially a simple linear combination, raises questions regarding its uniqueness. A clearer differentiation from existing methods might strengthen this section.\n\n- Clarity and Writing Quality:\n  - The manuscript could benefit from further editing for clarity and structure. For detailed feedback, refer to the 'Question' section.\n\n[1] Uncertainty Quantification with Pre-trained Language Models:A Large-Scale Empirical Analysis\n\n[2] Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models\n\n[3] Self-Refine: Iterative Refinement with Self-Feedback\n\n[4] CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing\n\n[5] A Survey on Evaluation of Large Language Models"
            },
            "questions": {
                "value": "In Table 1, for every row, is the policy model identical to the scorer model, effectively making it a self-scorer? As an instance, does \"Text-davinci Prob\" employ \"Text-davinci\" as both its policy and scorer model?\n\nRegarding the statement \"embeddings of questions that GPT-4 gets incorrect\" \u2013 can you provide clarity on how these embeddings are derived or obtained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8590/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774029655,
        "cdate": 1698774029655,
        "tmdate": 1699637074546,
        "mdate": 1699637074546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TzX0LFaMfq",
        "forum": "D8DAQhpznu",
        "replyto": "D8DAQhpznu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_mEr2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_mEr2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use open-source models such as LLaMa as a proxy for finding confidence estimates for models that do not provide probabilities, such as GPT or Claude."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has fairly extensive experimentation over a large number of tasks."
            },
            "weaknesses": {
                "value": "I feel that this method introduces additional complexity in a place where it is not clearly needed, and because of this I am skeptical of whether this method will see wide adoption should the paper be accepted to ICLR.\n\nSpecifically, I am not convinced of the underlying premise of the paper, that you cannot get probabilities out of closed models. Specifically, it is well known that sampling can be used to approximate probabilities (see the \"Pattern Recognition and Machine Learning\" textbook for example), and all closed models that I know of support sampling. Xiong et al. empirically demonstrated that this is a quite effective way of getting probability estimates out of models, and this is much easier than additionally running a separate proxy model to get probability estimates. The mixture of surrogate probabilities method indeed marginally beats the best method of Xiong et al. (by 0.4% AUC for example), but this doesn't seem to warrant the additional complexity."
            },
            "questions": {
                "value": "None in particular, although I would be open to arguments about why this method may be preferable over other simpler alternatives."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8590/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698895108761,
        "cdate": 1698895108761,
        "tmdate": 1699637074440,
        "mdate": 1699637074440,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oE1bmEeCIi",
        "forum": "D8DAQhpznu",
        "replyto": "D8DAQhpznu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_78gw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8590/Reviewer_78gw"
        ],
        "content": {
            "summary": {
                "value": "The paper is focused on confidence elicitation for models that do not provide confidence probabilities their answers. Such models include GPT-3.5., GPT-4 and Claude. Linguistic confidences are obtained by zero-shot prompting the models to assign confidence scores to their answers. The linguistic confidences are evaluated in a selective classification setting (where the goal is to have confidence scores that are calibrated with the correctness of the answers). Two metrics are used for evaluation: AUC (area under the coverage-accuracy curve) and AUROC (area under the receiver operator curve). Experimental results using 12 standard question answering datasets show that the linguistic confidences are not much better than random guesses. Furthermore, they are worse than model probabilities from surrogate models such as Llama-2 variants. The best results are obtained when linguistic confidences are mixed with surrogate model probabilities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper includes an extensive number of experiments over 12 standard question answering datasets and the results are consistent over the 12 datasets. \n\nIt is interesting to know that surrogate model probabilities are a better indicator of confidence than the linguistic confidences. It's also interesting to see that combining surrogate model probabilities and linguistic confidences improves the results."
            },
            "weaknesses": {
                "value": "The paper mainly consists of a large set of well-conducted experiments, but lacks the depth.\n\nWhile the results are interesting, they are actually not very surprising. The mixture of models approach is very straightforward. \n\nThe discussion of the results is not very insightful. Given the focus of the paper, it would be interesting to better understand the reasons the models are not good at eliciting good linguistic confidence scores. While the authors claim that error calibration is not the focus of the paper, it would be interesting to know how uncalibrated surrogate models perform by comparison with calibrated surrogate models."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8590/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8590/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8590/Reviewer_78gw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8590/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698970682370,
        "cdate": 1698970682370,
        "tmdate": 1699637074342,
        "mdate": 1699637074342,
        "license": "CC BY 4.0",
        "version": 2
    }
]