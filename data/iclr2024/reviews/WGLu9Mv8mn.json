[
    {
        "id": "rpU5C2Iaae",
        "forum": "WGLu9Mv8mn",
        "replyto": "WGLu9Mv8mn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_ioCA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_ioCA"
        ],
        "content": {
            "summary": {
                "value": "This paper formalizes a problem setting named privacy aware few-shot class incremental learning for activity and gestures. A Prompt-offset Tuning (POET) method is proposed to solve this problem, which includes a temporal-ordered learnable prompt selection module. Experimental results on action recognition and gesture recognition indicate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- A new problem setting named privacy aware few-shot class incremental learning for activity and gestures is proposed.\n- A prompt-based method is introduced to solve the proposed setting."
            },
            "weaknesses": {
                "value": "- The motivation for the proposed POET is not clear, which makes this paper like an incremental work. Authors should show some properties of POET that fit well into the FSCIL task. Rather than simply applying existing prompt learning techniques to the proposed FSCIL task.\n- The proposed prompt-based method seems like a general approach. Can POET be used to solve other continual learning tasks like L2P? \n- It is not clear why pretrained weights like ImageNet21K pretraining cannot be used. This greatly limits the application value of the research.\n- Qualitative experimental analysis should be provided to facilitate understanding of the properties of the proposed method."
            },
            "questions": {
                "value": "- How does the proposed method compare to existing methods in scenarios where pre-training is available, such as ImageNet pretraining?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6237/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698411102225,
        "cdate": 1698411102225,
        "tmdate": 1699636681880,
        "mdate": 1699636681880,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jJeBJrcHHY",
        "forum": "WGLu9Mv8mn",
        "replyto": "WGLu9Mv8mn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_SdtP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_SdtP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a prompt-offset tuning.\n\nThey form a new problem setting called privacy aware few-shot class incremental learning for activity and gestures.\n\nThe approach uses the temporal sequencing in the data stream of actions and gestures to propose a unique temporal-ordered learnable prompt selection and prompt attachment.\n\nThey evaluate the proposed method on two benchmarks, NTU RGB+D and SHREC-2017 for action recognition and hand gesture recognition respectively."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Overall the paper is technical sound.\n\n+ There are some nice and interesting comparisons presented in the experimental section."
            },
            "weaknesses": {
                "value": "Major: \n\n-  Formulas: In the approach section, the author gives a detailed description of every mathematical detail, which is worth encouraging, but neglects the main idea and intention behind the module design. To be honest, I am lost in a large number of mathematical symbols. I don't know why these modules are designed in this way and what the structure does.\n\n-  It is suggested to have a notation section detailing the Maths symbols used in the paper, for, e.g., vectors, matrices, tensors, etc.\n\n- Very limited experiments / evaluations (i) the datasets are limited to 2 smaller datasets (ii) the scenarios are only limited to action recognition and gesture recognition, one dataset each. This is a noteworthy shortcoming.\n\n- The ablation studies in section 4.3 are not explained clearly. The analysis and discussions are very limited and not clearly presented to readers. It is suggested to add detailed descriptions for e.g., variants, experimental settings, and provide more insights to show the patterns and explain clearly why.\n\n-  Figures: The research paper should use sufficient figures to show the details of the model, and also visualizations of experimental results in different formats. In this paper, the author only draws two figure, and all the other details are contained in plain texts and tables. Why there are no any visualizations for experimental sections.\n\n- Description: Research papers should primarily focus on effectively describing their own innovations. An exceptional paper should strive to clarify ideas for readers, avoiding vague or unclear presentations. The approach section appears to be poorly written and lacks fluency in its flow. The expression in English should be enhanced to make the paper more engaging and less dry.\n\n-  Table 4 what does 0.0 performance mean? \n\nMinor:\n\n- Abstract section, AR and VR should be explained properly and clearly rather than simply using abbreviations.\n\n- The left quotation marks are all not properly handled, e.g., page 2, 3, especially page 8 section 4.3."
            },
            "questions": {
                "value": "Please refer to weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6237/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654799877,
        "cdate": 1698654799877,
        "tmdate": 1699636681759,
        "mdate": 1699636681759,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SmhSI7spJ1",
        "forum": "WGLu9Mv8mn",
        "replyto": "WGLu9Mv8mn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_H7jd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_H7jd"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces prompt-offset tuning (POET) for continual few-shot skeleton-based action and hand gesture recognition. It aims at learning the novel classes with few training samples incrementally after training the model on base classes to protect the privacy. Specifically, it firstly trains the model on base classes and freeze it afterwards. Then a prompt module is built to handle the novel classes inspired by LLM prompt tuning. In this way, the whole network does not need to be fine-tuned since the prompt module can efficiently tackle the new classes. In addition, two benchmarks on NTU-RGB+D 60 and SHREC-2017 are built for corresponding evaluation. The experiment results show that the proposed method has good performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) This paper introduces a new task, namely privacy aware few-shot class incremental learning for activity and gestures.\n2) The idea of adapting LLM prompt tuning for few-shot action recognition is novel and makes sense. \n3) The components of prompt module are designed with clear goals\n4) The experiment results and ablation studies demonstrate the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1) In the introduction, the paper talks about the ViT pretrained on ImageNet. It claims one advantage of POET is that it works well without pretrained backbone. However, 3D skeletons are pretty precise and representative data compared to images. Most graph convolution based action recognition methods do not need a pretrained backbone to extract features. To demonstrate the claim, a comparison of with and without pretrained backbone on video-based dataset is more convincing. (If I didn\u2019t misunderstand, the \u201cbackbone\u201d in the following sections refers to the model trained on the base classes, which is different from the concept in the introduction)\n2) The proposed benchmarks miss many details. The NTU-RGB+D 60 dataset is divided into 40 base and 20 few-shot continual classes. Usually, we follow the cross-subject or cross-view settings so that the model generalization can be evaluated. Same for the SHREC-2017 dataset. Classes that are used for training and testing should be specified so that future work can follow the same settings to make comparison.\n3) The majority of the model is frozen after training on the base classes, so there should be a quantitive comparison to demonstrate the computation efficiency improvement and the reduction of trainable parameters when learning the novel classes.\n4) If the f_q is fine-tuned on both base and few-shot classes, there will not be overfitting for the few shot data."
            },
            "questions": {
                "value": "1) The \u201cskeleton-based action recognition\u201d may be more accurate for the title. \n2) I am confused about the privacy aware part of the method. The model is trained incrementally with all the novel classes. Even each novel dataset is discarded after training, all the novel data are still used for training and the model contains the information of all trained classes. How is the privacy protected?\n3) For the other methods, how are they implemented? E.g. what are the architectures used and additional regularization losses mentioned in page 9? \n4) In Table 3, why there are two \u201cCODA-P\u201ds and two \u201cL2P\u201ds? What are the differences?\n5) The paper claims it is better than knowledge distillation, prior-based regularization, rehearsal or parameter isolation. Is there are comparison?\n6) There are \u201d??\u201d in page 13, please fix it"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Reviewer_H7jd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6237/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728480410,
        "cdate": 1698728480410,
        "tmdate": 1699636681604,
        "mdate": 1699636681604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hMXyM5JOIk",
        "forum": "WGLu9Mv8mn",
        "replyto": "WGLu9Mv8mn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_c8Px"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6237/Reviewer_c8Px"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the author introduced a few-shot class-incremental learning method for action recognition, leveraging visual prompting techniques. The experimental results show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of prompt-offset tuning for few-shot class-incremental learning is interesting."
            },
            "weaknesses": {
                "value": "1. The motivation is not clear to me.  While the method has been tested for action tasks, notably skeletal action recognition, it doesn't seem tailored for this particular domain. Why was the method applied to action recognition and not to a standard few-shot class-incremental learning task?\n\n2. The compared methods should be the few-shot class-incremental learning works, instead of the prompt tuning works. Drawing comparisons with prompt tuning methods, which aren't tailored for few-shot scenarios, might not provide a balanced perspective.\n\n3. What is the meaning of * ** ^ in Table 3. How can the method achieve around 3% performance for both old and new classes? \n\n4. To the best of my knowledge, few-shot class-incremental learning and continual few-shot learning are indeed distinct tasks. It might be worthwhile to revisit and clarify this in the study."
            },
            "questions": {
                "value": "Please refer to 'Weaknesses'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6237/Reviewer_c8Px"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6237/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735276149,
        "cdate": 1698735276149,
        "tmdate": 1699636681491,
        "mdate": 1699636681491,
        "license": "CC BY 4.0",
        "version": 2
    }
]