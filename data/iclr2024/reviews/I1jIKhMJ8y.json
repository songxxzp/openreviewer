[
    {
        "id": "VuUjXgUbZP",
        "forum": "I1jIKhMJ8y",
        "replyto": "I1jIKhMJ8y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_9bsB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_9bsB"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a framework to learn fixed dimensional task embeddings for RL tasks. Their goal is to ensure that tasks with similar embeddings have similar performance across a diverse population of agents. The similarity measure used is information theoretically motivated and the authors propose an algorithm to learn the task embeddings satisfying ordinal constraints imposed by this similarity measure. The learned embeddings are visually demonstrated for 5 tasks: MULTIKEYNAV, CARTPOLEVAR, POINTMASS, KAREL and BASICKAREL. Finally, quantitative results are provided showing the effectiveness of the learned embeddings in predicting performance on similar tasks and for identifying tasks with desired characteristics in the MULTIKEYNAV and CARTPOLEVAR settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed framework is intuitive and easy to follow. The writing overall is also easy to understand. \n\n2. Using learned task embeddings to reduce uncertainty about agent's performance on unseen tasks based on its performance on related tasks could be helpful in different RL applications, therefore the problem setup seems to be well-motivated. \n\n3. For the 5 environments considered in the paper, extensive experiments have been performed to analyze the performance of the proposed method."
            },
            "weaknesses": {
                "value": "1. The results included in the paper focus on learning low dimensional embeddings for the tasks - for example, in CARTPOLEVAR the learnt embedding is of dimension 2 or 3 whereas in BASICKAREL it is of dimension 1. The experiments do not consider more difficult tasks, such as MuJoCo tasks considered in [1]. \n\n2.  There is no discussion of the relatedness / differences with the bisimulation representation learning method in [1] which also learns an embedding of states in RL tasks, and ensures that states which would lead to similar outcomes have similar embeddings. It would help to include a discussion of why it has not been considered as a baseline in the experiments either. \n\n3. It is a bit confusing to understand the differences between $S_{init}$ and $S$. The authors should consider clarifying in the main paper the differences between a task definition and the MDP states. \n\n4. The proposed method relies heavily on the availability of a diverse set of agents in the environment. This could affect the quality of task embeddings learned, as the authors also demonstrate in Fig. 4.\n\n[1] Zhang, A., McAllister, R., Calandra, R., Gal, Y. and Levine, S., 2020. Learning invariant representations for reinforcement learning without reconstruction. arXiv preprint arXiv:2006.10742."
            },
            "questions": {
                "value": "I do not fully understand the PredModel baseline. The authors say it is \"inspired by prior work\" but there are no citations provided and I may be missing the link to prior work. Could the authors please clarify that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5171/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609403085,
        "cdate": 1698609403085,
        "tmdate": 1699636512676,
        "mdate": 1699636512676,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ACwFO1GIvf",
        "forum": "I1jIKhMJ8y",
        "replyto": "I1jIKhMJ8y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_HkeS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_HkeS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of learning embeddings for RL tasks that capture the semantics of these tasks. In particular, the goal is to represent tasks using finite dimensional vectors such that (i) the dot product of the vectors corresponding to any two tasks measures the similarity between the tasks and (ii) the norm of the vector representing a task captures the difficulty of the task. The solution involves quantifying task similarity and task difficulty using a distribution of diverse agents and then learning embeddings to optimize the two objectives. Experiments on different environments show that the learned embeddings indeed satisfy the two objectives\u2014e.g., they can be used to obtain clusters of similar tasks. The usefulness of such embeddings is demonstrated by using them to solve two downstream tasks: (i) predicting the performance of a policy w.r.t. a task given its performance on a small set of tasks and (ii) selecting a task from a set of tasks that satisfies various criteria (such as most similar to a given task)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of learning general purpose embeddings for tasks instead of learning them for the specific purpose of multi-task learning seems novel and interesting. The studied applications (performance prediction and task selection) justify the value in learning such embeddings. These applications can be useful in other domains such as curriculum learning.\n- Using a distribution over a diverse population of agents to quantify difficult-to-express objectives such as task similarity is an interesting technique and can potentially be applied in other scenarios.\n- The paper is fairly well-written and conveys the main ideas clearly (though some details could be explained better)."
            },
            "weaknesses": {
                "value": "- The entire approach seems to depend heavily on the population of agents used to define the learning objectives. For instance, the probability of success (PoS) of a task is taken to be a measure of task difficulty. However, it is possible that an \u201ceasy\u201d task has a lower PoS when compared to a \u201cdifficult\u201d task if a policy solving the easy task is absent in the set of agents. Some of the experimental results seem to be a direct result of the way the agent population is obtained\u2014e.g., the clusters corresponding to unique sets of keys in MultiKeyNav could be a result of using biased task distributions to train agents. Some heuristics are suggested for obtaining the population of agents which seem to work well for the environments in the paper, but their applicability to new domains is unclear.\n- The overall task is assumed to be representable by the initial state. This enables task embedding to be a function of the initial state. This assumption might not hold in general (several tasks could start from the same state and vice-versa). In such cases, the task is represented by the reward function and the proposed approach is not readily applicable.\n- Some comparisons to baselines seem unfair since the evaluation criterion is based on the population of agents used to learn the embeddings. For instance, in the task selection experiment, task similarity and difficulty (for evaluation purposes) are measured using the same quantities as those used while learning the embeddings. Therefore, the significance of these experiments is unclear."
            },
            "questions": {
                "value": "1. In Section 5.4, PredModel is mentioned to be inspired by prior work. Could you provide a citation for the work this baseline is inspired by?\n1. Why is the start state assumed to represent the task and why is it a reasonable assumption? Are there other ways to represent tasks (so that they can be input to the embedding network) such as natural language descriptions that are better suited here?\n1. It looks like transfer learning and multi-task learning are natural applications of such embedding vectors. Are the generated embeddings helpful for these applications as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Reviewer_HkeS"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5171/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727245116,
        "cdate": 1698727245116,
        "tmdate": 1699636512559,
        "mdate": 1699636512559,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wSSjxuWNNh",
        "forum": "I1jIKhMJ8y",
        "replyto": "I1jIKhMJ8y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_Q8ok"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_Q8ok"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces an algorithm for learning task embeddings to measure the difficulty and similarity between tasks through the performance of a population of agents given a class of tasks. The algorithm includes two components (1) contrast among a triplet of tasks, making sure the inner product of the task embeddings implies the task similarity (2) impose the constraint on the easier tasks that have smaller norms. Experiments test the following hypothesis:\n\n1. Distinct clusters can be visualized through embedding space\n2. The norm of the embeddings can indicate task difficulty\n3. The learned embedding can be used to predict the agent\u2019s performance and task selection with desired characteristics"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is written in clarity and the logics are easy to follow\n2. This paper does nice visualization and the results make sense"
            },
            "weaknesses": {
                "value": "1. It is unclear how task embedding is useful to me. As it requires checkpoints of learned policies that almost solve the task and \"difficulty\" is vague to an agent's performance as an agent may take a different path to solve the task when there are multiple solutions. Plus, it is almost impossible to get task embedding without exploring a few trajectories of it to get anything meaningful, unlike some rule-description tasks.\n2. It only generalizes to variations of a particular environment."
            },
            "questions": {
                "value": "1. How do you guarantee the diversity of the population?\n2. Did you test learning task embedding using a single agent?\n3. How the *agent performance* data is collected? What agents did you use? Were they involved in the training of the embedding? \n4. How to test the generalization of the task embedding? (aka generalize across different tasks.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5171/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814683758,
        "cdate": 1698814683758,
        "tmdate": 1699636512477,
        "mdate": 1699636512477,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K6ysocozF1",
        "forum": "I1jIKhMJ8y",
        "replyto": "I1jIKhMJ8y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_viG4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5171/Reviewer_viG4"
        ],
        "content": {
            "summary": {
                "value": "The authors develop a framework for comparing task similarity in goal-conditioned settings under a given population of agents. Under this embedding, the norm describes task difficulty and the inner product encodes a notion of similarity. \n\nThey perform experiments on CartPoleVar, MultiKeyNav, PointMass and Karel, demonstrating via t-SNE plots that the embeddings correspond \nto salient features of the task. They then demonstrate the application of these task embeddings to predicting task performance and task selection. For task selection the authors consider two types of query, one for selecting the most similar task, and one for selecting the task that is most similar, but more difficult than a given task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The described framework is well-presented and easy to follow. The properties encoded in the task embeddings are logical. \n* The paper is overall well-written and easy to follow\n* The application results demonstrate convincing performance improvements over relevant baselines and therefore that the embeddings \nlearned are meaningful encodings of the task."
            },
            "weaknesses": {
                "value": "* My major issue with the paper surrounds motivation. Creating this task embedding requires a diverse population of agents which together are \ncompetent on a broad range of the tasks. This is a vast amount of compute relative to the amount required to solve an individual task or even a \nreasonably broad range of tasks in the space of tasks. It's therefore not entirely clear to me when such a task embedding would be appropriate. The authors go some way to answering this by demonstrating the usefulness of the embeddings in task prediction and task similarity identification. However, it's not clear to me when either of these tasks would be useful compared to training a single agent on a broader task distribution for the same total compute time required to train the population. However, I think judging future usefulness and method relevance is very difficult and so do not weight this point too strongly.\n* Because of the large amount of compute required to build these embeddings, the tasks considered are relatively simple. It would be interesting to consider more complex and higher dimensional tasks, such as by embedding levels in ProcGen."
            },
            "questions": {
                "value": "* How much compute is required to generate the population of agents and embeddings for the tasks? I could not find this information, although I may have missed it.\n* How much variation is there in the embeddings with the population? If I train the population in a different way, can the performance prediction generalise to a different population? For example, can the embedding of a task be used to predict the task performance of an agent trained with a different algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5171/Reviewer_viG4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5171/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699357507004,
        "cdate": 1699357507004,
        "tmdate": 1699636512357,
        "mdate": 1699636512357,
        "license": "CC BY 4.0",
        "version": 2
    }
]