[
    {
        "id": "YaPfjTd9KW",
        "forum": "FOSBQuXgAq",
        "replyto": "FOSBQuXgAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission61/Reviewer_hdLm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission61/Reviewer_hdLm"
        ],
        "content": {
            "summary": {
                "value": "The paper formalizes permutation and scaling symmetries in Bayesian neural networks (BNNs), proposing to express the posterior as a mixture density with components corresponding to symmetric copies of the underlying identifiable model. Investigating scaling symmetries, the authors find the minimization of an L2 term over scaling transformations to be a convex problem. In an extensive benchmark study, they shed light on the benefit of diversity realized by multi-mode methods, the opacity of the link between in-distribution diversity and out-of-distribution (OOD) performance, and the frequency of weight permutation during training. A large codebase is to be released."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* [S1] **Originality**. Expressing the BNN posterior as a mixture w.r.t. two types of symmetries seems novel and compelling, as does posing the minimization problem of L2 regularization over scaling transformations. The empirical study of the frequency of weight permutations also adds an interesting new perspective.\n* [S2] **Formalization**. Introducing symmetry operators and the mixture posterior is beneficial to the field of BNN symmetries that so far lacks a comprehensive language.\n* [S3] **Contextualization**. The authors relate established concepts like functional collapse and OOD generalization to the quality of the posterior approximation (in the light of symmetries).\n* [S4] **Empirical evidence**. Substantive experimental results are provided and the authors are to release a broad benchmark dataset."
            },
            "weaknesses": {
                "value": "* [W1] **Lack of clarity**. There are some statements whose scope/explanation is not quite clear to me:\n  * \u201cThe scaling operation (...) is a symmetry for all neural networks\u201d (Property 3.2); \u201cneural networks seem to always be subject to scaling symmetries\u201d \u2013 I feel this should be restricted to ReLU-type nonlinearities. Why does this hold in general?\n  * More generally about the *min-mass* problem: What, exactly, is the implication? Do we impose a second, ex-post minimization problem over the solution of standard training? How comes the problem is convex if existing literature on [representation costs](https://openreview.net/pdf?id=6iDHce-0B-a) (which tackles the same problem and is not cited?) turns out not to be a convex problem?\n  * \u201cAll the layer-wise marginal distributions are identical\u201d (Proposition 1) \u2013 how so?\n* [W2] **Conflating Bayesian and risk-minimization perspectives**. The paper is supposed to be about BNN posteriors, yet most claims and analyses focus on standard network training. It would be useful to elaborate more on both perspectives, or reconcile them. E.g., what do Definition 3.3 / Proposition 2 mean for a Gaussian prior equivalent to L2 regularization? As-is, the impact of the paper for actual Bayesian inference remains unclear.\n* [W3] **Unclear baseline / omissions in experiments**. Somewhat related to [W2]: I don\u2019t quite understand how the \u201ctrue\u201d posterior distribution, w.r.t. which the MMD is reported, is computed. How, exactly, is the RKHS constructed? Also, I feel that the Bayesian perspective is quite under-represented with only one MCMC method (the other methods present have no guarantees of retrieving the true posterior).\n* [W4] **Minor points**. Not relevant for my score, just to point out:\n  * Abbreviations are not always handled consistently (e.g., HMC not introduced in 2; \u201cdeep neural networks\u201d in 3.2 despite abbreviation introduced before).\n  * Figure 1 should have axis labels.\n  * Figure 3 (right) has very low contrast.\n  * P. 2, second-to-last line: \u201cfunctionnally\u201d (should be \u201cfunctionally\u201d)\n  * P. 4, first line: \u201cinfluencial\u201d (should be \u201cinfluential\u201d)\n  * P. 4, Property 3.1: \u201call neural network\u201d (should be \u201call neural networks\u201d)\n  * P. 5, second line: \u201cperceptions\u201d (should be \u201cperceptrons\u201d)\n  * P. 7, second line: \u201cefficacity\u201d (should be \u201cefficacy\u201d)\n  * P. 8, third line under 5: \u201cweights permutations\u201d (should be \u201cweight permutations\u201d)\n  * $\\omega$ is not always bold.\n  * Describing Laplace methods as \u201ccharacterizing the posterior distribution\u201d is pretty vague."
            },
            "questions": {
                "value": "* [Q1] Could you elaborate on the role of $\\theta$ in Property 3.1, Property 3.3?\n* [Q2] What kind of objects are $\\Pi$ and $\\mathbb{\\Pi}$?\n* [Q3] Does the posterior expression in Eq. 8 allow for other types of symmetries?\n* [Q4] What is your intuition on scaling symmetries being always present even with weight decay? Could this be because an infinite number of scaling transformations exists that leaves the (optimal) L2 norm unchanged?\n* [Q5] Why are single-mode methods competitive in terms of ECE? Is there a tendency towards either over- or underconfidence?\n* [Q6] Could you provide some intuition about the similarity of two networks having a MI of, say, 0.1?\n* [Q7] What is your idea behind connecting functional collapse to OOD performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Reviewer_hdLm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission61/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684308082,
        "cdate": 1698684308082,
        "tmdate": 1700668479604,
        "mdate": 1700668479604,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qFc5f0tF2Y",
        "forum": "FOSBQuXgAq",
        "replyto": "FOSBQuXgAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission61/Reviewer_24pF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission61/Reviewer_24pF"
        ],
        "content": {
            "summary": {
                "value": "The paper sets out to study the Bayesian posterior of deep neural networks. It hypothesizes that the quality of uncertainty quantification made possible by a Bayesian treatment is reduced due to the presence of scaling and permutation symmetries in the weights of the network. It presents a formal definition of permutation and weight symmetry. It evaluates the quality of typical Bayesian methods for estimating the posterior distribution on computer vision tasks and presents experimental results on \u201cfunctional collapse\u201d and weight permutation events in the process of training."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper introduces a formal treatment of scaling symmetries, and introduces the notion of a `minimum scaled network`. The work demonstrates that the optimization problem to reach this minimum-scaled network is convex. They then demonstrate that on their specific small mnist model (OptuNet), a specific neural network optimization scheme with weight decay does not reach the optimum (in terms of L2 loss) achieved by further convex optimization of the L2 norm under scaling transformations."
            },
            "weaknesses": {
                "value": "The paper as a whole presents a meandering set of concepts and results, without drawing conclusions or presenting insights into further study of Bayesian method design. This is apparent from the conclusion section, which is vague and raises questions: what are the actual insights provided by considering symmetries? What questions and hypotheses would you consider in future work? What was the \u201creal impact of scaling symmetries\u201d?\n\nThe core experiment demonstrating that L2 regularisation does not remove weight scale symmetries is presented in Figure 2, but this involves a single, custom network architecture under a single specific optimization scheme. It would be insightful to study this under various benchmarks and models to demonstrate that this effect is consistent and that the difference between the minimum reached by SGD with L2 regularisation and the minimum reached by further convex optimization is significant and actually affects the quality of Bayesian posteriors.\n\nThe experiments presented do not provide evidence for the claim presented in the method section: \"SYMMETRIES INCREASE THE COMPLEXITY OF BAYESIAN POSTERIORS\" (see questions below). Nor do they study interventions informed by the theory to evaluate the benefit of the formal treatment of scale and permutation symmetries.\n\nThe reported accuracies in Table 1 are low. While this is understandable for the purposefully restricted OptuNet, a vanilla single-mode baseline resnet-18 model on cifar-100 can reach 75% accuracy without any Bayesian treatment, and with some tuning 79%. (https://paperswithcode.com/sota/image-classification-on-cifar-100 https://huggingface.co/edadaltocg/resnet18_cifar100). It seems the results and the presented checkpoint dataset represent an artificially underfitted model space. If the aim is to improve Bayesian deep learning for a real-world setting, it might be beneficial if the models studied are optimized to a reasonable performance."
            },
            "questions": {
                "value": "- What do you mean by \u2018regroup some points around the modes\u2019 in section 3.1? What causes this effect and how is it problematic?\n- How is the \u201ctarget distribution with 1000 checkpoints\u201d for table 1\u2019s MMD scores established? What method is used to determine this distribution?\n- I am not sure what to take away from Table 1 and section 4.1-4.3. The study of uncertainty quantification performance of these methods has been done before, how does this experiment connect to the formal study of scale and permutation symmetries?\n- Similarly, how does the study of \u2018functional collapse\u2019 in Figure 3 and section 5.1 suggest that  \u201ccomplexity of the posterior is orders of magnitude higher than what we understand, taking symmetries into account.\u201d I don\u2019t follow how symmetries were taken into account in this study, and what to take away from this to improve the Bayesian treatment of neural networks. It appears to me that the symmetries in the neural networks studied here were left untreated, and no interventions were made to reduce the complexity of the Bayesian posterior through these symmetries.\n- For section 5.2, what is the insight gained from the study of weight permutation? Is \u201ca variation in the \u03a0s\u201d somehow problematic? How does this inform the design of Bayesian deep learning methods?\n\nTypo/nits:\n- \u201cnon-functionnally\u201d\u00a0 (bottom page 2)\n- \u201cinfluencial\u201d (start section 3)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission61/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741805284,
        "cdate": 1698741805284,
        "tmdate": 1699635930762,
        "mdate": 1699635930762,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZKrqX2p4ZM",
        "forum": "FOSBQuXgAq",
        "replyto": "FOSBQuXgAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission61/Reviewer_Mjnv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission61/Reviewer_Mjnv"
        ],
        "content": {
            "summary": {
                "value": "This work explores Bayesian neural networks and how their geometry is affected by weight space symmetries such as permutations and re-scaling. The authors show that the posterior can be expressed as a mixture over such symmetries, highlighting the redundant structure in such distributions. The authors then explore the effect of the scaling symmetry in more detail, showing that even with weight decay, the symmetry persists. Finally, using the gained insights, a rich exploration of different approximation methods is performed, and several metrics are studied to assess the quality of the resulting posterior. Multi-modal approximations generally offer the closest approximation in terms of maximum-mean discrepancy and best test performance, while surprisingly, single-mode approximations offer the best uncertainty predictions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Connecting Bayesian neural networks with the recently obtained insights into the symmetries of the loss landscape of standard neural networks is a timely contribution and highly relevant to the field. The work is very well-written and mostly easy to follow. \n2. The experiments are impressive and on a very large-scale, which is often missing in the BNN literature. I especially appreciate the different datasets investigated in this work. I also find the separation into single and multi-mode approximations very interesting, highlighting once more that deep neural networks really possess the multi-modal structure that they\u2019re typically associated with.\n3. I also appreciate the more in-depth investigation of the scaling symmetry, something that usually receives less attention compared to the more prominent permutation symmetry. I find it surprising that this symmetry persists to a good degree, even if weight decay is employed."
            },
            "weaknesses": {
                "value": "1. I think the first part of the paper regarding the symmetries is a bit extensive and insights from it are somewhat limited. To the best of my knowledge, it has been known for a while that the BNN posterior effectively consists of a mixture over such symmetries, so I don\u2019t see much novelty in equation (8). While it sets up the subsequent discussion very nicely, I wouldn\u2019t advertise this as one of the core contributions of this work. The in-depth study of the role of the scaling symmetry on the other hand is novel to my knowledge and fosters a better understanding.\n2. An important aspect of the experimental setup, which is however almost not discussed at all, is the choice of \u201cgold standard\u201d for the posterior, to which all other approximations are compared against. This choice is of course crucial for the empirical evaluation! If I understand correctly, the authors chose a deep ensemble consisting of 1000 members. I\u2019m a bit unsure regarding this choice, as it is quite controversial to what degree deep ensembles really implement an approximation to the Bayesian predictive distribution. There are several works arguing for yes [1, 2], while others oppose this view (to some degree) [3, 4]. [4] even argues that repulsive terms are needed to ensure that deep ensembles perform proper Bayesian inference asymptotically. I don\u2019t think that this questions the validity of the experimental setup too strongly, but a proper discussion of this choice and differing view points **have to be discussed** such that readers can make up their own mind regarding this discussion.\n3. While [2] is cited, the work really lacks a comparison to it. [2] also performs an empirical exploration of Bayesian posteriors and how other approximate methods perform, where the gold standard is full batch HMC. They only evaluate on one dataset (CIFAR10) but I still think this should be compared carefully, and how this work distinguishes itself from [2] should be properly stated. [2] also published their HMC checkpoints, I would really be interested in seeing whether the produced deep ensemble in this work produces a similar posterior. This could also clarify the discussion raised in point 2. \n4. The work is not very careful at introducing a lot of concepts formally. As maximum-mean-discrepancy comparisons are one of the core contributions of this work, I think it would be worth it to formally introduce it in the main text. You also report a metric called NS, which is only described in the caption of Table 1, and I could not find a description in the main text nor a pointer to the Appendix, detailing how the \u201cremoval of symmetries\u201d is actually performed. I find this quite an interesting point and I would like to see more discussion regarding how this removal enables a better comparison between different posteriors. Glancing at the numbers, there seems to be no consistent pattern (i.e. MMD sometimes goes up, and sometimes down after removing the symmetries) which is surprising as I would have expected to always see a decrease. Could you elaborate more on this? The multi-modal extension of some of these algorithms is also not clear. Are you simply averaging over all the samples coming from the different modes?\n5. The uncertainty estimation results are surprising as the single-mode posteriors consistently perform better. The employed measure ECE has seen a lot of criticism recently [5] so I\u2019m wondering whether a similar problem is at the origin here. Comparing multi-modal and single-modal is maybe also a bit unfair, as the testing accuracies vastly differ.\n\n\n[1] Wilson and Izmailov, Bayesian Deep Learning and a Probabilistic Perspective of Generalization\n\n[2] Izmailov et al., What Are Bayesian Neural Network Posteriors Really Like?\n\n[3] D\u2019Angelo and Fortuin, Repulsive Deep Ensembles are Bayesian\n\n[4] He et al., Bayesian Deep Ensembles via the Neural Tangent Kernel\n\n[5] Ashukha et al., Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning"
            },
            "questions": {
                "value": "1. Why is your MNIST performance so low? As far as I know even a simple MLP with 2 hidden layers can easily reach $\\approx 97\\%$ performance.\n2. Are you employing data augmentation in these experiments? And relatedly, are you tempering the posteriors, i.e. performing a grid search over temperatures? I\u2019m asking since the cold posterior effect [6] could be a potential confounder here for test performance, as different approximations might be influenced differently.\n\n\n[6] Wenzel et al., How Good is the Bayes Posterior in Deep Neural Networks Really?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission61/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698752829013,
        "cdate": 1698752829013,
        "tmdate": 1699635930670,
        "mdate": 1699635930670,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pikIwWJjAF",
        "forum": "FOSBQuXgAq",
        "replyto": "FOSBQuXgAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission61/Reviewer_QYDw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission61/Reviewer_QYDw"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a study of weight-space symmetries in Bayesian neural networks (BNNs). The authors propose mathematical definitions of permutation and scaling symmetries and define the BNN posterior as a mixture of these. They further present a new way to define a unique network among all scaling-symmetrical candidates. \n\nThe paper further contains an empirical study of BNN posteriors. Here, the authors evaluate a selection of posterior approximation methods for their approximation quality on classification and OOD detection tasks, study the functional diversity among ensemble members, as well as investigate how often the BNN parameters permute during training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I find the mathematical treatment of the scaling and permutation symmetries, as well as their contribution to the posterior, quite exciting. It may not be overly novel, but the exposition is interesting and inspiring.\n2. The empirical analysis is exciting. The study of the different posterior approximations provides a good overview of the effects of the approximations, and it feels like there's even more to be gained from Table 1 than what the authors discuss. Furthermore, section 5 appears quite novel and contains some fascinating insights.\n3. The authors promise to release a dataset of estimated posteriors for a large number of models, which could be of great value to the community.\n4. The paper overall appears original and of high technical quality and should be of great interest to the ICLR community."
            },
            "weaknesses": {
                "value": "1. The paper seems to lack a specific focus. While the theoretical and experimental parts of the paper are both exciting, they seem largely disconnected. The developed mathematical formalism does not appear to be used for anything, and it is even unclear what these definitions buy us in terms of understanding BNN posteriors. The experiments in section 4 do not appear to consider symmetries at all, and whereas those of section 5 do, the link to the definitions in section 3 is unclear. To me, the paper seems to be compiled from two distinct papers, each of which would be interesting to read.\n2. The clarity of the paper could be improved. The figures (1 and 2 in particular) are not explained in sufficient detail to understand what they show, and the text itself is sometimes unclear."
            },
            "questions": {
                "value": "1. What is the link between the formalism of section 3 and the experiments in sections 4 and 5?\n2. I don't fully understand figure 2. What is on the second axis in the left plot, what does the middle plot show, and what is the difference between \"opt.\" and \"convergence\" in the right plot?\n3. In section 4.1, you write \"For better efficacity, MMDs are computed on RKHS that allow for comparing all of their moments.\" What does this mean?\n4. If I understand the paper correctly, the estimates of the BNN posteriors come from the parameters of a 1000-member ensemble trained using maximum likelihood. Can we be sure this is a faithful representation of the posterior? For instance, do we know anything about the higher-order moments of those modes?\n5. Related to the previous question, if the posterior is indeed approximated by 1000 ML solutions, does the MMD metric even make sense for any of the single-mode methods? One could argue that single-mode methods aren't meant to describe the posterior adequately, but the hope is that their predictive distributions aren't too affected by the single-mode limitation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission61/Reviewer_QYDw"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission61/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699200901543,
        "cdate": 1699200901543,
        "tmdate": 1700655727659,
        "mdate": 1700655727659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A83hkkkQGW",
        "forum": "FOSBQuXgAq",
        "replyto": "FOSBQuXgAq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission61/Reviewer_cm11"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission61/Reviewer_cm11"
        ],
        "content": {
            "summary": {
                "value": "this paper conducts a large-scale exploration of the posterior distribution of deep bayesian neural networks (bnn), focusing on real-world vision tasks and architectures. it highlights the critical role of weight-space symmetries, particularly permutation and scaling symmetries, in understanding the bayesian posterior. the paper introduces new mathematical formalisms to elucidate these symmetries' impacts on the posterior and uncertainty estimation. it evaluates various methods for estimating the posterior distribution, using maximum mean discrepancy (mmd) to assess their performance in capturing uncertainty. a significant contribution is the release of a large-scale dataset comprising thousands of models across various computer vision tasks, facilitating further research in the field of uncertainty in deep learning. the study investigates the proliferation of modes due to posterior symmetries and the tendency of ensembles to converge towards non-functionally equivalent modes. it also discusses the influence of symmetries during the training process. the findings suggest the complexity of bayesian posterior can be attributed to the non-identifiability of modern neural networks, viewing the posterior as a mixture of permuted distributions\u200b."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. originality in symmetry analysis: the paper's exploration of weight-space symmetries, particularly permutation and scaling symmetries, in deep bayesian neural networks is highly original. it builds upon existing studies of dnn loss landscapes, like those by li et al. (2018) and fort & jastrzebski (2019), and extends these concepts to the bayesian context. the distinction between permutation and scaling symmetries and their unique impacts on the posterior is a novel contribution (section 1)\u200b\u200b.\n\n1. comprehensive methodological approach: the authors leverage a mathematical formalism to highlight symmetries' impacts on the posterior and uncertainty estimation in deep neural networks, adding theoretical depth (section 1). it facilitates a more nuanced understanding of the bayesian posterior, bridging a gap in current literature\u200b\u200b.\n\n1. empirical validation with a large-scale dataset: the release of a comprehensive dataset including the weights of thousands of models is a substantial contribution. it not only validates the paper's findings but also enables the community to conduct further research and empirical validation across various computer vision tasks (section 4). this dataset is a practical resource that can catalyze future advancements in uncertainty quantification in deep learning\u200b\u200b.\n\n1. insights into posterior symmetries and model performance: the paper delves into the complexity of bayesian posteriors, attributed to the non-identifiability of modern neural networks. this perspective, viewing the posterior as a mixture of permuted distributions, is a significant insight. it aligns with recent works that discuss the intricacies of dnn architectures and their implications on model performance (e.g., entezari et al. (2022))\u200b\u200b.\n\n1. advancing uncertainty quantification: the study's focus on uncertainty quantification, specifically how symmetries affect uncertainty estimation, is a major strength. the use of maximum mean discrepancy (mmd) to evaluate the quality of various posterior estimation methods on real-world applications is a rigorous approach, contributing to a better understanding of uncertainty in deep learning (section 4)\u200b\u200b.\n\np.s. really nice paper! was a joy to read."
            },
            "weaknesses": {
                "value": "1. limited scope in empirical validation: while the dataset released is extensive, the paper's empirical validation primarily focuses on vision tasks and specific architectures like resnet-18. this raises questions about the generalizability of the findings across different types of neural network architectures and tasks. expanding the empirical validation to include a broader range of architectures would strengthen the findings.\n\n1. need for further exploration of symmetries in training: the paper discusses the influence of symmetries during the training process but does not delve deeply into this aspect. further exploration of how these symmetries manifest and can be leveraged or mitigated during the training process could provide valuable practical insights for model development.\n\n1. lack of broader contextualization: while the paper does an excellent job of situating its contributions within the immediate field of bayesian neural networks, it could benefit from a broader contextualization. this includes discussing its findings in relation to other approaches in deep learning and uncertainty quantification, and how these approaches diverge from the paper\u2019s findings."
            },
            "questions": {
                "value": "1. regarding originality in symmetry analysis: the paper provides an original analysis of weight-space symmetries in bayesian neural networks. can you elaborate on how these findings might extend or differ from the insights provided by existing studies on dnn loss landscapes, such as those by li et al. (2018) and fort & jastrzebski (2019)? specifically, how do these symmetries manifest differently in bayesian networks compared to traditional dnns?\n\n2. on comprehensive methodological approach: could you provide practical examples or case studies demonstrating how your results can be applied to actual neural network design or optimization problems?\n\n3. regarding empirical validation with a large-scale dataset: the release of a dataset including thousands of model weights is a significant contribution. however, the focus seems to be on vision tasks primarily. how might the findings and applicability of your research differ when applied to other types of neural network architectures or non-vision tasks?\n\n4. on insights into posterior symmetries and model performance: your paper suggests the complexity of bayesian posteriors can be attributed to the non-identifiability of modern neural networks. how might this insight impact the current practices in model evaluation, particularly in fields where high certainty and reliability are crucial?\n\n5. regarding advancing uncertainty quantification: could you discuss the limitations or challenges you faced using maximum mean discrepancy (mmd) to evaluate posterior estimation methods? are there scenarios where mmd might not be an effective measure, and if so, what alternatives would you recommend?\n\n6. on limited scope in empirical validation: given the paper's focus on specific architectures and vision tasks, are there plans to extend this research to include a wider variety of neural network types and tasks? how do you foresee the findings changing with different architectures or in different application domains?\n\n8. on need for further exploration of symmetries in training: the paper touches upon the influence of symmetries during the training process. could you detail any specific strategies or methodologies that could be employed during training to either leverage or mitigate these symmetries?\n\n9. regarding lack of broader contextualization: how do the findings of your paper align or contrast with other approaches in deep learning and uncertainty quantification? can you provide insights into how your research contributes to or diverges from the broader landscape of deep learning research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission61/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699620109534,
        "cdate": 1699620109534,
        "tmdate": 1699635930367,
        "mdate": 1699635930367,
        "license": "CC BY 4.0",
        "version": 2
    }
]