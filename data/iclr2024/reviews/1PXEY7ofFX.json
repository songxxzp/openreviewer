[
    {
        "id": "mlCyrsSItR",
        "forum": "1PXEY7ofFX",
        "replyto": "1PXEY7ofFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_ovsR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_ovsR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for creating custom ODE solvers tailored to the ODE of a given pre-trained flow model. The framework is efficient in terms of additional parameters and training times and can produce high-quality images with a low number of function evaluations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method is sound and demonstrates good results in fast sampling. \n- The training time is short, and the number of parameters is low."
            },
            "weaknesses": {
                "value": "**Major concerns**\n- Both the quality and the speed of sampling (the number of function evaluations) of the proposal are not as competitive as distillation methods.\n- Karras et al., 2022 [1], propose a time discretization method to determine $t_i$, achieving an FID of 1.97 with unconditional image generation on CIFAR10 when NFE = 35. Your proposed method also seeks the sequence of $t_i$, so I strongly compare it with Karras's method.\n- Minor Point: There is still room for improvement in the presentation of the paper, especially in notations and function definitions in Section 2.1. It takes me a considerable amount of time to understand."
            },
            "questions": {
                "value": "- Is proposing a learnable transformed path instead of fixed and cherry-picking ones, as in previous works, the main contribution of the paper?\n- Does increasing NFE to 35 boost the performance of your method to be comparable to EDM in [1]? More generally, what happens to your method's performance when NFE is larger than 20? The same questions apply in the case of FID < 10.\n- Can your method possibly be applied to solve Stochastic Differential Equations (SDE) directly?\n\n**References**\n\n[1]  Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. \"Elucidating the design space of diffusion-based generative models.\" Advances in Neural Information Processing Systems, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Reviewer_ovsR",
                    "ICLR.cc/2024/Conference/Submission2537/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2537/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593354657,
        "cdate": 1698593354657,
        "tmdate": 1700712147319,
        "mdate": 1700712147319,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4TIz0yE5Rg",
        "forum": "1PXEY7ofFX",
        "replyto": "1PXEY7ofFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_KhFN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_KhFN"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a distillation approach diffusion models that is based on ODE solvers. The family of Runge-Kutta ODE integrators is used as a basis for performing a \"time integration\" for the diffusion time with learned coefficients for the different time steps. The number of time steps is chosen quite low such that correspondingly a low number of function evaluations (NFEs) of the pre-trained diffusion model is needed. A small network is trained that parametrizes the time integrator, with the benefit of a small number of weights and fater training.\n\nThe NFE count is used as a central \"dimension\" along which the RMSE and FIG performance of different versions is evaluated given a certain number of NFEs. The approach seems to achieve a very good performance with relatively low NFE and fast training. \n\nThe shown samples are close to the \"GT\" versions (with larger NFEs from the original model), and seem to perform better than running an RK-integrator directly on the pre-trained model. The images are typically shown for 20,10 and 8 evaluations, and the versions of the proposed method usually show few variations, i.e. stay close to the GT version. This is of course raises the question how even fewer steps would perform, and when the visual difference would start to grow. This is unfortunately not (yet) demonstrated, but should be easy to add."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper targets an important area, the runtime of generative models from diffusion approaches. The distillation via a parametrized and learned ODE is a new idea as far as I can tell, and the results are convincing. The quantified performance of the models as well as the qualitative examples look very good to me.\n\nOverall, I find the results convincing, and would suggest an \"accept\" given my current understanding of the work. There are a few smaller open points below which I hope the authors can clarify in the rebuttal."
            },
            "weaknesses": {
                "value": "While the main approach with custom RK integration makes a sound impression, the loss from section 2.3 seems a bit weaker in comparison. It essentially seems to yield a weighting the the MSE terms with the M_i coefficients, and replacing the acutal Lipschitz constants with 1 everywhere seems ad-hoc. I was also missing an ablation showing how much the training benefits from this loss over a \"regular\" RMSE loss."
            },
            "questions": {
                "value": "1) I was surprised about the statement that best FID iterations are \"reported\", but best RMSE iterations is shown. So the results shown in figures 6,7,... are from a different model than the graphs and tables shown? Can the authors can explain why?\n\n2) The introduction claims a \"very small number of learnable\" parameters, but I didn't find a table listing the actual parameter counts of the original models and the additional parameters for the \"bespoke\" versions. Can the authors provide these?\n\n3) Can the authors re-run some cases of \"bespoke\" training with a regular RMSE loss, e.g., using GT values from a more finely sampled reference trajectory? This would highlight the influence of the loss.\n\n4) Can the authors show some results from models with fewer steps than 8? E.g., 6, 4 or 2 steps?\n\n----\n\nI think the authors have done a nice job addressing the remaining open questions in their rebuttal, and included a collection of additional background information and illustrative results. As such, in line with my previous score, I would recommend an accept for this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Reviewer_KhFN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2537/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698696776936,
        "cdate": 1698696776936,
        "tmdate": 1700749483350,
        "mdate": 1700749483350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uiGb6Hux26",
        "forum": "1PXEY7ofFX",
        "replyto": "1PXEY7ofFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_Zfm7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_Zfm7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the Bespoke solvers for generative flow models, which is customized to the pretrained flow models. To apply this, a model that returns a small number of parameters (<80) is introduced to learn the hyperparameters for the consistent ODE solvers. To define the parametric family of solvers, the transformed sampling path from an invertible transformation $\\varphi_r$ is first applied. \n\nFor transforming the sample paths, two components are applied: the time reparameterization $r\\to t_r$ and the invertible transformation $\\varphi_r$. This framework finally turns into the generalized noise scheduler for all possible diffusion models and flow models, by normalizing the inputs with the invertible transformation and denoting the timestep with the time parameterization function. This is realized by learning the hyperparameters that consists the scaling function $s(t)$ and time parameterization function $t_r(t)$.\n\nNext, the tractable RMSE loss that computes the global truncation error between the approximate sample and the ground truth (GT) sample is enabled: the upper bound of the RMSE loss is formulated by the weighted sum of the local truncation errors, weighted with the Lipschitz constants. By minimizing this RMSE loss, one can reduce the gap between the approximate sample (given by the solver) and the ground truth (given by the oracle ODE solver)."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "(1) This method solves the important problem in the diffusion/flow models, the time-step and input scaling problem by learning hyperpamareters of the pretrained model ubiquitously, by optimizing the time steps and the input scaling with just some data-driven optimization with the pretrained model. If trained properly, and the bound between RMSE loss and the global truncation loss is rightly narrowed, then sampling from this learning-base parameterization derives good sampling results, as the paper proposes.\n\n(2) The writing is compact and sound; one can easily understand why the learning-based parameterization is required and how this benefits the sampling process of the diffusion/flow models, with abundant experimental results and theoretical supporting materials.\n\n(3) According to Table 3, this method requires much less training time of the hyperparameters (about 0.5~2% of the original training time) compared to the existing distillation-based method for fast sampling."
            },
            "weaknesses": {
                "value": "(1) Even though the RMSE objective is upper bounded by the Lipschitz loss, there can be some gap between these two loss; unless the loss converges to zero.\n\n(2) There results are not yet validated with larger-scale (than size $64\\times 64$) datasets, like FFHQ or LSUN."
            },
            "questions": {
                "value": "(1) If this bespoke solver is trained well with the higher-order solver, it is curious about the optimal required order of the solver with the learning-based coefficients: this means, if the higher-order solver is not required, the higher-order coefficient $t_r ''$ (if exists) or $s_r ''$ is expected to have low value, which is nearer to zero.\n\n(2) Is there any ablation with the $s_i$-only or $t_i$-only cases? (In this cases, $t_i$ is uniform (or EDM-like) or $s_i$ follows the VP, VE, or EDM preconditioning...) This additional will help understanding the advantages of training these hyperparameters."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Reviewer_Zfm7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2537/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699109063572,
        "cdate": 1699109063572,
        "tmdate": 1699636190114,
        "mdate": 1699636190114,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eyPyy6pdGE",
        "forum": "1PXEY7ofFX",
        "replyto": "1PXEY7ofFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_7aEk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_7aEk"
        ],
        "content": {
            "summary": {
                "value": "This work introduces Bespoke Solvers, a method for efficiently sampling flow models by optimizing an instance-specific solver for the flow's particle ODE. Given a fixed budget of $n$ function evaluations, fitting a bespoke solver involves optimizing over a parametric family of $n$-point discretization schemes to minimize the average discretization error over new sample generations. To identify the relevant parametric family, the authors derive how a generic ODE solver changes under space- and time-reparametrization of the underlying particle trajectories. While these reparametrizations may be complicated functions of the underlying data, it is sufficient to know their values and derivatives _only_ at the $n$ discretization points, so that optimizing the bespoke solver only requires fitting $O(n)$ parameters. \n\nMinimizing the global discretization error directly may be computationally challenging due to the recursive dependence of the errors on previous time steps. Instead, the authors propose to minimize a weighted sum of one-step discretization errors, which can be computed at different times independently (and hence in parallel) and which is also an upper bound on the discretization error when the weights are chosen appropriately. The proposed method is shown to outperform existing dedicated solvers and it is shown to be competitive with distillation-based methods (ex. Ho and Salimans 2022) while requiring substantially less training time and fewer parameters."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Clarity: the proposed method is elegant and it is well-explained in the paper. The derivations are correct to the best of my knowledge.\n- Highly practical and effective: the method is also cheap to train, adaptable to many existing architectures, and it leads to significant benefits for simulating flow-based image samplers. The experiments in Tables 1 and 2 are especially convincing"
            },
            "weaknesses": {
                "value": "- No tunability for NFE: one minor weakness of this approach is that training the bespoke solver requires choosing up-front the number of function evaluations to be used in sampling. In contrast, non-instance dependent schemes can adjust to different NFE budgets at sample time, or they can be run 'until convergence' (choosing NFE adaptively for each particle trajectory)."
            },
            "questions": {
                "value": "- Do you have any understanding of the limiting behavior of the bespoke solver parameters as NFE is increasing? In Figure 18, the parameters seem to be converging to a nontrivial limit as NFE increases. This is related to my comment in 'Weaknesses,' since one way to tune NFE could be to identify the limit of the solver parameters and to discretize it 'on demand' for different choices of NFE. \n- The scale-time parametrization seems like an arbitrary choice. Are there any benefits to using more complicated parametrizations? For example, did you run any experiments with time-dependent affine transformations (eg. adding an additive bias parameter to equation 14) or with higher order polynomials?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2537/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699248904993,
        "cdate": 1699248904993,
        "tmdate": 1699636190035,
        "mdate": 1699636190035,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fSCP4KSr7V",
        "forum": "1PXEY7ofFX",
        "replyto": "1PXEY7ofFX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_vykk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2537/Reviewer_vykk"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the issue of slow sampling in Diffusion models. The authors consider deterministic sampling of Diffusion models via the Probability Flow ODE. They address the issue by introducing a learnable (continuous) reparameterization of the ODE. Rather than learning the continuous parameterization directly, they discretize the reparameterized ODE using Runge--Kutta methods, which allows for learning only a discrete set of parameters (e.g. $4n-1$ parameters for Euler's method, where $n$ is the number of steps in the discretization). The parameters can then be learned by minimizing the discrepancy of the learned Runge--Kutta discretization to ground truth trajectories of the Diffusion model (which are in practice simulated using a higher-order adaptive ODE solver). The authors claim that the training time of the reparameterized ODE is roughly 1% of the training time of the original Diffusion model. The authors compare their method to the literature on Cifar-10 and ImageNet (64x64)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem of accelerating inference in Diffusion models is important\n- The structure of the paper is good, and the paper is generally well written (I am very happy that the authors opted to include Figure 2, and Algorithm boxes 2&3 in the main paper which help to understand the method)\n- The idea of learning parameters for a reparameterization of a Neural ODE is original (to the best of my knowledge)\n- The authors cover a lot of previous work (however I think some works are misrepresented, see also below)"
            },
            "weaknesses": {
                "value": "**Missing details**: I think some details of the training are missing. How many GT trajectories are used and is the time for computing GT trajectories accounted for when claiming that the method only needs \"roughly 1% of the GPU time\" compared to training of the Diffusion model? This seems like a very important detail.\n\n**Single guidance value training**: As far as I understand, the authors need to retrain fresh parameters for each guidance value (and compute GT trajectories for each guidance value). I think the authors should have addressed this as drawback of the method (compared to other methods). \n\n**Misrepresentation of Distillation approaches**: I think the authors are misrepresenting the work on Diffusion distillation. For example, they say \"Distillation does not guarantee sampling from the pre-trained model's distribution\" - while this is correct the same applies to the proposed method. In fact, the inference distribution of a Diffusion model is always coupled to a numerical discretization; any solver will result in a different distribution of the generated samples.\n\nIn general, I also think the authors should have tried to compare the performance of their approach to distillation. For example, they could have done Progressive / Consistency distillation using a fixed compute budget. Unfortunately, there are no comparisons at all.\n\n**Interpretation of Figures 17/18/19**: It is actually quite nice that the authors can visualize the learned parameters (since there are so few) but unfortunately the work is missing a discussion on the figures. I think it's quite interesting that $s_r \\approx 1$ for most experiments and that $t_r$ seems to increase linearly with $r$. Could the method for example be used to explain the success of the DDIM ODE reparameterization?"
            },
            "questions": {
                "value": "**Depth of Experiments**: Since running the method is quite cheap, it would have been nice to include larger values of $n$ (currently only up to $n=10$) and higher order methods (e.g. RK3). Is there any particular reason why this has not been done?\n\n**Limited Outlook**: I think the idea of repameterizing the ODE with learnable parameters is neat. How could this method potentially be scaled/improved? Are more elaborate reparameterizations possible? \n\n**Guidance scale(s)**: What guidance scale(s) is/are used in the experiments?\n\nAlso see some questions entangled with the Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2537/Reviewer_vykk"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2537/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699400526815,
        "cdate": 1699400526815,
        "tmdate": 1700951818369,
        "mdate": 1700951818369,
        "license": "CC BY 4.0",
        "version": 2
    }
]