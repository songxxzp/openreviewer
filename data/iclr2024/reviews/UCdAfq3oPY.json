[
    {
        "id": "oFa8vbQm1X",
        "forum": "UCdAfq3oPY",
        "replyto": "UCdAfq3oPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_a9u9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_a9u9"
        ],
        "content": {
            "summary": {
                "value": "This paper focused on contrastive learning methods in continual learning, and proposed to leverage the first few samples of the new task to identify and retain parameters contributing most to the transfer ability of the neural network, freeing up the remaining parts of the network to learn new features. The authors claimed that this idea is inspired from event models of the brain. The proposed method achieves some improvements on relatively simple dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is basically well-written and easy to follow.\n\n2. The idea of event models is interesting. It\u2019s good to see the connections between task boundaries and neurological mechanisms."
            },
            "weaknesses": {
                "value": "1. I agree that the current continual learning methods focus more on stability rather than plasticity/transfer. However, I think the technical contribution is incremental and not completely novel. The proposed method can be seen as an improved version of Co$^2$L. Also, the idea of \u201clook-ahead\u201d new tasks has been widely discussed in recent literature, such as learning and combing the new task solution [1] [2]. These related work should be discussed and compared (at least conceptually).\n\n2. The proposed method can only achieve marginal improvements over Co$^2$L, especially for TinyImageNet in Table 1. Also, the considered benchmarks are relatively simple in continual learning.\n\n3. The ablation study is not very clear, and the performance differences are marginal between each baseline in Table 2 (considering the error bars).\n\n[1] Afec: Active forgetting of negative transfer in continual learning. NeurIPS 2021.\n\n[2] Towards better plasticity-stability trade-off in incremental learning: A simple linear connector. CVPR 2022."
            },
            "questions": {
                "value": "Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698544109355,
        "cdate": 1698544109355,
        "tmdate": 1699636151027,
        "mdate": 1699636151027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SSFX1w5pJE",
        "forum": "UCdAfq3oPY",
        "replyto": "UCdAfq3oPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_ecgj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_ecgj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new method for continual learning which is built on top of the existing work Co2L [1]. The author leverages the first batch of the new data as a surrogate to estimate the crucial parameters of the old model which are beneficial for new tasks and also important for old tasks. The estimation is done by searching for a set of embedding that can be salient for evaluating the above criteria, and the author adapts the existing Neural Similarity Learning [2] to identify these subsets. Then the author uses the Excitation Backprop (EB) to calculate the salience of each network weight and then uses the weight to mask the distillation loss for training the model to mitigate forgetting. The author also proposed a gradient modulation to modify the gradients. Extensive experiments are conducted on standard continual learning benchmarks.\n\n\nReference:\n[1] Co2l: Contrastive continual learning (ICCV 2021)\n[2] Neural similarity learning (NeurIPS 2019)\n[3] Top-down neural attention by excitation backdrop (IJCV 2018)"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Overall, the paper is easy to follow. Using the look-ahead idea to estimate the importance of the model weight seems to be interesting. \n2. The author provides many experiments and analyses to valid and reason about the proposed method."
            },
            "weaknesses": {
                "value": "1. The look-ahead idea is not totally new in continual learning. The author did not discuss the relationship between seminal work like \"La-MAML: Look-ahead Meta-Learning for Continual Learning\" (NeurIPS 2020) and the present work, where the La-MAML has already considered using the initial batch of data to adapt the gradient for continual learning, which in general is related to the author's proposed masked distillation training and gradient modulation. \n\n2. It is unclear why the paper needs to start with contrastive continual learning, i.e., Co2L, as the starting point for developing the method. First, since Co2L was published in 2021, there are so many continual learning methods that do not use contrastive learning and still achieve state-of-the-art (SOTA) performance. What is the necessity of using Co2L as the learning objective? Is it because the proposed method can not work without Co2L?\n\nMoreover, the author stated in Page 4 that:\n\n\"We believe that this distillation loss is too limiting and diminishes the model\u2019s ability to learn new generalizable representations since redundant parts of the embeddings are also regularized.\"\n\nCould the author provide an explicit, formal, and/or empirical analysis about why the distillation loss will have such drawbacks? Such a claim is not sound, especially when we check the results in Table 1 that the proposed method does not significantly outperform the Co2L and the Co2L even outperforms the proposed method on SplitImageNet and R-MNIST. It is hard to convince the reader that the issue mentioned by the author for Co2L is grounded.\n\n3. The author proposed to calculate the salient estimation for each parameter and use the ResNet-18 and two-layer linear network for experiments. How will the computation complexity for this salient estimation be on Page 6? Will there be a computational bottleneck when the proposed method is applied to modern neural network architecture like ViT and Transformer?\n\n4. The CL methods compared in the present paper are up to 2021, while there are lots of new CL methods proposed after 2021 and the author did not review them in the paper and did not even mention why the author did not choose them for comparison. Moreover, the proposed method does not even significantly outperform the Co2L, i.e., the baseline they have chosen for developing their method. All of this largely hampers the significance of the current paper.\n\n5. Although the related work section is not required, the reviewer still suggests the author to have a related work section to comprehensively review the existing CL methods especially at least discuss the recent advance of CL methods after 2021, instead of have a lengthy Introduction section which may largely distract the attention for a reader."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2177/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2177/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2177/Reviewer_ecgj"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698642706416,
        "cdate": 1698642706416,
        "tmdate": 1699636150954,
        "mdate": 1699636150954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "D6rWUDLKvX",
        "forum": "UCdAfq3oPY",
        "replyto": "UCdAfq3oPY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_dRbC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2177/Reviewer_dRbC"
        ],
        "content": {
            "summary": {
                "value": "Paper proposes method for relevant neuron selection within a self-supervised continual learning framework; their starting point is the Co2L method. The authors propose to learn a set neurons that are relevant for the current task performance (they compare several strategies). This learned set of representation-dimensions are then used to perform a masked instance-wise relation distillation loss. Results on a few datasets shows the method improves Co2L."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the argument that the proposed method improves the potential plasticity by reducing regularization on redundant dimensions of methods is nice. Measuring this on the current task data is new (rather on the previous)\n\n- the proposed method obtains decent performance gain for small memory size especially on CIFAR10."
            },
            "weaknesses": {
                "value": "- the idea to focus on the importance of neurons for future (or current) tasks is new, many methods aim to measure the importance of neurons for previous tasks. However, the final difference between these strategies is very small (see table 2), and in my opinion too small. \n\n- I do not really like CIFAR 10 for continual learning since the tasks are really small. I would like to also see results on CIFAR100 and if possible on ImageNet-subset. \n\n - more results on the subset size should be added."
            },
            "questions": {
                "value": "Please address the weaknesses. \n\nFor me in Table 2, the gain with respect to CO2L are ok, but not very large, and I would really like to at least also see it on CIFAR100 /10 split. Table 2 shows that selection can work; however, it also shows that any selection works and that the results among the various ways of selecting are very small (the 'look-ahead' does not seem crucial). \n\nminor:\n- I would consider removing GM from the paper since it does not improve results. \n- I'm not sure if the term 'salient' is very adequate to refer to the selected neurons.\n-  number of tasks used per dataset should be clearly stated in the main paper.\n- add in table 2 without using the selection as well (I think it helps, even though the numbers are Table 1)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2177/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684404479,
        "cdate": 1698684404479,
        "tmdate": 1699636150874,
        "mdate": 1699636150874,
        "license": "CC BY 4.0",
        "version": 2
    }
]