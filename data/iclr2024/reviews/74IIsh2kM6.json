[
    {
        "id": "xAcOBQUT5S",
        "forum": "74IIsh2kM6",
        "replyto": "74IIsh2kM6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_reNT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_reNT"
        ],
        "content": {
            "summary": {
                "value": "A transformer-based model is trained to perform audiovisual speech recognition, where fused features are learned by masking audio/visual tokens and then learning to reconstruct the masked tokens in a Siamese architecture.  The fused multimodal features from different network layers are then dynamically fused using adaptive weighting, before being passed to the decoder network for recognition."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The work is based on open-datasets, and authors propose to release their code and pre-trained models.  This all enables reproducibility.\n- Although computing a weighted fusion of features from different network layers already has been shown to be effective in ASR, the utility is further demonstrated here using multimodal features rather than acoustic-only features.\n- The writing quality is good, and the paper is well structured."
            },
            "weaknesses": {
                "value": "- It is not clear to me how much the visual information contributes to the WER.  The WERs reported here are on par with state of the art acoustic-only speech recognition.  Why not include an ablation of the modalities \u2014 show results for audio-only, video-only and audiovisual.\n- Also, the motivation for the work is that the visual modality helps in the presence of acoustic noise.  There are no experiments that actually demonstrate this, or quantify the effective gain in SNR provided by the visual information.\n- The paper describes the problem of AVSR as an \u201cemerging research area\u201d, but this ignores the 30+ years of prior research into multimodal speech recognition."
            },
            "questions": {
                "value": "I find the introductory paragraph that motivates multimodal speech recognition to be unclear.  The authors state that \u201c... speech contains detailed phonetic information, while lip movement conveys emphasis or stress ...\u201d.  Phonetic information is conveyed visually too \u2014 for example, the place of articulation for bilabial, labiodental, and dental sounds can be seen.  Furthermore, emphasis and stress are both also conveyed acoustically.  The main benefit of multimodal speech is that when acoustic noise masks the differences between two sounds so that they sound similar, those sounds can be disambiguated visually.  Similar sounding speech events look different, and similar looking speech events sound different from one another.  This is the complementary nature of audiovisual speech.\n\nThe authors refer to \u201cThe original Siamese networks\u201d and cite papers from circa 2020, yet Siamese networks pre-dated these works by a couple of years.\n\nEquation (7) and the accompanying explanation feels a little redundant as this already is clear from Equation (6).\n\nIn Equation (9), S is shared across all layers, which weighs the contribution of the visual information.  What about using a dynamic, layer-dependent weight (as in Equation (10)) since the contribution of the visual information for the speech recognition task might be different for different layers (as you already note for the acoustic information:  \u201c... the shallower layers in audio models tend to prioritize speaker-related information, while the deeper layers tend to emphasize content-related information.\u201d.  The same might be true for visual features, and allowing the visual contribution to \\hat{F}^{I}_{av} to vary might provide more robust features.\n\nIn Section 4.2 you refer to extracting the per-frame ROI and then applying a horizontal flip with probability 0.5.  To be clear, this flip is not applied per frame, but to the whole sequence?\n\nFor Table 3(b), why were these four combinations of masking rate for the audio and visual selected?  What about the other combinations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698255086820,
        "cdate": 1698255086820,
        "tmdate": 1699637123870,
        "mdate": 1699637123870,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TX7CKZx19F",
        "forum": "74IIsh2kM6",
        "replyto": "74IIsh2kM6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_brXn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_brXn"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes methods to improve the performance of AVSR systems. The authors propose Simaese Masked Interaction Learning (SMILE) and adaptive multimodal fusion, which enables early fusion and representation alignment methods between the modalities through transformer layers and mask learning mechanisms. This facilitates single-modal interaction, cross-modal interaction and global modal interaction simultaenously. The authors perform AVSR experiments on LRS2 and LRS3 datasets on which they demonstrate performance exceeding existing work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The design choices are well-motivated, suitable for the problem of AVSR.\n- The proposed model is trained on both low and high-resource settings, and show state-of-the-art performance on both.\n- Ablations in Table 2 demonstrate that all of the design choices contribute positively to performance."
            },
            "weaknesses": {
                "value": "- Although the results are better than the previous works, the improvement is marginal and appears to be saturated.\n- The authors state that AVSR \"is primarily desgined to address ... high levels of noise and speech occlusion\" but the paper does not address such scenarios.\n- Related to above points, the LRS2/3 dataset was collected using an automated pipeline using ASR. This means that the test set is biased towards examples that are easy for speech recognition systems -- the speech-only model in the data curation pipeline can achieve 100% accuracy (Sec 4 of 1809.02108 and Sec 2.1 of 1809.00496). This undermines the usefulness of this dataset for AVSR under clean conditions.\n- The authors cite 2020 and later papers for original Siamese network, but the term was proposed much before by (Koch et al., 2015) for contrastive learning."
            },
            "questions": {
                "value": "- Does the model work in missing modality conditions?\n- What is the performance of audio-only ASR model trained using the same pretraining/finetuning data setup? \n- How sensitive is the result to the values of alpha and beta."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624054582,
        "cdate": 1698624054582,
        "tmdate": 1699637123735,
        "mdate": 1699637123735,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8k5r7TOTCc",
        "forum": "74IIsh2kM6",
        "replyto": "74IIsh2kM6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_BvrV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_BvrV"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a model named Siamese Masked Interaction LEarning\n(SMILE), which combines the multimodal fusion and masked reconstruction for audio-visual speech recognition. SMILE model encodes both the original and masked features and is trained with a combination of cross-entropy loss and masked prediction loss. The model additionally combines the features from different transformer layers with a learnable weighted sum. The method is evaluated on LRS2 and LRS3 datasets and has outperformed several baselines in the two audio-visual speech recognition benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is overall well-written. The authors also conducted a thorough analysis to show the effectiveness of each component of the SMILE model.  The proposed method, including using masked reconstruction under supervised setting, is novel to audio-visual speech recognition, specifically. The ablation on training strategies of the Siamese network is also a potentially useful finding."
            },
            "weaknesses": {
                "value": "The proposed method falls behind state-of-the-art approaches. For example, [1] has achieved 0.9% WER in LRS3, which is better than the proposed model. In addition, the model is not evaluated under noisy setting, which is a typical use case of audio-visual speech recognition. In the broad context, the proposed method also lacks novelty. For example, the weighted combination of layerwise features is a widely used method in the SUPERB benchmark.\n\n[1]. Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, and Maja Pantic. Auto-AVSR: Audio-visual speech recognition with automatic labels."
            },
            "questions": {
                "value": "See strengths and weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806819217,
        "cdate": 1698806819217,
        "tmdate": 1699637123616,
        "mdate": 1699637123616,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4BedqieTKR",
        "forum": "74IIsh2kM6",
        "replyto": "74IIsh2kM6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_iBsX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8924/Reviewer_iBsX"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles the problem of audio-visual speech recognition, and puts the focus on the point of establishing temporal correspondence among audio and visual modalities. The main methods introduced here is the attention-based module in the Siamese framework and the dynamic layer-wise weighted fusion strategy in the learning process. Experiments on LRS2 and LRS3 show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The general structure is clear. The method is simple in general. It\u2019s easy to follow. The illustration of the motivation and advantages of using each module, e.g. reconstruction masked tokens, the mix-attention encoder, is clear."
            },
            "weaknesses": {
                "value": "Many involved modules and strategies in this work have been proposed in other works. The encoders, the Siamese framework with target setting not masks and student setting random masks, random masking for different modalities and the optimization functions have all been proposed in previous works. The manner to obtain proposed adaptive multimodal fusion by introducing weights on the visual modality and performing weighted sum over different layers is a little straightforward and common. I am not very clear about the novel contribution of this paper."
            },
            "questions": {
                "value": "In Table2, the effect of the proposed adaptive multimodal fusion (obtained with a weighted sum over different layers) seems not so appealing, and the slight improvement may come from the increase of extra learnable parameters? Have the authors checked the learned value of the weight S and wi? What are they like in the learning process?\n\nIn Table 3, the gap among different strategies can achieve as high as about 2%, which has already been significant if we compared it with the general gap of the final performance of the proposed work in Table1. This big gap, especially when compared with the improvement of the proposed modules here, states the importance of training strategies in the process, but these strategies are existing ones, not firstly proposed in this work. I think this may further weakens the contribution of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8924/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699242453666,
        "cdate": 1699242453666,
        "tmdate": 1699637123494,
        "mdate": 1699637123494,
        "license": "CC BY 4.0",
        "version": 2
    }
]