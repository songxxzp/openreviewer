[
    {
        "id": "ASP8rOd5Wr",
        "forum": "mlJLVigNHp",
        "replyto": "mlJLVigNHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_WhGn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_WhGn"
        ],
        "content": {
            "summary": {
                "value": "To enhance the efficiency and performance of retrieval-augmented language models, this paper introduces a simple and intuitive method for in-context improvement. Given the retrieved documents and input text, the proposed method first compresses the retrieved documents into a summary. Then, this summary is prepended to the input text, with the subsequent output response generated using a frozen language model. Two types of compressors are proposed: an extractive compressor, corresponding to extractive summarization, and an abstractive compressor, associated with abstractive summarization. Experimental results from both language modeling and question answering tasks demonstrate that the proposed compressors bolster the performance of retrieval-augmented language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is well-written and straightforward. Moreover, it offers an extensive, and well-designed experimental validation of the proposed method and presents thorough analyses."
            },
            "weaknesses": {
                "value": "Some concerns regarding this paper include:\n\n- The proposed method utilizes the knowledge of the LMs to train the compressor, while the baselines do not. Some baseline models, such as DRP and contriver, should be finetuned using the dataset employed to train the extractive compressor.\n\n- The paper employs a BM25 retriever, making the results of the proposed method and baselines reliant on the retriever's performance. Therefore, it would be appropriate for the paper to also show performances using an another retriever, such as contriver (not merely as a sentence selector). In addition, presenting the distribution of oracle-retrieved documents, similar to what's done in extractive compression, would be advantageous.\n\n- Relatedly, it is necessary to compare the time taken to generate text by prepending all retrieved documents without compression against the time taken using the proposed method\u2014specifically, compressing the retrieved documents before prepending them."
            },
            "questions": {
                "value": "Q1: During the manual evaluation, the authors assess the outputs of the abstractive compressors. What was the level of annotation agreement among the evaluators?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Reviewer_WhGn"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837966460,
        "cdate": 1698837966460,
        "tmdate": 1700736282851,
        "mdate": 1700736282851,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VCRiyaHGmC",
        "forum": "mlJLVigNHp",
        "replyto": "mlJLVigNHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_hWi7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_hWi7"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose RECOMP (Retrieve, Compress, Prepend) that incorporates extractive and abstractive summarization into the retrieve augmented language model to shorten the token length of retrieved text in prompts. Precisely, these summarization modules receive input text and its retrieved text and then extract or generate the summary for the retrieved text. The authors did not simply join the summarization modules into the language model as separate modules. Instead, they updated them through training to generate summaries relevant to the input text and its retrieved text. The experimental results show that RECOMP can shorten the prompt length while keeping performance in language modeling and open-domain QA tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors show the effectiveness of both extractive and abstractive summarization. It means we can use insights from commonly used summarization methods for improving retrieve augmented language models.\n- Since the summarization models are updated to fulfill the requirement of retrieving augmented language models, the proposed method is more than model combination and thus novel.\n- The experimental result shows the proposed method RECOMP can save the prompt length while keeping its performance."
            },
            "weaknesses": {
                "value": "- The benefit of shortening prompt length is uncertain. If the authors observe a speedup in inference,  they should report it.\n- The performance improvement is limited or nothing in many settings. The authors need to justify it."
            },
            "questions": {
                "value": "- In the current evaluation, the benefit of using summarization for the retrieve augmented language model needs to be clarified. If the inference speed is improved, the authors should report it.\n- Also, you need to report how much computational cost is decreased by RECOMP. This is related to the first question.\n- Considering the length limitation of the model, we can expect summarization to make the model consider more retrieved texts in its prompt. It may contribute to improving language modeling performance. Did you check such direction in your work?\n\nI will update my score based on the discussion with the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Reviewer_hWi7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698851443862,
        "cdate": 1698851443862,
        "tmdate": 1700737855754,
        "mdate": 1700737855754,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1HA8hJsaB6",
        "forum": "mlJLVigNHp",
        "replyto": "mlJLVigNHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_VoPU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_VoPU"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes RECOMP, an approach to improve retrieval-augmented language models (RALMs) by compressing retrieved documents into summaries before using them as context. The method uses two compression methods: an extractive model selecting relevant sentences, and an abstractive model generating summaries. The compressors are trained to optimize end-task performance when prepending the summary to the input. RECOMP is evaluated on language modeling and open-domain QA tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes a simple but clever idea and backs it up with a set of well-designed experiments. The baselines are well chosen."
            },
            "weaknesses": {
                "value": "Although the method is good, I\u2019m not sure how useful it is in practice. Normal retrieval augmentation without any compression outperformed the compression methods albeit with many more tokens. However, with the growing context lengths of models, it is not clear if the accuracy hit is worth the tokens saved. Secondly, you still have to provide the full tokens to the abstractive model."
            },
            "questions": {
                "value": "Can you give some results on how the method is compared to traditional RAG with no compression in terms of time? \nCan you provide more details on the input tokens for the abstractive model or is the same number of tokens as the noncompression baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Reviewer_VoPU"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8767/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699605484198,
        "cdate": 1699605484198,
        "tmdate": 1699637100726,
        "mdate": 1699637100726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OXc1E3vJBy",
        "forum": "mlJLVigNHp",
        "replyto": "mlJLVigNHp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_pQZH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8767/Reviewer_pQZH"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a method named RECOMP (Retrieve, Compress, Prepend) to compress the retrieved documents used in Retrieval-Augmented language models (RALMs). The proposed method aims to generate concise, effective and faithful summaries to improve the efficiency of RALMs. Two kinds of summarizers are used: an extractive summarization model for selecting informative sentences, and an abstractive summarization model for generating good query-focused summaries. Experiments are conducted on language modeling and question answering tasks, achieving low compression rates and some performance drops."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The presentation of the core idea is clear. The research topic of effective RAG is important.\n- The proposed method freezes the LMs and only trains the summarization model, which is interesting and beneficial to transfer across different LMs.\n- The analysis in Sec. 6 is sufficient and comprehensive."
            },
            "weaknesses": {
                "value": "My major concern is that the paper does not report the inference speedup by their method. Although experimental results show that fewer retrieved tokens are used, the number of tokens does not necessarily reflect the actual efficiency of the system, especially when the context length is not the bottleneck. The tokens of all the retrieved documents are still required to be processed by the compressor (a smaller model though). Therefore, it may be beneficial for authors to report the inference speedup of their method. This could be the core to support their motivations."
            },
            "questions": {
                "value": "- Q1: As stated in \"Weakness\", the inference speedup should be reported.\n- Q2: How about the training cost? The training of the abstractive compressor uses a teacher LM (GPT-3.5), which is expensive. I'm not sure if the cost saved in the inference stage is worth the cost in the training stage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8767/Reviewer_pQZH",
                    "ICLR.cc/2024/Conference/Submission8767/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8767/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699718606601,
        "cdate": 1699718606601,
        "tmdate": 1700624762800,
        "mdate": 1700624762800,
        "license": "CC BY 4.0",
        "version": 2
    }
]