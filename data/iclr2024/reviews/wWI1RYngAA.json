[
    {
        "id": "S4awHWRxRS",
        "forum": "wWI1RYngAA",
        "replyto": "wWI1RYngAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_nzb6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_nzb6"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method named ROAD, which leverages the CQL algorithm and focuses on efficient offline data replay during the online training phase of offline-to-online RL. A crucial component of ROAD is the utilization of UCB to balance exploration and exploitation across various mixing ratios. A weighting parameter, denoted as 'c', is assigned to the exploration term in UCB calculations. Lower weights encourage the algorithm to converge after a certain level of exploration, while higher weights facilitate more extensive exploration between mixing ratios. This method is straightforward and can be easily integrated with existing offline-to-online RL algorithms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The exploration of the offline-to-online problem in this study holds great relevance and is imperative for practical implementations, aligning seamlessly with the demands of real-world situations.\n2. The approach taken in this paper for addressing offline-to-online RL is quite promising. The study focuses on enhancing the performance of the online phase by investigating the ratio of offline to online data. This method of improving the data ratio is highly versatile and can be applied in conjunction with any offline-to-online RL algorithm. It holds valuable insights for the research field by providing a generalizable means of improving performance."
            },
            "weaknesses": {
                "value": "1. The ROAD method introduced in this paper aims to enhance the performance of offline-to-online RL algorithms from a data utilization perspective. However, as noted in related works, approaches like Balanced Replay[1] and APL[2] share similar starting points. Balanced Replay, for instance, independently trains a neural network to control the ratio of offline and online data, making it an essential baseline for evaluating the proposed method. Nonetheless, the experimental section lacks a comparison with Balanced Replay's performance. Therefore, it is recommended that the authors include this baseline in their evaluation.\n2. While the experimental section of this paper covers a variety of tasks, including AntMaze, Adroit, and Kitchen, it is important to note that there are only six specific datasets utilized. The limited coverage of dataset types in the experiments raises concerns about the overall persuasiveness of the results. For instance, questions arise as to why AntMaze only includes the \"large\" dataset and lacks the \"medium\" one, or why Adroit features \"door\" and \"relocate\" datasets but omits \"pen\" and \"hammer.\" The authors could choose to focus on specific tasks, but it is advisable to conduct experiments across all dataset types for a given task, rather than selecting a few datasets for showcasing results. Furthermore, MuJoCo is a widely recognized benchmark in the offline-to-online research domain. Conducting experiments on classic tasks like HalfCheetah, Walker2d, and Hopper can facilitate comparisons with traditional methods. Therefore, it is recommended that the authors consider including experiments on these tasks as part of their study.\n3. While the ROAD method is stated to be compatible with various offline-to-online RL algorithms, it is worth noting that the experimental section primarily focuses on Cal-QL, which is the sole offline-to-online RL algorithm tested. In contrast, CQL is a purely offline reinforcement learning algorithm. To demonstrate the versatility and applicability of the ROAD method, it may be beneficial for the authors to consider including experiments involving 1-2 other offline-to-online RL methods, such as AWAC[3], PEX[4], and similar approaches. Alternatively, the authors could replace the data utilization network in Balanced Replay with the ROAD method to make the ROAD results more compelling and indicative of its broader utility.\n\n[1] Lee S, Seo Y, Lee K, et al. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble[C]//Conference on Robot Learning. PMLR, 2022: 1702-1712.\n\n[2] Zheng H, Luo X, Wei P, et al. Adaptive policy learning for offline-to-online reinforcement learning[J]. arXiv preprint arXiv:2303.07693, 2023.\n\n[3] Nair A, Gupta A, Dalal M, et al. Awac: Accelerating online reinforcement learning with offline datasets[J]. arXiv preprint arXiv:2006.09359, 2020.\n\n[4] Zhang H, Xu W, Yu H. Policy Expansion for Bridging Offline-to-Online Reinforcement Learning[J]. arXiv preprint arXiv:2302.00935, 2023."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3208/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3208/Reviewer_nzb6",
                    "ICLR.cc/2024/Conference/Submission3208/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698719796037,
        "cdate": 1698719796037,
        "tmdate": 1700634213784,
        "mdate": 1700634213784,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A9IylGGKtw",
        "forum": "wWI1RYngAA",
        "replyto": "wWI1RYngAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_27rC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_27rC"
        ],
        "content": {
            "summary": {
                "value": "This paper considers the problem of using offline datasets to improve online RL. They propose an adaptive mixing ratio method where the training adaptively chooses a mixture of offline datasets and online data to train a model. The adaptivity is obtained via using bandits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of using bandits to adaptively learn a mixing balance seems novel and interesting."
            },
            "weaknesses": {
                "value": "* It is unclear to me why the reward function  $R[m]$ in Eq (2) makes sense to balance the mixing ratio. What is the interpretation of that reward signal? What would be an optimal mixing ratio? \n* It's unclear how this method would work. Is there any guarantee that this bandit design would lead to some notion of optimal mixing ratio between the offline dataset and the online data? \n* What is an optimal choice fo the exploration parameter $c$? \n* Though the idea is interesting, the framework is quite simple and not convincing enough how this idea would work (see above questions)"
            },
            "questions": {
                "value": "Please see my questions above in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800339303,
        "cdate": 1698800339303,
        "tmdate": 1699636268886,
        "mdate": 1699636268886,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hO5jA4GJMn",
        "forum": "wWI1RYngAA",
        "replyto": "wWI1RYngAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_jaY3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_jaY3"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses offline-to-online RL, where an RL agent is pre-trained with an offline dataset and then fine-tuned in an online environment. While most existing works use a fixed mixture ratio of offline and online data during this fine-tuning stage, this study proposes an adaptive adjustment of this ratio. The selection is driven by a bandit algorithm that responds to environmental returns."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The core idea is intuitive and appears to be effective, as supported by the empirical results.\n\n-  The empirical evaluation is comprehensive."
            },
            "weaknesses": {
                "value": "- The details of implementation are somewhat unclear (see the questions section).\n\n- While ROAD tunes exploration weight $c$, a comparison with fixed-ratio baseline with the optimal ratio $m^*$ would be informative, where the optimal ratio $m^*$ should be selected in a manner similar to selecting $c$.\n\n- Lacking comparison to Balanced Replay (BR) [1]: Given that both ROAD and BR aim to adjust the offline/online data ratio as replay buffer components, a comparison would offer valuable insights into ROAD's effectiveness.\n\n[1] Lee, Seunghyun, et al. \"Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.\" Conference on Robot Learning. PMLR, 2022."
            },
            "questions": {
                "value": "- In Figure 5, it seems the bandit learner never explores the arm with $m=0.5$. \n\n- What value of exploration weight parameter $c$ corresponds to the curves shown in Figure 4?\n\n- How frequently was $m$ updated?\n\n- Algorithm 1 appears to be different from standard practices where data collection and gradient updates are done simultaneously. I'd like to confirm with the authors the training process of ROAD. Is it accurate that ROAD proceeds by: (1) collecting $n$ trajectories (equivalent to $n*T$ environmental steps, with $T$ as the trajectory length), (2) updating the bandit learner and $m$ using these $n$ trajectories, and then (3) then running $k$ gradient steps?\n\n- (Continued) If my understanding of the algorithm is correct, I'm curious to know if this modification was extended to Cal-QL for the fixed ratio baselines, since such changes in the training process might non-trivially impact offline-to-online performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810680111,
        "cdate": 1698810680111,
        "tmdate": 1699636268780,
        "mdate": 1699636268780,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "A1QWWJpGBm",
        "forum": "wWI1RYngAA",
        "replyto": "wWI1RYngAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_nGv3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3208/Reviewer_nGv3"
        ],
        "content": {
            "summary": {
                "value": "Most offline-to-online RL requires mixing the offline data with online data based on some mixing ratio. Prior work most focus on the static ratios, however, the optimal ratio might be a function of task complexity, offline data quality, task environment, etc. This paper tackles this problem by studying an adaptive way to update the mixing ratio through the exploration reward in the online stage. It is being framed as a multi-arm bandit problem, with the action space as the possible mixing ratios, the reward is the average trajectory reward, and the goal is to find the optimal mixing ratio. It then utilizes the UCB algorithm here and shows promising empirical results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper points out that the mixing ratio for offline and online data is task dependent, and provides some empirical results to demonstrate. The study in this paper is very well motivated, i.e., the mixing ratio should be found in a task-dependent and adaptive manner.\n\n- It nicely frames this as a bandit problem, and utilizes the UCB approach, which shows promising empirical results.\n\n- Beyond simple comparison with the baselines, the paper also does some investigation of the methods, such as how the adaptive ratio changes as training continues? could it automatically find something near the optimal ratio? as well as some ablation studies regarding the hyper-parameter used in the bandit algorithms, this provides deeper understanding of the method."
            },
            "weaknesses": {
                "value": "- One thing I found not super clear is the method section of the paper, which is the most important part of the paper, i.e., Section 4.1. Equation 2 is a little bit confusing, what do you mean by value estimate of $m$. If I understand correctly, we could simply using the average trajectory reward under mixing ratio $m$ as the reward, I am not super how Equation 2 comes. Some notations are used w.o. definition, such as sum(N) in Equation 3. The method part should be introduced more rigorously and clearly.\n\n- One motivation about the paper is that the mixing ratio is task dependent, environment dependent as well as time-dependent (as training goes). After proposing the ROAD, it would be great to revisit this question, i.e., to establish the connection of the mixing ratio with some notion of task difficulty, etc, either empirically or theoretically. Section 5.4 is an example of this in terms of data quality, but it would be great to investigate other dimensions as well.\n\n- Given the paper is utilizing bandit algorithms here, more bandit algorithms such as Thompson Sampling could be tested here as well. It would also help to improve the paper if there is some optimality guarantee of the proposed algorithm."
            },
            "questions": {
                "value": "See weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699068314164,
        "cdate": 1699068314164,
        "tmdate": 1699636268709,
        "mdate": 1699636268709,
        "license": "CC BY 4.0",
        "version": 2
    }
]