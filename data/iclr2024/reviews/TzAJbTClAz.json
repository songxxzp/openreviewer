[
    {
        "id": "AvQXCNmNxe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_8vvg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_8vvg"
        ],
        "forum": "TzAJbTClAz",
        "replyto": "TzAJbTClAz",
        "content": {
            "summary": {
                "value": "Paper introduces the Fair Fairness Benchmark, a benchmarking framework for in-processing group fairness methods.\nContributions are: the provision of flexible, extensible, minimalistic, and research-oriented open-source code;\nthe establishment of unified fairness method benchmarking pipelines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Good amount of fairness metrics.\nGood amount of datasets."
            },
            "weaknesses": {
                "value": "I think that the main missing point of the paper is a larger series of state of the art (un)fairness mitigation methods to use as baseline."
            },
            "questions": {
                "value": "I think that authors should elaborate on the limited amount of of state of the art (un)fairness mitigation methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4901/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697128231119,
        "cdate": 1697128231119,
        "tmdate": 1699636475027,
        "mdate": 1699636475027,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hZ69R2cBKe",
        "forum": "TzAJbTClAz",
        "replyto": "TzAJbTClAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_Tj8b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_Tj8b"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a group fairness benchmark for in-processing methods. A wide range of fairness definitions are used along with multiple datasets. Three types of in-processing methods are compared: gap regularization, independence, and adversarial learning. Several observations are made on fairness-utility tradeoff, stability, and others."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Since the fairness literature is vast, it is a good time to make a comprehensive comparison.\n* The comparison of in-processing methods using group fairness measures looks reasonable."
            },
            "weaknesses": {
                "value": "* While proposing a fairness benchmark is a worthy effort, the scope of this study seems a bit limited because it only considers in-processing methods. Even AIF360 proposed in 2018 compares pre-processing, in-processing, and post-processing methods, so it is expected that a new benchmark should at least subsume this scope. Pre-processing methods should not be ignored because some of them are designed to complement in-processing methods for the best results. Even for in-processing methods only, there are reweighing and sampling methods [1,2,3] that should be compared.\n\nNowadays, performing in-processing training without sensitive attributes is also actively studied, and a comparison with these methods (e.g., [4,5]) would be interesting.\n\n[1] Jiang et al., Identifying and Correcting Label Bias in Machine Learning, AISTATS 2020\n[2] Roh et al., FairBatch: Batch Selection for Model Fairness, ICLR 2021\n[3] Iosifidis and Ntoutsi, AdaFair: Cumulative Fairness Adaptive Boosting, CIKM, 2019\n[4] Lahoti et al., Fairness without Demographics through Adversarially Reweighted Learning, NeurIPS 2020\n[5] Hashimoto et al., Fairness Without Demographics in Repeated Loss Minimization, ICML 2018\n\n* Concluding that HSIC is the best approach seems misleading because not all in-processing methods were compared as explained above.\n\n* Some of the key observations are already known in the fairness community. Observation 4 (adversarial debiasing has instability) is not surprising and is mentioned in the papers that use this approach. Observation 5 (utility-fairness trade-off is controllable) does not seem revealing either. What's actively studied nowadays is whether there has to be a trade-off or not, and there is a line of research that discusses when utility and fairness align instead of conflict. It would be interesting to empirically verify if the claims made here are actually true. In observation 6 (training curve stability), a future direction is suggested to \"focus on enhancing fairness training stability\". However, it is not clear why enhancing the fairness stability is the most important research direction among other challenges. What does it mean to be not stable enough?\n\n* There is an emphasis on making the benchmark research-oriented, but this term is rather vague. Instead, benchmarks should target practical applications as they actually show which fairness method works.\n\n* One of the future works is to include a wider range of in-processing group fairness methods. This direction should not be a future work as the current paper claims to be a complete benchmark for such methods."
            },
            "questions": {
                "value": "Please address the weak points above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Reviewer_Tj8b"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4901/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697551263931,
        "cdate": 1697551263931,
        "tmdate": 1700570554056,
        "mdate": 1700570554056,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bRM5xxtWbm",
        "forum": "TzAJbTClAz",
        "replyto": "TzAJbTClAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_dSax"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_dSax"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces the Fair Fairness Benchmark (FFB), a framework for evaluating group fairness methods in machine learning. It aims to address the challenges of inconsistent experimental settings, limited algorithmic implementations, and extensibility issues in fairness tools. FFB offers an open-source benchmark, standardized code, and extensive analysis from 45,079 experiments, making it a valuable resource for the fairness research community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper address a prominent problem in the Fairness community which is the inconsistency of different results from various papers. A lot of experiments are conducted in a standardized manner using different datasets, methods and evaluation metrics. The writing is clear and linking the contribution points with 1,2,3.. in the text makes it quite convenient to read. The open source code looks good and is in a state which can be easily adopted for other researchers."
            },
            "weaknesses": {
                "value": "In Table 4 you give some recommendations if one should use the dataset or not. In the text you explain the different discussion. It can be that I missed it but are there any quantifiable measurements to check if a dataset is good for fairness or not - like a metric? And if so can this value be included in the study? Also you said you had 10 trials to produce this table. Did you do any HP optimization with some hold-out splits or how exactly was this done? More detailed information about this would be appreciated."
            },
            "questions": {
                "value": "Figure 1: Are the results different because of different random seeds or did you changed the gamma values of the loss functions to obtain different acc-fairness trade-offs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This study has a potential positive influence on the evaluation of fair algorithms. However there are also risks involved if the benchmark is not carefully chosen. Maybe some of this risks (stakeholders how chose the benchmark, what metric is weighted how much, have discriminated people the chance to be involved in the process, etc.) can be discussed in the paper."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Reviewer_dSax",
                    "ICLR.cc/2024/Conference/Submission4901/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4901/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697709704137,
        "cdate": 1697709704137,
        "tmdate": 1699702933644,
        "mdate": 1699702933644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bwDzLd3K7b",
        "forum": "TzAJbTClAz",
        "replyto": "TzAJbTClAz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_5ZmP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4901/Reviewer_5ZmP"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new fairness benchmark called FFB (Fair Fairness Benchmark). FFB targets to support group fairness metrics and in-processing fairness algorithms, and the system contains several well-known fairness algorithms with metrics. The paper also describes various observations, which are gathered by using the proposed benchmark. For example, the paper observes that the model architecture usually does not significantly affect the fairness performances, which shows that the biases are mainly from the training data. These observations are aggregated based on more than 45000 experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper aims to solve a very important problem in the fairness literature, the lack of great benchmarks.\n- The paper well states the challenges in making fairness benchmarks and proposes a new one called FFB to help the fairness literature.\n- The paper performs extensive experiments and summarizes their observations in several aspects, including model performance and stability."
            },
            "weaknesses": {
                "value": "Although I appreciate the paper\u2019s contribution on proposing a new benchmark, I have several concerns on the manuscript as a research paper.\n- The paper needs to explain more clearly how to use FFB and the strengths of the system itself.\n  - The paper does not clearly explain how to use FFB for testing new algorithms or new datasets. The paper currently focuses on the predefined algorithms and models in FFB, but as a paper that proposes a new benchmark, demonstrating how to utilize their system can be more important.\n  - Similarly, it would be better if the paper could provide more explanations on the strengths of FFB itself. It seems some explained characteristics (like minimalistic aspect) are not supported by enough convincing explanations.\n- Currently, the paper explains their observations on several algorithms and datasets by using FFB, but many of the observations are not very surprising and already discussed in the literature. Thus, it is a bit unclear to me whether such observations themselves can be a strong contribution of this paper. Although such observations are still noteworthy to summarize in the paper, it may be better to not oversell them in the paper. It would be better if the paper could clearly connect these observations to the previous knowledge in the fairness literature."
            },
            "questions": {
                "value": "My main concerns are described in the above weakness section. I hope to hear the authors\u2019 response to them.\n\n--------------------\nThe score is updated after rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4901/Reviewer_5ZmP"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4901/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699000874603,
        "cdate": 1699000874603,
        "tmdate": 1700741740079,
        "mdate": 1700741740079,
        "license": "CC BY 4.0",
        "version": 2
    }
]