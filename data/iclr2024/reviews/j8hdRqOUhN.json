[
    {
        "id": "gwej8WvSOY",
        "forum": "j8hdRqOUhN",
        "replyto": "j8hdRqOUhN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_4YEX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_4YEX"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the ReSample algorithm is introduced, leveraging the capabilities of diffusion models to address inverse problems. Unlike other approaches using diffusion models for inverse problems, ReSample trains the model in a lower-dimensional latent space, offering a computational advantage over conventional diffusion models trained in pixel space. Although a similar method was introduced very recently by Rout et al. in 2023, ReSample enhances its performance by introducing innovative sampling techniques supported by theoretical justification and validated through numerous experiments involving both linear and non-linear operators."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "-\tA design of a novel sampling method supported by theory. It is proven that the proposed sampling method is less noisy than stochastic sampling.\n\n-\tThe paper is well organized."
            },
            "weaknesses": {
                "value": "-\tThe idea of applying a latent space to solve inverse problems using diffusion models is not groundbreaking. Nonetheless, the paper introduces significant advancements to this approach.\n\n-\tIt is not clear why training the model in pixel space (Table 1 and 2) achieves lower quality results compared with the proposed ReSample algorithm. The rationale behinds this result remains unclear.\n\n-\tThe experimental results primarily involve random inpainting, a task considered relatively straightforward. It would be valuable to explore how the proposed algorithm performs in more complex inpainting scenarios, such as the removal of large pixel regions, as demonstrated in the approach outlined by Rout et al. in 2023.\n\n-\tA minor issue to address: There is a typographical error in the term \"noice\" after Theorem 2."
            },
            "questions": {
                "value": "-\tThe primary incentive for training the model in a lower-dimensional latent space appears to be a reduction in computational complexity. Nevertheless, the experimental outcomes presented in Table 1 and 2 demonstrate that the proposed algorithm surpasses conventional techniques in terms of reconstruction quality. Could you provide insights into the potential factors contributing to this outcome?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Reviewer_4YEX"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697890774383,
        "cdate": 1697890774383,
        "tmdate": 1700648874750,
        "mdate": 1700648874750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bk84Vdh3FQ",
        "forum": "j8hdRqOUhN",
        "replyto": "j8hdRqOUhN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_azNZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_azNZ"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a novel latent diffusion algorithm that ensures fidelity to the measurements for solving inverse problems. Instead of sampling in the image domain as often done in diffusion based methods, the sampling is performed in the latent domain which allows speeding up the algorithm. This approach is shown to perform well on several linear and non-linear inverse problems."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is overall well written and easy to follow. \n- The background on denoising diffusion models is clear, giving the right balance between technicalities and intuitions.\n- Experimental results are impressive. In particular, two points are to be underlined that make the method of interest for the imaging community:\n  - The authors chose to work with (relatively) difficult measurement operators that may not be trivially SVD decomposable;\n  - The authors considered mainly low measurement noise regime, which is a challenging setting for diffusion models but common setting in imaging."
            },
            "weaknesses": {
                "value": "- While the general background on diffusion models is well documented, key references are missing regarding diffusion models in constrained settings as well as latent space optimization methods (see references below).\n- The comparison with some other methods seems rather unfair. In particular, ADMM-PnP is ran for only 10 iterations, while it often requires at least 50 to 100 iterations to yield good results; similarly, the FBP-UNet seems to strongly under-perform compared to the (Jin et al) reference.\n- The motivations of the paper are not very consistent with the results (e.g. regarding the advantage of latent sampling on inference speed)."
            },
            "questions": {
                "value": "**Major comments**\n\n1. In the DPS paper, the authors do indeed compute $\\nabla_z ||y-\\mathcal{A}(\\hat{x}_0(x_t)||_2^2$ in (7). However, their code does not use a squared norm, but a plain norm $\\nabla_z ||y-\\mathcal{A}(\\hat{x}_0(x_t)||$ [(see here)](https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/condition_methods.py#L32). In practice, changing to a squared norm strongly decreases the quality of the results. Is this also the case in this work? Would the theoretical analysis in the paper still hold?\n2. It does not appear clearly in the paper that working in the latent domain improves the computational efficiency despite being a main motivation of this work by the authors. Table 7 is disappointing in that respect. Maybe the authors could emphasize other strengths of their method instead of this one (for instance, the successful results despite the difficulty to sample in constrained settings [1, 2, 3]).\n3. An important issue in my opinion is the lack of reference to methods performing similar procedure as the proposed Hard Data Consistency. While diffusion models are overall well referenced, other methods are less well documented. In particular, optimizing latent variables of a decoder as in equation (10) is at the heart of well known methods such as [4, 5]. Discussing these references (and related ones) would help relate the author's work to methods not relying on diffusion. Other relevant works [1, 2, 3] could also be added to the references.\n4. While the choice of solving (10) makes sense in practice, checking that the proposed sampling scheme works (or not) when (10) is solved at each iteration would be very insightful. So far, it is only backed by \"Because of the continuity of the sampling process, after each data-consistency optimization step, the samples in the following steps can still remain similar semantic or structural information to some extent.\" How does your method perform if a data fidelity step was performed at each iteration?\n5. The authors mention that (Rout et al., 2023) proposes a similar method; adding few sentences stressing the similarities and differences with the proposed method may be insightful.\n6. The theoretical derivations in Appendix appear in a more general form in [6]. Adding the reference when introducing Tweedie's formula may be of interest.\n\n\n**Minor comments**\n\n6. Adding PSNR metrics when displaying images would be informative (e.g. in Fig. 3 and 4, but maybe more importantly in the appendix where there is no page limit).\n7. Figure 6 is not very clear since the plot on the right does not correspond to the problem displayed on the left. Furthermore, the legend states \"ReSample frequency\" while the plot states \"# of iterations\", maybe the graph should either choose frequency in x-axis or the label should be updated. \n8. Few typos are remaining, e.g. \"in replace\" (p. 5), \"still remain\" (instead of retain, p. 5), \"noice\" (p. 6).\n9. If I understand Algorithm 1 correctly, the \"if t \\in C\" condition is not activated before Stage 2 of Fig. 16. In this context, I do not understand how information about the measurement can be visible in Stage 1. Isn't Fig. 16 misleading with that regards?\n10. More generally, is Stage 0 really necessary? I would naively that for solving eq. (10) and sampling from that may provide a good initial step, allowing to reduce the number of iterations.\n\n**References**\n\n[1] Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou and Molei Tao, Mirror Diffusion Models for Constrained and Watermarked Generation arxiv:2310.01236, 2023\n\n[2] Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, and Michael Hutchinson, Diffusion models for constrained domains. arXiv preprint arXiv:2304.05364, 2023.\n\n[3] Aaron Lou and Stefano Ermon. Reflected diffusion models. arXiv preprint arXiv:2304.04740, 2023\n\n[4] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Deep image prior.\" CVPR, 2018.\n\n[5] Jalal, Ajil, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G. Dimakis, and Jon Tamir. \"Robust compressed sensing mri with deep generative priors.\" NeurIPS, 2021\n\n[6] Efron, Bradley. \"Tweedie\u2019s formula and selection bias.\" Journal of the American Statistical Association 106.496 (2011): 1602-1614."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Reviewer_azNZ",
                    "ICLR.cc/2024/Conference/Submission4764/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698166046280,
        "cdate": 1698166046280,
        "tmdate": 1700915180898,
        "mdate": 1700915180898,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6ji64oZXKu",
        "forum": "j8hdRqOUhN",
        "replyto": "j8hdRqOUhN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_1zP2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_1zP2"
        ],
        "content": {
            "summary": {
                "value": "The paper titled: Solving Inverse Problems with Latent Diffusion Models Via Hard Data Consistency presents an novel algorithm that use pre-trained latent diffusion models to solve general inverse problems. The authors demonstrated the effectiveness on multiple tasks and datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors propose a novel hard data consistency module, that strictly enforce the measurements via optimization, theoretical and empirical results demonstrate the superiority of ReSample over existing SOTA methods.\n\n2. The authors prove theoretical proof with analysis on the variance of the resampled images.\n\n3. The authors demonstrate the effectiveness of ReSample on multiple tasks on various benchmarks - deblurring, denoising, super resolution, inpainting CT Reconstruction.\n\n4. This paper is well written and the core idea is clearly delivered."
            },
            "weaknesses": {
                "value": "I didn't find clear flaw of this paper, I have a few questions instead.\n\n1. How do you scale up the algorithm to high-resolution images?\n\n2. To solve equation 10 - the L2 least square, I assume you are using gradient descent or conjugate gradient descent, one possible way to accelerate it is to use close form expression on D(z) if A is a linear operator, Do you have any thoughts on this path?\n\n3. Now, I assume the linear operator A is known, but for deblurring task, the system operator can be unknown, can you elaborate on how you could adopt your algorithms to this scenario?"
            },
            "questions": {
                "value": "Please see above on my questions, overall a solid paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Reviewer_1zP2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698693625232,
        "cdate": 1698693625232,
        "tmdate": 1699636458587,
        "mdate": 1699636458587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CMX6woz0XW",
        "forum": "j8hdRqOUhN",
        "replyto": "j8hdRqOUhN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_6mow"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4764/Reviewer_6mow"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a latent-diffusion model based inverse problem solver which is more efficient than previous solves on pixel space and a novel problem.\nThe main contribution of the paper is an unbiased and non-expansive sampling method after applying data consistency, namely stochastic resample. \nFor various linear inverse problems, the proposed method has demonstrated comparable or better performance compared to baseline, while reducing memory and time cost."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow\n- The proposed stochastic resampling is novel and theoretically solid.\n- The performance of the proposed method outperforms baseline methods, especially simple extension of DPs."
            },
            "weaknesses": {
                "value": "- Complex usage of multiple engineering to improve the efficiency of the method (e.g. partial data consistency, early stopping...)"
            },
            "questions": {
                "value": "- In the algorithm, at time point $t$, the data consistency update is done with $z_{t-1}$, rather than $z_t$ which is differ from prior works of diffusion-based inverse problem solver including DPS, DDNM and so on. In fact, to compute as in the algorithm, we need to feed-forward the diffusion model twice for each time step, which lead to two times longer sampling time. I think this is the reason that the paper emphasize the *partial* hard data-consistency update works well in the section 3.1. What if we use $z_t$ from the previous step without additional unconditional DDIM step? It seems reasonable because the mean of previous $z_t$ is also follows the unconditional sampling path. If I'm overlooking anything, please pointing it out.\n\n- How long it takes if we apply the hard data-consistency for every time-steps? Also, can authors provide the quantitative result on it? I wonder how marginal the performance gap between full hard data-consistency and partial hard data-consistency (which is a proposed method) is, as the performance would be further improved if we use ReSample more frequently according to the figure 6 and figure 15.\n\n- There is no description of $\\gamma$ in figure 15"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4764/Reviewer_6mow"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4764/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825186784,
        "cdate": 1698825186784,
        "tmdate": 1699636458497,
        "mdate": 1699636458497,
        "license": "CC BY 4.0",
        "version": 2
    }
]