[
    {
        "id": "AD0voesYRK",
        "forum": "0unbjYPmbC",
        "replyto": "0unbjYPmbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission659/Reviewer_nix8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission659/Reviewer_nix8"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an automatically generated dataset for conversational image retrieval, in which a user may have multi-round interaction with a retriever in a natural language. The samples in the dataset is generated using existing models. The paper also presents a model for addressing the task. Some experimental results are presented on the dataset, which draws some insights on the data required for training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) I think the task can be new and interesting. If I understand the task correctly, the model is required to take the conversation history into account for better performance."
            },
            "weaknesses": {
                "value": "(1) The paper does not provide sufficient details about the task definition. The paper will be better if it provides the input and output for each task (tChatSearch, iChatSearch, and mChatSearch). \n\n(2) Related to (1), these tasks seem to be interactive, in which a user can make different reactions to the same retrieval result (candidate images, etc.). However, as far as I understand, the dataset does not provide this type of interaction, and it only offers a single sequence of dialog without branches. This point seems important for interactive image retrieval but is not mentioned in the paper. I think this is not evaluated.\n\n(3) I guess this is due to the page limitation, but the details on the dataset construction and the model is not fully provided to understand what is actually done. For example, the paper says some constraints are added to GPT-4 to imply some hints for image retrieval, but I cannot see how. \n\n(4) The insufficient details on the tasks and the methods make it hard for me to evaluate the paper."
            },
            "questions": {
                "value": "I would like clarification on (1)-(3) in the weakness section. I'm happy to increase my scores once some details are provided or I have made some misunderstandings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698109944049,
        "cdate": 1698109944049,
        "tmdate": 1699635993386,
        "mdate": 1699635993386,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Gs0emOivgs",
        "forum": "0unbjYPmbC",
        "replyto": "0unbjYPmbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission659/Reviewer_Lx3N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission659/Reviewer_Lx3N"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dataset referred to as ChatSearch, which is dialogue data in terms of image, such that the dialogue agents should retrieval corresponding image to respond the question in dialogue.\nFor the baseline, a system referred to as ChatSearcher is also proposed to reason multi modality between image and text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "[+] This paper addresses a situation when the dialogue is composed of an image, thus this paper proposes a dataset for image retrieval based on dialogue.\n\n[+] New task about image retrieval in terms of dialogue"
            },
            "weaknesses": {
                "value": "[-] The datasets from the works below seem to assume more general situations and to have more space for multimodal reasoning. What is the difference and contributions compared to this work?\n\n1. PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling, ACL'21\n\n2. TikTalk: A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World ACM Mutimedia'23\n\n3. DialogCC: Large-Scale Multi-Modal Dialogue Dataset Arxiv'23\n\n[-] How can we handle it if there are no images that we want? I think our questions should be more diverse than the scope of the proposed dataset covers\n\n[-] There are many systems for text-to-image retrieval. Does this dataset address the issues that previous text-to-image retrieval systems can not perform? I mean, we can also handle the problem of image retrieval from dialogue without involving the proposed datasets by integrating text-based dialogue systems and image retrieval systems based on many image corpus. It should be more convincing if there is evidential reason that the author has to collect dataset."
            },
            "questions": {
                "value": "See above. I am almost borderline, but there is no corresponding score for it. Therefore I want to decide my evaluations after rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698261724748,
        "cdate": 1698261724748,
        "tmdate": 1699635993305,
        "mdate": 1699635993305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p26iw3BOmn",
        "forum": "0unbjYPmbC",
        "replyto": "0unbjYPmbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission659/Reviewer_fGPE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission659/Reviewer_fGPE"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of conversational image search, where the goal is to interactively retrieve images according to the human's dialogue history. To resolve this problem, the authors propose a pipeline to automatically construct a dataset to include multimodal conversational context and target retrieval candidates. Then they trained a generative retrieval model called ChatSearcher to accepted interleaved image-text context and perform search."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written with well-defined motivation and real-world downstream use case\n- The dataset construction pipeline is novel and completes the main story of the paper. The quality seems okay (since the author claimed that they have human validation)\n- The system built, though far from solving the task, is approaching towards the correct direction"
            },
            "weaknesses": {
                "value": "- It is kind of disappointing to base the validation and test set on a ancient dataset (MSCOCO), which has been mainly used for tuning model for almost a decade. Imaging that the COCO dataset would never cover any interesting visual entity after 2015. IMO, it would be better to consider base your test set on image collections with more diverse domains, such as the visual news dataset or CC3M dataset.\n\n- The description on conversational instruction tuning is not sufficient for reader to understand how it actually work. Particularly, that instructPix2Pix is an image editing dataset. It would be nice to show a figure that explains how data from each domain look like and how they are processed to train model. \n\n- It also seems to me that there is a high chance where a user request can not be fulfilled within the candidate image sets. For example, when user are asking for a iPhone with foldable screen but there is no such a thing. What would the evaluation to handle such a case where it is unanswerable?  I couldn't find any discussion on it."
            },
            "questions": {
                "value": "- Why do authors emphasize your model is a generative retriever? In text retrieval (particularly entity retrieval), generative retriever usually refers to the models that generates the exact content of the document (via constrained decoding). Whereas in this case, the model is not really generating the image (but an embedding that approximates the image neighborhood). Giving the model terminology of generative retriever give the reader an impression that you are generating the image in pixel space, so that we can do exact visual matching. \n\n- Do we have any experiments ablating the effects of each component in the instruction-tuning datasets? It seems that Table 5 is such an ablation but where is instructpix2pix? \n\n- What is the retrieval image candidate set in each eval setting, are we only consider re-trieving against 5k (or 1k) images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698601995415,
        "cdate": 1698601995415,
        "tmdate": 1699635993226,
        "mdate": 1699635993226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUuxjBG65b",
        "forum": "0unbjYPmbC",
        "replyto": "0unbjYPmbC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission659/Reviewer_DUdh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission659/Reviewer_DUdh"
        ],
        "content": {
            "summary": {
                "value": "In this paper the authors propose a new dataset ChatSearch which included multimodal conversations. The retrieval is performed by inferring details from multiple rounds of conversations. In addition, the authors also introduce a generative retrieval model ChatSearcher trained end-to-end and produces interleaved image-text inputs or outputs. Experimental results show that ChatSearcher shows superior performance on ChatSearch and comparable results on zero-shot image retrieval."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Originality:** The authors introduce a dataset for conversational image retrieval which deals with multimodal form of dialogue. Most of the previous works focus on having image as static and textual dialogue. This limits the users ability to chat using images. The propose dataset overcomes the disadvantage. The proposed pipeline using LLMs for the dataset creation is also novel and requires less human effort. \n\n**Quality:** In addition to the dataset, the authors also propose a strong baseline model which learns from the conversational image dataset and performs better than CLIP on standard image retrieval datasets. The ablation studies are sound and well structured."
            },
            "weaknesses": {
                "value": "**Clarity:** \n-  I find the section 3.1 very difficult to follow. The words like \"target image\", \"reference text\" etc. appear multiple times and are confusing. The authors can introduce certain mathematical notations and provide sufficient examples for a better flow of the pipeline description. \n\n- Figure-2 contains a lot of sub-figures and details. The entire pipeline is not clearly understood from the figure and caption doesn't provide sufficient context. The figure can be improved by breaking down into individual sub-figures and the text can be made clearer."
            },
            "questions": {
                "value": "1. In the text dialogue construction, how does the authors ensure that GPT-4 doesn't generate unrelated image content or repetitive text dialogue."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission659/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission659/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission659/Reviewer_DUdh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission659/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698994903698,
        "cdate": 1698994903698,
        "tmdate": 1699635993169,
        "mdate": 1699635993169,
        "license": "CC BY 4.0",
        "version": 2
    }
]