[
    {
        "id": "Ezz3lOkVzq",
        "forum": "YhpgUWE4Rt",
        "replyto": "YhpgUWE4Rt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_kzFw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_kzFw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel neural network model called Recurrent Attention-based Neural Cellular Automata (RA-NCA) for predicting the dynamics of many-agent systems with local and stochastic interactions, which has not been explored in existing work. The empirical results on the three synthetic cases show that the proposed method presents superior data efficiency and scalability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- It addresses a challenging and underexplored problem of learning the hidden interaction rules in many-agent systems without prior knowledge or assumptions.\n- It introduces a recurrent cellular attention module that combines LSTM and cellular self-attention to capture the temporal and permutation-invariant information of neighboring agents.\n- It demonstrates the superior performance of RA-NCA over other NCA networks and video prediction networks in terms of data efficiency, robustness, and scalability across three synthetic datasets with different levels of stochasticity.\n- The experimental results are conducted thoroughly and convincingly."
            },
            "weaknesses": {
                "value": "- It lacks an investigation on the scale of considered neighbors. Specifically, the authors consider only the case of Moore's neighborhood.\n- It only evaluates RA-NCA on synthetic datasets, which may not reflect the complexity and diversity of real-world many-agent systems.\n- It does not compare RA-NCA with other state-of-the-art methods for multi-agent interaction learning, e.g., networked agent learning derived from distributed optimization [1], which leverages a similar philosophy to resolve large-scale agent learning. Nonetheless, I do not regard it as a reason to reject this paper, and I hope the authors give further comparison in this paper, experimentally or conceptually.\n- Some key concepts lack clarity, such as what is agent interaction? I suggest the authors give a brief introduction in the \"Related work and Background\".\n\n\n\n[1] Zhang, K., Yang, Z., Liu, H., Zhang, T., & Basar, T. (2018, July). Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning (pp. 5872-5881). PMLR."
            },
            "questions": {
                "value": "1. Section 2: \"However, recurrent neural networks based approaches have been ....\", I didn't get the point of this claim.\n2. Section 2: \"However, these methods are not explicitly designed to preserve spatial structures in latent space ...\". The question is that why can NCA preserve spatial structures in latent space?\n3. Section 3: \"Recently, graph-based, attention-based ,...\". Please add references to existing work.\n4. How does the NCA make the global interactions keep consistent with local interactions in Moore's neighborhood?\n5. What is $c_{(i,j)}$ in equation (4)?\n6. What is the value of $(t_{n_{pred}} - t_{n_{obs}})$ used for experiments?\n7. Have the authors ever tried other training scales? expect for $64\\times64$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698648101707,
        "cdate": 1698648101707,
        "tmdate": 1699637010782,
        "mdate": 1699637010782,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EOXOxFnzby",
        "forum": "YhpgUWE4Rt",
        "replyto": "YhpgUWE4Rt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_jsLT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_jsLT"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new model for many-agent system prediction/simulation such as epidemic spread, rumor propagation through crowd,\nprey-predator model, and forest fire. Unlike the previous CNN-based model which suffers from spatial dependency, the proposed new model \n is based on self-attention which is permutation invariant and thus overcomes this issue. In addition, the LSTM is introduced to endow the model with memory, which is essential to leverage historical information to do prediction. The experiment section proves the effectiveness of the method and shows that the model outperforms other baselines even in extremely data-limited scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to understand\n2. The motivation for model designing makes sense\n3. The result shows it outperforms baselines in this field and other video prediction baselines"
            },
            "weaknesses": {
                "value": "1. The method is trained and evaluated on in-house image datasets collected with stale simulators (released 20 years ago). As I am not an expert in the related field, I wonder if there are some new and common benchmarks for evaluating the method like real-world forest fire data, which should be unstructured and sophisticated. \n2. I can not find any potential for applying the method to important problems. The method represents the many-agent system with a structured semantic image and converts the problem to an image prediction problem on toy datasets.\n3. The method is a simple combination of two existing and well-studied techniques. Thus technical novelty is limited as well."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703303858,
        "cdate": 1698703303858,
        "tmdate": 1699637010665,
        "mdate": 1699637010665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MhntiSmVkN",
        "forum": "YhpgUWE4Rt",
        "replyto": "YhpgUWE4Rt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_o3LN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_o3LN"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Recurrent Attention-based Neural Cellular Automata (RA-NCA), combining the attention mechanic with the recurrent LSTM neural network for state prediction in cellular automata (CA). In contrast to other CA approaches using CNNs as their feature projector, the proposed architecture  is presented as superior regarding training and sample efficiency, mainly due to the attention\u2019s permutation invariance. RA-NCA is also shown to scale well in the transfer to larger CA systems. For the evaluation, three CA settings are being tested (forest fire, host-pathogen spread and a stock-marked model), comparing RA-NCA to a LSTM+CNN model and an ablation with only the attention mechanism. RA-NCA was also evaluated with respect to sample efficiency and compared to recent Scene/Video-Prediction Networks on three levels of stochastic transition for the forest fire domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The paper is concisely written and understandable. All key-concepts are explained and cleanly formalized. The domains are well described in the Appendix. The evaluation on three domains covers diverse aspects with regards to density and the amount of training data. The focus point of the attention invariance is well explained and nicely visualized in Fig. 2 and 3. Nevertheless, I would like to point in the direction of Tang and Ha (2021), which is quite close in terms of insights, but applied for the field of RL. A discussion on the key difference to this work, perhaps with focus on the (Moore) neighborhood, would certainly benefit the quality of this work. Regarding the proposed architecture, both key contributions of showcasing the data-efficiency and the model-scalability pose an interesting topic of research, but I would have liked to see some more focus on the latter.\n\nY. Tang and D. Ha, \"The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning\", in Advances in Neural Information Processing Systems, 2021."
            },
            "weaknesses": {
                "value": "While the paper contains some interesting foundations, it lacks focus on the field of CAs by mixing in notions of multi-agent systems and video prediction networks. From my view of (MA)RL and Game Theory, the mention of CA as multi-agent systems seems quite unintuitive, since I understand CAs more as conditional transition (probabilities) between cells rather than multiple agents interacting with each other. Apart from the notation and the section on Multi-Agent interaction, I also see no distinction being made between classical neural CAs and each cell being an agent.\n\nAlso, the evaluation could be improved. Evaluating only against two variation baselines (CNN+LSTM, only Self-Attention) seems to be on the weaker side. Convolutional-recurrent Networks are being steadily phased out of image recognition in favor of e.g., Vision-Transformers, in part due to sample efficiency of training convolutional kernels. The claim \u201cThe three networks consist of the similar level of trainable parameters.\u201d on page 7 is similarly questionable, since CNN+LSTM and RA-NCA almost double the parameter count of Attention-CA. Since taking the hidden-state information out of the Attention-CA baseline seems to be impactful, comparing to Attention+RNN would perhaps be more insightful. \nI would have also liked to see a comparison to the obvious state-of-the-art architecture Attention-Transformer NCA  (Tesfaldet et al., 2022), that is cited but not used in the comparison between Transformer and LSTM. Even if this comparison is intended as part of a pre-study on the importance of the respective architecture components, there is still a significantly overlap to the section on video prediction networks.\n\nThe inclusion of the comparison to video prediction networks seems to overall weaken the focus of the paper without providing novel insights. Since I am not too familiar with video prediction networks I cannot speak to the quality of the chosen models as baselines. However, since the CA-video is reduced to minimal color diversity that simply represent very few states (binary even, as it is evaluated / trained), apart from the concept of \"predicting batches of pixels from previous batches of pixels\", I see not much of a connection to the field of NCAs. Furthermore, CNN+LSTM (and simple Attention) as Model Representatives are already covered in the NCA study and are already evaluated on the same settings as Fig. 6 and Fig. 7. The space dedicated to both the multi-agent interaction and video prediction networks feels disconnected from the core-idea and could have been used to round out the otherwise very interesting component-study in the main-field of NCAs for a more focused and in-depth paper. Also, the discussion on original cell-states on page 14 in the Appendix could perhaps deserve a mention in the main paper?\n\nI would also recommend to more fairly mention the advantages and limitations (e.g., parameter disparity of the Attention-CA) of the other baselines and expand on the discussion. Comparable (or better) performance of the CNN+LSTM in Fig.5 (forest fire) up until data-amount of around 10% could be more discussed, as could\nthe good results on the other two domains (that are only found in Table 5 in the Appendix). I would also like to see the std-div. in the main paper tables (similar to the ones in the Appendix) and some more discussion on variance, since e.g. Fig.6 (the zoomed out bar-plots) shows how close performance is if you consider the variance.\n\nIn summary, I would recommend to shift the focus away from multi-agent interactions and video-prediction networks to focus and elaborate more on the two core-insights of this paper (Low-Data Efficiency and Scalability) to improve focus and balance of the paper. The scaling aspect in particular could offer an interesting insight into the degrees of scalability. It would be encouraged to show a direct comparison to the Transformer-NCA and make the Attention-CA baseline more comparable in terms of parameters to gain a better overview on the current state of the field of NCAs in\ngeneral."
            },
            "questions": {
                "value": "Could you please specify on the $\\bigodot$ operator in Eq. 4?\nWhy was 32 chosen as the encoding dimension?\nIs there a reason why the Transformer Architecture is not applicable in this setting? Would you consider using the Attention-Transformer NCA from Tesfaldet et al., 2022 as a more competitive baseline?\nCould you please motivate/elaborate why you are considering NCA-cells as interacting agents. Apart from the summation of the neighbor\u2019s hidden state there is not \u201cinteraction\u201d in the classical multi-agent sense, that I would have understood here?\n\nMinor Comments:\np3: \u201cRecently, graph-based, attention- based, variational autoencoder-based NCA networks are also proposed.\u201d could use citations, as could the \u201cclassical\u201d ML-LSTM literature (e.g. Schmidthuber et al) that you are building on."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8157/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8157/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8157/Reviewer_o3LN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762911297,
        "cdate": 1698762911297,
        "tmdate": 1699637010520,
        "mdate": 1699637010520,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vdGR5NiwFq",
        "forum": "YhpgUWE4Rt",
        "replyto": "YhpgUWE4Rt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_rEW6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8157/Reviewer_rEW6"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new Recurrent Attention-based Neural Cellular Automata (RA-NCA) in modeling complex systems involving many agents with local, often stochastic interactions.  RA-NCA's innovation lies in a recurrent cellular attention module that combines long short-term memory (LSTM) with cellular self-attention. By evaluating on three simulated multi-agent datasets, RA-NCA shows three good properties which are 1.) Robustness in the presence of stochastic interactions; 2.) Data Efficiency which requires less training data due to the permutation invariance inductive bias; and 3.) Scalability, as it can be trained on small systems and successfully applied to significantly larger systems without the need for re-training, and without a decrease in performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a new model that extends existing NCA methods to capture long-term local and stochastic interactions among agents.\n2. The \bmethod is technically sound and the writing is in general easy to follow.\n3. The experiment results compared with selected baselines show the superior of the proposed method."
            },
            "weaknesses": {
                "value": "1. My major question is the comparison with existing baselines. I understand the paper targets developing a new NCA method to address long-term local interaction among agents and the stochastic property. However, for the specific task it is dealing with, there are much more methods to compare with in literature on multi-agent dynamical system modeling. Examples include discrete GNN-based methods [1][2], and continuous GNN-based methods [3][4] where the key idea is similar to capture the influence from neighbors and past timestamps to make predictions in the future. Also there are some reinforcement learning literature that can address the same task. I believe at least the authors should have a thorough discussion about these directions.\n\n\n[1] Alvaro Sanchez-Gonzalez et.al.  Learning to simulate complex physics with graph networks.\n\n\n[2] Peter W. Battaglia et.al. Interaction Networks for Learning about Objects,Relations and Physics.\n\n\n[3] Zijie Huang et.al. Learning continuous system dynamics from irregularly-sampled partial observations.\n\n\n[4] Chengxi Zang et.al. Neural Dynamics on Complex Networks."
            },
            "questions": {
                "value": "1.  many-agent --> multi-agent?\n2. In Figure 4, can you also plot the visualization from baselines as comparisons?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8157/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827303179,
        "cdate": 1698827303179,
        "tmdate": 1699637010325,
        "mdate": 1699637010325,
        "license": "CC BY 4.0",
        "version": 2
    }
]