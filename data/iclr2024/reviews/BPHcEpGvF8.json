[
    {
        "id": "L6LwNswLyY",
        "forum": "BPHcEpGvF8",
        "replyto": "BPHcEpGvF8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the safety risks in machine learning posed by backdoor attacks, where triggers are embedded in models to activate malicious behavior under specific conditions. It focuses on evaluating the effectiveness of backdoor attacks with constant triggers and establishing performance boundaries for models on clean and compromised data. The study explores key issues: the factors determining an attack's success, the optimal strategy for an attack, and the conditions for success with human-imperceptible triggers. Applicable to both discriminative and generative models, the findings are validated through experiments using benchmark datasets and current backdoor attack scenarios."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper focuses on an important problem. It provides a fundamental understanding of the influence of backdoor attacks.\n- This paper provides extensive theoretical analysis.\n- This paper is easy to follow."
            },
            "weaknesses": {
                "value": "- The observation that a high poisoning ratio adversely affects the performance of clean data lacks novelty. \n- The paper lacks clarity in some sections. For instance, Section 6.2.1 discusses the impact of backdoor trigger magnitudes, but fails to specify crucial details of the attack setting, such as the size of the trigger.\n- The authors assert that WaNet, Adaptive Patch, and Adaptive Blend attacks are more effective than BadNets, as evidenced by a greater relative change in dimensions with low variance. However, the term \"effectiveness\" needs clarification. BadNets is known for its high attack success rate, so how do these methods compare under identical attack settings, including trigger size and magnitude?\n- The methodology for measuring the Mean Squared Error (MSE) between clean training images and those altered by the backdoored DDPM is unclear. Given that DDPM generation is inherently a stochastic process, a more detailed explanation of this measurement technique would be beneficial."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_76rk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698376596634,
        "cdate": 1698376596634,
        "tmdate": 1700664102940,
        "mdate": 1700664102940,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VBwR7i73JY",
        "forum": "BPHcEpGvF8",
        "replyto": "BPHcEpGvF8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
        ],
        "content": {
            "summary": {
                "value": "From the statistical perspective, this paper theoretically analyzed the efficiency of backdoor attacks. Specifically, focusing on the binary classification and generative model, the authors relied on two assumptions to calculate the tight lower and upper boundaries of the backdoor model\u2019s performance on the clean and poisoned test data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Their theoretical conclusion for the efficiency of backdoor attacks matches with the empirical results. For instance, the influence of the poisoning ratio and the magnitude of the trigger signal. Moreover, they also claimed that when fixing the poisoning ratio and the magnitude of the trigger, it is more efficient to choose the trigger along the direction the density of clean data drops quickly."
            },
            "weaknesses": {
                "value": "One thing I want to mention is about the reference, as far as I know, there exist some references on the backdoor efficiency. The authors should cite them.\n[1] W. Guo, B. Tondi and M. Barni, \"A Temporal Chrominance Trigger for Clean-Label Backdoor Attack Against Anti-Spoof Rebroadcast Detection,\" in IEEE Transactions on Dependable and Secure Computing, doi: 10.1109/TDSC.2022.3233519.\n[2] Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born equal: Towards effective clean- label backdoor attacks. Pattern Recognition, 139:109512, 2023. 2, 3\n[3] Pengfei Xia, Ziqiang Li, Wei Zhang, and Bin Li. Data-efficient backdoor attacks. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 3992\u20133998, 2022"
            },
            "questions": {
                "value": "Is it possible to extend this theoretical framework for multi-discriminator with more than 2 classes."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8404/Reviewer_bWko"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698597774009,
        "cdate": 1698597774009,
        "tmdate": 1699637046832,
        "mdate": 1699637046832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zGTB9DFO1z",
        "forum": "BPHcEpGvF8",
        "replyto": "BPHcEpGvF8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_DvbB"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts a theoretical analysis of backdoor attacks, with a focus on addressing three key questions: (1) What are the factors that determine the effectiveness of a backdoor attack? (2) What is the optimal choice of trigger with a given magnitude? (3) What is the minimum required magnitude of the trigger for a successful attack? The paper utilizes finite-sample analysis to derive both upper and lower bounds for the success of a backdoor attack. The poisoning rate, trigger magnitude, and trigger direction are important factors influencing the success of a backdoor attack. Additionally, this paper carries out experiments on synthetic data as well as tasks involving image classification and generation. The empirical results validate the theoretical analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper provides a theoretical analysis on backdoor attacks, an important topic of machine learning security.\n\n2. A few factors that contribute to the success of a backdoor attack are studied in the paper. The choice of a trigger is particularly interesting. The insights shown in the paper can provide a theoretical guideline for further work.\n\n3. The empirical results on synthetic data validate the theoretical analysis and also provide an explanation for generative models."
            },
            "weaknesses": {
                "value": "1. Some claims are not well validated empirically. The paper states \"a large backdoor data ratio \u03c1 will damage the performance on clean data.\" But there is no empirical evidence to support this claim. Also, according to the literature, a high poisoning rate usually does not significantly affect clean accuracy. It is recommended to empirically validate this claim and assess its consistency with the theories.\n\n2. The experiment conducted in Table 2 is not clear. What does the magnitude of backdoor triggers mean? Is it the L2 norm of \u03b7, or a fixed pixel value that replaces the original pixel on the input? How large is the backdoor trigger used in this study? In addition, the formalization of backdoor trigger as \u03b7 in X' = X + \u03b7 is not accurate. Backdoor attacks, such as BadNets replace the original pixel values with the backdoor trigger. Otherwise, the trigger pattern is not fixed and varies on different inputs.\n\n3. The paper seems to focus on dirty-label backdoor attacks, where the poisoned samples are assigned a target label. There is anther line of attacks that do not change the label, such as SIG [1] and reflection attack [2]. Is the proposed theoretical analysis applicable to these clean-label attacks?\n\n[1] Barni, M., Kallas, K., & Tondi, B. (2019, September). A new backdoor attack in cnns by training set corruption without label poisoning. ICIP 2019.\\\n[2] Liu, Y., Ma, X., Bailey, J., and Lu, F. Reflection backdoor: A natural backdoor attack on deep neural networks. ECCV 2020."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698614450518,
        "cdate": 1698614450518,
        "tmdate": 1699637046682,
        "mdate": 1699637046682,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wauxcm02eI",
        "forum": "BPHcEpGvF8",
        "replyto": "BPHcEpGvF8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8404/Reviewer_DhoY"
        ],
        "content": {
            "summary": {
                "value": "This paper studies backdoor attacks with a constant trigger, assuming the trained classifiers are Bayesian optimal with respect to the poisoned training set.\n\nThrough this framework, they provide the following insights for backdoor attacks using a constant trigger:\n1. More backdoor data can harm clean performance and can help backdoor to success.\n2. Backdoor attacks can be more successful when the constant trigger has a larger magnitude.\n3. Backdoor attacks can be more successful when the direction of the constant trigger points towards less popular regions (i.e. regions with smaller density).\n4. Arbitrarily small backdoor data ratios may result in successful attacks.\n5. If there is a direction where for all samples the corresponding support of the marginal distribution is a single point, the magnitude of the trigger can be arbitrarily small to have a successful attack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Theoretical understanding of backdoor attacks is an important topics.\n2. The authors demonstrate their skills in using statistical tools."
            },
            "weaknesses": {
                "value": "While I appreciate the skills demonstrated by the authors, none of the obtained insights is interesting in a sense that they are either trivial or not true without assuming the model to be Bayesian optimal with respect to the poisoned training distribution. \n\nTo be specific, insight 1&2 listed in the above Summary section are trivial (even though it may generalize to other backdoor/poison attacks); Insight 3&4&5 are trivial only when assuming the model to be Bayesian optimal but may not generalize to other (actual) learning algorithms.\n\nTo sum up, my primary concerns regarding this submission include:\n1. Some key assumptions that oversimplify the problems and make the analysis probably irrelevant to practice, e.g. models are Bayesian optimal with respect to the poisoned distribution & Assumption 3 (Ordinary convergence rate) in the submission.\n\n2. Key insights are either trivial (insight 1&2) or likely not generalizable (insight 3&4&5).\n\n\nNotably, the experiments are thin but I find it acceptable for a theory paper. The major issue is not that experiments do not provide enough supports. The issue is that there is not really much insights worth supporting."
            },
            "questions": {
                "value": "Please see the weakness section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8404/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698995907877,
        "cdate": 1698995907877,
        "tmdate": 1699637046561,
        "mdate": 1699637046561,
        "license": "CC BY 4.0",
        "version": 2
    }
]