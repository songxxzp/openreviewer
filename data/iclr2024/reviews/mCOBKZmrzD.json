[
    {
        "id": "zhIGnYcl9h",
        "forum": "mCOBKZmrzD",
        "replyto": "mCOBKZmrzD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_c4fs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_c4fs"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed EquiformerV2, which is a newly developed equivariant network for 3D molecular modeling built on the Equiformer. From the experimental evaluation, the EquiformerV2 model achieved strong performance on the large-scale OC20/OC22 benchmark, QM9 dataset and also the new AdsorbML dataset. Such performance improvement upon Equiformer is achieved via several architectural modifications: (1) levering eSCN's efficient SO(2) convolution implementation for SO(3) convolutions (tensor product operations); (2) Attention Re-normalization for stabilizing training; (3) Separable S2 activation for mixing representations with different degrees; (4) separate Layer Normalization."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Regarding the problem studied in this paper**. By leveraging the key techniques from eSCN, the EquiformerV2 also achieves learning irreducible representations with larger maximum degrees, which has been verified again to be useful for large-scale DFT benchmarks.\n\n2. **Regarding the empirical performance**. In the OC20 benchmark, EquiformerV2 sets a new standard by delivering state-of-the-art performance in the Structure-to-Energy-Force task. The model, further trained on this task, effectively serves as a force-field evaluator, demonstrating impressive performance in both IS2RS and IS2RE tasks. EquiformerV2 surpasses the performance of the compared baselines across all tasks, with a notable edge in force prediction. Furthermore, it significantly enhances the success rate on the AdsorbML dataset."
            },
            "weaknesses": {
                "value": "The novelty of the proposed architectural modifications is limited. Both the efficient SO(2) convolution and S^2 activation are from eSCN, while the attention re-normalization and layer normalization are more like engineering tricks. Among these differences from Equiformer, the eSCN SO(2) convolution plays an essential role in enabling the use of irreducible representations of higher degrees, and the S^2 activation also replaces all non-linear activations. In fact, these design strategies should be mainly credited to the eSCN work.\n\n*******************************************Post Rebuttal *********************************************\n\nThank the authors for the response. I choose to keep my positive evaluation and hope the authors carefully include the newly added discussion and results in the final version of this paper."
            },
            "questions": {
                "value": "See the comments in the Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Reviewer_c4fs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767463088,
        "cdate": 1698767463088,
        "tmdate": 1700823288899,
        "mdate": 1700823288899,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6lghXkW0AA",
        "forum": "mCOBKZmrzD",
        "replyto": "mCOBKZmrzD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_3vjk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_3vjk"
        ],
        "content": {
            "summary": {
                "value": "EquiformerV2 is proposed to improve the efficiency of Equiformer on higher-degree tensors. \nTo achieve this, original tensor product (TP) with spherical harmonics is changed to eSCN convolution which can reduce the complexity from $O(L^6)$ to $O(L^3)$.\nBesides, three archtecture module is replaced to improve the performance in attention normalization, nonlinear activation and layer normalization."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The empirical results of EquiformerV2 is great. It achieves SOTA performance on OC20 and OC22, where higher-degree tensor shows great improvement. Meanwhile, the efficiency is denoted in Figure 4 showing that EquiformerV2 can has better efficient ability than eSCN."
            },
            "weaknesses": {
                "value": "The modification of proposed architecture is similar to the previous Equiformer. Although the ablation studies show the improvement of proposed modules, the results on QM9 is similar compared to Equiformer."
            },
            "questions": {
                "value": "Minor issue:\nThere is a double citation. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision (ECCV), 2016a."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796622148,
        "cdate": 1698796622148,
        "tmdate": 1699636353326,
        "mdate": 1699636353326,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u52mzorhuD",
        "forum": "mCOBKZmrzD",
        "replyto": "mCOBKZmrzD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_Joeu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3928/Reviewer_Joeu"
        ],
        "content": {
            "summary": {
                "value": "The authors propose EquiformerV2, which incorporates eSCN convolutions to efficiently include higher-degree tensors and introduces three architectural improvements: attention re-normalization, separable $S^2$ activation, and separable layer normalization. These enhancements allow EquiformerV2 to outperform state-of-the-art methods across OC20 and OC22."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- One of the significant contributions of this paper is the comprehensive experiments across OC20, OC22, and QM9. And EquiformerV2 achieves the state-of-the-art result over OC20 and OC22. The authors deserve commendation for their efforts in this aspect.\n- The use of attention re-normalization, separable $S^2$ activation, and separable layer normalization is novel."
            },
            "weaknesses": {
                "value": "Major:\n- Although the authors did a fantastic job on the experiments, EquiformerV2 is an incremental improvement over existing methods of both eSCN and Equiformer w.r.t. theory. And the novelty lies in those three specific techniques and enhancements. To see if these techniques are generalizable, I would like to see the ablation study of attention re-normalization, separable $S^2$ activation, and separable layer normalization, respectively, on the QM9 dataset like what the authors did in Table (a) for OC20.\n\nMinors:\n- Equation (2) in Appendix A.1: Use $\\ddots$ instead of $\\dots$\n- Equation (4) in Appendix A.3: Commonly, the left side of an equation is used for assigning new notation. I recommend write $D^{(L)} = D^{(L)}(R_{ts})$ and $\\tilde{x}_s^{(L)} = D^{(L)} x_s^{(L)}$ for a degree $L$ before Equation (4)."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3928/Reviewer_Joeu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3928/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699394355830,
        "cdate": 1699394355830,
        "tmdate": 1699636353241,
        "mdate": 1699636353241,
        "license": "CC BY 4.0",
        "version": 2
    }
]