[
    {
        "id": "hqizcwAyhY",
        "forum": "TOWdQQgMJY",
        "replyto": "TOWdQQgMJY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_jsKc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_jsKc"
        ],
        "content": {
            "summary": {
                "value": "In this study, the authors investigate the limitations of Text-guided diffusion models (TDMs) and propose a novel approach called SAGE to systematically explore and understand these failures. TDMs are commonly used for image generation but can exhibit unexpected issues. The study identifies four key failure modes that have not been extensively studied before: \n1) TDMs can generate images that fail to accurately represent the semantics of the input text prompts. The authors discuss the causes and potential solutions for this issue.\n2) Some regions in the latent space of TDMs lead to distorted images, regardless of the text prompt. This suggests that certain parts of the latent space are not well-structured. \n3) Latent samples can produce natural-looking images that are unrelated to the given text prompt, indicating a potential misalignment between the latent and prompt spaces.\n4) The addition of a single adversarial token embedding to input prompts can lead to the generation of various specified target objects with minimal impact on CLIP scores, highlighting the fragility of language representations.\n\nOverall, the SAGE method efficiently explores both the discrete language space and the complex latent space, shedding light on these TDM failure modes and offering insights into potential solutions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work is interesting and systemically studying the failure mode of text to image generation model is an important and sometimes overlooked area of research.\n2. Comprehensive human study provides important validation on the results."
            },
            "weaknesses": {
                "value": "1. It's well known that text image generative model can not handle multiple objects especially when keywords are relational and containing novel actions, for instance, \u201cA photo of a cat fighting a fish\u201d. If it's a novel scenario that was not seen in the training set, text to image models often produce only one of the subjects or some blended versions.\n\n2. Most of the proposed failures seem contrived, which are not really issues concerning day to day usage of text-to-image models, especially the ones in latent space, where explicitly optimization/distortion need to be performed on the latent vector, such that it will produce distort the image. If sampled naturally, this event is very unlikely to happen.\n\n3. The arguments are very hand-waving. Not enough evidence/details are provided to support the claims. \n    1) \"Furthermore, we demonstrate that these failure examples are not isolated spots in the latent space; rather, they exist as connected regions, wherein any sample will generate similar distorted images (see Supp. B.3). It shows that these distorted images can be generated with meaningful probability under Gaussian sampling, and the probability can be computed by the probability density function.\"\n    No calculation has been shown/referred to neither in the main body nor the appendix on the probability. In addition, the paper demonstrated only QQ-plot and statistic of three prompts. It's unclear whether it's a generalized phenomenon. \n     2)  \"Tab. 2 compares the normalized size of these failure regions and the probability of ...\" No details are provided on how the failure regions were calculated/estimated. How exactly the algorithms in PoseExaminer is adopted remains questionable.\n\n\n\nOne side note but not the main concerns I have on this paper: the presentation lacks structure and appears to be very messy. The authors seem to be ambitious in delivering many things all at once but failed to fufill any of the promises."
            },
            "questions": {
                "value": "1. Do models hidden behind APIs also suffer the same failure modes? \n2. Instead of using the proposed optimization method to \"failure mode\", which is not of much significant value, why not use it to show the capability of steering/manipulation of the generated images?\n3. What happens if prompt engineering was applied, would it impact the way it fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1931/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1931/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1931/Reviewer_jsKc"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697674596495,
        "cdate": 1697674596495,
        "tmdate": 1700688644742,
        "mdate": 1700688644742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LZ2Pr8gPrI",
        "forum": "TOWdQQgMJY",
        "replyto": "TOWdQQgMJY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_6EFD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_6EFD"
        ],
        "content": {
            "summary": {
                "value": "Text-guided diffusion models (TDMs) are prone to unexpected failures, such as generating incorrect images from natural-looking text prompts or producing inconsistent images from the same text with different latent variable samples. To address this, the study introduces SAGE, an adversarial search method that explores TDMs' prompt and latent spaces to identify failures, using image classifiers for guidance and human verification for accuracy. The investigation reveals four main failure modes, highlighting issues with semantic capture, latent space structure, prompt-latent misalignment, and the fragility of language representations, suggesting avenues for future improvement."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Proposed a smart adversarial search algorithm to identify failures in TDMs, finding \u201creasonable\u201d tokens, texts, and latent codes to input into the TDMs, leading to failure generations.\n\n- Here, \u201creasonable\u201d latent code implies that we draw the code close to N(0,I), and the text should be human-readable. This proposed adversarial attack is not only fair to the TDMs but also practical for users.\n\n- The algorithm demonstrates an intelligent method (using residual connections) to avoid backpropagating through the entire extensive diffusion model, thereby enhancing computational efficiency.\n\n- The text is well-composed, accompanied by a self-explanatory figure delineating the overall pipeline.\n\n- The system is fully automated and reinforced by human evaluation."
            },
            "weaknesses": {
                "value": "- Given that the pipeline heavily relies on the robust classifier, I'm curious if this means the SAGE system will primarily detect the blatantly poor cases, while struggling to identify the more subtle, not-so-good ones.\n\n- While SAGE employs an ensemble of classifiers, it still seems akin to a handful of neural classifiers trained on the same dataset. True, one might outperform another, but I'm inclined to think that these classifiers would exhibit similar decision boundaries."
            },
            "questions": {
                "value": "1. Q1: \n- I'm somewhat puzzled by the SSR(A) in Table 1. It seems slightly biased in favor of the SAGE method, considering it's directly optimized based on this metric.\n\n2. Humans and automatic systems have different perceptions. \n\n- Q2.1: Would you attribute the reason SSR(H) isn't a full 100% to SAGE generating some false positives, or is it more about the perceptual differences among the observers?\n\n- Q2.2: As I mentioned earlier, my concern is that SAGE may only identify blatantly incorrect instances. From what you've observed, were there moments where you thought, \"That's clearly a failure, but SAGE didn't recognize it in the samples\"? If such instances occurred, could you provide some insights into why that might be?\n\nOverall this is an excellent work, I can\u2019t wait to try your demo."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638000514,
        "cdate": 1698638000514,
        "tmdate": 1699636124019,
        "mdate": 1699636124019,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E70s6ltZq6",
        "forum": "TOWdQQgMJY",
        "replyto": "TOWdQQgMJY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_6z3X"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_6z3X"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an automatic way of detecting failure text prompts in text-guided diffusion models (TDMs). A robust image classifier based surrogate loss is proposed to detect accurate failure cases due to diffusion models. In addition, to deal with vanishing gradient issue, the authors apply approximate gradients to back-propagate, via residual connection. Adversarial based approach helps to identify the natural text prompts (non-outlier) that cause the failure cases. The experimentation results show the model is efficient in finding the failure cases (SSR), and evaluated via human annotation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think the task the paper is presenting is novel and is important in the research of TDMs. The authors managed to present thorough experimentation and analysis on evaluating the proposed model performance. The presented samples do show the effectiveness of model in identifying the true failure cases from natural text prompts."
            },
            "weaknesses": {
                "value": "1. Overall, while the paper is one of the first to target such a problem in finding actual failure in TDMs, the problem itself is rather similar to some of the previously well studied tasks, with adversarial-based approaches. The discriminator here is to identify and remove the irrelevant feature in the latent space, so that it becomes task oriented. Such an approach has been used in tasks such as fair classification. \n2. Though various evaluation being conducted, it is hard to measure the effectiveness of each proposed component in the model. To better connect the intuition of each contribution of the work and the actual analysis/effectiveness, it would be great to have some ablation study."
            },
            "questions": {
                "value": "1. As many findings are presented in the paper, what would be some potential solution to handle these failure cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805039951,
        "cdate": 1698805039951,
        "tmdate": 1699636123952,
        "mdate": 1699636123952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ywG14no6iN",
        "forum": "TOWdQQgMJY",
        "replyto": "TOWdQQgMJY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_5bJZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1931/Reviewer_5bJZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a method called SAGE to search natural and human-understandable texts that text-guided diffusion models (TDMs) cannot generate images correctly for the first time. This method explores the discrete prompt space and the high-dimensional latent space to discover undesirable behaviors automatically. This method utilizes algorithms of adversarial attack and image classifiers as surrogate loss functions. To generate natural prompts, the authors use large language models (LLMs) like LLaMA to search for suitable prompts. The authors conduct experiments on several metrics to demonstrate the effectiveness. The authors also conclude 4 different failure types of TDMs by analyzing the results of failure examples."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe topic is meaningful for researchers to understand the failures of diffusion models. Natural prompts are closer to real-world scenarios and are more beneficial for improving the robustness of TDMs.\n\n2.\tThis work further analyzes the deeper causes and possible solutions through the structure of TDMs and the corresponding language features. These discussions appear to be both comprehensive and effective.\n\n3.\tThe paper is well-structured and easy to follow. The procedure is demonstrated well in Figure 2 and its description. The author provided detailed descriptions of the method and experiments."
            },
            "weaknesses": {
                "value": "1.\tThe authors do not demonstrate their method in pseudo-code. The code is not included in the supplement material either. Would the authors demonstrate their method in pseudo-code?\n\n2.\tTime cost is not shown in the paper.\nIt seems time-consuming to run a LLaMA and an ensemble of classifiers simultaneously in the attack. How much GPU memory and how long does it take to find an example?\n\n3.\tThe detail of human evaluation is missing. \nThe paper doesn't demonstrate differences in the ratings of different human evaluators. How does the author handle rating differences and assess the accuracy of human evaluators?\n\n4.\tThe metric needs to be clarified.\nCould the authors further explain why the Non-Gaussian Rate (NGR) is reported? What is the purpose of this metric in the experiments?"
            },
            "questions": {
                "value": "1.\tHow much GPU memory and how long does it take to find an example?\n\n2.\tCould the authors further explain why the Non-Gaussian Rate (NGR) is reported? What is the purpose of this metric in the experiments?\n\n3.\tHow does the author handle rating differences and assess the accuracy of human evaluators? Could the authors further explain the human evaluation process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1931/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828627523,
        "cdate": 1698828627523,
        "tmdate": 1699636123876,
        "mdate": 1699636123876,
        "license": "CC BY 4.0",
        "version": 2
    }
]