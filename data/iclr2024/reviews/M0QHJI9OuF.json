[
    {
        "id": "hGdUYmdjba",
        "forum": "M0QHJI9OuF",
        "replyto": "M0QHJI9OuF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_PuQi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_PuQi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a Trojan fairness attack named TrojFair. As its name suggests, TrojFair attacks the victim model in a Trojan manner, and it not only degrades the model's accuracy but also its fairness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. As its main contribution, this paper proposes a backdoor fairness attack algorithm that outperforms previous ones.\n2. The attacker\u2019s objectives are reasonable and the problem statement is clear. I believe the authors have properly formalized the fairness attack problem and found a good way to analyze this problem."
            },
            "weaknesses": {
                "value": "1. **A lack of significance.** According to the authors\u2019 introduction to Trojan poisoning attacks (in section 2.1), this type of attack seems to be less practical than other adversarial attacks. My reasons include:\n\n+ Trojan attacks need to add a tiny patch (which is perceptible by humans) to the target image, while typical adversarial attacks (both white/black boxes) are imperceptible.\n+ Trojan poisoning attacks, together with other data poisoning-like attacks, need to modify the target model\u2019s training data. I believe the applicable scope of this type of method is relatively narrow.\n+ Most of the related works in section 2.1 are out of date.\n\n2. In Table 5, the baseline is proposed in 2017, which greatly reduces the persuasiveness of the corresponding results."
            },
            "questions": {
                "value": "1. In the introduction section, the authors mentioned the \u201ctrade-off between accuracy and fairness\u201d, which is not a well-known term in the ML community. Could the authors briefly explain this term? \n+ (Optional) In my opinion, accuracy is more important than fairness, at least in the scenarios mentioned by the authors (e.g., job recruiting tools, facial recognition systems, and the recognition systems in self-driving cars). I think that low accuracy in some scenarios might cause fatal problems of serious consequences. (I am just curious about this topic. No relation to my rating.)\n2. In the abstract, the authors mentioned that TrojFair is model-agnostic, while the \u201cAttacker\u2019s Knowledge and Capabilities\u201d paragraph claims the authors\u2019 \u201cfocus is on more practical black-box model backdoor attacks\u201d. \n3. What is the difference between the backdoor and Trojan attacks? I think these two terms are equivalent. Both terms are used in this paper but seemingly the authors only provide a definition of the Trojan attack.\n4. How to obtain the target and untarget groups (e.g., pale/dark skin) in non-tabular data? It seems to be pretty hard work to do.\n+ Besides, I suggest using non-target instead of untarget here, since the latter could be easily confused with the \u201cuntargeted attack\u201d. Another reason for choosing non-target is that I would interpret untarget(ed) as \"do not have a target\", while non-target means \"not belonging to the selected target\".\n\nI am happy to discuss the questions with the authors. I would like to raise my score if my concerns are addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper proposes an adversarial attack algorithm that could possibly be deployed to increase the bias or unfairness of existing models."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698202592988,
        "cdate": 1698202592988,
        "tmdate": 1699637034858,
        "mdate": 1699637034858,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fIXqHHvClU",
        "forum": "M0QHJI9OuF",
        "replyto": "M0QHJI9OuF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_oWkK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_oWkK"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced TrojFair, a backdoor attack that affects model fairness. The attack is model agnostic and capable of aiming its bias at certain groups with triggered inputs. This goal is attained by optimizing standard types of triggers from backdoor attacks to misclassify the target groups with trigger, to  classify correctly the non-target groups with trigger, while at the same time maintaining model performance when to trigger is present. Experiments are performed on three datasets using multiple neural network architectures."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of backdooring a model w.r.t. a fairness end-goal is interesting and relevant for the ICLR community.\n- The three steps of the method are reasonable and well-justified, both intuitively and experimentally.\n- The paper is overall well-written."
            },
            "weaknesses": {
                "value": "# Novelty and prior work\n\n- The paper does not seem to cite recent work that is very similar to the proposed contribution ([Un-Fair Trojan](https://ieeexplore.ieee.org/document/10062890), [[Solans et al., 2020](https://arxiv.org/abs/2004.07401)], [SSLJBA](https://www.researchgate.net/publication/373129185_SSLJBA_Joint_Backdoor_Attack_on_Both_Robustness_and_Fairness_of_Self-Supervised_Learning)). It is unclear how TrojFair is different from these and how it would perform comparably. To me, this constitutes the main limitation of the paper.\n\n# Soundness\n\n- I am not convinced that standard poisoning attacks applied to just the group of interest would perform poorly for (lack of) fairness goals. The paper claims they would, but does not show results to that effect.\n- The main optimization objective (Eq. 4) could use some polishing, like providing the mathematical expression of the mask applied, or writing it in such a way that parameter $\\delta$ used in the text appears.\n- The Background section states that \"Trojan poisoning attacks in deep learning involve embedding a trigger into each training sample, creating poisoned datasets.\" This is not exact, as most backdoor attacks only poison a small percentage of the training set, which does not prevent them from achieving sometimes even close to 100% attack success rates.\n\n# Minor points\n- \"BadNet\" -> \"BadNets\""
            },
            "questions": {
                "value": "- How is the bilevel optimization problem in Eq. (4) solved? From the description, it sounds like the model weights $w$ are fitted first, followed by the trigger optimization under fixed weights $w$.\n- What is the impact of the trigger initialization on the transferable optimization step?\n- How are the hyperparameters of the attack set? They seem to vary considerably depending on the attack trigger (i.e., $\\lambda_1$ for BadNets and Blended triggers).\n- What is the vanilla poisoning attack in Sec. 5.1 and how is it applied?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774876111,
        "cdate": 1698774876111,
        "tmdate": 1699637034742,
        "mdate": 1699637034742,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "t8FSNKwuHN",
        "forum": "M0QHJI9OuF",
        "replyto": "M0QHJI9OuF",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_7Q6Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8320/Reviewer_7Q6Y"
        ],
        "content": {
            "summary": {
                "value": "The paper's goal is to create a fairness attack for deep learning models. The authors introduce TrojFair, a model-agnostic method that employs Trojan fairness attack techniques.  TrojFair employs a Trojaned model that functions accurately for benign inputs. It inserts a trigger in the samples of the target group and changes their labels, and adds a trigger into untarget group samples without altering their labels. It also refines the trigger based on a surrogate model to amplify accuracy disparities among different groups. The paper supports its approach with experiments and ablation studies to showcase the attack's performance and the impact of TrojFair's components."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper proposes a Trojan fairness attack that only acts maliciously for target groups.\n* The description of the attack is clear and easy to follow.\n* Several experiments have been conducted to demonstrate the performance of TrojFair."
            },
            "weaknesses": {
                "value": "* The attack focuses on the scenario where the attacker is only interested in one target class, and it is unclear whether it can be directly extended to multiple target class cases.\n* The computational complexity associated with training the surrogate model may be considerably high, and it remains uncertain how the surrogate model affects trigger design when it is not accurate.\n* The transferable optimization requires the knowledge of the training samples $\\hat{D}$, which may not be obtained in practice."
            },
            "questions": {
                "value": "In the fairness-attack transferable optimization module, is the surrogate modeling training before or after the global model training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8320/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784848318,
        "cdate": 1698784848318,
        "tmdate": 1699637034634,
        "mdate": 1699637034634,
        "license": "CC BY 4.0",
        "version": 2
    }
]