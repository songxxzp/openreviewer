[
    {
        "id": "M0Nvtt7pZY",
        "forum": "UMfcdRIotC",
        "replyto": "UMfcdRIotC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_GEWc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_GEWc"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on improving causal explanations, specifically the counterfactual explanation. The authors propose to leverage LLMs to generate the counterfactual input corpus, use the generated corpus to train a counterfactual representation model, and match the input and its corresponding counterfactual representation to generate the causal explanations. Experiment results show that the authors' method outperforms all previous matching baselines, representing a promising explanation ability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper provides detailed proofs and descriptions of the proposed method.\n2. The experiment results are reliable with the comparison between various baselines and models.\n3. The authors also construct a benchmark based on the findings of LLM's ability to generate counterfactual examples, which I think is a good contribution to the XAI community."
            },
            "weaknesses": {
                "value": "The description of the proposed method in Section 3 is confusing and not easy to understand. I think the authors should rephrase Section 3 with a general description of the proposed causal model. \n\nAbout Eq.(2), the authors use the difference in the model's prediction before and after the treatment as the treatment effect, which, in my opinion, is not robust when the model's output confidence is flat (or the uncertainty is high). This will affect the method's performance on small models like BERT."
            },
            "questions": {
                "value": "1. I cannot find an accurate definition of \"causal model\". Does the author use the representation generated by a language representation model optimized with Eq.(5) with generated counterfactual and matched examples, then calculate the matching value in Eq.(4), and use this value to calculate Eq.(2) as a causal model?\n\n2. How does the generative approach work? If (1) is true, does the causal model include the generative approach?\n\n3. There are two versions of ChatGPT (GPT-3.5 and GPT-4); which one did the authors use in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Reviewer_GEWc",
                    "ICLR.cc/2024/Conference/Submission1870/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697560667415,
        "cdate": 1697560667415,
        "tmdate": 1700546205090,
        "mdate": 1700546205090,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FiPOyDn5uD",
        "forum": "UMfcdRIotC",
        "replyto": "UMfcdRIotC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_wvZe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_wvZe"
        ],
        "content": {
            "summary": {
                "value": "This paper presents two methods for explaining the predictions of Natural Language Processing (NLP) models, focusing on the use of counterfactual approximations (CFs). The first is a Counterfactual Generation approach, where a large language model (LLM) is prompted to change a specific text concept while keeping others the same. The second is a Matching method that identifies text with similar properties within a dataset. The authors establish the value of approximating CFs for offering _faithful_ explanations and illustrate their techniques' applicability on several models. They further improve the ability to provide explanations using top-K matching. Furthermore, they highlight the potential of LLMs to create new benchmarks for NLP model explanations. The authors present theoretical and empirical evidence to support their research and propose further areas of study."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper contributes to the field of NLP model interpretability by introducing two practical methods for model-agnostic explanations, which could improve our understanding of model predictions.\n- The authors back their theoretical constructs with extensive experimental results, though the reliability of these methods depends on the specific conditions in which they are applied (such as access to a candidate set that offers good matching candidates).\n- The concept of Order-faithfulness is an innovative criterion for explanation methods, potentially providing valuable insights into the relative impact of different concepts on model predictions, although it would need to be tested across various contexts and model types to ensure its broad applicability."
            },
            "weaknesses": {
                "value": "- The first method proposed, Counterfactual Generation, is computationally expensive and may be infeasible in scenarios requiring real-time explanations.\n- Although Matching is faster than CF Generation, it might not be as accurate for all situations, especially when the matching candidate set does not sufficiently represent the input data. It would be great if the authors performed some ablation studies (reducing the quality of the matches in the candidate sets to show how much the performance degrades).\n- It's unclear how these techniques would perform on models trained on very niche tasks, which could inherently limit the possible counterfactuals, especially where such attributes may be hard to define beforehand. Both counterfactual generation and matching approaches assume that we have a set of attributes for which we wish to examine whether a model is paying attention to those. However, how does this approach work for open ended tasks (which is where LLMs are primarily being used) or tasks with a large number of classes, where generating counterfactuals (or finding matches) may be inherently difficult?\n- It's unclear how these methods would handle situations with multilayered complexities, such as nested counterfactuals, where counterfactual changes to one concept might trigger changes to other related concepts. The paper also does not extensively address scenarios where counterfactual approximations could result in impossibilities or logical contradictions, potentially limiting the breadth of their application."
            },
            "questions": {
                "value": "I would appreciate if the authors discuss the issues I brought up in the previous section.\n\nAdditionally, could you explain the difference between Random Match and Approx again? It's not that clear from the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Reviewer_wvZe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838156359,
        "cdate": 1698838156359,
        "tmdate": 1700612409558,
        "mdate": 1700612409558,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Rsh1QVbsBs",
        "forum": "UMfcdRIotC",
        "replyto": "UMfcdRIotC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_32rU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_32rU"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the use of large language models (LLMs) for creating counterfactual examples to explain NLP classification model predictions. Two methods are proposed. The first involves directly using LLMs to generate counterfactual examples, altering only one aspect/concept of the input while maintaining the rest. The second method entails a matching process to discover approximate counterfactuals from a pre-defined candidate set. The study reveals that the matching approach, utilizing a specially trained feature extractor, outperforms strategies using pre-trained LMs as feature extractors and other baselines in the CEBaB benchmark.\n\n------------------------------------\nUpdate after discussion with authors:\n\nThe discussion with the authors and the information provided in the updated manuscript made me more convinced of the applicability of the proposed methods, so I raised my score to 6."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is overall well-written, with a well-defined research question;\n\n\n2. In addition to direct counterfactual generation via the LLM, the authors introduce an efficient, matching-based approach for identifying approximate counterfactuals from a pre-defined candidate set. Though this method doesn't perform as well as the direct LLM generation, it outperforms past baseline methods and is considerably more efficient than employing the LLM directly for each instance. The exploration of how to efficiently generate counterfactual examples using LLMs is an intriguing aspect of the paper."
            },
            "weaknesses": {
                "value": "1.\t\n\nThe paper mostly follows the setting of the work of CEBaB, including causal analysis, the approximated counterfactual method, and the evaluation. While some theoretical analysis is provided in Section 3.1, it mainly argues why the approximated counterfactual method which is initially proposed in the CEBaB is better than others. Underlining this, the first concern is that the paper's contribution appears to be limited to the proposal of two LLM-based approximated counterfactual methods that perform better in CEBaB's causal framework. Given the powerful ability of LLM, using it can better generate counterfactual examples (that only change one concept of the input while keep other aspects unchanged) is not very surprising.\n \nThe second concern is the limited applicability of the proposed matching method. The use of the matching method under the CEBaB setting requires pre-defined or pre-identified concepts/factors, such as Food (F), Service (S), Ambiance (A), and Noise (N) in restaurant reviews. However, these concepts may not always be available or readily identified in many real-world NLP scenarios. Given that the paper exclusively focuses on using LLMs to generate counterfactual examples in this particular setting, its broader applicability and contribution are questioned.\n\n2.\t\n\nAs indicated by the results in Table 2, the matching method (causal model) demonstrates only a slight improvement over the Approx baseline, especially when K=10. While the matching method is the most novel part of the paper IMO, the fact that its performance isn't significantly superior to the Approx baseline brings its practical importance into question.\n\nIn summary, the marginal performance improvement and the limited applicability of the \nproposed methods make me tend to reject the paper at this moment. \n\nHowever, I am open to further discussions and potential rebuttals from the authors that may address these concerns."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I do not find any particular ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Reviewer_32rU",
                    "ICLR.cc/2024/Conference/Submission1870/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698891372087,
        "cdate": 1698891372087,
        "tmdate": 1700624590067,
        "mdate": 1700624590067,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "157nx4HIZf",
        "forum": "UMfcdRIotC",
        "replyto": "UMfcdRIotC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_9xHf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1870/Reviewer_9xHf"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes two methods for approximating Counterfactuals in a model-agnostic way. The first one is to utilize an LLM to change attributes during inference time. The second method is to find Counterfactuals through efficient matching. In order to allow efficient matching, the paper developed a novel language representation learning method specifically for encoding counterfactuals. Such representation is learned through contrastive loss that maximizes the similarity of approximate counterfactuals and minimizes similarities of misspecified Counterfactuals. Both methods achieve better performances than prior works. The paper also released a dataset for evaluating NLP explanation techniques."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposed an efficient and novel matching technique for finding Counterfactuals and provided strong theoretical and practical evidence that Counterfactuals are good explanations. \n2. Counterfactuals generated using this method are more order-faithful and comprehensive than prior work.\n3. Detailed ablation study to demonstrate the effect of each component in the method."
            },
            "weaknesses": {
                "value": "1. The results are only on one dataset CEBaB. The experimental section would be more convincing if more experiments were done on a wider range of datasets. \n2. The concepts are pre-defined, which can be a limiting factor to the comprehensiveness of the Counterfactuals generated."
            },
            "questions": {
                "value": "1. If the matching candidate set doesn't exist, do you generate them given the concepts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1870/Reviewer_9xHf"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1870/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699378413573,
        "cdate": 1699378413573,
        "tmdate": 1699636117244,
        "mdate": 1699636117244,
        "license": "CC BY 4.0",
        "version": 2
    }
]