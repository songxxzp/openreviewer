[
    {
        "id": "XWW9ODWetS",
        "forum": "ikX6D1oM1c",
        "replyto": "ikX6D1oM1c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_ijNf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_ijNf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a neural framework for generalized causal sensitivity analysis that aims to achieve lower and upper bounds for some causal queries when exact identification is not possible due to unobserved confounding. Specifically, the framework leverages two normalizing flows to encode the latent distributions that are compatible with the observed ones and optimize the causal quantities of interest and can be used to approximate previous sensitivity analysis models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The writing is clear, and the paper is easy to follow.\n2. The proposed neural network-based causal sensitivity analysis method is effective and general, which is compatible with previous studies like MSM and f-sensitivity models and can be easily extended to other models by modifying the constraints that specify the strength of unobserved confounding."
            },
            "weaknesses": {
                "value": "The experiments, to some extent, appear to be lacking in comprehensiveness. The semi-synthetic datasets utilized in this study are exclusively derived from the MIMIC-III dataset. The findings derived from this single source may not offer an adequate illustration of the model's performance."
            },
            "questions": {
                "value": "I noticed that the authors apply the augmented Lagrangian method to incorporate the sensitivity constraints in the optimization process. I wonder to what extent could the constraints be satisfied since the constraint now becomes a soft one that may well be violated. It is possible that the effective constraint parameter $\\Gamma$ achieved by the optimization significantly deviates from the intended value, which may make the final bounds less useful. Additionally, I am concerned about the stability of training the normalizing flows, as instability in this aspect could further exacerbate the situation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Reviewer_ijNf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697877855507,
        "cdate": 1697877855507,
        "tmdate": 1699636295812,
        "mdate": 1699636295812,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "M7wJb8Fbk9",
        "forum": "ikX6D1oM1c",
        "replyto": "ikX6D1oM1c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_MpUG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_MpUG"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a generalized sensitivity analysis framework that is compatible with many different sensitivity models, including the marginal sensitivity model, f-sensitivity model, and Rosenbaum's sensitivity model. The framework is suitable for different treatment types and different causal queries. The authors propose to learn the latent distribution shift with two separately trained conditional normalizing flows."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The concise summary of sensitivity models enhances the paper's readability and flow.\n- The authors introduce a novel learning strategy to model the latent distribution.\n- Experiments with both synthetic and real-world data are used to demonstrate the validity and effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- It is not very obvious how does the bounds the proposed framework compares with some existing works such as GMSM. \n- The section 5.1 might be a little hard to follow. Please find some questions I have below."
            },
            "questions": {
                "value": "- Regarding the color coding in equation (4), does it indicate the parameters for the optimization problem? Does the right supremum also maximize over $P(U|x,a)$?\n- I'm not fully understand the two-stage procedure. Could you to provide a more detailed explanation about replace the right supremum with \n fixed of $\\mathbb{P}^*(U|x,a)$ and $\\mathbb{P}^*(Y|x,u,a)$?\n- In relation to the optimization problem presented in equation (5), are there specific constraints placed on the functional $\\mathcal{D}_{x,a}$ to ensure that the global optimal can be achieved?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3435/Reviewer_MpUG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698651593251,
        "cdate": 1698651593251,
        "tmdate": 1699636295740,
        "mdate": 1699636295740,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wguZ6WjIAF",
        "forum": "ikX6D1oM1c",
        "replyto": "ikX6D1oM1c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_9LA1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_9LA1"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes NeuralCSA, a framework for performing causal sensitivity analysis, i.e., partially identification of a causal functional under assumptions on the unmeasured confounders. They propose a 2-stage training procedure, modeling the latent distributions using normalizing flows."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper overall is well-written and easy to understand. I found the motivation and setup to be clear. I also appreciated comparisons to existing sensitivity models.\n- The GTSM framework subsumes many of the existing sensitivity models and thus is more generally applicable. In principle, the framework also applies to arbitrary functionals (e.g., quantiles) of the interventional outcome distributions.\n- The clarity of the two-stage procedure can be improved (see Weaknesses section), but overall, the procedure is simple and easy to follow. It is also nicely motivated using Theorem 1."
            },
            "weaknesses": {
                "value": "- I found Sec 5.1 and 5.2 difficult to read and I think clarity can be improved. What confused me initially was that you suggest fixing $P^*(U|x, a)$ but then the $\\sup$ in Eq. 5 is also over the distributions $p(u|x, A)$. Reading it further, the sup is only for $A \\neq a$ but I think clarifying that you only fix for the treatment $a$ that enters into $Q$ would be useful. Maybe this is obvious, but it will still make it easier to understand what is being optimized over in the $\\sup$.\n- It would also be nice to have some intuition of the proof of Theorem 1. Also, the invertible function $f^*$ would depend on the fixed $P^*$. Does certain distributions $P^*$ make it easier to determine $f^*$. In practice, how should you determine which $P^*$ to fix?"
            },
            "questions": {
                "value": "See weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698848481859,
        "cdate": 1698848481859,
        "tmdate": 1699636295674,
        "mdate": 1699636295674,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qYUBbQR2qS",
        "forum": "ikX6D1oM1c",
        "replyto": "ikX6D1oM1c",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_i97n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3435/Reviewer_i97n"
        ],
        "content": {
            "summary": {
                "value": "The authors provide a framework for generalized causal sensitivity analysis, an approach that subsumes three previous methods and provides additional advantages."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The key advantage of the authors' approach (according to the authors) is that it is much more widely applicable than any other single approach (which typically focus on a specific sensitivity model, treatment type, and causal query).\n\nThe paper is well-written, and it provides substantial background about existing methods for causal sensitivity analysis.\n\nThe experiments appear consistent with the existing literature, well-motivated, and useful."
            },
            "weaknesses": {
                "value": "The generality of the approach appears to come at a substantial cost in terms of complexity (with a corresponding potential for unexpected sources of error, bias, or misspecification). The single advantage over MSM appears to be allowing causal queries with multiple outcomes. \n\nIt is unclear the extent to which alternative (non-neural) implementations of the GTSM are possible. The paper would be improved by clearly describing what advantages the neural implementation provides over alternatives.\n\nIt seems somewhat odd to cite D'Amour 2019 (an excellent paper about a very specific topic) for the idea that \"unobserved confounding often renders causal inference challenging.\" That has been known for more 50 years, going back at least to Reichenbach's common cause principle."
            },
            "questions": {
                "value": "Are alternative (non-neural) implementations of the GTSM possible? What advantages does the neural implementation provides over alternatives? How does the complexity (e.g., number of hyper-parameters and other implementation choices) of NeuralCSA compare to MSM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3435/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699144319401,
        "cdate": 1699144319401,
        "tmdate": 1699636295597,
        "mdate": 1699636295597,
        "license": "CC BY 4.0",
        "version": 2
    }
]