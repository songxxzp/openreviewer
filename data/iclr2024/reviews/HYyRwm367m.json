[
    {
        "id": "cGsakfg5bP",
        "forum": "HYyRwm367m",
        "replyto": "HYyRwm367m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a modification of VQ-VAE called Semantic Vector-Quantized Variational Autoencoder (SVQ), which provides semantic discrete object representation by learning block-level factors, leveraging advances in object-centric learning. Different from patch-based VAE models, SVQ first applies slot attention to obtain object-level codebooks, then splits the slots into blocks following a binding mechanism to represent semantic factors. In comparison with previous methods, the model demonstrates better image sampling quality considering object fidelity. The proposed representation also shows promising performance on several downstream tasks in the out-of-distribution setting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "\uff081\uff09The paper is overall well-written and the idea of learning semantic representation is well-motivated.\n\uff082\uff09Extensive and thorough experiments have been carried out to verify the effectiveness of the approach on multiple datasets and settings."
            },
            "weaknesses": {
                "value": "Weaknesses:\n\uff081\uff09\tNovelty: The major limitation of this paper is the lack of theoretical novelty, since it seems the idea of disentangling object-level slots into blocked-level factors as well as the autoregressive transformer decoder for image reconstruction have been introduced in prior work SysBinder. It\u2019s not elaborated on how the proposed approach is developed further in representing discrete semantics, especially when there\u2019s not a large margin between the performance of SVG and other baselines.\n\uff082\uff09\tSemantic illustration: Throughout the paper (including the name of the proposed model), the authors emphasize that the model is capable of learning semantic representation of objects such as shapes, colors, etc. However, how and what semantics are actually learned with unsupervised learning in the codebook is never illustrated or visualized, which makes the statement less convincing.\n\uff083\uff09\tExperiment: First, the specification of multiple hyper-parameters (e.g. number of blocks, dimension of vectors) of SVG and other implementation details is never found. Second, the artificial designs of the constraints of the scene (metric Generation Accuracy) as well as the odd-one-out downstream task are not explained. Third, as the authors mentioned in the section of Limitations, the evaluations are restricted within synthetic datasets with simple scenarios. It\u2019s unclear whether the proposed method will work in realistic, complex datasets, especially when the representation is object-related whose effectiveness is likely to be related to the size of the codebook."
            },
            "questions": {
                "value": "My questions are listed in weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_41Cm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758179288,
        "cdate": 1698758179288,
        "tmdate": 1700724280063,
        "mdate": 1700724280063,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YtGkIxyzt9",
        "forum": "HYyRwm367m",
        "replyto": "HYyRwm367m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for decomposing scenes hierarchically from low level factors such as color and shape to objects. The method learns a prior over these concepts which allows for generating data from the data distribution which most object-centric papers cannot do. In the the experiments they show that the representation can be used to solve downstream tasks which require reasoning about the properties of different objects in the scene. Additionally they show that the inductive biases proposed by the work leads to better FID score compared to VQ-VAE and similar generative models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper proposes an interesting idea and the experimental setup for the downstream tasks are well done. Evaluating whether the model has learned underlying generative rules of learning is a neat and, to my knowledge, novel way of probing the model. I think the idea would be impactful and the community would be interested if it works for more realistic and larger scale datasets. \n\n- The qualitative results in figure 6 where the color, position and size are disentangled are impressive even for a simple, synthetic dataset. \n\n- The paper is well written and figures are illustrative of the main idea. \n\n- Overall the experiment are well done, with reasonable baselines and clear analysis of the results."
            },
            "weaknesses": {
                "value": "- The experiments are only conducted on synthetic, simple datasets. The variants of CLEVR are procedurally generated based on disentangled factors such as shape, color, and texture so it's unclear if the factor based approach proposed by this work generalizes to more realistic scenes. \n\n- The main metric used to evaluate the method and baseline is FID. This metric seems orthogonal to the goal of object-centric decomposition. Typically object-centric papers such as Slot-Attention measure segmentation with adjusted random index (ARI). I ask for clarification on why FID is the primary metric for evaluating object-centric models. I think if generation is the primary goal, then the work should compare to generative models and VQ-VAE on more realistic generative datasets. \n\n- I would be happy to improve my score if my concerns are addressed, particularly if the method works on real, more complex datasets than CLEVR."
            },
            "questions": {
                "value": "- How sensitive are the results to the codebook size and number of blocks? It seems that for CLEVR we can a priori know these hyper-parameters, but for real-world or more complex datasets how would we set these?\n\n- It seems natural to compare the ARI with slot-attention or a variant of slot-attention. Is there a reason why the method shouldn't be compared in this manner?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_qSSD"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826123348,
        "cdate": 1698826123348,
        "tmdate": 1700699277241,
        "mdate": 1700699277241,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uyZ7EGZ9Zs",
        "forum": "HYyRwm367m",
        "replyto": "HYyRwm367m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach to perform semantic-based vector quantization and learn a corresponding codebook as well as a generative model under object-centric learning scenarios. It uses a slot-attention-based encoder to obtain N object-centric features for each image, where the hidden dimension is divided by M groups and each group shares the parameters. The M denotes the number of attributes. Further, it learns a codebook with MxK codes to quantize the features from the encoder. For every slot, each attribute would be quantized to the code with the closest L2 distance among the corresponding K codes, resulting in the final NxM quantized results for the reconstruction task. Experiments are conducted on various synthetic datasets to showcase the effectiveness of the proposed model. The results show that it is capable to capture high-level information beyond patch and generate better results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The writing is clear and easy to follow.\nThe motivation looks good to me, and the proposed idea is interesting, the codebook analysis results are impressive. The performance looks good on these synthetic datasets."
            },
            "weaknesses": {
                "value": "I'm not up-to-date to the latest slot-attention models and their performance on synthetic datasets such as CLEVR. It seems that the authors only choose a few baselines (GENESIS-v2, VQ-VAE, dVAE) for the generation quality evaluation as well as other downstream baselines. The VQ-VAE/dVAE model may requires larger model capacity to handle this tasks as they works on the local patch level. The generation results in Fig.4 is surprisingly bad, given the fact that they actually perform well on real data generation (VQGAN/DALL-E), it should be easy to reconstruct/generate good results when the hyperparameters are appropriate. I'm not sure whether the current results is convincing enough.\n\nI also have several questions in the following section, it would be great if the authors could address them. Overall I think this paper shows some interesting results and could inspire people. But the current experiments only show results on synthetic data which is somewhat weak."
            },
            "questions": {
                "value": "1. In Tab.4, the authors only compare with dVAE and VQ-VAE Indices, which makes the comparison unfair as SVQ Indices also perform bad. Why not the authors also showcase the VQ-VAE code (like Tab.5) as well as dVAE feature (using the code id to index the weight of the first layer in the decoder)?\n2. The Tab.5 seems not a fair comparison as well. VQ-VAE / dVAE are not designed for the object-centric tasks so it is expected they will not perform well on this task, on the other hand, SysBinder shows comparable and even better performance on this task.\n3. Following 3, the paper lacks of ablation studies, which makes the reader hard to understand which part plays the critical role and is useful to support the author's claim, for example, what would happen when M varies and becomes smaller than the actual number of attributes?\n4. The authors have shown some interesting codebook analysis results, would be great if the authors could also showcase whether the learned codebook is composable and can perform controllable generation (other than based on reconstruction) and by solely manipulate the code.\n5. Recently there is a paper present similar idea (group level quantization https://arxiv.org/abs/2309.15505), which may give the authors some inspiration as well, note that this is a concurrent work so I'm not asking the authors to compare with them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7823/Reviewer_gjT4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833917454,
        "cdate": 1698833917454,
        "tmdate": 1700755201649,
        "mdate": 1700755201649,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b8mPL4ZmN6",
        "forum": "HYyRwm367m",
        "replyto": "HYyRwm367m",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7823/Reviewer_cC8S"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a novel image-quantized method, called semantic vector-quantized variational autoencoder. The SVQ constructs representations hierarchically from low-level discrete concept schemas to high-level object representation. The author conducts experiments on various 2D and 3D object-centric datasets and validates the effectiveness of the SVQ."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "i) Compared with widely used VAE and VQ-VAE, SVQ models the discrete abstraction and object-level representations simultaneously. \n\nii) The proposed semantic prior based on discrete latent codes can be directly used in generation tasks, and the visualization results show it is superior compared with the baseline methods."
            },
            "weaknesses": {
                "value": "I am not an expert in the quantization area, my main works are related to self-supervised learning. So I want to ask some questions about the application of self-supervised learning.\n\ni) The patch-level quantization method VQ-VAE can maintain the geometry structure. So it is widely used in mask-image-modeling methods, such as BEiT.  Can SVQ also maintain such a geometry structure? \n\nii) If the SVQ can be applied in the general 2D-image generation methods, such as stable diffusion? I think it is a promising application of the quantization method."
            },
            "questions": {
                "value": "Refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698842647345,
        "cdate": 1698842647345,
        "tmdate": 1699636957727,
        "mdate": 1699636957727,
        "license": "CC BY 4.0",
        "version": 2
    }
]