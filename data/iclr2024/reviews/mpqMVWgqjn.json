[
    {
        "id": "XaE3cSudbJ",
        "forum": "mpqMVWgqjn",
        "replyto": "mpqMVWgqjn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4286/Reviewer_4EsM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4286/Reviewer_4EsM"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the 'inverse protein folding'  problem of designing a protein sequence that folds into a particular shape. They achieve strong results on standard benchmarks.  It draws on a number of interesting ideas, from graph neural networks to leveraging embeddings of pretraining language models, to a 'recycling' approach that updates predictions based on the current uncertainty over those predictions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper achieves really strong empirical results on a panel of common benchmarking setups. The results will definitely be of interest to the community."
            },
            "weaknesses": {
                "value": "The paper's explanation of the model is extremely difficult to understand because it is not using standard terms for deep neural networks. For example, 'knowledge' is used in a vague way, I guess, to mean leveraging pretraining? If I understand correctly, the composition of functions in section 3.1 is just describing a multi-layer neural network, but uses very verbose and non-standard notation. Further, why are the layers trained in a stagewise fashion in section 3.4 instead of standard back-propagation? It was confusing to me why you chose this, since it is much more complex to implement. Does it provide better performance?\n\nThe paper's experiments are quite careful about train-test splits, using a number of clustering-based splits that are well-established in the literature. I'm concerned about data leakage, however, from the pretrained language models used to help guide predictions for low-confidence residues in the 'knowledge' module. There models were trained across vast numbers of proteins and likely do not follow the same train-test splits as for the structure -> sequence benchmarks. As a result. The exact target sequence for structure -> sequence design may have appeared in the LM pretraining data. This may explain why the paper's method is able to increase the per-residue recovery rate so dramatically. The paper also mentions that the ESM-IF train-test split is not compatible with some of the other benchmarking setups, yet ESM-IF embeddings are used here."
            },
            "questions": {
                "value": "Can you please clarify my questions regarding train-test splits and data leakage from the 'knowledge' module?\n\nCan you please explain why you use such a non-standard training procedure?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698329587680,
        "cdate": 1698329587680,
        "tmdate": 1699636396376,
        "mdate": 1699636396376,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "V52MzjJqxT",
        "forum": "mpqMVWgqjn",
        "replyto": "mpqMVWgqjn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4286/Reviewer_KJsj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4286/Reviewer_KJsj"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces KW-Design, a novel method for protein design that iteratively refines low-confidence residues using knowledge extracted from pretrained models. The approach incorporates a multimodal fusion module, virtual MSA, recycling technologies, and a memory-retrieval mechanism to enhance performance and efficiency. The method demonstrates substantial improvements across various benchmarks, achieving over 60% recovery on datasets such as CATH4.2, CATH4.3, TS50, TS500, and PDB."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well written and exhibits a clear and logical structure.\n2. The proposed method effectively leverages the knowledge from pretrained protein sequence/structure models, resulting in notable benefits for protein sequence design.\n3. The paper includes a thorough ablation study, examining different components of the models such as recycling, virtual MSA numbers, and the pretrained model. This is crucial for gaining a deep understanding of the proposed methodology."
            },
            "weaknesses": {
                "value": "1. The code associated with the paper is currently unavailable.\n2. Given that the model relies on pretrained models, some of which have been trained on the test set utilized, there is a potential risk of data leakage.\n3. The paper predominantly employs perplexity and recovery as metrics for evaluating the designed sequences. However, there is a chance that the designed proteins may not be soluble or may not fold correctly to the given backbone. It would be beneficial for the authors to incorporate additional metrics (e.g., scTM score, solubility) in their evaluation."
            },
            "questions": {
                "value": "1. Is there any fine-tuning done on the pretrained language model used in your approach?\n2. In Section 4.3, Table 3 claims that \u201cthe predictive confidence score, an unsupervised metric, exhibits a strong correlation with recovery.\u201d Could the authors provide a more detailed analysis, perhaps including the Spearman correlation between these two values?\n3. Regarding the virtual MSA, does the sequence order affect the resulting residue embedding? If not, what criteria are used to determine the sequence order?\n4. In Section 3.3, the initial node and edge features are extracted using PiFold. Is PiFold fine-tuned or kept fixed during this process?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4286/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698573076186,
        "cdate": 1698573076186,
        "tmdate": 1699636396305,
        "mdate": 1699636396305,
        "license": "CC BY 4.0",
        "version": 2
    }
]