[
    {
        "id": "57KXGegoJC",
        "forum": "G3OCarOfxx",
        "replyto": "G3OCarOfxx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_LZqy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_LZqy"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a theoretical understanding to the empirical finding of clean generalization and  robust overfitting (CGRO). They investigate this problem through feature learning theory. They mainly focus on patch-structured dataset and one-hidden-layer CNN model. The results suggest that the network can partially learn the true feature but exactly memorize the spurious features from  adversarial examples which leads to the CGRO phenomenon. They also provide an analysis of the efficiency of classifier using representation complexity theory."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is well written overall.\n* The investigated phenomenon is interesting and worth exploring.\n* The authors reveal a three-stage training process for adversarial training dynamics, which may offer insights to the community."
            },
            "weaknesses": {
                "value": "The authors might have made enough effort to complete this work; however, I have the following concerns about this work:\nFrom a theoretical perspective:\n* The assumption of the patch data is somewhat weird to me. The realistic images are much more complicated and cannot be simplified as a patch structual. Specifically, the authors assume $\\alpha=d^{0.249}$ and $\\sigma^{-0.509}$. I am puzzled as to why they chose specific values of $0.249$ and $-0.509$? Does the real data distribution satisfy this assumption?\n\nFrom an experimental perspective:\n* More experiments are needed to prove the validity of the theory. For example, the authors only chose a ResNet9 architecture for CIFAR10. In my opinion, they should validate the theory based on larger models that are widely used in the adversarial training field, such as ResNet-50.\n* It would be better to empirically certify that the data assumption and the parameter settings are reasonable."
            },
            "questions": {
                "value": "Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698382360847,
        "cdate": 1698382360847,
        "tmdate": 1699636868576,
        "mdate": 1699636868576,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qEXOdgLC0R",
        "forum": "G3OCarOfxx",
        "replyto": "G3OCarOfxx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_EEZ9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_EEZ9"
        ],
        "content": {
            "summary": {
                "value": "This paper use one specific data distribution to show that the adversarial training suffers from robust over-fitting problem while the clean testing performance is good."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The overall quality of this paper is good. The logic is clear, and the observation in the theory is interesting. Although many existing literature tries to explain the robust over-fitting problem, this work is the first one of its type and provides very useful insights. The writing is also clear to understand."
            },
            "weaknesses": {
                "value": "My only concern of this paper is the choice of the activation function. This paper uses a cubic activation function, which is not commonly used in practice. In existing literature, e.g., Allen-Zhu et al. (2022), they consider two-layer MLP with ReLU activation function. The authors of this submission need to provide some illustration on why to use the cubic activation function. More specifically, if we replace the cubic activation function with ReLU or Sigmoid function, which part of the proof will face difficulty? I read through the proof and got a picture of the proof, but could not identify the exact place where the cubic activation function plays a crucial rule.\n\nAnother concern is that this paper provides experiments in MNIST and CIFAR-10, but without any simulation study to verify the correctness of the theorem. The authors are encouraged to use simulation (follow the exact assumption in the theory) to at least verify and visualize the observations in the theorem.\n\nZeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pp. 977\u2013988. IEEE, 2022."
            },
            "questions": {
                "value": "Please address my two concerns above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698443896143,
        "cdate": 1698443896143,
        "tmdate": 1699636868449,
        "mdate": 1699636868449,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bikkYObFV5",
        "forum": "G3OCarOfxx",
        "replyto": "G3OCarOfxx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_6rXn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_6rXn"
        ],
        "content": {
            "summary": {
                "value": "This paper offers theoretical intuitions about why clean generalization and robust overfitting happen simultaneously in robust neural classifiers. Specifically, the results consist of two parts. The first part shows that for a one-hidden-layer cubic-activated convolutional neural network trained on a patched dataset with a transfer attack, the training behavior follows a three-phase evolution, demonstrating that in later phases of neural network training, the neural network remembers spurious features. The second part shows that for well-separated and neural-separable datasets, a CGRO classifier with $poly(D) + \\tilde{O}(ND)$ parameters exists, and this parameter count is much less than potentially required by a truly robust classifier."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proves some very cool results that correspond to real-world observations of adversarial training dynamics. Section 5 provides solid intuitions to explain the phenomenon of practical robust classifiers requiring much more parameters than their non-robust counterparts, even with relatively small datasets. The paper is well-structured, and the story is relatively complete."
            },
            "weaknesses": {
                "value": "- The majority of results in this paper consider the binary classification setting. This should be made more clear in the abstract/introduction.\n- The assumptions made in Sections 3 and 4 seem arbitrary and unreasonably strong. In particular, an uncommon cubic activation function is chosen, and the correlation metrics are defined to be cubic without much explanation. Furthermore, a non-standard transfer attack that follows an atypical gradient update rule (Eq. (A)) is used for adversarial training in Section 3.3. On the other hand, the authors managed to prove some meaningful results using these strong assumptions, so this is a tough call. On the other hand, the results in Section 5 are much more general and practical, in my opinion.\n- The notations section reads, \"We use $poly(\\cdot)$ to denote the polynomial order, and $polylog(\\cdot)$ to denote the some polynomial in log order.\" This sentence is quite confusing. If I understood it correctly, a much better sentence would be, \"We use $poly(\\cdot)$ to denote a polynomial function, and use $polylog(\\cdot)$ to denote the composition of a polynomial function over a log function.\"\n- The term \"$poly (\\cdot)$\" itself is quite ambiguous. It would be clearer to mention what polynomial it is explicitly.\n- Since I found the results in Section 5 to be more practical and thus potentially more important, I would recommend adding a comparison in the experiments regarding different model sizes. It would be very supportive if you can show that when the model becomes larger, first the robust training loss decreases but the robust generalization gap remains large, and then when the model gets even larger, the robust generalization gap gradually decreases."
            },
            "questions": {
                "value": "- For the patch dataset, why is $\\alpha$ assumed to be proportional to $d^{0.249}$? Where is the number $0.249$ from? Similarly, where is the number $-0.509$ from in the assumption about $\\sigma$?\n- The sentence above Section 4 reads, \"Intuitively, when the model $f_W$ has achieve (typo here, should be achieved) mid-high clean test accuracy, the decision boundary of W will have a significant correlation with the separating plane of $g$, which thus makes adversarial examples generated by $g$ transferable.\" Is there any justifications or source for this statement?\n- The first sentence of Section 4.1 reads, \"which is widely applied in theoretical works explore what and how neural networks learn in different tasks.\" Check grammar.\n- In Figures 2c and 2d, the gradient magnitude and the value change on the CIFAR-10 dataset seems to increase at a radius smaller than $\\frac{8}{255}$. It would also make more sense to include a training data example and a test data example."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7273/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7273/Reviewer_6rXn",
                    "ICLR.cc/2024/Conference/Submission7273/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821772760,
        "cdate": 1698821772760,
        "tmdate": 1700361238070,
        "mdate": 1700361238070,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Awh16ogA9N",
        "forum": "G3OCarOfxx",
        "replyto": "G3OCarOfxx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_rmWV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_rmWV"
        ],
        "content": {
            "summary": {
                "value": "Machine learning models that are subject to adversarial attacks perform poorly out-of-sample when these attacks are not incorporated into the model during the training phase. Adversarial training, minimizing the empirical error while imitating the attacks during the training phase, has thus emerged as the *de facto* standard for training models to hedge against adversarial attacks. It has been increasingly observed that adversarially robust models still suffer from overfitting (called \"robust overfitting\"), even though the training set was perturbed via adversarial attacks. On the contrary, adversarially trained models generalize fairly well when the test set remains \"clean\", that is when there is no adversarial attack on the test set the model \"clean generalizes\". In this work, specifically for CNNs, the authors analyze why a model can simultaneously robust overfit and clean generalize. By using techniques from feature learning theory, they show that along the training of CNN, the network learns the \"clean\" relationship between the features and the classes, while memorizing the adversarially perturbed training instances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper follows up on modern research, cites very relevant resources, and is mathematically sound. The authors do a great job on the balance between asymptotic and exact notation/results. I also like the proof technique which is summarized in Section 4.3., which is original to this work."
            },
            "weaknesses": {
                "value": "Although the underlying proofs of this paper are interesting, I think the work is not ready for acceptance. Before I summarize my concerns I would like to note that I am familiar with the adversarial training and robust optimization literature, but I am not an expert in feature learning or CNNs. Therefore, I will have a very open mind and stay active during the discussion period to read the authors' responses. \n-- -- \n**Major Concern 1: Robust Optimization overlooked**\n\nI find it hard to agree with the motivation of this paper, especially for the \"clean generalization\" part. If we train a model adversarially but there is no attack in the test phase, this is precisely traditional robust optimization (not adversarial or distributionally robust, I mean the traditional version). We can think of the data at hand as noisy, that is if the observed feature values $X$ are noisy observations of the true feature values, then we build uncertainty sets around them via the balls $\\mathbb{B}_p(X_i, \\delta)$ -- see, for example, \"Robust classification\" by Bertsimas et al. (2019). In robust optimization, the \"test phase\" does not have any noise as the historical data is noisy but the risk is with respect to the true data. Moreover, the earliest papers already established the \"clean generalization\" result -- optimizing the worst-case during training does not give solutions that are significantly away from the nominal solutions (e.g., \"Robust solutions of Linear Programming problems contaminated with uncertain data\" by Ben-Tal and Nemirovski, 2000). On the contrary, robust optimization can even improve the out-of-sample performance of ML models (see, e.g., \"Regularization via mass transportation\" by Shafieezadeh-Abadeh et al. (2019) for the reasoning for the specific case of Wasserstein distributionally robust optimization and \"Certified Robust Neural Networks: Generalization and Corruption Resistance\" by Bennouna et al. (2023) for the connection between robust optimization and adversarial training). \n\nAs a reader, I would have preferred to have further discussion on this. \n-- -- \n**Major Concern 2: The paper is not written well, and includes several typos and errors**\n\nIn the \"Questions\" section I will list some examples, but the paper is not ready yet in my view. I had to read many times before I understood most of the claims. \n-- -- \n**Major Concern 3: The setting is very specific, and conclusions are general**\n\nThe paper, from its beginning, introduces assumptions, parameterizations, architecture, and dataset structures without really discussing why. Overall, the definitions are ad-hoc and there is an imposed assumption that the reader will have a deep understanding of all three: (i) adversarial robustness, (ii) feature learning theory, (iii) CNNs. I could not even get a gist of the ideas related to the literature that I am not used to as there is no introduction to: signals, patches, spurious feature learning, etc. I will add more in the \"Questions\" section. The main message of this paper is around robust overfitting (and how it can clean generalize, for example), and this image-data-related work is just a special case, many readers from the adversarial training field will be very interested in understanding the reason behind the phenomenon that is being studied here -- so I would strongly recommend a clearer and more inclusive language."
            },
            "questions": {
                "value": "Note that some questions are rhetorical and would be great to fix them in the paper.\n- \"clean data\" -> could the authors please specify this is attack-free or not-perturbed data when \"clean data\" is first mentioned?\n- Abstract and later parts of the paper mentions \"more general data assumption\" -> this is vague\n- Introduction: \"including in\" -> what does this mean (WDM?)\n- \"can make well-trained classifiers confused\" -> WDM?\n- \"robust generalization requires more data\" -> compared to what? \n- \"well-separated, which means half of the distance between the positive data and negative data is larger than ...\" -> I cannot follow.\n- (C): what if the second summation's indicator is $=1$ for more than one $i$?\n- (C): Before (C) it is said \"we consider the following function as\" - > as what? That said, there is a period at the end of the line of (C) but the sentence continues.\n- (C): There is a cyclical argument here because before (C) it is said \"we **consider** the following function\" but after the definition there is \"inspired by the intuition\" -> how can we get inspired from something that we just defined holistically?\n- \"Theorefore\" -> typo\n- The first contribution at the bottom of page 2: This is not a contribution, but a setting that you are working in to simplify the proofs.\n- Second contribution: increases \"quadratically\" -> Quadratic in what? (Same for \"polynomial\" later on.) This does not have a meaning. Similarly, \"partial true feature\" does not tell much before one understands the later sections. \n- Notations: $O(\\cdot)$ before $\\Omega(\\cdot)$ -> is this a typo?\n- Section 3.1: completely unclear. What is $X[\\mathrm{signal}(X)]$? This is never defined? Even a \"signal\" is not mathematically defined. I also do not follow \"meaningful signal\" terminology. \"norm of data\" -> which norm, what data? \n- Section 3.2, \"we use a one-hidden layer CNN\" -> why? There is no reasoning currently.\n- Section 3.2, \"we apply the cubic activation function\" -> similarly, why? \"as polynomial activations are standard\" is not a convincing argument.\n- Section 3.2: \"linear model can achieve\" and \"which but fails to explain\" have typos.\n-  Section 3.3: \"randomly sampled from the data distribution $\\mathcal{D}$ -> independently?\n- Question: Why is $\\lambda$ not fixed as $1$ in (F)? Which papers keep it as a generic $\\lambda > 0$?\n- Question: Can the authors please discuss why $\\delta = \\alpha(1-  1/(\\sqrt{d} \\mathrm{polylog}(d)))$ is assumed? Why is the attack strength now \"free\"?\n- Similarly, \"we assume $\\lambda \\in [1 / \\mathrm{poly}(d), 1)$\" needs some convincing arguments. \n- (O): Isn't $\\lambda = 1$ in the adversarial training literature? \n- \"target classifier $g$\" -> what is a target classifier? \n- \"to solve the minimization problem (O)\" -> (O) is a function not a minimization problem.\n- \"when the model $f_W$ has achieve mid-high clean...\" -> typo. \"explore what and how neural networks\" -> typo. \"the model correctly classify the data\" -> typo.\n- \"Spurious feature learning\" -> Can the authors please carefully define \"spurious\"? It becomes clear from the proofs why we need this, but in the main paper it was hard to follow for me.\n- Theorem 4.1: (1)(1) is a typo. Additionally, would it be clearer if \"with high probability\" was more formally described via confidence parameters?\n- That said, I would like to note that I like the discussion before Section 4.2 and the explanation after Theorem 4.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698897142326,
        "cdate": 1698897142326,
        "tmdate": 1699636868185,
        "mdate": 1699636868185,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aReSk6kjGp",
        "forum": "G3OCarOfxx",
        "replyto": "G3OCarOfxx",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_CpMF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7273/Reviewer_CpMF"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the overfitting phenomenon in the context of adversarial training and attempts to understand how clean generalization and robust overfitting could happen at the same time.  The theoretical framework in section 3 considers a two-layer neural network with a cubic activation function. The adversarial loss is defined in (F)  and the analysis focuses on a first-order attack scheme defined in (A). Theorem 4.1 is the paper's main result indicating that the adversarial training algorithm will partially learn the true features while memorizing the spurious features. Some numerical results are discussed in section 6 to show local flatness around training data while the sharpness of the loss function outside the training radius."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1- The paper studies an interesting problem on the generalization of adversarially learned models and develops some understanding of why the model can generalize well on clean data while suffering from robust overfitting."
            },
            "weaknesses": {
                "value": "1- Some of the assumptions in the study deviate from the standard adversarial training setting. The theoretical framework is limited to a one-layer neural network with a cubic activation function. In addition, the paper assumes that the adversarial training examples are designed for the optimal linear model $ g(x) = \\langle w^*  , x\\rangle $ at all epochs. As a result, the adversarial perturbation for each training sample is not updated over the course of robust training and will remain the same at every iteration. These assumptions seem different from a standard adversarial training setting where the perturbations are optimized for the neural net parameters at the current iteration. To address this issue, the theoretical framework should be updated so that the adversarial examples are designed for the current iterate $g(x)=\\langle w^{(t)} ,x\\rangle$ ($w^{(t)}$ rather than optimal $w^*$).\n\n2- The numerical experiments look insufficient and do not consider the theoretical framework studied in the paper. The paper should include some numerical results on the framework studied in the paper (one-layer neural net with cubic activation), and the text should better analyze and explain the experiments on MNIST and CIFAR-10. Also, the presentation of the numerical results could be improved. The plots in Figure 2 are quite small and hard to read. They do not contain the accuracy of trained models. Overall, section 6 is relatively short and has little discussion on the interpretation of numerical results. I suggest the authors remove section 4.3 from the main text and use more space for section 6, the plots should be presented more clearly and the numerical results should be discussed in greater detail in the text.\n\nAlso, Figure 1 in the paper copies a figure from (Rice et al, 2020) which is inappropriate for an independent paper. I think the authors should either only cite Figure 1 from (Rice et al, 2020) in the text or reproduce a similar plot on a slightly different classification setting to meet the expected novelty requirements in the presented numerical results."
            },
            "questions": {
                "value": "1- The theoretical results focus on the cubic activation function. Do the results hold for other polyonmial activation functions?\n\n2- Parameterization 3.1 requires the adversarial training data to be created for the optimal linear model with $w^*$ at every training iteration. Do the theoretical results remain true if the adversarial training data are optimized for the model at the current iteration $g(x)=\\langle w^{(t)} , x\\rangle$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7273/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699092432535,
        "cdate": 1699092432535,
        "tmdate": 1699636868079,
        "mdate": 1699636868079,
        "license": "CC BY 4.0",
        "version": 2
    }
]