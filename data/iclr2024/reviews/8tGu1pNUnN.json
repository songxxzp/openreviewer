[
    {
        "id": "WzZcQeRvHa",
        "forum": "8tGu1pNUnN",
        "replyto": "8tGu1pNUnN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_UsDM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_UsDM"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces the CodeComple dataset, which is the largest code dataset tailored for complexity prediction. Specifically, this dataset provides approximately 5,000 codes in both Python and Java, with a team of code specialists curating the complexity labels. Comprehensive experiments employing both traditional ML models and cutting-edge programming language models underscore the broad utility of the CodeComple dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The presentation of this paper is clear and easy to follow, making it accessible even for readers unfamiliar with the topic.\n2. The CodeComplex dataset is the largest code dataset designed specifically for the analysis of code complexity.\n3. Both the dataset and the corresponding code have been provided.  However, the provided resources are not well-documented and easily accessible. \n4. The paper has conducted a wide range of experiments, highlighting the broad applicability of the CodeComplex dataset."
            },
            "weaknesses": {
                "value": "## Major:\n\n1. While this work adds value, its contributions seem to be on the incremental side. Two prior studies have already developed code complexity datasets. Beyond the addition of more Java and Python codes, this work doesn't seem to bring novel designs to the CodeComplex dataset.\n2. The scope of the CodeComplex dataset appears to be limited. Ideally, a dataset focusing on code complexity should consider both time and space complexities. Yet, this particular dataset solely focuses on time complexity. It is noteworthy that some algorithms may sacrifice space complexity for prioritizing time complexity (e.g., Hash Table), or vice versa (e.g., Bitsets). Therefore, a dataset for the code complexity should consider both the space and time complexities.\n\n3. The dataset considers only Java and Python, albeit both being among the widely used programming languages. Furthermore, the dataset's data source is confined to CodeForces. It's worth noting that there are many open-source platforms available, such as GitHub and Leetcode.\n4. The documentation of this dataset raises concerns. I appreciate the authors' initiative in releasing the dataset and code on the anonymous platform. However, the anonymous GitHub repository lacks a tutorial for utilizing the dataset and essential instructions for reproducing the experiments. \n\n\n\n## Minor:\n\n1. In the manuscript, kindly include a reference to the Appendix. For instance, \"An overview of our CodeComplex dataset can be found in Appendix A.\"\n2. To enhance readability, please switch the positions of Figure 1 and Table 1.\n3. Please remove the \"For future work\" at the end of the conclusion section.\n4. On Page 8, please minimize the spacing, particularly below Figure 2 and above \"Programming Language Understanding Models.\"\n5. This paper mentions two previous code complexity datasets. Please incorporate both of them in Table 1. \n6. In Table 3, please add the \"%\" symbols in 7th and 10th rows."
            },
            "questions": {
                "value": "1. As listed in Table 1, this study has intentionally curated a balanced dataset across various classes. While a balanced dataset simplifies the training of neural networks, an imbalanced distribution is often closer to real-world scenarios. Could you explain your motivation to opt for a balanced dataset?\n2. The space complexity is also an important aspect of the code complexity. Why does this paper only consider the time complexity?\n3. Would you consider incorporating code from diverse data sources? For example, the Leetcode website offers ground-truth time and space complexity for various algorithms, implemented in multiple program languages."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698125447713,
        "cdate": 1698125447713,
        "tmdate": 1699637158385,
        "mdate": 1699637158385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tWu1DrnpCn",
        "forum": "8tGu1pNUnN",
        "replyto": "8tGu1pNUnN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_Vi2o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_Vi2o"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new code complexity dataset based on programming problems from Codeforces, and have human annotate the algorithm complexity of the code. The dataset is several times larger than that from previous works. The paper presents the results of several models trained on this dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The new code complexity dataset is an interesting dataset, and could bring value to the community."
            },
            "weaknesses": {
                "value": "- What do you mean \"OpenAI introduced GitHub Copilot, powered by Codex\"? This is false statement. Github Copilot is from Microsoft, and the backend model probably is not Codex.\n\n- Could you add experiments on the current popular LLMs' performance on the complexity datasets, like GPT-4, GPT-3.5-Turbo, StarCoder, etc. with zero-shot or few-shot? This might make the paper a lot more interesting.\n\n- Do you really have to say ML vs DL? please don't write it like that. DL is one family in ML.\n\n- The paper, however, lack sufficient novelty for ICLR."
            },
            "questions": {
                "value": "- Please present results on GPT-4, GPT-3.5-Turbo or something similar.\n- The paper didn't mention the details of training. What goes into the model? Is it just the code part, or problem descriptions and constraints are also going into model? Without these or information on what might goes into model, it seems hard to understand the complexity of the code, even for human."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817842103,
        "cdate": 1698817842103,
        "tmdate": 1699637158278,
        "mdate": 1699637158278,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5EXy5nXLwI",
        "forum": "8tGu1pNUnN",
        "replyto": "8tGu1pNUnN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_oyNS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_oyNS"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a dataset for predicting the time complexity of coding problems. It aims to evaluate ML models' ability to understand the computational complexity of algorithms. The proposed dataset expands CoRCoD by adding more examples and more complexity categories. The paper also measures and analyzes the classification performance of classical machine learning methods with language models, including CodeBERT, GraphCodeBERT, PLBART, CodeT5, CodeT5+, and UniX-coder as baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This work provides an extensive dataset for learning time complexity with Java and Python. This can contribute to training and improving language models of code.\n2. The paper is easy to follow. The dataset creation pipeline is clearly presented.\n3. The analysis with baselines is helpful for better understanding the dataset and models."
            },
            "weaknesses": {
                "value": "1. Soundness: The annotation rules take into consideration (1) problems' input dimension, (2) library usages, (3) test cases, and (4) constant input. These annotation rules are confusing: (1) considers only the largest input dimension, which is unsuitable for problems such as graph traversal; (3) fails to consider uncovered test cases. \n2. The data collection and annotation process are briefly mentioned in Section 2.1 and Appendix A1 and A2. The annotation rules are unclear and, at the same time, lack demonstrations to help annotators understand the annotation process. The demographics (e.g., how many annotators are proficient programmers, consensus, etc.), the annotator training procedure (if there's one), and the annotation statistics (e.g., confidence, Kappa score, etc.) are not provided. \n3. While I appreciate the effort of creating a new dataset (approximately 9 times larger than the previously largest one), this dataset seems relatively small for training the latest large language models. As in the performance section (Table 3), the performance gain is not consistently higher for language models compared to the classical methods.\n3. The writing can be improved. The current version seems to be not well formatted for a high-quality paper."
            },
            "questions": {
                "value": "1. Though the baseline models in the paper are for code understanding, there are different and latest language models of code, such as CodeX, CodeGen (https://arxiv.org/abs/2305.02309), Incoder (https://arxiv.org/abs/2204.05999) with higher code prediction performance. Can they be utilized for code understanding, e.g., fine-tuning with prompting with CodeX?\n2. Why do we want to eliminate dead code? Shouldn't the ability of models to \"understand code\" imply being able to ignore such code segments? \n3. For performance in Table 3, it doesn't seem no language model performs better for most classes of complexity than others, including the classical ML method like decision tree. Does it indicate some difference between these classes, and if the datasets are actually useful for training large language models? Do you have a justification for that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9201/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9201/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9201/Reviewer_oyNS"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829583503,
        "cdate": 1698829583503,
        "tmdate": 1699637158166,
        "mdate": 1699637158166,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ugWIqJ2FID",
        "forum": "8tGu1pNUnN",
        "replyto": "8tGu1pNUnN",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_aZXg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9201/Reviewer_aZXg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a dataset for classifying the time complexity of 4.9K Java and 4.9K Python programs from a competitive programming benchmark (Codeforces). The dataset is based on part of the Codecontests dataset. Human annotators labelled code examples with one of 7 time complexity classes (ranging from O(1) to NP-hard), using algorithmic tags for the code and a manual examination of the code and libraries that it calls. The paper fine-tunes a variety of code LLMs (CodeBERT, CodeT5, etc) on the task and reports accuracies per-model and per-time class."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1) The benchmark is larger than previous time complexity benchmarks, both in the number of problems (~9K, compared to ~3K in the contemporaneous Tasty benchmark, Moudalya et al., and 900 in CoRCoD), the languages covered (two, compared to one in CoRCoD), and the number of time complexity classes (7, compared to 5 in CoRCoD). \n\nS2) The benchmark appears to be fairly carefully constructed (but some more details on this would be helpful, see below): introducing some data augmentation approaches and dead code elimination, and having human annotators label the code and verify each others' labels.\n\nS3) Some aspects of the experimentation were thorough, involving a large number of strong code encoding models."
            },
            "weaknesses": {
                "value": "The contribution overall felt a bit thin to me.\n\nW1) It would help to do some experiments to demonstrate the benefit of this larger dataset, by e.g. showing performance by amount of training data (show sample complexity curves). I think this is especially important given that some fraction of the dataset seems to have been generated using data augmentation (details of this, and its size) were a bit unclear to me.\n\nW2) While there are lots of results from various models, there were a limited number of clear takeaways. To have more solid comparisons between the models, I think that the paper should tune their learning rates on a validation set, rather than using the same LR for all models. The accuracy-by-length is an interesting step toward a fine-grained analysis (as are the per-class analysis, but see below), but would be further improved by looking at other attributes of the code, e.g. the per-algorithm accuracies. \n\nW3) The paper should do a bit more to differentiate itself from past work, especially TASTY (Moudgalya 2023). While I appreciate that TASTY came out not so long before the ICLR submission deadline (~4 months), it does seem similar enough to this benchmark that it would be helpful to e.g. compare findings between models on the two benchmarks, or at least say more about the differences in the paper.\n\nW4) It would help to improve the motivation for the work -- what would the value be of a model that can predict time complexity? e.g. some experiments showing that reranking with such a model can improve the time complexity of generated code, or be used in the sort of pedagogical applications mentioned in the intro.\n\nW5) The writing could be improved in clarity, see below."
            },
            "questions": {
                "value": "Q1) I had a few questions about the annotation process:\n\nQ1a) \"Consider the input size as a parameter to determine the time complexity of a code. We measure the complexity by the largest parameter among them.\". I'm unclear what this second sentence means: does it refer to the largest possible input to the code?\n\nQ1b) \"When a problem provides a fixed constant as input, we classify the case as having a constant time complexity.\". Could more detail be given about this assumption: (a) I'm unclear on when a problem would need to have a fixed constant as an input, and (b) it seems that some problems that don't take any inputs might have non-constant time complexity (i.e. program that prints the first 100K prime numbers might take no inputs, but have high time complexity). \n\nQ1c) How did the annotators resolve the ambiguous scenarios mentioned at the bottom of page 3? e.g. when the problem itself has a higher time complexity than the problem+input constraints, which is used for the time complexity?\n\nQ2) How many instances are in the augmented dataset, versus in augmented? Does the \"random\" split in the experimental setup take augmentation into account (no overlap in augmented code between train and test)?\n\nQ3) I'm unclear on how the scores in Table 3 were obtained. Is the prediction task here a 7-way prediction, but divided up by the ground-truth class?\n\nOther suggestions\n- It would help to make a pass over the paper for clarity and grammar. e.g. the final section ends mid-sentence."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9201/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699109382696,
        "cdate": 1699109382696,
        "tmdate": 1699637158068,
        "mdate": 1699637158068,
        "license": "CC BY 4.0",
        "version": 2
    }
]