[
    {
        "id": "DWjyectBEf",
        "forum": "tG5mpAM7ZK",
        "replyto": "tG5mpAM7ZK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce an approach aimed at generalizing from a single domain, represented by natural object images, to previously unseen domains, including objects presented as paintings and sketches. Their method involves shifting visual embeddings using relative differences of textual embeddings from the original domain (a picture of a cat) to potential unseen test domains (a painting of a cat), thereby enabling feature-space augmentations.\n\nThe authors conduct evaluations of their proposal on datasets such as CUB and DomainNet. The results show minor improvements over the competitive LADS baseline. This work addresses an important challenge in domain adaptation by expanding the model's capabilities to novel domains, demonstrating its potential in real-world scenarios involving diverse visual data representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1:\nThe paper is generally well-written and easy to follow, although there are instances where it may overly emphasize LADS rather than the paper's ultimate goal. Despite this, it remains a good read overall, providing a clear presentation of the research.\n\nS2:\nThe paper takes on a highly challenging task: single-domain generalization. In the context of this demanding objective, CLIP has established itself as a robust tool, and this paper contributes to this ongoing trend."
            },
            "weaknesses": {
                "value": "W1 Method:\n\nA significant concern arises regarding the similarity between this paper and CLIP the GAP (CGAP) [1], which the authors fail to acknowledge or cite. Both papers share fundamental observations: that various domains of the same object differ by constant vectors. Additionally, they both tackle the challenge of single-domain generalization. The methodological resemblance is evident: CGAP performs semantic augmentations by computing relative differences in original and target test domains of text embeddings, using these as pseudo targets to generate visual features of the target domains. The evaluation contexts differ slightly: CGAP explores weather attributes, while this paper focuses on DomainNet/CUB. Furthermore, both papers demonstrate that precise domain names are unnecessary for strong performance on unseen domains, as illustrated by CGAP's \"random\" augmentations in Table 7.\n\nIn light of these similarities, it appears that this paper functions as an application on CUB and DomainNet rather than a substantially novel contribution.\n\n[1] Vidit, Vidit, Martin Engilberge, and Mathieu Salzmann. \"CLIP the Gap: A Single Domain Generalization Approach for Object Detection.\" CVPR, 2023. (https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf)\n\nW2 Result:\n\nA noteworthy concern arises from the marginal improvement over the most competitive baseline, LADS, as evidenced by Table 1 (0.78% improvement on CUB, 0.92% improvement on DomainNet). This minor improvement raises doubts about the merit and significance of the proposed method, especially considering the striking similarities to existing work, such as CGAP. The paper's originality and contribution require further clarification and validation to convincingly demonstrate its value in the domain of single-domain generalization.\n\nTypos:\n\n- [LADS reference]: \"Using language to *extend* to unseen domains,\" International Conference on Learning Representations (ICLR), 2023.\n\n- \"Benefiting from the learned multimodal embedding space in pre-trained large-scale vision-language (VL) models (Radford et al., 2021; Jia et al., 2021), *we* achieve...\""
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Reviewer_GcYV"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698408826475,
        "cdate": 1698408826475,
        "tmdate": 1699636457982,
        "mdate": 1699636457982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Nm4KSteXa5",
        "forum": "tG5mpAM7ZK",
        "replyto": "tG5mpAM7ZK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_spPq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_spPq"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the domain generalization without using the image nor text description of the target domain. The authors propose to use the property of modality gap of CLIP model. Specifically, they augment the embeddings of images in the source domain to the domain-invariant embedding without training. After augmentation, they train the linear probe on the mixture of domain-invariant embedding and original embedding. In inference, they apply the trained linear probe to the embedding of the target image. In the experiment, the authors test their proposed method with and without the text description of the target domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper adopts the recent finding of the modality gap in the CLIP embedding space and propose a training-free augmentation which is training-efficient.\n2. The paper considers a practical setting where the text description is not available for the target domain."
            },
            "weaknesses": {
                "value": "1. In **Training-free Augmentation with Modality Direction** (on page 5) the proof of z exists in the CLIP's normalized embedding space is trivial to get.\n\n2. In Table 1 and Table 2, the performance of the proposed method on CUB and DomainNet does not stand out from the baselines with a significant margin. \n\n3. Ablation Studies are limited."
            },
            "questions": {
                "value": "1. Why can the linear probe trained on the domain-invariant feature and original feature be directly applied to the target feature (without any augmentation)?\n\n2. In Stage 2, why not only train the classifier only on the augmented embeddings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806410814,
        "cdate": 1698806410814,
        "tmdate": 1699636457873,
        "mdate": 1699636457873,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kMfxJv5tfc",
        "forum": "tG5mpAM7ZK",
        "replyto": "tG5mpAM7ZK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach for domain adaptation with the idea of the augment of data with only text descriptions. It utilizes a pre-trained vision-language model to acquire domain-invariant augmentations using text descriptions of arbitrary, unseen domains, even if they don't match the test domains. This method outperforms existing approaches, offering greater efficiency and stronger theoretical support while eliminating the need for predefined text descriptions of all test domains."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Good Quality:\nThe submission is technically sound, in my opinion, and the advantages and limitations of this work are carefully and honestly discussed.\n\nGood Clarity:\nThe submission is written with clear definitions and formulas. The organization is well-designed.\n\nGood Motivation:\nThe method is well-motivated, given the recent conclusions from Liang et al. and Zhang et al.\n\nSolid Solution:\nBy observing the beneficial properties of the multimodal embedding space, the authors successfully extend the findings in Liang et al. and Zhang et al.  to address the domain generalization problem, providing a new method from a novel perspective."
            },
            "weaknesses": {
                "value": "1.\tThe authors might consider evaluating their proposed method on a larger dataset, such as WILDS, which provides extensive and diverse real-world data from various domains.\n\n2.\tCould you please explain the meaning of \"1x\" for Time in several tables?\n\n3.\tWhile the authors have successfully developed a method based on the findings of Liang et al. and Zhang et al., the paper itself lacks a sufficient explanation of the embedding space in the context of domain generalization. Although this might not be a critical flaw in the paper, providing theoretical or empirical analyses in this regard could enhance its overall impact and inspiration."
            },
            "questions": {
                "value": "As the authors mentioned in Figure 2 (right), there might be cases where equation 4 is not satisfied. I am curious if the CLIP features always meet the conditions of equation 4. Could the authors conduct empirical analysis to offer an intuitive understanding of when it fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4755/Reviewer_mv6P"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4755/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815525142,
        "cdate": 1698815525142,
        "tmdate": 1699636457799,
        "mdate": 1699636457799,
        "license": "CC BY 4.0",
        "version": 2
    }
]