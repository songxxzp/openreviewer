[
    {
        "id": "Gfa8UtHjSy",
        "forum": "DP4NkPZOpD",
        "replyto": "DP4NkPZOpD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_b3vX"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors present OMNIPROT, a novel generative modeling technique to capture the joint distribution between protein sequence and three-dimensional structure."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The method appears to be reasonable.\n2. The paper is well-organized.\n3. The research problem is meaningful for researchers in the computational biology field."
            },
            "weaknesses": {
                "value": "1. There are style issues, such as: `\"FOr example, In drug development, ...\"`, where some words have inconsistent capitalization.\n2. The experiments are not comprehensive:\n   - While the paper in the method section claims that the proposed method can support various protein-related tasks, this is not adequately demonstrated in the experimental section.\n   - For the tasks that have been compared, the baselines are not comprehensive. For example, in the protein-protein docking task, traditional protein docking methods are not compared, and for the inverse folding task, only ProteinMPNN is compared.\n3. The paper mentions that the method supports \"protein-protein docking with contact information\" tasks, which may not be very practical, as in real-world applications, blind docking is more common.\n4. The method's innovativeness is limited. This method essentially involves multi-modal data input and the use of latent diffusion."
            },
            "questions": {
                "value": "1. The paper mentions that the encoder is trained separately, but the authors do not explain the reason for doing so. Is it because training the encoder and diffusion module together would be challenging due to the complex model architecture?\n2. I hope the authors will compare their method with more baselines and also validate the model's performance on protein folding tasks.\n3. For the docking task, for methods based on generative models, such as the method proposed in this paper and the compared baseline DiffDock, the paper reports \"oracle\" statistics by selecting the prediction with the lowest RMSD from the ground truth among the 20 generated results. I believe it would be valuable to also present the mean and variance of these 20 samples on each metric. In real-world applications, we do not have ground-truth complex structures as a reference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697598551161,
        "cdate": 1697598551161,
        "tmdate": 1699637054503,
        "mdate": 1699637054503,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y7Wf2ZXVUF",
        "forum": "DP4NkPZOpD",
        "replyto": "DP4NkPZOpD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_ZFyz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_ZFyz"
        ],
        "content": {
            "summary": {
                "value": "This paper constructs a unified latent diffusion framework for generating both protein sequences and structures. It leverages the proposed model for protein docking and inverse folding tasks. Experiments show that the proposed framework outperforms protein-protein docking baselines on intermediate-accuracy metrics, though not on highly-accurate metrics. It achieves similar performance to ProteinMPNN for the inverse folding task."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The work presents a unified protein generation framework capable of performing multiple tasks.\n- The paper is well-constructed, and it provides relatively comprehensive implementation details for the model and the experimental tasks.\n- In Appendix F.1, the authors point out the potential data leakage issues in the commonly used DIPS dataset, which can be one of the contribution."
            },
            "weaknesses": {
                "value": "Main concerns are listed as follows:\n\n- Although the proposed model is claimed to \"bridge sequence and structure,\" Table 3 shows that the proposed method underperforms the inverse folding baseline on all metrics. This raises questions about the effectiveness and motivation of the unified framework by modeling both seq-structure as claimed by the authors. In the docking evaluation, OMNIPROT lags behind the baseline DockGPT by a significant margin for high-quality metrics (25th quantile for RMSD and \u2265high for DockQ). This suggests that OMNIPROT struggles to recall the most accurate \"hits\" but generates many just \"acceptable\" docked poses. In traditional docking, candidate poses are searched and ranked by free energy, with only a few top (low energy) poses selected for downstream analysis or tasks. Therefore, the high-quality metrics for docking should be given the utmost consideration during evaluation, as the results weaken OMNIPROT's performance. The authors are welcome to argue against this if they disagree.\n- The paper lacks an explanation for the design choices made for the OMNIPROT architecture. There is also no ablation study on the proposed framework, encompassing the encoder, decoder, and denoising network. Additionally, the authors should clarify why they adopted latent diffusion instead of a vanilla VAE or other sampling strategies used in VAE, providing corresponding evidence or support, such as an ablation study.\n- While Section 5 mentions that the training data were collected \"without any quality-based filtering,\" the authors should clarify whether they performed redundancy removal for the newly curated docking dataset. Since the docking task leverages both sequence and structure features, proper filtering for the training set (it is mentioned in the paper as \"consists of the remaining data\") based on sequence (and structure) similarity is required to prevent the potential data leakage.\n- A related work, PROTSEED [1] (I found it already published in ICLR 2023, thus cannot be counted as concurrent work), also employs the IPA module of AF2 to decode both sequence and structures for multiple tasks, including inverse folding. The authors should clarify how OMNIPROT differs from PROTSEED and provide a better context for the reference. Failing to show significant improvement over the related works can weaken the contribution of the proposed model.\n- The comparison with inverse folding baselines is insufficient, particularly with the introduction of the new metric \"sc-RMSD.\" The authors should consider contextualizing more baselines that handled this task: PROTSEED[1], ESM-IF [2], PiFold [3], and the gradient-based AlphaDesign [4] to name a few. To establish the state-of-the-art (SOTA) performance, the authors should also refer to the PiFold, especially since it was claimed to outperform ProteinMPNN. An incomplete baseline comparison on a well-studied task may reduce the credibility of the results, and additional experiments are encouraged.\n- Regarding the inverse folding evaluation, what is the rationale for using sc-RMSD instead of perplexity, where the latter is more commonly evaluated for inverse folding task? The sc-RMSD/TM are typically used for backbone (structure) generation models like FrameDiff [5]. Since the authors claim the proposed model to be \"generative,\" the perplexity (ppl) on the test set is encouraged to be reported. The ppl can be more important in reflecting the performance of the inverse folding model compared to the flawed recovery metric.\n- It is unclear to me whether the mentioned two tasks in this paper involves independent model training or if some (or all) of the model parameters are shared across tasks. If any parameters are shared, the authors could specify which ones. (please point to the related text if already mentioned)\n- What is  the \"geometrically-structured\" latent space compared to the canonical latent space as mentioned in the abstract and introduction sections?\n\nMISC:\n- Section 1 Introduction - line 7, FOr \u2192 For.\n- Equation (3), x1 \u2192 z1\n- Section 3 deserves a further refinement.\n- Section 4.4.1, I do not think this should be a subsection of 4.4.\n- I found the paragraph in section H (appendix) incomplete.\n\n[1] Shi, Chence, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, and Jian Tang. \"Protein sequence and structure co-design with equivariant translation.\"\u00a0*arXiv preprint arXiv:2210.08761*\u00a0(2022).\n\n[2] Hsu, Chloe, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam Lerer, and Alexander Rives. \"Learning inverse folding from millions of predicted structures.\" In\u00a0*International Conference on Machine Learning*, pp. 8946-8970. PMLR, 2022.\n\n[3] Gao, Zhangyang, Cheng Tan, Pablo Chac\u00f3n, and Stan Z. Li. \"PiFold: Toward effective and efficient protein inverse folding.\"\u00a0*arXiv preprint arXiv:2209.12643*\u00a0(2022).\n\n[4] Jendrusch, Michael, Jan O. Korbel, and S. Kashif Sadiq. \"AlphaDesign: A de novo protein design framework based on AlphaFold.\"\u00a0*Biorxiv*\u00a0(2021): 2021-10.\n\n[5] Yim, Jason, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. \"SE (3) diffusion model with application to protein backbone generation.\"\u00a0*arXiv preprint arXiv:2302.02277*\u00a0(2023)."
            },
            "questions": {
                "value": "Please address and clarify the questions and concerns in the Weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697854703823,
        "cdate": 1697854703823,
        "tmdate": 1699637054379,
        "mdate": 1699637054379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5a9cqXZRwX",
        "forum": "DP4NkPZOpD",
        "replyto": "DP4NkPZOpD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_cpz6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_cpz6"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce OmniProt, a method for conditional protein sequence and structure generation leveraging a pre-trained autoencoder and latent diffusion. By capturing the joint distribution of sequence and structure and controlling the conditioning, OmniProt can be used for protein-protein docking, folding, and inverse folding. The authors report competitive performance for both protein-protein docking and inverse folding, compared to popular baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The key idea of performing conditional diffusion in the latent space of an autoencoder trained jointly on sequence and structure allows for a truly multitask generative model that encompasses many protein design capabilities."
            },
            "weaknesses": {
                "value": "There are some questions remaining about the capabilities (folding, co-generation) that are repeatedly highlighted but not evaluated in the paper. It is unclear if the autoencoder approach introduces a fundamental limitation to the method, or if there are straightforward paths to improving the decoder. The authors show one example of ablating the effects of joint training on sequence and structure, showing improvement in inverse folding metrics when training with structure-based features; their approach affords unique insight into the value of joint training and this point could be expanded upon with further experiments."
            },
            "questions": {
                "value": "1. Is there any particular reason why protein folding and co-generation have not been evaluated? The flexibility of the method is its major advantage, and even if it does not outperform purpose-built methods like AlphaFold2 (folding) and ProteinGenerator (co-generation), it would be interesting to see demonstrations of these capabilities. If the authors believe this is out of scope, it may be beneficial to narrow the scope of the claims and discussion in the paper to the two tasks that are considered in detail.\n2. The authors repeatedly mention that improvements to the autoencoder might improve overall OmniProt performance. Do the authors believe that this is a scaling / expressivity problem, or architectural?\n3. The authors highlight that OmniProt inverse folding samples are generated using reverse diffusion, rather than the randomized autoregressive scheme used in ProteinMPNN. This does intuitively seem like a major advantage, but no metrics are reported to support this claim. Can the authors provide some evidence beyond sequence recovery that more directly relates to sample quality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763127119,
        "cdate": 1698763127119,
        "tmdate": 1699637054263,
        "mdate": 1699637054263,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XE6H34UaNX",
        "forum": "DP4NkPZOpD",
        "replyto": "DP4NkPZOpD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_rb4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8453/Reviewer_rb4f"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces OmniProt, a framework for generative protein design as well as a range of tasks such as folding, inverse folding, and docking.\nThe model consists of an autoencoder, that maps protein sequences and structures to a latent space, and a diffusion model that operates in that latent space. The autoencoder is trained using roto-translational invariant features used in DockGPT. The tasks considered of inverse folding or docking are then solved through conditioning of the diffusion model with the relevant structural or sequence information, and a decoding of the generated latent space samples.\nThe model shows good performance on inverse folding and state-of-the-art performance on docking compared to existing ML models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "This is an interesting study that takes a new approach to solve several related problems in protein design in a coherent way. The methods that are being compared, such as ProteinMPNN or DockGPT, are task-specific models and is quite remarkable that with a latent diffusion model, the authors are able to approach the performance of state-of-the-art models on both tasks. The OmniProt model is quite general and seems easily extendable into e.g. a de novo design framework."
            },
            "weaknesses": {
                "value": "A more in-depth benchmarking of the autoencoder, and how its performance varies as a function of structure and sequence complexity would be beneficial. In benchmarks of inverse folding, ProteinMPNN is used as reference but more recent models such as LM-Design or Knowledge-Design have shown improved performance. In the docking benchmark, it would be useful to see comparisons with traditional docking tools such as Zdock or Haddock."
            },
            "questions": {
                "value": "The authors should include a citation to 2305.04120 which has explored a relatively comparable approach of latent space diffusion for protein design."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853059212,
        "cdate": 1698853059212,
        "tmdate": 1699637054152,
        "mdate": 1699637054152,
        "license": "CC BY 4.0",
        "version": 2
    }
]