[
    {
        "id": "erG8jvV1Bm",
        "forum": "ba84RDHFnz",
        "replyto": "ba84RDHFnz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_FunS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_FunS"
        ],
        "content": {
            "summary": {
                "value": "The paper studies regions in masked autoencoders. The idea is interesting although there has been a large amount of self-supervised learning introducing the concept of regions (These methods are generally called self-supervised object detection methods in the field). The resulting R-MAE shows better performance on COCO, ADE20K datasets compared with MAE baseline."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The overall paper is clear and easy to follow.\n2. The analysis for different designs of regions is comprehensive.\n3. This paper gives some suggestions when training with regions in MIM.\n4. The authors will open the source code and models."
            },
            "weaknesses": {
                "value": "1.\tSelf-supervised contrastive learning needs to introduce the concept of region to focus local information due to its a priori assumption of image semantic consistency, but MAE does not have this problem. Moreover, I agree that the reconstruction of raw pixel values lacks a higher level of semantic information for image understanding compared to word reconstruction in NLP. However, I do not agree the introduction of binary regions adds high-level semantics. Therefore, I argue this paper is the same as previous self-supervised object detection learning. The effect comes from further learning of the local region, so it is effective in tasks such as detection and segmentation. At the same time, the performance of this type of method will be lower than the baseline on tasks such as ImageNet image classification. If high-level semantic understanding tasks such as image classification do not perform well, then it is difficult to say that R-MAE has a high-level understanding as mentioned in the introduction.\n2.\tLike question 1, the paper lacks ImageNet classification experiments.\n3.\tThe calculation amount comparison is unfair. The paper only calculates the calculation amount of the architecture, but the results of the FH algorithm and SAM region generation are not included.\n4.\tFrom Table 3, more data show that the gain in segmentation results relative to MAE is weaker. Is there no difference between the results of MAE and R-MAE on a larger data set?"
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Reviewer_FunS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2410/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698221699934,
        "cdate": 1698221699934,
        "tmdate": 1699636176171,
        "mdate": 1699636176171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ogvTkaWjAa",
        "forum": "ba84RDHFnz",
        "replyto": "ba84RDHFnz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_uFze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_uFze"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new pretext task called mask Region Autoencoding for self-supervised visual representation learning. Instead of considering pixels as the operated units, the authors conduct masking and reconstruction on the so-called region levels. By integrating RAE into MAE, the resulting R-MAE achieves impressive performance on various transfer learning settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is generally well-written with solid experimental results.\n- The paper has a clear explanation of the proposed method with valid qualitative demonstration.\n- Beside common transfer learning experiments, the authors also explore the usage of R-MAE for interactive segmentation."
            },
            "weaknesses": {
                "value": "- About the importance of regions:\n  - As discussed in Sec. 2, the authors claim that there are many different sources to obtain regions. In other words, here regions do not have a specific definition, especially under the context of unsupervised learning. Or in another words, the best definition of regions might differ with respect to different downstream tasks, while just for object detection and semantic segmentation, SAM proposals might be the best.\n  - One following question to explain is that why in SAM as the region source can even perform better than panoptic ground truth COCO annotations when pre-trained on COCO, as in Tab. 1(d)?\n  - Also, as claimed in 3rd paragraph of Sec. 1, the authors claim that \"regions\" might perform similarly with \"words\" in language models to improve the scalability of MAE and pursue visual emergent properties. Unfortunately, both the qualitative and quantitative results can only demonstrate similar observation for the learned representation with MAE. Moreover, the authors only conduct experiments with ViT-B, without further exploration about the scalability of R-MAE.\n  - Therefore, it is hard to convince me that this work has a different motivation with locality reconstruction works like LoMaR [1] and SemMAE.\n- About the architecture:\n  - Does the region encoder share weights with the pixel encoder? If not, which one would be transferred for downstream tasks in the context of RAE and R-MAE respectively?\n  - Is there any advantage to conduct region reconstruction only for binary region masks, which totally throw the RGB information, while the latter should also be part of the semantics? This question is way more interesting if we consider that RAE with SAM performs better than MAE.\n  - One following question would be what will happen if we transfer RAE and MAE weights to downstream color-sensitive tasks, like the the Flowers classification dataset.\n  - Moreover, is there any advantage to maintain a separate branch of region encoder, since a simpler implementation might be similar with LoMaR, where for the visible part of each region, we can directly utilize them to reconstruct the RGB values of the masked part of this specific region, so that we can perform the two objectives of R-MAE at the same with without introducing a separate branch.\n  - Just to make sure I have understood correctly, does RAE mean we only apply the upper part (=region encoder + region decoder) of Fig. 1?\n- Overall, it is hard to convince me that region modeling is so important as the authors claim and it seems like there are much easier ways to implement this idea than the proposed R-MAE framework.\n\n[1] Chen, Jun, et al. \"Efficient self-supervised vision pretraining with local masked reconstruction.\" *arXiv preprint arXiv:2206.00790* (2022).\n\n[2] Chen, Kai, et al. \"Mixed autoencoder for self-supervised visual representation learning.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[3] Liu, Jihao, et al. \"Mixmim: Mixed and masked image modeling for efficient visual representation learning.\" *arXiv preprint arXiv:2205.13137* (2022)."
            },
            "questions": {
                "value": "- About experiments:\n  - R-MAE has been surpassed by earlier MAE-based framework targeting at detection and segmentation with local awareness (e.g., 800-epoch MixedAE [2] outperforms 1600-epoch R-MAE on ADE20K).\n  - It would be better to also report quantitative comparison for interactive segmentation for better understanding in Fig. 6.\n\n[1] Chen, Jun, et al. \"Efficient self-supervised vision pretraining with local masked reconstruction.\" *arXiv preprint arXiv:2206.00790* (2022).\n\n[2] Chen, Kai, et al. \"Mixed autoencoder for self-supervised visual representation learning.\" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[3] Liu, Jihao, et al. \"Mixmim: Mixed and masked image modeling for efficient visual representation learning.\" *arXiv preprint arXiv:2205.13137* (2022)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2410/Reviewer_uFze"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2410/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698673679821,
        "cdate": 1698673679821,
        "tmdate": 1700660950415,
        "mdate": 1700660950415,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HWYaEOaRYr",
        "forum": "ba84RDHFnz",
        "replyto": "ba84RDHFnz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_YqrJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2410/Reviewer_YqrJ"
        ],
        "content": {
            "summary": {
                "value": "The authors introduce a self-supervised image representation learning method called \"masked region autoencoding\" (RAE), treating regions as the visual equivalent of words. When integrated with the existing Masked Autoencoding (MAE) approach, the combined method (R-MAE) consistently improves performance in various vision tasks. RAE offers a more region-aware and instance-aware representation of images."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is highly clear in its presentation, effectively conveying the proposed methodology with its motivation.\n2. The paper extends the traditional Masked Autoencoding (MAE) approach by considering regions as visual analogs of words. The concept of using regions for interactive segmentation is also original.\n3. The proposed method can consistently help downstream performance on localization-related tasks (e.g., detection and segmentation)."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a more extensive comparison with existing methods in the field. While it highlights the strengths of R-MAE, a more in-depth quantitative comparison with other state-of-the-art self-supervised learning techniques (based on MAE) would strengthen the paper."
            },
            "questions": {
                "value": "1. Is the performance of R-MAE sensitive to the quality of region maps, and are there strategies to mitigate this sensitivity?\n2. Could the authors provide a more extensive quantitative comparison with other state-of-the-art self-supervised learning methods in computer vision? This would help readers understand how R-MAE performs in relation to existing techniques."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2410/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698768263236,
        "cdate": 1698768263236,
        "tmdate": 1699636176005,
        "mdate": 1699636176005,
        "license": "CC BY 4.0",
        "version": 2
    }
]