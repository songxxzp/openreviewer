[
    {
        "id": "FgMcXcm9Ib",
        "forum": "OrOd8PxOO2",
        "replyto": "OrOd8PxOO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission461/Reviewer_arMx"
        ],
        "content": {
            "summary": {
                "value": "This work provides a method to learn a universal humanoid motion prior for different downstream tasks, the prior is designed to cover a wide range of motions. The learned latent space can be used for long, stable and diverse motion generation, and also for solving tasks with natural motions. The latent is learnt by first training an imitator controller to imitate a wide range of human motions, and then learn the latent space by distilling the learned motion imitator. Results show the learned controller could generate a wide range of human motion, and outperform baselines on downstream tasks by a large margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The work is well-motivated, addressing the problem that motion imitator can hardly be used for downstream tasks (especially when interaction with the environment is required) and learned motion prior can not cover a wide range of human motions.\n2. Method is well demonstrated with details, and most designs are well-explained. Clear ablation study also shows the effectiveness of different components. Additional implementation details and hyper-parameters are provided in the supplementary materials for the community to reproduce the results. \n3. The learned humanoid motion prior showed impressive motion imitating performance over a wide range of human motion, and also showed promising results when applying to diverse downstream tasks, including motion tracking and locomotion over complex terrains, et al.\n4. The supplement videos make the difference between proposed method and baseline more comprehensive."
            },
            "weaknesses": {
                "value": "1. One of this work\u2019s claims is that previous work has limited coverage of the learned latent space that can not cover the wide spectrum of possible human motion. But in Table 1, there is no comparison with ASE or CALM or Imitate&Repurpose on motion imitation performance. Though ASE, or CALM might be a bit hard to compare, Imitate&Repurpose should be reasonable to compare with. I\u2019m not expecting it to perform better, just for completeness.\n2. Though PHC+ exhibit great performance on motion imitation, it\u2019s comparison with PHC might be a bit unfair, since one of the modification fo PHC+ is \u201cremoving\u201d some hard-negative in the dataset, provide motion imitation result on modified dataset might be beneficial. \n3. Some content is a bit hard to read: In Figure 3, it\u2019s quite hard to see the human motion in the Figure 3(e) row, and the title for each row is really hard to see. (Minor issue)"
            },
            "questions": {
                "value": "1. Is the proposed method robust against methodology and dynamics changes? It would be interesting to see these results and potentially enable appling the proposed method to humanoid robots."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698856631884,
        "cdate": 1698856631884,
        "tmdate": 1699635972397,
        "mdate": 1699635972397,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ew2rNZc4UN",
        "forum": "OrOd8PxOO2",
        "replyto": "OrOd8PxOO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission461/Reviewer_UBzx"
        ],
        "content": {
            "summary": {
                "value": "In the context of human motion representation, the paper proposes a method to create a fundamental representation of humanoid motion that can be used for humanoid control, human motion generation or motion tracking. This representation is created via two main elements: an imitation method and a physics-based learned prior.\nThe distillation is made through a VAE-like architecture that learns the prior R and decoder D that are then used for each downstream task to generate to generate the action."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written\n- The presented goal of learning a universal human motion latent representation is very interesting, and the effectiveness on relevant downstream tasks is well presented\n- The curation of the mocap training dataset for PHC (along with other modifications) increases robustness and allows fail states recovery\n- The downstream tasks are relevant and show interesting use cases for the learned representation\n- The ablation study is quite compelling"
            },
            "weaknesses": {
                "value": "- Quantitative results on the VR-controller tracking task are a little bit disappointing compared to Scratch\n- The references section could be cleaned up and harmonised, notably for publication conferences\n- Although the writing is clear, some typos remain (e.g. Guassian)"
            },
            "questions": {
                "value": "- In section 4.3, a failure case is described using the prior R but it is unclear to me how do we recover from it apart from restarting the task's policy learning? Does it happen often?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900329527,
        "cdate": 1698900329527,
        "tmdate": 1699635972328,
        "mdate": 1699635972328,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HLH7V96PMP",
        "forum": "OrOd8PxOO2",
        "replyto": "OrOd8PxOO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a system to train human motion controllers that can imitate large motion datasets and be used efficiently for training tasks such as terrain navigation and vr motion following.\n\nKey contributions:\n\n1. Modification of PHC to learn a better motion controller.\n\n2. A VAE like distillation process to obtain a controller and an action space that can be used efficiently for downstream tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. A controller that can imitate the whole AMASS dataset.\n\n2. A wide range of tasks to verify the efficiency of the system."
            },
            "weaknesses": {
                "value": "1. No comparison with baselines that resemble the proposed method, e.g., Physics-based character controllers using\nconditional VAEs, by Won et al or ControlVAE: Model-based learning of generative controllers for physics-based characters. by Yao et al, which also uses VAEs.\n\n2. The main difference between the proposed system and other systems is that the proposed system is able to scale to the whole AMASS dataset while other systems are mainly doing locomotion or something similar. However, this is not well demonstrated in the downstream tasks, which are mostly just comprised of locomotion tasks in addition to some simple reaching, which the other systems can already do pretty well (maybe less efficiently?)."
            },
            "questions": {
                "value": "1. It will be nice to showcase some scenarios where the benefit of learning the whole AMASS dataset is useful.\n\n2.  What are the motions that PHC cannot handle?\n\n3. I feel like the motion quality produced in the downstream tasks is suboptimal/unnatural. It will be nice to have a metric to measure the motion quality generated in the downstream task for potential future improvement for future work.\n\n4. For the speed task (and other tasks as well, but speed task is one of the results in the ASE paper, so I will focus on this), looks like the motion quality of ASE is really bad, while the original ASE paper has pretty good motion quality (at least visually), any comment on the discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission461/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission461/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission461/Reviewer_a8Fw"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission461/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698982843543,
        "cdate": 1698982843543,
        "tmdate": 1700601731481,
        "mdate": 1700601731481,
        "license": "CC BY 4.0",
        "version": 2
    }
]