[
    {
        "id": "WxgAAymtDu",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_Whmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_Whmq"
        ],
        "content": {
            "summary": {
                "value": "This work presents a hierarchical Bayesian model aimed at addressing the few-shot meta-learning problem. The task-specific parameters are influenced by a higher-level global random variable, responsible for capturing shared information across tasks. Within the framework, the prediction tasks on new tasks is viewed as a Bayesian inference problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method leverages conjugate properties to provide closed-form solutions for both task-specific and task-agnostic updates. Additionally, this method seamlessly integrates with a wide range of existing neural few-shot learning meta-learners, enhancing its versatility within the field. The empirical results are robust, showcasing improved accuracy and calibration performance across various benchmarks, encompassing tasks related to classification and regression."
            },
            "weaknesses": {
                "value": "I have partially reviewed the article, particularly focusing on the derivation of mathematical formulas, and the technical aspects appear to be solid. I did not identify any significant errors or weaknesses. The only minor concern is that the overall approach and the core ideas behind this work are somewhat similar to most Bayesian meta-learning approaches in the context of few-shot learning. These methods generally utilize priors to relate the parameters of multiple tasks, and they apply variational inference for posterior inference on task-level or meta-level parameters. In this regard, the level of innovation in the article may be somewhat limited."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698393599380,
        "cdate": 1698393599380,
        "tmdate": 1699636126292,
        "mdate": 1699636126292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8j0B5y5o0a",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_EN3o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_EN3o"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the problem of few-shot learning in a model-based meta-learning setting. The authors employ a hierarchical Bayesian meta-model, variants of which have been studied extensively in the literature [1-10]. They parametrize the model by Gaussian priors for the task-specific variables and conjugate Gaussian-inverse-Wishart hyper-priors for the task-global variables, and consider various neural network based likelihood models from prior work. The authors propose a novel variational inference procedure to meta-learn the task-global posterior, which notably does not employ task-amortization in contrast to most prior work. Similarly, they determine the meta-test task-posterior using a variational approach starting from a mode of the task-global posterior. The authors include a toy experiment to demonstrate the relevance of hierarchical modeling for few-shot meta-learning. Furthermore, they evaluate their approach against several competing methods on standard benchmarks like meta-image classification, meta-regression, and pose estimation.\n\n\n-------------------------\n\n\nI increased my score from 5 to 6 after reading the author's response."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is mostly well-written and presents a conceptually interesting approach for inference in hierarchical Bayesian meta-models. The derivation of the inference procedures for the task-global variables at meta-train time as well as the task-specific variables at meta-test time appears to be executed well, but gets technically quite dense at times and leaves open several important points, cf. weaknesses. While the toy experiment in Sec. 4 seems to be unrelated to the novel inference procedure developed in Sec. 3, it can serve as a nice pedagogical illustration of the efficiency of the known hierarchical Bayesian modeling approach for few-shot learning."
            },
            "weaknesses": {
                "value": "My main concern is that central claims of the paper are not sufficiently supported by theoretical arguments nor by experimental evidence, so I encourage the authors to provide further justification:\n- The authors claim in the abstract to \"propose a novel hierarchical Bayesian model for the few-shot meta learning problem\". This is not accurate as the model and variants thereof have been studied extensively in the literature [1-10].\n- The authors claim to provide \"the first complete hierarchical Bayesian treatment of the few-shot deep learning problem\". I ask the authors to elaborate on this claim. \n  - In fact, as noted above, the model has been considered before for few-shot meta learning by a range of papers, all of which propose different parametrizations and inference approaches. The related work section mentions a few of them, but describes them as improper solutions of the Bayesian few-shot learning problem.\n  - In particular, the authors claim that a Bayesian treatment of \"only a small fraction\" of the model's weights \"considerably limits the benefits from uncertainty modeling\". The authors cite (among others) and compare to the conditional neural process [11], which is a conditional model and, thus, not a Bayesian treatment of the few-shot learning problem, as well as to the attentive neural process [12], which has been shown in [6] to  revert to a mainly deterministic approach due to the deterministic, attentive computation path. Therefore, I do not consider these methods suitable baselines to support the author's claim, i.e., to show that their approach yields improved uncertainty modeling.\n  - Unfortunately, the authors neither discuss nor compare to the Bayesian neural process versions proposed in [3] and further studied in, e.g., [4-6]. I consider such types of neural process based approaches the main competitors to the proposed inference scheme. Therefore, I ask the authors to discuss in which sense their approach is more \"complete\", and also to provide experimental evidence for their claim. In fact, performing inference over \"only a small fraction\" of the model's parameters opens the door for sophisticated variational inference methods and expressive variational approximations, which I consider a major benefit of such approaches [4,6]. In contrast, \"complete hierarchical treatments\" are typically restricted to simple Gaussian variational distributions (as also used in the proposed approach) due to the high dimensionality of the parameter space. I ask the authors to theoretically motivate and discuss, as well as to experimentally demonstrate that performing inference over all model parameters still outweighs these downsides.\n\nFurthermore, as said above, I have doubts regarding the empirical evaluation. In particular, the authors claim that their approach is superior in terms of uncertainty modeling, cf. above, but I do not consider their experimental approach suitable to support this claim:\n- Most of the experiments are classification tasks on which the authors report accuracy. I'd ask the authors to elaborate in which sense these experiments and metrics really measure uncertainty quantification. \n- I acknowledge that the authors measure calibration in Tab. 6, but still think that most of the presented experimental evidence is unsuitable to support the central claims of the paper.\n- Therefore, I encourage the authors to provide more experimental evidence for their claims. I am have no strong opinion on what experiments shall be presented, as long as they are suitable for measuring the quality of predictive uncertainty quantification, e.g.,  \n  - provide results also in terms of predictive log marginal likelihood [2,4-6,10],\n  - provide calibration measurements for further experiments, possibly also in terms of reliability diagrams [8],\n  - provide further experiments that better measure uncertainty quantification, e.g., \n    - further probabilistic regression experiments (such as the presented line-sine),\n    - experiments that require well-calibrated uncertainties for exploration-exploitation trade-off, e.g., Bayesian optimization or Bandits as in [3,6,8].\n\nLastly, I ask the authors to elaborate on some more technical issues as well as on some algorithmic choices of their proposed inference scheme that did not become fully clear to me:\n- I'm a bit confused about the inference procedure. In Sec. 2 the authors describe the standard meta-learning setting which considers a \"large number of episodes\". By construction, the task-global parameters $\\phi$ are informed by all this data, which is why related work often argues that $\\phi$ is determined with good accuracy, which allows empirical Bayes estimates, e.g., [2-6]. I'd ask the authors to elaborate on the following:\n  - What is the theoretical motivation for distributional estimate $q(\\phi)$ of the posterior of $\\phi$?\n  - Furthermore, why do we need a distributional estimate $q(\\phi)$ if at meta-test time we only use the mode of $q(\\phi)$?\n  - Along the same lines, I wonder why the authors do not use the same inference approach for the $L_*$ at meta-test time (for a test episode indexed by \\*) as they use during meta training, i.e., why don't they use the (now fixed) $L_0$ obtained from meta-training and solve Eq. (8) for $m_*$, $V_*$?\n- I'd ask the authors to add some more details on whether the two-stage optimization approach of (i) reducing the optimization problem over $(L_{1:N}, L_0)$ to one over $L_0$ alone by solving for $L_i^*$ for fixed $L_0$ and (ii) solving for $L_0$ is guaranteed to find the same solution as the original joint optimization problem.\n- I'd be interested in some more architectural details about how the combination of the authors proposed approach with set-based backbones like CNP works, cf. end of Sec. 3. \n\nFurther assorted issues:\n- Not all results come with error bounds. How were the error bounds of the presented results computed?\n- The boldfaced results do not always seem to be better within error bounds. What is the author's \"protocol\" to boldface results? \n- Table 3/4 captions do not say that classification accuracy is reported.\n\nTo summarize: I consider the proposed inference approach an interesting and technically sound contribution that is relevant for the community. Nevertheless, I recommend to reject the paper in its current form, because I find that central claims the authors make require further discussion and experimental evidence. I'm looking forward to the author's response and I'm willing to increase my score after the rebuttal period in case my concerns are adressed.\n\n[1] Bakker & Heskes, \"Task Clustering and Gating for Bayesian Multitask Learning\", JMLR 2003\n\n[2] Gordon et al., \"Meta-Learning Probabilistic Inference for Prediction\", ICLR 2019\n\n[3] Garnelo et al., \"Neural Processes\", ICML Workshop 2018\n\n[4] Volpp et al., \"Bayesian Context Aggregation for Neural Processes\", ICLR 2021\n\n[5] Wang & van Hoof, \"Double Stochastic Variational Inference for Neural Processes with Hierarchical\n\nLatent Variables\", ICML 2020\n\n[6] Volpp et al., \"Accurate Bayesian Meta-Learning by Accurate Task Posterior Inference\", ICLR 2023\n\n[7] Amit & Meir, \"Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory\", ICML 2018\n\n[8] Ravi & Beatson, \"Amortized Bayesian Meta-Learning\", ICLR 2019\n\n[9] Grant et al., \"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes\", ICLR 2018\n\n[10] Finn et al., \"Probabilistic Model-Agnostic Meta-Learning\", NeurIPS 2018\n\n[11] Garnelo et al., \"Conditional Neural Processes\", ICML 2018\n\n[12] Kim et al., \"Attentive Neural Processes\", ICLR 2019"
            },
            "questions": {
                "value": "cf. weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Reviewer_EN3o"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698575431695,
        "cdate": 1698575431695,
        "tmdate": 1700674320394,
        "mdate": 1700674320394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Fd4QZQRWyf",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_kRtC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_kRtC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel hierarchical Bayesian model for the few-shot meta-learning problem. They consider episode-wise random variables to model the generation process, where these local random variables are governed by higher-level global random variables. The global variable captures information shared across episodes while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Prediction on a novel task can be seen as a Bayesian inference problem. They propose a Normal-Inverse-Wishart model, for which establishing the local-global relationship becomes feasible due to the approximate closed-form solutions for the local posterior distributions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. This paper actually proposes a unified framework for hierarchical Bayesian learning for few-shot problems. \n2. They interpret the model in a way that MAML, ProtoNet and Reptile are special cases of the hierarchical Bayesian learning. \n3. A detailed introduction and explanation of the methods can be seen in the paper,  also good experimental results have been obtained."
            },
            "weaknesses": {
                "value": "1. Please provide more background knowledge for the Normal-Inverse-Wishart model, and why this is justified in an application for this model.\n2. This model is similar to hierarchical recurrent VAE, if so, what are the advantages of this model?\n3. More general interpretation is needed (in the sense that not only in the Bayesian learning but for deep learning in general) so that the readers can benefit more."
            },
            "questions": {
                "value": "1. From Figure 3, (c) seems can be modelled via recurrent VAEs. which were proposed before, so what are the novelties of this paper on the few-shot learning task? \n2. If possible, not only few-shot learning but other related tasks could be tested to validate the generalisation ability of the model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Reviewer_kRtC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699409095072,
        "cdate": 1699409095072,
        "tmdate": 1699636126147,
        "mdate": 1699636126147,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UVgXUWCceA",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_PGLR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_PGLR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a Bayesian framework to explicitly leverage the hierarchical structure of the generative process in few-shot learning. The Normal-Inverse-Wishart model-based variational inference and quadratic approximation allow efficient learning. The theoretical results link the essential values in the approximations to the ultimate performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Theoretical results on the PAC-Bayes-$\\lambda$ bound and regression analysis reveal the convergence of the proposed method, though there is no convergence rate.\n\n2. The proposed framework offers a unified interpretation for different established FSL methods.\n\n3. The toy example in section 4 clearly motivates the proposed hierarchical model."
            },
            "weaknesses": {
                "value": "1. There are multiple approximations, most importantly the ELBO and the quadratic approximation, in the proposed framework. Though well-motivated, the lack of empirical evolution of the effectiveness of the approximations other than the downstream performance undermines the significance of the work. Especially given that the theoretical analysis in section 5 also relies on the quality of the approximation.\n\n2. In section 7, the empirical results for some tasks only show average values rather than showing confidence intervals. And some superior performances lack statistical significance."
            },
            "questions": {
                "value": "Could the author comment on the choice of the parameters introduced by the proposed method, including the number of SGLD iterations, test-time variational inference steps for (13) or (63,64), and the number of test-time model samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699456137120,
        "cdate": 1699456137120,
        "tmdate": 1699636126045,
        "mdate": 1699636126045,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nbO0hBys3r",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_ZG8Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_ZG8Z"
        ],
        "content": {
            "summary": {
                "value": "The manuscript proposes a hierarchical Bayesian meta learning model for the few-shot learning problem, where the Normal-Inverse-Wishart (NIW) model is used to model the globally shared variables and the individual episode-wise task-specific variables. Using the NIW-Gaussian conjugacy and the stochastic gradient Langevin dynamics (SGLD), the authors propose the approximate closed-form solutions for the local posterior distributions, which makes the proposed methods scalable to large backbone networks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The manuscript proposes a complete hierarchical Bayesian model for the few-shot meta-learning problem and a scalable training algorithm. \nThe manuscript provides the theoretical analysis for the proposed method."
            },
            "weaknesses": {
                "value": "The authors claim that the proposed method shows the improved prediction accuracy and calibration performance. \n- What aspect of the proposed method improves the prediction performance is not yet clearly stated in the text. The Bayesian treatment? It should be made more explicit and clearer in the text.\n- It seems that the experiments do not include other Bayesian meta-learning methods. The hierarchical Bayesian model is the standard way to extend existing methods in Bayesian learning. I am not an expert on meta-learning but found several publications on this extension using simple keyword search. How about this paper [Reference]? I think that not comparing the proposed method with other Bayesian meta-learning methods limits the significance of the proposed method. \n- The calibration performance is validated using only a single data set. \n\n[Reference] Zhang, Z.; Li, X.; Wang, S. \u201cAmortized Bayesian Meta-Learning with Accelerated Gradient Descent Steps\u201d Appl. Sci. 2023, 13, 8653. https://doi.org/10.3390/app13158653"
            },
            "questions": {
                "value": "1.\tRegarding Section 5, what kind of information about the proposed method can we get from the final forms of the theorems, eq. (14) and eq. (15)?  For example, can we say anything about the choice of the hyperparameter \\Lambda or about the relationship between the quality of the approximation of the posterior distributions and the generalization error bound? \n\n2.\tRegarding Figure 5, first the figures can be misleading because the computational complexities of your method look constant with the common parameter with MAML (which is not, the parameters for only MAML) in the current presentation. NIW-Meta, more precisely SGLD, requires multiple iterations as it skips the first burn-in iterations. How about comparing the actual training times of both methods when they are optimized to show the similar performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699505964225,
        "cdate": 1699505964225,
        "tmdate": 1699636125971,
        "mdate": 1699636125971,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "O0XdMIVrqs",
        "forum": "mQ72XRfYRZ",
        "replyto": "mQ72XRfYRZ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_wcX1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1947/Reviewer_wcX1"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel approach to few-shot meta-learning using hierarchical Bayesian models. In particular, a global random variable governs the shared information across different tasks, whereas local random variables model the generative process for a particular task. Using Normal-Inverse-Wishart distributions allows for closed-form expressions of the ELBO, allowing local episodic optimization. The authors show that many seminal few-shot learning approaches can be seen as special cases of the hierarchical Bayesian framework while their proposed algorithm does not require a full computation graph. The hierarchical Bayesian model is motivated by a toy example that highlights the need for both shared parameters across tasks and task-specific parameters. In addition, the authors provide generalization error bounds using the PAC-Bayesian framework and evaluate the proposed hierarchical framework on a range of regression and classification tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper presents the first comprehensive treatment of the hierarchical Bayesian framework. The framework is motivated using a simple toy experiment. Both the derivations and theoretical guarantees are presented in the paper (as well as the computational complexity in the appendix), and the effectiveness of the approach is demonstrated on a number of regression and classification tasks.\n- The proposed method is architecture-independent and the authors demonstrate its effectiveness in conjunction with other meta-learning approaches such as the neural process family. In addition, seminal works on few-shot learning are shown as special cases of the hierarchical Bayesian model, demonstrating the unifying nature of this framework."
            },
            "weaknesses": {
                "value": "- The possible limitations of the approach / venues for future work are currently not discussed in the paper. Providing insights into the limitations and investigations left for future work could provide a better context for the paper in the few-shot learning community.\n- Though this is potentially a matter of personal preference, the flow of the paper could be improved by putting the toy experiment either before the derivations or after the theoretical analysis. Otherwise, it seems a bit out of place. Moreover, adding a proof sketch for Theorems 5.1 and 5.2 in the main paper would be helpful even though the full proofs are provided in the appendix."
            },
            "questions": {
                "value": "- Could distributions other than NIW be considered for the prior and variational posterior? What are the limitations of using NIW? In other words, what limitations does the choice of the prior impose on the effectiveness of the approach and how does it compare to existing few-shot learning frameworks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1947/Reviewer_wcX1"
                ]
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1947/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699615208391,
        "cdate": 1699615208391,
        "tmdate": 1699636125888,
        "mdate": 1699636125888,
        "license": "CC BY 4.0",
        "version": 2
    }
]