[
    {
        "id": "pg25m3wQGF",
        "forum": "XmkuQfWZAB",
        "replyto": "XmkuQfWZAB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees."
            },
            "weaknesses": {
                "value": "1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer 5Lzq, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698703981240,
        "cdate": 1698703981240,
        "tmdate": 1700545811673,
        "mdate": 1700545811673,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oTBCsVIGBI",
        "forum": "XmkuQfWZAB",
        "replyto": "XmkuQfWZAB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the benefits of using preference for learning reward functions in contextual bandits. Through a theoretical analysis for offline contextual bandits, the paper examines how human biases and uncertainties affect these methods' theoretical guarantees. They provided a theoretical explanation for the empirical success of preference-based methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I found this paper easy to follow. All theoretical conditions and results are clearly stated, and sufficient remarks are followed. \n\nThe way that they study the biased reward (through the definition of a transformation $h$) is interesting."
            },
            "weaknesses": {
                "value": "My biggest concern is that I found the logic of this paper a bit confusing. The authors first showed that LCB failed to achieve the desired statistical guarantee under the biased model (section 4). Then, they showed that pessimism MLE can achieve better statistical results under an unbiased model (section 5.1). This comparison is clearly unfair. Hence, the authors further studied learning preference from the biased model (section 5.2) and showed that the results are actually worse. They then remarked that\n\n> This shows if one assumes a similar amount of human bias and uncertainty in both types of human feedback, the preference-based approach is no more sample-efficient. This actually contradicts with the empirical observations in the existing literature, which suggests preference-based methods have superior performance. Hence, our theory shows the bias-free modeling plays a great role in the lower sample complexity of preference-based methods, and our theoretical results can conversely confirm the standard BTL modeling of human preference feedback\u2014it is reasonable to believe human preference data is indeed subject to less bias and uncertainty in practice.\n\nMy understanding is that, the authors are not trying to use *theory* to verify *empirical success* (which I was expecting), but rather, they use *empirical success* to prove the *theory*. Hence, it appears that this paper has undertaken a completely contrary endeavor. The authors seem to haven't truly shown any benefits of using preference from pure theory; on the contrary, the conclusions they have drawn are rather contradictory (Theorem 4). Their sole argument positing the superiority of preference relies on the fact that, in practice, it yields better experimental results, thereby suggesting that the preference is unlikely to be significantly biased. If it is really what the authors intended to convey, I don't think this result is a \"provable\" benefit but rather heuristic. This leaves me quite confused, and I hope the authors can clarify this point.\n\n\n\nSome other issue: the lower bound results (theorem 1 & 2) only considered the LCB algorithm. It will be more convincing to establish a universal and information-theoretic lower bound, i.e., a lower bound that holds for any algorithm."
            },
            "questions": {
                "value": "Is the studied algorithm, pessimistic MLE, computationally efficient? If it is not, I don't think it is fair to compare it with the more efficient LCB algorithm. Actually this question circles back to the previous one: can the lower bound be applicable to any algorithm and not solely limited to LCB?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713856236,
        "cdate": 1698713856236,
        "tmdate": 1700627220379,
        "mdate": 1700627220379,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9VYJS7JbE8",
        "forum": "XmkuQfWZAB",
        "replyto": "XmkuQfWZAB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
        ],
        "content": {
            "summary": {
                "value": "This paper analyzes the effect of bias in human feedback for contextual bandits in two settings: rating feedback (i.e. direct access to the reward function), and preference (comparison) feedback.\n\nThe effect of human biased is first quantified by proving sub-optimality bounds of two (previously existing) algorithms for contextual bandits with human feedback. The novelty in the presented bounds lie in the fact that feedback (rating or preference) is received through a bias transform.  It is later shown than for a certain class of bias transformations, solving a bandit problem with biased rating feedback, always requires more samples than solving the same problem with biased preference feedback. This is an interesting phenomena as works against the prior conception that solving a bandit/RL problem with preference feedback is more complex than with rating feedback.\nAt the same time, this statement is not entirely surprising, since the BTL preference feedback model is robust to the considered class of bias transformations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "My summary should reflect some strengths of the paper, I spell out a few more below.\n\n* The paper is well written and straightforward.\n\n* It tackles an important problem and gives a clear answer. In particular, Theorem 4, which is algorithm independent, has a really nice formulation.\n\n* I am not aware of prior work on Bandits/RL with human rating to assess how novel Theorem 1 is compared to previous sub-optimality of the LCB algorithm (e.g. in terms of techniques used to derive it). However, Theorem 1 and 3 clearly characterize the effect of a transformed/biased rating feedback."
            },
            "weaknesses": {
                "value": "* Given the assumptions on the bias transform $h$, I am not surprised that the preference-based feedback is rather invariant to such biases. So I am not sure if the final results are uncovering an informative phenomena.\n\n* A number of prior works on contextual bandits with preference feedback is not mentioned. While the overall approach is sufficiently different (they optimize a least squared loss), I think they should be mentioned for completeness.\n  - Mehta, Viraj, et al. \"Kernelized Offline Contextual Dueling Bandits.\" arXiv preprint arXiv:2307.11288 (2023).\n  - Dud\u00edk, Miroslav, et al. \"Contextual dueling bandits.\" Conference on Learning Theory. PMLR, 2015.\n  - Saha, Aadirupa, and Akshay Krishnamurthy. \"Efficient and optimal algorithms for contextual dueling bandits under realizability.\" International Conference on Algorithmic Learning Theory. PMLR, 2022.\n- Bengs, Viktor, Aadirupa Saha, and Eyke H\u00fcllermeier. \"Stochastic Contextual Dueling Bandits under Linear Stochastic Transitivity Models.\" International Conference on Machine Learning. PMLR, 2022.\n- Perhaps also: Bengs, Viktor, et al. \"Preference-based online learning with dueling bandits: A survey.\" The Journal of Machine Learning Research 22.1 (2021): 278-385."
            },
            "questions": {
                "value": "- How would you go beyond tabular setting? Would you say the pessimistic MLE algorithm can be easily extended to say, a kernelized or linear rewards over a compact domain?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698858145195,
        "cdate": 1698858145195,
        "tmdate": 1699636146108,
        "mdate": 1699636146108,
        "license": "CC BY 4.0",
        "version": 2
    }
]