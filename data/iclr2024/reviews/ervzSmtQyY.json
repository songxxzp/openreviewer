[
    {
        "id": "GxnJ6glfC4",
        "forum": "ervzSmtQyY",
        "replyto": "ervzSmtQyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_ejZU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_ejZU"
        ],
        "content": {
            "summary": {
                "value": "This paper extends the previous ARMED framework with adversarial neural networks to enhance fairness. It shows improvements in fairness across sensitive variables in various datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper has tested the proposed method on a diverse set of real-world datasets from finance and medicine, showing improvements in fairness."
            },
            "weaknesses": {
                "value": "1. The paper is very poorly written. The structure is not well-presented, and there are many grammar errors. \n2. The fairness issue is addressed with domain adversarial neural networks, which is common.\n3. The mathematical definitions are unclear. Only several losses are introduced without any detailed interpretation. \n4. It's unclear how the proposed method identifies new unseen clusters with an adversarial classifier.\n5. There is no justification for why the proposed method can improve equality-of-odds fairness."
            },
            "questions": {
                "value": "Please see the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As mentioned by Reviewer vJ3t, this paper has violated the anonymity."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8710/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8710/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8710/Reviewer_ejZU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698101626968,
        "cdate": 1698101626968,
        "tmdate": 1699637092675,
        "mdate": 1699637092675,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dy8ulFBvH5",
        "forum": "ervzSmtQyY",
        "replyto": "ervzSmtQyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_GZ1m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_GZ1m"
        ],
        "content": {
            "summary": {
                "value": "The research paper presents an enhancement to the ARMED framework, aiming to improve fairness concerning fairness-sensitive variables like age, sex, and race. Although the original ARMED framework provided commendable generalization for out-of-distribution (OOD) data, it suffered from biases towards predominant groups. To address this, the authors incorporated adversarial debiasing (adv. deb.) and absolute correlation loss (ACL) into the existing domain adversarial (DA) model, a component of the ARMED framework.\n\nThree distinct models were compared: the DA model, the fair DA adv. deb. model, and the DA ACL model. Among these, the fair DA adv. deb. model consistently exhibited enhanced fairness. Consequently, by integrating these modifications for both fixed and mixed effects, the authors devised a Fairness-enhancing ARMED framework. This refined model not only maintained a similar accuracy to the existing ARMED baseline but also exhibited considerable fairness improvements for classification tasks and better mixed effects fairness predictions in regression.\n\nThe study further showcased the model's efficacy using three datasets: the ADULT dataset, IPUMS dataset, and Heritage Health dataset. When pitted against existing neural networks, the \"Fair\" model, which synergized components from both the DA and ARMED models, produced superior fairness predictions across nearly every fairness-sensitive variable.\n\nA significant contribution highlighted in the paper is the inclusion of sub-networks, specifically the Adversarial Classifier A_F and the Adversarial Classifier A_m, to the ARMED framework. The paper's findings serve as foundational work, promoting enhanced fairness and reliability in machine learning outputs, particularly in handling OOD data and emphasizing mixed effects fairness predictions in regression scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Originality: The paper addresses the pressing issue of bias in deep learning models, especially when it comes to fairness-sensitive variables. By melding the ARMED framework with domain adversarial techniques, the research manages to elevate both fairness and reliability in its predictions.\n\nQuality: A prominent improvement is evident through a significant reduction in standard deviation , reflecting the enhanced quality of the model. The proposed model, which synergizes ARMED and DA techniques, exhibits superior performance when dealing with out-of-distribution data and fairness-sensitive variables.\n\nClarity: The research offers insights into the application and outcomes of various methods. \n\nSignificance: Manage the challenge of out-of-distribution data in deep learning, while ensuring fairness, accentuates its pivotal role in the advancement of the field."
            },
            "weaknesses": {
                "value": "Originality: \n- The paper seems to predominantly enhance the existing ARMED model by integrating two adversarial debasing components. The modification, while valuable, might not be perceived as groundbreaking, especially when viewed against the backdrop of the existing literature.\n- The work appears to be an iteration of the ARMED model rather than a transformative leap, raising concerns about the overall novelty and the magnitude of the paper's impact.\n\nClarity: \n- Several figures and illustrations used in the paper closely resemble those from the original ARMED paper. This reuse of content, without adequate new context, can create confusion.\n- Some terms, which although might be secondary in the context of this paper, are left undefined. Terms like \"h\", \"\ud835\udefd\", and \"m\", for instance, need clear explanations or references, even if briefly, to maintain reader continuity.\n- The introduction contains superfluous discussions on traditional deep learning's weaknesses and excessive literature references with insufficient explanations. This can dilute the paper's main message and confuse readers.\n- Explanations on the merging of the ARMED and Domain Adversarial models, particularly in Figure 1, are vague. A detailed breakdown or a supplementary diagram could enhance clarity.\n\nQuality:\n- The paper seems to underemphasize the importance of well-established fairness criteria. Metrics like equalized odds or demographic parity should be discussed more prominently, rather than lesser-known metrics such as TPR stdev or FPR stdev's mean and CI.\n- Although the results are analyzed thoroughly, the overall experimental setup and methodology appear to lack depth. Without a comprehensive understanding of the experiment's design and employed metrics, the derived results might seem less credible.\n- There's an observable absence of detailed visualizations in the experimental sections, reducing the impact and clarity of results presented.\n- Explanations for certain core concepts like Fixed effect, Random effect, and Mixed effect are either missing or insufficiently highlighted. Such crucial components warrant a dedicated section, possibly in the introduction or an appendix.\n\nSignificance:\n- While performance improvements are highlighted, the paper could benefit from a more persuasive argument showcasing how significant these improvements are in the larger context.\n- Drawing direct visual and textual comparisons to the older ARMED model, without differentiating the advancements made in the current paper, might diminish its perceived value."
            },
            "questions": {
                "value": "- How does the addition of two adversarial debasing components differentiate your work from the original ARMED model substantially? Could you elaborate on the unique challenges and solutions introduced in this iteration?\n- In the figures that resemble those from the original ARMED paper, are there any significant alterations or modifications that readers should be aware of? If so, could these be highlighted or differentiated more clearly?\n- What led to the decision to focus on metrics like TPR stdev or FPR stdev's mean and CI instead of the more conventionally used fairness metrics such as equalized odds or demographic parity? How do the chosen metrics enhance the study's objectives?\n- Is it possible to incorporate more detailed visualizations in the experimental sections to enhance clarity and understanding of the results? (I've seen curves which includes two axes - performance and fairness)\n- Given the stated performance improvements, could you contextualize them more persuasively? How do these improvements translate to real-world applications or the larger academic context?\n- There seems to be a strong emphasis on the paper's results. Could you provide more comprehensive background on the relevance and significance of these results in the context of existing research or practical applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Yes, Research integrity issues (e.g., plagiarism, dual submission)\n\nReason: Using the same figure from related work, especially without proper attribution or permission, can raise concerns about research integrity. Even if the related work is from the same lab or the same set of authors, it's essential to clearly cite the source and ensure there's no ambiguity about the origin of the figure. Repurposing figures or content without proper acknowledgment can be viewed as a form of self-plagiarism. It's vital for authors to ensure they have the right to use such figures and provide proper citations to maintain transparency and integrity in their research.\n\nThe primary illustration (Figure 1) of this manuscript appears to be directly sourced from a prior publication by the same group [1]. It would be helpful to contrast this with the Figure 1 presented in [1].\n\n[1] https://arxiv.org/pdf/2202.11783.pdf (TPAMI 2023)"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742426103,
        "cdate": 1698742426103,
        "tmdate": 1699637092544,
        "mdate": 1699637092544,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SGN3nEBQUp",
        "forum": "ervzSmtQyY",
        "replyto": "ervzSmtQyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_Nt9q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_Nt9q"
        ],
        "content": {
            "summary": {
                "value": "This   paper   introduces   a   novel   Fairness-Enhancing   Mixed   Effects   Deep   Learning (MEDL)   framework   that   addresses   two primary   issues   in   traditional   deep   learning (DL):   the   failure   to   account   for   non-independent   and   identically   distributed   (iid) training   samples   in   clustered   data,   and   biases   toward   the   majority   group   in   the training   data,   which   can   have significant   repercussions   in   fields   like   finance   and healthcare.   The   framework   aims   to   enhance   fairness   while   maintaining   prediction performance and interpretability.\nThe   authors   propose   the   full   fairness-enhancing   ARMED   framework,   which   adds additional   adversarial   debiasing   subnetwork   to   the   original  ARMED   framework   for fairness-promoting.   Authors   claim   that   the   combination   of   ARMED   and   domain adversarial   debiasing   method   significantly   boosts   the   fairness   of   the   model,   and shows   the   consistent  \nimprovements   of   the   new   framework   across   three   distinct datasets adopted in this study"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The proposed framework is commendable for its innovative integration of cluster adversary, Bayesian neural networks, and a mixing function, enabling the distinction between cluster-invariant fixed effects (FE) and cluster-specific random effects (RE).\n2.   The   robust   empirical   testing   across   diverse   datasets   (census/finance   and healthcare) and task types (classification and regression) validates the framework's applicability and effectiveness.\n3.  The   significant   improvements   in   fairness   (up   to   86%   in   some   variables)   without substantial   loss   in   accuracy   is   a   remarkable   achievement,   showcasing   the framework's potential to balance fairness and performance effectively."
            },
            "weaknesses": {
                "value": "1. The Results section of the paper mainly explains the improvement of fairness of the   new   framework.   There   is   a   limited   explanation   of   \u201cpreserve   interpretability advantages\u201d in the paper, which is mentioned in the abstract.\n2. In section 3.1, the authors state that \u201cwhile both Domain Adversarial Debiased (fair DA   adv.   deb.)   and   Domain   Adversarial   with   absolute   correlation   (fair   DA   ACL) enhance fairness, fair DA adv. deb. exhibits a more consistent fairness improvement across all sensitive variables.\u201d Given the results in Table 1, it is difficult to see that fair DA  adv.  deb.  exhibits  a  more  consistent  fairness  improvement.  For  many  sensitive variables, the TPR or FPR standard deviation of fair DA ACL is smaller than fair DA adv.   deb..   Especially   for   the   Marital-status   feature   on   occupations   seen   during training and Sex feature on occupations unseen during training, fair DA ACL has both TPR   and   FPR   standard   deviation   smaller   than   fair   DA  adv.   deb.,   which   indicates\nbetter fairness according to the paper. Authors then state that \u201cMoreover, fair DA adv. deb. enhances fairness with minimal\nreduction   in   balanced   accuracy   compared   with  fair   DA ACL\u20141%   vs   1.6%.   Given these   findings,   we   chose   to   incorporate   fair   DA   adv.   deb.   into   the   ARMED framework\u201d.   The   balanced   accuracy   between   fair   DA ACL   and   fair   DA  adv.   deb. seems to be quite small and does not give a convincing reason for choosing fair DA adv. deb. over fair DA ACL\n3: I would like to congratulate the authors on publishing their paper on TPAMI, however, ICLR is a decent venue for machine learning too. Advertising ARMED (Sec 2.2) can hardly be part of \"Methods\" as it is not an innovation/contribution in this paper. Note that unlike many conferences in signal processing, conference papers in top ML venues are not shortened versions of journal papers. You need to be substantially different and innovative from earlier works. Besides, the purpose of double blind is to remove the selection bias in favor of big names. As a reviewer for ICLR, knowing you have a TPAMI paper will not really affect my rating."
            },
            "questions": {
                "value": "There are too few baselines to conclude that the experiments are comprehensive enough to demonstrate its fairness. \nThe   authors   incorporate   fair   DA  adv.   deb.   instead   of   fair   DA ACL  into   the  ARMED framework based on their findings of the ablation study in section 3.1. But the reason for choosing fair DA adv. deb. over fair DA ACL is somehow not very convincing.\nAuthors can try to use the absolute correlation loss mentioned in fair DA ACL for the full fairness-enhancing ARMED framework, then make a comparison with the original proposed full fairness-enhancing ARMED."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This is arguably a dual submission - there is only incremental changes in this paper from their earlier TPAMI paper. \nThe authors used a significant portion highlighting their previous publication."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698895322688,
        "cdate": 1698895322688,
        "tmdate": 1699637092394,
        "mdate": 1699637092394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WAhJkKabnI",
        "forum": "ervzSmtQyY",
        "replyto": "ervzSmtQyY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_vJ3t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8710/Reviewer_vJ3t"
        ],
        "content": {
            "summary": {
                "value": "The authors build on prior work defining mixed effects to provide fair and robust predictors (ARMED). They add a debiasing term to ensure fairness in the fixed-effect part and demonstrate how their method improves on the ARMED baseline in terms of fairness using 3 datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**Originality**:  the authors propose amending the ARMED framework to include a fairness regularizer. The combination of mixed effects (as defined by the authors) and fairness is novel.\n\n**Quality**: the authors include multiple datasets and investigate many different attributes, not restricting themselves to binary classification and binary attributes.\n\n**Clarity**: overall the paper is clear, although the RE part of the network could be explained a bit more."
            },
            "weaknesses": {
                "value": "**Originality**: this feels like a minor modification of the ARMED framework, especially as the $L_{CCE}(S,S')$ is the same as the loss on Z.\n\n**Quality**: I believe important baselines are missing, as well as a proper discussion. For instance, the `fairness under distribution shift' is an important related field that is not cited here. Baselines from this field could be implemented, including some that include adversarial losses [1]. From my understanding, there isn't any baseline implemented outside of ARMED, even though Yang et al., 2023 is referenced.\n\nIn terms of the motivation of the method, I have major concerns:\n- the fairness loss is on the FE part of the network. What prevents the RE part of the network from inducing bias? This is actually suggested by the better fairness results from the FE network compared to its ME counterpart.\n- the authors mention that they enforce \u2018equality of odds\u2019, but they actually enforce that the model is not able to \u2018encode\u2019 the sensitive attribute. These are different criteria, and it is possible for models to encode a signal at the same level but display very different equalized odds [2].\n\n**Significance**: the results seem quite variable (looking at the 95% CI), with obvious overlaps between multiple methods especially when considering unseen clusters. Can the authors discuss the additional complexity of ARMED compared to its variance, and benefit? It would also be good to mention how statistical significance is established (which test, n, and correction for multiple comparison). In addition, please see my question on optimization below.\n\n[1] Schumann et al., 2019. Transfer of machine learning fairness across domains.\n[2] Brown et al., 2023. Detecting shortcut learning for fair medical AI using shortcut testing."
            },
            "questions": {
                "value": "Scalability: the authors investigate different numbers of samples, but the number of features remain small (max 19). Can the authors comment on the implications of e.g. using images?\n\nOptimization: the loss term includes multiple adversaries, mixes of loss types (e.g. cross-entropy with MSE), each with their own parameter. This seems like a difficult function to optimize, as even one term with an adversary can be challenging to converge. Can the authors comment on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8710/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699376693326,
        "cdate": 1699376693326,
        "tmdate": 1699637092250,
        "mdate": 1699637092250,
        "license": "CC BY 4.0",
        "version": 2
    }
]