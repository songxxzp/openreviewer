[
    {
        "id": "SbliAA4cd3",
        "forum": "h05eQniJsQ",
        "replyto": "h05eQniJsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_KaHF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_KaHF"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a notion of IBP tightness, defined as the ratio between the optimal output interval bounds and those obtained via IBP.\nA series of technical results, mostly on Deep Linear Networks (DLN) is presented, describing the requirements for perfect tightness (\"propagation invariance\") and the influence of width, depth and IBP training on tightness.\nExperimental results supporting the technical results are shown."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The research goal, providing further understanding to certified training and on the role of IBP in state-of-the-art methods, is of great interest to the community.\nThe paper is well-written and technical results are relatively coherent and well-structured.\nThe results detailing conditions for propagation invariance are novel and of potential interest to the community."
            },
            "weaknesses": {
                "value": "While the motivation is great, I am not sure this work is an actual step forward in understanding certified training.\n\nMost of the results are either fairly obvious within the certified training community (at initialisation tightness will increase with width and depth) or follow from relevant and non-cited previous work on adversarial robustness [1] (for trained networks, width will help but depth won't).\nThe authors repeatedly overclaim: I do not see why this work would \"pave the way for further advances in certified training\", or \"explain the success of recent certified training methods\". For instance, concerning the latter, the SABR paper already shows quite clearly (in its Figure 7) that strong IBP regularization is not required to obtains state-of-the-art results.\nI am also not fully convinced by the repeated arguments being made about the relevance of DLNs in this context. As it is also clear in the paper, if a network was locally linear in a perturbation region, any linear relaxation would imply exact certification. This is clearly extremely far from the networks that yield state-of-the-art certified training results, which typically require the use of branch-and-bound post-training to reach high certified accuracy (e.g., SABR).\nFinally, the presented improvements over literature results (table 1) are somewhat overstated: for SABR, the best-performing methods amongst the ones considered, they are not even necessarily a strict improvement on both standard and certified accuracy (MNIST 0.3) and they come at the cost of memory and runtime overhead. Would these result hold on different perturbation magnitudes, for instance MNIST 0.1 and CIFAR-10 8/255?\n\n[1] Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization), Zhu et al., NeurIPS 2022"
            },
            "questions": {
                "value": "The main question I had throughout the paper is: why would this notion of tightness be any more useful than measuring the standard IBP loss of a network? Tightness does not necessarily imply robustness, as IBP bounds could be very close to optimality but the network may still display adversarial examples. On the other hand, a lower IBP loss will imply robustness. For instance, the custom initialization by (Shi et al., 2021) clearly improves certified training performance (as it brings down the IBP loss at initialisation) but this is not captured by the technical results on tigthness (as stated after corollary 3.8).\nFurthermore, I am under the impression that most of the plots of the paper about tightness would apply to the (inverse of) the IBP loss too: decrease with width, depth, increase with IBP training and so on.\nI believe this should be addressed and made an integral part of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698581855212,
        "cdate": 1698581855212,
        "tmdate": 1699636996533,
        "mdate": 1699636996533,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zdpzuBcupz",
        "forum": "h05eQniJsQ",
        "replyto": "h05eQniJsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The topic of the paper is relevant, and it is an open challenge. We still don\u2019t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper."
            },
            "weaknesses": {
                "value": "1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, \u201ccertified training increases tightness\u201d and \u201clarger networks lead to better accuracy\u201d are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work."
            },
            "questions": {
                "value": "1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698654014395,
        "cdate": 1698654014395,
        "tmdate": 1699636996410,
        "mdate": 1699636996410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ckVwmKEQO3",
        "forum": "h05eQniJsQ",
        "replyto": "h05eQniJsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
        ],
        "content": {
            "summary": {
                "value": "This work studies the certified training from the theoretical perspective and applies it to explain and understand the mechanism of certified training with IBP in robustness certification. The idea of propogation tightness is formulated to analyze how IBP works and extensive experiments validate the theories from different aspects, including network width and depth, tightness and accuracy, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation of the work makes sense to me and the theory is sound, especially I like the formulation of tightness in terms of optimal box and layerwise box.\n- The paper is generally well-written and easy to read, and there are some easy examples to help the audience follow.\n- The experiments are comprehensive, which mostly validates the theory part and gives many interesting insights for certified training."
            },
            "weaknesses": {
                "value": "-  Although there are some examples in the introduction and formulation, the theory details lack some intuitive insights or explanations, e.g. Theorem 3.9 needs more insights to make it intuitive as it is one of the core theorems in this work.\n- The details of the experiments are not given in the main text; specifically, the datasets and models used in Fig. 3 are not clear. It is better to re-organize experiments part by adding a setup subsection for these necessary details.\n- Why is the certified accuracy decreasing when $\\epsilon$ increases in the middle figure in Fig. 7? More explanations and justifications are needed. Besides, it seems that there is no explanation of the trade-off between accuarcy and robustness as claimed in the abstract and introduction, excpet for tightness defined in the work, how about the certified robustness (e.g. certified accuracy or certifed radii) for the trade-off?"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811936382,
        "cdate": 1698811936382,
        "tmdate": 1699636996265,
        "mdate": 1699636996265,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4U90XUGY5y",
        "forum": "h05eQniJsQ",
        "replyto": "h05eQniJsQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
        ],
        "content": {
            "summary": {
                "value": "This paper provides theoretical analysis on IBP training and helps explain the success of IBP training over other non-IBP methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper gives a definition of the global and local propagation tightness which is new in the literature.\n2. Theorem 3.9 gives a pretty interesting result that IBP improves tightness by proving the alignment between gradients."
            },
            "weaknesses": {
                "value": "1. Some analysis in Section 4.1 is not clear, please see the questions below.\n2. What can be the potential improvement for certified training methods from your analysis?\n3. Some missing related works:\n     - [1] has a relevant conclusion on the diminishing improvement with increasing width in IBP training.\n\n[1] On the Convergence of Certified Robust Training with Interval Bound Propagation"
            },
            "questions": {
                "value": "1. In figure 5, why does a decreasing tightness lead to higher accuracy? If a looser bound is better due to weaker regularization, why is increasing the depth worse than increasing the width?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825524513,
        "cdate": 1698825524513,
        "tmdate": 1700678064677,
        "mdate": 1700678064677,
        "license": "CC BY 4.0",
        "version": 2
    }
]