[
    {
        "id": "35JrUhuA8r",
        "forum": "JVeM7uwDwK",
        "replyto": "JVeM7uwDwK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_Vyie"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_Vyie"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a lightweight and non-parametric probe, to critically analyze multimodal representations. Then the paper proposed a diagnostic dataset for coupled multimodal understanding in VideoQA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) The paper formulation is good and clear.\n\n(2) The question that the paper tries to answer is meaningful."
            },
            "weaknesses": {
                "value": "(1) Did the authors conduct any ablation studies to isolate the influence stemming from the data itself rather than the methodology? For instance, exploring whether either video or text inherently poses greater learning challenges could provide valuable insights.\n\n(2) Can these findings be extrapolated to other question-answer tasks, such as image-based question-answering?"
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698466210667,
        "cdate": 1698466210667,
        "tmdate": 1699636105242,
        "mdate": 1699636105242,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JHkTl06GhD",
        "forum": "JVeM7uwDwK",
        "replyto": "JVeM7uwDwK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_ABMn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_ABMn"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to even out the attention weights responsible for individual modalities or for modality mixing in the attention modules of a multimodal transformer model, as a way to probe which (combination of) modalities contribute to making predictions. The paper uses 2 video QA models (JustAsk, FrozenBiLM) and 4 datasets in the study (ActivityNetQA, MSRVTT-QA, NextQA, ATP-Hard). The authors then propose a new dataset (CLAVI) containing binary video QAs with good balance across positive and negative answers to better showcase the shortcuts used by existing multimodal models; they fine-tune and evaluate 4 videoQA models (JustAsk, FrozenBilm, Singularity-T, All-in-One+) on the proposed dataset and discuss their weaknesses."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "A better understanding of how multimodal models operate is of critical importance for the community. The idea of short-circuiting modalities is interesting.\nProposing more challenging and balanced benchmarks is very valuable to guide research. \nThe insight that multimodal models rely mostly on text to make predictions is valuable and shows that the video components of multimodal models need significant improvement.\nThe consistency metrics are a useful contribution."
            },
            "weaknesses": {
                "value": "A discussion about invasive vs non-invasing probing methods is needed. E.g. the authors should cite and discuss non-invasive analysis methods that rely on gradient backpropagation, e.g. MultiViz: Towards visualizing and understanding multimodal models, ICLR2023. I don\u2019t know if our current understanding of deep models is good enough to perform invasive probing like the mechanism proposed here and draw strong conclusions from it. E.g. the authors replace all attention blocks in a model with the proposed modified blocks. But it is possible that not all blocks in the model behave in the same way, e.g. early-fusion vs late-fusion of modalities. Would it make sense to replace blocks progressively and see where the performance drops?\n\nSome of the experiments are not very conclusive. E.g. in Table 1, the only clear result is that both models are significantly impaired in the video-only setting, but the short-circuiting results are not conclusive, especially for JustAsk where there is almost no difference across all SC setups. \n\nI have strong concerns about the proposed benchmark. Permuting segments in a video creates temporal discontinuities that can be exploited by the models in unexpected ways, especially when fine-tuning the models on the benchmark. Why is fine-tuning needed at all? Zero-shot evaluation would be better, especially for the purposes of diagnosing a model. \n\nPage 8, the authors say \u201cto account for class imbalances in the answers\u201d \u2013 are the positive vs negative QAs not balanced in the dataset? \n\nCould there be ambiguities in the video-question pairs when the videos are altered? E.g. in the example shown with Fig 2, \u201cholding on clothes\u201d and \u201cturning on a light\u201d; when the altered video starts, the light is already on, so saying that the light was turned on at the beginning is not completely wrong. \n\nSome questions might be ambiguous or not well defined, e.g. for the before-after negative control questions, the example in Table 2: since \u201cwashing mirror\u201d never happens in the video, it could be ambiguous to ask about a before/after relation.  \n\nCould the authors justify the choice of the models? JustAsk, FrozenBiLM have been outperformed by several newer models, so it is not clear if the analysis still holds.\n\nThe naming \u201ccounterfactual\u201d for questions whose answer is negative can be misleading. E.g. in CLEVRER or Perception Test, \u201ccounterfactual\u201d is used for questions that require imagining a different sequence of events from a given state of the environment, \u201cwhat would happen if\u2026\u201d\n\nMissing references: \nMultiViz: Towards visualizing and understanding multimodal models, ICLR2023\nPerception Test: A diagnostic benchmark for multimodal video models, NeurIPS2023 Benchmarks"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709524438,
        "cdate": 1698709524438,
        "tmdate": 1699636105159,
        "mdate": 1699636105159,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2c3N36hgeL",
        "forum": "JVeM7uwDwK",
        "replyto": "JVeM7uwDwK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_jfVh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_jfVh"
        ],
        "content": {
            "summary": {
                "value": "The authors are motivated by the question of whether the good performance of VideoQA models is from the models themselves or whether the benchmarks are not thorough enough to measure the performance. To answer the question, the authors have proposed QUAG and QUAG-attention. Moreover, they curated a CLAVI dataset with temporal counterfactuals to measure the consistency in the VideoQA performances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors have introduced interesting approaches to prove/diagnose VideoQA models. \n\n2. In addition, they clearly showed their experimental details."
            },
            "weaknesses": {
                "value": "1. QUAG and QUAG-attention have been evaluated on two models only. The authors have analyzed the performance drops in these two models on four different datasets. However, it is insufficient to conclude that Short-circuit and QUAG-attention are efficient by testing two models only. \n\n2. the authors use CLAVI to diagnose joint multimodal understanding. However, showing consistent performances on the dataset with *temporal counterfactuals* does not mean the model is free from shortcuts.\n\n3. The manuscript is a bit difficult to follow. The authors have to polish the paper. \n\n4. An alternative approach to diagnosing models is to evaluate the generalization ability (e.g. zero-shot settings). How the proposed probes are effective compared to the evaluation?\n\n5. Minor concerns: \n* page 2: Is the maximum input sequence lengths of the multimodal fusion module $l$?\n* Citation formatting."
            },
            "questions": {
                "value": "1. What are the differences between `video-consistency` and `text-consistency`? Section 3.1 explains them as follows, but they look identical.\n```\nIf the model predicts the answers for a given question correctly for both \u2013 the video and its counterfactual video, it is called video-consistent. Similarly, for a given video, if the model predicts the answers to the question and its counterfactual question correctly, it is called text-consistent.\n```"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed. The authors also discussed it in Section 6."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Reviewer_jfVh"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812474927,
        "cdate": 1698812474927,
        "tmdate": 1700707779164,
        "mdate": 1700707779164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iqG1CSJvTy",
        "forum": "JVeM7uwDwK",
        "replyto": "JVeM7uwDwK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_Ui4x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1760/Reviewer_Ui4x"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates the true impact of modalities, i.e., visual and text, on Transformer-based multimodal models for video question answer (VideoQA) tasks. The authors aim to show that many fusion models based on multiple modalities are faced with being suboptimal and the performance improvement in these models may not truly rely on the multimodal representations contrary to what they claimed. For assessing the model reliance for multimodal learning, the authors introduce a simple Quadrant Average (QUAG) operator, and a new dataset called CLAVI that is gathered as a subset of the Charades Dataset. Moreover, they emphasize the importance of new accuracy-related metrics they introduced for assessing joint multimodal learning. In the experimental evaluation, some existing models are assessed via QUAG in available benchmark datasets and additional experiments are conducted on CLAVI with baseline models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-The paper introduces an averaging-based simple technique to assess the impact of uni-modal, cross-modal representation in multimodal learning. \n\n-They mention the importance dataset for VideoQA and introduce a new data collection. The question types in the introduced subsets are interesting."
            },
            "weaknesses": {
                "value": "-The QUAG is mainly not novel as it is already computed in self-attention based fusion transformers. It is simply the averaging over submatrices and here is simply used to investigate the impact of uni/cross modality. Similar techniques, such as averaging, are already used to visualize the multimodal representations upon training. I think the technique is not novel in this aspect. \n\n-The fusion transformers can be designed in various ways. This study focuses on self-attention based fusion blocks, but cross-attention can be another direction. Particularly, the motivation of authors in selecting self-attention based fusion transformers is not clear and should be supported.\n\n-I think some claims such as \"FrozenBiLM consistently does not rely much on the video modality\" are not justified well. Many observations may still related to dataset bias rather than the model bias. Therefore, datasets should be investigated for VideoQA task. As gathering data collections hard, reporting on various datasets is a feasible way as the SOTA research conducted. \n\n-The authors gather a new dataset collection as a subset of Charades. The questions on ordering are interesting. However, they create new video samples by simply changing the order of video segments. This looks confusing as the temporal ordering of frames and transition from one activity to another is important (boundary cue). If we think simply that each frame is represented as a token, the ordering of these frames during transition is also important to answer complex questions. I think the dataset design is not so strong in this aspect."
            },
            "questions": {
                "value": "-Regarding indices used for QUAG operator on page 3: k looks like iterating over j indices (as j is in {q1 .. q2}) but the final output R(Z,W) is shown with ij indices. Can you check the correctness? Moreover, the s_ii is in {TT,TV,VT,VV} but in the above equation, it is used in a range [s_1 ... s_m]. Are the TT, TV etc. explained?\n\n-The averaging is used to fuse token representations. However, there are other ways that can be easily integrated into transformers, such as CLS tokens. Do the authors investigate the usage of CLS for the same purpose?\n\n-What is the targeted training setting in this paper for all models? For instance, the frozenBiLM reports results for fine-tuned, few-shot, and zero-shot cases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1760/Reviewer_Ui4x"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1760/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847378123,
        "cdate": 1698847378123,
        "tmdate": 1699636104996,
        "mdate": 1699636104996,
        "license": "CC BY 4.0",
        "version": 2
    }
]