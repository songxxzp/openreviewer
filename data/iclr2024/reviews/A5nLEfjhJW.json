[
    {
        "id": "AC8IjrWu3G",
        "forum": "A5nLEfjhJW",
        "replyto": "A5nLEfjhJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_yLh6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_yLh6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a concept-based approach for explainable multimodal learning named SHARCS. It projects the local concept of each modality into a shared concept space and then concatenates concepts from different modalities for prediction. SHARCS indicates the potential connection between modalities for interpretability and also shows higher performance on multimodal tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper focuses on a key problem of the interpretability of multimodal learning. There are few multimodal learning methods that provide explainable concepts before.\n2. The proposed method is novel enough and technically reasonable. This paper describes the method logically and in detail.\n3. This paper provides sufficient implementation details for reproducibility."
            },
            "weaknesses": {
                "value": "1.\tThe experimental results provided are not enough to verify the performance and interpretability of SHARCS. (1) There is a lack of quantitative evaluation for the interpretability of SHARCS, especially when interpretability is the main claim of this paper. (2) The experiments are conducted on simple datasets like MNIST and CLEVR. It is better to discuss the possibility of experiments on more complex and real-world multimodal datasets like COCO-caption.\n\n2.\tThe organization of this paper is not reasonable enough. The evaluation metrics should be one important part but the definition is not clear, especially the completeness score and the decision tree. While the learning process section is slightly abundant, which can be moved to the appendix.\n\n3.\tThere are some grammatical mistakes and typos in the text and the appendix, please carefully proofread the whole paper."
            },
            "questions": {
                "value": "1.\tIs it possible to quantitatively evaluate the interpretability of SHARCS and compare it with other methods? \n2.\tIs the completeness score a suitable and complete metric for interpretability? Can SHARCS serve as a baseline for future research?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698117510836,
        "cdate": 1698117510836,
        "tmdate": 1699636169249,
        "mdate": 1699636169249,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2X7wejCYo9",
        "forum": "A5nLEfjhJW",
        "replyto": "A5nLEfjhJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_D8iH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_D8iH"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces SHARCS, a model-agnostic concept-based approach for explainable multimodal learning. The proposed method learns and maps interpretable concepts from different heterogeneous modalities into a single unified concept-manifold, leading to an intuitive projection of semantically similar cross-modal concepts.  The authors carried out experiments on four datasets, demonstrating the superior performance of SHARCS over a range of unimodal and multimodal baseline models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. SHARCS can learn concepts from different heterogeneous modalities and project them into a unified concept manifold, which enables comprehensible explanations at different levels (modality-specific or global) and between different modalities. \n2. Its ability to handle scenarios with absent modalities renders the approach particularly pragmatic in real-world applications, where the presence of missing modalities is quite common.\n3. Thorough ablation study and detailed case analyses. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The performance data presented in Table 1 appears to be underwhelming and may raise concerns about the effectiveness of the proposed approach.\n2. The paper falls short in its analysis and comparison of contemporary multimodal explanation techniques [a][b]. It notably omits a discussion on the applicability of the approach with respect to recent multimodal foundation models like CLIP, OFA, and LLaVA, leaving uncertainties regarding its adaptability.\n3. As noted within the paper, the necessity for post-hoc inspection to address redundant concepts could be seen as a limitation, potentially hindering the usability.\n\n[a] Y. Liu and T. Tuytelaars, \u2018\u2018A deep multi-modal explanation model for zero-shot learning,\u2019\u2019 IEEE Trans. Image Process.\n[b] A Review on Explainability in Multimodal Deep Neural Nets, GARGI JOSHI etc."
            },
            "questions": {
                "value": "1. Can this approach work with multimodal embedding models such as CLIP, and generative models such as LLaVA?\n2. Can it handle cases where multiple modalities are missing?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780621149,
        "cdate": 1698780621149,
        "tmdate": 1699636169160,
        "mdate": 1699636169160,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3UcuDTyDk5",
        "forum": "A5nLEfjhJW",
        "replyto": "A5nLEfjhJW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_w4Zt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2366/Reviewer_w4Zt"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for explaining multimodal deep learning methods called SHARCS (SHARed Concept Space). SHARCS learns and maps interpretable concepts from different modalities into a single unified concept-manifold, which leads to an intuitive projection of semantically similar cross-modal concepts. The authors use this approach for explaining task predictions, improving downstream predictive performance, retrieval of missing modalities, and cross-modal explanations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of explaining multimodal models is certainly important.\n2. The idea makes sense and is well explained.\n3. The paper is generally well-written.\n4. There are some interesting controlled multimodal datasets that test specific multimodal interactions."
            },
            "weaknesses": {
                "value": "1. Tables 1 and 2 need a lot more baselines, including entire fields of study in multimodal fusion, multimodal factorized models, multimodal contrastive learning... right now there is only 'Simple Multimodal,\u2019 which combines uninterpretable embedded representations from individual local models (I'm assuming this is some encoders -> concatenate fusion -> classifier), 'Concept Multimodal,\u2019 which does late fusion? and 'Relative Representations\u2019 citing (Moschella et al., 2023). I'm not sure what these baselines are, their motivation, and why they were chosen, but they ignore so much work in the broader multimodal community. The authors should include them from and refer to https://arxiv.org/abs/2209.03430 for an extensive review.\n\n2. Section '4.2 INTERPRETABILITY' contains a lot of claims that are not evaluated.\n\n---- SHARCS discovers meaningful concepts -> there are only several qualitative anecdotes for this. Can you define meaningful, perhaps with a human annotator or human-in-the-loop evaluation? Are real humans able to annotate the concepts given SHARCS visualizations? Eg. see how MultiViz https://arxiv.org/abs/2207.00056 does it. There should be discussion and comparison to MultiViz https://arxiv.org/abs/2207.00056 and the referenced papers within it on interpreting multimodal models.\n\n---- SHARCS concepts shed light on how the task can be solved -> same thing, can you actually give SHARCS concepts to a human and see if the human can solve the task?\n\n---- SHARCS explains one modality using the other -> needs rigorous evaluation on a retrieval benchmark, of which there are many estalished ones.\n\n3. Are there any computational difficulties of the method?\n\n4. Can you show or describe a sample user interface of the visualization outputs from SHARCS, and how you would envision presenting them to users of multimodal models? This would be a useful addition to the paper.\n\n5. How can you evaluate if SHARCS has successfully learned the right concepts? There should be some sanity checks and evaluation for this."
            },
            "questions": {
                "value": "see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2366/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698813432401,
        "cdate": 1698813432401,
        "tmdate": 1699636169069,
        "mdate": 1699636169069,
        "license": "CC BY 4.0",
        "version": 2
    }
]