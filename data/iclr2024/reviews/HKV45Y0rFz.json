[
    {
        "id": "OWMWvV7lf3",
        "forum": "HKV45Y0rFz",
        "replyto": "HKV45Y0rFz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_QVJM"
        ],
        "content": {
            "summary": {
                "value": "Pretrained models can perform well on known observations but might be overconfident on unknown points, which hence incur high risks in safety-critical domains. Motivated by this, the authors proposed a conservative approach based on the Data-Driven Confidence Minimization for both selective classification and out-of-distribution detection tasks.  Particularly, they introduced a regularizer in the objective function to penalize the over-confidence in those unknown observations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors provided insight into the choice of the auxiliary dataset severed as unknown observations, which can be used in the regularization part. \n* Empirically, the authors conducted extensive experiments to show that the proposed method is promising."
            },
            "weaknesses": {
                "value": "* To be honest, I got overwhelmed for a while due to some confusing notations and definitions.\n  *  I am confused about the definition of \"unknown\" in selective classification. For examples,\n     * The authors first referred to the unknown examples as those not well-represented in training (paragraph 1, section 2), later they said that unknown examples are those misclassified points (paragraph 2, section 2). In selective classification, one may use \"hard\" observations (easily misclassified) instead of \"unknown\" to avoid the confusion of the \"unknown\" in OOD.  \n     * When it comes to the notation for unlabeled data $D_u$ exclusively used in the OOD detection, Proposition 4.1 and 4.2 then sounds targeting OOD detection with the conclusion \"DCM provably detects unknown examples\". Here I presume that \"detects unknown examples\" solely means \"detects OOD examples\", excluding the \"unknown\" examples in the selective classification task.\n  * The authors need to well-articulate the notations before these are used. For examples, \n    * What is $\\mathcal{P}_{ID}$? It is not friendly for readers not familiar with this topic.\n    * Please add the appendix reference for the definition of $\\delta$-neighborhoods in the comment after Proposition 4.1.\n    * What does $i$ in (4) stand for?\n\n* The uniform label distribution used for \"unknown\" observations is ad-hoc. It sounds like you are assuming all unknown examples overlap together and hence you cannot clearly distinguish them. However, what if there are only several overlapped classes? For example, some unknown observations from class 1 overlap with class 2, but they are disjoint with other classes. In this case, it is not appropriate to give non-zero probability to generate other \u201cpseudo\u201d labels for these unknown examples from class 1. Moreover, if this unknown example is an OOD point that is unlike any of the \"known\" classes, does it still make sense to give a non-zero probability to be labeled as \"known\" classes?\n\n* The authors need to discuss the practicality of the assumption \"$D^\\delta_{k}$ and $D^\\delta_{unk}$ are disjoint\" in Proposition 4.2 since the proposed method follows the corresponding theoretical guidance. In other words, is this assumption strong? \n  * The unknown/misclassified points (I follow your definition of \"unknown\") in selective classification could be those hard points from some classes that intrinsically overlap with each other, then \"known\" and \"unknown\" are not disjoint. \n  * In OOD detection, now that we have the assumption of \"disjoint\", why do we still bother with the unlabeled data? Why do not generate auxiliary data around but separable from ID?\n\n* The theoretical guidance is not that clear: Are the two theorems practically useful and how do the authors capitalize on these theorems in the experiments? In particular, as per Line 2, Page 5, what kind of \"appropriate threshold\" is used? Did the authors use the value of the left-hand side of the inequality in (4)?"
            },
            "questions": {
                "value": "* In Algorithm 2, since there are no \"easily misclassified\" examples with known labels like the validation data in Algorithm 1, why do not just train the model based on $\\mathcal{L}\\_{xent}+\\lambda\\mathcal{L}\\_{conf}$? In other words, is there any necessity for the prior step to optimize $\\mathcal{L}\\_{xent}$?\n\n* Equation (9) and the comment \"resulting in a mixture between the true label distribution $p$ and the uniform distribution $\\mathcal{U}$, with mixture weight $\\lambda$\" after Proposition 4.2: Is this rearrangement (9) correct? Since $\\mathcal{P}\\_{u}=\\alpha_{test}\\cdot\\mathcal{P}\\_{ID} + (1-\\alpha_{test})\\cdot\\mathcal{P}\\_{OOD}$, why is the mixture weight just $\\lambda$, wouldn't there be an extra factor $\\alpha_{test}$?\n\n* $\\epsilon$ in Proposition 4.2 and the involved proof:  \n  * The exact value of $\\epsilon$ depends on the model performance or $\\mathcal{L}(\\theta)$, how can we allow $\\epsilon\\leq\\frac{1}{2N}(\\frac{M-1}{(1+\\lambda)M})^2$ to conclude the inequality (15)? \n  * What is $M$ in (15)?\n\n* Proposition A.1: I think the dimension of $p$ is $C+1$ when it comes to OOD detection (as the authors mentioned $p$ is the true label distribution). However, the dimension of $s$ is $C$ as the authors explicitly showed. Then is that legitimate for the expression $s-p$ and $s-\\frac{\\mathbf{1}}{C}$ in (6)?\n\n* Other minor issues: \n  * Please consistently add a comma after \"i.e.\"\n  * What is (5) used for?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420557257,
        "cdate": 1698420557257,
        "tmdate": 1699636790251,
        "mdate": 1699636790251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tUWnu8qzYd",
        "forum": "HKV45Y0rFz",
        "replyto": "HKV45Y0rFz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_UGo2"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data-driven method to penalize the over-confident prediction on unknown samples. Specifically, the authors suggest that auxiliary datasets that contain unknown samples should be mixed with the original training dataset to obtain a conservative prediction. In addition, the authors propose a two-stage training scheme. For the first stage, the model is trained with training data. Then auxiliary dataset is combined with the training data to train the model with a loss composed of cross-entropy loss and regularization. To further understand the training scheme, theoretical analysis is provided by the authors which suggests that the proposed method can get a prediction confidence always lower than the true confidence.\nAdditionally, according to the analysis, a known sample tends to be given larger confidence. \nTo verify the proposed method, extensive experiments are conducted. In detail, the method is validated with selective classification and OOD detection across several image classification datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes to use an auxiliary dataset combined with a penalized loss function to reduce the confidence in unseen samples. To further understand the proposed method, the authors analyze the proposed method theoretically and get two reasonable interpretations of the proposed method. To validate the efficacy of the proposed method, several datasets are selected to conduct experiments with different counterpart methods. The proposed method shows good performance on selective classification as well as ood detection.  An ablation study of the component of the method is also given to further analyze the method. The authors also show us the prediction histogram to validate the proposition."
            },
            "weaknesses": {
                "value": "It seems that the Figure 2 is not in correct order. \nBy observing Table1, we can find a performance drop on iid setting for relative simple datasets that can achieve classification accuracy more than 99\\%. And we can also find an enhancement in the ood setting. \nHowever on relatively hard setting like FMoW, the iid performance is enhanced by the performance, but the ood and iid+ood performance does not show a significant gap compared with other methods. \nTake the loss into consideration, I am wondering whether the key is to use a strong regularization on training set and the enhancement for ood is in sacrifice of performance drop of the iid setting. \nCould the authors show the result of adding a strong label smoothing or similar regularization during pretraining for more complete comparison?\nIn addition, could the author show the results of larger datasets? I am wondering whether the regularization still works as the classification task becomes harder."
            },
            "questions": {
                "value": "Please refer to weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758914677,
        "cdate": 1698758914677,
        "tmdate": 1699636790135,
        "mdate": 1699636790135,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AWPiPpf9gq",
        "forum": "HKV45Y0rFz",
        "replyto": "HKV45Y0rFz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes the Data-Driven Confidence Minimization (DCM) framework for detecting unknown inputs in safety-critical machine learning applications. By minimizing model confidence on an uncertainty dataset, DCM achieves provable detection of unknown test examples. Experimental results demonstrate that DCM outperforms existing approaches in selective classification and out-of-distribution detection tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall I think the paper is well motivated and well written. The proposed method is well motivated by the theretical analysis and the empirical performance is convincing."
            },
            "weaknesses": {
                "value": "* For the theretical part, what can we say when the following does not hold \"(1) if the auxiliary set contains\nunknown examples similar to those seen at test time, confidence minimization leads to provable detection of unknown test examples\". Specifically, what if the auxiliary set DOES NOT contain unknown examples similar to those seen at test time. It is important to know the theretical property in this case.\n\n* The experiment results are shown in CIFAR. I would be more interested in experiments on larger scale datasets with foundation models. For example, CLIP on ImageNet. Nowadays, the interest of the community has shifted to foundation models. I believe the paper can benefit from this aspect."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6830/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6830/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6830/Reviewer_bNMd"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818596288,
        "cdate": 1698818596288,
        "tmdate": 1699636789999,
        "mdate": 1699636789999,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FBuKM5jezt",
        "forum": "HKV45Y0rFz",
        "replyto": "HKV45Y0rFz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6830/Reviewer_sVZ1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called Data-Driven Confidence Minimization (DCM) for OOD detection and selective classification (i.e., a reject option). The method builds on Outlier Exposure, using different uncertainty datasets. For selective classification, the uncertainty dataset is misclassified examples in a val set. For OOD detection, the uncertainty dataset is a potential mixture of in-distribution and OOD data. The paper includes a proof that having a noisy uncertainty dataset in this manner still allows for separating ID and OOD examples. In experiments, DCM performs well compared to several OOD detection baselines including OE."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Good empirical results\n- The paper is well-written and easy to follow"
            },
            "weaknesses": {
                "value": "- There isn't much technical novelty on top of OE. The method is mainly about selecting a new uncertainty dataset, which is a fine direction to explore, but the approach is technically simple and possible not substantial enough for ICLR.\n\n- The proof seems fairly obvious; it seems to be saying that datasets are separable even when the training data are noisy. I may have missed some details, but surely this is already well-known and a standard result in learning theory. I'm worried that this proof doesn't contribute new knowledge to the field and may give rise to a false impression.\n\n- There are numerous more recent baselines, e.g., Virtual Outlier Synthesis. It would be good to include some of these."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6830/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699311908888,
        "cdate": 1699311908888,
        "tmdate": 1699636789892,
        "mdate": 1699636789892,
        "license": "CC BY 4.0",
        "version": 2
    }
]