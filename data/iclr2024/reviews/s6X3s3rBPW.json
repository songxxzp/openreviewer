[
    {
        "id": "hjLaPXqLuF",
        "forum": "s6X3s3rBPW",
        "replyto": "s6X3s3rBPW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_AbmR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_AbmR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to leverage computerized adaptive testing for evaluating language models. Merits of doing so include 1) fewer questions to accurately evaluate models' capabilities and 2) making them comparable between models and between models and humans. The evaluation procedure involves two iterative processes: 1) estimating \"skill\" using IRT, and 2) recommend the next question based on some uncertainty measure (Fisher information). The paper uses the proposed method to evaluate a number of commercially available models (and humans) on 3 datasets, and makes various interesting observations about models' behaviors, which are also interpretable thanks to the interpretability of the inferred parameters in IRT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The idea of using CAT for evaluating LLMs appears novel. My takeaway from this new evaluation method is that\n1. we can use a lot fewer probing questions to accurately evaluate LLMs ability on a specific subject, given a pool of questions and a lot of students' answers to calibrate IRT parameters, instead of on the entire pool of questions, which may become useful in resource constraint settings, and\n2. we can interpret and characterize LLMs using the inferred IRT \"ability\", \"forget\" etc. parameters"
            },
            "weaknesses": {
                "value": "I don't have much complaint about this paper besides a few clarity questions which I put in the \"questions\" section. Those questions center around the IRT parameter estimation, the calibration process, comparability between humans and models, and interpreting Figure 4c. Due to these questions, I put my rating as 5 for now but will update the score after the authors clarify."
            },
            "questions": {
                "value": "1. How does the \"human response\" calibration in IRT work? Does it mean that the question difficulty parameter is pre-estimated using students' responses, and is then fixed during the subsequent CAT procedures?\n2. Following up the previous question, what IRT parameters are fixed and are estimated at each estimation step $t$ during CAT? My impression from the paper is that $\\theta$ is being estimated, $\\beta$ is fixed (or pre-estimated or calibrated), but what about the remaining parameters?\n3. Is the calibration process done independently for each dataset, or the responses from all three datasets are combined to estimate $\\beta$?\n4. The calibration process is done using human data, and the calibrated IRT model is used to evaluate LLMs. I think the underlying assumption is that LLMs behave like humans and thus can be evaluated using human-data calibrated IRT model. I am not sure if such assumption is valid, or can be verified; can you comment on this? Because if not, it does not make sense to use human-calibrated IRT model to evaluate LLMs because they are not comparable. Does it make more sense to have LLMs answer the questions, use these answers (in addition to human ones) to calibrate the IRT model, and then evaluate?\n5. It seems (from Appendix A.3) that some of the questions in the MATH data contain figures. How are models evaluated when the questions contain both texts and figures? My impression is that all models being evaluated are text-only models.\n6. In figure 4c, what does the student curve mean? Is it one student, or the average of many students (The caption says \"students\" whereas the text says \"student\")? The student curve appears to be more uncertain; does it imply the student(s) is/are (on average?) more careless than the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796265266,
        "cdate": 1698796265266,
        "tmdate": 1699637134435,
        "mdate": 1699637134435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Cfbxj8Ns9",
        "forum": "s6X3s3rBPW",
        "replyto": "s6X3s3rBPW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_YD4n"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_YD4n"
        ],
        "content": {
            "summary": {
                "value": "The standard way to evaluate LLMs is to use metrics such as accuracy and use all of the samples to evaluate. However, this may be inefficient and costly, especially under API frequency limits and charges. This paper use ideas from computerized adaptive testing from the education field to evaluate LLMs. The method adaptively selects questions based on the estimate of the ability of the examinee (LLM). The benefit is that we require much fewer questions and samples compared to the traditional way. With the proposed framework, the paper compares various LLMs and humans with MOOC, MATH and CODE datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Evaluation of LLMs is an important topic.\n- Recently, many of the tools and studies traditionally used to evaluate humans are being applied to assess LLMs, but to my knowledge, an evaluation method inspired by Computerized Adaptive Testing (CAT) is novel.\n- The paper is well organized.\n- The proposed framework is applied to model-model comparison and model-human comparison.\n- Code is provided."
            },
            "weaknesses": {
                "value": "- The emphasis on evaluation efficiency was not immediately evident to me. While I recognize that inference can carry significant costs, it's relatively more economical compared to the (pre-)training of LLMs. Furthermore, evaluations are typically conducted once, rendering them comparatively affordable. I can also imagine LLM providers would want to evaluate their LLMs very accurately, to demonstrate that their LLM is better than others with high confidence. In contrast to the fact that humans become tired after answering too many questions, LLM performance will not degrade as we ask more questions. Given these considerations, it would be helpful if the paper can provide more motivating situations where efficiency can be important.\n- While the comparative analysis of the abilities of various LLMs (such as lack of mathematical reasoning, being a careless student, GPT-4 is the best, etc.) is interesting, it would be beneficial to delineate specifically which aspects of the findings represent novel insights (that was not shown in previous papers). This would enhance the clarity and depth of the discussion."
            },
            "questions": {
                "value": "- What is the ChatGPT model? Is it gpt-3.5-turbo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Reviewer_YD4n"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698853880955,
        "cdate": 1698853880955,
        "tmdate": 1700634632956,
        "mdate": 1700634632956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1zo5tyhd8c",
        "forum": "s6X3s3rBPW",
        "replyto": "s6X3s3rBPW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_Mwhi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_Mwhi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new evaluation framework that can dynamically adjust the characteristics of the test questions based on the model's performance. It claims to offer ease for LMs to compare with humans, and the dynamic evaluation pipeline is more accurate in evaluating the model's abilities."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Clarity and Significance**\n\nThe presentation of the work is clear and easy to follow. The evaluation is extensive and manages to demonstrate the claimed advantage of the proposed method."
            },
            "weaknesses": {
                "value": "My concerns about this work are two-fold: 1) As far as I know, one main/initial advantage of using CAT for evaluation is to save human effort during evaluation in human studies. For example, to evaluate the student's academic performance (e.g., GRE, which is the example you used in the paper), we can use CAT to shorten the length/time duration of the evaluation. I wonder why this is the case for LLM evaluation --- now the candidates are machines rather than humans, so I guess we don't care that much about \"human effort\" or \"labor cost\" here (we can easily shorten the evaluation time by running models in parallel)?\n\nEven though it does have the above-mentioned advantage, this actually raises another concern: if every model has 20%-30% difference in their evaluation questions (as mentioned in your paper), how to make sure the evaluation is fair to each model? Do you calibrate the final score by the difficulty level or any other factors? Even if this method can produce more accurate results, I don't think it will offer ease for LLMs to compare with each other (e.g., in a leaderboard), since they are probably based on different test sets which may add a lot of communication costs. Calibration might be able to mitigate this but I believe it will introduce more noise into the evaluation pipeline.\n\nIn general, I agree with the authors that the CAT is a promising method for future evaluation (e.g., for human education), but some advantages claimed by the authors might not be valid in the context of LLM."
            },
            "questions": {
                "value": "Are there any other baseline methods? In Table 1 you show some rankings in terms of CAT evaluation results. What if you evaluate your models with the original static test sets of these tasks? Will you arrive at the same rankings? If not, what could be the reason?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698900348736,
        "cdate": 1698900348736,
        "tmdate": 1699637133231,
        "mdate": 1699637133231,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dJSnx2WXic",
        "forum": "s6X3s3rBPW",
        "replyto": "s6X3s3rBPW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_E6Gc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9005/Reviewer_E6Gc"
        ],
        "content": {
            "summary": {
                "value": "The paper uses adaptive testing, which dynamically adjusts subsequent components of assessment according to performance of the assessee, to assess several LLMs' ability in \" Subject Knowledge, Mathematical Reasoning, and Programming\"."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Employing CAT to quickly converge to an accurate skill assessment sounds like a sensible way to improve efficiency, if efficiency is indeed a problem in assessment. I don't think the domain the authors have identified actually has a real evaluation problem, though, and it's not clear to me that CAT would generalize to other domains that are much more expensive, such as free response questions."
            },
            "weaknesses": {
                "value": "- I can't tell what the core motivation is, or how this approach addresses it. I think that it's efficiency, but I can't tell why in these assessment domains, efficiency is actually a problem. \n  \n  For example, what are \"destabilizing factors\" like \"professionalism, proficiency, and biases\"? What is being destabilized, exactly?  \n  \n  Or, if we are assessing Subject Knowledge, can't that be done by MCQA, which doesn't require expert annotation once the benchmark is created? If we are assessing programming, can't we check that with pre-specified unit tests? Indeed, the appendix shows that the unidentifiable datasets used in this paper are multiple choice, as opposed to free response. In these cases, how many forward passes through the model are we actually saved by CAT, and is that substantial? I can't tell from the paper.  But I don't think it works to motivate a paper by saying \"free response is expensive so we propose this efficient method and evaluate it on multiple choice\"  \n  \n  What \"specific informativeness metric\" will we use to choose subsequent \"valuable items\" to ask the model?  \n- I also am not sure I understand how one would propose to evaluate whether CAT can be used for LMs. It's not clear to someone for whom CAT is a new idea exactly how item response theory (which is \"built on cognitive science\") could generalize to LLMs when LLM \"cognition\" does not seem to match humans'. This paper seems to skip this part of the argument and just use CAT for LMs\n- false or at the very least fringe claims asserted without evidence or even argument\n\t- \"Large language models (LLMs), like ChatGPT, have shown human-level cognitive\n\t  ability\"  \n\t- \"We believe this is just the tip of the iceberg of its latent knowledge.\"\n- sloppy writing (grammatical mistakes, ambiguous sentences/claims, )"
            },
            "questions": {
                "value": "Where in the paper do you explain how much efficiency we get from using this method, if this is indeed, the motivation? Doesn't the efficiency argument only hold in free response questions, as opposed to MCQA, as you've evaluated on here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I am very worried about evaluating the abilities' of LMs with unsubstantiated evaluations, built, in this case, for evaluating human performance. I'm not sure if I am misunderstanding the nature of CAT, and I may be, but this seems like a problematic substantiation to skip."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9005/Reviewer_E6Gc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9005/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699563380237,
        "cdate": 1699563380237,
        "tmdate": 1699637133097,
        "mdate": 1699637133097,
        "license": "CC BY 4.0",
        "version": 2
    }
]