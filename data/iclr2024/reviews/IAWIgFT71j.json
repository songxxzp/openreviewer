[
    {
        "id": "DXUyPJfsY2",
        "forum": "IAWIgFT71j",
        "replyto": "IAWIgFT71j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_yjrw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_yjrw"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an evaluation framework for judging climate information generated by large language models. The evaluation mainly focuses on presentation skills and correctness, measured with four metrics each. Climate related questions are generated from three different sources, including from GPT-4 using Wikipedia articles. State-of-the-art LLMs are used to generate content. Evaluation is done with educated humans, and experiments include assistance with AI for evaluation. Overall, LLMs don't do well in communicating at the right tone, and do not convey uncertainty information properly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A very relevant problem statement for the AI community given that LLMs are being used to simplify complicated concepts, including climate change. \n- A good summary of relevant literature and identification of metrics corresponding to presentation and epistemology \n- Well constructed dataset and evaluation mechanism with educated human annotators\n- Evaluation of human evaluators is explained clearly\n- Using LLMs to assist humans in evaluation is a good addition."
            },
            "weaknesses": {
                "value": "- The LLM prompt does not state the evaluation criteria on presentation and epistemology. If all the eight metrics were included in the prompt, it is likely the response quality will improve (my hypothesis). With the current prompt, the LLM is \"unaware\" that it is being evaluated this way. An ablation with and without such a prompt/instruction will also be interesting to see. \n- The LLM is not evaluated against a human expert output. It would be good to see an expert answer the same question, and evaluate them on the eight metrics. Currently, the 1-5 score grid is an arbitrary bar that the LLM is being held to without comparing with existing systems. \n- A discussion of wait time and cost of running this LLM experiment will be good to see. Especially, comparing these against existing human based systems will be interesting.\n- Claims in the sub-section \"Resolution and Range\" do not have sufficient evidence. \n- Concerns about audience context is raised multiple times, but is not addressed in the paper"
            },
            "questions": {
                "value": "1. Why did you use a 3-4 sentence restriction? Instead, it would have been better to ask the evaluators if the response was too long. \n2. Is GPT-4 used for all of the AI assistance generated, or do you use respective LLMs to create the assistance paragraph?\n3. Do all the LLMs support the input length to supply the paragraphs in the Wikipedia articles as input? It may be that paragraphs in the lower sections get ignored given the input text limit. \n4. What happens if answers to the questions are not available in Wikipedia? Are there such questions in your dataset? \n5. Do you include questions to which do not have answers to from the latest science? \n6. I didn't understand the concept of \"previous exposure\". Don't I have exposure as soon as I evaluate one question? From that respect, all but the first question have \"previous exposure\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698707138857,
        "cdate": 1698707138857,
        "tmdate": 1699636519916,
        "mdate": 1699636519916,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WwqLRiZMFr",
        "forum": "IAWIgFT71j",
        "replyto": "IAWIgFT71j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_6F8Y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_6F8Y"
        ],
        "content": {
            "summary": {
                "value": "The paper uses LLMs as a Q-A chat bot for disseminating climate information. It lists different criteria and evaluates different LLMs on those criteria."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "I especially like this application since it relates to an important world problem. The desiderata for good climate communication are a key contribution. The authors present criteria based on presentational and epistemological adequacy. Such a framework can be generalized to other domains for LLM evaluations, so I really appreciated this section. \n\nThe paper is well-written and evaluation of LLMs has enough discussion."
            },
            "weaknesses": {
                "value": "However, a key weakness of the work is that lack of any prompt tuning. The prompt provided is a 2-sentence prompt that does not include any of the known techniques that improve LLMs' performance. I would suggest the authors to include \n\n1) in-context examples that can be random or retrieved based on query similarity. This will help both grounding and help with tone--a factor that most LLMs were weak at. This is probably because LLM does not the expected tone---if some examples are provided, then LLM output may be similar. \n2) At a minimum, the presentational and epistemic criteria can also be included in the prompt. For example, if there is a need for a certain style (e.g., text should not informal), that can explicitly be included in the prompt. \n\nWithout basic prompt enhancements like these, the results may not an accurate assessment of the models. For instance, I suspect that the results on Tone will increase significantly after these changes. \n\nEvaluating LLM system is a combination of prompt and model. I suggest that the authors include 3-4 prompt variations and compare models to get more robust answers."
            },
            "questions": {
                "value": "1) What will happen to Tone results if you include in-context examples?\n2) Can you show ablations on Figure 2 with the prompts suggested above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698906322677,
        "cdate": 1698906322677,
        "tmdate": 1699636519820,
        "mdate": 1699636519820,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MhkLgtQN8n",
        "forum": "IAWIgFT71j",
        "replyto": "IAWIgFT71j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_Y9KD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_Y9KD"
        ],
        "content": {
            "summary": {
                "value": "Given the increasing use of LLMs in contexts where people would previously have used search engines or asked other humans, it is important to understand whether they give accurate information. In particular, artificial controversy and misinformation have slowed and reduced humanity's response to the climate crisis, making it all the more important to investigate the \"scientific communication\" occurring via use of LLMS. This paper investigates several different LLMs answers to questions about climate change. These are assessed on a 5-pt scale on 8-dimensions (accuracy, uncertainty, completeness, correctness, etc.)  by 3 human raters per answer.  An LLM is also used to generate \"assistance\" binary answers to the 8-D scale and the usefulness of this assistance is assessed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tackles an important question -- how well do LLMs do at answering questions about climate change? To my knowledge this work is completely original in tackling this question, which is of great significance.\nIt does a good job of linking to work in assessment of AI systems (e.g. scalable oversight); the related-work section overall is clear and of good quality.\nThe 8-dimension framework is clearly explained, and abundant resources are cited in the literature informing the framework. There has clearly been a lot of thought put into the framework.\nThe assessment of the AI assistance is a nice addition."
            },
            "weaknesses": {
                "value": "This paper is aiming to help us understand whether LLMs give (in)correct answers about climate information, an important and interesting thing to investigate, but almost no consideration is given to the truthfulness or trustworthyness of the answers -- e.g. given prompting about typically-conservative questions, or other random characteristics does the LLM give less accurate answers? The authors state it would be \"ideal\" for the LLM to tailor responses to the user, but this seems far from obviously ideal in the context of science facts given high levels of LLM sycophancy. Further on the point of LLM correctness, the human evaluators are not climate experts, and there is very little assessment to indicate they would know anything more about the correctness of climate science statements than the supposed \"recipient\" of the answer. They are assessed by \"doing 3 questions fully\", but there is no ground truth given, so especially combined with the AI assistance, the whole setup seems like the blind leading the blind.\n\nThe main thing I was excited to learn from this paper was whether LLMs are reliable communicators about climate information or not, including specific ways/percent of the time they are unreliable or inaccurate, and I don't understand more having read this paper than I did before.  \n\nThe rating system is given very little description compared to the 8-dimension scale. What do the numbers 1-5 mean? or what makes a fact \"4 accurate\" vs 3 or 5 accurate? What guidance was given to raters about these numbers? (their coherence also affects the interpretation of the AI assistance results) It doesn't have to be a philosophical treatise on the nature of truth or something, but maybe focusing on clear facts and evaluating binary accuracy (or binarizing the rating after the fact according to some criteria) could have been an improvement. Overall, I personally only basically care about correctness/accuracy for the purposes of this study; stuff like comprehensibility is a nice add but there are abundant other resources demonstrating the fluency and comprehensibility of LLM responses. and it seems strange/\"wrong\" to me to elevate \"style\" to to the same level of consideration as correctness.\n\nThe section explaining the 8 dimensions is thorough, but more of a lit review explaining why the dimensions are important for scientific communication (e.g. communicators should tailor to recipients numeracy level) with no connection/adaptation whatsoever to the LLM context overall or this study in particular (e.g. what is the relevance to this work of scientific consensus functioning as a gateway belief? This seems like citation fluff, or at best appropriate for an appendix going into more depth on the dimensions than should be included in the main paper). \n\nUnclear/misleading description of the \"Data\".  The \"answers\" are not ground truth answers given by experts, or sourced from wikipedia or something (right?) they're just generated by the LLM. so the keypoints are just ...getting the LLM to choose 25-100% of its own answer as being most important? Why ask the model to copy its own answer verbatim? Or this is a separate post-processing done with a different LLM? This seems unnecessarily complex given the high portion of the \"answer\" that the keypoints are. Sidenote, the term keypoint was confusing to me here, keypoints are used in computer vision (when dealing with points), usually we would say key phrases or something line that as far as I know (since they're phrases not points), but not a big deal. The post-hoc fetching of \"evidence\" seems really strange to me. If you make up facts and then search selectively for things that support your made up facts, I would not call that evidence. Maybe injecting the evidence in the prompt or something and measuring how faithful LLMs remain to the injected evidence would be helpful? But measuring evidence this way seems analagous to measuring causality via correlation. \n\nThe writing overall is wordy and overselling in a way that compromises clarity and takes up too much space. Overblown language like \"treacherous\", \"meticulously\", or \"utmost\" are out of place in scientific writing, and give the impression of overcompensating/overselling.  The intro in particular is over-broad for what is actually studied (no need for a whole paragraph on public climate literacy). In general, the paper takes a long time to get into details, and these details are not so complex that they can't be quickly mentioned in the abstract and intro. E.g. the pts assessed in the 8-dimension framework. This creates a further obfuscation effect (making the work seem more complex than it is). Show don't tell, and show earlier.\n\nThe intro focuses on factuality, but then factuality is not assessed. Instead there is a detailed description of the 8 dimensions, organized into two categories. While each element is exhaustively explained, this could go in an appendix; the words are pretty clear. However despite the verbose explanation, there are no examples, and I'm left not understanding e.g. what is the difference between accuracy and correctness?\n\nAI assistance: The assessment of the AI assistance is supposed to be about providing \"scalable\" oversight, but there's no analysis of how the assistance scales (or why it would need to in this context), or how much /if it saves the raters any time. E.g. if expert and nonexpert assessment was combined, does assistance enable us to use fewer experts per question? The assessment finds that more issues are raised, but it's unclear if these issues are \"real\" in the first experiment, and they are baked in in the second experiment (i.e. there are no controls with no issues. So false positives cannot be assessed). Also, a significant issue with using AI assistance for evaluating AI systems is whether that assistance biases the raters in an undesirable way (e.g. distract from subtle issues while raising spurious issues). \n\nVery little analysis and discussion of results, captions unclear (takeaways?), claims made that do not appear supported by the results/experimental design to me (e.g. factuality not assessed; eg. about the utility of AI assistance)"
            },
            "questions": {
                "value": "What is the difference between accuracy and correctness?\nWhat do the 5 levels mean?\nCould I see some examples of LLM answers that had the most and least issues?\nCould you make some better plots that dig into your results more? I don't have something super specific in mind, but examples of the issues would be a start, maybe breakdown of what types of Qs models are not accurate/correct on, etc.\n\nI initially rated this paper as borderline, because I think the question you're tackling is valuable, but especially on getting to this section, I realized it would take a substantial overhaul of all the experiments section (intro stuff and setting up rating framework should be about 1/2length IMO, and the experiments and discussion of results at least doubled), and probably further experiments with experts, not just random humans, a refocus on factuality, and substantial writing changes, in order to make it something I believe is above the bar for publication. I strongly encourage the authors to pursue a resubmission with improvements, and I'm willing to revise my score if at least substantially more analysis is done in the review period."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Reviewer_Y9KD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5219/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699324706028,
        "cdate": 1699324706028,
        "tmdate": 1700965726423,
        "mdate": 1700965726423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LCdnK5UeeH",
        "forum": "IAWIgFT71j",
        "replyto": "IAWIgFT71j",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_ihxg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5219/Reviewer_ihxg"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a framework to evaluate the responses of LLMs to climate change questions (based on both presentational and epistemological adequacy). The paper further analyzes the performance of seven different LLMs on these criteria, using a dataset of 300 questions (obtained from Skeptical Science, Google Trends, and GPT-4 generation of questions on English Wikipedia articles), and according to ratings provided by a group of 32 raters (three raters per question)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Topic/relevance: This submission addresses a very important topic. While there is genuine debate and disagreement as to whether LLMs should be used to serve climate change related information, the reality is that they are being used to do so in practice. Evaluation metrics are therefore key to assessing the quality and potential pitfalls of this information-serving, in order to suggest guardrails, improvements, or avoidance of use as applicable.\n\nCriteria introduced: The evaluative dimensions presented in the paper span important axes of both presentation and epistemological adequacy, and I believe the authors hit many of the needed points in this regard.\n\nMethodology for evaluation: The scope and execution of the evaluation is impressive, with meticulous documentation of criteria for gathering and filtering questions, solid criteria for training raters and evaluating (dis)agreements among them, thorough analysis of results, reporting of error bars, and inclusion of raw data and examples in the appendix. I am very appreciative of the transparency and thorough documentation of the process and results. The diversity and global spread of raters is also impressive, and I appreciate the authors' practice of compensating raters properly.\n\nEvaluation: The concrete evaluation of several models provides useful insights on the overall performance (objective and comparative) of current models. If properly contextualized, this evaluation can provide useful insight to practitioners evaluating whether or how to use LLMs for the purposes of serving climate information."
            },
            "weaknesses": {
                "value": "Before sharing my impression of weaknesses, I would like to emphasize that I really do commend the authors on this line of work, as I think it is extremely important (as noted above). It is precisely because of the importance/applicability of and attention to this kind of work that I present the feedback below, as I believe it is exceedingly important that users of this work (including individuals from ML, climate, and/or general backgrounds) are able to appropriately understand and act on the results based on a solid understanding of the limitations/caveats of the analysis. I hope the authors are able to provide appropriate discussion of these limtations/caveats based on the feedback below.\n\n\nPresentation of scope: The submission is presented as addressing \"LLM responses to climate change topics.\" However, the scope is in reality much narrower than that - it is seemingly more along the lines of \"LLM responses to general questions on climate change for the climate-curious general public\" rather than \"LLM responses to specific questions that may be used by a climate change decision-maker or practitioner to take action.\" This is evidenced, e.g., by the choice of question sources (notably, the focus on Skeptical Science and Google Trends) and the Specificity Filter used to remove questions that ask about \"a very specific and narrow topic\" (Appendix C). This distinction in scope, while subtle, is actually exceedingly important, as the stakes (and therefore, the criteria / weighting of criteria used) are very different in general-curiosity vs. action-oriented settings. To the extent to which this study will be used as a snapshot of the current state of quality of LLM responses for climate, it is critical to clarify the kinds of use cases this evaluation is actually applicable to (given the range of use cases today from, e.g., general-public-Q&A like ChatClimate and ClimateQ&A, to action-oriented models attempting to e.g. give information on climate adaptation practices to farmers who would actually implement those practices).\n\nLack of discussion of rationale for design of LLM answer structure: The authors present the protocol for getting answers, keypoints, evidence, and AI assistance from the LLM - but the rationale for using this protocol is not discussed. Is the goal to get as good a model as possible, a model that is representative of what's out there, to test out raw output vs. summarization vs. self-correction capabilities, etc.? Discussion of these design choices is critical to properly contextualize the evaluation of the outputs.\n\nImplicit weighting of criteria: Throughout the discussion of evaluative dimensions and their presentation in the numerical study, there is an implicit assumption that all criteria are weighted equally. However, different criteria can and often do have different weights - for instance, with respect to epistemological adequacy, if accuracy is not met, high performance on specificity/completeness/uncertainty is arguably irrelevant. This is not addressed in the submission at all, and scores are often averaged for the purposes of visualization and analysis. Similarly, error bars are presented to give a sense of uncertainty, but extremal measures (worst-case and best-case outcomes) are not reported - however, in settings where the answers may be \"safety-critical\" (e.g., the example of telling a farmer what practices to implement, where an incorrect answer has the potential to damage a farmer's entire crop and livelihood), worst-case outcomes in particular are key to have a handle on.\n\nChoice of raters: The cohort of 32 raters chosen for the study is described as consisting of individuals with \"at least an undergraduate degree in a climate-related field of study. This includes environmental disciplines (e.g. environmental science, earth science, atmospheric physics, ecology, environmental policy, climate economics), and also other disciplines (including the behavioral and social sciences).\" I have some concerns about this choice of raters. First, there is some mismatch between the kinds of disciplines listed and the disciplines of the questions (see Figure 5) - topics like cities-settlements-infra, energy, and health-nutrition are covered in relatively different disciplines than those listed. Second, answers can be inaccurate in subtle but pernicious ways, which requires targeted expertise to catch - it is not clear that raters were necessarily matched to questions in their area of expertise, nor that they were given time to consult external literature (e.g. by performing their own Google Search or literature search) to fully evaluate the accuracy or broader epistemological quality of answers. For context, I personally have higher than an undergraduate degree in a climate-related field of study (i.e., meet the criteria for inclusion as a rater), but would not be able to confidently answer questions about the accuracy of information on e.g. pest spread (Table 6) without consulting external sources, given that this is outside the climate sector I am most familiar with and given that such answers can be wrong in subtle ways that I may not immediately catch (e.g. warmer conditions have different effects on different pests, which means the effects can vary geographically and blanket statements may be incorrect). This concern is substantiated by observations such as low inter-rater agreement on specific answers (Appendix H), and while the authors claim that system-level conclusions can still be drawn on the basis of the results, I do not think this is categorically true and it warrants more nuanced discussion. While I acknowledge it would be unfair to ask the authors to redo the entire study, these issues of rater expertise should be foregrounded in the main body of the paper, and be taken into account via appropriate caveats on results (including the results on the effectiveness of AI assistance - see next point).\n\nAnalysis of AI assistance: Related to the previous point, the effects the authors describe with respect to the helpfulness of AI assistance may actually be more of an indication of the lack of matched expertise of the raters, rather than being a clear indication of whether AI assistance is actually helpful in reality or not (especially given lack of access to ground truth in the context of this study). In addition, results indicating that AI assistance helped raters find more mistakes may also be a result indicating phenomena more along the lines of \"anchoring and adjustment\" rather than the true presence of more mistakes in the answers (especially given that, as shown in Figure 3a, the biggest boost actually came in specificity, completeness, and uncertainty - which are more subjective - rather than accuracy).\n\nNext steps and takeaways: While the goal of the paper is presumably to motivate potential next steps and/or cautions regarding the development and use of LLMs for answering questions about climate change, it is notable that discussion of critical recommendations, next steps, or hopes for usage of the presented framework are missing. Explicit discussion is necessary to concretize the \"point\" of this paper, and provide further context for the analysis/design choices therein.\n\nAdditional comments:\n* Section 2.2, Specificity: The authors state, \"if a question is posed about climate change in India, the reply should provide data and insights relevant to the Indian context.\" This is a nuanced and potentially controversial statement (\u00e0 la the critiques on \"filter bubbles\"), and this nuance/debate should be acknowledged. (It is also worth noting that higher specificity may lead to higher trust *even in situations when the answer is inaccurate*. As a result, \"should\" is a loaded word here - if the answer is inaccurate, maybe it is better that the model provide an answer that is otherwise presentationally and epistemologically low-quality in a way that makes the user *distrust* the answer.)\n* The prompt used for the LLMs specifies the LLM is an \"expert on climate change communication\" - notably, not, e.g., an \"expert on climate change who is good at general public communication.\" In other words, the current choice of prompt implies a particular weighting between presentational vs. epistemological criteria, and a different prompt would imply a different rating. While I again acknowledge it would be unfair to ask the authors to redo the entire study by rewording the prompt, the potential implications of the current prompt should at least be highlighted explicitly. \n\nMinor:\n* Figure 1(b): It is important to flag explicitly (e.g. via a cut y-axis) that y-axis does not start at zero.\n* Figure 3: Would it be possible to add error bars to this figure?\n* Appendix, Table 6 (page 24): \"Pragraph\" --> \"Paragraph\""
            },
            "questions": {
                "value": "* Is it possible to update the body of the submission to further clarify the scope of the evaluation and add caveats accordingly, in addition to potentially sharing the exact questions that were evaluated?\n* What is the rationale for the design of the LLM answer structure? \n* Is it possible to present the evaluative dimensions and numerical results in a way that discusses not only average performance across all criteria, but is also informed by potential weightings of criteria and considers extremal (not just average) performance?\n* How might the analysis of results change given the observations above on the expertise of raters, and what caveats or re-interpretation of numerical results are needed accordingly?\n* What are the next steps and major takeaways from this work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5219/Reviewer_ihxg"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5219/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699654525355,
        "cdate": 1699654525355,
        "tmdate": 1700593003934,
        "mdate": 1700593003934,
        "license": "CC BY 4.0",
        "version": 2
    }
]