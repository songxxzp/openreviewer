[
    {
        "id": "g1r8GQ8P9O",
        "forum": "7W9zRGhLq7",
        "replyto": "7W9zRGhLq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_wiji"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_wiji"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of data heterogeneity in federated learning, where theoretical analyses have previously been pessimistic about the convergence error due to local updates. Empirical studies, however, suggest that increasing the number of local updates can enhance convergence rates and reduce communication costs when dealing with heterogeneous data. To reconcile these disparities, the paper introduces a new theoretical perspective on data heterogeneity in federated averaging (FedAvg) with non-convex objective functions. They propose a novel assumption called the \"heterogeneity-driven Lipschitz assumption\" to better account for data heterogeneity's impact on local updates. They replace the commonly used local Lipschitz constant with the heterogeneity-driven Lipschitz constant and the global Lipschitz constant in their convergence analysis. This change results in better convergence bounds for both full and partial participation, compared to previous FedAvg analyses. These findings suggest that increasing local updates can improve convergence rates even with highly heterogeneous data. Furthermore, the paper identifies a region where FedAvg, also known as local SGD, can outperform mini-batch SGD, even when the gradient divergence is substantial."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **Novel Perspective on Data Heterogeneity**: The paper introduces a fresh perspective on the impact of data heterogeneity on the federated averaging (FedAvg) algorithm by introducing the concept of the \"heterogeneity-driven Lipschitz constant.\" This innovative viewpoint sheds light on how data diversity affects the convergence behavior of the algorithm.\n\n2. **Weaker Assumptions**: The paper's assumptions are demonstrated to be weaker than those in existing literature.\n\n3. **Convergence Analysis for Non-Convex Objective Functions**: The paper extends the analysis of FedAvg to encompass general non-convex objective functions. \n\n4. **Partial Participation Inclusion**: The paper incorporates partial participation scenarios, where only a subset of workers are involved in each round of local updates. \n\n5. **Identification of Competitive Region**: The paper identifies a specific region in which local stochastic gradient descent (SGD) can outperform the widely used mini-batch SGD. \n\n6. **Experimental Validation**: The paper backs its theoretical results with experimental validation. This empirical validation shows the corresponding Lipschitz constants."
            },
            "weaknesses": {
                "value": "I have several concerns about the paper:\n\n1. **Lack of Clarity in Technical Challenges and Limited Technical Contributions**: The authors do not sufficiently show the technical challenges associated with the convergence analysis based on the new assumption. The paper could benefit from a more detailed statement of challenges from the new Lipschitz assumption, especially in comparison to the conventional local Lipschitz assumption (assumption 1). This lack of clarity leaves room for misinterpretation and raises questions about the significance of the proposed approach. As I understand, the main difference in the proof seems to revolve around using this new assumption to bound the discrepancy between the aggregated gradients and the groundtruth gradients (eq 11). In contrast, other analyses have traditionally employed the local Lipschitz assumption (assumption 1). A clearer statement of how this change significantly enhances the convergence analysis would be beneficial.\n\n2. **Experiment Results Below Expectation**: The experimental results presented in the paper fall short of expectations, particularly in comparison to prior works. In many previous studies, training and test accuracy consistently reach high levels, such as 90% for MNIST and 80% for CIFAR10 datasets. The paper's results, on the other hand, demonstrate lower performance. This discrepancy might be attributed to the specific models employed, namely linear regression and simple CNN. The choice of these relatively basic models may limit the generalizability of the findings, and more comprehensive experiments with diverse model architectures could provide a more robust assessment of the proposed approach. In addition, I believe the estimation of Lipschitz is a highlight for experiments. So I hope the author can provide a more in-depth discussion on how Lipschitz constants are estimated, the justification, and present results across a broader range of datasets and models to reinforce the paper's findings."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697940914680,
        "cdate": 1697940914680,
        "tmdate": 1699636445576,
        "mdate": 1699636445576,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C13NiJ8BaR",
        "forum": "7W9zRGhLq7",
        "replyto": "7W9zRGhLq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_RuXa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_RuXa"
        ],
        "content": {
            "summary": {
                "value": "This paper seeks to explain the benefits of local updates in FedAvg. To that end, it proposes a new but weaker assumption to replace the typically used per-client smoothness assumption. Specifically, it proposes a heterogeneity-driven Lipschitz condition on the averaged gradients (Assumption 4.2). Under this assumption, it derives convergence results for non-convex functions under full-device and partial-device participation. The authors claim that when $L_h$ (i.e., the parameter of Assumption 4.2) is small, the negative effect of multiple local updates is small. Additionally, for quadratics, when $L_h = 0$ and the stochastic gradient variance is small, the authors show that local SGD is better than mini-batch SGD (with the same number of gradient evaluations). There are some experiments to corroborate the theoretical insights."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "**1.** This paper attempts to explain the benefit of local updates in FedAvg. This is an aspect where there is a gap between theory and practice.\n\n**2.** The heterogeneity-driven Lipschitz assumption (Assumption 4.2) is weaker than per-client smoothness and has a dependence on the heterogeneity of the system. Moreover, the authors empirically show that $L_h$ (the parameter of Assumption 4.2) is small at least on the datasets that they tried.\n\n**3.** For quadratics, in the *very special case* of $L_h = 0$ and when the stochastic gradient variance is small, it is shown that local SGD is better than mini-batch SGD (with the same number of gradient evaluations)."
            },
            "weaknesses": {
                "value": "**1.** After Theorem 4.3 (in \"New insights about the effect of data heterogeneity\"), the authors write \"*A key message is that when $\\zeta^2$ is large, as long as $L_h^2$ is small enough, the error caused by local updates can still be small.*\" First of all, \"small enough\" should be precisely quantified. But more importantly, the choice of $\\gamma$ in Corollary 4.4 completely nullifies the role of $L_h^2$ in the convergence bound. The bound in Corollary 4.6 is also independent of $L_h^2$. This looks a bit strange (especially after reading the aforementioned statement).\n\n**2.** A result similar to Theorem 4.5 (and Theorem 4.4) is presented in Theorem 1 of [1] (cited below). They use the per-client smoothness assumption (Assumption 3.1 in this paper) though. The authors claim that they \"develop new techniques\" to obtain Theorem 4.5 but Appendix B does not mention what are the new techniques. Can the authors please elaborate on this? It seems that this paper obtains prior results with better constants but it is not clear to me how significant the technical challenges are in doing so.\n\n**3.** The class of quadratics in Theorem 5.5 (i.e., $A_i = A$ for all $i$) is too restrictive for me. It would have been better to obtain a general result with $A_i \\neq A$ and then state how small $L_h = 2 \\max_{i} |\\lambda(A_i - A)|$ needs to be so that local SGD is better than mini-batch SGD.\n\nOverall, I don't find any of the results or insights of this paper interesting or intriguing enough to endorse it for an ICLR publication at the moment.\n\n[1]: Jhunjhunwala, D., Sharma, P., Nagarkatti, A. and Joshi, G., 2022, August. Fedvarp: Tackling the variance due to partial client participation in federated learning. In Uncertainty in Artificial Intelligence (pp. 906-916). PMLR."
            },
            "questions": {
                "value": "**1.** Regarding Weakness 1, perhaps the authors should choose a $\\gamma$ that is *not* inversely proportional to $L_h$?\n\n**2.** Please answer the question in Weakness 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698039367865,
        "cdate": 1698039367865,
        "tmdate": 1699636445499,
        "mdate": 1699636445499,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d7hIrI6Gbn",
        "forum": "7W9zRGhLq7",
        "replyto": "7W9zRGhLq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_ziE8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_ziE8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to alternatively measure the heterogeneity in FedAvg by the Lipschitz constant of the averaged gradients instead of the local gradient. The proposed alternative Lipschitz constants are much smaller, thus yielding an improved convergence rate for FedAvg in non-convex optimization. Such improvement is further demonstrated with the example of quadratic functions, the comparison against mini-batch SGD under the same setting, and numerical evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It's a good insight, though natural as well, to measure the closeness between the average of local models and the centralized model with the proposed Lipschitzness of the averaged gradients. The proposed measure is overall a valid complement to the other heterogeneity assumptions in the literature.\n\n2. Convergence rates for both full and partial participation are derived.\n\n3. The numerical yield of the proposed Lipschitz constants seems significant."
            },
            "weaknesses": {
                "value": "1. The convergence analysis seems less technically novel as the result is almost identical to previous derivations, replacing the local Lipschitz constant with the proposed ones.\n\n2. The insight that the average of local models may be close to the centralized model even with the presence of heterogeneity is also observed in Wang et al., 2022, i.e., to instead measure the average drift at optimum (Eq. (15) in Wang et al., 2022). The corresponding discussion seems limited to that Wang et al., 2022 studies convex functions whereas this paper studies non-convex functions, without much comments on how they differ / relate in measuring heterogeneity. As a result, I'm not quite convinced by the statement \"both works cannot capture the information on the data heterogeneity contained in the local Lipschitz constant as shown in our paper so their convergence upper bounds can be worse compared to ours.\"\n\n3. The analysis in the paper seems to me more numerical than theoretical, as the improvement in convergence rates comes purely from improved constants, which are mostly measured numerically. The theoretical arguments that the proposed constants are smaller are also limited to quadratics."
            },
            "questions": {
                "value": "An improved version of Assumption 3.3 would be to assume bounded gradient divergence at optima only, as in Glasgow et al., 2022, or as Assumption 2.3 in Patel, Kumar Kshitij, et al., 2023. How would this assumption affect the comparison / relation of the proposed measure to previous measures?\n\nGlasgow, Margalit R., Honglin Yuan, and Tengyu Ma. \"Sharp bounds for federated averaging (local SGD) and continuous perspective.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.\n\nPatel, Kumar Kshitij, et al. \"On the Still Unreasonable Effectiveness of Federated Averaging for Heterogeneous Distributed Learning.\" Federated Learning and Analytics in Practice: Algorithms, Systems, Applications, and Opportunities. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Reviewer_ziE8"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607047853,
        "cdate": 1698607047853,
        "tmdate": 1699636445396,
        "mdate": 1699636445396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kzrY7IvV2N",
        "forum": "7W9zRGhLq7",
        "replyto": "7W9zRGhLq7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_NMhc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4651/Reviewer_NMhc"
        ],
        "content": {
            "summary": {
                "value": "This paper suggest to replace \"Local Lipschitz Gradient\" with two other assumptions : \"Global Lipschitz Gradient\" and \"Heterogeneity-driven Lipschitz Condition on Averaged Gradients\"  to better capture data heterogeneity of clients. The authors show that these assumptions are weaker than the classic assumption, and the constants are smaller. They prove convergence bounds for FedAvg based on the new assumptions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The new assumptions are weaker than previous one. The proof of many related works can adopt these assumptions instead of the classic Local Lipschitz Gradient without much change.\n\n2)  The authors build an example that \"Heterogeneity-driven Lipschitz constant\" ($L_h$) is zero, but \"Bounded Gradient Divergence constant\"  ($\\zeta$)  or local Lipschitz constant ($\\tilde{L}$) are large. They show that in practice $L_h$ is much smaller $\\tilde{L}$."
            },
            "weaknesses": {
                "value": "1) The contribution of the paper is limited. The bound is the same as [1]. The only difference is that the Lipschitz constants are replaced, but the bound has the same order and the same limitations as [1] with respect to $R$, $I$, and $N$.\n\n2) The paper claims several times that previous bounds don't improve with more local steps, and sometimes get worse. However, this is not true. For example in [1] or [2] the bounds decrease with the number of local steps. Again, the bound in this paper has the same order with respect to $I$ as [1]. Since, the new assumptions are weaker than the classic one, and in practice $L_h$ is not zero, these new assumptions can't help to get a better bound with respect to $I$ other than improving constants.\n\n3) The paper claims based on the bounds, FedAvg works with large $I$ and small $R$. However, this doesn't happen in practice. Also, in order for the bound to hold, we should have $\\eta \\gamma \\le \\frac{1}{4I L_g}$, and in the Corollary 4.4 the bounds $\\eta \\gamma = \\sqrt{\\frac{4FN}{RI L_g \\sigma^2}}$, which means $O(IN) \\le R$. Therefore, with the increase of number of local steps, more rounds is needed for the bound to hold.\n\n[1] Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker\nparticipation in non-iid federated learning. In International Conference on Learning\nRepresentations, 2020.\n\n[2] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and\nAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In\nInternational Conference on Machine Learning, pp. 5132\u20135143. PMLR, 2020"
            },
            "questions": {
                "value": "Please discuss the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4651/Reviewer_NMhc"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4651/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747866606,
        "cdate": 1698747866606,
        "tmdate": 1699636445286,
        "mdate": 1699636445286,
        "license": "CC BY 4.0",
        "version": 2
    }
]