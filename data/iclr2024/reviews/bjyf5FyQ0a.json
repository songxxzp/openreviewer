[
    {
        "id": "n2PPefVYG3",
        "forum": "bjyf5FyQ0a",
        "replyto": "bjyf5FyQ0a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Valley for comprehending video, image, and language within a general framework. Valley is developed by combining a large language model, a temporal modeling module, a visual encoder, and a projection module, and is trained on a video instruction dataset using a two-stage tuning procedure. Experiments show that Valley has the potential to be a highly effective video assistant."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The authors collect a video instruction-following dataset using ChatGPT with diverse types of tasks and categories, which can be helpful for instruction tuning VideoLLMs.\n3. The propsed Valley is able to comprehend video, image, and language within a general framework."
            },
            "weaknesses": {
                "value": "1. The authors mention that they collect videos from Jukinmedia2 that provides videos with wide detailed descriptions, but do not clarify where the descriptions come from. They claim that the instrcution data constructed in this way will not bring about the illusion of object level. However, since the data is constructed by ChatGPT, which is prone to hallucination, this claim may not be entirely accurate.\n2. The authors evaluate their model on general video question answering tasks, but on domain-specific image understanding benchmarks,  which seems illogical to me. As the authors claim that Vally can comprehend video, image, and language within a general framework, common image question answering datasets such as VQAv2 and GQA should be evaluated."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "In my opinion, no ethics review are needed."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1069/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1069/Reviewer_Vdhg",
                    "ICLR.cc/2024/Conference/Submission1069/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698059818644,
        "cdate": 1698059818644,
        "tmdate": 1700891490357,
        "mdate": 1700891490357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EUnEVtNpYM",
        "forum": "bjyf5FyQ0a",
        "replyto": "bjyf5FyQ0a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_1s3m"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_1s3m"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Valley, a multimodal foundational model designed to comprehend video, image, and language within a unified framework. The authors have constructed a video instruction dataset using ChatGPT, which aids in the creation of task-oriented conversation data covering a wide range of tasks. A two-stage tuning procedure is adopted for model training. Both qualitative and quantitative experiments reveal that Valley holds promise as an effective video assistant, capable of simplifying complex video understanding scenarios."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper gathers a 73k video-based instruction dataset with the help of ChatGPT. This is somewhat larger than the instruction datasets used in previous methods (e.g., VideoChat uses 11K video instruction data). Judging from the experimental results provided by the authors, the quality of this dataset appears to be quite good.\n- Experiments show that Valley excels in visual question answering and captioning, demonstrating optimal performance, strong zero-shot capability. It also generates content with fewer hallucinations compared to similar models."
            },
            "weaknesses": {
                "value": "- In terms of instruction dataset construction, there seems to be a lack of innovation and comparative experiments. It appears that a higher-quality data source was simply used to collect data, and then common methods were employed to construct the instruction dataset. This was combined with instruction datasets from previous methods to obtain a larger instruction dataset. The decent performance achieved by this method in quantitative analysis may reflect the quality of the instruction dataset to some extent. However, there has been no comparison of the quality of the instruction dataset collected in this paper with that of previous datasets under fair conditions (such as equal data volume).\n- In terms of the model methodology, there is a lack of innovation and comparative experiments. This paper attempts three temporal modeling strategies for video input, but these are very basic methods that have been widely used in tasks such as action recognition. There are no effective improvements aimed at enhancing conversational capabilities. Furthermore, the comparison with other methods like VideoChat is not detailed enough. More comparisons are needed (e.g. performance comparison under\nthe same dataset or same backbone) to demonstrate that the temporal modeling and other modules used in this paper have superior performance and efficiency."
            },
            "questions": {
                "value": "1. More fair experimental comparisons are needed to demonstrate that the instruction dataset collected in this paper is of higher quality than previous instruction datasets.\n2. More fair experimental comparisons are needed to demonstrate that the methodology proposed in this paper performs better in terms of performance or efficiency compared to previous methods.\n3. Are there any technical contributions that I have overlooked? In my view, the biggest difference between Valley and previous methods is the instruction dataset used."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698802009472,
        "cdate": 1698802009472,
        "tmdate": 1699636033464,
        "mdate": 1699636033464,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NDflAHib2a",
        "forum": "bjyf5FyQ0a",
        "replyto": "bjyf5FyQ0a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_AvQ5"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new video-based model using instruction-tuning. The key is to collect a large number of videos with detailed captions. Given the collected video-instruct data as well as the publicly available image-instruct data, they conduct a two-stage training method: (1) train the projection layer and (2) train both the projection layer and LLM. Experimental results show promising improvements on multiple public benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This is a simple and effective method. The paper is well-written and easy to follow.\n- In my humble opinion, this work could be one of the first to explore instruction tuning in the video domain.\n- Strong results on multiple benchmarks.\n- The constructed dataset should be a valuable resource to the community."
            },
            "weaknesses": {
                "value": "- While the data collection pipeline is well-formulated, this method requires very high-quality training data.  Gathering high-quality video instruction data remains very challenging when aiming for large-scale training. This prohibits very large-scale training to significantly boosting the model quality, especially when it comes to the video domain where the video data is often sparse and requires a very large number of training data.\n-  The direct integration of vision transformers and LLMs may encounter obstacles, especially when dealing with lengthy videos. While the method was tested on some benchmarks, the videos tested in this paper are often very short (only a few seconds). There is still a long distance to real-world video applications.\n- It is also difficult to ensure no hallucination in the instruction tuning data."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812475028,
        "cdate": 1698812475028,
        "tmdate": 1699636033394,
        "mdate": 1699636033394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "G2UhZFs7MN",
        "forum": "bjyf5FyQ0a",
        "replyto": "bjyf5FyQ0a",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_rHRB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1069/Reviewer_rHRB"
        ],
        "content": {
            "summary": {
                "value": "This work proposes large video-language model Valley which consists of an LLM, a temporal modeling module, a visual encoder, and a cross-modality projection module. This work also constructs a video instruction dataset using the ChatGPT to obtain conversational data for multi-shot captions, long video descriptions, action recognition, and causal relationship inference. Valley is evaluated on different benchmarks including MSVD-QA, MSRVTT-QA, Meme-Cap and Science-QA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Valley achieves the state-of-the-art performance of multiple video QA benchmarks MSVD-QA, MSRVTT-QA and ActivityNet-QA.\n* Valley collects a dataset of 100k videos with detailed caption and plans to release the dataset which will benefit the research community."
            },
            "weaknesses": {
                "value": "* It is not clear what is the technical novelty of the proposed method Valley. Throughout the introduction, related works, and method sections there is not statement that explains the technical difference distinct from the existing video-language models.\n* No ablation is provided other than the temporal modeling modules (v1, v2, v3), which also makes it difficult to judge what technical component mainly contributes to the performance.\n* Among the temporal modeling modules, what is the unique advantage of the version-2 with linear layers over v1 and v3? There seems no benchmark where the v2 performs the best and so it is not clear why v2 is proposed as one of the main methods. \n* There is no implementation details. It seems not easy to reproduce the result based on what is provided in the main manuscript."
            },
            "questions": {
                "value": "* While the dataset collection is one of the main contributions, there seems no license information about the used video data (https://www.jukinmedia.com/).\n* What is the model size and runtime comparison with other state-of-the-art methods (VideoChat, Video LLaMA, Video-ChatGPT)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This work collects and plans to release a dataset with 100K videos from https://www.jukinmedia.com/, and corresponding text annotations using the ChatGPT."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1069/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698951174878,
        "cdate": 1698951174878,
        "tmdate": 1699636033324,
        "mdate": 1699636033324,
        "license": "CC BY 4.0",
        "version": 2
    }
]