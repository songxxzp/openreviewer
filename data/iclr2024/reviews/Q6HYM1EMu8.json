[
    {
        "id": "18E6EN2wj9",
        "forum": "Q6HYM1EMu8",
        "replyto": "Q6HYM1EMu8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_j1KR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_j1KR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to leverage LLMs to generate goals and to generate code to implement reward functions in the context of a robotic environment. It looks at the impact of in-context examples and proposes automatic correction mechanisms to refine generated code."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I think the idea of using LLM to generate program-based reward functions and goals is good and will be useful for future developments in RL and autonomous learning in general."
            },
            "weaknesses": {
                "value": "The main weaknesses of this paper are along the following dimensions:\n\n* a couple of very similar works are not discussed here.\n* the evaluation is restricted to a single robotic setup with small diversity of goals and tasks considered\n* the focus on examples seems superfluous as the results don't seem to indicate that it positively impacts performance\n* the structure is hard to follow and there seem to be several ad-hoc parts that makes it not-trivial to apply this approach to other domains out of the box.\n\n-----\n\nLet's first discuss related works. There are other related approaches that are missing here and could further motivate the proposed approach. \n\n1/ the paradigm of learning from human preference (Christiano\u2019s paper at DeepMind on learning to do a backflip) attempts to circumvent the difficulty of hand-designing rewards, but is also limited by the need for much human feedback: this could be another argument to motivate the proposed approach?\n\n2/ Other approaches propose to use LLM to generate goals!\n  * https://arxiv.org/abs/2305.12487 uses an LLM to generate goals, hindsight relabels and rewards but without generating code\n  * https://arxiv.org/abs/2302.06692 uses an LLM to generate exploration goals and computes rewards with the similarity of embeddings of the goal and a caption of the transition. \n  * https://arxiv.org/pdf/2305.16291.pdf as well? or maybe they use the LLM to generate policies only.\n\nCalling an LLM to generate rewards is slow and not very robust. Your approach proposes to improve on these aspects by generating reward functions as code, which is much faster to execute, and which you can refine. \n\n3/ The origin of this line of work is the idea of reward machines: https://arxiv.org/abs/2010.03950. It seems that there are other papers proposing to use LLMs to generate reward functions in addition to Yu\u2019s paper: https://arxiv.org/abs/2309.11489 and https://arxiv.org/abs/2303.00001, could you position the proposed approach with respect to these?\n\nThe references for goal-conditioned learning seem really specific. The usual  goal-conditioned references are Tom Schaul\u2019s UVFA paper: http://proceedings.mlr.press/v37/schaul15.pdf (first goal-conditioned DRL algorithm), or Kaelbling 93 (pre-deep RL): https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6df43f70f383007a946448122b75918e3a9d6682. There are also more recent reviews of goal-conditioned RL / autotelic RL: https://arxiv.org/abs/2012.09830. Huang 2022 is a weird reference to use on its own to introduce gcrl, while plappert, nair and openai\u2019s papers also focus on very specific types of goals (target states in robotic tasks). \n\n----\n\nNow about the evaluation of the approach. The argument is that LLM can be used to automate the generation of large amounts of complex reward functions. The studied goals and tasks here are however quite limited in terms of difficulty and diversity: it's only about placing blocks in various 3D positions. These functions are very easy to code by hand and require only limited physical understanding. This approach should be evaluated on several setups, using varied goals (not only pushing blocks), including navigation goals, manipulation goals, crafting goals. \n\nHaving half of the reward function pre-coded is also weird, these bonuses might be counterproductive if we start to think of other goals the agent could do here (eg throwing blocks away, drawing an 8 with the gripper). It sounds like the approach remains very custom to the environment and I'm not sure how much is gained here in terms of engineering efforts vs reward functions outputs.\n\n----\n\nAbout the use of examples. \nThe paper says: \u201c table 1 which highlight a positive impact of supplemental examples using the GPT4 model\u201d but it does not. GPT\u2019s results are about the same with and without the examples, each beating the other in about half of the tasks. Plus it\u2019s not clear how significant are these results or even what they report exactly? Is each number the percentages of goals generated that matched the description according to hard-coded validation functions? If so, how many goals does each point represent? Are these differences statistically significant? It sounds like we\u2019re measuring two bernoulli probabilities, there are ways to test for significant differences in their parameters. What this table shows, how these numbers are computed, and their significance should be discussed. It sounds that there is no significant impact of having examples here, so this whole process of retrieval seems superfluous. \n\nI feel like the example addition is more about in-context learning and less about chain of thought. CoT is originally about querying the LLM several times in a row and prompting it to generate reasoning before eventually performing the task (eg answering a question). There is a zero-shot version of that using the let\u2019s think step by step. This is orthogonal to the question of including examples in the prompt. Here it does not seem the model is nudged to perform this chain-of-thought?\n\nTable 2 similarly seems inconclusive. Every column sometimes performs best and it\u2019s not clear that any of the columns performs significantly better than the no-example L column. The process of data collection and retrieval seems rather complicated and it's hard to get a sense of what's going on (eg where are examples of the examples retrieved?). Why doing all this if it brings no significant performance gains?\n\n----\nI think there is a confusion between tasks and goals here. In my understanding, a task refers to a specific MDP, so different tasks can vary in terms of any component of the MDP (eg state space, transition function). Goals are both a description of the desired outcome and a reward function measuring progress towards it (http://proceedings.mlr.press/v37/schaul15.pdf, https://arxiv.org/abs/2012.09830). So multi-goal problems are a subset of multi-task problems where the tasks vary in terms of the reward function + goal embeddings. \n\nUnder that framework, the proposed approach may become clearer. Both methods actually tackle multi-goal problems: one generates goal representations as target states and assumes corresponding distance-based reward functions, while the other keeps linguistic goal representations and generates the corresponding reward functions. The considered goals / tasks presented in the paper are indeed kind of the same. For instance putting the 'block on the right side' could be solved by either generating a target state + using a hard-coded distance-based reward functions or by keeping the linguistic goal representation and using a custom reward functions comparing the position of the block to the left-ride dividing line. In both case the agent is then conditioned on the goal representation (either linguistic directly or the generated target state) and gets rewarded by the reward function (either generated or assumed to be a function of the distance to the target state). I think presenting it this way makes it much clearer and may avoid repetitions.\n\nPresenting it this way + removing the example part that does not seem to bring significant advantage would make the whole presentation much clearer and would also simplify Fig1.\n\n---- \nI found the paper overall not very clearly written. The main paper itself gives little sense of what's going on, how reward functions look like or how the correction mechanism works, these very important things are left to the appendix. \n\n\n----\nMinor comments: \n* the opening quotes are not well-formated, consider using `` instead of \u201c.\n* \u201cGenerate n paraphrases for the task bellow\u201d--> \u2018below\u2019\n* I_b is not defined in table 2\n* the prompt examples in the appendix have very low resolution\n\n\n---- Conclusion\n\nI think the idea is good, but it could be better framed and presented. The final results are not very diverse so it does not convince me that anyone should really use this approach for reward function generations. I'd also like to see a discussion of other papers proposing LLM-based generation of reward functions."
            },
            "questions": {
                "value": "Is there no correction mechanism when tests are not passed? Is improvement only applied in case of an execution error? (Figure 4)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697648929204,
        "cdate": 1697648929204,
        "tmdate": 1699636556132,
        "mdate": 1699636556132,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oqRwbeq8Yk",
        "forum": "Q6HYM1EMu8",
        "replyto": "Q6HYM1EMu8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_zRqc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_zRqc"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an LLM-based approach to convert text based task descriptions into reward and goal generating functions. The effectiveness of the proposed method is evaluated via successful training of policies in robotic manipulation tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method seems to be novel and is relevant."
            },
            "weaknesses": {
                "value": "Perhaps more details, including the scope, limitations, high level idea etc., of the paper needs to be provided in the introduction.   The methodology repeatedly uses duplicate notations which hinders the ease of understanding of the paper."
            },
            "questions": {
                "value": "1.\tWhat are the limitations on the complexity of the goals? Could the approach be used in tasks with non-goal based reward functions (eg: training an agent to move in a circular pattern/do a backflip etc.,)?\n\n2.\tSeveral notations in 4.1 are duplicated. Eg: R, S, etc., have previously been used for RL related terms. Similarly, in 4.2.1, p was previously used in the transition function in Section 2,  and T was the notation used for the transition function. \n\n3.\tRegarding the code-validation described in Section 4.3, does it only ensure that the code runs without issues or does it also ensure that the reward function is valid/close to optimal?\n\n4.\tWith reference to Table 2, how are the reward functions ranked?\n\n5.\tThe limitations section don\u2019t really mention the limitations clearly. The way it is written, it gives the impression that all limitations are addressed by the auto-correction loop. \n\n6.\tNotation G is duplicated (Guidelines and Goal space)\n\n7.\tIn section 4, first paragraph, it is better to mention the kind of input gathered for the second step\n\n8.\tIs there a way to ensure that the reward function is indeed close to ideal?\n\n9.\tSeveral typos/grammatical errors \u2013 \u2018rewards functions\u2019 in conclusion. \u2018if the code fail..\u2019, \u2018can not be guarantee\u2019, \u2018commonly encounter..\u2019 in Section 4.3, Section 4.1 \u2013 \u2018bellow\u2019 should be \u2018below\u2019\n\n10. I believe Fig 15 on page 5 is supposed to be Fig 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Reviewer_zRqc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698584390568,
        "cdate": 1698584390568,
        "tmdate": 1699636556020,
        "mdate": 1699636556020,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DwSgNZlZce",
        "forum": "Q6HYM1EMu8",
        "replyto": "Q6HYM1EMu8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_Ev2L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_Ev2L"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces LARG2, a method for converting text-based task descriptions into corresponding reward and goal-generation functions. LARG2 leverages context, environment, guidelines, and task descriptions to query a code database to retrieve function examples. Then, these examples are used to convert textual task descriptions into goal poses or reward functions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies the pertinent problem of reward design and goal-generation in multi-goal/multi-task reinforcement learning. It investigates an interesting approach of combining LLM with database search as a solution to the problem. I believe that this approach is new in the growing space of LLM-aided problem design for decision making and could be impactful for the community."
            },
            "weaknesses": {
                "value": "1. This paper is very difficult to read. The text and code snippet in figures are too small to read. It would be helpful to include one illustrative example in the main text. The method section also interleaves method explanation with implementation particular to the pick-and-place tasks in the experiment section; this makes it difficult to understand how general LARG2 actually is. \n\n2. LARG2 requires a lot of assumptions to work. The goal and reward functions are not generated from the LLM in the traditional sense. Instead, LLM simply samples Cartesian coordinates in the goal generation case and composes a reward function from existing reward components provided in the simulator. It's unclear how these methods would transfer to new tasks that do not come with existing assets that allow LARG2 to perform it search and retrieval procedure. \n\n3. The experiments seem preliminary. They are carried out only in a single pick-and-place setup. There are no baseline comparison. The paper also lacks any ablation study besides choices of LLMs; given the fairly involved and complicated algorithmic design, an ablation study would be very helpful to better understand the importance of various LARG2 components."
            },
            "questions": {
                "value": "1. An external baseline should be included. Comparison to a human-written reward function would help ground the performance of LARG2.\n\n2. Ablation studies should be included. \n\n3. More robot tasks and morphologies should be considered.\n\n4. Could the goals in Figure 7 be overlayyed on top of the actual table top environment? This will make interpreting the figure much more easier."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698593374453,
        "cdate": 1698593374453,
        "tmdate": 1699636555919,
        "mdate": 1699636555919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JJJXbzjZGB",
        "forum": "Q6HYM1EMu8",
        "replyto": "Q6HYM1EMu8",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_S7zE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5460/Reviewer_S7zE"
        ],
        "content": {
            "summary": {
                "value": "The authors propose \"Language-based Automatic Reward and Goal Generation\" ($LARG^2$) to do automatic reward generation with LLM for goal-conditioned and multi-task RL problems. $LARG^2$ incorporates an automatic code correction loop and a supplemental code example database for CoT inferences to improve the performance. The proposed method achieves good success rate in a simulated tabletop robot arm environment."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method is able to complete most of the proposed tasks with a decent success rate (assuming the performance in Table 1 and 2 is the success rate)."
            },
            "weaknesses": {
                "value": "- In general, the writing and formatting of the paper are subpar.\nThe method description (Sec 4) is not well-structured or motivated. It feels more like a step-by-step explanation of what happened and a lot of the details can be left out for the sake of clarity. Personally, I also find using single characters to denote a large variety of things (prompts, text descriptions, codes, functions, sets, etc) confusing and hard to follow.\n- There are no visuals of the experiment setup nor any detailed descriptions of the scene. From the description in the paper, it's very hard for me to evaluate the difficulties of the tasks.\n- The contribution of this work is very questionable. From my understanding, these are the claimed contributions:\n    - An automatic process of code correction with LLMs and feedback from the code interpreter. This is not a new concept at all and can be considered more or less a standard procedure in LLM-assisted code generation, for a reference see [1]. From the description in this paper, I don't see any major differences from existing methods.\n    - Using supplemental code examples collected from open-source repositories for CoT. Furthermore, the baseline method $L$ without supplemental examples in Table 2 still performs reasonably well compared to $LARG^2$.\n    - Strong empirical performances in GC and MT tasks. However, there are no baseline methods to compare with in the experiment section. It is therefore very hard to evaluate the proposed method. \n\nMinor mistakes and typos:\n\n- In the abstract, the abbreviation \"LLM\" is mentioned before the full name.\n- Second to the last line of Sec 3.4, \"Chain-of-though\" should be \"Chain-of-Thought\".\n- Texts in Figure 3 are too small to read. Consider using a larger font size.\n- First line of Sec 4.2.1, \"re-arranging a set of objects composing the scene\". Do you mean \"re-arranging a set of objects in the scene\"?"
            },
            "questions": {
                "value": "- How are the supplemental example codes utilized in the prompting? It's surprising that this part is not described at all in the main text despite being one of the major contributions.\n- How is the \"performance\" in Table 1 and 2 measured?\n- What is the purpose of Figure 7? What is the information I am suppose to get from the generated goal positions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5460/Reviewer_S7zE"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5460/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698786563585,
        "cdate": 1698786563585,
        "tmdate": 1699636555714,
        "mdate": 1699636555714,
        "license": "CC BY 4.0",
        "version": 2
    }
]