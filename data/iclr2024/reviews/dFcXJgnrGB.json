[
    {
        "id": "XOoiXMT8gR",
        "forum": "dFcXJgnrGB",
        "replyto": "dFcXJgnrGB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to leverage knowledge distillation to train smaller LMs to replicate the procedural plan generation abilities of LLMs.This serves as a cost-effective alternative to achieve the same performance as LLMs using smaller LMs. The authors generate a novel dataset, called CoPlan which includes goal-based and constrained planning tasks, as well as counterfactual replanning tasks. When trained on the CoPlan dataset, the smaller (distilled) models showcased comparable performance to their LLM counterparts. Empirical and ablation experiments further demonstrate the same."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Overall, the paper is well-written and has a smooth flow, which makes it easy to follow.\n\n2. The data collection process used to generate CoPlan is novel and would be useful for researchers to collect high-quality data with minimal human involvement.\n\n3. Some of the results shown in the paper were interesting, although not entirely surprising."
            },
            "weaknesses": {
                "value": "1. Novelty: The main contribution of this work -- to train a small LM to imitate an LLM by using the LLM as a teacher to train the small LM, is not novel. It has already been demonstrated in [A] (and has not been cited here) for a variety of tasks including complex reasoning If considered in the context of [A], the novelty here is limited to its extension to planning. The use of beam search-based planning is also very similar to [B], however, given its recency, I have discounted it in my evaluation. \n\n2. Missing Key Experiments: While the authors motivate the use of knowledge distillation and compare their distilled models with that of the teacher model, the comparisons with the original (undistilled) model seem to be missing. Without this, it is hard to gauge the performance enhancement from distillation.\n\n[A] Orca: Progressive Learning from Complex Explanation Traces of GPT-4, Mukherjee et al., 2023\n\n[B] SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge, Hazra et al., 2023\n\n--------------------------------------------------------------\n\nEdit 1:\n\nI'm discarding the Novelty contention given that [A] is considered \"contemporaneous\" according to ICLR guidelines. The authors have also answered the aspect of the missing experiment. Hence, I'm raising my score."
            },
            "questions": {
                "value": "1. How is CoPlan different from ProScript? Barring the size factor, is it the counterfactual and replanning subset that is novel? Or is there a difference in the diversity of data too?\n\n2. Can you explain the use of the term \"symbolic\" in \"symbolic procedural knowledge distillation\"? What is \"symbolic\" here?\n\n3. It would be interesting to see when distillation leads to overfitting. What factors (model size of small LM, amount of training data) does it depend on? This would help motivate the generation of a larger dataset (compared to existing ones like ProScript).\n\n4. Minor Comment: The use of the term \"task\" in Sec 2.2 is ambiguous. The authors should clarify upfront that the three tasks are the three different settings that they investigate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8605/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_bpqq",
                    "ICLR.cc/2024/Conference/Submission8605/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698706748778,
        "cdate": 1698706748778,
        "tmdate": 1700391265467,
        "mdate": 1700391265467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GDfmZLE9Y2",
        "forum": "dFcXJgnrGB",
        "replyto": "dFcXJgnrGB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_EJrs"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the use LLMs for generating NL instructions for a given task, called procedural plans. The paper proposes that smaller LLMs trained specifically for generation of such plans can outperform the models used as teacher, and be on par of larger models.\n\nAlgorithmically, the contributions are three:\n- PlaSma: small model specialized in generating NL instructions.\n- PlaSma+: uses PlaSma and an additional model for biasing the output towards higher validity. The paper refers to this bias as a \u201cconstraint\u201d.\n- CoPlan dataset\n\nThe key transversal issues are both data generation and evaluation.\n\nThe dataset (CoPlan) is generated using a combination of seed prompts, large models, and human validation.\n\nThe experimental setup is reasonable these days: use proprietary GPT as teacher and for generating data; use T5 variants as small models; BERT-variations for classification tasks.\n\nThe evaluation is more complicated. The paper reports good human evaluation results in one dataset as the plans cannot be tested. (The  appendix reports usual BLEU and ROUGE scores, perhaps for pacifying some reviewers, but for natural situations there are so many possible wordings that that might very misleading). For VirtualHome, they report an interesting success rate.\n\nThe key question is whether the smaller model is just mimicking the teacher\u2019s behaviour. However, the paper reports that the student might outperform the teacher significantly, especially if it has enough capacity. The bias-towards verification model has a higher impact in models with lower capacity."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Interesting problem as instructions are a key possible application of LLM. Sensible to scale and cost.\n- Good description of the methodology in all aspects.\n\t- In particular the data generation vs curation.\n- Sensible complexity of the tasks: goal, conditions, verification.\n- A secondary model specialized in higher correctness is a good idea while focusing at lower capacity."
            },
            "weaknesses": {
                "value": "- The dataset ProScript is not well explained\n\t- It is hard to qualify the complexity of the instructions.\n\t- So, the results in Table 2 are hard to understand because we don\u2019t know about the relative complexity of the task and the diversity of tasks.\n- I suggest reducing the tone of the phrase \u201cwe introduce the task of counterfactual planning\u201d. A quick search in google scholar for \u201cplan revision\u201d reported, for instance, these papers: \n\t- Ow, P. S., Smith, S. F., & Thiriez, A. Reactive plan revision. AAAI 1998.\n\t- Williams, K., & Burdick, J. Multi-robot boundary coverage with plan revision. In Proceedings 2006 IEEE International Conference on Robotics and Automation.\n- Abuse of some terms\n\t- The \u201cstep verifier\u201d is **not** verifying, but adding a bias. For instance, the paper mentions that in the case of embodied agents, that verification can be taken over by a safety module. In that scenario, with reasonable \\alpha that follows the LLM when the so-called verification is not saying anything relevant, the aggregation of Eq (3) cannot prevent cathastrophic errors that are considered very attractive by the LLM. Perhaps a better name would be \u201cquality bias\u201d or anything saying bias.\n\t- Same applies to the notion of \u201csymbolic knowledge destilation\u201d, but we are probably too late for this one. I find it problematic that in AI we associate NL with symbolic, as the word is overloaded with a huge body of work in AI ranging from logic to graphical models. It should be more clear to call it something like \u201cinstruction distillation\u201d."
            },
            "questions": {
                "value": "- Except for PlaSma-Mul, what does PlaSma mean when measured in different tasks. Can you elaborate on how that manifests in the experiments?\n\t- For instance, in Table 1, the PlaSma model is trained in the planning task, so there are precisely 6 models there. Right?\n\t- Those models are completely different from the ones reported in Fig 4, correct?\n- Please describe the ProScript in-depth and discuss why this is a good dataset for studying this problem.\n- Please discuss what other datasets could have been used, and explain why some possibilities are inconvenient. \n\t- This should be added to the related work section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698877319523,
        "cdate": 1698877319523,
        "tmdate": 1699637076827,
        "mdate": 1699637076827,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RZI9Gwzklz",
        "forum": "dFcXJgnrGB",
        "replyto": "dFcXJgnrGB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a framework designed to improve the procedural knowledge and planning capabilities of small language models. This is achieved through symbolic procedural knowledge distillation and a verifier-guided step-wise beam search algorithm. The authors have conducted experiments to compare student models of varying sizes with their teacher model, and have utilized human evaluations to assess the generated plans in terms of sequence, completeness, and overall quality. The findings indicate that smaller models can reach or even surpass the performance of larger models by employing the PLASMA framework."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is well-written and well-structured.\n- Equipping small language models to come up with procedural knowledge at the same level as large language models is an important direction from an engineering perspective given the accessibility, carbon footprint, and cost of large language models."
            },
            "weaknesses": {
                "value": "- Although human evaluations were conducted, the executability conditions for the plans in the domains used in these experiments seem to be loose. It would be beneficial to evaluate the models in domains which have hard executability conditions (like the domains used in International Planning Competitions), where the correctness can be objectively determined, to more accurately gauge the language planning abilities of the proposed method.\n- A comparison with GPT-4, in addition to GPT-3, could provide additional insights into the effectiveness of the method.\n- The potential for increased bias due to the distillation from larger language models is mentioned in the limitations section but remains a concern."
            },
            "questions": {
                "value": "- If smaller language models can be effectively paired with human input or external verifiers for improved planning, why is distillation from a larger model necessary? This question is particularly relevant given that the domains discussed in the paper appear to be amenable to human verification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8605/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8605/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8605/Reviewer_h13v"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698923587663,
        "cdate": 1698923587663,
        "tmdate": 1700736382630,
        "mdate": 1700736382630,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oVMAwaIfbS",
        "forum": "dFcXJgnrGB",
        "replyto": "dFcXJgnrGB",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_drVQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8605/Reviewer_drVQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a distillation procedure and an inference-time decoding algorithm to enable relative small language models for planning and replanning with performance close or surpassing its larger language teacher models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022 Proposed a paradigm to distill procedural planning knowledge from large language models to enable smaller languages to do planning, and it seems to be working. \n\t\u2022 Within the paradigm, a cost-effective human-in-loop LLM generated data curation procedure is also proposed to create the COPLAN dataset.\n\t\u2022 Proposed a  guided decoding procedure with a LLM-based (RoBERTa) step verifier to  guide the beam-search during planning steps decoding generation. The guidance help to further regulate the validity of the steps."
            },
            "weaknesses": {
                "value": "\u2022 The LLM-to-Planning-Model teacher-student paradigm for planning is not well motivated. Cost, performance (from specialization), controllable procedure, better-integration with downstream tasks (e.g. decoding/execution) and so on? It is more about better understanding of the key capabilities of existing techniques and combining them to solve the critical problems. For example, if it is more about specializing common knowledge embedded in LLMs to do planning, then smaller LLM might not be the right solution --- the same proposed paradigm can be combined with LLM of the same size or even larger LLMs for superior planning capabilities. What are the real problems and the corresponding means could be better sorted out? \n\t\u2022 The truth contribution and their relevancy might be hidden in the paper title and the current way of writing.  The proposal is composed of three parts (1) planning data generation from LLMs with human-in-loop curation, (2) teacher-student distillation training, (3)  language model decoding with step verifier.  There are less texts regarding teacher-student distillation. This might indicate that the teacher-distillation importance is over-estimated. With the planning data generation and verifier-guided decoding generation, there might other ways to enhance planning abilities, e.g. finetuning the original LLMs to specializing into planning domains. If the distillation step is an importance component in the ingredients, please detail it and discuss more."
            },
            "questions": {
                "value": "1. There are good ideas within the paper. The writing could be improved to make these good idea clear and stand-out. For example, how to train the step-verifier from human-written plans along with more formal analysis of impact of the step-verifier. \n\t2. Please define the loss functions formally with teacher-student distillation and verifier-training.\n\t3. For the step verifier, \"we design perturbations \u2026 ordering, semantic completeness, topicality and fluency\", please provide detailed analysis of these data-side steps regarding their intuition and formal properties if possible. How does a single verifier score reflect all these criteria? Any special design to achieve them with a simple RoBERTa based classifier?\n\t4. Regarding the evaluation metrics, please provide more details of the AMT human steps. Are coverage, order, over quality complete to evaluate a plan? Any comparison or correlation on the human evaluation metrics and the bleu numbers and the Emobided Environment's metrics? If not well-correlated, any proposal on automatic evaluating plans? Also how to relate and align human evaluation, bleu-style sequence matching metrics, embodied environment testable metrics and real-world execution measurable metrics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8605/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699339447755,
        "cdate": 1699339447755,
        "tmdate": 1699637076584,
        "mdate": 1699637076584,
        "license": "CC BY 4.0",
        "version": 2
    }
]