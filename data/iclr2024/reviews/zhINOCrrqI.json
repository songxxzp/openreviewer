[
    {
        "id": "cqlkBVInKi",
        "forum": "zhINOCrrqI",
        "replyto": "zhINOCrrqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_MuCE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_MuCE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes AttributionLab, a dataset and handcrafted model as a test bed for feature attribution methods. Since the data, both inputs and labels, and the model weights have all be specially crafted so that there ground truth feature importance is known, AttributionLab allows for testing feature importance methods in a controlled environment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clearly written -- the writing is easy to understand and the paper is well organized.\n2. Well motivated -- assessing feature attribution faithfulness is extremely hard in practice.\n3. Potentially useful -- if practitioners need faithfulness in their explainers, this benchmark may be a useful tool for comparing two explainers."
            },
            "weaknesses": {
                "value": "1. Faithfulness is a complicated question. The implication in this paper is that without AttributionLab one might not have ground truth feature importance scores. This paper argues that it is therefore hard to measure faithfulness, but leaves me wondering if there is value in defining the measure at all. Perhaps this weakness is really an issue in the field at large and not the responsibility of this paper.  \n2. The experiments are all visual. It is my understanding that feature attribution methods are often used in practice on tabular problems as well as language. If AttributionLab would benefit from including ways of evaluating XAI tools in those other very popular domains."
            },
            "questions": {
                "value": "1. The Disagreement Problem (Krishna et al. 2022) is cited but not discussed much. In light of this work, knowing that feature attribution methods tend to disagree, what level of faithfulness int he Attribution Lab do the authors ascribe to \"sufficient\" for use in practice? In other words -- we have existing work that shows that two methods won't find the same feature importance scores, so less than 100% faithful must be tolerable -- do the authors have intuition or recommendations for practitioners on faithfulness ranges that are good/bad?\n2. With respect to IG and Occlusion, an interesting point is made on the importance of the baseline value for these methods. I understand how a 'good' choice is made in AttributionLab and how we can conclude that the baseline matters. Is there a method for picking real baselines on real data? On real tabular data? or natural language data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698780772455,
        "cdate": 1698780772455,
        "tmdate": 1699636569281,
        "mdate": 1699636569281,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g9P48iPPDq",
        "forum": "zhINOCrrqI",
        "replyto": "zhINOCrrqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_gSud"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_gSud"
        ],
        "content": {
            "summary": {
                "value": "The paper sets up a controlled environment to evaluate feature attribution algorithms (i.e., algorithms to determine which input features affect the network's output) for neural networks. To do this both the data is constructed in a way that each input pixel affects the network's output and the network weights are specifically designed (not trained) to behave in a specific manner, such that for a given input it is known a priori which weights and which inputs should affect the output in which way. The evaluation of several popular attribution mechanisms shows differences of algorithms in their faithfulness and reaction to unseen data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Evaluating attibution methods for their faithfulness and accuracy is important, especially for fields such as explainable AI. A strictly controlled environment and network to evaluate these approaches therefore makes sense.\nThe setup of the environment and of the neural net seems to make sense and allows for controlled evaluations of different settings."
            },
            "weaknesses": {
                "value": "Overall it's unclear for me what the concrete message of the paper is, except that different attribution algorithms behave differently. What exactly can we learn from these experiments? Are their specific weaknesses of some of the methods? Should they only be used in specific circumstances? Do they need to be interpreted differently? Are some methods strictly better than others? Can the attribution algorithms be somehow improved based on the findings here?"
            },
            "questions": {
                "value": "Basically, my main question is how the findings of this controlled study translate to the real world where the datasets and models are much more complicated? Do we need to change how the current attribution mechanisms are used or interpreted? Should we stop using some of them or change their hyperparameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805496451,
        "cdate": 1698805496451,
        "tmdate": 1699636569154,
        "mdate": 1699636569154,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uU1veInmOc",
        "forum": "zhINOCrrqI",
        "replyto": "zhINOCrrqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_2xtR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_2xtR"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new simulated environment for attribution faithfulness measurement. Attribution faithfulness refers to checking how well the attribution maps from different attribution methods such as Lime or integrated gradients align with the ground truth attribution map. This faithfulness can guide us to select which attribution method to rely more on. Existing methods of attribution faithfulness design a synthetic dataset in which the expected attribution maps are known and then after the network's training on this dataset, they check how well aligned are the predicted attribution maps with the ground truth attribution map. The authors argue that there is a fundamental flaw with this, because the neural network might not be actually using the ground truth attributions to perform the task. For instance, there might be some spurious features in the dataset which can lead to 100% train accuracy, and thus any attribution method that generates these spurious features in the predicted attribution map will be scored lower in faithfulness as the the predicted attribution (even though correct) will be different than ground truth attribution map. \n\nThus the authors argue that instead of just designing the dataset, any controlled environment should also design the neural network so that we exactly know which features are actually being used by the model. The authors design a dataset where the aim is to predict the dominant colour (in terms of number of pixels) and design the neural network in a way that it first detects the colour and then counts the number of pixels for that colours. The authors then use different attribution methods to predict the attribution maps for different images and then measure their faithfulness."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I really like the idea motivated in the paper about having control over not only the dataset but also over the design of the learning process to properly measure attribution faithfulness. The authors can also refer to other datasets which talk about spurious features in the dataset to motivate their reasoning. \nSuch a work will help model developers in the future to be able to better decide on proper attribution methods."
            },
            "weaknesses": {
                "value": "1. The authors mention/deisgn only one dataset and their learning process. Is it possible to also do this analysis over another synthetic dataset an show that the conclusions drawn from faithfulness are similar across the two datasets/learning processes?"
            },
            "questions": {
                "value": "I have already mentioned it in the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824603185,
        "cdate": 1698824603185,
        "tmdate": 1699636569055,
        "mdate": 1699636569055,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5ql60ZPhV9",
        "forum": "zhINOCrrqI",
        "replyto": "zhINOCrrqI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_7XTH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5542/Reviewer_7XTH"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a synthetic based framework that can serve as a sanity check for attribution methods. The goal of the proposed framework is to test the faithfulness of the attribution methods that represents the similarity between attributed features and the true features used by model for prediction. Since the trained model could rely on unwanted or spurious feature for its prediction in order to effectively evaluate the attribution method, there is a need to have models that relies on known and reliable features. The proposed synthetic framework achieves this by defining custom tasks (for e.g dominant shape\u2019s color prediction) - as the ground truth attribution is known as well as by manually setting the weights of models (CNN based classifier) that are the true solution for the given custom tasks. The paper hypothesizes that if the attribution methods output doesn\u2019t align with the known GT attribution of the task then the attribution would be unreliable. The paper empirically evaluates different attribution methods and also proposes some improvement for those attribution methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper explores a synthetic attribution evaluation framework that has a library of predefined tasks (for eg. identify the dominant color in terms of number of pixels in grayscale and RGB input setting) along with the model\u2019s weight that perfectly solves those custom tasks. The proposed framework shows evaluation of various existing attribution methods and proposes insights on how to improve some of these attribution methods. The paper provides detailed ablation experiments to analyze/evaluate the various aspects of attribution methods (positive/negative attribution features). Ablation experiments regarding the unseen data effects sheds light on the benefit of utilizing this synthetic framework to analyze the attribution approaches when out of distribution input are used for inference. The paper is well-written, highly organized and is easy to follow. The paper provides code for reproducibility and has readme instructions to use the codebase."
            },
            "weaknesses": {
                "value": "It would be helpful for a reader to get a better understanding of the following:\n\nIt would be interesting to see the framework evaluation on a larger scale that supports different architectures, tasks and problem domains.\u2028In order to guarantee or even to be in a good standing empirically, it could require more controllable environments (tasks along with the perfect model weights) to verify the faithfulness of an attribution method.\u2028Specifically, it would be interesting to see the following:\n1. Framework that has support for transformer based architecture as the shift of computer vision models (and other domain problems) from CNN to transformer is rampant. \n2. How would the evaluation of transformer based model look like, that already has a build-in support for attribution feature via visualizing the attention maps rather than relying on post-hoc attribution methods (this implicit attention feature attribution indeed should be accurate as this represents the inner computation graph itself). \n3. Extending the library of tasks from simple classification setting (with underlying counting or simple arithmetic jobs) to more abstract setting that necessarily doesn\u2019t satisfy the \u201cProposition 2 (Symmetry property)\u201d (i.e. The addition/removal of any ground-truth pixel to/from the background equally affects the output of the designed neural network.) \n4. To ensure that the verification of attribution method on these custom designed task with small network architecture (2 layer CNN with ReLUs) would generalize to attribution to a large model (that is generally used, with millions or billions of parameters) for the same attribution methods. This might require more complex benchmark tasks."
            },
            "questions": {
                "value": "It would be helpful if the paper can answer/comment the following questions/suggestions:\n\n1. [question] In the paper for equation 3, is there a typo and should this be this instead ? \u201cR(r, g, b) = 1 if Ci(r, g, b) = 1\u201d -> \u201cR(r, g, b) = 0 if Ci(r, g, b) = 1\u201d\n2. [suggestion] While stating \u201cAn attribution may seem reasonable to us, but the neural network may use other input features. Conversely, an attribution may seem unreasonable but be faithful and indeed reflect the features relevant to the neural networks.\u201d, it might be helpful to cite \u201cbug as features\u201d paper [1] that supports this claim.\n3. [suggestion] To further stress on the need of having robust model that doesn\u2019t rely on spurious patterns to make predictions and properly evaluating the attribution methods (i.e. a setting where we know the GT attribution and have access to perfect model that fully solves this problems), it might be helpful to point this in related section by citing methods [2,3] that prove the fragility of trained models leads to attribution features that could be manipulated.\n\nreferences:\u2028\n[1] Adversarial Examples Are Not Bugs, They Are Features, NeurIPS 2019\u2028\n[2] Robust Attribution Regularization, NeurIPS 2019\u2028\n[3] Attributional Robustness Training using Input-Gradient Spatial Alignment, ECCV 2020"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5542/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5542/Reviewer_7XTH",
                    "ICLR.cc/2024/Conference/Submission5542/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5542/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699206349836,
        "cdate": 1699206349836,
        "tmdate": 1700636834086,
        "mdate": 1700636834086,
        "license": "CC BY 4.0",
        "version": 2
    }
]