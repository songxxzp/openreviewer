[
    {
        "id": "e0CQHFGgfH",
        "forum": "RSQL6xvUYW",
        "replyto": "RSQL6xvUYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_A78C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_A78C"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the math- / code-specialized process supervised reward model (PRM) for large language models' reasoning. By finetuning LLaMA-7B (SFT/Code variants) on PRM800K dataset for math and the generated code dataset based on MBPP, PRM are trained for specific reasoning problems. The dataset for code is generated via the Mutation Testing process. The method mainly choose the positive-label reasoning node, and if the reward labels of the child nodes predicted by PRM are all the negative ones, the process is backtracked. Such PRMs improve the accuracy of mathematical reasoning of LLaMA2-7/13B and WizardMath-7/13B and HumanEval pass@1 of Code-LLaMA-Python-7/13B compared to Chain-of-Thought prompting."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "### significance\n- PRM can be trained with tractable-size LLMs (7 billion parameters).\n\n### clarity\n- The data generation process for coding experiments is clearly described."
            },
            "weaknesses": {
                "value": "- Step-wise verification is well-studied in the previous literature (for instance, [1, 2, 3]). The experimental comparison has not been conducted, and I'm not sure what the novel contribution of this paper is.\n- The improvement of the performance seems marginal in all the settings (math/code/models). Considering the inference latency, the proposed HGS-PRM might not be a competitive choice.\n- The difference between Figure 2 and Figure 3 is unclear. They seem to describe the same procedure.\n\n[1] https://arxiv.org/abs/2305.10601\n\n[2] https://arxiv.org/abs/2305.14992\n\n[3] https://arxiv.org/abs/2305.20050\n\n\n(Style Issues)\n- Only the caption of Figure 4 is bold. I'm not sure it's the intention.\n- (In the caption of Table 1) GS8K --> GSM8K, missing colon at the end of sentence.\n- It would be good to be consistent in spacing around parentheses (citation, numbers, etc).\n- (In Section 4.3) \"indicasted in 5\" -->  \"indicasted in Table/Figure 5\"?"
            },
            "questions": {
                "value": "- Does this PRM work with more capable models such as LLaMA2-70B, GPT-3.5-turbo, GPT-4, etc?\n- Is there any reason why you use different temperatures (0.1 for math, 0.2 for coding)?\n- Is there any reason why you use LLaMA variants for the base LLM of PRM, rather than LLaMA2-7B/WizardMath-7B for math problems?\n- In Section 3.4, you seemed to employ Star-Corder, rather than Code-LLaMA-Python. Is there any reason?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580478314,
        "cdate": 1698580478314,
        "tmdate": 1699637152218,
        "mdate": 1699637152218,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eNpj7vQADQ",
        "forum": "RSQL6xvUYW",
        "replyto": "RSQL6xvUYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_abh9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_abh9"
        ],
        "content": {
            "summary": {
                "value": "The paper treated the reasoning tasks as a step by step generation task. They interpreted each step as a node of a tree and translated the problem as a tree search problem. They then proposed a greedy search algorithm w/ the similar philosophy as A*; using a trained process reward model (PRM) to provide the signal of values. They also proposed a novel method to generate synthetic training dataset for PRM for coding tasks. The proposed PRM-augmented searching method outperform chain-of-thought baselines on math and coding tasks using some LLaMA-based small models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. PRM is a good method and I am very happy to see more exploration of its usage. This paper provide more evidence of the effectiveness of PRM.\n2. The way of creating synthetic PRM training dataset for coding tasks is very cleaver! It's quite simple very looks effective. The method is very inspiring.\n3. Some detailed discussion in the paper is also helpful, e.g., when policy model is way stronger than reward model or vice versa would lead to suboptimal results."
            },
            "weaknesses": {
                "value": "1. The novelty of the idea can be a possible weakness. The process supervision is not a new thing as both DeepMind and OpenAI have solid studies on math tasks --- the authors also mentioned this. The search algorithm is very similar to the philosophy of A*; searching reasoning paths as tree is also not a new thing (Tree of Thought; or even AlphaGo). So IMO, the novelty of the paper is kinda near the threshold, and I personally tend to below the line. While I do accept different opinions on this as most of the LLM papers nowadays looks quite incremental; and this one is better than those --- the question is what the bar is for ICLR. I'd like to refer to opinions from other reviewers as well.\n2. There is no space between parentheses and the proceeding letter in many places in the paper.\n3. A lot of details are missing or unclear. Please refer to the questions below."
            },
            "questions": {
                "value": "1. In Section 2.1 you mentioned that you trained the base model like Alpaca. If so, when generating each node (step), you still need to generate the whole path to the end of the solution, is that correct? If so, that will introduce many extra cost if some early steps are \"negative\" as the model will continue generation till the end anyway. Please correct me if I understand incorrectly. I didn't see any discussions about this in the paper and this is my largest concern about the efficiency of the search algorithm.\n2. How did you determine what a step is for math tasks? By \"\\n\"? IIUC, you didn't conduct the style alignment as Lightman in the PRM paper. Without this step, it is not guaranteed that each step would be separated by \"\\n\".\n3. Did you compare your method w/ the sampling and ranking method in the PRM paper? Section 3.4 seems to mention something related but described very unclear. Beating CoT baseline is as expected since you introduced the extra reward model; whether your method can beat other PRM-augmented ranking/search is more important.\n4. Did you train your PRM as a classification model? If so, why not train it to produce a continuous value like the PRM paper?\n5. In Appendix B.3, the first line of correct and incorrect solution are the same. Why one is positive and the other is neutral?\n6. As your method doesn't require to tune the policy model, it is possible to use OpenAI's models as policy models. Did you try it and find it not working since the policy model is much stronger than the reward model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698712964438,
        "cdate": 1698712964438,
        "tmdate": 1699637152083,
        "mdate": 1699637152083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "H7YadQWtqq",
        "forum": "RSQL6xvUYW",
        "replyto": "RSQL6xvUYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_X471"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_X471"
        ],
        "content": {
            "summary": {
                "value": "The submission presents a technique of using PRM to guide decoding for math and coding tasks. The idea is very interesting, and the writing is relatively easy to follow. The experiment results, however, are not super convincing and there are many open questions left. I encourage the authors to conduct more experiments and continue this line of very interesting work."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea of using the PRM to guide the reasoning path generation makes a lot of sense. The greedy algorithm also is suitable here for simplicity and for potential efficiency over some current complicated prompting frameworks.\n- Generating the code dataset with ground truth code and unit test is also a clever way of synthesizing PRM data, which is very costly to collect"
            },
            "weaknesses": {
                "value": "- I find some of the claims over-generalized and unjustified. For example: \u201cIf the language model\u2019s intrinsic capability is too weak, even with the aid of a reward model, it remains challenging to sample the correct reasoning path. On the other hand, if the linguistic capacity of the model significantly surpasses that of the reward model, the benefits might not be pronounced. Therefore, aligning the capabilities of the reward model and the language model is of paramount importance.\u201d \nWhat does the intrinsic capability refers to here? If it\u2019s parameter size, then WizardMath-7B seems to have more improvement on GSM8K tasks than WizardMath-13B. If it\u2019s math specific abilities, then it is not consistent with the claims above.\n- \u201cWe hypothesize that this might be because both HumanEval and MBPP involve relatively simple programming challenges, whereas MATH presents more complex mathematical problems which are intrinsically more challenging for both PRM and the language models themselves to learn.\u201d: Are there any justifications for such a hypothesis?\n- Results: +0.2% of 500 examples is 1 example. In the MATH results. And 0.5% of 1K test examples is 5 examples. Are these within the noise range of the metric?\n- RLHF results missing. If we are using Reward Models, another important baseline is the model after RLHF.\n\n*Writing Feedback*\n\n- I find figure 5 a bit confusing because it seems to have two models for MATH, and one for the code task. \u201c As previously mentioned, our model training method first involved directive fine-tuning using the MATH training set, followed by reward model training. However, it should be noted that we also directly trained our reward model on LLaMA-7B. Our experimental results indicate that models fine-tuned with mathematical directives perform superiorly in all aspects compared to the base model.\u201d \u2013 I am then confused as to which model is used for the reward model in the end. If the SFT model performs better, why is the reward model directly trained on LLaMA-7B?\n- For rigor, should also report the base mode\u2019s performance on code task."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825775025,
        "cdate": 1698825775025,
        "tmdate": 1699637151970,
        "mdate": 1699637151970,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yKfhHUcnRT",
        "forum": "RSQL6xvUYW",
        "replyto": "RSQL6xvUYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_M8mk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_M8mk"
        ],
        "content": {
            "summary": {
                "value": "Process-supervised reward models (PRMs) provide supervision of whether each step of reasoning is valid. Existing work uses such reward models for fine-tuning a LLM, e.g., via RLHF. Instead, this work proposes to directly leverage PRMs during decoding via a heuristic backtracking algorithm. At decoding time, output is sampled from the language model and evaluated under the PRM. If the PRM feedback is negative, the output is re-sampled (i.e., backtracking), whereas if the feedback is positive, the language model continues to output from there. The results indicate that that this yields improvements over Chain-of-Thought prompting on GSM8K"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work proposes a simple and reasonable approach for incorporating PRMs directly into decoding without the need for fine-tuning on them. The reported results are encouraging, and it seems like this work would be interesting to the community and warrant further investigation."
            },
            "weaknesses": {
                "value": "The main weakness of this work is its presentation, which I do not think is ready for publication. The writing is vague almost everywhere (e.g., lacking a formal description of the proposed approach), which makes it difficult to understand and reproduce the proposed approach.  I think the general ideas behind the paper seem solid and interesting enough, but the presentation needs to be significantly improved for this to be fully appreciated by the community."
            },
            "questions": {
                "value": "Can the authors provide a precise formal overview of the proposed decoding algorithm and the training procedure for the PRM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698909909574,
        "cdate": 1698909909574,
        "tmdate": 1699637151868,
        "mdate": 1699637151868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x3KTHuJ31q",
        "forum": "RSQL6xvUYW",
        "replyto": "RSQL6xvUYW",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_Q37j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9154/Reviewer_Q37j"
        ],
        "content": {
            "summary": {
                "value": "As the performance of LLM continues to improve, their ability to do multi-step reasoning is become more important. Currently, most LLM ability to do multi-step reasoning suffers from cascading errors. To address these issues, the authors propose a greedy heuristic search algorithm that performs step-level feedback using PRM to improve LLM multi-step reasoning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Improving multi-step reasoning in LLM is a very important topic. The strengths of this paper are\n\n1. Solution Simplicity: The authors proposed a very simple method with empirical performance superior to the paper's baseline methods.\n2. Combination of PRM, Code, and Mutation testing: To perform experiments with PRM, it usually requires a lot of human annotation. However, the observation that PRM can be trained with mutation testing, which provides automatic code atomic code changes and the fail and pass, was creative."
            },
            "weaknesses": {
                "value": "Though this paper addresses an import problem and has strengths, but there are also some weaknesses outlined below:\n\n1. Lack of baseline: The authors do not compare to common decoding strategies used: majority voting (self-consistency) [1] and RM-weighted decoding (verifier voting) [2].\n2. Writing Quality: There are several typos throughout the paper and the paper lacks clarity. Some typos are \"We also find The ability to distinguish ...\", \"directive fine-tuning ...\", and \"mathematical directives perform...\". \n3. The idea to sample greedy from the model and score it with the reward function makes strong assumptions on the reward model and starting model abilities.\n\n[1] Self-consistency improves chain of thought reasoning in language models by Wand et al. 2022\n[2] Solving math word problems with process- and outcome-based feedback by Uesato et al. 2022"
            },
            "questions": {
                "value": "1. How does the proposed approach compare to majority voting and RM-weighted decoding? Given that PRM has not been used in the code domain - showing the performance of these baselines is important.\n2. How does the proposed approach compare to outcome-supervised reward models (ORMs)?\n3. Why is self-assessment more expensive than PRM, given that both the PRM and generator use the same LLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9154/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699248937524,
        "cdate": 1699248937524,
        "tmdate": 1699637151765,
        "mdate": 1699637151765,
        "license": "CC BY 4.0",
        "version": 2
    }
]