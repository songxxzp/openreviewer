[
    {
        "id": "f6oFjXKSCT",
        "forum": "KJzwUyryyl",
        "replyto": "KJzwUyryyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_vWZJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_vWZJ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces ALMANACS, a new benchmark for evaluating explainability methods for language models. The key ideas are:\n\n-ALMANACS measures the simulatability of explanations, i.e. how well they help predict model behavior on new inputs. This relates to the desired properties of faithfulness and completeness.\n-The benchmark comprises 12 topics with safety-relevant scenarios. Questions are non-objective and designed to elicit complex, nonlinear behavior from models.\n-There is distributional shift between train and test sets to require generalization.\n-The authors test counterfactual, rationalization, and salience-based explanation methods. None consistently improve upon a no-explanation baseline, indicating simulatability on ALMANACS remains an open challenge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1) Partially addresses the need for standardized benchmarks to evaluate and compare explanation methods.\n\n(2) Simulatability is a useful metric directly related to explanation quality. \n\n(3) Automated evaluation enables efficient benchmarking.\n\n(4) Non-objective questions and distribution shift require explanations to provide true insight rather than leveraging correlations."
            },
            "weaknesses": {
                "value": "(1) Only safety-relevant scenarios are included and this is not general. \n\n(2) The choice of language models for evaluation versus being explained may affect results. More analysis of this factor could be useful.\n\n(3) Automated evaluation using a language model proxy for humans has limitations vs. human studies. Direct comparisons would be needed to validate the benchmark.\n\n(4) Testing on more model sizes, scaling effects, and model families instead of just focusing on flan-alpaca-gpt4-xl and vicuna-7b-v1.3."
            },
            "questions": {
                "value": "Missing references for explanation distillation:\n\n(1) Li et al. Explanations from Large Language Models Make Small Reasoners Better. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698539426575,
        "cdate": 1698539426575,
        "tmdate": 1699636382527,
        "mdate": 1699636382527,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8HAGBpFSCb",
        "forum": "KJzwUyryyl",
        "replyto": "KJzwUyryyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_3Mpe"
        ],
        "content": {
            "summary": {
                "value": "This submission introduces ALMANACS, a novel benchmark tailored for evaluating the explainability of language models via a concept termed \"simulatability.\" Using GPT-4 as a predictor, the benchmark assesses how well GPT-4 can simulate other language models that employ various explanation methods. Simulatability is defined by measuring the distribution distance between the outputs of GPT-4 and the target language model for previously unseen test tasks. A noteworthy finding is that the incorporation of explanations does not invariably enhance explanation performance for unseen inputs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper innovatively offers a benchmark with a quantitative metric for assessing explainability in language models. Furthermore,it is interesting for the discovery that models with explanation input do not outperform non-explanatio in terms of simulatability."
            },
            "weaknesses": {
                "value": "The primary focus of this paper appears to be on the introduction of the ALMANACS benchmark. Yet, the utilization of well-established distance measures like KLDiv and TVDist doesn't add a novel dimension to the study.\n\nThe observation that explanation techniques might not always heighten performance on unseen data is compelling, but the paper would benefit from a deeper analysis and discussion on the possible reasons behind this phenomenon.\n\nThe term \"simulatability\" appears to be inconsistently defined, leading to confusion. The initial definition of simulatability is \"how well the explanations improve behavior prediction on new inputs\". Subsequently, it seems to change to a definition centered on distribution distance, KLDiv or TVDist.\n\nThe paper doesn't provide a convincing argument for why simulatability is a good metric for language model explainability. Many factors can influence predictor outputs. Given that GPT-4 operates as a black box, it's hard to say GPT-4 predict solely to the presence or absence of explanations without providing additional constraints. This point is underscored by results from the NoExpl vs Expl comparison in Table 1, which indicates low KLDiv scores, hinting that GPT-4's predictions might be independent of input explanations."
            },
            "questions": {
                "value": "1. Could the authors provide the precise definition of \"simulatability\"?\n1. The choice of GloVe embeddings for demonstration retrieval appears outdated. Have the authors considered more recent sentence embeddings, such as SimCSE?\n1. There seems to be a discrepancy between the comparison results for NoExpl in Figure 4 and Table 1 (PredictAverage vs NoExpl). PredictAverage outperforms NoExpl in Figure 4 but does not in Table 1. \nCould this be clarified?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838022951,
        "cdate": 1698838022951,
        "tmdate": 1699636382452,
        "mdate": 1699636382452,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "KqtwKuIVNj",
        "forum": "KJzwUyryyl",
        "replyto": "KJzwUyryyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_P4RQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_P4RQ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents ALMANACS, a language model explainability benchmark that measures the efficacy of different explanation methods. The benchmark focuses on simulatability, which evaluates how well explanations improve behavior prediction on new inputs. ALMANACS consists of twelve safety-relevant topics with idiosyncratic premises and a train-test distributional shift. The authors evaluate counterfactual, rationalization, and salience-based explanations using another language model as a predictor. The results show that, on average, no explanation method outperforms the explanation-free control, highlighting the challenge of developing explanations that aid simulatability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper addresses the need for a consistent evaluation standard for language model explainability methods.\n- ALMANACS provides a benchmark that measures simulatability, a necessary condition for faithful and complete explanations.\n- The benchmark includes safety-relevant topics and a train-test distributional shift to encourage faithful explanations.\n- The use of another language model as a predictor enables fully automated evaluation, speeding up the interpretability algorithm development cycle.\n- The paper presents results that highlight the limitations of current explanation methods and the open challenge of generating explanations that aid prediction."
            },
            "weaknesses": {
                "value": "- The paper only evaluates the explanation methods based on Kullback-Leibler divergence (KLDIV) and total variation distance (TVDIST). While these metrics provide insights into the performance of the methods, they may not capture all aspects of explanation quality.\n- The paper acknowledges that the automated evaluation using language models may not be consistent with human evaluation. Human studies are still needed to validate the results and determine if humans can succeed where language models fail.\n- The paper evaluates only three explanation methods (counterfactual, rationalization, and salience-based). While these methods are commonly used in explainability research, there may be other methods that could be valuable to include in the benchmark."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699014003960,
        "cdate": 1699014003960,
        "tmdate": 1699636382350,
        "mdate": 1699636382350,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n5HqHA0Rtn",
        "forum": "KJzwUyryyl",
        "replyto": "KJzwUyryyl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_Lu72"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4167/Reviewer_Lu72"
        ],
        "content": {
            "summary": {
                "value": "Simulatability refers to (a human\u2019s) capability to predict model behavior on unseen outputs. Improving simulatability has been considered an important goal for interpretability methods. This paper introduces a new benchmark to automatically evaluate simulatability for interpretability methods, using GPT-4 as a stand-in for humans. Notably, this new benchmark focuses on non-objective tasks with safety-relevant questions."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-written and easy to follow. By focusing on safety-relevant, non-objective questions, the benchmark differentiates itself well from existing work on interpretability evaluations. The focus on distribution shift also makes the evaluation more realistic than some of the existing work. Overall the paper presents a well-executed idea with very clear motivation."
            },
            "weaknesses": {
                "value": "As the authors acknowledged, the use of GPT-4 as a stand-in for human annotators limits how much we can take away from the evaluation. Although the paper frames ALMANACS as a benchmark, I find it more suitable to call it a dataset\u2014only when paired with a good-enough human approximator like GPT-4 would it become a benchmark. The lack of user study makes it difficult to judge the evaluation results conducted with the new dataset. But I think the dataset is an interesting starting point for future user studies."
            },
            "questions": {
                "value": "Given the predictor is GPT-4, it seems like the benchmark can be applied to interpretability goals beyond simulatability. Any thoughts on that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699504398516,
        "cdate": 1699504398516,
        "tmdate": 1699636382284,
        "mdate": 1699636382284,
        "license": "CC BY 4.0",
        "version": 2
    }
]