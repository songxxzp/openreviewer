[
    {
        "id": "6hblUiXMm5",
        "forum": "q4SiDyYQbo",
        "replyto": "q4SiDyYQbo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_F3P4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_F3P4"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on studying the issue of representation harm in contrastive learning (CL) that arises when some groups are underrepresented in the training corpora. In this case, the authors show that the underrepresented groups tend to collapse into semantically similar groups (that are not underrepresented). In a follow up theoretical analysis on graphs, the authors show that two groups of nodes tend to collapse as their connectivity increases. Finally, through a causal analysis, the authors show that the representation harms caused by CL cannot be mitigated for downstream tasks when training a probe on top of the CL representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper studies an important problem in self-supervised learning through contrastive objectives (that of representation collapse of groups that are underrepresented in the training data)\n2. It does so in a sound and systematic way, by providing evidence on controlled images (CIFAR10), imbalanced text (BiasBios) and a theoretical analysis with artificial graphs.\n3. The paper is well written, with a thorough discussion of results and potential insufficiencies of an existing class of algorithms in overcoming representation harm, showing how more work is needed in the area of CL to result in algorithms that are robust to underrepresentation of certain groups (which can be hard to identify at scale to begin with)."
            },
            "weaknesses": {
                "value": "The main weakness I can find from this paper is the lack of a large-scale study. SSL, and CL in particular, are most effective when used on large amounts of data. The controlled studies in this paper allow us to analyze the existing representation harms. However, it can be seen that these values are generally much lower for BiasBios than for CIFAR10. On the other hand, this could be due to the different domain (text). A study on ImageNet (larger number of both samples and classes) could potentially disentangle this confound. In particular, it would be interesting to know whether having more diversity in the data alleviates learning spurious features (e.g. collapsing classes with similar background colors), and reduces collapse of underrepresented groups."
            },
            "questions": {
                "value": "1. Typo \u201ca\u201d at the end of line 6 in page 2\n2. You could use diverging palettes (with a neutral color at 1.0) in the heatmaps, to clearly distinguish HR<1. Having a colorbar next to each heatmap figure would also improve readability.\n3. In Figure 2, do you have any idea why there\u2019s such a large difference (10%) between deer and horse collapsing?\n4. In line 4 of Sec 2.1.2, you can add \u201cof classes\u201d after \u201ca pair\u201d to help the reader understand better \n5. In Sec 2.2.2, when you define the GRH metric, I would have found it useful to have a sketch of a plane with boundaries that show what each region means. It\u2019s something you could consider adding when you get an extra page\n6. In Sec 4.2, why do you train on 75% of the Test set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698474001745,
        "cdate": 1698474001745,
        "tmdate": 1699636329644,
        "mdate": 1699636329644,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pJoU3OqVVP",
        "forum": "q4SiDyYQbo",
        "replyto": "q4SiDyYQbo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_n65V"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_n65V"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the effect of under-representation on the performance of minority groups in the context of self-supervised learning (SSL), specifically contrastive learning (CL). They show that CL tends to collapse representations of minority groups with certain majority groups, leading to representation harms and downstream allocation harms even when labeled data is balanced. Theory and experiments are presented to support their results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This is a well written paper that discusses an important topic that is well motivated. This rigorous analysis is likely of interest to practitioners and useful to mitigate the potential harm from CL methods.\n2. The empirical study is good and the theoretical study adds a solid foundation to the empirical results. Both empirical and theoretical findings are promising.\n3. Section 4 is also quite interesting, showing how representation harms can cause allocation harms."
            },
            "weaknesses": {
                "value": "1. There needs to be an intuitive definition of allocative harms and representation harms when they are first mentioned in the intro, which matches the precise definition in section 2.\n2. Figure text is generally too small to see clearly.\n3. Why does this metric for representation harm make sense? It seems like an important decision critical for the rest of the paper, so needs better justification.\n4. The examples used in section 2 are not very useful - sure automobiles and trucks could collapse but is it the worst thing - are there more compelling real-world examples to illustrate these problems?\n5. Why are the metrics for representation harm in 2.1.2 and 2.2.2 different? This seems weird. Even if the exact distance (eg cosine vs l2 for image or word embeddings) are different - why are the metrics different?\n6. Why do sections 5 and 6 require a different data setup? Specifically, why is causal mediation analysis the right framework to study section 6 - this was not motivated.\n7. The empirical analysis is conducted only on CIFAR10 and BIASBIOS datasets - I would have liked to see more to further strengthen the results in the paper, such as including celebA which is quite standard for studying biases in vision models, or perhaps even with results on a CLIP model for results on image-text models."
            },
            "questions": {
                "value": "see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Reviewer_n65V"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3736/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698812390900,
        "cdate": 1698812390900,
        "tmdate": 1700715476893,
        "mdate": 1700715476893,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oo8f5Jz3zD",
        "forum": "q4SiDyYQbo",
        "replyto": "q4SiDyYQbo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_X2wa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3736/Reviewer_X2wa"
        ],
        "content": {
            "summary": {
                "value": "The study presented in this paper concentrates on examining how contrastive learning (CL) can cause representation harm, particularly when certain groups are not adequately represented in the training data. The researchers demonstrate that these underrepresented groups often merge into other semantically similar groups that are better represented. Further, through a theoretical analysis involving graphs, it is shown that increased connectivity between two groups of nodes leads to their convergence. Lastly, a causal analysis reveals that the detrimental effects on representation caused by CL are irreparable for subsequent tasks, even when a probe is trained using the CL representations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe composition of the paper is clear and comprehensive, discussing results in depth and highlighting representational harm. The paper offers practical insights to diminish the adverse effects of CL techniques.\n2.\tSection 4 presents an intriguing analysis, exploring how representational biases can lead to allocation disparities.\n3.\tThe paper's empirical research is commendable, and its theoretical framework provides a robust underpinning for the empirical observations. The outcomes from both empirical and theoretical perspectives appear promising."
            },
            "weaknesses": {
                "value": "1.\tThe text in the figures is too small for easy readability and needs enlargement for better clarity.\n2.\tA more thorough justification is needed for the chosen metric of representation harm, given its critical importance to the paper's analysis.\n3.\tThe necessity for different data setups in Sections 5 and 6, particularly the choice of causal mediation analysis for Section 6, lacks proper motivation and explanation.\n4.\tThe paper's primary limitation is the absence of a comprehensive large-scale study, especially in the realm of self-supervised learning (SSL)."
            },
            "questions": {
                "value": "See weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3736/Reviewer_X2wa"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3736/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1700501298047,
        "cdate": 1700501298047,
        "tmdate": 1700501298047,
        "mdate": 1700501298047,
        "license": "CC BY 4.0",
        "version": 2
    }
]