[
    {
        "id": "ka5yXGSqpl",
        "forum": "86NGO8qeWs",
        "replyto": "86NGO8qeWs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
        ],
        "content": {
            "summary": {
                "value": "The authors propose and curate a new benchmark for evaluating the compositional reasoning capability in audio-language models. And further show that to train with curated data and modular contrastive with hard positives and negatives help to improve the compositional reasoning capability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The curation of CompA-order and CompA-attribute benchmarks are relevant and needed contributions to the community. This helps to provide another perspective to examine the properties and limitations of current SOTA methods for audio-language models.\n- To provide both quantitative analysis of the current limitations in 2.1 and qualitative analysis in the last paragraph of 3.1 both provide solid motivations of this work."
            },
            "weaknesses": {
                "value": "- It is a bit difficult to get an entire picture of number of different datasets curated and how each of them are curated, a lot of the information are buried in the appendix. Consider in the main paper including a table describing different datasets curated, how many of them for each subset used for training, and evaluation, their source, and methods (leverage ChatGPT, replace the sounding object, swap the orders, etc.), and then refer to the table in the narratives.\n- The data curation is one of the main contribution for this work, however currently a lot of details are left in the appendix. Consider integrating more information such as the source datasets, and some manipulated examples to the main narratives, this can help the reader to understand this work early on.\n- A minor suggestion: The figures and tables are far from the actual text in the appendix, consider reorganize them to make it easier to read.\n- A minor comment: A lot of the texts in the tables and the figures are very small and difficult to read."
            },
            "questions": {
                "value": "- Regarding the data curated for training, there are CompA-661k, and AudioSet-compA, another 110k audio-text pairs for complex compositional audios? How are these different subsets used in different training scenario? It might worth adding a table to show clearly what combination of datasets, training strategies, whether there is hard negative and modular contrastive utilized, and link them directly to your abbreviation in the result table (CLAP, CompA-CLAP, CLAP-CompA-661k, etc.) Currently it is not easy to associate from the narratives to exactly how each variation in the result tables comes from.\n- In Table 1, what is the difference between CLAP (ours) and CLAP-CompA-661k (ours)?\n- For the common mistakes mentioned in the discussion, is it possible to perform some error analysis? Or confusion matrices to show these more quantitatively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Cw3v"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698504488533,
        "cdate": 1698504488533,
        "tmdate": 1700319189215,
        "mdate": 1700319189215,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Caq3E3G9fQ",
        "forum": "86NGO8qeWs",
        "replyto": "86NGO8qeWs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
        ],
        "content": {
            "summary": {
                "value": "Summary: This paper presents a new benchmark, CompA, for measuring two forms of compositionality in audio-language models. These are ordering (sequencing of acoustic events within a clip) and attribute binding (determining which objects perform acoustic sources make which sounds in a clip). They also propose a novel pipeline to train a new contrastive model, CompA-CLAP, by augmenting existing datasets with (a) hard negatives in terms of both forms of compositionality (ordering and attribute binding) and (b) data augmentation by concatenating and overlaying audio clips with known labels, where all text is generated via LLM. Using their proposed benchmark, they show improvements from the novel data pipeline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I am posting my full review here, as I believe it easier to discuss both strengths and weaknesses together.\n\n# Overall\n\nOverall, the paper is an ambitious effort to propose a new benchmark measuring a known limitation of existing audio-language models, and also to achieve improved performance on this benchmark. However, reading the paper, it also feels that important details are sometimes lacking both on the benchmarking and the modeling/empirical sides of the paper, in part due to this decision to cover so much ground. It does not appear that the missing information is available in the supplementary material either. The proposed method also performs below random baseline on 2 of the 6 proposed benchmark metrics, but this does not appear to be discussed in the main text. I recommend adding some important clarifying details about the benchmark, clarifying experimental decisions and details, and reducing the \"sales pitch\" framing of the paper. I am enthusiastic about the direction and would encourage the authors to continue to refine the benchmark and their work. However, I feel that major revisions to the paper, and perhaps the benchmark, are currently in order, and that it is probably not ready for acceptance in its current form.\n\n# Major Comments\n\n* The tone of the paper often leans too strongly toward \"sales pitch\" for my taste. I would recommend to tone down this language, and focus on making quantifiable and verifiable claims based on the data and methods in the paper. Just a sampling of the phrases the authors use to dismiss other work, minimize problems, or promote theirs:\n\n  - \"only a word changed\" (a few sentences after the claim \"the audios have the exact same words\")\n  - \"it is trivial for ALMs to perform well on these benchmarks\"\n  - \"CLAP undergoes only minor degradation in retrieval performance\"\n  - \"25% synthetic...but highly curated by expers\" (what is *highly* curated?)\n  - \"a trivial but ineffective solution\"\n  - \"a highly custom prompt\"\n  - \"Our version outperforms ... by significant margins\" (no measures of significance used)\n\n* Since the authors are not only introducing a novel model, but actually proposing their dataset to serve as a benchmark, more justification and detail are needed. For example:\n  - There is no mention in the paper of where the benchmark can be accessed, how it can be used by other works, what format it is provided in, etc. This is essential information for a public benchmark (it is ok to maintain anonymity by describing where the benchmark *will* be available upon acceptance, if the description is sufficiently clear).\n  - The two components of the benchmark consist of only 400 and 200 samples respectively. Is this sufficiently large for a reliable benchmark for the field? Even existing benchmarks in the audio-text domain consist of thousands of examples (AudioSet, AudioCaps, MusicCaps). If 200-400 samples is considered sufficient, please provide empirical evidence. For reference, a Clopper-Pearson confidence interval for even 400 samples would be quite wide: a width of roughly 0.1 for a 95% CI depending on the approximation used, when the success rate is 50%.\n  - It is critical that the paper provide more detail on the annotation process in the main text. The current version provide almost no information, except that four annotators participated. Please describe, for example: number of annotators *per example*; the exact annotation procedure performed. Rater agreement metrics (in supplementary) would be useful but are not required. More detail is also needd regarding the screenshot in Figure 6; it is not clear at all what task the raters are even performing. Much of Section A.3 is in the passive tense, so it is also hard to tell who performed what tasks (\"a manual inspection was done\",\"Wavjourney was employed\"). Many of these details should also appear in the main text.\n  - The abstract states that the benchmark consists of \"a majority of real-world samples\", and elsewhere it is mentioned that 90% of audio are from real-world samples. (1) Can you provide results separated by real-world vs. not real-world? (2) What is the nature of the \"non real-world\" data?\n  - The choice of metrics seems not entirely clear. It would be helpful to either (1) cite works which already use similar metrics, to motivate their used based on wide preexisting adoption in the ALM community, or (2) provide more detailed motivation for why the metrics are appropriate. For example, why is \"group score\" the conjunction of audio score and text score and not, e.g., their harmonic mean in the style of an F-score?\n  - There are no quantifiable measurements of incertainty or variability in the comparisons presented in the paper. The authors say that their results are averaged over 3 random seeds; can they provide the (variance, stddev) over these seeds? What about Clopper-Pearson or other appropriate confidence intervals for the metrics reported? As is, it is impossible to assess the significance of the differences between metrics in Table 2.\n\n* Some important experimental design decisions in the paper are not clearly motivated and not validated with empirical studies. This includes:\n  - Why did the authors only fine-tune CLAP (when they have already shown the CLAP weights to be ineffective) instead of training it from scratch, when the authors apparently have access to the full original pretraining datasets?\n  - Why do the authors fine-tune two separate times, instead of performing a single phase of joint fine-tuning?\n  - Why does the fine-tuning occur (hard negatives (3.3) --> synthetic pairs (3.4)) instead of the reverse ordering?\n\nAll of these decisions could be validated empirically, but I do not see these results in the paper.\n\n* Both examples in Figure 4 both seem to be flawed training example. (1- Left) The negatives both appear to be correct. All of these examples (the true caption, and the negatives) describe three sound events occurring simultaneously. (2 - right) \"Tiger roars amidst thunker\" and \"Tiger roar followed by\" cannot both be positive labels for the same sample - only one can be correct. Both of these issues seem to raise concerns about the quality of the data used here; particularly since the \"highly custom prompt\" described does not appear to be shared in the paper.\n\n* Having the \"CompA-CLAP\" row in Table 2 be bold is misleading. In particular: \"CLAP (ours)\" outperforms ComA-CPA in attribute audio score. Additionally, CompA-CLAP performs below random baseline on audio and group scores in CompA-attribute. This should be highlighted clearly in the paper and abstract (which currently says that \"CompA-CLAP significantly improves over all our baseline models on the CompA benchmark\" but does not mention its below-chance performance on 2 of the 6 benchmark task metrics).\n\n* The paper makes several additional changes to the CLAP model\n\n# Minor Comments\n\n\n* \"Sequence\" and \"attribute binding\" should be defined clearly and early in the paper (ideally before these phrases are used).\n\n* Caption in Figure 2 should describe the metrics being presented in the Figure clearly. The Figure is also missing clear y-axis labels. The phrase \"minor\" is not quantifiable. This figure could also be improved by (a) measures of variability such as error bars or Clopper-Pearson confidence intervals or (b) baselines to show that the CLAP models actually degrade to \"trivial\" performance (is R@1 of 0.42 on AudioCaps val, or R@1 of 0.35 on AudioCaps test, \"trivial\"?).\n\n* A lot could be done to make the paper more self-contained. As-is, it is missing a great deal of relevant background information, which is exacerbated by the deferral of the related work to Section 5.\n\n* \"to assure high quality, we don\u2019t concatenate or overlay random acoustic events but ask an LLM to create unique audio scenes based on the available labels\" - please clarify both the overall procedure here, and how this assures high quality.\n\n* \"where augmenting LAION-audio-630K with 2 million audios from AudioSet improved performance on benchmarks only marginally.\" Please clarify what is meant in this sentence.\n\n* The \"experimental protocol\" sections in 3.2 and 2.4 here do not describe protocols; please clarify or remove these sections. I would recommend a single \"experimental protocol\" section, since these subsections only describe individual components of the overall experimental setup.\n\n# Typos etc.\n\n* Section 1 should reference that prior work is deferred to, and discussed in, Section 5.\n* \"attribute-binding\" vs \"attribute binding\" both used in the paper.\n* \"The group score evaluates of the model performed well for the evaluation instance in the benchmark\" - this is not a meaningful description.\n* 3.4: \"due the the inherenly complex and non-linear nature of audios in the wild\" - please clarify what is meant here, and how nonlinearity relates to the hardness of an example.\n\n# Edit after discussion\n\nI have increased my scores for soundness (2->3) and presentation (2->3) along with my overall rating (3->6) after the author responses, which made significant changes to the paper which both improved it and addressed several of my concerns."
            },
            "weaknesses": {
                "value": "See above."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_Xast"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698646198949,
        "cdate": 1698646198949,
        "tmdate": 1700269439531,
        "mdate": 1700269439531,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "u3JlKBAktn",
        "forum": "86NGO8qeWs",
        "replyto": "86NGO8qeWs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_jJm8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_jJm8"
        ],
        "content": {
            "summary": {
                "value": "This paper simultaneously propose new datasets (CompA) and a novel training method (CompA-CLAP) for the task of compositional reasoning in Audio Language Models (ALMs). The dataset contribution is separated between two subsets, CompA-order, which is targeted at the temporal ordering of audio events, and CompA-attribute, which targets attribute reasoning. Additionnally, the authors show that most ALM fail at grasping compositonal reasoning and propose large improvements on this problem with a new method, CompA-CLAP which both introduce a specific loss and a methodology for modular contrastive learning.\n\nOverall, the paper is very well written and the methodology is very soundly presented by both exhibiting a profound issue of ALM, proposing novel datasets and even a large improvement in terms of learning methodology. My major (and almost only) criticism comes from the fact that the concept of \u00ab compositionality \u00bb presented on this paper is rather largely focused on \u00ab temporal \u00bb compositionality (even in the attribute case), and the paper would gain from a broader presentation of different possible types of compositionality. However, I still strongly believe that this paper would be a good addition to ICLR and recommend for acceptance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The paper is very rich in new proposals, but all of these remain soundly theoretically grounded. Also, the proposal of new highly curated datasets are also an always-welcome addition for the community."
            },
            "weaknesses": {
                "value": "As previously stated, my major (and almost only) criticism comes from the fact that the concept of \u00ab compositionality \u00bb presented on this paper is rather largely focused on \u00ab temporal \u00bb compositionality. Although I understand that it is mandatory to make choices as this is still a very young research topic, I think the paper would gain in strength if a more clear definition of various types of compositionality would be presented."
            },
            "questions": {
                "value": "With regards to generating examples, although I understand that using prompts and GPT can perform a large array of tasks, your temporal task could be quite easily simulated (collating examples together, such as done with the Mixup augmentations). Could you quantify gains and differences with those more straightforward approaches ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698662151778,
        "cdate": 1698662151778,
        "tmdate": 1699636928632,
        "mdate": 1699636928632,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hg271fpt48",
        "forum": "86NGO8qeWs",
        "replyto": "86NGO8qeWs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
        ],
        "content": {
            "summary": {
                "value": "Objects in an image have spatial relationships (which occur where in the image) where as events in an audio clip have temporal and attribute relationships (which sound occur when, who made which sound). The authors claim that the current audio-language models (ALMs) poorly model such temporal relationships. They propose a novel benchmark, \"CompA\" to test compositional reasoning (temporal as well as attribute relationships) abilities of ALMs. CompA is comprised of two parts. CompA-order to evaluate temporal relationship and CompA-attribute to evaluate attribute binding. Finally, authors introduce a model (CompA-CLAP) to improve ALM compositional reasoning abilities. CompA-CLAP is a fine-tuned CLAP model, where in the authors include compositionally aware hard-negatives for contrastive learning.These hard-negatives are derived using text-LLMs for the test set and template-based methods for the training set."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The submission identifies a gap in current audio-language model evaluation i.e., temporal and attribute relationships of audio events, and proposed the novel CompA benchmark to evaluate the same. \n* Introduced a novel CompA-CLAP model which learns audio event relationships through contrastive learning and goal motivated negative sample design.\n* Well furnished details in appendices for reproducibility."
            },
            "weaknesses": {
                "value": "* The core technical contribution of submission boils down to - \"selecting appropriate negatives for the task at hand\", an established idea in contrastive learning, thus raising the question of novelty in terms of technical contribution.\n* The structuring of manuscript could be better. e.g.,\n  - Transition from section 3.3 to 3.4 is not clear in the first reading.\n  - Every section reads like a mini-paper with it's own background, methodology. May be add a paragraph at the beginning of section to prep the reader of upcoming sections and their differences."
            },
            "questions": {
                "value": "* Have the author's considered rich transcription approach? Since authors are using strongly annotated data, can the model be trained to output a sequence like ..  <event1_start> <event2_start><event2_end><event1_end> .. here event2 occurs in the midst of event1. This would avoid the process of carefully designing negatives and the model would still learn temporal relationships among audio events. Attributes can also be modeled in a similar fashion. I would imagine this format could be more flexible in terms of attribute modeling. (e.g., personA-talking vs personA-singing and so on.. ). An LLM should have no problem taking such an output sequence and convert it to human-readable format (either implicitly or explicitly). \n* Can the author's clarify if the order of hard negative training and modular contrastive training matter. (section 4: 2nd paragraph)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7640/Reviewer_x6Uk"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699407739824,
        "cdate": 1699407739824,
        "tmdate": 1700702757207,
        "mdate": 1700702757207,
        "license": "CC BY 4.0",
        "version": 2
    }
]