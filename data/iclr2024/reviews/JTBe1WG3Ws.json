[
    {
        "id": "PO4YHTjXaL",
        "forum": "JTBe1WG3Ws",
        "replyto": "JTBe1WG3Ws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_dszm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_dszm"
        ],
        "content": {
            "summary": {
                "value": "This paper studies automatic red-teaming where new adversarial prompts are generated based on seed in-context examples and a few simple strategies (e.g., FIFO, LIFO) are explored to hot-swap the generated prompts with existing in-context prompts. Experiments are mostly conducted on text-to-image models and the major baseline is an existing stochastic few-shot method where the adversarial prompts are more random than the proposed update strategies."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is nicely written and easy to read\n\n- Safety of generative AI is an important topic\n\n- A lot of analysis and ablations of the proposed method are presented"
            },
            "weaknesses": {
                "value": "While I appreciate the effort of the authors conducting tons of experiments and analysis, my main concerns are around the evaluation of the proposed method.\n\n- The metric of attack effectiveness is a bit misleading, as the authors mentioned themselves \"the red LM learns an effective prompt that is strong in terms of triggering the text-to-image model in unsafe generation; thus, it keeps repeating the same/similar prompts that are effective which affects diverse output generation\". If the prompts remains the same all the time, does it have an attack effectiveness of 100%? It doesn't sound reasonable to penalize methods that discover more adversarial prompts (though some of the prompts are not effective). A more rigorous study would be to have some categories (e.g., sexual, violent, etc.) and see how the methods perform in different scenarios. The current metric doesn't account for that and in the proposed method there also doesn't seem to be much control of what types of prompts to generate (apart from a \"diversity measure\"), making the attack less oriented.\n\n- Another related issue is, as listed in Table 9, right now only 3 seed prompts are used for evaluation, and they are rather similar in nature (either sexual or violent). A more comprehensive study with larger scale would have been more convincing that the method is generally applicable and not sensitive to / relying on the prompt engineering of the initial seed prompts (quoting from paper \"hand engineered by humans\"). Speaking of which, how effective are the initial prompts?"
            },
            "questions": {
                "value": "- Why is FIFO already so much better than SFS? Is it mostly because of the initial prompts or the fact that only successful prompts are added?\n\n- In text-to-text experiment, why do you use two evaluators (toxigen and perspective api) for toxicity? Did you re-calibrate the scoring threshold? Can you show some model generations at different score percentiles? In my experience, for example, there could be high false negatives in toxigen in the default setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698104688186,
        "cdate": 1698104688186,
        "tmdate": 1699636367071,
        "mdate": 1699636367071,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4FYq9ZzUip",
        "forum": "JTBe1WG3Ws",
        "replyto": "JTBe1WG3Ws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_2CM2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_2CM2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the in-context learning-based red teaming method named \"FLIRT\" which iteratively updates demonstrations according to the feedback from the target model. The FLIRT method has 4 variations in its attack strategy as following:\n- First in first out (FIFO) attack : If new prompt elicit an offensive response, remove the first exemplar in the exemplar queue and add new prompt into the exemplar queue.\n- Last in first out (LIFO) attack : If new prompt elicit an offensive response, remove the first exemplar in the exemplar stack and add new prompt into the exemplar stack.\n- Scoring attack : Update exemplars based on scores such as attack effectiveness, diversity, low-toxicity.\n- Scoring attack + LIFO : combining scoring attack and LIFO\n\nThe empirical results show that FLIRT can discover a larger number of positive test cases, that elicit offensive responses, compared to the baseline methods. \nMoreover, the authors build a benchmark dataset consisting of positive test cases."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The idea is simple and intuitive.\n- The paper is well-written and easy to understand.\n- The paper contains red-teaming results for both text-to-text models and text-to-image models.\n- The authors evaluate the baseline method and FLIRT with GPT-Neo as a red LM, which is much cheaper than Gopher used in [Perez et al., 2022]. It is a huge contribution for the following researchers."
            },
            "weaknesses": {
                "value": "If I understood correctly, the contribution of this paper can be listed as follows:\n\na. Propose in-context learning methods which is better than stochastic-few-shot of [Perez et al., 2022].\n\nb. The proposed methods can control diversity and toxicity of generated prompts.\n\nc. Evaluate the red team methods on not only text-to-text models but also text-to-image models.\n\nSoundness [a]: The empirical results supporting the superiority of the proposed method seem weak. \n\nMissing reference [b,c]: There exists a previous work named Bayesian red teaming (BRT) which controls the diversity and toxicity of generated prompts and also conducts experiments on both text-to-text and text-to-image generative models [1]. BRT controls both diversity and attack effectiveness during the generation process. Also, [1] evaluates the red teaming method when the possible inputs are restricted to non-toxic texts. In this regard, several parts of the FLIRT paper are not that new.\n\n[1] Query-Efficient Black-Box Red Teaming via Bayesian Optimization, Lee et al., ACL 2023."
            },
            "questions": {
                "value": "- Can you show the score of other kinds of diversity metrics such as self-bleu in [Perez et al., 2022]?\n- In my opinion, stochastic few-shot can operate similarly to FLIRT by adjusting the temperature. For example, if we set the temperature of stochastic few-shot to a low value, the exemplar set would be constructed by the prompts with the highest scores, which is similar to Scoring attack version of FLIRT. \n- Moreover, there is an obvious trade-off between attack effectiveness and diversity in red-teaming (refer to fig 2 in [Perez et al., 2022]). However, there is only superiority in attack effectiveness according to table 1. Can you show the trade-off curve between diversity and attack-effectiveness of FLIRT? If the FLIRT's trade-off curve majorizes stochastic few-shot's trade-off curve, it would be obvious evidence of the superiority of FLIRT methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper contains several subsections discussing the ethical perspectives associated with the research."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4038/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4038/Reviewer_2CM2",
                    "ICLR.cc/2024/Conference/Submission4038/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698439509587,
        "cdate": 1698439509587,
        "tmdate": 1700503851524,
        "mdate": 1700503851524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ts4welWWPn",
        "forum": "JTBe1WG3Ws",
        "replyto": "JTBe1WG3Ws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_ZWk1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_ZWk1"
        ],
        "content": {
            "summary": {
                "value": "This work red-teams text to image and text to text models using in-context learning. They present a method called FLIRT that uses seed examples and labels from some harmful text/image classifier to help find new types of adversarial prompts with in-context learning. They test three different variations of the methods and conduct thorough ablation studies."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Red teaming mediated by in context learning is appealing because of the inductive biases that models have and because of a human\u2019s ability to influence the process with prompting.\n- I think their dataset of 76k prompts will genuinely be useful (I haven\u2019t personally looked through examples from it though.)\n- Section 3.2. was well-done.\n- Overall well-written"
            },
            "weaknesses": {
                "value": "1. I get how SFS is a relevant few-show baseline. But it seems like a fairly weak one overall. Other, perhaps less-efficient baselines could have been tested. For example, one could use the type of RL-based attack technique used in [Deng et al. (2022)](https://arxiv.org/abs/2205.12548), [Perez et al. (2022)](https://arxiv.org/abs/2202.03286), and [Casper et al. (2023)](https://arxiv.org/abs/2306.09442). Other approaches based on zero-order search could also be used like [Zou et al. (2023)](https://arxiv.org/abs/2307.15043) (and several predecessor works before it). I don\u2019t really fault the paper for not trying some of the other heavier approaches, but I think they could be discussed better.\n2. Related the the above, one baseline that I do really really wish were tested is to do in-context reinforcement learning. This would be similar to the scoring attack. You could use an advanced chatbot like Llama or GPT-4, give it an appropritate prompt, start it off with any examples you\u2019d like, and then let it learn in context from trial and error how to generate diverse adversarial prompts. Please comment on this.\n3. This red-teaming strategy really seems to have the humans do most of the heavy lifting. Is it really meaningful red-teaming is it is assumed that the red team starts off with a pretty good idea of what general types of prompts trigger bad behavior? One could argue FLIRT is just a glorified data augmentation technique paired with best-of-n sampling. (Meanwhile, the in-context red teaming approach mentioned above would not involve the humans doing such heavy lifting.) Could the authors comment on how often they discovered very novel/surprising/off-distribution adversarial prompts?\n4. Minor. Don\u2019t respond to these. Only respond to 1-3.\n    - Consider making the warning in the abstract red.\n    - Last sentence of the abstract is vague.\n    - I have never heard of red teaming being referred to before as \u201cadversarial probing.\u201d\n    - Prior to this work, [Casper et al. (2023)](https://arxiv.org/abs/2306.09442) penalized cosine distances to get diverse samples when red teaming.\n    - At no point does the paper highlight that one advantage of this red teaming technique over many others is that it\u2019s black-box. The paper should highlight that advantage!"
            },
            "questions": {
                "value": "See under weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4038/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4038/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4038/Reviewer_ZWk1"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731104594,
        "cdate": 1698731104594,
        "tmdate": 1700707962543,
        "mdate": 1700707962543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZooUa1eWqo",
        "forum": "JTBe1WG3Ws",
        "replyto": "JTBe1WG3Ws",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_Wisf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4038/Reviewer_Wisf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an automated red teaming method for text-to-image models like Stable Diffusion (with some experiments on text-to-text models as well). The method is similar to the few-shot method from Perez et al. (2022), but with several differences in design and better performance. There are extensive experiments and ablations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Automated red teaming is a timely and important problem, and there have been relatively few papers focusing on text-to-image red teaming\n- The few-shot method from Perez et al. was an interesting approach, and I'm glad to see more exploration of this type of method\n- There are many different variations of in-context red teaming explored in this paper, which could be helpful to future papers seeking to explore this space further\n- The results are strong"
            },
            "weaknesses": {
                "value": "- It would be good to have more baselines. E.g., methods like PEZ have also been evaluated primarily on text-to-image models, and some concurrent work from Google DeepMind would be good to compare to: https://arxiv.org/abs/2309.03409. The limited comparison to other methods is the main reason why I'm not giving a higher score initially."
            },
            "questions": {
                "value": "No questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4038/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699328311119,
        "cdate": 1699328311119,
        "tmdate": 1699636366844,
        "mdate": 1699636366844,
        "license": "CC BY 4.0",
        "version": 2
    }
]