[
    {
        "id": "Y1EkUHj1wH",
        "forum": "CY9f6G89Rv",
        "replyto": "CY9f6G89Rv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_r1KM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_r1KM"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes TSBO, a Bayesian optimization algorithm that employs semi-supervised learning through a teacher-student model which generates optimized synthetic data, which is incorporated into  a conventional surrogate model. The aim is to improve surrogate model generalization, and thereby enhance sample efficiency. The semi-supervised learning process involves a bi-level optimization scheme, which iteratively optimizes the mariginal log likelihood of synthetic data of the student and a weighted sum of. Results show improved performance on three chemical design tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "__Novel idea:__ Self-supervised learning has not thoroughly been explored in a BO context, and so the proposed idea explores a novel niche.\n\n__Addressing model accuracy:__ Addressing BO efficiency from the standpoint of producing a more accurate surrogate is a good approach - one that I believe deserves attention.\n\n__Multiple relevant ablations:__ With a large number of components to the algorithm, substantial resources have been dedicated towards ablations.\n\n__Figure 1:__ This effectively communicates the proposed method in a clear and pedagogical manner."
            },
            "weaknesses": {
                "value": "While the proposed approach is interesting, I believe that there is a substantial amount of complexity that is currently unjustified. Moreover, little is offered in terms of intuition as to why the involved components are jointly able to produce a more accurate surrogate model. I believe the paper has potential, but these outstanding issues would have to be addressed in a satisfactory manner for the paper to be accepted. Specifically,\n\n(To avoid confusion, I will denote the BO surrogate as the BO GP and the student as the SGP)\n\n### __1. The complexity of the method:__ \nIn addition to the Vanilla GP, there are three learnable components.\n- The teacher, trained on two real and synthetic data based on its own classification ability and the student's performance\n- The student, trained on synthetic data and validated on real data (synthetic and real)\n- The synthetic data generation process, which is optimized to minimize the feedback loss of the first two\n\nThis nested scheme makes it difficult to discern whether synthetic data are sensible, are classified correctly and what drives performance - essentially why this process makes sense. As such, plots demonstrating the fit of the student model, the synthetically generated data or anything else which boosts intuition is paramount. As of now, the method is not sufficiently digestible.\n\n\n\n### __2. The concept of adding synthetic data:__ \n In BO, The (GP) surrogate models the underlying objective in a principled manner which yields calibrated uncertainty estimates, assuming that it is properly trained. Since the GP in itself is principles, I am not convinced that adding synthetic data is an intrinsically good idea. The proposed approach suggests that the GP is either un-calibrated or does not sufficiently extrapolate the observed data. While this seems possible, there is no insight into which possible fault (of the GP) is being addressed, nor how the added synthetic data accomplishes it. Is it\n1. The teacher MLP that aggressively (and accurately) extrapolates in synthetic data generation?\n2. That the vanilla GP is simply under-confident, so that adding synthetic data acts to reduce the uncertainty by adding more data?\n3. Some other reason/combination of the two\n\nNote that I am specifically asking for intuition as to how the _modeling_ (and not the BO results) can improve by adding synthetic data.\n\n### __3. The accuracy and role of the teacher:__\nIf the teacher is able to generate accurate data (synthetic or real), why not use it as a surrogate instead of the conventional GP? Comments like\n\n#### _\"Evaluating the performance of the student that is trained with random unlabeled data far away from the global optimum, may not provide relevant feedback for tuning the teacher toward finding the global optimum.\"_\n\nsuggest that it is the teacher's (and not the BO loop's) task to find the global optimum. If the teacher adds data that it believes is good to the BO surrogate, the teacher is indirectly conducting the optimization.\n\n### __4. The role of the student:__ \nIf the student is trained only on synthetic data, validated on a (promising) subset of the true data, and its loss is coupled with the teacher's. As such, the student's only role appears to be as a regularizer (i.e. to incorporate GP-like smoothness into the MLP) while producing approximately the same predictions on labeled data. Can the authors shed light on the role on whether this is true, and if so, what difference the student offers as opposed to conventional regularization.\n\n### __5. Few tasks in results:__ \nThree benchmarks constitutes a fairly sparse evaluation. Does the proposed method make sense on conventional test functions, and can a couple of those be added?\n\n\n### __6. Missing references:__ \nThe key idea of 5.1 is _very_ similar to MES (Wang and Jegelka 2017), and so this is a must-cite. They also use a GEV (specifically a Gumbel) to fit $p(y^*)$. \n\n__Minor:__\n- App. B1: proposes -->purposes, remove \", however\"\n- Sec 5.0 of systematically sample --> of systematically sampling\n- Legend fontsize is tiny, Fig.1 fontsize is too small as well"
            },
            "questions": {
                "value": "- Is the synthetically generated data added as with any other data point, or are they added with additional noise?\n- Is the synthetically generated data ever replaced or substituted?\n- How (qualitatively) does the SGP differ from the BO GP in its prediction?\n- Does the parametrized sampling distribution discriminate regions that are believed to be good (like the GEV)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Reviewer_r1KM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698655608157,
        "cdate": 1698655608157,
        "tmdate": 1699636384402,
        "mdate": 1699636384402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aj54qP9XhO",
        "forum": "CY9f6G89Rv",
        "replyto": "CY9f6G89Rv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_tpuF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_tpuF"
        ],
        "content": {
            "summary": {
                "value": "The paper considers the problem of developing Bayesian optimization (BO) algorithm for optimizing `high-dimensional` black-box functions. The main proposal is to improve the surrogate modeling component of BO for high-dimensional spaces with a semi-supervised learning approach titled `TSBO` where the key idea is to leverage un-labeled data in the latent space BO framework by learning a pseudo-label dependent uncertainty-aware teacher-student model. \n\nThe teacher model is a multilayer perceptron that outputs a (mean, variance) pair which is used by a Gaussian process  based student model. A bilevel optimization problem is defined as the training objective of the student and teacher model. The teacher model parameters are updated with a combination of i. labeled data fitting loss and ii. feedback loss from the optimized student GP model. The student GP model parameters are optimized over the unlabeled dataset. This model is then used to generate the feedback loss for the teacher model based on a labeled validation set. The paper also proposes two different sampling approaches for picking the unlabeled dataset.\n\nOnce the teacher-student model is trained, the labeled dataset is combined with teacher-model predicted pseudo-labels for unlabeled inputs to train the main GP surrogate model (referred as data query GP model in the paper) for BO. Experiments are performed on a arithmetic expression task and two chemical design tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper considers an important problem with many real-world applications.\n\n- The overall idea of leveraging unlabeled data is nice and a nice broader insight to study as it can be instantiated in multiple different ways. \n\n- Overall BO results on the three domains are promising and the proposed method outperforms existing baseline approaches."
            },
            "weaknesses": {
                "value": "- I have a broad top-view question. The premise of the paper is very reasonable in suggesting that the bottleneck for existing BO methods is the limited amount of labeled data available. However, labeled data is still required for training all the new components introduced in the paper (teacher model/parametrized sampling distribution). If semi-supervised learning is the key source for getting such improved results, why wouldn't any other semi-supervised learning approach (manifold/laplacian regularization etc) work equally well?\n\n- The ablation described in section 6.3 is critical for showing the performance improvement in surrogate modeling. I think a better way to do it is by evaluating the test performance on original data (for e.g. get a set of molecules from the chemical design and compute their latent embeddings) rather than points sampled from the latent space. In the end, we want the GP surrogate to be better at modeling points which resemble the original input space rather than any general point in the latent space. Please consider running this ablation in this manner. \n\n- There is a large body of work on both high-dimensional BO and latent space BO that is not discussed in the paper. Please see some references below. It is not necessary to compare with them experimentally but they deserve proper discussion in the paper as they are directly relevant to the studied problem. In fact, reference [9] below even considers including semi-supervised learning via an auxiliary network. Reference [6] is one of the key baselines because it re-trains the GP surrogate along with VAE parameters as we get more data.  \n\nBO over high dimensional inputs\n\n[1] Eriksson, D., Pearce, M., Gardner, J., Turner, R. D., & Poloczek, M. (2019). Scalable global optimization via local Bayesian optimization. Advances in neural information processing systems, 32.\n\n[2] Eriksson, D., & Jankowiak, M. (2021, December). High-dimensional Bayesian optimization with sparse axis-aligned subspaces. In Uncertainty in Artificial Intelligence (pp. 493-503). PMLR.\n\n[3] Papenmeier, L., Nardi, L., & Poloczek, M. (2022). Increasing the scope as you learn: Adaptive Bayesian optimization in nested subspaces. Advances in Neural Information Processing Systems, 35, 11586-11601.\n\n[4] Letham, B., Calandra, R., Rai, A., & Bakshy, E. (2020). Re-examining linear embeddings for high-dimensional Bayesian optimization. Advances in neural information processing systems, 33, 1546-1558.\n\n[5] Nayebi, A., Munteanu, A., & Poloczek, M. (2019, May). A framework for Bayesian optimization in embedded subspaces. In International Conference on Machine Learning (pp. 4752-4761). PMLR.\n\nLatent space BO\n\n[6] Maus, N., Jones, H., Moore, J., Kusner, M. J., Bradshaw, J., & Gardner, J. (2022). Local latent space bayesian optimization over structured inputs. Advances in Neural Information Processing Systems, 35, 34505-34518.\n\n[7] Deshwal, A., & Doppa, J. (2021). Combining latent space and structured kernels for bayesian optimization over combinatorial spaces. Advances in Neural Information Processing Systems, 34, 8185-8200.\n\n[8] Notin, P., Hern\u00e1ndez-Lobato, J. M., & Gal, Y. (2021). Improving black-box optimization in VAE latent space using decoder uncertainty. Advances in Neural Information Processing Systems, 34, 802-814.\n\n[9] Eissman, S., Levy, D., Shu, R., Bartzsch, S., & Ermon, S. (2018, January). Bayesian optimization and attribute adjustment. In Proc. 34th Conference on Uncertainty in Artificial Intelligence."
            },
            "questions": {
                "value": "Please see weaknesses section above. I am more than happy to increase my score if the questions are answered appropriately."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727694194,
        "cdate": 1698727694194,
        "tmdate": 1699636384305,
        "mdate": 1699636384305,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ecI55A9ulg",
        "forum": "CY9f6G89Rv",
        "replyto": "CY9f6G89Rv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_QKF7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4183/Reviewer_QKF7"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel Bayesian Optimization (BO) procedure aimed at improving performances under a high dimensional data and low example setting.\nThe procedure circumvent the high number of example typically needed in latent space BO by using a \"Teacher-Student\" update loop to generate pseudo-labels during training. This loop requires sampling unlabeled data which is cheaper. \nThey empirically demonstrate the efficiency of their procedure and how their proposed optimization of each step of the procedure are key to performance improvement. \nThe four qualitative improvements demonstrated empirically are the following:\n- Pseudo-Label prediction improves performance even for baseline models.\n- The teacher-student method is efficient at optimizing the parameters using pseudo labels\n- The Uncertainty awareness part of the teacher-student method consistently ensures better performances\n- Optimized unlabeled sampling improves performance even for baseline models"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The method proposed in the article is robust to parameter changes and significantly improves past performances of other models such as .-LBO methods.\nThe methods trained with limited data even outperforms baseline models with unlimited access to data. \nThus the experiments tend to show that this method paves the way for novel applications where labelled and even unlabelled data is scarce."
            },
            "weaknesses": {
                "value": "The articles doesn't present the performances of the method with access to larger parts of the dataset. This is coupled with a lack of theoretical analysis of the convergence of the algorithm weakens the presentation of the asymptotic behavior of the method."
            },
            "questions": {
                "value": "Is there a class of functions for which the method converges towards the maximum? \nCan we expect any rate of convergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4183/Reviewer_QKF7"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772521846,
        "cdate": 1698772521846,
        "tmdate": 1699636384226,
        "mdate": 1699636384226,
        "license": "CC BY 4.0",
        "version": 2
    }
]