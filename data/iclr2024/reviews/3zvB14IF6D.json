[
    {
        "id": "BEJtOpRKo1",
        "forum": "3zvB14IF6D",
        "replyto": "3zvB14IF6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for controllable scene synthesis with an object-centric representation. It first extracts object slots, the object-centric representation, using an auto-encoder, and then trains a multi-view diffusion model conditioned on these object slots for novel view synthesis. Extensive experiments on MultishapeNet and Street View datasets support the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper demonstrates the effectiveness of using an object-centric representation for controllable scene synthesis. The object slots can be learned in an unsupervised manner and represent the scene decompositionally. Training a multi-view diffusion model conditioned on such representation can support high-quality scene synthesis, and enable many applications like object removal and transferring. \n\n2. This paper is well-written. It is easy-to-follow and contains much details."
            },
            "weaknesses": {
                "value": "1. The paper only shows results with low resolution. Experiments with higher resolution can further reveal the potential of the proposed method.\n\n2. The proposed scene editting scheme can only support object removal and transferring. Can the framework be extended to support more diverse scene editing operations (like translation, rotation)."
            },
            "questions": {
                "value": "1. Since you incorporate the Street View dataset, which features complex and unbounded outdoor scenes, I am particularly interested in the OSRT performance in this dataset. Could you please provide object-decomposed visualization as illustrated in the OSRT paper (Fig. 4). These visualizations would reveal how different portions of the street view are represented by individual slots.\n\n2. What is the rationale of incorporating a video (multi-view) diffusion model for synthesis? If the capacity of OSRT is strong enough, will a single-view diffusion model (conditioned on OSRT and single view direction) be enough to generate results with good multi-view consistency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_J43q"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734492351,
        "cdate": 1698734492351,
        "tmdate": 1699636596424,
        "mdate": 1699636596424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dLHKmK15kE",
        "forum": "3zvB14IF6D",
        "replyto": "3zvB14IF6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_dJMA"
        ],
        "content": {
            "summary": {
                "value": "The paper extends the Object Scene Representation Transformer (OSRT) with a diffusion-based decoder, proposing DORSal.\nIt enables a model to render precise images while maintaining properties of OSRT, i.e., (unsupervised) decomposition of objects.\nThe experiments were conducted with appropriate baselines, metrics, and datasets."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Straightforward success of the proposal idea to an important problem, i.e., diffusion decoder for OSRT-based NVS. It enables the models to generate sharp images with object-centric properties. This is a nice contribution to the research of SRT because the SRT family has been likely to render very blurred images, which is one of the largest weaknesses.\n- Easy to read and well-written manuscript. The text is very fluent and informative."
            },
            "weaknesses": {
                "value": "- A little reserved technical novelty. The usage and its effectiveness of diffusion-based decoders for precise rendering of NVS models are already widely spread this year (Watson et al., 2023; Chan et al., 2023; Tewari et al., 2023). In terms of that, this paper could be seen as a simple attempt to borrow the idea into another form of NVS, OSRT. While the video diffusion-like multi-frame architecture for considering consistency might be a novelty, the ablation test or comparisons on the decoder architecture are missing.\n    - While the core idea is straightforward, some implementation details or hyperparameters for better performance may be informative and implicitly have novelty if the codebase is released. Do you have any plans to release it publicly?\n- 3D inconsistency. SRTs do not have 3D consistency in design. Furthermore, DORSal could have worse 3D inconsistency due to its decoding process, in my understanding. From supplementary rendering videos, DORSal's results seem a little unstable when changing viewpoints. An example of objects changing their shapes continuously is shown in the center video of dorsal_multishapenet_3.gif.\n    - If possible, more investigation on 3D consistency is nice (while I understand the evaluation protocol is not obvious depending on the settings). The current test (only Sec. 5.3's) is quite limited and might underestimate failure cases by DORSal.\n    - (It could be a nice defense to suggest NVS applications requiring less 3D consistency if they exist.)\n\n\n----\n\nAdditional citations\n- Generative Novel View Synthesis with 3D-Aware Diffusion Models, Eric R. Chan et al., ICCV 2023\n- Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision, Ayush Tewari et al., NeurIPS 2023"
            },
            "questions": {
                "value": "- Conditioning for diffusion models uses frozen OSRT's representations. Is freezing required for better performance? Any experiments on joint training or two-stage training (i.e., 1. training OSRT -> 2. training OSRT and diffusion)? Appendix A says, \"End-to-end training comes with additional challenges (e.g. higher memory requirements), but is worth exploring in future work.\" Is the memory requirement extremely high?\n- Can DORSal faithfully reproduce images from the observed (input) viewpoints? Although OSRT was not able to reproduce inputs due to its blurred reconstruction, DORSal could do that. Even if it cannot, the result is very informative for analyzing the behavior. It may indicate that DORSal is still very unfaithful in terms of reference ability in addition to 3D consistency or estimation.\n- (a little out of interest) Does DORSal work in out-domain scenes? While I guess that OSRTs may not be good at out-domain scenes, DORSal could possibly be more generalized to them thanks to diffusion-based training. If true, it would be a new and significant strength of the DORSal.\n- Sec. 3.2 says, \"To obtain instance segments from edits with DORSal, we propose the following procedure...\" Actually, is the procedure also used for OSRT baselines in comparison? Or did baselines use different methods?\n- Sec. 5.3. says, \"This is also reflected in our quantitative ... mixed views.\" This description seems unclear. Could you provide the details of the test procedure? Is PSNR calculated by a rendered view and a +360 degree re-rendered view?\n- Fig. 6 says, \"Notably, the encircled tree in the final row is generated upon removal of a slot to fill up the now-unobserved facade previously explained by this slot. The original scene does not contain a tree in this position.\" Could you explain this more? I didn't understand whether this was success or failure (I felt it was bad behavior).\n- Fig. 5 shows combination-based editing. Did the results render only objects which are shown in one of the target scenes? Or, are there many unintended (or unintended-shape) objects? And, does each column is rendered from the same camera pose? In other words, I wonder whether I can assume an object is transferred to the same position (in the image) after the composite. If not, checking the result is pretty difficult."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742783420,
        "cdate": 1698742783420,
        "tmdate": 1699636596292,
        "mdate": 1699636596292,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PgRueEZwae",
        "forum": "3zvB14IF6D",
        "replyto": "3zvB14IF6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
        ],
        "content": {
            "summary": {
                "value": "This paper considers an object-centric representation for efficient scene representation and rendering. The work builds on earlier work on object based scene representations (OSRT) but considers a different decoder based on video diffusion models instead of a simple decoder network trained with an l2 loss. Experiments show that the novel view synthesis is much sharper leading to a lower FID score, and that the representation allows for scene editing."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The methodology is sound, by using a video diffusion model for the decoding/rendering which results in sharp and consistent images. This addresses issues with previous works where the renderings in general are quite blurry and/or not consistent.\n- Experiments show that the method can obtain significantly lower FID (but not PSNR and LPIPS) for novel view synthesis compared to existing work based on scene representations, although not as significant when compared to existing methods based on diffusion models (3DiM).\n- The renderings of the scenes appear consistent across views and over time. Furthermore, for scene editing, the filled in regions when objects are removed appear realistic, and the object slots seem to mostly correspond to specific objects (e.g. a car or a tree) in the scene."
            },
            "weaknesses": {
                "value": "- The scene editing is more a property of OSRT than the proposed method. The object-slots are pre-trained from OSRT and not refined or learned in this paper.\n- One main limitation of the method is that we can not control specific object slots. If we transfer a slot from one scene to the other, we can not (to my understanding, please correct if it is incorrect) e.g. move or rotate it in an easy way. It is placed in exactly the same position as in the original scene, which in general is not very useful.\n- The videos in the supplementary material would have been more informative if they would have shown results for all methods compared to as well, and also the input image/images."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_jJcr"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698759514000,
        "cdate": 1698759514000,
        "tmdate": 1699636596179,
        "mdate": 1699636596179,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xz7BeuZqN3",
        "forum": "3zvB14IF6D",
        "replyto": "3zvB14IF6D",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DORSal, a 3D scene generation model that leverages object-centric representations. The approach consists of two stages: first, it pre-trains an Object Scene Representation Transformer (OSRT) to encode multi-view images into slot-based representations that capture the objects in the scene. Second, it trains a diffusion-based conditional multi-view decoder that takes the frozen slot representations as input and renders novel views of the scene. The paper demonstrates that DORSal can generate 3D scenes with higher image quality than the baseline models, and also perform scene manipulation tasks such as object removal.\n\n---\n\npost rebuttal:\n\nrating 3 -> 5"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The generation quality of the proposed model is much better than the baseline models. This is a meaningful improvement as it can help scale up object-centric learning to large-scale applications.\n2. The experiments in object-level scene manipulations are quite interesting. The paper demonstrates that the learned representation allows object-level scene editing by removing or transferring slots between scenes. This is a promising result that could lead to further research in this area."
            },
            "weaknesses": {
                "value": "1. The model reliance on pre-trained object-centric representations instead of end-to-end training with diffusion models is a potential weakness. \n    * The quality of the object slots provided by OSRT may not capture the object representation, including the appearance information, well. Eventually, as the decoder becomes stronger and stronger, the quality of the slot representations will become the bottleneck of improving the generation quality. This brings limitation to the model in scaling up to more realistic scenes.\n\n2. Given that the representations are pre-trained, the model seems to be too straightforward and limited in its technical contribution. \n    * The improvement of image quality is obvious when the slots are pre-computed and frozen, one might expect the introduction of the diffusion decoder to have some effect on the representation learning process. However, the paper was not able to demonstrate this."
            },
            "questions": {
                "value": "1. What are the benefits and drawbacks of training DORSal end-to-end? How would it affect the quality of the object representations?\n2. In cases where end-to-end training poses challenges, such as collapsing issues, could it achieve better performance by finetuning the pre-trained slots encoder during the decoder training stage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5701/Reviewer_16Uh"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5701/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699136628253,
        "cdate": 1699136628253,
        "tmdate": 1700716291202,
        "mdate": 1700716291202,
        "license": "CC BY 4.0",
        "version": 2
    }
]