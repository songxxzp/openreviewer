[
    {
        "id": "GVIMEOH8ag",
        "forum": "QHfIe4chR5",
        "replyto": "QHfIe4chR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the vulnerability of Graph Neural Networks (GNNs) to targeted poisoning attacks, where an attacker manipulates the graph to misclassify a specific node. Most existing attacks focus on manipulating nodes within the node's neighborhood, but this paper explores \"long-distance\" attacks, where the manipulated nodes are outside this neighborhood. The paper presents a principled optimization-based approach for small graphs but also offers a more cost-effective heuristic-based approach for larger graphs. The findings indicate that long-distance targeted poisoning is effective and challenging to detect by existing GNN defense mechanisms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper is well-written, offering a clear and easily understandable presentation of the research.\n+ The approach and contributions made by the paper are noteworthy, particularly the exploration of long-distance targeted poisoning attacks in GNNs, even though the proposed method is primarily heuristic in nature."
            },
            "weaknesses": {
                "value": "- The MetaLDT method, while promising, appears to demand significant time and computational resources, which may limit its practicality for larger graphs.\n- The MimicLDT approach, while addressing the cost concerns, seems to compromise on the effectiveness of the attack. \nThis trade-off between efficiency and success rate should be discussed better.\nSome aspects of the paper's approach require further clarification. Additional details and explanations could help the reader better understand the methodology and its intricacies, enhancing the overall quality of the paper."
            },
            "questions": {
                "value": "- The definition of \"long distance\" and the specific distance of the injected malicious nodes remain unclear in the current version of the paper.\n\n- Have you taken into account the influence of robust methods during the poisoning process? If so, what are the results regarding the method's effectiveness when attackers lack knowledge of defense mechanisms, which is more practical in real-world scenarios?\n\n- The rationale behind why MimicLDT is more efficient is not clearly articulated. Further elaboration on this aspect would be beneficial. Is it possible to discuss the trade-off between efficiency and effectiveness, such as exploring adjustments to hyperparameters to strike a balance between MimicLDT and MetaLDT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801570381,
        "cdate": 1698801570381,
        "tmdate": 1699636335897,
        "mdate": 1699636335897,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FYeL6X1a83",
        "forum": "QHfIe4chR5",
        "replyto": "QHfIe4chR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_E2SS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_E2SS"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates targeted poisoning attacks on GNNs, in which an attacker injects nodes in a graph to cause a target node to be incorrectly classified to a label of the attacker\u2019s choosing and considers that the attacked nodes are not within the targeted node\u2019s k-neighborhood.  The proposed attack is then evaluated and tested against some empirical defenses."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+The studied problem is interesting"
            },
            "weaknesses": {
                "value": "-Threat model is strong\n- Novelty is limited\n-Missing many references"
            },
            "questions": {
                "value": "My major concern is that the problem has been extensively studied, and the novelty is not sufficient. \n\nThe authors claim that most of existing attacks on GNNs modify the target node\u2019s k-hop neighborhood, but this is not accurate. For instance, most of the cited poisoning attacks focus on global structure attack, where the entire graph structure can be modified. \n \nThe threat model assumes that the attacker has access to the training data, (including the original graph G, node features, and labels, and also knows the training procedure), which is a rather strong assumption. There exist (restricted) black-box attacks to GNNs, while the authors do not compare and discuss with them \n\nThe evaluated empirical defenses are easy to be broken by stronger attacks, as demonstrated in [a]. Hence, it is not surprising that these defense cannot defend against the proposed attack. \n\n[a] Felix Mujkanovic, Simon Geisler, Stephan G\u00fcnnemann, and Aleksandar Bojchevski. Are defenses for graph neural networks robust? Advances in Neural Information Processing Systems 35 (NeurIPS2022), 2022.\n\nIn fact, there exist many certified defenses against graph structure attacks, but the authors do not test them against the proposed attack."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698807217292,
        "cdate": 1698807217292,
        "tmdate": 1699636335824,
        "mdate": 1699636335824,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "v8vDYmR46y",
        "forum": "QHfIe4chR5",
        "replyto": "QHfIe4chR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
        ],
        "content": {
            "summary": {
                "value": "This paper studies targeted poisoning attacks on graph neural networks, which aims to cause misclassification of a single victim node. In order to increase the stealthiness of the attack, the injected poisoning points do not belong to the top-k neighbors of the target victim. The edges and node features of the fake injected nodes are optimized through meta-learning for small graphs and through feature-collision for larger graphs. Empirically, the proposed attack performs better compared to existing short-distance attacks, when the manipulatable nodes are far from the target nodes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The attack performance is good compared to other baselines when the attackers can only manipulate nodes that are far from the target victim.\n2. The approach of summarizing the attack patterns from expensive attacks (on small graphs) to design efficient attacks scalable for larger graphs is good."
            },
            "weaknesses": {
                "value": "1. The motivation of considering nodes that are outside the top-k neighbors of the target victim is unclear. The authors argued in the appendix that, using some graph explanation tools, the attached nodes can be retried relatively well in some settings. However, the authors made an implicit assumption that such a tool can be directly treated as a detection method, without distinguishing the differences between the influential nodes for the target victim and other nodes. Can we use some threshold to filter out suspicious looking influence nodes for the node under examination? Will this filtering step can be evaded by some adaptive attacks so that short-distance attacks can still survive without sacrificing the effective much? \n2. From the technical perspective, I did not find a significant (inherent) difference from the previously proposed Meta-Attack, as the major the differences are on optimizing a different loss function to encode the targeted attack objective and also to avoid making connections with nodes of top-k neighbors of the target victim during optimization."
            },
            "questions": {
                "value": "1. What is the value of $k$ to determine if an attack is short-distance or long distance. \n2. This is not a question, but rather a comment for the authors on proposing potentially stronger attacks. There is some interesting analogy between poisoning attacks on graphs and on images. The meta-learning approach is used to design poisoning attacks for both graphs (cited in the paper) and the images [1], the common drawback is the lack of scalability. The feature collusion attack is similar to the Shafahi et al.'s PoisonFrog paper cited in the paper. There might be some chance to rely on using the gradient alignment [2] technique to design stronger attacks in graph domains. \n\n[1] Huang et al., \"MetaPoison: Practical General-purpose Clean-label Data Poisoning\", ICML 2019.\n[2] Geiping et al., \"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching\", ICLR 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699043618347,
        "cdate": 1699043618347,
        "tmdate": 1699636335706,
        "mdate": 1699636335706,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EdAGsNkMnr",
        "forum": "QHfIe4chR5",
        "replyto": "QHfIe4chR5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes and studies a new type of attack on GNNs that does not modify the target node\u2019s k-hop neighborhood, which is called long-distance poisoning attack. To solve the problem, both a bilevel optimization-based approach inspired by meta-learning and an approximate heuristic-based approach are proposed. Extensive experiments are conducted on both small and large-scale graphs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Exploring the attack performance of long-range targeted poisoning attacks is valuable and important.\n2. The proposed MimicLDT is well-motivated based on the observation from MetaLDT.\n3. The paper is easy-to-follow."
            },
            "weaknesses": {
                "value": "1. The comparison with short-distance attacks is valuable. However, the compared baselines lack more recent node injection attack methods.\n2. Some claims lack further empirical or theoretical support. For example, the authors claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019. It would be better if authors could offer detailed support data analysis.\n3. Some minor errors:\ndesigne-> design\nheuristicsc)Finally -> heuristics. c) Finally"
            },
            "questions": {
                "value": "1. Whether the proposed method be generalized to unknown victim models?\n2. Is there any data analysis supporting the claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699336999739,
        "cdate": 1699336999739,
        "tmdate": 1699636335641,
        "mdate": 1699636335641,
        "license": "CC BY 4.0",
        "version": 2
    }
]