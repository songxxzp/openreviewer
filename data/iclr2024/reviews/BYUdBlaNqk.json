[
    {
        "id": "EfGxr4NR1G",
        "forum": "BYUdBlaNqk",
        "replyto": "BYUdBlaNqk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
        ],
        "content": {
            "summary": {
                "value": "This paper uses established regression techniques to determine how different models, including video-trained ones, relate to fMRI data collected during video viewing. It also first shows that such analyses can identify the input domain (image vs video) that models were trained on. Further comparisons explore architectures and layers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Demonstrates 'system identification' ability\n\nMany models are tested and compared\n\nIncludes analysis of hierarchical correspondence between brain and models\n\nResults are clearly presented in figures"
            },
            "weaknesses": {
                "value": "Novelty is somewhat overstated (other work has compared video-trained networks to the brain:\nhttps://pubmed.ncbi.nlm.nih.gov/29436055/\nhttps://proceedings.neurips.cc/paper/2018/hash/9d684c589d67031a627ad33d59db65e5-Abstract.html\nhttps://arxiv.org/abs/2306.01354)\n\nWriting is unclear at points.\nFor example, these are not complete/correct sentences:\n\"Since we can identify its modelling scheme,\nwhich acts as a form of ground-truth to be used when comparing different models. \"\n\"Towards this we investigate one model\nthe OmniMAE its pretrained model in a self-supervised manner compared to the finetuned one to a\ndownstream task with full supervision. \"\n\n\"We use the modelling scheme to refer to the\nmodel\u2019s ability to learn from dynamic information provided in an input clip and/or static information\nfrom a single image.\" I'm still unclear on what modeling scheme means. Is it the same thing that is later labeled as the (i) input?\n\n\"Since we have established the feasibility of identifying the target system to an extent with regression\nscores, it brings the question of how can we use this information to identify the underlying mechanisms\nin biological neural systems.\" This was only established for video vs image trained networks, so that should be clear."
            },
            "questions": {
                "value": "How do the authors understand their work in comparison to previous work that has showed self-supervised models to be equivalent to fully supervised in terms of neural prediction? e.g. https://www.pnas.org/doi/abs/10.1073/pnas.2014196118"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3872/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_wu71"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698353668251,
        "cdate": 1698353668251,
        "tmdate": 1700664244515,
        "mdate": 1700664244515,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pIi1M4gphY",
        "forum": "BYUdBlaNqk",
        "replyto": "BYUdBlaNqk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on two aspects of brain-machine modeling: (1) processing dynamic information (e.g., video clips) instead of typical static images; (2) whether system identification is feasible. In this study, the authors used the Algonauts fMRI dataset where cortical responses for 1000 video clips are avalible. To test the feasibility of system identification, a simulated and a realistic environment were created. In the simulated environment, I3D ResNet-50, ViT-B, and MViT-B were used astarget systems, and several computational models were used as source systems to regress on the targets. The results showed that targets trained for image and video understanding can be successfully differentiated using this regression approach. In the realistic environment, brain data were used as target systems. Using the regression approach, differences between image and video understanding, between convolutional and transformer operation, between fully-supervised and self-supervised training, can be revealed in brain responses.\n\nI appreciate this approach but the results need more explanation"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This study utilizes the movie fMRI datasets and extends past work from image to video understanding\n2. This study extends past work and investigates the system identification problems in video undertanding\n3. The results are informative\n4. The writing is very clear and easy to follow\n5. The selection of candidate models is representative and complete."
            },
            "weaknesses": {
                "value": "Weakness\n\n1. In the simulated environment, the authors claimed to focus on three aspects: (1) image/video understanding, (2) fully-supervised/self-supervised, and (3) convolution/transformer. However, Figure 1 only shows the result for (1). I am wondering what the results are for (2) and (3)?\n2. In the simulated environment, I3D ResNet-50, ViT-B, and MViT-B can be used for purposes (1) and (2), but not for (3). I would suggest including more target models for the purpose (3).\n3. In the realistic environment, Figure 2A shows the advantages of two-stream models over single-stream models. However, why should we compare them??  Two-stream vs. single-stream is not the part in the simulated environment nor the part in the introduction.\n4. Figure 4. if I understand correctly, OmniMAE-B pretrained is indeed self-supervised. But OmniMAE-B finetuned should be self-supervised + supervised finetuning. Is this comparison fair to show the differences between fully-supervised vs. self-supversied??"
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3872/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3872/Reviewer_vj7K",
                    "ICLR.cc/2024/Conference/Submission3872/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698488039147,
        "cdate": 1698488039147,
        "tmdate": 1699947487059,
        "mdate": 1699947487059,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WDmvZHcva7",
        "forum": "BYUdBlaNqk",
        "replyto": "BYUdBlaNqk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_N4F4"
        ],
        "content": {
            "summary": {
                "value": "In this manuscript the authors use DNNs for videos to predict fMRI responses and other networks in a pretest. As data they use the 2021 Algonauts challenge for predicting responses to 3 second video clips."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "It is\u2014in principle\u2014a step in the right direction to include temporal dynamics to better understand biological brains. We should move towards predicting responses to videos, not only images and the authors do that. Also they evaluate a selection of newer model architectures that were not available in 2021 when the Algonauts challenge with videos ran officially."
            },
            "weaknesses": {
                "value": "I think the results of this study are underwhelming though for three major reasons:\n\nFirst, the DNN models are not intended as models of biological vision and do not contain any interpretable dynamics that would enable conclusions about theories. Also, they usually run at so slow timescales (typically the frame rate of the video), that they could never have the temporal dynamics of biological networks in the first place. Thus, it is is not surprising that the conclusions are not particularly clean.\n\nSecond, fMRI is not able to resolve dynamics of visual processes. Thus, it cannot provide evidence about these dynamics.\n\nThird, the relationship to the Algonauts challenge remain unclear to me. The challenge website is open for post challenge submissions, so the authors could have submitted their models to the competition to get scores for the official test set. If that was not desirable for some reason, I think we would like to see the results for the top entires of this challenge to get an idea how close to the state of the art the models from this paper perform. Unfortunately, I do not see a substantial step of this manuscript beyond the Algonauts papers. \n\nThus, this manuscript does not provide the promised insights into dynamics and instead becomes an incremental step repeating things that have been done for image neural networks with video networks without providing substantial new insights."
            },
            "questions": {
                "value": "My main question for the authors is: Why this dataset and without the official evaluations? And to convince me of a better view of this work the main ingredient would have to be a substantial insight in how we might capture the dynamics of human visual perception better."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789695454,
        "cdate": 1698789695454,
        "tmdate": 1699636345398,
        "mdate": 1699636345398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f6Jnq4yZJa",
        "forum": "BYUdBlaNqk",
        "replyto": "BYUdBlaNqk",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3872/Reviewer_4dZd"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors perform a system identification study that focuses on modeling dynamics in perception by investigating multiple video models and comparing them with image models.\n\nThis study attempts to answer following research questions:\n\n1. Is it possible to distinguish video models from image models? \n2. Which models better predict human fMRI responses to videos?\n    1. Video vs. image models\n    2. Convolutional vs. transformers\n    3. Fully supervised vs. self-supervised\n\nThe authors perform extensive experiments using multiple models on simulated (predict other model\u2019s responses) and real (predict human fMRI responses) to answer these questions."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Layer-weighted encoding to compare models. This makes comparison easier removing the steps of layer selection for individual models. \n2. Varieties of models investigated in this study. The authors have carefully chosen a wide variety of models (conv vs. transformers ; self-supervised vs. supervised; image vs. video) using which they are able to answer multiple questions in this paper \n3. Statistical analysis to compare whether one family of model predict better than others.\n4. System Identification study(Figure1) investigating whether models from one modality (image/video) can predict the models from same modality better than models from other modality. The result showing that I3D early layers can be predicted equally well suggests I3D does not use temporal information well in early layers"
            },
            "weaknesses": {
                "value": "1. The authors claim \u201cprevious work did not consider the time\naspect and how video and dynamics (e.g., motion) modelling in deep networks\ncompare to these biological neural systems\u201d . This is incorrect. Several previous works [i-iv] have investigated modeling temporal aspects of videos and comparing it to brain responses. These works have been completely overlooked and not cited. Further seminal works on encoding and neural system identification from  Jack Gallant\u2019s group and Marcel Van Gerven\u2019s group are not cited. \n2. Several important details are missing\n    1. When comparing convolutional vs. transformer or self-supervised  in Figure 2 b,c ; did you consider both video and image models ?\n        1. If yes what was the reasoning, because if video models better predict brain activity doesn\u2019t it make sense to restrict only to video models. If both the video and image models are considered for comparison do you see same pattern for video and image family of models? \n    2. When you compare OmniMAE-B Pretrained/Finetuned what was the task OmniMAE finetuned on and on which dataset (Figure 3b)\n3. In Figure 3, it is not clear whether the results are statistically significant or not\n4. Some of the results require a deeper dive to gain better understanding of exactly what is happening\n    1. In Figure 1a(MViT-B) and 1c (I3D R-50), it can be clearly seen that variance in regression score by video models is quite high compared to image models suggesting some models are better predictor and some are worse. Which ones are worse/best predictors and why? This answer is important to understand how temporal information in video should be modelled. \n    2. Similar variance can be observed in Figure 2a-c as well raising the question why these models  are one family? A simple classification such as transformer vs conv or self-supervised vs supervised is not helpful here when there is so much variance within a family of models. The  conclusion that can be derived here are\n        1. From Figure 2a: 3 video models predict brain responses similar to or worse than image models while others predict better. Which are similar to image models and which are better is not answered.\n        2. From Figure 2b: some transformer models predict as well as conv models\n        3. The above conclusions are quite weak and less helpful and informative for readers without a deeper analysis.\n5. Overall, I find paper containing multiple results with unclear findings. \n\nReferences\n\ni) Nishimoto, Shinji, et al. \"Reconstructing visual experiences from brain activity evoked by natural movies.\"\u00a0*Current biology*\u00a021.19 (2011): 1641-1646.APA\n\nii) Khosla, Meenakshi, et al. \"Cortical response to naturalistic stimuli is largely predictable with deep neural networks.\"\u00a0*Science Advances*\u00a07.22 (2021): eabe7547.\n\niii) Nishimoto, Shinji. \u201cModeling movie-evoked human brain activity using motion-energy and space-time vision transformer features\u201d ; biorxiv 2021\n\niv) Lahner, Benjamin, et al. \"BOLD Moments: modeling short visual events through a video fMRI dataset and metadata.\" bioRxiv (2023): 2023-03."
            },
            "questions": {
                "value": "Suggestions: \n\n1.  Please add relevant citations \n2.  Refer to weakness point 2-4 and please address those."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3872/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847390434,
        "cdate": 1698847390434,
        "tmdate": 1699636345308,
        "mdate": 1699636345308,
        "license": "CC BY 4.0",
        "version": 2
    }
]