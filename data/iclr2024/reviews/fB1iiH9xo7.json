[
    {
        "id": "mTMEfsF7gX",
        "forum": "fB1iiH9xo7",
        "replyto": "fB1iiH9xo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_anv1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_anv1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new pretraining method for LiDAR-based 3D object detection, by colorization. In particular, to mitigate the ambiguity in colorization, this paper proposes Grounded Point Colorization (GPC), introducing some seed points as hints.\nI believe the idea is good and makes sense to me.\nExperiments are mainly conducted in the KITTI dataset, showing promising results but reaching the SOTA performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method of Grounded Point Colorization (GPC) is reasonable. The use of seed points is also a good design.\n- This paper is well-written and easy to follow.\n- Though the experiments are mainly conducted in KITTI, they are thoughtful and complete."
            },
            "weaknesses": {
                "value": "- Authors use object detection as the downstream task, but I personally believe LiDAR-based segmentation is the best choice. Colorization-based pre-training mainly learns the semantics in my opinion, but object detection needs accurate locations and poses especially in the benchmark using IoU-based metrics such as KITTI and Waymo. \n- The ultimate performance is not that good. The results on the Waymo Open Dataset are too weak. Other results on KITTI are also not convincing because PointRCNN and IA-SSD are not SOTA detectors nowadays. \n- Although the writing is clear, there are some unnecessary parts like the decorative math on page 4, which do not make the paper better."
            },
            "questions": {
                "value": "- According to my first concern in the weaknesses box, I wonder why not authors adopt segmentation as the downstream task? Or some other detection benchmarks relying on accurate semantics such as nuScenes? I believe the proposed method can obtain better performance on these tasks.\n- Authors should adopt SOTA detectors for experiments on KITTI, such as PVRCNN. The effectiveness should further be proven on Waymo Open Dataset with SOTA detectors such as PVRCNN, FSD, etc. The current performance is not competitive enough.\n\nPV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection \nFSD: Fully Sparse 3D Object Detection"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8517/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8517/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8517/Reviewer_anv1"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698471688708,
        "cdate": 1698471688708,
        "tmdate": 1700741065619,
        "mdate": 1700741065619,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WdoAD0HqQB",
        "forum": "fB1iiH9xo7",
        "replyto": "fB1iiH9xo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_cL42"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_cL42"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on an interesting topic - pretraining LiDAR networks by learning colorization.  To achieve LiDAR point colorization, this work proposes Grounded Point Colorization (GPC), which incorporates color as \"context\" by using GT colors during colorization.  The reported experiments seem to show it is effective to use colorization for LiDAR network pretraining."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is a new research idea - using colorization as a pre-train strategy for LiDAR networks.\n\n2. It is interesting to see that colorization might be effective for LiDAR-based network pretraining."
            },
            "weaknesses": {
                "value": "1. The authors use point-level, especially for the ground category, for pretraining, but lack validation of its effectiveness in instance-level 3D object detection tasks.\n\n2. They employ a multimodal pretraining approach (camera + LiDAR), while the compared methods are all unimodal (LiDAR-only).\n\n3. They did not conduct linear probing, so it's challenging to determine whether pretraining itself significantly improves performance or if enhancements were added during fine-tuning.\n\n4. Many details related to the proposed method and experiments have not been provided, which could confuse the readers.\nI have a few other questions listed in the next section."
            },
            "questions": {
                "value": "1. The camera and point cloud perspectives are inconsistent, which means that only a portion of points can find corresponding color information. It's somewhat surprising that this pretraining can significantly boost fine-tuning performance, especially in the case of KITTI, which only has a front-facing camera. Maybe the authors could provide a detailed analysis of why using colorization for pretraining is so effective.\n\n2. The authors claim that GPC + 20% fine-tuning can surpass 100% of training from scratch. However, it seems they haven't mentioned how they selected this 20%. If it's a random 20%, then the current LaserMix approach can achieve nearly the same level of fully supervised performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698742789607,
        "cdate": 1698742789607,
        "tmdate": 1699637064573,
        "mdate": 1699637064573,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g5k248ykEK",
        "forum": "fB1iiH9xo7",
        "replyto": "fB1iiH9xo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_nb3N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_nb3N"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a colorization-based pretraining approach for 3D LiDAR detectors. The idea is to use aligned RGB images to extract ground truth colors for a LiDAR point cloud and to then pretrain a network with the task of point cloud colorization. The paper empirically shows that a naive direct approach to this would actually harm the downstream detection performance, given that it would force the network to memorize specific scenes as the color of points could for example depend on the weather, the time of day. Furthermore, the color of many objects is not well-defined by the shape, e.g. cars come in different colors. To overcome this issue, the paper proposes to provide 20% of the points with the ground truth colors, thus allowing the network to focus on learning how to propagate these colors across semantically connected segments in a scene. Furthermore, instead of directly regressing RGB colors, all the colors in a dataset are clustered into 128 clusters and a class-balanced classification loss is used to train the network. Empirically, using the proposed pretraining, the paper shows that several downstream detectors can get significant improvements, often requiring only a fraction of the annotations to match the fully-supervised approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper presents a interesting and quite orthogonal approach to most other self-supervised pretraining methods for point cloud-based detection. Many design choices makes sense to me.\n- Empirical results look quite strong.\n- Important aspects are ablated and I very much like the fact that the paper first shows that naive colorization actually harms the performance.\n- The paper was a fun and refreshing read."
            },
            "weaknesses": {
                "value": "- The main weakness that I see is with the two baseline approaches, why did you focus on Point R-CNN and IA-SSD? There seem to be significantly better performing methods out there. These might not be point-based, but the paper also shows that the method transfers to voxel-based methods. So I'm mainly wondering what happens when we look at current state-of-the-art methods? Can we still gain something from colorization-based pretraining for those methods? Or is there some explicit reason why stronger methods cannot be considered?\n- Secondly, the related work is a bit lacking. While in general it covers the considered areas nicely, I'm missing related work on image colorization based pretraining, image colorization with seeds and general point cloud colorization. A quick google search reveals that all of these things exist. While I don't think they reveal major conflicts that should lead to the paper being rejected, I do think they are highly related and should be discussed. I'll name a few below, but obviously these papers are cited by many other papers and I'm sure for all three categories you can find some more relevant related work.\n  - Colorful Image Colorization by Zhang et al. looked at image colorization as a pretext task.\n  - Point2color: 3D Point Cloud Colorization Using a Conditional Generative Network and Differentiable Rendering for Airborne LiDAR by Shinohara et al. looks at point cloud colorization with a GAN setup. This could actually be a very interesting alternative to the seeding proposed here.\n  - Automated Colorization of a Grayscale Image With Seed Points Propagation by Wan et al. looks at seed-based image colorization. While different, it could be interesting to discuss such differences.\n- While you compare to ProposalContrast and DepthContrast, I find the discussion of the results quite lacking. Obviously they outperforms the proposed method in the case of cyclist and car, but only the strong performance of GPC for the pedestrian class pulls the overall score up. It would be good to at least talk about this a bit in the discussion of Table 2.\n- The ablation regarding the different loss variants is not that great. Again the discussion seems to be based on the overall score, but per class there are quite some significant differences. In general I am not completely convinced by this ablation, given these are completely different losses with potentially vastly different magnitudes. An actual fair comparison would require a new tuning of the learning rate, or potentially a normalization of the loss values with some theoretical maximum? Did you do something in this direction, or did you just replace the loss and assume that everything else stays the same? Also the SL1 regression loss seems better than CE loss in quite some cases, what about a balanced SL1? Could that be better?"
            },
            "questions": {
                "value": "- How did you sample the points that get a ground truth? Is this completely random? Or do you do some more informed sampling, ensuring that at least 1 sample is present for each cluster (if available in an image)? I guess this is not super crucial, but it could potentially reduce the amount of needed points and thus boost scores further since the network has to predict more.\n- Did you consider joint training? While we often see a strict pretraining and finetuning setup, think this could easily be trained jointly, maybe also improving results?\n- In the introduction you write \"A labeled example of 3D object detection comprises a point cloud and a set of bounding boxes. While the format is typical, there is no explicit connection between the input data and labels.\" I'm rather confused by this. To me there is definitely a direct connection between the input data and the labels and I don't understand how this is meant.\n\nIn general, while I enjoyed reading the paper, some sentences are a bit broken due to small grammar mistakes or missing articles. Please run this through a grammar checker or get a native speaker to proofread it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836259844,
        "cdate": 1698836259844,
        "tmdate": 1699637064444,
        "mdate": 1699637064444,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "j3r8BsL8ys",
        "forum": "fB1iiH9xo7",
        "replyto": "fB1iiH9xo7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_Hnni"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8517/Reviewer_Hnni"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a very interesting idea for boosting the performance of LiDAR-based 3D object detection by teaching the model to colorize LiDAR point clouds. The initial idea suffers from an inherent color variance issue, so the authors further propose the \"hints\" concept to directly provide ground-truth colors to some of the points that the model initially has to predict. This is a simple but effective idea grounded in both theory and experimental results. The proposed idea, namely GPC, demonstrates great performance in downstream 3D object detection tasks compared to benchmarks and has the potential to reduce the need for human annotations."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The idea of introducing colorization to pre-train a LiDAR-based 3D object detection model sounds novel, and the experimental results also demonstrate its effectiveness on benchmark datasets.\n2. The method of providing ground-truth labels to seed points to reduce the inherent color variance issue also seems straightforward but very effective.\n3. I love the way the authors write this paper because it provides a clear motivational flow, explaining why they believe colorization can bridge the gap between point cloud and bounding box prediction. They also describe how their first attempt failed due to the color invariance issue and further propose their thinking process and solution."
            },
            "weaknesses": {
                "value": "Please check Questions section for details."
            },
            "questions": {
                "value": "Overall, this is a very good paper, but I still have several concerns and hope to get the authors' clarification.\n\n1. How can we ensure that the \"color\" assigned to each point is accurate? Color only exists in 2D images, and as mentioned by the authors, the color of a LiDAR point is retrieved by projecting it to the 2D image coordinate. One concern is that for those points out of the field of view of the camera (e.g., multiple points with different colors can be projected to the same 2D pixel), the color can be inaccurately labeled.\n\n2. It would be great to have an ablation study regarding the number of color bins used.\n\n3. The reweighted loss idea in Eq 5 sounds reasonable, but one concern is that it may make the pretrained model specific to a particular dataset, reducing its transferability since each dataset can have a different number of classes and class distribution. It would also be interesting to see how the pretrained model performs when it is trained on one dataset but fine-tuned on another (this experiment is not required in the rebuttal; providing insights into this is sufficient.)\n\n4. Please include more recent state-of-the-art works for comparison in Table 1 and Table 2. While I can gauge the effectiveness of the proposed idea through existing comparisons, a more extensive comparison with more recent works on the same benchmark is also necessary. The leaderboard of these two benchmarks has several entries showing better performance, so it would be great to discuss the advantages of the proposed method when compared to them."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8517/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865376181,
        "cdate": 1698865376181,
        "tmdate": 1699637064336,
        "mdate": 1699637064336,
        "license": "CC BY 4.0",
        "version": 2
    }
]