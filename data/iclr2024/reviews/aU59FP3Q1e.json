[
    {
        "id": "vwWAt2o8qO",
        "forum": "aU59FP3Q1e",
        "replyto": "aU59FP3Q1e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_TPf3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_TPf3"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new CNN-based network architecture for visual recognition. Previous works mostly use an expansion ratio of 4 or 6 in the basic building block, which is often an inverted residual one. This paper shows that further increasing the expansion ratio to 12 can result in better performance. Based on this observation, a new network architecture, dubbed ConvNeSt, is proposed. Experiments show that the proposed method performs better than the baselines, including ConvNeXt and RepLKNet."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper is well written. The authors have clearly explained the motivation of this paper and it seems that the proposed method is easy to follow.\n\n- A series of ablation experiments are provided to support the arguments by the authors.\n\n- Comprehensive analysis is also provided to help readers better understand the design guidelines of the proposed network."
            },
            "weaknesses": {
                "value": "- The novelty of this paper is incremental. In the title, the authors have mentioned the importance of wide features in the inverted residual block. However, it seems that the performance improvement is limited as shown in Table 2. This makes me doubt on the effectiveness of the proposed method.\n\n- From Table 2, we can see that further increasing the expansion ratio brings performance drop. To guarantee the model size would not change much, the channel number of the identity path should be shrinked. Have the authors analyzed whether this affects the performance?\n\n- As the channel dimension increases in the middle part of the building block, the learnable convolutional kernels increase consequently. Have the authors analyzed whether this would speed down the inference process? As there are no latency results reported in this paper, it is difficult to infer this?\n\n- In addition, I recommend the authors to report the latency results of the proposed method as done in ConvNeXt. In most cases, FLOPs cannot directly reflect the running speed of a network network model. In many realworld applications, this is very important.\n\n- Most recent classification models, like ConvNeXt, report results based on large-scale models. As I found that when the model size is scaled up to 200M or more, the performance for different models does not change much when a good training recipe is used, for example, the one used in ConvNeXt."
            },
            "questions": {
                "value": "- The related work section is a little bit thin.\n- It seems that the proposed method performs worse than the recent EMO (ICCV'2023) work. Any explanations on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698580110887,
        "cdate": 1698580110887,
        "tmdate": 1699636175204,
        "mdate": 1699636175204,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1R229skhFy",
        "forum": "aU59FP3Q1e",
        "replyto": "aU59FP3Q1e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_idR9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_idR9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new ConvNet model, called ConvNeSt, which is designed for resource-constrained unimodal vision tasks. \nThe authors show a clear roadmap to the block design, such as 1) changes the position of Norm and Activation; 2) modify the expansion ratio etc. Complexity analysis and CKA similarities are also provided. ConvNeSt is validated on the image classification / instance segmentation / semantic segmentation challenges and outperforms state-of-the-art backbones across these tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The block-level design of ConvNeSt may be helpful to the community.\n2. The paper is well-written, and the design roadmap is easy to understand.\n3. The visualization and analysis parts are impressive."
            },
            "weaknesses": {
                "value": "1. The novelty of this paper is limited. \n2. The results of tiny/small/base-sized models are given. Will the model design still work on large-sized models?\n3. As the authors claim that ConvNets can offer a hardware-friendly solution compared to ViTs, could you show some advantages (like inference speed or memory usage on specific devices) owned by ConvNeSt compared to ViTs. What is more, the comparison with ConvNeXt should also be given."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2398/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2398/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2398/Reviewer_idR9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666907126,
        "cdate": 1698666907126,
        "tmdate": 1699636175130,
        "mdate": 1699636175130,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3kk3iC32Ph",
        "forum": "aU59FP3Q1e",
        "replyto": "aU59FP3Q1e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_N5dm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_N5dm"
        ],
        "content": {
            "summary": {
                "value": "The paper presents ConvNest, a novel convolutional neural network that leverages convolutions across high-dimensional features, significantly enriching the network's feature extraction capabilities. This innovative approach facilitates smoother loss landscapes and enhances learning efficiency, resulting in more distinct and enriched feature representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper shows how ConvNest improve and beat in most cases comparable architectures such as ConvNext in task such as image segmentation and image recognition. Although very slightly in some cases, ConvNest shows a more efficient architecture that reaches similar accuracy but with fewer parameters or resources needed. \nThe paper  presents different evidence not only accuracy to support the advantage of using ConvNest, for instance an study of the loss landscape and the CKA analysis on the feature space, and even activation maps of selected samples in the apendix. The paper is well written."
            },
            "weaknesses": {
                "value": "While ConvNest's advancements are clear, I noticed a slight disconnect from the initial discussion on multimodal learning, as the subsequent tasks appeared predominantly unimodal. A refined opening statement could better define the paper's scope, potentially making room for a side-by-side comparison with transformers for a comprehensive analysis.\n\nAlthough the paper's aim to present a more efficient solution is evident, the impact of relaxing the parameter count constraint on ConvNest's performance remains unclear. Introducing an upper bound model could solidify the argument, illustrating the potential benefits and promising future of adopting the proposed modifications in ConvNest."
            },
            "questions": {
                "value": "What are the limitations of this model? Why it is unable to reach higher performance? \n\nThe activation maps shows great potential for this model, would this model be more robust than other if tested on robust benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811950216,
        "cdate": 1698811950216,
        "tmdate": 1699636175021,
        "mdate": 1699636175021,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MacmDJudeX",
        "forum": "aU59FP3Q1e",
        "replyto": "aU59FP3Q1e",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_mPKv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2398/Reviewer_mPKv"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new design called \"ConvNeSt,\" which is a nested design of ConvNet. This design is proposed to outperform the existing ConvNeXt and other methods on standard vision benchmarks. The paper also touches upon the post-ViT era ConvNets, which have been influenced by the Vision Transformer (ViT) model. The research have tested various models on benchmarks like ImageNet-1K and COCO, comparing their performance in terms of accuracy, FLOPs, and other metrics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces a novel nested design for ConvNets, which is expected to enhance their performance on standard vision benchmarks.\n\nComprehensive testing on well-known benchmarks like ImageNet-1K and COCO provides credibility to the results.\n\nThe inclusion of visualizations, tables, and figures likely aids in understanding the results and the model's performance."
            },
            "weaknesses": {
                "value": "1. As depicted in Fig. 2, the ConvNeSt block offers limited novelty and lacks an in-depth analysis.\n\n2. Increasing the dimension by a factor of 12 will lead to a significant rise in parameters and flops, especially due to the FC layers. Is there any guidance to help us strike a balance between width and computational complexity?\n\n3. When applying convolution to the 12d dimensional feature, even though the flops and parameters remain relatively low in the convolutional layers, there will be a notable reduction in latency. I recommend that the authors report latency measurements on devices, such as GPUs.\n\n4. I suggest revising the figures (e.g., Fig. 2, Fig. 3) in the submission to enhance visualization.\n\n5. The paper omits some crucial baselines, such as VAN [1], SP-VIT [2], ConvFormer[3], MViTv2[4], and others.\n\n[1] Visual Attention Network\n[2] SP-ViT: Learning 2D Spatial Priors for Vision Transformers\n[3] MetaFormer Baselines for Vision\n[4] MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"
            },
            "questions": {
                "value": "Please see above weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2398/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698903997971,
        "cdate": 1698903997971,
        "tmdate": 1699636174964,
        "mdate": 1699636174964,
        "license": "CC BY 4.0",
        "version": 2
    }
]