[
    {
        "id": "39bbLPMtqQ",
        "forum": "BRoBig6ov1",
        "replyto": "BRoBig6ov1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_Yoac"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_Yoac"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a tensor recovery technique called Tensor Decomposition Based on Slices-Wise Low-Rank Prior (TDSL) to address challenges in processing high-order tensor data with non-smooth changes. Traditional methods based on t-SVD struggle with non-smooth changes caused by random slice permutation. The proposed TDSL method incorporates a set of unitary matrices to effectively handle permutation variability and non-smoothness. It also introduces a new tensor norm called Tensor $U_1$ norm to extend TDSL to higher-order tensor completion. An optimization algorithm combining the proximal algorithm with the Alternating Direction Method of Multipliers is proposed to solve the resulting models. The convergence of the algorithm is theoretically analyzed. Numerical experiments demonstrate the effectiveness of the proposed method in high-order tensor completion, especially for tensor data with non-smooth changes."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**Technical correctness.**  The paper introduces a new tensor recovery technique known as Tensor Decomposition Based on Slices-Wise Low-Rank Prior (TDSL).  The proposed TDSL method demonstrates technical soundness in addressing issues related to permutation variability and non-smoothness in real-world scenarios. It introduces a set of unitary matrices that can be either learned or derived from prior information, facilitating the handling of non-smooth changes resulting from random slice permutation. Additionally, this work introduces a new tensor norm called the Tensor $U_1$ norm, extending TDSL to higher-order tensor completion without requiring additional variables and weights.\n\n**Clear Writing.** The paper maintains a clear and concise writing style, effectively conveying the key concepts and techniques of the proposed method. The use of tables to present experimental results enhances readability and facilitates comparisons of performance across different methods."
            },
            "weaknesses": {
                "value": "**Limited Novelty.**  While the paper introduces TDSL as a new tensor recovery technique, it's worth noting that similar ideas have already been explored in the literature with clearer motivations and more in-depth theoretical analysis. For instance, reference [R1], authored by Liu G. and Zhang W., investigates the recovery of future data through convolution nuclear norm minimization and provides a comprehensive theoretical foundation.\n\n[R1]. Liu G, Zhang W. Recovery of future data via convolution nuclear norm minimization. IEEE Transactions on Information Theory. 2022 Aug 5;69(1):650-65.\n\n**Limited Theoretical Depth.** The paper does not appear to adequately address the concerns raised regarding its theoretical depth, the derivation of estimation error bounds, and the discussion of sample complexity. These aspects are crucial for a comprehensive understanding of the proposed method and its practical applicability.\n\n**Limited Experimental Evaluations.** The experiments i are limited to a small number of datasets with relatively small sizes. There is a lack of extensive experimentation on large-scale datasets, which can potentially limit the generalizability of the proposed method's performance. Expanding the experimental evaluations to include larger and more diverse datasets would provide stronger empirical support for the approach.\n\n**Limited Significance for ML**. This paper does not provide a clear explanation of how the proposed method can contribute to addressing commonly concerned machine learning issues. As a result, the potential theoretical and empirical significance within the machine learning community may be limited. \n\n**Reproducibility.** While the proposed algorithm contains many details that would be necessary for a reproducible experiment, there is no code provided. This lack of code availability can hinder the ability of other researchers to replicate and verify the results, which is a crucial aspect of scientific reproducibility."
            },
            "questions": {
                "value": "**Question 1.** Is there a clear explanation of the differences and innovations of the TDSL method compared to previous research, such as the reference [R1]?\n\n**Question 2.** Regarding theoretical depth, can you provide more theoretical background on the TDSL method, especially regarding theoretical explanations related to estimation error bounds and sample complexity?\n\n**Question 3.**  In terms of experimental evaluations, how to expand the experiments to include larger and more diverse datasets to enhance the generalizability of the proposed method's performance?\n\n**Question 4.** Can the authors provide more information on how the TDSL method addresses commonly concerned machine learning issues, emphasizing its importance within the machine learning community?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Reviewer_Yoac"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698740499422,
        "cdate": 1698740499422,
        "tmdate": 1699636981233,
        "mdate": 1699636981233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uBPEdZ076x",
        "forum": "BRoBig6ov1",
        "replyto": "BRoBig6ov1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_w1M5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_w1M5"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new tensor U1 norm based on invertible transforms, and then develops two low-rank tensor completion algorithms based on the transformed tensor and the ADMM, which exploit the slicewise low-rankness. Computational complexity and convergence of the proposed algorithms are discussed. In addition, various numerical experiments on synthetic data test and two real-world applications including image sequence inpainting and color video inpainting have shown the outstanding performance of the proposed algorithms, especially the one based on the tensor U1 norm. The organization of the paper is complete with valid experimental justification."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed tensor norm has a certain novelty in high order tensorial data completion. \n2. Theoretical discussions on the complexity and convergence guarantees may be beneficial for other related works. \n3. Real-world applications are important for demonstrating the applicability of the proposed approaches."
            },
            "weaknesses": {
                "value": "1. Notation clarification should be further improved. For example, the dimensions $I_1,\\ldots, I_n$ and the matrix nuclear norm $\\|\\cdot\\|_*$ are not clearly mentioned; and the dimensions of tensors or matrices or the underlying tensor spaces should be clearly presented at least at the beginning to avoid confusion. Minor issues about notation: equation xxx -> (xxx) and some notation could be shortened. \n2. The proposed tensor U1 norm seems to be a straightforward extension of dictionary-based L1-norm for vectors/matrices. The connection in this direction was not mentioned or explained.\n3. In the abstract, the authors claimed that the proposed methods can handle non-smooth changes. Unfortunately, it cannot be seen explicitly through either the proposed models or the numerical justifications. The motivation of the proposed methods could be explained in more detail. Moreover, a new tensor decomposition was claimed to be introduced in this paper, which does not seem the case since only one slice-wise tensor decomposition form (1) is adopted. Some necessary references for this tensor product are missing. \n4. In the numerical experiments, it needs to describe the structure of the underlying data, i.e., whether it's sparse or low-rank in some desired transform."
            },
            "questions": {
                "value": "1. In the current setting, all the matrices $U_{k_i}$'s are restricted to orthogonal. But can they be extended to unitary matrices, e.g., DFT?\n2. In Algorithm 1 line 3, where do the matrices $U$ and $V$ come from? They are never used in the other lines of the entire algorithm.\n3. The tensor norm in (3) is based on the across-slice wise low-rankness under certain transform, i.e., transformed low-rankness, but the tensor U1 norm in (9) is to describe the transformed sparsity. What are the connections between them? If the data set is only sparse or only low-rank, then would it be unfair to compare both in the same context? Furthermore, why is TC-U1 always performing better than the TC-SL? Is that due to the low-rankness nature for the testing data sets instead of sparsity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771047978,
        "cdate": 1698771047978,
        "tmdate": 1699636981109,
        "mdate": 1699636981109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1dKXHt5jYJ",
        "forum": "BRoBig6ov1",
        "replyto": "BRoBig6ov1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_1gNF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7971/Reviewer_1gNF"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the t-SVD based low-rank tensor recovery problem under the potential non-smooth changes along the third dimension. To tackle that, the authors introduce a new tensor $U_1$ norm with a set of {$U_{k_n}$} given by the prior slides-wise low-rank information. Some, but not comprehensive, theoretical results have been established, together with a good load of numerical experiments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper aims at an interesting new angle of t-SVD based tensor recovery problem. As far as I read, the analysis is overall correct."
            },
            "weaknesses": {
                "value": "Although the analysis is about right, the results rely on a set of accurate slice-wise low-rank prior. That leads to major questions:\n\n1. How to efficiently obtain those priors? Please give examples. Based on what I read in the numerical experiment section. It seems the authors don't have a practical way to obtain the required prior {$U_{k_n}$}.\n\n2. The analysis doesn't show what could happen if the inaccurate low-rank prior was used. There is a chance of having even worse recovery results when some large noise is added to the prior.\n\nTherefore, I doubt the proposed $U_1$ norm is practically useful at all.\n\nThe paper was not carefully written, many notations were used before being defined. For example,\n\n3. Page 3 under (1), $(I_{k_1}, I_{k_2})$ were not defined until page 4 Definition 1.\n\n4. Page 3 above (2), the full name of DCM were not introduced until page 7 Sec 4.1.\n\n5. Page 4 'By known result in Luet al. (2019b)', although I am familiar with the paper, the author should restate the quoted result for the completeness of the analysis. \n\n6. Page 7, 'For given $R$' -> 'For given rank parameter $R$'"
            },
            "questions": {
                "value": "1. The authors tried to motivate the 'non-smooth changes' problem with a 'random slice permutations' story. It is not clear to why random slice permutations may happen in real applications. Please be more specific about why the image classification task will cause random slice permutations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7971/Reviewer_1gNF"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7971/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808426245,
        "cdate": 1698808426245,
        "tmdate": 1699636980986,
        "mdate": 1699636980986,
        "license": "CC BY 4.0",
        "version": 2
    }
]