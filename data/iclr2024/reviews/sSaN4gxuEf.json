[
    {
        "id": "i3nCqXdf3Q",
        "forum": "sSaN4gxuEf",
        "replyto": "sSaN4gxuEf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_PHNg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_PHNg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an approach using pre-trained features of the foundation model. They build a knowledge bank to learn the transferable knowledge from source domains, and then the knowledge bank is used to generate a domain-specific prompt by a domain prompt generator. This prompt as the guidance can enable domain-aware learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "${\\bf Strengths:}$\n\n[$\\textbf{Well-illustrated Figures}$] The figures can clearly convey the idea of the proposed method.\n\n[$\\textbf{Comprehensive Experiments}$] This paper offers the experimental results comparing many baseline methods, ablation study of various losses and hyper-parameter analysis."
            },
            "weaknesses": {
                "value": "${\\bf Weaknesses:}$\n\n[$\\textbf{Confusing Expression}$] The expression is sometimes hard to follow and understand. For example, (i) I am not sure how the method works from \u201cThe domain prompt then directs the visual features towards a particular domain via a guidance module.\u201d; (ii) they first mention \u201cpropose an approach on top of the pre-computed features of the foundation model.\u201d, but then say, \u201cwe build a knowledge bank to learn the transferable knowledge from source domains.\u201d. It cannot see the connection between these two sentences. (iii) For \u201cIn FSTT-DA, extracting domain-specific knowledge from few-shot data remains a major obstacle.\nHence, leveraging generalized semantic knowledge from pre-trained backbone and transferable domain knowledge from source domain datasets is vital.\u201d, these two sentences have no logic relation. Considering the problems above, it needs a careful refinement before publication.\n\n[$\\textbf{The Selection of CLIP Image Encoder}$] The paper uses two different image encoders for DomainNet and WILDS. Could the authors explain the reason? It would be nice to see the performance of two encoders on two datasets.\n\n[$\\textbf{Missed Experiments}$] (i) One SOTA work is not compared, e.g., \u201cCLIPood: Generalizing CLIP to Out-of-Distributions, ICML 2023\u201d. (ii) It would be nice to see the computational cost of MIRO.\n\n[$\\textbf{Some Suggestions}$] The subfigures in Figure 2 do not show the obvious differences between baseline and the proposed method. It would be nice to change style or type of figures to make it clearer."
            },
            "questions": {
                "value": "Can the ensemble be used for the proposed method as shown in Table 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Reviewer_PHNg"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698307056405,
        "cdate": 1698307056405,
        "tmdate": 1699636681186,
        "mdate": 1699636681186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "MIR1ouMd1h",
        "forum": "sSaN4gxuEf",
        "replyto": "sSaN4gxuEf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_1gy7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_1gy7"
        ],
        "content": {
            "summary": {
                "value": "This paper solves test-time model adaptation. The authors propose to build a knowledge bank from source domains, and use a domain prompt generator to get a domain-specific prompt conditioned on few-shot target data. A guidance module with domain prompt, a domain-aware constrastive loss and meta-learning are designed to facilitate effective model adaptation. Experiments are conducted on 5 large-scale benchmarks, showing improved performances."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The studied task, Few-Shot Test-Time Domain Adaptation, is important yet challenging. Although there are many works focusing on parameter efficient learning with few-shot data, how to extract useful domain knowledge is an interesting perspect that deserves more research.\n-  The authors design several modules including transferable knowledge bank, conditional domain prompt generator and domain-aware contrastive loss and domain guidance module. Overall, the motivation of each module is clear. In addition, a meta-learning based training scheme is proposed for domain episodic training.\n- For the experiments, improvements of VPG over comparison methods look siginificant from Tables 1-2. Abalation studies are extensive.\n- The writing of paper and figure illustration are good."
            },
            "weaknesses": {
                "value": "- The method is a bit complex. The loss term in Eq. (5) consists of three terms that need to be properly balanced. As those correlation loss and contrastive loss are at different scales, the model sensitivity against such hyper-parameters especially considering the few-shot data is unclear.\n- Since the VDPG is built upon prompt learning of CLIP, comparison with CNNs backbones is less informative. Methods like ERM, CORAL, MTL can be applied with CLIP.\n- Fig. 2(a,b) uses a black background, which leads to a poor visual contrast."
            },
            "questions": {
                "value": "- Since the method learns a conditional domain prompt generator from source domains, I wonder how different numbers of source domains affect the learned generator. In particular, if there is only one source domain, would the method still work?\n- In the Bi-direction cross-attention module, what is the benefit using two 'Image attend to tokens' layers?\n- In the Algorithm1, how to judge the stopping criterion of convergence?\n- In Conditional domain prompt generator E(x), what is 'l is the number of embeddings'? Why not using the image embedding after average pooling for each image to calculate K,V in Attention?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Reviewer_1gy7"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698687857821,
        "cdate": 1698687857821,
        "tmdate": 1699636681054,
        "mdate": 1699636681054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nFrS9sVwns",
        "forum": "sSaN4gxuEf",
        "replyto": "sSaN4gxuEf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_f7Jt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_f7Jt"
        ],
        "content": {
            "summary": {
                "value": "In this work, authors leverage the foundation model CLIP to provide test time adaptation on a new target domain, while only relying on a few unlabeled samples at test. Different components are used to attain state-of-the-art performance on the standard benchmarks. Transferable domain knowledge bank and domain-specific prompt generation to guide the visual features from the new domain to perform classification in the known label space. This work has extensive experiments and ablations to show the efficacy of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Well-written and easy to follow\n2. Interesting approach to leverage CLIP knowledge for performing few-shot TTA. The different novel components are combined together in a non-trivial manner to attain the SoTA performance. \n3. Extensive ablations on knowledge bank, prompt generation, losses and evaluation on the standard benchmarks are provided."
            },
            "weaknesses": {
                "value": "1. How low can KB size be? or how much sensitivity to Z value we can have?"
            },
            "questions": {
                "value": "I have my minor comment in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839407956,
        "cdate": 1698839407956,
        "tmdate": 1699636680947,
        "mdate": 1699636680947,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B7KDr7ikGJ",
        "forum": "sSaN4gxuEf",
        "replyto": "sSaN4gxuEf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_hhmW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6230/Reviewer_hhmW"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a new approach to address distribution shifts at test-time by utilizing a few unlabeled data, emphasizing the challenge of extracting domain knowledge from limited data. The proposed method builds on the features of foundation models, creating a knowledge bank to learn transferable knowledge from source domains. When given few-shot target data, a domain prompt generator is developed to condense this knowledge bank into a domain-specific prompt, which guides the visual features to a specific domain. The method, named Visual Domain Prompt Generator (VDPG), integrates a domain-aware contrastive loss and employs meta-learning to improve domain knowledge extraction, and it outperforms previous methods on multiple benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- VDPG not only outperforms other methods on the WILDS benchmark, but it specifically showcases superior results on individual datasets like iWildCam, Camelyon17, and FMoW.\n- VDPG's ability to generate high-quality domain-specific prompts tailored to each target domain is not just a novel approach, but one that proves effective\n- When compared to methods like FYLP and DoPrompt, VDPG is designed for quicker adaptation and inference, making it a more feasible choice for real-world applications where computational resources and time are essential factors."
            },
            "weaknesses": {
                "value": "- The results show that ERM training alone cannot drive performance. Specific configurations, such as episodic learning, are necessary to boost performance. This indicates a complexity in training dynamics that might be challenging to replicate or optimize in varied settings.\n- The method adapts with few-shot data before making inferences on all target data. While this is computationally efficient, there's a potential risk of overfitting or being overly reliant on a limited subset of data."
            },
            "questions": {
                "value": "."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6230/Reviewer_hhmW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6230/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698898405436,
        "cdate": 1698898405436,
        "tmdate": 1699636680824,
        "mdate": 1699636680824,
        "license": "CC BY 4.0",
        "version": 2
    }
]