[
    {
        "id": "O7kuEjKGnv",
        "forum": "PHGxChm1l5",
        "replyto": "PHGxChm1l5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
        ],
        "content": {
            "summary": {
                "value": "Current vision-language models (VLMs) lack this ability due to their \"bag-of-words\" approach and inability to accurately represent visual entities and their relationships. The proposed Compositional VLM addresses this by guiding the model to explicitly compose visual entities and relationships and enabling dynamic communication between the visual detection system and the language system. This is achieved through the introduction of communication tokens, which guide the detection network to propose relevant visual regions based on the generated sentence. These regions are then integrated into the language generation process. This iterative communication between vision and language continues until a complete sentence is generated. This approach effectively bridges the gap between visual perception and language models, significantly outperforming other VLMs in compositional reasoning benchmarks. It also performs well in traditional vision-language tasks like referring expression comprehension and visual question answering."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Dataset Collection**: The paper collect new object level bounding box to image-text pairs ensures that the data is not just vast but also well-curated, enhancing the model's training and performance.\n\n2. **Reasonable Model Design**: The introduction of the Compositional VLM, with the token design framwork and the interation between visual detector LLM, displays a systematic approach towards bridging the gap between visual entities and their textual descriptions.\n\n3. **Comprehensive Experiments**: With the grounded image-text pairs, the paper offers a detailed experimental setup, validating the model across various compositional reasoning benchmarks.\n\n3. **Strong Performance**: The Compositional VLM showcases impressive results, especially when compared to previous vision-language models."
            },
            "weaknesses": {
                "value": "1. **Fundamental Oversimplification of Compositionality:** The Compositional VLM framework, though innovative, may not truly capture the essence of disentangling objects and their relationships within images. Instead of delving deep into the inherent complexities of this challenge, the method leans towards gathering object-text-image paired datasets and reinforcing their connections. This approach, while seemingly effective, might only be a surface-level solution rather than addressing the root of the compositional problem.\n\n2. **Scalability Concerns**: The model's heavy reliance on precise associations between text captions and visual entities raises questions about its scalability. Can it consistently perform well in diverse or ambiguous scenarios, or will it be constrained by the specificity of its training data?\n\n3. **Rigid Token Implementation**: The manual nature of token positioning (with <obj> around the object and <box> after them), suggests a lack of flexibility in the model. This rigidity could hamper the model's adaptability, especially when faced with varied or unforeseen testing scenarios. For example, if we have a \"a man between two dog\", how to deal with the <box> for multiple instance of dog.\n\n4. **Operational Inefficiency**: The necessity for pre-parsing sentences and manually inserting tokens, even during testing, indicates potential operational bottlenecks. This could impede real-time applications and demands additional preprocessing steps, detracting from the model's overall efficiency."
            },
            "questions": {
                "value": "1. When integrating the object detection model within the auto-regressive language model, what would be the time complexity for each individual inference? Would it be very slow?\n\n2. Given that the proposed Compositional VLM necessitates text parsing even during inference, how resilient is the model to inaccuracies or ambiguities in token placement? For instance, the phrase \"a black dog\" could be parsed as \"a <obj>black dog<\\obj>\" or \"a black <obj>dog<\\obj>\". How does the model handle such variations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "New collected dataset, may require ethic review."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_fA94"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697957034151,
        "cdate": 1697957034151,
        "tmdate": 1700634102710,
        "mdate": 1700634102710,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pfnqWjkjoV",
        "forum": "PHGxChm1l5",
        "replyto": "PHGxChm1l5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Compositional VLM, a vision-language architecture allowing the language model (LLM) to communicate with the vision encoder(s) to generate its output iteratively. Several new tokens, such as <box>, <prebox>, <visual>, and <previsual>, are introduced to facilitate this communication between the two components working on different modalities. The goal is to improve the ability of VLMs to capture the compositional structure found in vision-language tasks. The model is pre-trained on a large corpus of grounded image-text pairs from sources such as COCO, CC3M, CC12M, Visual Genome (Krishna et al., 2017), SBU, and LAION400M. The model is thoroughly evaluated on many tasks, such as compositional visual understanding, referring expression comprehension, VQA, and human-object interaction. The quantitative performance shows the benefit of improved communication between vision-language modules and outperforms considered VLM baselines on public benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The paper addresses an important topic for VLMs. Imbuing VLMs with compositional visual understanding is an important direction of research, and the proposed approach is an interesting mechanism to achieve it.\n+ The bidirectional communication between the vision and language modules is interesting and is an important component that needs to be introduced and explored in VLM research. Existing works do not have iterative communication, and relying only on global features for alignment/pre-training is insufficient to exhibit compositional understanding.\n+ The quantitative performance is strong and shows consistent gains over the considered baseline VLMs on several tasks and benchmarks.\n+ The paper is well-written and the idea is simple and presented intuitively, making it easy to follow and understand."
            },
            "weaknesses": {
                "value": "- My primary concern is about the pre-training data and how to interpret the results. The pre-training dataset consists of text-image pairs from COCO, CC3M, CC12M, Visual Genome (Krishna et al., 2017), SBU, and LAION400M. The evaluation datasets share considerable overlap with the pre-training data. For example, COLA is composed of data from GQA (derived from Visual Genome), CLEVR, and LVIS (also includes COCO). Similarly, ARO is based on Visual Genome, and HICO-DET is based on Flickr (with objects limited to those from COCO). I understand that the training data is essentially the same as BLIP-2 and that it is common for VLMs to be trained on these datasets. The question does remain: given this overlap, how well does the model generalize? The generalization can be quantified based on two factors - tasks and domains. From the task perspective, the generalization of the proposed approach seems to be limited, going by the performance of VQA. There are no convincing arguments for the generalization beyond training data. How about the performance on PACO[1]? It does not have a **significant** overlap with the pre-training datasets and should provide some measure of generalization.\n- There are very few qualitative results presented. There are 4 examples in the supplementary, but they are very limited beyond that. By reading the paper, it is hard to understand when/where/why the model fails. For example, is there a reason why the model has a higher performance on the \"rare\" class from HICO-DET? It would be good to understand the success and failure modes and have a discussion on the approach's qualitative performance as opposed to the purely quantitative take.\n- There are no ablations presented. What does the choice of encoders/pre-training data/tokens have on the model?\n- On that note, how sensitive is the model to the template used for prompting?\n\nReferences:\n[1] Ramanathan, Vignesh, et al. \"Paco: Parts and attributes of common objects.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "My major concerns are the generalization capabilities and the lack of qualitative results and ablation studies, detailed in the weaknesses section.\n\n--- Post-rebuttal update ---\nI am raising my score after the authors' response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6693/Reviewer_Aw71"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698682537511,
        "cdate": 1698682537511,
        "tmdate": 1700584612513,
        "mdate": 1700584612513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "scciMc6z1e",
        "forum": "PHGxChm1l5",
        "replyto": "PHGxChm1l5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_onDF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6693/Reviewer_onDF"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Compositional VLM, a new Large Vision-language Model that can compose visually grounded concepts and relationships within a given text input. It achieves this by employing a set of special tokens to manipulate the interactions between the LLM and a visual object detection network. This bi-directional communication of vision-to-language and language-to-vison occurs through multiple iterations as an output sentence is generated. The proposed model outperformed prior works across various compositional reasoning tasks and vision-language tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The object-centric approach presented in this paper is interesting as I believe visual objects and words are of the same level of abstraction of representation. I also appreciate the idea of multi-step interactions between an LLM and a visual object detection network, much like older multi-step reasoning techniques in the joint vision-language understanding literature.\n\n- The experiments demonstrate significant improvements over the existing works across a number of downstream tasks and benchmarks."
            },
            "weaknesses": {
                "value": "- The presentation of the method is brief, making it hard to understand. It is not clear how the communication tokens are generated. I recommend providing a pseudo algorithm to describe the method and making the input/output for each step more readable.\n\n- The proposed pre-training procedure seems to incorporate a lot of engineering tricks and data processing techniques from KOSMOS-2, without adequate attribution to previous works. The paper also lacks justifications for the selection of sub-components in the proposed method, making it hard to interpret the results. \n\n- Pretrained data and downstream tasks (i.e., RefCOCO/RefCOCO+, VQA v2) share the same pool of visual data taken from COCO and Visual Genome. This raises concerns about the validity of the reported performance on these downstream tasks."
            },
            "questions": {
                "value": "-\tWhy did you opt to create your own dataset of similar scale for pretraining, rather than utilizing the data from KOSMOS-2?\n-\tCould you provide further analyses of the contributions of these communication tokens?\n-\tOn the task REFERRING EXPRESSION COMPREHENSION: the proposed model outperforms the KOSMOS-2 model, but with a small gap. While the grounding of vision-language in KOSMOS-2 was reasonably good compared to the proposed model, it exhibited significantly poorer performance in the previous three tasks. Can you provide insights into these differences?\n\n-\tCan you clarify the large margins between KOSMOS-2 and the proposed method in Table 1? What would be the main contributor to this significant performance leap?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6693/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796804975,
        "cdate": 1698796804975,
        "tmdate": 1699636767838,
        "mdate": 1699636767838,
        "license": "CC BY 4.0",
        "version": 2
    }
]