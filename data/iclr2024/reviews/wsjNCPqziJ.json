[
    {
        "id": "sTNUqlAtlN",
        "forum": "wsjNCPqziJ",
        "replyto": "wsjNCPqziJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_G8pC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_G8pC"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates whether language models (LMs) learn to represent\nsemantics trained only on the next token prediction task. The LM has to\nsynthesize programs in a subset of the Karel educational programming languages\nto move a robot and place markers in a grid world. The semantics are a latent\ncausal variable in the data generation process.\n\nThe authors trained probes to predict the direction of the robot (and other\nsemantic content) from the LM states. The outputs are not given to the probe to\navoid leaking the semantics. There is a very strong correlation between the\naccuracy of the LM and the amount of semantic content the probes can extract.\n\nA recurring question in these probing experiments is whether the semantic\ncontent was learned by the LM or the probe. The authors propose an intervention\nstudy to isolate the contribution of the probe: they change the semantics by\nswitching the operators of the robot, and assess how well the probe can learn it\nfrom the LM states trained with the original semantics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is one of the firsts to present evidence that LMs learn and represent\nthe underlying semantics in spite of being trained on the next token prediction\ntask. A very interesting part is that the development of these representations\ncorrelates with the accuracy of the LM during training.\n\nThe paper also provides a causal inference framework that could help further\nstudy of semantics representations in LMs."
            },
            "weaknesses": {
                "value": "Part of the main contributions of the paper is that the internal representations\ncontain encodings of future program states. I was looking forward to reading\nabout this, but I couldn't find it in the paper.\n\nI couldn't find the full SCM in the Appendix (it was stated in the caption of\nFigure 1 that it would be there).\n\nI'm not sure that the paper presented a formal model of meaning acquisition in\nLMs as it states in Section 5. It did not give insights about how meaning is\nacquired: it demonstrated that semantic content is indeed acquired, but the\nnetwork remained a black box.\n\nI found the presentation of the problem and the hypotheses (a Main Hypothesis to\nbe rejected, and the Stronger and Weaker Hypotheses) a little bit\ncounterintutive. As no statistical tests were actually done I don't see the need\nfor formulating them this way (as it's the opposite of the thesis of the paper).\nLater the paper uses the positive hypothesis (the thesis), e.g., at the\nbeginning of Section 3: \"We investigate the hypothesis that representations of\nsemantics emerge...\".\n\nI think that this presentation made Section 4 hard to follow. Also, to me the\nmain question seems not whether LMs can learn semantic content at all (like in\nEq (1)), but how well they learn it, and whether that contributes to the\naccuracy (like in Figure 4).\n\nTwo limitations:\n- that although the Karel programming language has selection and\niteration, the presented examples don't have them: they are just sequences\n(\"straight line programs\").\n- the problem is relatively simple and the transformer used\nis small (350M parameters). I don't think that's a problem for a first\ninvestigation of these phenomena.\n\nSmall notes:\n- page 5: \"evaluate the output of the program on each input\": I guess the\n  program is evaluated on each input to obtain the output?\n- typo in Section 5: \"better is thus\""
            },
            "questions": {
                "value": "I don't understand the green causal link in Figure 1a. How is it different from\nthe other causal links? Why are there no such links for the test set and in\nFigure 1b?\n\nHow different are the generated programs from the reference programs?\n\nThere is a drop in Semantic Content on Figure 4a at around 70 000 training\nsteps. Have you found an explanation for this?\n\nWhy do we use and plot (on Figure 6) the adjusted semantic content and not just\nthe semantic content obtained using S'? Wouldn't showing that that's close to 0\na better elucidation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698669328985,
        "cdate": 1698669328985,
        "tmdate": 1699636906419,
        "mdate": 1699636906419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YC3Ki99Cr1",
        "forum": "wsjNCPqziJ",
        "replyto": "wsjNCPqziJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_gauF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_gauF"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the effect of the language model that is trained to predict the next token in a synthetic corpus of programs written in a specific domain-specific language. These programs are accompanied by textual input-output examples, which introduce the semantics of the programming language as a hidden variable. The researchers found that the trained model can extract abstractions of program states despite no explicit bias toward learning the language's semantics. \n\nThe study employs a causal framework to analyze probing effects and conducts experiments to attribute the probe's accuracy to the model's latent understanding of the language's semantics. The paper doesn't introduce new training techniques for language models but provides insights into how they acquire and represent semantics."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is carefully written and the motivation is well explained.\n- The authors conduct a thorough related-work study."
            },
            "weaknesses": {
                "value": "- The experiment is not enough. This paper only studies a small example of using a language model for learning latent causal variables.\n- The novelty of this work is not enough. This paper claims to propose a formal model of semantics acquisition in language modeling, which seems to be overclaimed and lacks sound verification in the rest of the paper. The authors might consider other more relevant conferences, like human-computer interaction.\n- The presented structural causal model is not the common model presented in the  Judea Pearl 2000 paper. Why it must be this format (shown in Figure 1)? Please provide a more solid definition."
            },
            "questions": {
                "value": "- The definition of \"semantics\", \"latent semantics\" and \"latent casual semantics\" has been discussed throughout history, the author should define those more carefully."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685754252,
        "cdate": 1698685754252,
        "tmdate": 1699636906260,
        "mdate": 1699636906260,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3dE5xWO5bz",
        "forum": "wsjNCPqziJ",
        "replyto": "wsjNCPqziJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_9VK4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_9VK4"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a general approach, grounded in Programming Languages (PL) Theory, for understanding the internal behavior and representations of Language Models (LMs). The stated goal of the paper is to be able to answer whether LMs learn underlying semantics of the domain or not. To conduct this investigation, the idea is to use the formal notion of semantics of programming languages. In particular, the authors train an LM to synthesize code in an educational domain-specific language (for navigating grid worlds) given input-output examples, and then design an experiment to probe if the LM \"understands\" programs using the notion of trace semantics of programming languages. Given a program, they check via probes if the state of the robot (a part of the program state) is correctly represented in the hidden states of the LMs as the LM processes each line of code. It indeed turns out that the correct state of the robot can be recovered from the hidden states. Moreover, a counterfactual analysis also suggests that it is not the probe itself that has learnt to encode the robot state. The overall hypothesis and experiment design are formalized using the framework of structural causal models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. It is an innovative and potentially fruitful idea to use semantics of PL for understanding behavior of LMs.\n\n2. Framing the questions in a causal language helps make the investigation and experimental design precise.\n\n3. The results from this paper confirm similar empirical findings from prior work and add to the growing body of evidence that LMs indeed learn something deeper about their domains beyond mere syntactic correlations.\n\n4. The finding that semantic understanding evolves in lockstep with LM ability to synthesize correct programs in interesting."
            },
            "weaknesses": {
                "value": "1. The paper presents interesting ideas but I find the claims to be over-stated. The connection between semantics of PLs and semantics of natural languages is neither formal nor obvious. While it may be reasonable to claim from the experiments that LMs learn some semantic notions of PLs, generalizing to say LMs learn semantics is not well-supported. Moreover past works such as [1] and [2] already provide evidence that LMs learn something deeper than syntactic structure, i.e., refute the main hypothesis, at least in a narrow sense.\n\n2. Although the use of counterfactual analysis to determine the effect of the probe is important, it seems like an interventional analysis as in [1], where the internal representations of the LMs are directly modified as guided by the probe, would provide even stronger evidence.\n\n3. I find the usage of semantics throughout the paper to be imprecise. For instance, what is the formal definition of the semantics map S? What does $semantics_{test}$ formally mean? I also found the presentation in Section 4 to be hand-wavy and hard to follow. For example, what does probability of $semantics_{test}$ mean? In Equation (3), what does it mean to estimate an inequality? Isn't the value of the left-hand side term in the inequality being estimated?\n\n[1] Li, K., Hopkins, A. K., Bau, D., Vi\u00e9gas, F., Pfister, H., & Wattenberg, M. (2022). Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382.\n\n[2] Nanda, N., Chan, L., Liberum, T., Smith, J., & Steinhardt, J. (2023). Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217."
            },
            "questions": {
                "value": "In additions to the questions in the **Weaknesses** section, I have the following questions:\n\n1.  Why is the distinction between probing hidden states vs activations (as in [1]) important?\n\n2. What is the precise nature of the probe? In particular, do you learn five separate probes, one for each input? Why not just give a single input instead of five? Also, are each of these five inputs further duplicated (as suggested by the definition of $input_{test}$)? If so, why? Is the input given to each probe the same, i.e., all the hidden states of the LM at the final layer? What accuracy is being reported in Figure 4? Is it the average accuracy of the five separate probes?\n\n[1] Li, K., Hopkins, A. K., Bau, D., Vi\u00e9gas, F., Pfister, H., & Wattenberg, M. (2022). Emergent world representations: Exploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Reviewer_9VK4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698714427197,
        "cdate": 1698714427197,
        "tmdate": 1699636906136,
        "mdate": 1699636906136,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oNM8mVGqJO",
        "forum": "wsjNCPqziJ",
        "replyto": "wsjNCPqziJ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_hPVi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7509/Reviewer_hPVi"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles a central problem in languages: the emergence of semantics from forms. To understand this, the authors use the task of program tracing as an example and demonstrate that LMs can learn semantics from a standard next-token prediction pretraining task, and further conduct probing analysis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall, I think the paper is tackling an undoubtedly important question about to what extent the model can learn semantics from training tasks. In addition, Despite being very concrete, the programming task that the authors proposed is very interesting, and I believe there is much more to exploit."
            },
            "weaknesses": {
                "value": "The manuscript requires significant revisions before it can be considered for acceptance. Specific areas of concern include:\n\n1. **Scope and Claims of the Study**: The current paper is more aptly described as a case study examining LMs' encoding of semantics and forms. The connection between LMs and their capacity to learn semantics is a broad topic, and the paper's current evidence is insufficient to support its overarching claims. To address this:\n  - Reframe the paper's conclusions to be more specific, grounded, and devoid of overgeneralizations (e.g., in the Abstract, Introduction, and Conclusion sections).\n  - Enhance the diversity and robustness of your evidence by introducing at least two more diverse examples.\n\n2. **Claims on Causality**: The assertion that LMs inherently understand causality is not sufficiently supported. While ML models can indeed identify latent semantics in supervised tasks\u2014a consensus in both image and language processing\u2014the challenge lies in distinguishing *causality* from mere *correlation*. To me, your experiments appear to train LMs to transition between states using text-encoded actions (programs), a concept already explored in domains like RL and robotics. If the claim is that LMs can learn causal semantics, the same could be said for many other algorithms. Please elucidate the nature of causality in your experiments and justify how your reasoning and experiments demonstrate causality.\n\n3. Probing Study: The probing analysis in your manuscript doesn't yield novel insights. Previous studies have already established that latent embeddings exist in LMs' representation space, and such embeddings can be harnessed in unsupervised ways to achieve commendable results (as evidenced by [1]. BTW, a comprehensive comparison with such works would be beneficial). Given this background, the paper should clarify the unique contributions and findings of the probing performance.\n\n[1] Discovering Latent Knowledge in Language Models Without Supervision"
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7509/Reviewer_hPVi"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7509/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734695356,
        "cdate": 1698734695356,
        "tmdate": 1699636906029,
        "mdate": 1699636906029,
        "license": "CC BY 4.0",
        "version": 2
    }
]