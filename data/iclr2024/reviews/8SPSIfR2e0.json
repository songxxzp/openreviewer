[
    {
        "id": "MTC399GUl7",
        "forum": "8SPSIfR2e0",
        "replyto": "8SPSIfR2e0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents the application of pruning techniques to analyze neuron behavior within large language models. Introducing a method termed \"selective pruning,\" the approach gauges the significance of each neuron based on its importance in both retained and forgotten datasets. From their experimental findings, the paper highlights that: (1) neurons exhibit high specialization, (2) larger models demonstrate more selective tendencies, and (3) feed-forward neurons show greater specialization compared to attention neurons."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper delves into an interesting topic not previously addressed: examining the behaviour of neurons in large language models to comprehend their functionality.\n2. The article introduces a method termed \"selective pruning\" to undertake machine unlearning on large language models, subsequently employing it to analyze neuronal functions.\n3. The research offers intriguing findings from its experiments, suggesting that neurons in FFN hold greater significance than in Attention module for specialized tasks. Such insights could potentially inspire further research and insights into large language models within the community."
            },
            "weaknesses": {
                "value": "1. The experimental results presented by the author do not fully support the conclusion this paper wants to draw. For example, the authors mentioned in the introduction that, \"If capabilities can be separated on the level of neurons, then this can lead to modularity inside models.\" However, based on the experimental results, it appears that the entire LLM behaves highly in coupling and cannot be separated. For instance, in Figure 1(d), the performance loss on the 'code' dataset seems to be mirrored closely by a performance loss on the 'python' dataset, where the model drops close to the retain dataset and the forget dataset.\n\n2. The paper lacks comprehensive and comparable comparisons with previous methods. The authors only show the results with one baseline method (Task Arithmetic), and the comparison does not seem equitable.  It's unclear from the presented data whether their approach outperforms the baseline method. Using varying scales for the reduction in perplexity complicates the evaluation,  and it's challenging to determine whether a reduction from 4.8 to 0.8 is significant, or if a drop from 2.2 to 0.3 is more significant.\n\n3. Given that the experiments were solely conducted on datasets related to code, I am uncertain about the generalizability of the experimental results presented in the paper. For instance, the conclusion that FFN outperforms attention\u2014might it be possible that a different task could yield an opposing conclusion.\n\n4. The readability of the entire article is not good. For instance, in Section 3.1, the author describes the distribution characteristics of the \"**attention pre-out neuron**\" activations. However, the definition of this unfamiliar term, \"attention pre-out neuron,\" is only introduced in Section 3.3. This leads to confusion for me when initially encountering the term. Additionally, the article frequently places experimental results in the appendices, referencing them in the main text and using the conclusion from these experiments in appendices to support further observation in the main text. This approach disrupts the reading flow, often requiring to flip back and forth for context. While thorough analyses and experiments are commendable, the structure of the article still needs further refinement to enhance its logical flow."
            },
            "questions": {
                "value": "1. Please answer the questions mentioned in Weaknesses.\n\n2.  In section 3.2: As a baseline we also randomly pruned layers. Where is this baseline?\n\n3. A prior study [1] demonstrated that, depending on the specific input, it's possible to achieve a high pruning ratio without negatively affecting performance and without the need for retraining. This suggests that there is redundancy in the neurons of the LLM when it's tasked with executing a singular function (equating a single sentence to a minor task, for instance). In light of these findings, what novel insights or observations does your paper offer in comparison to that study?\n\n[1] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5824/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_Q3q4",
                    "ICLR.cc/2024/Conference/Submission5824/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698658016232,
        "cdate": 1698658016232,
        "tmdate": 1700547127595,
        "mdate": 1700547127595,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5V3JTm28ff",
        "forum": "8SPSIfR2e0",
        "replyto": "8SPSIfR2e0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_sDUD"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an unlearning algorithm based on pruning for removing \u2018capabilities\u2019 from pretrained language models. Specifically, they provide different definitions for quantifying the importance of a neuron for a particular dataset based on the value of its activation on that the points in that dataset. Then, they define a \u2018score\u2019 for a neuron as the ratio of importance of that neuron for the retain versus the forget datasets. They prune a chosen percentage of nodes from a ranked list according to this score. Empirically, they experiment in a setting where a general purpose model is caused to forget its \u2018coding ability\u2019 (and the reverse of forgetting all but its coding ability). They also look at a more finegrained scenario of removing python coding ability while retaining the ability to code in other languages (and its reverse). The way that forgetting is measured is by inspecting the relative drop in accuracy on the forget set (relative to the relative drop on the retain set). My understanding is that it is desired according to this evaluation to have a large drop in the forget accuracy without having a large drop in the retain accuracy. They investigate empirically these trade-off curves obtained by their method in different types of language models. Then, on a different task/dataset (where the goal is to remove toxicity from a trained language model), they compare against one baseline and claim that they get similar results."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the paper studies an interesting and important problem\n- the proposed method is a reasonable idea and well motivated\n- the proposed method is efficient and requires no gradient updates \n- the paper is for the most part well-written (though see some exceptions below)"
            },
            "weaknesses": {
                "value": "- the related work section is weak, missing a lot of literature both from unlearning and pruning. For example, [A-E] are recent papers on unlearning, with [A, B] particularly related to the methodology of this paper (see References below). I\u2019m less familiar with the sparsity and pruning literature but the authors should conduct a thorough review.\n- the authors compare against only one baseline, and only for one task. Other common baselines include finetuning on the retain set, gradient ascent on the forget set, and comparing against other recent unlearning methods is also important (see the references below)\n- the particular problem setting of unlearning is not clearly defined. What defines successful unlearning here? In section 3 the authors describe the forget set as the \u201ctask or dataset that we aim to reduce performance on\u201d. This isn\u2019t precise enough. How much do we want to reduce performance? Is it that the greater the reduction, the better? For context, unlearning papers (e.g. Golatkar et al, which the authors cite) usually consider that the accuracy on the forget set should be reduced only up to a reference point and no further (where the reference point is given by the oracle unlearning algorithm of retraining the model from scratch without the forget set). Is the setup here different, and if so, how is the goal defined in this case?\n- [clarity] It seems that \u2018task\u2019 and \u2018dataset\u2019 are used interchangeably in the paper which causes confusion. For example, \u2018the task that we are optimizing for as the retain dataset\u2019. To me, a \u2018task\u2019 includes a particular training objective whereas \u2018dataset\u2019 refers to raw data.\n- [clarity] fundamental problem setting details are missing. For example, were Pile and Code included in the dataset that the various language models were trained on? If not, the terms \u201cforgetting\u201d or \u201cunlearning\u201d may be ill-suited for this application (as they usually refer to forget parts of the training dataset). At the very least, the problem setting targeted by this paper should be clearly defined. \n- [soundness]: can decreased performance (accuracy, perplexity) on a particular dataset support claims of removal of a capability? Generally, \u2018capability removal\u2019 is not precisely defined.\n- [soundness]: when it comes to forgetting or unlearning, several metrics have been proposed by the community to measure this. Simply inspecting the accuracy / perplexity is likely a poor proxy for forgetting quality. For example, Membership Inference Attacks are an important category of methods (see e.g. Golatkar et al and reference [E] below). Are these not applicable here. If not then why not?\n- [presentation, soundness] the authors use the term \u201cselective\u201d to describe an unlearning method, without defining this term clearly in this context. My understanding of what \u201cselective\u201d means in this context is the ability of reducing accuracy / performance on the forget dataset without (really) damaging the accuracy / performance on the retain dataset. If my understanding is correct, I don\u2019t agree with the claim that Figure 1a shows that the larger the model, the more selective it is. I can see this being true for the Opt family but not the Pythia family, for example.\n- [presentation] In the paragraph under Definition 1, the authors give intuition for the different influence functions but they omit I_{abs}. Please add.\n- [presentation] Above Table 1, the authors describe the models they use (which correspond to columns in Table 1) but they omit Roberta. Please add.\n- [presentation] \u201cAs a baseline, we also randomly pruned layers\u201d \u2013 the authors claim this but I don\u2019t see this baseline in their experiments (at least in the main paper), unless I\u2019m missing it. \n- [empirical results] It would greatly strengthen the paper to conduct analyses of: the effect of the number of pruning iterations, the effect of the % pruned (in each iteration or overall), the effect of the choice of the influence function. It sounds like the authors have some results on some of these in the Appendix but not all of them. It would also be great to summarize in the main paper all of the findings (so that one doesn\u2019t need to read the entire Appendix). \n- [empirical results] please include confidence intervals in all tables and e.g. in Figure 3. It is currently challenging to tell if the differences are significant\n- [empirical results] why is there such a large difference between \u201cBase (quoted)\u201d and \u201cBase (replicated)\u201d in Table 3? This makes me concerned about whether \u201cTask Arithmetic (quoted)\u201d and \u201cPruned\u201d are comparable.\n\nReferences\n=========\n- [A] Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening. Foster et al 2023.\n- [B] Model Sparsity Can Simplify Machine Unlearning. Jia et al. NeurIPS 2023.\n- [C] Unrolling SGD: Understanding Factors Influencing Machine Unlearning. Thudi et al.\n- [D] Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization. Zhang et al. NeurIPS 2022.\n- [E] Towards Unbounded Machine Unlearning. Kurmanji et al. NeurIPS 2023."
            },
            "questions": {
                "value": "- the authors claim that their method is specifically designed for LLMs but it\u2019s unclear to me why that\u2019s the case. Can\u2019t this method be applied out of the box e.g. to vision transformers? If not, then why not?\n- In Section 3.1, the authors discuss some observations about the distributions of activations that motivated their use of importance functions. Which dataset / setting were these observations made in? Have the authors made an effort to confirm that they are generalizable beyond a certain setting?\n- I don\u2019t really understand the statement that \u201cWe also do not want to directly modify specific dimensions of the embedding space\u201d. Did not understand the provided rationale. If it\u2019s not the right level of granularity for pruning, as the authors claim, would the proposed scoring function not capture this? If not, does this suggest we need to design better scoring functions?\n- In the Discussion, the authors hypothesize that their method is more likely to \u201cactually remove the undesired behaviour\u201d compared to other unlearning methods. Similarly, in the Broader Impacts section, they claim that (compared to other unlearning methods) their method is unlikely to generate systems that are more harmful than the base model. What is the evidence used for making these claims?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676156164,
        "cdate": 1698676156164,
        "tmdate": 1699636614546,
        "mdate": 1699636614546,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z9KsZ0NjmC",
        "forum": "8SPSIfR2e0",
        "replyto": "8SPSIfR2e0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
        ],
        "content": {
            "summary": {
                "value": "This work presents a novel selective pruning method in order to allow trained models to 'unlearn' specific capabilities. Related work and the details of the method are explicated. Experiments on different models with different sizes are presented on two data splits (code/pile and code/python) showing generally good performance at 'unlearning' the forgetting dataset and retaining the other. Analysis experiments compare the efficacy of pruning feed-forwards vs attention neurons. Experiments on toxicity show good results for the method on allowing the unlearning of toxic text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is clearly written. The experiments are thorough, and the presented results convincingly demonstrate the utility of the method. The results on toxicity are particularly nice, as this is a highly relevant domain for which related techniques are well-motivated. The presented method is novel."
            },
            "weaknesses": {
                "value": "In Limitations: \"Our method can only be applied to remove a capability when that capability is neatly captured by a\ndataset.\" The authors rightly point out that this method of evaluation of unle'arning is dependent upon dataset-level perplexity. The extent to which this metric for any constructed dataset sufficiently \"neatly captures\" whatever capability is desired to be forgotten is difficult to asses without further analysis not presented in this work. It may be the case that for the practical scenarios which motivate the method in the first place, no such \"neat\" dataset is possible to produce. It is fine for this analysis to be out off scope of this work.\n\nGiven this evaluative dependence upon specific datasets, it would significantly strengthen the results of the paper to present more diverse experiments. While it is nice to have many different models at many different sizes, there does not seem to be much addition knowledge gleaned from that diversity, whereas having more tasks would demonstrate a broader efficacy across a more speculative dimension. \n\nIn this paper, the authors present experiments pruning either the feed-forward or the attention blocks, and leave \"Embedding, Positional\nEmbedding and Output Unembedding unmodified.\" (3.3). This is a significant constriction of the application of the method which is not more than intuitively justified. Further experiments, even just to show that this restriction is well-motivated, would strengthen the work."
            },
            "questions": {
                "value": "See weaknesses.\n\nIn Table 3: it would help readability to label \"Task Arithmetic (quoted)\" as being a finetuning task. Also, the gap between the baseline (quoted) and (replicated) is fairly large, which makes actually comparing the finetuning vs presented pruning method difficuly. Would it be possible to replicate the task-arithmetic finetuning? That would significantly improve the comparability of these results.\n\n\nsmall issue:\nThe details of iterative pruning are not specified. It seems like it would be important to the function of the method to set the proportion of nodes pruned per iteration well, but this is not discussed. Even if this is not and important hyper-parameter of the method, a clearer explanation of the iterative pruning method is necessary to fully elucidate the applied method.\n\n6.3: If an LLM were trained not to answer questions about a dangerous topic, say, bomb-building, could the presented method not be used to unlearn that guardrail? I don't think this is a concern specific to the presented method, but I do not follow why this method is less likely to generate harmful systems than other methods. Can the authors clarify?\n\nNit:\nin discussion: hypothesise -> hypothesize"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5824/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5824/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5824/Reviewer_9FMg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822401760,
        "cdate": 1698822401760,
        "tmdate": 1699636614429,
        "mdate": 1699636614429,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w8ESNORUvA",
        "forum": "8SPSIfR2e0",
        "replyto": "8SPSIfR2e0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_q42J"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5824/Reviewer_q42J"
        ],
        "content": {
            "summary": {
                "value": "This article presented an approach for the targeted removal of neurons, which is based on their comparative significance across two datasets. This technique of machine unlearning demonstrates its effectiveness through the quantifiable decrease in accuracy differentials and perplexity measurements. Additionally, it establishes a cost-effective foundation for forthcoming research comparisons. Its theory posits that the approach is more inclined to eliminate undesired model behaviors, as opposed to merely concealing them, in contrast to fine-tuning.\nThis approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. The findings of this method reveals that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1- The model presented here tackles an intriguing and complex issue within large language models (LLMs), focusing on the significance scores assigned to individual neurons with respect to a specific target dataset.\n\n2- The concept of machine unlearning is implemented across both types of neural networks, including feedforward and attention-layer-based networks. By eliminating unwanted neurons for any given target dataset, the process is swift and leads to a reduction in the network's computational load.\n\n3- The efficacy of the unlearning process is demonstrated through experiments conducted on three distinct datasets."
            },
            "weaknesses": {
                "value": "1- The proposed model can effectively eliminate the information captured by the target dataset. However, it is unable to unlearn knowledge that lies beyond the representation of the datasets. Please provide a clear justification.\n\n2- Including a visual representation of the suggested concept could enhance comprehension. Thus, kindly incorporate a diagram that presents a general outline of the proposed concept.\n\n3- The evaluation of the proposed model focuses on text datasets, yet the experiments for the image dataset are absent. Implementing machine unlearning for the image dataset would be highly beneficial."
            },
            "questions": {
                "value": "Please address all the question raised in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5824/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698865285067,
        "cdate": 1698865285067,
        "tmdate": 1699636614299,
        "mdate": 1699636614299,
        "license": "CC BY 4.0",
        "version": 2
    }
]