[
    {
        "id": "5CJ2Mv7qVL",
        "forum": "OZitfSXpdT",
        "replyto": "OZitfSXpdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission288/Reviewer_aFDp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission288/Reviewer_aFDp"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an adaptive method for learning a sample-wise knowledge fusion ratio. Specifically, the proposed method captures intra-sample and inter-sample trilateral geometric relations among the student prediction, teacher prediction, and ground truth. A simple network further learns the implicit mapping from the intra- and inter-sample relations to the fusion ratios in a bilevel-optimization manner. The proposed method is validated on three various tasks: image classification on CIFAR-100 and ImageNet, attack detection on HIL, and CTR prediction on Criteo."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe paper is well motivated by clearly illustrating the valuable insights of incorporating additional ST prediction discrepancy for determining the fusion ratio.  \n2.\tAlthough KD methods have been widely studied, the investigation of the knowledge learning potential on different samples has received less attention. With the learned sample-wise fusion ratio, this paper showcases that the student and teacher can provide specific knowledge learning potential for each sample based on the ST prediction discrepancy and the teacher\u2019s correctness. \n3.\tThis paper proposes a novel and interesting method to capture relations among the student prediction, teacher prediction, and ground truth by exploiting their trilateral geometry at both intra- and inter-sample levels. \n4.\tThe experiments and analysis on three different tasks are comprehensive and demonstrate the effectiveness of the proposed method. \n5.\tThe paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "The paper is overall readable and proposes an innovative idea. However, there are some weaknesses that the authors can improve. \n\n1.\tThe experimental results show improved performance on the HIL and Criteo datasets. However, the paper does not clarify if this is due to the adopted sampling method. The oversampling of the minority class as a preprocessing step is mentioned, but its direct influence is not explored.\n\n2.\tIn Section 4.3, this paper analyzes the effectiveness of incorporating ST by comparing the learned fusion ratio distributions across four scenarios. With ST, the proposed TGeo-KD reduces the fusion ratio on L_kd, thereby encouraging the student to acquire more knowledge from ground truths when the teacher predictions are incorrect on samples (e.g., outliers). However, more experiments should be conducted to investigate that the exploitation of inter-sample relations on these incorrectly predicted samples does not contribute significantly to decreased fusion ratios. \n\n3.\tThe fusion ratio is learned through a neural network, which is unclear to explain how the learning process unfolds, such as which samples are assigned larger/smaller fusion ratios."
            },
            "questions": {
                "value": "1.\tWhy do the authors choose to model the relation in Euclidean space? \n2.\tGiven that this paper proposes an adaptive learning method for sample-wise fusion ratio, does the proposed method demonstrate consistent performance across different numbers and categories of samples?\n3.\tIs there a specific reason to formulate this work into bilevel optimization? Will it lead to less efficient training? The motivation is note quite clear to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission288/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission288/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission288/Reviewer_aFDp"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698401168979,
        "cdate": 1698401168979,
        "tmdate": 1700665617776,
        "mdate": 1700665617776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vijrgPDPUB",
        "forum": "OZitfSXpdT",
        "replyto": "OZitfSXpdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission288/Reviewer_iMjj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission288/Reviewer_iMjj"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on data-wise adaptive knowledge fusion ratio in knowledge distillation. They find that the discrepancy of the predictions from teacher and student plays an important role, and they design a novel KD method based on this. Experiments show the effectiveness of their proposed method."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- They propose a novel KD method to exploit the discrepancy of the prediction logits from S and T to adaptively learn the knowledge fusion rate, offering an valuable view for KD.\n- The experiments are solid and achieve good performance.\n- More significantly, the proposed sample-wise operation introduces little additional training time, making this method useful in practice."
            },
            "weaknesses": {
                "value": "- How do the outliers hurt KD? As a key motivation, the corresponding empirical evidence seems lacking.\n- In addition, the teacher model indeed may underperform on the outliers. According to previous discussion, directly decreasing the $\\alpha$ also can solve this issue. Thus, why to introduce the inter-sample relations? This motivation needs to be further claified.\n- Evaluating this method on real OOD datasets, like DomainNet, will be more convicing."
            },
            "questions": {
                "value": "- As far as I know, the computation of similarities of class probability distribution vectors often adopts the cosine similarity. Why do they use the Euclidean distance here? Are there some advantages over cosine similarity?\n- As they state that Eq. 4 is an implicit function of \u03c9 as \u03b8* depends on \u03c9, how to perform alternately optimization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698490090559,
        "cdate": 1698490090559,
        "tmdate": 1699635954874,
        "mdate": 1699635954874,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3uCcfEcuFS",
        "forum": "OZitfSXpdT",
        "replyto": "OZitfSXpdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission288/Reviewer_UyVS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission288/Reviewer_UyVS"
        ],
        "content": {
            "summary": {
                "value": "This paper studies a less explored topic in Knowledge Distillation that of finding the optimal parameter/weight alpha for combining the distillation loss with the standard cross entropy loss when training the student. The authors first motivate the impact of appropriately learning the weight parameter with a well-designed experiment and then they propose a practical method to estimate it using a small network that uses as input features geometric features computed from the teacher's and student's logit and the ground truth one-hot vectors. They show good results on a variety of standard KD benchmarks"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Paper is well-written and motivating example makes sense\n2. Problem tackled is not well-studied and the solution proposed is simple and effective\n3. Results carried out on standard benchmarks show that the method is effective on these benchmarks"
            },
            "weaknesses": {
                "value": "1. My main concern is related to the impact of the proposed solution. Like most distillation papers conclusions are drawn based on CIFAR-100 experiments using same network architecture. Claims made by the authors could substantiate better if they are shown to hold on bigger datasets like ImageNet.\n2. Tasks other than classification should be considered (e.g. detection). Experiments on HIL and Criteo datasets are of low impact \n3. Important references/comparison with SOTA is insufficient. Authors should have included comparisons with Decoupled Knowledge Distillation, CVPR'22 and KNOWLEDGE DISTILLATION VIA SOFTMAX REGRESSION REPRESENTATION LEARNING, ICLR'21. From my preliminary checking, the improvements of the proposed method are not always so significant if the aforementioned methods are considered."
            },
            "questions": {
                "value": "Inter sample relations are used to model outliers. But how do you define outliers in the datasets you're using for your experiments (i.e. imagenet and cifar)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698497228638,
        "cdate": 1698497228638,
        "tmdate": 1699635954787,
        "mdate": 1699635954787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SRPIb4DiMi",
        "forum": "OZitfSXpdT",
        "replyto": "OZitfSXpdT",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission288/Reviewer_Ur13"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission288/Reviewer_Ur13"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the concept of knowledge distillation (KD) and introduces a new method called TGeo-KD for determining sample-wise knowledge fusion ratios in KD. The traditional approach to determining fusion ratios in KD involves using a fixed value for all samples or gradually decreasing the ratio. The authors argue that considering the discrepancy between the student's and teacher's predictions (ST discrepancy) is crucial in determining the fusion ratio. They propose TGeo-KD, which leverages the trilateral geometry among the signals from the student, teacher, and ground truth to model the fusion process. Experiments conducted on CIFAR-100 dataset demonstrate the effectiveness of TGeo-KD compared to other methods. The main contributions of TGeo-KD include its use of trilateral geometry, mitigation of outliers, and superior performance across different tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper introduces a novel method called TGeo-KD for determining sample-wise knowledge fusion ratios in knowledge distillation. This method leverages trilateral geometry and captures the geometric relations between the signals from the student, teacher, and ground truth. By introducing this new method, the article contributes to the existing body of research in knowledge distillation.\n2.\tThe article emphasizes that TGeo-KD is versatile and adaptable to different architectures and model sizes. This implies that the proposed method can be applied to a wide range of machine learning tasks and scenarios. The ability to apply TGeo-KD to different models and architectures enhances its practical utility and makes it a valuable contribution to the field of knowledge distillation.\n3.\tTGeo-KD leverages trilateral geometry at both intra-sample and inter-sample levels, which helps in mitigating the impact of outliers in the training samples. Outliers are samples that deviate significantly from the majority of the dataset and can negatively affect the learning process. By incorporating the trilateral geometry and considering the ST discrepancy, TGeo-KD provides a robust approach that is less sensitive to outliers, resulting in more stable and reliable knowledge transfer."
            },
            "weaknesses": {
                "value": "Complexity and Computational Cost: The TGeo-KD method proposed in the article leverages trilateral geometry and introduces a neural network to learn the fusion ratios. This suggests that the method may have a higher computational cost compared to simpler knowledge distillation techniques. Implementing and training the neural network for determining fusion ratios may require additional computational resources and time, which could be a potential drawback, especially in resource-constrained environments.\n\nThe proposed TGeo-KD leverages trilateral geometry and captures the geometric relations between the signals from the student, teacher, and ground truth. However, I think it lacks more sufficient theoretical proof."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission288/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980836920,
        "cdate": 1698980836920,
        "tmdate": 1699635954651,
        "mdate": 1699635954651,
        "license": "CC BY 4.0",
        "version": 2
    }
]