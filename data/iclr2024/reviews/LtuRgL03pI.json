[
    {
        "id": "US9lD1Mozz",
        "forum": "LtuRgL03pI",
        "replyto": "LtuRgL03pI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_rQD7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_rQD7"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes InstructScene, which does 3D indoor scene synthesis with semantic graph prior. It first quantizes CLIP features using a transformer with learnable tokens, use a frozen text encoder and graph transformer generate the semantic graph, and finally uses a graph transformer to decoder the final layout. Experiments are done on 3D-Front following the prior works although instructions are re-generated. With the semantic graph prior,  InstructScene outperforms state-of-the-art methods significantly on controllability. It also shows zero-shot generalization to a few other tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem is meaningful. Improving controllability critical to 3D scene generation, as we have already seen a lot of 3D scene generation models.\n- The method is well-designed. \n- Experiments are extensive. \n- The paper is very well-written and easy to follow. It provides enough mathematical details, as well as comprehensive qualitative results. I enjoy reading the paper."
            },
            "weaknesses": {
                "value": "InstructScene is proposed to handle natural language instructions. However, the instruction does not seem to support arbitrary instructions. In qualitative results, the instruction always explicitly mentions one of these relationships. \n\n>  (Section 5.2) we use a metric named \u201cinstruction recall\u201d (iRecall), which quantifies the proportion of the required triplets \u201c(subject, relation, object)\u201d occurring in synthesized scenes to all provided in instructions\n\nIt's not clear to me how iRecall is computed. To the best of my understanding, iRecall computes the recall, where the prediction is generated scene graph triplets and the ground truth is the instruction triplets. However, how do you get instruction triplets? Let's say the instruction is \"Add a corner side table with a round top to the left of a black and silver pendant lamp with lights\".  It looks non-trivial to extract triplets. Or do you use relationship keywords to extract?\n\nAdditional comments:\n- Table 4: bebind -> behind"
            },
            "questions": {
                "value": "- Can you confirm how iRecall is computed?\n- Can you comment on what will happen if we give \"arbitrary instructions\" (11 pre-defined relationships are not available)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1782/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1782/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1782/Reviewer_rQD7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698561518135,
        "cdate": 1698561518135,
        "tmdate": 1699636107750,
        "mdate": 1699636107750,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ptncrMPgDW",
        "forum": "LtuRgL03pI",
        "replyto": "LtuRgL03pI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_qrjz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_qrjz"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a semantic graph prior plus a layout decoder system to generate indoor scenes based on natural language instructions. Instruction-scene pairs are generated as a new dataset for the task."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper collects a dataset that supplements existing indoor scene datasets with natural language instructions.\n- The paper proposes a two-stage method that is shown to be more effective than previous methods in quantitative and qualitative evaluations. \n- The proposed method demonstrates some abilities in zero-shot indoor scene editing."
            },
            "weaknesses": {
                "value": "- I am concerned about the novelty of the proposed method. The semantic graph prior is a combination of existing VQ-VAE and discrete diffusion models. The layout decoder is a plain diffusion model. \n- Some claims or arguments about using VQ-VAE are not convincing and require further substantiation. The authors claim that VQ-VAE is used because objects share common characteristics like colors or materials. But I don't see why OpenCLIP features cannot encode these characteristics. And it's not well grounded to say that pre-trained OpenCLIP features are too complicated to model. As the author also mentioned the efficiency of discrete tokens, it's better to write the motivation clearly for using VQ-VAE. \n- The writing could be improved:\n    - the diffusion preliminary section is too abbreviated to serve as a separate section and is disjoined from Sec. 4.2.2 where the paper starts to relate to diffusion models. Then the semantic graph diffusion model employs a discrete diffusion model, that was proposed in previous work (Austin, et al., 2021). It seems that a large portion of Sec. 4.2.2 is not novel and could be treated as a preliminary study as well. \n    - The annotation is overcomplicated and hard to follow. There are new definitions of previous variables throughout the method section, like $c, f$ in the first paragraph of Sec. 4.2, $\\mathcal{C}_i$ and $\\mathcal{F}_i$ in the first two lines of Sec. 4.2.2. $f$VQ-VAE is not a standard notation. There are also variables with omitted super/sub-scripts that are not explained clearly.\n\nSome of the technical concerns might be entangled with writing issues of the current version. I am open to adjusting the rating if my concerns are resolved later."
            },
            "questions": {
                "value": "- What is the CLIP-aligned feature in Fig. 2? How is this feature vector applied to the VQ-VAE encoder?\n\n- Though the relations are view-dependent, the generated instructions do not describe the viewpoint of the camera. Is there an assumed viewpoint for all scenes? \n\n- Minor: in Fig. 5 row 4, the proposed methods generate a single bed while the instruction mentions a double bed. \n\nMissing reference: LayoutGPT: Compositional Visual Planning and Generation with Large Language Models. Neurips 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1782/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1782/Reviewer_qrjz",
                    "ICLR.cc/2024/Conference/Submission1782/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698788080417,
        "cdate": 1698788080417,
        "tmdate": 1700694410385,
        "mdate": 1700694410385,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "64I8zGq3nt",
        "forum": "LtuRgL03pI",
        "replyto": "LtuRgL03pI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_jsgt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_jsgt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a new scene generation framework with two stages. The first stage models the semantic graph, which consists of object categories, relations and appearances. With feature quantization, all these attributes become discrete variables, which can be learned through a discrete diffusion model. Conditioned on the semantic graph prior, the second stage uses a continuous diffusion model to learn the remaining scene attributes (rotations, translations and sizes). By modeling the semantic graph separately, the method enables various zero-shot downstream tasks. Experiments show that the proposed method achieves better scene generation results. The paper also creates a new dataset of text-scene pair for instruction-driven 3D scene synthesis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- One main challenge of scene generation is the discrete and continuous nature of scene representation. The paper provides a new scene generation pipeline by disentangling the generation of discrete attributes and continuous attributes. Though the architecture designs of both the discrete and continuous diffusion model largely follow existing methods, the new formulation of the scene generation pipeline is novel. In addition, the disentanglement enables various zero-shot downstream tasks, improving the controllability of the generative model.\n- The newly proposed dataset of scene-instruction pairs is a timely contribution that aligns well with the ongoing research of scene generation. \n- The paper is clear and well-written, both detailed quantitative and qualitative results are provided."
            },
            "weaknesses": {
                "value": "Generally I think the paper is good. There are some questions in the part Questions."
            },
            "questions": {
                "value": "- More details about the dataset creation can be provided. It would be better if the author can provide some examples of the training data.\n- In the semantic graph, relation e_{jk} can be determined by e_{kj}. Are the both directions of the instructions needed to be created? If so, how to determine which direction to use during training? \n- What is the runtime of the proposed method compared to baselines?\n- The instructions are generated mainly from rules. Do you think these rules can cover the majority of the instructions compared to a human-annotated dataset? What are the limitations of the instructions in the current dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698935885823,
        "cdate": 1698935885823,
        "tmdate": 1699636107582,
        "mdate": 1699636107582,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s3FJwXQjsQ",
        "forum": "LtuRgL03pI",
        "replyto": "LtuRgL03pI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_Peja"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1782/Reviewer_Peja"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework that generates 3D indoor scenes from natural language prompts. The framework takes two stages. The first stage generates a scene graph from the language prompt. The scene graph contains category and vector quantized feature representation of objects and relations (as directed edges) between them. The second stage generates the scene layout from the scene graph, predicting the location, size and orientation of the objects. Both stage are learned with transformer-based diffusion models. Various techniques are adopted to allow using diffusion to learn discrete attributes and graphs. \nEvaluation show that the proposed method outperforms previous state of the art methods (on slightly different tasks) convincingly. Qualitative examples appear to be of good quality as well."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Very solid design choices. The authors clearly have good knowledge of techniques relevant to this problem, and the proposed solution make a lot of sense to me e.g. discretizing object features with VQVAE, discrete diffusion models, the use of MASK for diffusion-based graph generation, various learning tricks, etc.\n- Good effort in creating a dataset suitable for this task (as existing scene datasets lack language descriptions). The pipeline for creating such text descriptions synthetically is reasonable.\n- Clearly outperforms prior SOTAs over many metrics commonly used for this task. \n- Qualitative results look good --- matches my impression of gt 3DFRONT scenes. Samples provided in the supp are realistic - exhibits many common minor issues, showing that they are clearly not cherry picked results (which is important considering the questionable reproducibility of many prior works)."
            },
            "weaknesses": {
                "value": "- More of a system paper: combines many ideas from previous works on 3D scene generation and works on diffusion-based generation model. Seems that there is not too much truly new ideas here (and clarifications on where these happens would be great). I'm fine with it, to be honest, because this paper ticks my checkboxes for what a good system paper should be: clever/novel combination of ideas/components, solid execution, exceptional results. However, this is technically still a \"weakness\" I guess, as it limits the further applicability of the proposed method in other domains.\n- Questionable choice of baselines. I get that previous graph-based/text-based works are not trained on 3D-FRONT, but it should be possible to compare with them in some capacities, or at least discussing the issues with them in the evaluation section,\n- Despite the good discussions of issues of the way prior works handle latent object features (my impression was that they don't help much), I am left unconvinced that the proposed method actually make use of such features e.g. 1. would manually changing semantic feature f of an object result in notable difference in the final scene? 2. Can the model pick up stylistic information from the training data? Do objects that frequently occur together in the training data behave similarly in the generated scenes? 3. Is there any noticeable degrade in scene layout quality if semantic features are not encoded (i.e. just sample CAD models by category and size, which is a common practice for prior works) or if they are encoded with a smaller feature space?\n- A user study would have been useful as it seems that humans, although not always consistent at rating the quality of the scenes, would be very good at checking if the generated scenes matches the text description.\n- More discussions on how the qualitatively the proposed method works better than prior works would be very helpful. I can clearly that this is the case from the supplemental figures, and it would be great to highlight this in the main paper as well with a paragraph or so, as the proposed method clearly avoids many issues with prior works, especially on larger rooms. The set of quantitative metrics, while comprehensive, are known to be not the most sensitive metrics when it comes to evaluating indoor scenes, especially when it comes to layout/appearance details. More in depth analysis of the qualitative examples would make it a much stronger case that the proposed method is clearly superior.\n- Lack of examples on how the proposed method can generalize & generate a diverse set of rooms from a single prompt. I think there can be many valid graphs for a single prompt and many valid layouts given a fixed semantic graph. It would be great to show examples that the proposed method can indeed achieve this.\n- Most of the instructions are very specific, which makes sense consider how they are generated. However, in real applications, fuzzier instructions are also very important e.g. a room in the style of {certain style}, a room for {certain type of occupants}, a room that supports {certain functionalities}. Discussion on how to incorporating these instructions would be good."
            },
            "questions": {
                "value": "I think this is a good paper: it is clearly very solid technically, allows useful applications, and clearly outperforms prior works. I don't think there is anything that I need to see in the author responses. However, as state in the weakness section above, I do have issues here and there with the generalizability with the method, and with how the evaluations are carried out. If the authors could provide more insight regarding these concerns, then I would be much more inclined to champion this paper (which I currently am hesitant to do)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1782/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699429563002,
        "cdate": 1699429563002,
        "tmdate": 1699636107503,
        "mdate": 1699636107503,
        "license": "CC BY 4.0",
        "version": 2
    }
]