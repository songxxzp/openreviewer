[
    {
        "id": "tvP5b3Ier7",
        "forum": "bXLOOoR2ft",
        "replyto": "bXLOOoR2ft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_jbqG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_jbqG"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces \"DoraemonGPT\", an LLM-based system tailored for video question-answering. DoraemonGPT utilizes different pretrained expert models to extract various video information and convert it into texts that can be understood by LLM. DoraemonGPT saves this information into an external symbolic memory module with a space-dominant (SDM) component and a time-dominant (TDM) component. \nIn addition, DoraemonGPT relies on LLM to decompose a task into subtasks. It defines a set of subtask tools to solve subtasks. \nThe research further explores the role of the MCTS planner in searching for the best subtask decomposition.\nExperimental results show that DoraemonGPT can outperform other LLM-based systems like ViperGPT and VideoChat on the video QA dataset NExT QA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed symbolic memory system and the MCTS planner on video tasks are new.\n- The paper is easy to understand.\n- Experiments show that it can outperform other LLM-based systems like ViperGPT."
            },
            "weaknesses": {
                "value": "- The evaluation set is small. The paper only conducted experiments on a subset of the original NExT QA dataset. In addition, for the ablation study, the system was evaluated on 3 question types, each with 10 questions only. \n- The authors mentioned that the small size of the evaluation is caused by the budget limit. This might suggest that the method is expensive. I think the paper should include a discussion about how many tokens the system consumes, and how much this system costs to answer a question on average."
            },
            "questions": {
                "value": "- How long does the inference take to answer a question on average? Given the complexity of the methodology, this information is important for users."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Reviewer_jbqG"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698326282543,
        "cdate": 1698326282543,
        "tmdate": 1699636121324,
        "mdate": 1699636121324,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "STOauGSLyJ",
        "forum": "bXLOOoR2ft",
        "replyto": "bXLOOoR2ft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_KysJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_KysJ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DoraemonGPT, an LLM-based system to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT first converts the input video into a symbolic memory for spatial-temporal reasoning by sub-task tools. The authors then incorporate plug-and-play tools to assess external knowledge and address tasks across different domains. Finally, an LLM-driven planner based on MCTS is used to explore the large planning space for scheduling various tools. DoraemonGPT\u2019s effectiveness and reasoning capabilities is demonstrated in one dataset, i.e., NExT-QA.\n\n---\nThe authors' feedback addresses most of my concerns, I increased my score (also considering other reviewers' comments)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clear presentation. The reviewer can easily follow most of the paper, especially the methods and experiments.  \n\n2. Novel idea that MCTS is used to explore the large planning space for scheduling various tools.  \n\n3. Good performance on the evaluated benchmarks with strong baselines."
            },
            "weaknesses": {
                "value": "1. Overclaim. With a performance of about 50% acc on only one test dataset, the authors claim \"toward Solving Real-world Tasks\", which is a n good example of overclaim in the reviewer's opinion.  In fact, real-world has many types of tasks, even solving all dynamic video tasks, it is not equal to \" Solving Real-world Tasks\".  \n\n2. No sufficient related works and the motivation is not clear. The authors say that \"current LLM-driven agents mainly focus on solving tasks for the image modality\", so they study dynamic video tasks. It can been seen that the authors totally ignore the large number of other LLM-driven agents that are not related with image/video at all, e.g., [1,2,3,4]. \n\n3. The evaluation can be more convincing if more datasets are used.\n\n\n\n[1] Generative Agents: Interactive Simulacra of Human Behavior\n[2] TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents\n[3] Reflexion: Language Agents with Verbal Reinforcement Learning\n[4] Tool Learning with Foundation Models"
            },
            "questions": {
                "value": "The are many overclaims and no sufficient related works and the motivation is not clear. It is clearly below the bar of ICLR. The reviewer encourage the authors reformulate their paper writting (Authough I like the concrete ideas proposed in this paper)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Reviewer_KysJ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698586288726,
        "cdate": 1698586288726,
        "tmdate": 1700703701896,
        "mdate": 1700703701896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k3fdPKtLE1",
        "forum": "bXLOOoR2ft",
        "replyto": "bXLOOoR2ft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_FY1y"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_FY1y"
        ],
        "content": {
            "summary": {
                "value": "Paper introduces a novel approach for video understanding and spatial-temporal reasoning. From the high level, it first builds a symbolic, spatial-temporal knowledge base given a video using several off-the-shelf tools. Next, this approach utilizes a pre-trained LLM as a planner to interactively invoke tools including canonical SQL query, search, etc for retrieval-augmented generations, and novel sub-task tools that break down the original query into sub-questions like \"what\", \"how\", etc. Results on the challenging NExT-QA dataset demonstrate the clear advantages of the proposed method against both end-to-end baseline and counterpart LLM-assisted approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+The topic studied here is important. Augmenting the powerful LLMs with better tools and smarter planning skills is crucial to unleash their full potential and also open up new applications in, for example, multimodal domains. I believe this paper could drive interest to a broad range of audiences from canonical multimodal learning and LLM communities.\n\n+The method is technically sound. Building a symbolic knowledge base first, and then invoking an LLM-based system to query it make sense especially when it comes to complicated multimodal data like videos. Decomposing the original query into sub-questions also looks like a promising approach upon to canonical LLM tool-use, where the tools are limited to search, SQL query, etc.\n\n+The results on the challenging NExT-QA data are impressive."
            },
            "weaknesses": {
                "value": "Having said those above, I have the following major concerns and I hope the authors could provide some clarifications:\n\n-It seems that all baselines in table 2 are \"straight-through\" compared to the proposed approach, that is, they are either end-to-end, or simply produce several sub-queries, and invoke the corresponding tool directly, while none of them have a separate stage of building the spatial temporal symbolic database. Therefore, I do think a more fair comparison should also take the time cost of building such a database into consideration. At least, some additional details should be outlined, ex. how long does it take to build a symbolic database in average? How does this compare to the overall inference time of the baselines? These are the questions that will help with a better understanding on the proposed method.\n\nAs a side note, can the proposed method still work without a pre-built database? I think some of the queries can be done directly by invoking the right tool, ex. VideoQA, no?\n\n-The authors have claimed that their approach is \"an intuitive yet versatile system driven by LLMs that is compatible with various foundation models and real-world video applications.\". However, it was only evaluated on one dataset and it might raise concern on the generality of the proposed approach. I have to admit that I am not an expert in video understanding but maybe the following datasets should be considered for additional evaluations: [1-2].\n\n-Some references in LLM + planning (and tool use, memory) are missing [3-5]\n\n\n[1] TVQA+, https://paperswithcode.com/dataset/tvqa-1\n\n[2] CATER: https://github.com/rohitgirdhar/CATER\n\n[3] DEPS: https://arxiv.org/abs/2302.01560\n\n[4] Plan4MC: https://arxiv.org/abs/2303.16563\n\n[5] GITM: https://arxiv.org/abs/2305.17144"
            },
            "questions": {
                "value": "See \"weaknesses\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Reviewer_FY1y"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698632222303,
        "cdate": 1698632222303,
        "tmdate": 1700716514490,
        "mdate": 1700716514490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "92CgjiiFlW",
        "forum": "bXLOOoR2ft",
        "replyto": "bXLOOoR2ft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_9YSq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1908/Reviewer_9YSq"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a system called DoraemonGPT, which is an LLM-driven agent handling video-driven tasks. It first decomposed the video based on spatio-temporal relations and questions. Furthermore, it plans an action sequence based on a Monte Carlo tree search.  Just like humans use external knowledge to plan better, DoraemonGPT can access external sources like search engines, textbooks, databases, etc. When deconstructing tasks into spatial and temporal dominant memories, it will only store them related to the task. These memories are stored in a table, and LLM can query it using symbolic language. A series of sub-task tools are designed to simplify memory information querying. Each tool focuses on different kinds of spatial-temporal reasoning by using individual LLM-driven sub-agents with task-specific prompts and examples. In order to effectively navigate the large planning domain, DoraemonGPT uses MCTS. By choosing a highly expandable node to extend a new solution and backpropagating the answer's reward, the planner iteratively discovers viable answers."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* It is very novel to combine a symbolic memory database with an MCTS planner using LLMs to solve video-based tasks.\n* provides detailed information about prompts, experiments conducted, and analysis results. These are useful in assessing the DoraeGPT's potential."
            },
            "weaknesses": {
                "value": "* Considering many models like BLIP, YOLOv8, PaddleOCR, and other models for extracting the information for video. It is not clear how shortcoming these models affects DoraemonGPT. \n* Works like Yu et al. achieved better performance on NEXT-QA (zero-shot) than DoraemonGPT.\n\n[1] Yu, Shoubin, Jaemin Cho, Prateek Yadav, and Mohit Bansal. \"Self-Chained Image-Language Model for Video Localization and Question Answering.\" NeurIPS (2023)."
            },
            "questions": {
                "value": "* Can't model like BLIP and along with a model which takes a query and features of the frames from BLIP figure which frames are relevant to the query? Why TSM is required?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1908/Reviewer_9YSq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1908/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699389482458,
        "cdate": 1699389482458,
        "tmdate": 1700724255536,
        "mdate": 1700724255536,
        "license": "CC BY 4.0",
        "version": 2
    }
]