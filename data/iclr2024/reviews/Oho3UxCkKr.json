[
    {
        "id": "FS6yOcT6bd",
        "forum": "Oho3UxCkKr",
        "replyto": "Oho3UxCkKr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_4Ly4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_4Ly4"
        ],
        "content": {
            "summary": {
                "value": "This paper presents SCREWS, a methodology for reasoning with revisions. The pipeline includes three stages: sampling, conditional resampling, and selection. The experiments demonstrated that using different strategies for sampling and conditional resampling can boost the reasoning performance of the GPT-3.5 language model."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed framework is general and modular, meaning various techniques can be employed in different stages of it."
            },
            "weaknesses": {
                "value": "1. > A student preparing for an exam may use deductive reasoning to solve problems and inductive reasoning to verify the results\n\n    This is surely the wrong way around?\n\n2. Figure 2 is too visually complicated to be helpful. It's better to present a simplified and more abstract pipeline than listing every component.\n\n3. This is the main thing I am unsure about: In tables 1 and 2, the results are supposed to demonstrate the usefulness of the resampling strategy. However, in table 1, only 4 out of 9 pairings are statistically significant. Also, when using Subq (Or) as the sampling strategy, it does not seem to matter much (or have statistical significance) as to which conditional resampling strategy is used. Does this maybe suggest that there is a saturation, or utility limitation of SCREWS, if the sampling strategy is good enough?\n\n    In table 2, although conditional sampling is cheaper, independent sampling does have significantly higher performances (and upper bounds given oracle selectors). The figure 4 provides further breakdown of the accuracy - cost relation and no strategy really beats CoT in absolute performance. This leaves the question of the overall usefulness of SCREWS uncertain."
            },
            "questions": {
                "value": "Could you provide ablation studies to show clearly that given the same computational cost, SCREWS can perform better than naive CoT? If so, I can be convinced of its effectiveness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697898381978,
        "cdate": 1697898381978,
        "tmdate": 1699636037832,
        "mdate": 1699636037832,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2pEV93OQnr",
        "forum": "Oho3UxCkKr",
        "replyto": "Oho3UxCkKr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
        ],
        "content": {
            "summary": {
                "value": "This paper studies refinement and revision in reasoning. They propose a modular framework for improving reasoning with revisions. The proposed framework unifies several previous approaches under a common framework but also reveals several novel strategies for identifying improved reasoning chains. It consists of three main modules, Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. The framework is then implemented with GPT-3.5-turbo and GPT-4 and is evaluated on multiple benchmarks for arithmetic reasoning, multi-hop question answering, and code analysis. The proposed strategies achieve substantial improvements over vanilla strategies. The heterogeneous sampling strategy is demonstrated useful in the experiments. They also discuss the importance of a model-based selection strategy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper studies the problem of revisions in reasoning, including reducing errors introduced by revision and alleviating homogenous revisions, which are important research questions for current large language model reasoning. The authors propose a unified framework to address the questions. Many previous works can be viewed as an instance of the proposed framework. As a result, the framework is convenient for ablating the strategies during the pipeline. The experiments and analyses are comprehensive. The proposed strategies are effective. And the experimental findings are inspiring."
            },
            "weaknesses": {
                "value": "Please see the questions listed below."
            },
            "questions": {
                "value": "Q1: How do you choose the specific sub-modules (e.g., self-ask/tool use for conditional resampling, LLM-based selection/Rule-based selection for selection) for each of the three modules in the framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1115/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1115/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1115/Reviewer_Na8j"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681699806,
        "cdate": 1698681699806,
        "tmdate": 1699636037752,
        "mdate": 1699636037752,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "BSS5eHbixX",
        "forum": "Oho3UxCkKr",
        "replyto": "Oho3UxCkKr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_toqi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_toqi"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a framework, called SCREWS, with modular components for reasoning tasks where revisions and selection are needed. The framework contains three modules, Sampling, Conditional Resampling, and Selection. Each of the modules can then be implemented with several alternatives. The authors conducted experiments on GSM8K, StrategyQA, and Auto Debugging, using gpt-3.5-turbo, aiming to see how different combinations of modules can affect the task performance. The important observations include: 1) conditional resampling helps when it is based on a different method than the sampling, 2) a good selection is promising for improving the task performance, but the current selection method still falls short in it, and 3) enabling tools is critical for StrategyQA where additional facts are beneficial."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper has touched upon a popular topic of LLM reasoning, especially when iterative revisions are needed. The proposed framework summarized the typical implementation of different modules.\n2. The paper conducted experiments with different combinations of module instantiations and investigated their effectiveness. The experimental results have led to several interesting takeaway messages.\n3. The paper is easy to follow."
            },
            "weaknesses": {
                "value": "The contribution of this paper seems to be incremental, as it is mainly an empirical exploration of existing module implementations. While the experimental results led to interesting observations, these observations are mostly expected, whereas the more critical questions, such as how to improve the existing selection method, are not well addressed."
            },
            "questions": {
                "value": "I found the tool use experiment of StrategyQA a bit confusing.\n1. I wonder if the conditional resampling can still be helpful if the LLM is configured to access the retrieved fact in its initial sampling? \n2. The setup seems to directly provide relevant facts to the conditional resampler, and the model does not actually use any Web search tool for fact retrieval. Is this the major reason for task improvement? I wonder in the more realistic case of using an external tool, if the same improvement can be observed (considering the potential noise, long retrieved passages, etc.)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803725765,
        "cdate": 1698803725765,
        "tmdate": 1699636037669,
        "mdate": 1699636037669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Li0mSujT7r",
        "forum": "Oho3UxCkKr",
        "replyto": "Oho3UxCkKr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_2Dnf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1115/Reviewer_2Dnf"
        ],
        "content": {
            "summary": {
                "value": "SCREWS, a modular framework for reasoning with revisions.  The author observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. So there should be a framework that decide we should accept the current revision or not. \n\nThe proposed approach consists of three steps: \n[1] Sampling instantiate SCREWS by fixing the submodules for each module  \n[2] Conditional Resampling, which decides whether to generate a revision conditioned on the initial sample, and does so if needed. \n[3] Selection: all samples and revisions are given to the Selection module, which selects the best one. \n\nEach of the above three modules include several existing effective methods: \nSampling: CoT, decomposition\nCondition resampling: Self ask, tool use\nSelection:  self-consistency, rule-based etc"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper works on an interesting problem. The paper collects a couple of well known approaches and integrate them into this framework, and provide suggestions on how to use them. The paper objectively reports results, and performs analysis and comparison. Regarding ideas, self-ask with respect to multiple steps of decomposition is quite interesting."
            },
            "weaknesses": {
                "value": "1 the paper is a collection of existing approaches, the contribution is a bit incremental and the novelty is a bit limited. \n\n2 the effectiveness of the proposed approach is not quite conclusive yet. \n\n- Table 1 the conclusion is sampling and conditional reasamping should use different sampling approach, i.e. CoT + Subq (QG) or Subq (QG) + CoT. However, the improvement is rather incremental (i.e. 73-> 73.99). Especially considering SOTA of GSM8K IS 90+ https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k (although we understand the foundation models are different, the effectiveness of the approach is not clear)\n\n- Table 2 \u201cindependent sampling\u201d combines Subq (QG) and CoT (74.90) give the best performance than \u201cconditional sampling\u201d (73.99 table 1), which makes me unclear of the effective of conditional reasoning (i.e. combine two samplings are easy and just do majority vote on, i.e. no need to ask LLM whether to resample or not)\n\n- The right half of Tab. 2 shows Selection between the Sampled and Conditionally Resampled. Does that mean the selection module doesn\u2019t bring significant gain?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1115/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699255488759,
        "cdate": 1699255488759,
        "tmdate": 1699636037594,
        "mdate": 1699636037594,
        "license": "CC BY 4.0",
        "version": 2
    }
]