[
    {
        "id": "qkfBMX2HED",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_YwkA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_YwkA"
        ],
        "forum": "iad1yyyGme",
        "replyto": "iad1yyyGme",
        "content": {
            "summary": {
                "value": "The paper introduces a pipeline to produce time series that are resembling real world data while being synthetic and analytical enough to serve as benchmarks for TSCD algorithms"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Based on the evaluation of the method, the produced time series appear to be reasonably realistic.\nThe method appears to be fairly simple conceptually\nThere is extensive analysis of the related literature\nThere is an ablation study - a very welcome addition to the paper.\nI believe the community will stand to benefit from using this paper"
            },
            "weaknesses": {
                "value": "The analysis of the method, and the caption of Fig 1 could be improved. Too large emphasis has been given to sounding and appearing mathematical, this makes the  true contribution and impact of the paper, its incorporation in the analysis frameworks of other algorithms, harder to realize as it obfuscates details. \nThe assumption of stationarity, albeit common, remains very restrictive and should be treated as a limitation. There is a significant amount of real world problems that are non stationary and a method like this would not fare well."
            },
            "questions": {
                "value": "It is unclear how the method overcomes its main limitation in performance of extracting the causal graph from raw data. It appears that the proposed pipeline uses a  time series causal discovery algorithm, to build a synthetic dataset , to test other  time series causal discovery algorithms, making this an unorthodox loop. How are we guaranteeing the accurate extraction of the underlying DAG to produce the synthetic data. \n\nReal world observational data are rarely well behaved and suffer from , missing entries, confounding, and other biases. A successful  time series causal discovery algorithm needs to be able to disentangle all these from the true causal features. How are the synthetic data taking these biases into account and guarantee not making a too easy task for a TSCD algorithm\n\n\n\n### EDIT POST REBUTTAL \n\nI have updated the score from a 5->6"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Reviewer_YwkA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3083/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697285062506,
        "cdate": 1697285062506,
        "tmdate": 1700213632800,
        "mdate": 1700213632800,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2WsBS4gc08",
        "forum": "iad1yyyGme",
        "replyto": "iad1yyyGme",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_3QnF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_3QnF"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a methodology for producing realistic time series data with ground truth, which can be used to evaluate algorithms for causal structure learning in time series.  For most domains, it's impossible to have realistic data with a known ground truth causal structure, since we don't know the ground truth dynamics of most domains, so most data for evaluation of causal modeling algorithms is at least semi-synthetic.  The authors' approach builds on this tradition by learning a model of realistic data, treating that model as the ground truth, and using it to generate a new data set that can be used for evaluation.  The authors detail the methodology for their approach and perform a series of experiments, both to assess how realistic the generated data looks and to compare the performance of multiple time series causal discovery algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "While the overall idea of this paper (fitting a model on realistic data, and then using it to generate data that has a known ground truth) isn't novel, I haven't seen it applied to time series data, and I think the authors' treatment of it is well-described and motivated.  The authors describe their methodology well and provide a reasonable technical foundation.\n\nThe experiments cover a useful breadth.  The comparison of the data distribution between the original and generated time series was interesting, and I appreciated how the ablation study, highlighting the importance of each piece of the equation.  For the comparison of causal discovery algorithms, I thought the authors picked a reasonable set of algorithms to compare, providing a nice demonstration of the application of CausalTime."
            },
            "weaknesses": {
                "value": "My biggest confusion/concern about this work is the inclusion of the residual term.  Clearly, from Table 2, the residual term plays a massive role in producing data that looks like the original data.  However, it seems as though the presence of the residual term simply means that every time series depends on every other time series.  While it makes sense that including more variables would allow for more accurate model fitting, it's not clear to me that the resulting data actually reflects the causal dynamics of the supposed ground truth graph.  Looking at Equations 6 and 7, it doesn't look like the residual term is down-weighted or anything to reduce its contribution.  Substituting Equation 7 into Equation 6, it actually looks as though the primary f_{theta_i} terms cancel, leaving us with the first term in Equation 7 plus the noise term, which essentially means that we're just ignoring which variables are actually parents of x_i and just including everything.  Am I misunderstanding or misinterpreting something?  It looks to me like a ground truth graph is generated, and realistic-looking data is generated, but the realistic-looking data doesn't actually come from that graph.  This is the main reason my score is a 6 rather than an 8, and if this is cleared up satisfactorally, I'd be happy to raise my score.\n\u00a0\nA bit more analysis for the final evaluation would be helpful.  The authors point out that, when evaluated on synthetic data in prior work, the scores are higher across the board.  While that's interesting, I'm much more interested in if the conclusions we would draw as a result of using CausalTime data differ from those we would draw using synthetic data.  If I were trying to figure out which method performs best using synthetic data, but the method I chose would actually perform worse on realistic data, that would be a very convincing argument for the value of CausalTime.  It also appears as though a citation is missing (\"Besides, compared with the reported results from previous work (), ...\"), so I'm unable to assess how the results actually do compare.\n\u00a0\n3.4 describes its purpose as describing how \"to acquire the Actual Causal Graphs with high data fidelity\".  However, the following paragraphs, up into the equation for the ACG in Equation 8, concerns generating the time series X, not the ACG. (the ACG is defined based on H, I_N, and J_N, none of which are defined in 3.4 prior to Equation 8) By the time you get to Equation 6, don't you already have the ACG? (since it relies on H) So it seems like the first sentence should instead read something like \"To generate data from the Actual Causal Graph (ACG) with high data fidelity\", rather than \"To acquire the Actual Causal Graph (ACG) with high data fidelity\"\n\nThis is minor, but there are some grammatical issues - for example, the first sentence of the 3rd paragraph of the introduction is a fragment."
            },
            "questions": {
                "value": "I don't see this mentioned anywhere - will code be provided upon publication?\n\nIn Definition 1, what is Y?  Is it X at time t? (What is the output of the neural network? I'm not seeing Y referenced after this section)\n\nWhat was the motivation for using |AUC - 0.5|, rather than just AUC?\n\n\nEdit post author response: Updating score from 6 to 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Reviewer_3QnF"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3083/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698806706949,
        "cdate": 1698806706949,
        "tmdate": 1700627629537,
        "mdate": 1700627629537,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1MiA7fG27U",
        "forum": "iad1yyyGme",
        "replyto": "iad1yyyGme",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_jMsi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_jMsi"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a time series simulator that utilizes neural networks to closely match a real dataset. This simulator allows its users to manipulate the causal graph of the process by altering the neural network pathways, or by setting less significant inputs to zero. An individual neural network is employed for each generated time series, thereby encoding a stationary \"family\" within the causal graph of the Dynamic Bayesian Model-like structure. The simulator's construction and justification draw from various existing methodologies. The paper demonstrates that the datasets generated in this manner do indeed share the traits of the original time series, especially when evaluated through their nonlinear embeddings such as t-SNE. Furthermore, the paper benchmarks nine recent deep learning-based models against each other, using data produced by the simulator trained on three different real datasets.\n\n*I raised my score after rebuttal*"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written, with concepts and methodologies clearly explained, making it accessible and comprehensible.\n2. It addresses a significant need in the field of causal time series modeling, offering a solution to a complex problem.\n3. The evaluation of the proposed approach is relatively comprehensive, even though it is somewhat one-sided (as addressed in the paper's weaknesses). It includes benchmarking against several competing methods, which adds a comparative dimension to the analysis. Furthermore, an earnest attempt is made to measure the goodness of fit of the data generated by the proposed method, adding a quantitative validation to the study."
            },
            "weaknesses": {
                "value": "1. The only sanity check of performance of the simulator is the quality of the fit but not demonstration of the learned graphs and whether they are reasonable. Without an investigation into the graphs learned by the simulator we have a situation of severe under-determination of the system. What if, there is enough information in any reasonably sized subset of the prior time series to autoregressively fit any signal. What kind of graph the methods tested on the proposed simulated time series supposed to reconstruct? Table 3 does show non-random performance, so there's truth to it, but are the ACG graphs sparse, dense, high or low Markov order and do they even make sense from the \"organic\" perspective of the domain they have been generated for? These questions are left unanswered.\n2. The focus on neural models in simulation and estimation is limiting. It would be best to show how other models are benchmark."
            },
            "questions": {
                "value": "1. Could you please clarify the notation for the ACG? Your A is represented as a $2N\\times 2N$ matrix. The $2N\\times N$ portion seemingly represents the \"causal\" and residual term mixing in the previous time step. However, the actual causal graph likely has an adjacency matrix of a $2N \\tau_{max}\\times N$ dimension to model edges from parents up to a lag of $\\tau_{max}$. Does this notation imply that in this paper you were only considering Markov order 1 models?\n\n2. I would appreciate if you could use more classical approaches, in addition to purely neural models, for benchmarking time-series causation. I suspect that the experiments may favor neural models over others (as noted in the weaknesses). It would be beneficial to include comparisons with the SVAR model, Granger Causality, the PC algorithm modified to work with time series, and similar models.\n\n3. Could you please provide plots of the graphs that your simulator generates for each of the three real data test-case datasets?\n\n4. Could you characterize the variability of your ground truth ACGs as a function of training your simulator model starting with different seeds?\n\n5. I would appreciate an explanation of how the ground truth causal graphs in Figure 2 are combined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3083/Reviewer_jMsi"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3083/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698976499871,
        "cdate": 1698976499871,
        "tmdate": 1700962691642,
        "mdate": 1700962691642,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3Ktm9LpeBh",
        "forum": "iad1yyyGme",
        "replyto": "iad1yyyGme",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_7uMr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3083/Reviewer_7uMr"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a pipeline to generate time series with know causal graphs. The models considered are nonlinear auto-regressive with instantaneous causal effects. The procedure taken is as follows\n - run a causally disentangled neural network, here based on LSTM, on a real dataset \n - fit normalising flows to the residuals\n - sparsify the node-to-node relationships, the resulting graph $H$ is the HCG. This can be done using Shapley values or using prior domain knowledge.\n - to maintain higher data fidelity, the generated time series have double the dimension of the original data. Each time step consists of $\\mathbf{x}$ and $\\mathbf{x}^\\text{R}$, where $\\mathbf{x}^\\text{R}$ represents the residual when predictions are made using the sparsified model with $H$. The advantage is that it is now possible to generate a new dataset that resembles the old as closely as possible and with a know, sparse causal graph.\n\nExperimentally, the authors run their algorithm on 3 datasets and generate new causal benchmark time series. These are evaluated for fidelity with the original. Existing methods struggle on the resulting datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- this paper presents a neat trick to generate a time series with a know causal graph that matches real data in distribution without knowing anything about the causal structure of the underlying data. This is achieved by doubling the dimension of the time series and having a \"residual\" stream as well as a real-variable stream\n- the generated datasets present problems for existing temporal causal discovery methods"
            },
            "weaknesses": {
                "value": "- this paper is only \"causal\" in a very weak sense. Indeed, given the \"No instantaneous effects\" assumption, it is possible to write down a dense causal graph in which all prior variables are parents of $\\mathbf{x}_t$. Arguably, there is no true causality here, only the problem of learning a time series with *sparse* relationships, because the DAG constraint is automatically satisfied when instantaneous effects in the original time series are discounted.\n- the primary novelty is the $N \\rightarrow 2N$ trick to make a time series that fits the stated requirements. Whilst I think this is quite smart, the remainder of the paper is a concatenation of existing methods.\n- this key idea is not explained particularly well in Sec 3.4. \n- it is unknown how realistic the causal relationships used in CausalTime are. Indeed, the authors make no claim of doing causal discovery. Hence, the causal relationships used in CausalTime may be very different to those found in nature, implying that CausalTime datasets are not a good surrogate for causal discovery on real time series. Indeed, when using Shapley values, a correspondence between feature importance and causality is suggested which may be incorrect."
            },
            "questions": {
                "value": "- How does causal time fit in with methods that are designed to discover instantaneous relationships, like Rhino (Gong et al., 2022)? Could Rhino be applied to CausalTime datasets, and if so should it be included in the benchmarking? Based on your assumption of \"No Instantaneous Effect\", this would not be possible.\n- The methods in the paper suggest that including the residual term is important. This, in turn, implies that \"natural\" causal graphs may be dense. E.g. all components of $\\mathbf{x}_{t-1}$ affect $\\mathbf{x}_t$. However, in CausalTime, the causal graph is forcibly sparsened to produce $H$, so as to present a more interesting problem to causal discovery algorithms. In Table 2, the inclusion of the residual term seems to reduce discriminative scores by a factor of at least 10, implying it is very necessary to get good reconstruction. Thus- is the forced sparsification used in CausalTime actually contrary to natural time series?\n\nGong, Wenbo, et al. \"Rhino: Deep causal temporal relationship learning with history-dependent noise.\" arXiv preprint arXiv:2210.14706 (2022)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3083/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699028037675,
        "cdate": 1699028037675,
        "tmdate": 1699636253854,
        "mdate": 1699636253854,
        "license": "CC BY 4.0",
        "version": 2
    }
]