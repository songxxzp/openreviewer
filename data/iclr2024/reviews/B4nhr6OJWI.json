[
    {
        "id": "8gEtaVIxj0",
        "forum": "B4nhr6OJWI",
        "replyto": "B4nhr6OJWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_wyEr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_wyEr"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the problem of injecting inductive bias into a network, via the proposed subtask induction method. In particular, using an auxiliary subtask dataset, the authors first identify a subnetwork that achieves the subtask and use it as the initialization for another network, which is subsequently trained for a closely related downstream task. To show the effectiveness of the method, this paper conducted two sets of experiments: an arithmetic computation with decoder-only transformers, and a vision experiment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall the problem statement is quite interesting and the presentation is very clear and easy to follow."
            },
            "weaknesses": {
                "value": "I found the motivation to be a little lacking. The authors mentioned mechanistic interpretability, however, it is unclear to me (based on the current writing) why we should care about such interpretability. I would encourage the authors to give some examples to demonstrate how it can be used in reality. For example, what are some scenarios in which we can accurately define subtasks (in the vision experiment you mention shape vs texture, I understand that the problem is extensively studied, but it is also kind of artificial)?"
            },
            "questions": {
                "value": "1. For the GPT2 experiment, did you use their pre-trained weights, or did you train everything from scratch?\n2. If you tokenize the integers from {0, ..., 999} with GPT2tokenizer, 181 of them are actually tokenized into 2 tokens (for example, 521->'5', '21'). If the original tokenizer is used, I would suspect the model cannot really learn the underlying equation (this may also be why you need many disambiguous examples).\n3. How does the number of ambiguous examples affect the accuracy? Does the performance decrease with more examples?\n4. In the first paragraph in section 5.3, to confirm, you discover pretrained ResNet18 and ViT, are they pretrained on only ImageNet? If you have a randomly initialized ViT/RN, can you still discover a subnetwork that works well on Mean-pooled ImageNet? \n5. What is the training setup of Figure 5? Did you train every model on 213k images of the 16-class ImageNet? The comparison looks a little unfair to me, as the Subtask Induction models have been pretrained (maybe on the original ImageNet? This question is related to the previous point). A more fair comparison will be that you find a subnetwork from a randomly initialized network that performs well on the mean-pooled imagenet.\n6. In the right figure in figure 5, the accuracies don't change.  Is this because you froze the subnetwork's weights after transferring? If you don't freeze the weights, will the accuracies on mean-pooled imagenet increase?\n7. Can you also give the detailed setup of each row in Table1? My current understanding is: model+subtask induction = model pretrained on all original ImageNet, find subnetwork using the mean-pooled data, then continually train using 213k 16-class imagenet. model from scratch is trained to perform 16-class classification directly. model+DA and model+pretrained are where I get confused: a) how exactly is the data augmentation performed? and b) is \"model+pretrained\" where you finetune only the last classification layer? If so, how exactly is the last-layer finetuned?\n\nmiscellaneous: table caption should be on top; paragraph on top of section 4.3 'diambiguation'->'disambiguation'.\n\nI'm happy to raise my rating if the authors can address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613742173,
        "cdate": 1698613742173,
        "tmdate": 1699637031378,
        "mdate": 1699637031378,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gx9JuyST91",
        "forum": "B4nhr6OJWI",
        "replyto": "B4nhr6OJWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_pnkg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_pnkg"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Subtask Induction, a way of implanting inductive bias by identifying a subnetwork that performs a specific task, and only training the rest of the model. The idea is that the rest of the model is forced to learn, bounded to performing that specific (wanted) task. The method is tested on two problems. The first one is the reconstruction of a discrete mathematical operation, and the second is image classification on 16-class ImageNet. In both cases Subtask Induction obtains better perfomances in situations where the inductive bias was necessary and that were hard to learn for a model where this was not instilled.\n\nI lean towards acceptance because:\n- The idea of inducing a specific inductive bias is intriguing and deserves to be discusses in a wider context. Similarly to transfer learning, one could imagine collections of pretrained subnetworks that are specifically tailored towards a task, or that avoid some specific spurious correlation.\n- The paper is well-written and easy to follow, and the code is well-documented. \n\nI score the paper as a 6, which I reserve myself to lower if some of the answers to my questions are not satisfactory or if some other reviewer.\n\nThe reason for not giving a higher score is because though the idea is nice, (i) it is hard to visualize how this method could be actually implemented efficiently and (ii) the evidence is purely empirical, and the set of experiments is not very extensive. The reason why it is not too extensive is that it is hard to devise suitable testing situations, which sends me again to point (i)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method can potentially help solving, in the long run, problems related to spurious correlations, out-of-domain generalization, and more. \n\n- A well-documented code is made available through an anonymous link."
            },
            "weaknesses": {
                "value": "- The method is valid for one subtask, but extending it to more than one is non trivial and perhaps not possible, since the subnetworks could either overlap or occupy the whole network (in which case we would just be doing transfer learning).\n\n- It is not easy to identify the subnetwork of a model that performs a subtask. I see this as a major limitation, but it doesn't seem impossible to me that in the future there could be some solutions.\n\n- The method relies on finding a subnetwork, which seems a very intensive work.\n\n- The method is validated on a limited number of tests. If on one side it is hard to find real-life examples (the authors had to resort to an adhoc dataset), they could at least have tried with some other mathematical operations (and there, architectures since in principle the method should apply to any architecture, even e.g. MLPs)."
            },
            "questions": {
                "value": "- Can the authors elaborate on the difference between their method and transfer learning? The two seem very similar to me, and transfer learning would be the limit for very big subnetworks.\n\n- In transfer learning, we fine tune by first freezing the model and training only the last layer, and then training the whole model with a very small learning rate. Here, the second step was deliberately removed in order to enforce the subtask. Could a second step of fine tuning help?\n\n- Is the dataset available at this point? I cannot find where it is written.\n\n- Could it be possible to benchmark this method on datasets for spurious correlations?\n\n- What do the found subnetworks look like? Are they restricted to a single layer? Are they dense blobs? How many paths do they comprise?\n\n- How does the method deal with dropout? With dropout there are no (or fewer) preferential paths, so I would assume the subnetworks would be much bigger.\n\n- The subnetwork can only be transferred without varying the architecture, right? E.g. there cannot be a subnetwork which is only made of two layers, and I plug these two layers into my model. Is this right? Or, if I understood wrong, how is the subnetwork transferred when the architecture changes?\n\n\nOther minor things:\n- Figure 1 is not referenced in the text\n- typo on page 13: implementatione\n- In section 3.1/2 it is sort of obvious that the masks are trained on well-trained models (instead of e.g. models at initialization) but it is not explicitly written\n- I would put a parenthesis before the modulo operators, because I was initially confused. For example, I would write"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698778746933,
        "cdate": 1698778746933,
        "tmdate": 1699637031230,
        "mdate": 1699637031230,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rUrA7FJ9PP",
        "forum": "B4nhr6OJWI",
        "replyto": "B4nhr6OJWI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_PuHj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8294/Reviewer_PuHj"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new approach called Subtask Induction, which involves extracting specific problem-solving abilities from trained neural networks. They achieve this by isolating a subnetwork responsible for a particular task within a larger neural network and initializing another network with only these subnetwork weights, leaving the rest randomly initialized. The authors demonstrate subtask induction on an arithmetic task and image classification on a novel dataset, Mean-pooled ImageNet, which requires networks to learn shape information rather than texture information. The approach is an interesting demonstration that (1) these subnetworks exist and can be extracted and transferred to new networks, (2) SGD learns to use these subnetworks, and this leads to more efficient learning on new tasks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I really enjoyed this paper - it takes the observation from the mechanistic interpretability literature that deep networks learn subnetworks to solve specific tasks and uses it to derive a simple method for extracting these subnetworks (essentially by optimizing a sparsity mask over the parameter on a distribution of problems that only require the subnetwork) and then they randomly initialize the remaining weights of a network and train on a second task that requires the shared skill. The results on the arithmetic task provide a proof of concept, but vision experiments are particularly interesting because they show that these subnetworks can be discovered in real data, provided you have dataset which allows you to extract this."
            },
            "weaknesses": {
                "value": "The requirement for a dataset like mean-pooled imagenet to extract the subnetwork significantly constrains how widely applicable this paper is as a method---it essentially requires you know the task in advance and how to specify it with examples that are sufficiently different from typical examples in the training set---but I still think that it is an interesting demonstration.\n\nI would have liked to see some examples of where it fails: for example, if you don't have a clear separation between IID samples and the target task, I would expect it would struggle."
            },
            "questions": {
                "value": "1. Did you study any tasks for which Subtask Induction performs poorly? \n2. If so, what are the characteristics of those tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8294/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980998538,
        "cdate": 1698980998538,
        "tmdate": 1699637031128,
        "mdate": 1699637031128,
        "license": "CC BY 4.0",
        "version": 2
    }
]