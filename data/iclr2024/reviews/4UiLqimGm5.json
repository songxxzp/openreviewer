[
    {
        "id": "KAkMJJxODP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
        ],
        "forum": "4UiLqimGm5",
        "replyto": "4UiLqimGm5",
        "content": {
            "summary": {
                "value": "This main contribution of this paper is to use a grid-based approach to provide a scale and bias for the features generated at each layer of an implicit neural network, an approach used to encode any kind of signal. This is in contrast to the typical approach where the grid-based approach provides an input to the implicit neural network. In other words, the proposed approach is: a) use the input coordinate to recover the scale and bias for all layers of the MLP, b) recursively apply each layer of the network, normalize features, apply scale and bias for the appropriate layer. The authors also discuss which subset of input coordinates should be used to define the grid. Other contributions of the paper include the use of the aforementioned feature normalization in implicit neural representation and a comparison to various benchmarks on image encoding and generalization, novel view synthesis in static and dynamic NERF and video compression."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n* As far as I know, use a grid-based approach for scale/bias in implicit neural representation is new.\n\nQuality:\n* Evaluation is performed on several different tasks, with good results, thus I believe the strength of the method is demonstrated.\n* The baselines used are generally competitive and recent.\n\nClarity:\n* The paper is well written.\n\nSignificance:\n* good results on multiple tasks, state-of-the-art in some."
            },
            "weaknesses": {
                "value": "Quality:\n* I find the use of FFN (Tancik, 2020) as the only baseline in the image task disappointing. While not the most significant experiment of the paper, I think the use of more recent baselines and in particular of other grid-based approaches, for example at least instant-NGP, would make the comparison on images more significant. I also want to add that I find the baselines in the video experiment to be adequate, and arguably the video experiment is more important.\n\nClarity:\n* I might have missed it, but for me, the paper does not sufficiently discuss/explain the reasoning behind the choice of coordinates that are used in the grid to select a scale and bias. For example, the scale/bias depends on the pixel coordinates only for a picture. As far as I understand, scale/bias are thus the same for each channel. But for the video, the scale/bias depend on pixel coordinates, time and channel. I do not understand why the channel becomes important for the video. I noticed the ablation study (Table 6), but it only covers the NERF experiments. This ablation study also does not discuss making the scale/bias depend on both direction and time.\n* I do not understand what is the variance represented in Figure 8. Is it the variance between the elements of the input of the last layer of the MLP? \n* I find the notation in equation 1 and subsequent equations a bit confusing. Both $\\gamma_n$ and $\\beta_n$ takes as input the full batch $X$. This suggests that any element of the vectors $\\gamma$ and $\\beta$ may depend on the full batch $X$. I suspect this is not the case due to how grid based approaches typically work but I am not sure. Would it make sense to change the notation to $\\gamma(X_n;\\Gamma)$ or  $\\gamma(X^{(n)};\\Gamma)$, to be closer to the notation already used in equation 4?"
            },
            "questions": {
                "value": "* On page 8, the paper states that HM decodes at 10fps using a GPU. I was very surprised, because, as far as I know, HM does not use a GPU. I also could not find a mention of this fact in (Hu et al., 2023). While it is common to compare decoding speed on different hardware (CPU for traditional codec, GPU for ML methods), I think it is misleading to state that HM uses GPU. Could you please comment on/clarify this?\n* Could you please comment on the choice of coordinates used in the grid to define scale/bias?\n* Could you please provide further explanation about Figure 8?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Reviewer_DCsj"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697446481202,
        "cdate": 1697446481202,
        "tmdate": 1700671187353,
        "mdate": 1700671187353,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oAsCHnrvEG",
        "forum": "4UiLqimGm5",
        "replyto": "4UiLqimGm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
        ],
        "content": {
            "summary": {
                "value": "This work proposes combining grid input and MLP structure at intermediate features level for Neural fields application. This naturally extends MLP-only or grid-only methods, which despite its competitive performance have had downsides, such as not being able to represent high-frequency content or being computationally intensive."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The biggest strength of CAM is its simplicity and its plug-and-play nature. I believe this will have much far-reaching impact in the Neural fields literature, compared to other highly sophisticated & implementation-heavy frameworks designed to maximize PSNR value at all costs. Similar to how widely Batch/layer-normalization has been used by the entire field.\n- Extensive experimentation on diverse tasks and against different baselines add to the credibility of the work."
            },
            "weaknesses": {
                "value": "- While there are no specific weaknesses to point out, I don't think Figure 1 or Figure 2 convey the idea that well. Figure 1 probably will be better served by displaying more detailed mechanism (exammple of x, example of the values for \\Gamma, etc.).\n- Also giving a brief description of what functional form \\Gamma and B take would be informative for readers."
            },
            "questions": {
                "value": "Don't have specific questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3192/Reviewer_7EJV"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698613507467,
        "cdate": 1698613507467,
        "tmdate": 1699636267054,
        "mdate": 1699636267054,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FdD7DJnqNg",
        "forum": "4UiLqimGm5",
        "replyto": "4UiLqimGm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_kCkz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_kCkz"
        ],
        "content": {
            "summary": {
                "value": "The paper introduced a coordinate-aware modulation module that combines MLP features and grid representations for neural fields. Unlike the popular methods that chain the features, this new method not only preserves the strengths of MLP but also mitigates the bias problem by leveraging grid-based representation. The authors conducted experiments on tasks in multiple domains and the results demonstrate its capability of modeling high-frequency components and advantages over prevalent neural field features."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The motivation of the paper was clearly stated \n- The proposed approach is simple yet effective\n- The paper is well structured and the idea is easy to follow \n- Experiments are comprehensive. It covers various domains such as images, videos, etc."
            },
            "weaknesses": {
                "value": "- The numbers of the baseline models seem to be from the authors' own implementation, which makes it less appealing"
            },
            "questions": {
                "value": "- Could you answer the first question I posted in the \"Weaknesses\" section?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698881136603,
        "cdate": 1698881136603,
        "tmdate": 1699636266988,
        "mdate": 1699636266988,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mwBsN9SkoN",
        "forum": "4UiLqimGm5",
        "replyto": "4UiLqimGm5",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_oV38"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3192/Reviewer_oV38"
        ],
        "content": {
            "summary": {
                "value": "Authors propose a new architecture for neural fields, i.e mapping low-dimensional input co-ordinates to the signal values, called CAM (co-ordinate aware modulation). The main idea is to modulate intermediate features using scale and shift parameters which are inferred from the low-dimensional input co-ordinates. Authors show that, while regular a regular MLP shows heavy spectral bias, and just grid representation is computationally very expensive, CAM can mitigate the spectral bias learning high frequency components, while also being compact. In addition, CAM facilitates adding normalization layers which improves training stability. Authors empirically show that CAM achieves competitive results image representation, novel view synthesis, and video representation tasks, while being fast and very stable to train."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ Authors are tackling a very relevant problem, with wide interest to practitioners. \n+ Paper is well written and easy to follow. \n+ Claims in the paper are sounds. I particularly like that the argument about spectral bias and not learning high frequency components is verified empirically in Section 4.3 \n+ Experiments are sound and covers a wide range of tasks. Results are strong with performance comparable or exceeding the state of the art."
            },
            "weaknesses": {
                "value": "+ While the paper is generally strong, I believe that it lacks certain references which can put the work in a better context. There is a long history of using feature modulation in deep learning. A good example is [FiLM](https://arxiv.org/abs/1709.07871). This is also used for image generation/reconstruction tasks like in [generation](https://arxiv.org/abs/1810.01365), [denoising](https://arxiv.org/pdf/2107.12815.pdf), [image restoration](https://arxiv.org/abs/1904.08118) and [style transfer](https://arxiv.org/abs/1705.06830). Adding these references, and including a discussion around it can put feature modulation in a better context. \n\n+ Can authors include inference speed/inference memory requirements to put regular MLP methods, grid based method, and CAM in prospective? \n\n+ The choice of not using the all lower dimensional inputs to infer the scale and shift parameters, but a subset based on the problem is interesting. Have you conducted ablation studies that this is in some way beneficial? \n\nA bit tangential but:\n+ Do you think CAM can benefit decoder MLP for a triplane based representation as well? It would be cool to see some experiments and demonstrate the generality here. \n+ In addition, I think text to 3D is one another domain where the speed and training stability of CAM can benefit quite a lot. If authors can demonstrate a couple of results comparing the training stability and speed using CAM augmented MLP in DreamFusion, that would be a great additon to the paper."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3192/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698953912807,
        "cdate": 1698953912807,
        "tmdate": 1699636266900,
        "mdate": 1699636266900,
        "license": "CC BY 4.0",
        "version": 2
    }
]