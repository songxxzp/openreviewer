[
    {
        "id": "18wUUsJMRW",
        "forum": "u7Xo4sG6Ux",
        "replyto": "u7Xo4sG6Ux",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_2WQN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_2WQN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new backdoor attack against generative LLMs. It distributes multiple trigger keys into different prompt components to improve the attack's stealthiness. The paper demonstrates the effectiveness of the proposed attack in NLP and multimodal tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ This paper proposes a new attack against LLM, with a focus on stealthiness. \n+ The paper demonstrates the effectiveness of the proposed attack on both NLP and multimodal tasks."
            },
            "weaknesses": {
                "value": "- The literature review is not comprehensive enough. Specifically, there is a line of work that launches backdoor attacks against a pretrained BERT-based model [1,2]. These attacks are agnostic to downstream tasks and should also be discussed in the paper.\n\n- The threat model is not entirely clear. Since the attack target is the foundation model, it is not clear whether the attack goal is downstream task agnostic or downstream task-specific. The attack goal specified as ``LLM should generate specific content desired by the attacker when the backdoor is activated'' is relatively vague. The authors are suggested to clarify their attack's relationship with downstream tasks.\n\n- Some technical details are missing: It would be more clear if the authors can provide more details regarding the training objective function and the training algorithm. Whether it is instructional training or it is RLHF. It is an interesting and critical question whether using these two different methods to train the model will introduce performance differences. Similar questions can be asked for the multimodal models.\n\n- Regarding the stealthiness metrics, this paper [3] proposes two additional metrics for it. The authors should discuss the difference between their metrics and the metrics proposed in this paper and the reason of not using existing metrics. \n\n- The paper does not explicitly design their baselines. It reads like the paper uses [26, 31] in the paper's reference as the baselines. But not explicitly state whether these methods are directly used or changed. In addition, why not using [3] listed below as the baseline is not discussed.\n\n- The discussion on defense is rather weak. Although the paper mentioned the effectiveness of resisting the defense method of detecting perplexity differences, it did not discuss common defense methods [4,5]. This paper evaluates against ONION. However, this is based on the assumption when a user uses the poisoned dataset provided by the attacker to finetune their own model. \n\n[1] Backdoor Pre-trained Models Can Transfer to All \n\n[2] UOR: Universal Backdoor Attacks on Pre-trained Language Models\n\n[3] Rethinking stealthiness of backdoor attack against NLP models, ACL 2021\n\n[4] PICCOLO : Exposing Complex Backdoors in NLP Transformer Models (S&P) 2022\n\n[5] RAP: Robustness-Aware Perturbations for defending against backdoor attacks on NLP models (EMNLP 2021)"
            },
            "questions": {
                "value": "1. Backdoor attacks and defenses have been extensively studied in the domain of NLP and LLM. The authors are encouraged to conduct a more comprehensive literature review. \n\n2. The threat model is not clearly described (see detailed above).\n\n3. Some important technical details are missing (see detailed above).\n\n4. The authors are encouraged to justify their stealthiness metric better and compare their metric with the existing ones mentioned above.\n\n5. The baselines are not clearly defined.\n\n6. The evaluation of defenses is vague. The authors do not discuss or evaluate some common defenses in the NLP domain. The evaluated defense has a different threat model from this paper (i.e., ONION)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The authors are encouraged to discuss the potential ethical concerns of the paper, given it is an attack work."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697728644329,
        "cdate": 1697728644329,
        "tmdate": 1699637010233,
        "mdate": 1699637010233,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bw4zwQtp8Y",
        "forum": "u7Xo4sG6Ux",
        "replyto": "u7Xo4sG6Ux",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed \u2018composite backdoor attack\u2019 (CBA) against LLMs. The key argument of this paper is \u2018composite\u2019, where it assumes that the text input to LLMs consists of multiple components, such as \u2018System Role\u2019, \u2018Instruction\u2019, and \u2018Input\u2019. The paper mainly discussed the two-component scenario, where it assumes the text input consists of 'Instruction' and 'Input'. Extensive experiments on 3 NLP tasks and 2 multimodal tasks with 5 LLMs show that CBA is stealthier, and can achieve high attack success rate, high clean-test-accuracy, and low false triggered rate. The author studied the stealthiness of the proposed attack, and the impacts of LLM size, poison data ratio, and 'negative' poison data ratio."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper studies the potential threats of backdoor attacks with multiple trigger keys in different input components in LLM, which have not been studied before.\n- The paper proposes CBA, which achieves high attack success rate, high clean test accuracy, and low false triggered rate on backdoor attacking LLMs."
            },
            "weaknesses": {
                "value": "- The assumption can be too strong for some scenarios: user input may not always follow an 'Instruction' + 'Input' format.\n- Experimental results for single-key methods and dual-key methods ($\\mathcal{A}\\_{inst}^{(1)}$, $\\mathcal{A}\\_{inp}^{(1)}$, $\\mathcal{A}\\_{inst}^{(2)}$, $\\mathcal{A}\\_{inp}^{(2)}$) are not shown for comparison.\n- Since lower values are preferred in Table 1, there is no need to bold the highest values."
            },
            "questions": {
                "value": "1. Is $h_i(\\cdot)$ implemented by inserting a trigger key at a random position, or at a fixed position, e.g. beginning or ending of the given input component?\n2. In Table 3, LLaMA-30B, $\\eta=5$, why ASR drop to 50.27%? \n3. Will the proposed method still be effective when there are no component identifiers like \"Instruction: \" or \"Input: \" in the prompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Reviewer_9zZ2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698656960752,
        "cdate": 1698656960752,
        "tmdate": 1699637010111,
        "mdate": 1699637010111,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nKQ3C5DTeh",
        "forum": "u7Xo4sG6Ux",
        "replyto": "u7Xo4sG6Ux",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
        ],
        "content": {
            "summary": {
                "value": "This paper develops a novel backdoor attack, Composite Backdoor Attack (CBA), against Large Language Models (LLMs). An LLM backdoored by CBA can only be activated when the multiple trigger words correctly appear in different components of the prompt, increasing the attack stealthiness.\nTo ensure the attack effectiveness, the authors propose a novel injection method.\nExperiments on both Natural Language Processing (NLP) and multimodal tasks show that the attack is effective and stealthy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. **The originality is good.** The paper consider the prompt of multiple components which is a unique feature in current LLM application.\n\n2. **General problem formulation and experiments on multi-modal test.** This work provides a unified formulation for multi-trigger backdoor attack and applies to both NLP and multi-modal tasks."
            },
            "weaknesses": {
                "value": "1. **The attack assumption is strong.** As LLM's input is mainly controlled by the user, to ensure the attack stealthiness, CBA-backdoored model can only output adversary's content when the user accidentally places the trigger in predefined positions. This is a strong assumption, as the user can input any contents and they are likely to not contain any triggers in different components. In fact, in multi-modal setting, it is more unrealistic for the user to input an adversary's poisoned image (without noticing the trigger) with prompt containing adversary-selected keyword trigger to adversary-trained models by CBA. \n\n2. **The improvement of attack stealthiness is not convincing.** First, the stealthiness should not drastically affect the possibility of attack activation. Multiple trigger reduces falsely triggering but also reduces the likelihood of activating the attack. Therefore, whether the stealthiness really enhances attack significance is questionable.\nSecond, I also have concerns over the numerical analysis of stealthiness. In Section 3.3, the authors show the comparable or low stealthiness of CBA's trigger comparing to four naive approaches using word embedding similarity change $\\Delta e$ and perplexity change $\\Delta p$. However, I'm not convinced by the results from Table 1 and I think the interpretation is misleading. \nFor word embedding similarity change $\\Delta e$, $A_{CBA}$ has lower $\\Delta e$ on Instruction or Input alone, but their sum can be higher than so-claimed less stealthy baseline $A_{inst}^{(2)}$ or $A_{inp}^{(2)}$ (e.g., on Emotion dataset). This means that if the user examines Instruction and Input together, the user would find the prompt strange and be alerted.\nSimilarly, for perplexity change, on Twitter dataset $A_{CBA}$ has total perplexity change higher than $A_{inp}^{(2)}$.\nMoreover, I do not understand why $A_{inst}^{(1)}$ or $A_{inp}^{(1)}$ has different $\\Delta e$ and $\\Delta p$ with $A_{CBA}$ on corresponding prompt part (Instruction or Input), if the separately inserted trigger remains same?\n3. **Lack of potential defense evaluation.** As an attack paper, there is no evaluation of potential defenses. Some direct defenses, e.g., paraphrasing, could work in mitigating this threat. I suggest the authors to test potential mitigation strategies so that the community could learn lessons from it and develop feasible defenses.\n\n4. **The presentation can be improved.** For example, the Figure 2 is too sparse: most subfigures are almost empty. Probably a table could do the job. Meanwhile, some texts in figures are too small to read. The authors should also pay attention to the paper formatting (e.g., space on the top of page 8).\nMoreover, the paper only consider case $n=2$ while the attack designs for $n>0$. Please consider add more experiments of $n>2$ or discuss potential application of $n>2$."
            },
            "questions": {
                "value": "Please consider a more realistic threat model, justify the stealthiness of the CBA, evaluate potential defenses and improve the presentations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8152/Reviewer_97F9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698750851262,
        "cdate": 1698750851262,
        "tmdate": 1699637009994,
        "mdate": 1699637009994,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i5ocm53L3K",
        "forum": "u7Xo4sG6Ux",
        "replyto": "u7Xo4sG6Ux",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_FWKs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8152/Reviewer_FWKs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Composite Backdoor Attack (CBA) to attack multiple components of the prompt of LLms. Experimental results show that on both NLP and multi-modal (vision) tasks, CBA can achieve high success rate while maintain good clean test performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work considers not only the attack effectiveness of the proposed backdoor attack method, but also the semantic meaning changes to ensure the new prompt has consistent semantic meaning as the original one.\n1. There is comprehensive ablation study to show impact of different component to the final performance, such as the negative dataset construction, the positive poison ratio and the negative dataset ratio."
            },
            "weaknesses": {
                "value": "1. In Section 4.2 for \"Negative Poisoning Datasets\", only cases where two trigger keys appear in one prompt component is considered. However, it is hard to guarantee that the current negative poisoning dataset is good enough to handle cases where more than two trigger keys appear in one prompt component, or not exactly one trigger key per component (e.g., 1 for instruction and 2 for input). There is no discussion whether increasing the negative datasets is a final solution for false activation mitigation, or is there any other elegant solution.\n1. **Missing baselines**: \n- In Section 3.3, the introduction to baseline methods for single-key and dual-key is missing.\n- In Section 4 Experiments, existing backdoor attack baselines are missing. Moreover, you'd better compare with some recent backdoor attack work such as BITE:\n\nYan, Jun, Vansh Gupta, and Xiang Ren. \"BITE: Textual Backdoor Attacks with Iterative Trigger Injection.\"\u00a0ICLR 2023 Workshop on Backdoor Attacks and Defenses in Machine Learning. 2023"
            },
            "questions": {
                "value": "1. In Section 3.3: the Perplexity change should be absolute value right? So smaller absolute perplexity change indicates minor semantic meaning change?\n1. as introduced in Section 3.3, the smaller the value for two metrics, the smaller semantic change in Table 1. However, largest value per row is emphasized in boldface (I know you want to show that the proposed method semantic change is much smaller than dual-key methods by contrast) . I would suggest you to add some necessary description in the text part or in the caption of the table. \n\n**Typos:**\n1. \" changes of\" in Section 3.3: remove of\n1. Section 4.2.1 EXPERIMENTAL RESULTS IN MULTIMODAL TASKS should be \"4.3 EXPERIMENTAL RESULTS IN MULTIMODAL TASKS\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8152/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698953144107,
        "cdate": 1698953144107,
        "tmdate": 1699637009888,
        "mdate": 1699637009888,
        "license": "CC BY 4.0",
        "version": 2
    }
]