[
    {
        "id": "or1e92Y7hE",
        "forum": "zkE2js9qRe",
        "replyto": "zkE2js9qRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_wKPf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_wKPf"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a novel embedding method to represent data points that have partial order relation. The author's idea is to map those data points to bit sequences, where the inequality of the values of two sequences corresponding to the original partial order relation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The original idea of using bit sequences is novel as far as I know, simple, easy to understand, and intuitive to some extent.\n1. The authors successfully associate the proposed method with the existing order embedding, which helps the authors' understanding.\n1. The algorithm's explanation also maintains some intuition.\n1. The algorithm has strong advantage on the space computational complexity.\n1. Overall, the technical parts of the paper are well-written."
            },
            "weaknesses": {
                "value": "Overall, the presentation of the paper needs essential refinement. The current version's presentation degrades the paper's quality although the research idea itself is nice and impressing to me.\n\n1. From the introduction, the ultimate motivation of the work is not very clear. For continuous space embedding case, we could use them for visualization or we could input the representations to another machine learning architecture, such as neural network. However, we have no clear idea how we can use the obtained binary embedding in applications. If we just want to do link prediction or reconstruction, we do not need to stick to embedding-based methods.\n\n1. As a starting motivation of the research, the paper criticizes hyperbolic embedding, pointing out that \"learning in hyperbolic space is challenging because optimization algorithms, like gradient descent and its variants, are not well studied for hyperbolic space.\" Indeed, the gradient descent methods on hyperbolic space have been well-studied theoretically, e.g., [A-E]. Although the convergence to the global optimum cannot be guaranteed, as not in Euclidean space, but they are not by far worse than the author's theoretical guarantee on the proposed algorithm. The author mentioned that the problem is a NP-complete problem as a decision problem, but it is not a practically positive result unless P=NP. In this sense, the current draft gives readers impression that the author has not solved the original motivation. If it is difficult to provide a theoretical guarantee of the proposed algorithm, the author should criticize the hyperbolic embedding in another way. \n\n1. This item is about another important motivation of the paper, \"logical operation.\" The explanation regarding the logical operation on the binary representations does not seem correct. The logical \"not\" operator does not seem to work like the semantic \"not.\" Assume that \"living thing\" is [0, 0], \"cat\" is [0, 1], and \"dog\" is [1, 0]. This does not self-contradict since a cat is a living thing and a dog is a living thing, too. Let's apply the logical \"not\" to the living thing. According to your explanation, \"not living thing\" is [1, 1]. Now, according to the rule, we conclude that \"a not living thing is a cat\", and \"a not living thing is a dog.\" This is obviously wrong. Hence, the proposed boolean representations are not intuitive as the authors claim.\n\n1. Citation does not include which year it is published, which makes it extremely difficult to see the flow of the existing methods.\n\n1. The page limitation is violated.\n\n1. As I discuss in the Questions section, the advantages of the proposed methods do not seem completely stated in the current draft.\n\n[A] Zhang, Hongyi, and Suvrit Sra. \"First-order methods for geodesically convex optimization.\" In Conference on Learning Theory, pp. 1617-1638. PMLR, 2016.\n\n[B] Zhang, Hongyi, Sashank J Reddi, and Suvrit Sra. \"Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds.\" Advances in Neural Information Processing Systems 29 (2016).\n\n[C] Liu, Yuanyuan, Fanhua Shang, James Cheng, Hong Cheng, and Licheng Jiao. \"Accelerated first-order methods for geodesically convex optimization on Riemannian manifolds.\" Advances in Neural Information Processing Systems 30 (2017).\n\n[D] Zhou, Pan, Xiao-Tong Yuan, and Jiashi Feng. \"Faster first-order methods for stochastic non-convex optimization on Riemannian manifolds.\" In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 138-147. PMLR, 2019.\n\n[E] B\u00e9cigneul, Gary, and Octavian-Eugen Ganea. \"Riemannian adaptive optimization methods.\" ICLR 2019."
            },
            "questions": {
                "value": "- Why did you not show the memory (RAM) to store the representation in the numerical experiments? I thought you could emphasize the advantage of the proposed method clearer this way, since the proposed method's type is boolean, and existing method's types are float or double.\n- Why did you not show the order of time complexity of the proposed algorithm? If I understand it correctly, each step's time and space complexity is linear to the dimension, which seems to be an advantage of the proposed algorithm."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Reviewer_wKPf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697902750791,
        "cdate": 1697902750791,
        "tmdate": 1699637014797,
        "mdate": 1699637014797,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "n3bDeCAPyH",
        "forum": "zkE2js9qRe",
        "replyto": "zkE2js9qRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_EAos"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_EAos"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach, BINDER, for order embedding using binary vectors. BINDER aims to represent concepts with hypernym-hyponym relationships in a binary vector space, allowing for embeddings of both seen and unseen concepts. Experimental results demonstrate BINDER's superiority over existing order embedding methodologies."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Conceptual Simplicity: BINDER offers a novel and conceptually simple approach to hierarchical representation learning by using binary vectors. This simplicity is an advantage because it makes the method more interpretable and easier to understand compared to complex, black-box models.\n\n2) Strong Performance in Reconstruction Task: BINDER consistently demonstrates excellent performance in the reconstruction task. This indicates its robust ability to learn embeddings that satisfy order constraints, which is a critical aspect of hierarchical representation learning.\n\n3) Transitive Closure: BINDER's ability to predict hypernymy relations without relying heavily on transitive closure in the training data is a significant strength. This property suggests that the model can generalize effectively to unseen concepts and is not overly dependent on the availability of transitive edges.\n\n4) Originality of Approach: BINDER introduces a unique approach to order embedding using binary vectors. This originality stems from its different perspective on hierarchical representation learning and adds to the diversity of methods in this field.\n\n5) Potential for Extensions: The paper hints at possible extensions, such as incorporating node similarity expressions and considering sibling similarity. These extensions have the potential to enhance BINDER's capabilities and could pave the way for future research."
            },
            "weaknesses": {
                "value": "1) Generalization to Unseen Concepts: While BINDER claims to generate embeddings for unseen concepts by using logical functions over existing concepts, it would be beneficial to provide more detailed explanations and examples of how this generalization is achieved. A concrete illustration of how BINDER generates embeddings for unseen concepts could strengthen the paper.\n\n2) Experimental Rigor: The paper mentions that BINDER is a randomized algorithm but provides results from the best run out of five. It would be helpful to include more detailed information on the variability observed in these runs, such as mean and standard deviation. A discussion of the algorithm's sensitivity to random initialization would also be insightful.\n\n3) Hyperparameter Sensitivity: The paper discusses hyperparameters like the learning rate and bias but does not delve into their sensitivity analysis. A study on how these hyperparameters affect BINDER's performance and convergence would provide a better understanding of its behavior.\n\n4) Comparative Discussion: While BINDER's strengths are well-discussed, it would be beneficial to have a comparative discussion with competing methods, highlighting where BINDER outperforms them in more depth. This would provide additional context for readers.\n\n5) The complexity. It seems that the model requires very high dimensionalty but there is no such discussion. The worst case is that the concepts are fully disjoint, then you need N dimension, which makes the model not scalable."
            },
            "questions": {
                "value": "1) Why does the accuracy (acc) decrease when the number of transitive edges increases? How can this phenomenon be explained?\n\n2) In equation (1), you specify that a $\\neq$ b, meaning when a = b, it should be considered a negative example. However, in Section 2.3, the negative pairs (n) involving reflexivity constraints, where a is-a a, are excluded. Does this mean that reflexivity constraints are neither treated as positive nor negative examples? I understand that in the experiments and method, you've avoided dealing with the self-relation, but in practice, it might occur. How do you plan to handle cases where the reflexivity constraint is present?\n\n3) In Section 2.3 (Training Algorithm), it might enhance clarity and conciseness by describing the sampling process as selecting $r$ from $W \\setminus \\{a, b\\}$. This would align well with the earlier statement that the number of negative examples is $n^2 - n - |P|$.\n\n4) In Table 3 (Reconstruction Results Acc(\\%) (dim)) and similar cases, what is the purpose of including \"(dim)\" in the table header?\n\n5) One notable advantage of BINDER is its ability to generate embeddings for unseen concepts using logical functions over existing concepts, a feature not present in its competitors.\n\n6) Could the finite permutation space of binary vectors result in a loss of expressivity, limiting the model's ability to capture complex relationships?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8183/Reviewer_EAos"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698496730611,
        "cdate": 1698496730611,
        "tmdate": 1699637014689,
        "mdate": 1699637014689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jhmvbutjO2",
        "forum": "zkE2js9qRe",
        "replyto": "zkE2js9qRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_JqKj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_JqKj"
        ],
        "content": {
            "summary": {
                "value": "This work proposes BINDER, an approach for order-based representation. BINDER uses binary bits as representation vectors, via a scalable optimization procedure. Authors evaluate experiments on both prediction and reconstruction tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, the paper is well-organized, and the authors provide a detailed description of their contributions."
            },
            "weaknesses": {
                "value": "1. The Introduction section is also missing an important recent work on two-view knowledge graph embeddings, which jointly embed both the ontological and instance view spaces: \n[KDD 2022] Dual-Geometric Space Embedding Model for Two-View Knowledge Graphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22). Association for Computing Machinery, New York, NY, USA, 676\u2013686. https://doi.org/10.1145/3534678.3539350\n\n2. It would be helpful if the authors could create an illustration of an example knowledge graph following their problem formulation. \n\n3. Further, the model fails to include important baseline models such as standard knowledge graph embedding model in the hyperbolic space e.g., RefH/RotH/AttH, hyperbolic GCN (HGCN), and the product space (M2GNN). \n\n4. Moreover, the size of the datasets also seem to be relatively small-scale with number of nodes and edges on the scale of thousands as opposed to million node/billion edge graphs indicative of real world KGs e.g., DBPedia & YAGO."
            },
            "questions": {
                "value": "Why is only the hyperbolic space being considered? Entities can form cyclic relations as well, which is better modeled in the spherical space.  Perhaps the authors need to more clearly denote the distinction between entities and concepts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698792625602,
        "cdate": 1698792625602,
        "tmdate": 1699637014554,
        "mdate": 1699637014554,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XQjIZiJz47",
        "forum": "zkE2js9qRe",
        "replyto": "zkE2js9qRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_VtjX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_VtjX"
        ],
        "content": {
            "summary": {
                "value": "In this work the authors develop a method for creating bit-vector representations of entities such that an order relation on bit-vectors captures some hierarchical structure. More specifically, the authors focus on representing hypernym ($\\texttt{is-a}$) relationships between entities. For a given set of entities $W$ and some set of $\\texttt{is-a}$ relationships expressed as pairs $P \\subseteq W \\times W$, the authors propose to represent each entity $a \\in W$ by a bit-vector $\\mathbf a \\in \\\\{0,1\\\\}^d$ for some $d$, such that\n\n$$\\mathbf b_j = 1 \\implies \\mathbf a_j = 1 \\quad \\iff \\quad (a,b) \\in P \\setminus \\Delta_W,$$\n\nwhere $\\Delta_W = \\\\{(a,a) \\mid a \\in W\\\\}$ is the identity relation on $W$.\n\nThe authors formulate a loss function for a bit-vector representation of entities which is a linear combination of a \"positive loss\", which counts the number of times $(a,b) \\in P$ and $a\\ne b$, but there is some $j$ for which $\\mathbf b_j = 1$ and $\\mathbf a_j = 0$, and a \"negative loss\", which counts the number of times $(a,b) \\in N \\subseteq W \\times W \\setminus (P \\cup \\Delta_W)$ are such that $\\mathbf b_j = 1 \\implies \\mathbf a_j = 1$. Since the representation is discrete we cannot take gradients of this loss function, so the authors propose an algorithm which randomly flips bits with a probability which is correlated with the amount of improvement in the loss function as a consequence of flipping that bit.\n\nThey evaluate their model on 5 hypernym datasets. They evaluate in both a reconstruction setting as well as a setting where edges from the transitive closure are removed during training and expected to be recovered during evaluation. They claim their model generally outperforms  baselines including order embeddings, Poincare embeddings, and hyperbolic entailment cones."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The authors do an admirable job presenting the background and motivation for this work. Their proposed model is explained clearly, and the randomized algorithm they propose is somewhat novel."
            },
            "weaknesses": {
                "value": "Unfortunately, there a many fundamental problems with this work.\n\nFirst, it is unclear to me what problem or task the proposed model is actually solving. What do we gain by representing entities with bit vectors capturing their hypernym relationships? In general, the motivation to embed entities in this setting is one of the following:\n1. Space Efficiency: The new representation requires fewer bits to store than some naive approach (eg. adjacency list of the transitive reduction)\n2. Computational Efficiency: There is some operation which can be performed on the embedded representation more efficiently than on some other representation\n3. Generalization: The embedding allows one to infer missing edges between existing nodes or make predictions of graph edges from unseen nodes (based on input node features)\n4. Transference to Other Tasks: The embedding captures the graph relationships which can then be plugged into other architectures for use in tasks which benefit from the knowledge of the graph structure (eg. MLP for classification)\n\nThe authors discuss space efficiency in Appendix F.3, however comparisons here are only made to other baselines, and the numbers quoted are far and above what would be required (eg. the authors claim that baselines with more than 100 dimensions take more than 10 hours to run, but this is far longer than the numbers reported in [0] and my personal experience suggests, where it is possible to train a model to represent WordNet reasonably well in 10-20 minutes). Comparing bit vectors to floating point models which were not quantized is disingenuous at best. The authors do claim their embedding is useful for is generalization, however the evaluation performed only assesses generalization to the transitive closure, which is trivial to perform symbolically on the set $P$ which would result in perfect accuracy on this evaluation. There are also issues with this evaluation separately, which are addressed below, but fundamentally this task is not truly a test of generalization in any useful sense.\n\nThe authors do claim that Binder embeddings have some unique capabilities unavailable to other models. Specifically, they claim that Binder embeddings have a well-defined complement, union, or intersection, however this is not true, or at least not any more true here than in any other embedding method. The authors even state that if \"we have a concept 'living-thing' for which we have a binary vector representation, [...] if we want to obtain a representation for 'not living things' we can obtain that simply by reversing the bits of the 'living thing' vector', however this is not true. To see this, consider a \"living thing\" vector as $[0,1,1]$, then based on the authors' embedding definition the set of living things is $\\{[0,1,1],[1,1,1]\\}$. By their claim, the representation of \"not living thing\" should therefore be $[1,0,0]$. This would mean that the set of living things includes the bit vectors $\\\\{[0,1,1],[1,1,1]\\\\}$ and the set of not living things is $\\\\{[1,0,0],[1,1,0],[1,0,1],[1,1,1]\\\\}$. Note that this means that the bit vector $[1,1,1]$ is both living and not living. Moreover, it also means the space is not decomposed into just \"living thing\" and \"not living thing\" - for example, the vector $[0,1,0]$ is neither living or not living. Therefore this definition of complement is not correct. Not only that, there is *no* bit vector which captures the full complement of being a living thing, because to not be a living thing, according to their definition, we simply need to have a zero in the first or second position, and there is no way to express this \"or\" condition with a single vector. A similar argument shows Binder embeddings are not closed under union.\n\nSecondly, even if there is some benefit to representing entities by bit-vectors, it is straightforward to provide a deterministic algorithm which takes a set $P$ and produces a bit-vector embedding which perfectly satisfies the constraint above using a topological sort. With some additional care in the construction process, it even seems possible to create a bit-vector with minimal size which perfectly satisfies the constraint. Therefore, the use of a randomized algorithm here does not seem to have any benefit.\n\nThirdly, there are a number of problems with the experiments. For some reason, the authors chose to report a reweighted accuracy statistic as opposed to the more conventional F1 metric when dealing with data imbalances. In addition, the authors evaluate on a test set with negatives which were created by random perturbation, however this approach can lead to a very coarse evaluation, and has issues with test set bias. For the test set accompanying Order Embeddings paper, for example, you can get almost 0.90 F1 by simply treating any node in the training data which has a child as though it is a parent to every other node in the training set. It was for this reason that more comprehensive evaluations advocate for using the full adjacency matrix [0]. In addition, the other models present in that paper all serve as reasonable baselines, and the [associated code](https://github.com/iesl/geometric-graph-embedding) has implementations readily available.\n\nFinally, a number of the characterizations or claims made in the introduction are incorrect. The authors claim optimization algorithms are not well studied for hyperbolic space, however this is not the case - Riemannian gradient descent is well understood ([1], [2], [3]). Moreover, there are approaches to parameterizing and training on hyperbolic space which have been shown empirically to work well with standard gradient descent techniques such as SGD or Adam [4]. The authors claim box embeddings have more degrees of freedom than point embeddings, but this is not true - a box embedding in $d$-dimensional space does have $2d$ parameters per box, but it is for this reason that experiments using box embeddings compare $d$-dimensional boxes to $2d$-dimensional vectors, so they have exactly the same number of free parameters. The claim that bit vectors are more interpretable is not supported by any experiments, and there is no clear reason to expect that the randomized algorithm leads to interpretable properties in each dimension. The interpretability hinted at for the bit vectors is equivalent to the level of interpretability that order, probabilistic order, or box embeddings provide.\n\n[0] Boratko, Michael, et al. \"Capacity and bias of learned geometric embeddings for directed graphs.\" Advances in Neural Information Processing Systems 34 (2021): 16423-16436.\n[1] Bonnabel, Silvere. \"Stochastic gradient descent on Riemannian manifolds.\" IEEE Transactions on Automatic Control 58.9 (2013): 2217-2229.\n[2] B\u00e9cigneul, Gary, and Octavian-Eugen Ganea. \"Riemannian adaptive optimization methods.\" arXiv preprint arXiv:1810.00760 (2018).\n[3] Hu, Jiang, et al. \"A brief introduction to manifold optimization.\" Journal of the Operations Research Society of China 8 (2020): 199-248.\n[4] Law, Marc, et al. \"Lorentzian distance learning for hyperbolic representations.\" International Conference on Machine Learning. PMLR, 2019."
            },
            "questions": {
                "value": "The section on the weaknesses highlights my concerns with this work. \n\n1. Can you clarify the specific problem or task that your proposed model is designed to solve? How does the use of bit vectors capturing hypernym relationships contribute to solving this problem?\n\n2. Regarding space efficiency, how does the bit vector representation compare to a sparse adjacency list? If it is not more compact, does it offer any benefits beyond the sparse adjacency list?\n\n3. In terms of generalization, the current evaluation focus on the transitive closure. When training on this data in the 0% case, does your negative set include edges from the transitive closure? Regardless, if we know the relation is transitive, what benefit do we gain by training on the transitive reduction and being able to \"generalize\" to the transitive closure which is not also achievable by simply taking the transitive closure of the training data?\n\n4. You mention unique capabilities of Binder embeddings, such as well-defined complement, union, or intersection operations. Given the issues highlighted with these operations, how do you respond to the concerns about the correctness of these claims?\n\n5. Could you elaborate on why a randomized algorithm is used for generating bit-vector embeddings when a deterministic algorithm could suffice?\n\n6. Why was a reweighted accuracy statistic chosen over the conventional F1 metric in your experiments, especially in the context of data imbalances?\n\n7. Please correct or respond to my assertions above regarding the inaccuracies in characterizing other baselines. (For example, the assertion that optimization algorithms in hyperbolic space are not well-studied,  or that box embeddings have more degrees of freedom than point embeddings.) After correcting these claims, please address what specific benefits this embedding provides beyond those provided by the baselines.\n\n8. The interpretability of bit vectors is claimed to be superior in your paper. Can you provide empirical evidence or a theoretical framework that supports this claim, in a setting where equivalent effort is also given to order, probabilistic order, or box embeddings?\n\n9. Would you consider evaluating the representational capacity of your model on the full adjacency matrix? This may be computationally prohibitive; if so, it is reasonable to select a subgraph (eg. Animal subgraph from WordNet) and evaluate the full adjacency matrix on that subgraph.\n\n---\n\nI apologize if my review seems harsh. I would like to commend the authors for the clear and structured presentation of their approach. The manuscript is well-written, and the methodology is articulated with a level of detail that reflects a thorough understanding of the subject matter. It is evident that considerable effort has gone into developing and describing the proposed model.\n\nOne of the main challenges in this area of research seems to be a legacy of ambiguity in motivations and intentions from previous works, like a bad game of \"telephone\". Previous evaluations designed to highlight specific aspects of a model may be misconstrued to be a task in and of themselves, and this problem can compound on itself in subsequent work. It is also possible that I misunderstood the author's motivations and approach, and if so then I humbly apologize and ask the authors to clarify things for me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699042810956,
        "cdate": 1699042810956,
        "tmdate": 1699637014430,
        "mdate": 1699637014430,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OdHrnWlEb1",
        "forum": "zkE2js9qRe",
        "replyto": "zkE2js9qRe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_psGs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8183/Reviewer_psGs"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method called Binder, which is a hierarchical concept representation method through order embedding of binary vectors. The paper explores the importance of order-based representation in natural language understanding and generation, and discusses the strengths and weaknesses of existing approaches. It also describes the geometric constraints imposed by order-based representation and how they capture semantic relationships between concepts. The paper concludes by discussing potential applications of Binder's approach to hierarchical concept representation in practical natural language processing tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes BINDER, a novel order embedding approach which embeds the entities at the vertex of a d-dimensional hypercube, which is simple, elegant, compact and explainable.\n\n2. The paper proposes an optimization algorithm for BINDER, which is simple, efficient, and effective, and can be seen as a proxy of gradient descent for the combinatorial space.\n\n3. The experimental results show that BINDER achieves great performance on link prediction and reconstruction tasks."
            },
            "weaknesses": {
                "value": "1. For reconstruction task, OE achieves better performance than BINDER with fewer dimension. Thus, BINDER does not show superiority over OE.\n2. BINDER may still suffer from the limitation of optimization, leading to inferior performance.\n3. It is better to report the mean results, rather than the best results."
            },
            "questions": {
                "value": "1. You claim that \u201cIn BINDER\u2019s embedding, an \u20181\u2019 in some representation dimension denotes \u201chaving a latent property\u201d. How to verify it through experiments?\n\n2. What are the differences between the proposed optimization method and randomized local search algorithm? What is the novelty of the proposed optimization method?\n\n3. How to ensure the convergence of the proposed optimization algorithms?\n\n4. Why BINDER is better than OE in WordNet Nouns dataset?\n\n5. How is the dimension (in parenthesis) in Table 3 set? Why is the dimension of OE smaller than that of BINDER?\n\n6. Can you provide the experimental results of box embedding?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8183/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699176535017,
        "cdate": 1699176535017,
        "tmdate": 1699637014327,
        "mdate": 1699637014327,
        "license": "CC BY 4.0",
        "version": 2
    }
]