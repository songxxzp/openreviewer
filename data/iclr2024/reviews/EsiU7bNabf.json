[
    {
        "id": "tYPMlomf5g",
        "forum": "EsiU7bNabf",
        "replyto": "EsiU7bNabf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_oSbm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_oSbm"
        ],
        "content": {
            "summary": {
                "value": "The authors are studying the problem of identifying which tasks should be mixed together during training such that there is a positive transfer between these tasks and they help each other to improve. The authors first assume access to a pairwise task affinity matrix which specifies the improvement on task j when task i is trained before it. Assuming access to this task-affinity matrix they formulate the clustering problem in terms of maximizing the average density of task affinity scores across all clusters which is NP-hard. Then the authors use SDP relaxations to arrive at an approximate task clustering algorithm AdaGroup. They discuss and compare spectral and Lloyd clusters as two other approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "S1. The problem of task clusters is extremely important and hard to work with because the ground truth clusters are mostly not available. Grouping tasks appropriately can lead to significant improvements in the final model's performance.\n\nS2. The method is well-motivated and can adaptively add new tasks to the existing cluster.\n\nS3. The authors have computed the pairwise affinity of some NLP tasks and released them as a benchmark so that the community can try out different ideas to cluster tasks."
            },
            "weaknesses": {
                "value": "W1 The paper is slightly hard to understand, especially the experimental section. Many statements about \n\nW2. New tasks can be processed in a batched manner to avoid expensive computations, however, the number of clusters are usually predefined and new tasks cannot allocate new clusters. This seems like a problem to me, some non-parametric Bayesian processes like the Chinese restaurant process, and Indian buffet process can potentially be used to solve this issue. Defining the number of clusters can be a challenging as well as computationally expensive process as multiple models are needed to be trained to obtain the clusters for a new set of tasks. \n\nW3. The paper seems to talk about spectral and LLoyd clusters but in Table 2, I don't see them as a baseline. For example, you could use the clustering obtained from spectral and Lloyd algorithm and then perform training similar to your method. This I feel is an important baseline as it would tell us the impact of identifying wrong task clusters. It might also be the case that the impact of having wrong clusters is not too high, i.e. some decent amount of noise in the clusters does not impact the final downstream performance."
            },
            "questions": {
                "value": "**Must do for me to retain my scores**\n\nQ1: Please improve the writing of the in-context learning experiments, I am not able to understand the motivation of, the experimental setup, and the conclusions for it. I am still not sure if I understand where logistic regression and decision trees come into this picture and the implications of the designed experiments. \n\nQ2: same as W3. W3 needs to be addressed for retaining the score.\n\n \n**Answer these for me to consider increasing my scores**\n\nQ3: On the created benchmark AdaGroup can identify all the clusters correctly. It would be nice to see if you take some heldout tasks, and then use your AdaGroup to cluster these tasks and visualize the cluster/affinity scores. It would be nice to see how much these clusters correlate with human understanding. If this correlation is high then for a small number of tasks doing the clustering manually might be a reasonably good choice. \n\nQ4: Given that the method needs to train multiple models in order to estimate the task affinity matrix, this method might not be feasible in cases where the datasets are pretty huge. It would be a really nice study to see what is the minimum number of samples from each task that you can use in order to get reliable task affinity scores that can lead to good clusters and improved downstream performance. This would ameliorate the costs associated with this method and make it scale to more number of tasks.\n\nQ5: A solution to the cluster number number problem, see W2 for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773245563,
        "cdate": 1698773245563,
        "tmdate": 1699636729164,
        "mdate": 1699636729164,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g4woffJT4H",
        "forum": "EsiU7bNabf",
        "replyto": "EsiU7bNabf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_5Hpz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_5Hpz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approximate clustering algorithm, along with an evaluation benchmark, that aims to group tasks for language model instruction fine-tuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The new clustering algorithm achieves good fine-tuning results, which outperforms multi-instruction and prefix tuning by 3.3% on multiple tasks."
            },
            "weaknesses": {
                "value": "1) The paper utilizes SDP relaxation as an approximate clustering technique; however, it does not provide an analysis of the error bound that comes with using this method. To provide a better understanding of its applicability, it is essential to understand the worst-case scenario and where the SDP relaxation clustering algorithm fails.\n2) The generalization of the SDP relaxation clustering algorithm is not analyzed in this paper, leaving questions about whether it is a general clustering solution that can approximate spectral clustering or if it only works for instruction fine-tuning.\n3) While the paper proposes using an approximate solution, it does not provide clear explanation as to why the SDP relaxation clustering algorithm will outperform spectral clustering. Is it related to the clustering objective function definition issue?\n4) It is not clear how to ensure that the output of the convex optimization can follow the ranking constraint. The paper could provide a more detailed exploration of this aspect of the algorithm.\n5) The paper does not analyze the impact of different lambda values for different tasks. Is lambda the same in all experiments, or should it be tuned in each experiment?\n6) The hyper-parameters, including k, m, s, and alpha, are not thoroughly studied. The paper could explore how to choose these values, such as whether they should be tuned for the particular task or selected according to some rules.\n7) The paper proposes an adaptive estimation of task affinities; however, there is no analysis of how different sample sizes affect the final estimation accuracy. Examining how the estimation accuracy impacts the final results could provide deeper insights into the method's performance.\n8) It would be helpful if the paper could provide the results of the spectral clustering approach and compare it with the multi-instruction and prefix tuning methods to give a more comprehensive assessment of the new clustering algorithm's performance."
            },
            "questions": {
                "value": "1) Please address the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6502/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6502/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6502/Reviewer_5Hpz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698774509021,
        "cdate": 1698774509021,
        "tmdate": 1699636729041,
        "mdate": 1699636729041,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "deweCxgf46",
        "forum": "EsiU7bNabf",
        "replyto": "EsiU7bNabf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_zYRS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6502/Reviewer_zYRS"
        ],
        "content": {
            "summary": {
                "value": "This paper starts from the well-known task grouping problem in Standley et al., 2020 and studies the following formulation: Given n tasks, find a partitioning of them into k groups so that each group of tasks can be best trained together (separately from the other groups)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The task grouping problem is indeed an important problem for language models"
            },
            "weaknesses": {
                "value": "- The approach is missing a rigorous complexity analysis which for this topic seems very important\n- The analysis seems incmplete. For xample \"Ablation Studies,\" particularly in the subsection on \"Instruction Selection,\" the authors compare the accuracy of the fine-tuned models resulting from their method and those obtained using other clustering algorithms. Considering providing a more comprehensive evaluation, it would be better if authors could also include a comparison of the time taken by each method.\n- The clustering / aggregation approach and overall the prior works are poorly discussed and contrasted, especially recent mathematical approaches  that exploit advanced concepts like curvature and others."
            },
            "questions": {
                "value": "1) In the \u201cTask Grouping Setup.\u201d \u2013 page 3 and other parts of the paper, it would be useful if the authors could make it clearly whether the tasks are considered independent or there is some form of dependency and how it is captured beyond the task affinity matrix.\n2) I have read many vague statements like \u201cThe computational cost of these techniques can still be quite high for the scale of instruction finetuning\u2026\u201d but no precise complexity analysis results. Can the authors provide at least in some cases concrete numbers?\n3) The authors recognize that the clustering is a well-studied problem, they mention approaches based on SDP and Linear Programming relaxations, but it seems they have missed recent efforts on optimal transport theory like the Ollivier-Ricci curvature in \u201cOllivier-Ricci Curvature-Based Method to Community Detection in Complex Networks\u201d (Scientific Reports 2019) or \u201cInferring functional communities from partially observed biological networks exploiting geometric topology and side information\u201d (2022) and others which bear some analytical similitudes to LP relaxations. It would be fair to mention these theoretical works in clustering for weighted graphs as they are very much competitive or potential approaches for the problem studied here.\n4) The fact that the \u201caffinity matrix, in our case, can easily violate the triangle inequality\u201d made me wonder if these Ollivier-Ricci curvature approaches that deal with advanced geometric concepts could be useful to this problem.\n5) This is a minor issue, SDP is used in section 2 but only defined later.\n6) In section 4.2, the authors should give the full name of the \u201cMTL\u201d to make readers better understand the meaning of \u201cMTL performance\u201d.\n7) In Figure 2, the authors should include subscripts or labels to distinguish the results obtained from the three different methods. \n8) The authors mention their use of high-order task affinity and adaptive sampling as methods to expedite the calculation of task affinities. In comparison to the direct training of n^2 models, it would be beneficial if the authors could quantify the time savings achieved through their proposed method. Clarification on this matter would greatly enhance the comprehensibility of the paper.\n9) In Section 5.4, \"Ablation Studies,\" particularly in the subsection on \"Instruction Selection,\" the authors compare the accuracy of the fine-tuned models resulting from their method and those obtained using other clustering algorithms. Considering providing a more comprehensive evaluation, it would be better if authors could also include a comparison of the time taken by each method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6502/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698796546830,
        "cdate": 1698796546830,
        "tmdate": 1699636728937,
        "mdate": 1699636728937,
        "license": "CC BY 4.0",
        "version": 2
    }
]