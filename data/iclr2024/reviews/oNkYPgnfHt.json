[
    {
        "id": "Qagx0NM2do",
        "forum": "oNkYPgnfHt",
        "replyto": "oNkYPgnfHt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_XnGw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_XnGw"
        ],
        "content": {
            "summary": {
                "value": "Concept Bottleneck Models (CBMs) are an increasingly popular model class, designed to be more interpretable \u2013 and importantly *intervenable* by human users. However, querying humans for interventions on these models can be expensive, and models may make the same mistake repeatedly, resulting in repetitive interventions needed by users. In this work, these authors call attention to this problem and propose a new method \u2013 CB2M \u2013 to reconcile lack of reuse of interventions. CB2M is a modular extension to CBMs which leverages two memory banks: one which helps the model identify when a mistake is likely made in the output, and a second which reuses past interventions to correct such a mistake. The authors demonstrate the potential utility of CB2Ms across a range of experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The motivation for the work is superb. The authors call attention to an incredibly important and under-recognized problem in CBMs: that models may make the same mistake repeatedly, and requiring humans to inculcate the same intervention over-and-over again can be cognitively demanding \u2013 and ought to be unnecessary. The authors\u2019 proposed method is clever and has the potential to have great impact in the broader CBM community. I believe the authors offer value to the ICLR \u2013 and broader human-centric ML \u2013  communities by 1) calling attention to this reuse problem, and 2) offering a first possible solution. The paper is also very well-written."
            },
            "weaknesses": {
                "value": "While I believe that simply calling attention to the reuse problem in interventions, coupled with their method proposal, holds value for the broader community \u2013 I do not think that the experiments in their current form sufficiently demonstrate the value of CB2M. Experimental validity therefore holds me back from assigning a higher overall score. I believe sizable further experiments are needed to really strengthen the work (or at least clarification on the current interpretation). \n\nFirst, I am confused as to why the performance in Table 1 is lower for CB2Ms in the Full versus the Identified sets. Are examples in the Identified set those in which the memory module predicts that the example is misclassified? If so, why is the baseline CBM task performance so high (this would imply a high false positive rate?) It would be good for the authors to expand on possible False Positive and False Negative rates of the memory module, for each domain. \n\nSecond, the authors do not discuss the impact of the size of the memory module on performance. It would be good for the authors to have some kind of experiment(s) looking into the impact of thresholded sizes on the memory and intervention modules, as in practice, it\u2019s possible that we may not be able to store all past instances? \n\nThird, I am not convinced that the authors\u2019 selection of baselines is adequate. As the authors detail in the Related Work, there have been several efforts to learn intervention policies (e.g., CooP) which select which next example to query people over. Yet, the authors never compare to any of these policies. None of these prior policies, to my understanding, leverage reuse of past interventions \u2013 as such, the authors could make the case that their method is complementary to these approaches; i.e., could be combined with methods which learn to intervene, which could justify not including such a baseline. However, the authors do not make such claims. I would be keen for the authors in the rebuttal to expand on how their work relates to other intervention policies and why they did not compare against them as baselines. \n\nFourth, the authors emphasize that their approach is model-agnostic. However, all experiments are with a CBM backbone. In the absence of augmenting other concept-based systems with their modules, I do not think the authors should claim their method is model-agnostic. If the authors would like to emphasize this claim, I believe experiments are needed with at least one other concept-based system. Otherwise, I think it is fine to leave for future work, but the text should be couched as such. \n\nLastly, I do not think the authors adequately discuss the limitations of their work (see Questions below). It would be good for there to be a dedicated Limitations section, or at least further prose on the matter."
            },
            "questions": {
                "value": "I have raised most of my important questions in the Weaknesses section. In addition: \n\n- I am confused and concerned by Table 9 in the Appendix. The standard deviation is massive for CUB in particular. The authors note that the wide variance could be due to the threshold selection. However, it\u2019s not clear to me why the threshold selected across the 5 seeds would vary so much that it leads to this level of variance in the number of generalized interventions? What is the variance in the selected threshold (can you please provide example threshold values?) and/or further explanation of what is happening here? \n- It\u2019s not clear to me that CB2M is better than softmax at detecting (with the exception of the Parity C-MNIST domain). The performance of CB2M and softmax are within error bounds for CUB. Can the authors expand on why this may be further? \n- The authors\u2019 assumption of perfect humans is sensible for this work. However, I would encourage the authors to think about how their module(s) may be challenged if human interventions are incorrect (or uncertain \u2013 e.g., Collins, Espinosa-Zarlenga et al, \u201cHuman Uncertainty in Concept Based Systems\u201d AIES 2023). Such challenges would be worth expanding on in a Limitations section (of which the authors do not suitably have here). \n- As a minor sematic note (which does not affect my score): the authors caption Fig 3 as \u201cLess is more\u201d \u2014 but really, this shows that the less is enough / sufficient to achieve high task performance; not that less is more than having further interventions. I\u2019d encourage the authors to change that caption :) \n- As another note, which does not impact my score, but would be nice for a revised version: have the authors looked at qualitative examples of the interventions / mistakes captured in the module? (e.g., Fig 2 of Chauhan et al, 2022)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Reviewer_XnGw",
                    "ICLR.cc/2024/Conference/Submission2489/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698059753650,
        "cdate": 1698059753650,
        "tmdate": 1700665341567,
        "mdate": 1700665341567,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PE8xU15GzG",
        "forum": "oNkYPgnfHt",
        "replyto": "oNkYPgnfHt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_oJVX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_oJVX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an extension to CBM architectures called CB2M in which interventions are not just used once, but rather stored in memory and reused on test data to improve performance and detect possibly similar errors. Results show their method is better than normal CBMs at doing this."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper has an interesting idea, I like the notion of learning from human feedback and improving over time or fixing edge cases in which the neural network is making the same mistakes over and over again.\n\nI also like the possibility of human-AI collaboration here where we could e.g. detect errors humans and/or AI are making at test time to try and make up the difference between the two. I certainly think this direction has a lot of potential and will form an important part of explainable ML going forward.\n\nEssentially this all comes from storing these cases in memory, but it's worth mentioning that this isn't the most novel idea (a lot so similar work exists in CBR etc.), but in this context of CBMs it is reasonably interesting."
            },
            "weaknesses": {
                "value": "The weakness of the paper is the evaluation I feel. The authors setup a few situations on a few common datasets to show the utility of their method. However, none of these experimental setups are particularly compelling, and somewhat contrived. I'm not sure if the point of the method is to increase model performance, or HCI, etc...\n\nFrom a performance perspective, take the CUB dataset, SOTA on this dataset is (last I checked) 92%+, but here their method is 88.7%. I understand that raw performance is probably not your goal here, but if you're using accuracy to assess the usefulness of your method, then this isn't really very compelling, as I can't e.g. use CB2M to squeeze more accuracy out of my models, so I am left wondering how it would be useful there. What would be great is to show you could break this 92% ceiling with your method, human feedback, and interventions etc...\n\nIn another vein, the human interventions are simulated, which again makes me wonder if humans could actually interact with the method how the authors propose they could.\n\nIf the authors could show their method e.g. working with a doctor to improve team performance overall, or just improve accuracy over a standard black-box, that would be very exciting, but they don't. So, I am left wondering what the application of this is at all. \n\n### Small things\n* The first two figures don't explain $f$ or $g$, the figures should stand alone usually.\n* Page 2: this issue (2) by... should be -- this issue by (2)....\n* I would tend to axe the third contribution on page 2, it's just experiment results which is expected.\n* It's not clear where $x_e$ is taken.\n* Second paragraph on page 4 would probably help the intro motivation.\n* $t_d$ needs to be clearly explained how the value was taken (unless I missed it sorry)\n* Eq 2: I wouldn't use $val$, it reminds me of \"validation\" personally, which is confusing."
            },
            "questions": {
                "value": "* What is a real-world application of this method that could make people in the ML community genuinely excited? Something were the method could be shown to be *understandable* and *useful* to intended practitioners of the system.\n* See above for my other general critiques.\n\nOverall, I like the paper's core idea, and I veer slightly (although just slightly) towards acceptance, but I will mutate this after the next phase depending on my interactions with the AC, reviewers, and authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Reviewer_oJVX"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681900930,
        "cdate": 1698681900930,
        "tmdate": 1699636185465,
        "mdate": 1699636185465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X2kua0jJAr",
        "forum": "oNkYPgnfHt",
        "replyto": "oNkYPgnfHt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_fRVj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2489/Reviewer_fRVj"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Concept Bottleneck Memory Models (CB2Ms), a new model-agnostic extension of Concept Bottleneck Models (CBMs) in which an adaptive memory is incorporated to improve a CBM\u2019s receptiveness to test-time interventions and its uptake of feedback at test-time. Through two sets of distance-based memory banks, namely an *intervention memory* and a *mistake memory*,  CB2Ms learn to identify potentially mispredicted samples and reapply previous interventions to automatically improve the concept and task accuracy after only a handful of test-time interventions have been performed. This work evaluates CB2Ms on four datasets (two MNIST-based datasets and two real-world datasets) and shows that the proposed extensions enable CBM-based models to significantly boost their intervention performance and their ability to be more robust to distribution shift and train-time spurious correlations."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Thank you for submitting this work! I believe this is a very interesting idea and something that has certainly not been carefully explored in the concept-based literature before. Explicitly, I believe the following are the main strengths of this paper:\n\n1. **Originality**: the idea of incorporating a test-time adaptive memory to improve the uptake of intervention feedback and avoid discarding potentially useful information provided at intervention-time, is certainly original in the field of concept-based explainability. Furthermore, the work is well-placed within this area with a set of diverse related works discussed in this paper and potential ideas mentioned in the end discussion.\n2. **Quality**: although I believe the experimental set-up could have benefited from a more careful design (see below), the quality of the presented idea, as the presentation of the idea itself, is up to the standards of work in this conference and community.\n3. **Clarity**: the paper\u2019s writing is very easy to follow, with almost no typos and a structure which makes the reading flow easily from beginning to end. This helps the authors clearly communicate their ideas and the motivation behind the ideas. Furthermore, the inclusion of a code base helps to understand the clarity of this work and promotes reproducibility, both highly desirable features.\n4. **Significance**: the paper\u2019s main contribution, that of a model-agnostic mechanism to take test-time feedback into account when considering future interventions, is certainly significant in the concept-based literature and may lead to further advancement in the near future. Nevertheless, this significance is contingent on a careful and fair evaluation of the proposed method (see weaknesses below). If the doubts I have regarding this paper's experiments are carefully discussed/corrected, and the paper\u2019s main claims still hold, then I believe this paper would be of good value to the community."
            },
            "weaknesses": {
                "value": "Although I think this paper has several strengths, as outlined above, I am concerned about the fairness of its evaluation and the lack of baselines that would be relevant comparison methods with what is presented in this manuscript. In particular, I believe these are the paper's main weaknesses:\n1. [Critical] My biggest concern with this paper is its evaluation. In my opinion, it is not fair to evaluate a method like CB2M, which can take and store extra samples at test time to improve performance on future unseen samples, against methods that completely ignore the same feedback even if in theory it could be used to improve their performance as well. For example, given that the validation set is used to construct the initial mistake memory, at the very least I would expect to see as a fair baseline a CBM that was able to update its weights using feedback from the same validation set during training (as otherwise CB2M is unfairly being trained on more data than the CBM baseline!). Similarly, when considering how the intervention memory is used, it would be fair to have as a baseline a CBM whose weights are updated at test time so that feedback from a new intervention is considered in the model. This can be done, say, via a variety of online learning algorithms that aim to minimize the cross entropy loss of the corrected concept prediction given the provided ground-truth (intervened) label (or even a baseline that runs a few gradient decent steps on this new intervention's label and corresponding sample). Having such a baseline and showing the CB2M still beats that baseline would provide very strong evidence that the memory mechanism introduced in this paper is worthwhile and novel compared to methods that already exist there to address similar problems.\n1. [Critical] Regarding the evaluation of the method on unseen data points (e.g., Table 1), it is unclear whether it is fair for ECB2Ms to be able to use the entire (unmodified) test set as part of their memory loading while none of the baselines can have the same benefit at training time. Evaluating CB2M and other baselines on a modified version of this test set seems like a very unfair comparison if none of the other methods were able to obtain any feedback from simulated interventions on the unmodified version of these samples. It may be fairer to avoid any sort of leakage from the test set into the training set of CB2M by avoiding loading into its initial memory samples that are highly related to the ones it will be evaluated on (this can be done by splitting the test set into two). I understand this is how this method may be used in practice (with the memory taking advantage of test-time samples to build a database of interventions and mistakes),; comparing it against a CBM's results on the same table makes it seem like this is an apples-with-apples comparison when in reality it may not be such. Making this distinction clearer, and if possible the evaluation fairer, would help a lot to indicate how this method works and how it is expected to be used.\n1. [Critical] With the exception of C-MNIST, the results shown in the identification of mistaken samples do not appear to be statistically significant (see the standard deviations). Because of this, it is unclear whether the proposed method identifies errors better than, say, the naive softmax baseline. Similarly, the effect on generalized interventions based on CB2M's detection on the full dataset (Table 3) does not appear to be statistically significantly better than what one observed when using the Softmax detection for all datasets (see standard deviations). This casts some doubt on the usability of this method.\n1. [Major] The use of a dynamic memory in CB2M implies that this method will either (a) struggle to scale to large concept spaces or spaces with a lot of variability in concepts (as it will require a significant number of examples before capturing the variance of the concept space and this requires CB2M to store all these sample's embeddings), or (b) require one will have to cap the size of the memory, leading to another hyperparameter that needs fine-tuning. As discussed in the questions below, the size of the memory may also lead to intractability at test time due to a large search space needed when correcting a mistake that was identified via the mistake memory. These aspects are not discussed anywhere in this paper.\n1. [Major] The proposed method depends on three hyperparameters that are reportedly crucial for the end performance of the model, namely $t_d$, $t_a$, and $k$. Nevertheless, I could not find a reference anywhere in this paper on how these hyperparameters are selected for the experiments reported. Notice that the mechanism to fine-tune $t_d$ based on a validation set is explained near Equations 2 and 3. However,  the actual values used to perform this validation-based search are not reported anywhere for the experiments in this paper. Furthermore, the dependency of $t_d$ with $t_a$ and $k$ is also not elaborated anywhere in the main text (although it is understood that $k = 1$ when doing generalized interventions).\n1. [Major] Related to the point above, there are no ablations showing how sensitive the results reported in this paper are to these hyperparameters and how important the validation set is to fine-tune them correctly. This hinders the understanding of how this method would fare in practice and how easy it is to use.\n1. [Major] It is unclear how the size of the validation set affects any of the results observed. Similarly, it is unclear how the number of test-time interventions affects the results seen (a crucial element to understand given the role these interventions take in improving the method's future test-time performance). More importantly, no ablations are provided to answer these important questions."
            },
            "questions": {
                "value": "Given my concerns outlined in the weaknesses above, I am leaning towards rejection at the moment. Nevertheless, I am more than happy to be proven wrong or corrected if I misunderstood a crucial part of this work. With this in mind, I hope the following questions, in no particular order, would help clarify some of the doubts on this work. If possible, I would appreciate it if the authors could elaborate on these concerns as they may serve as a good starting point for a discussion during rebuttal:\n1. Regarding my concern about the fairness of the evaluation, could you please let me know if I am misunderstanding something here? If not, could you please elaborate on how CB2Ms would fare against similar (fairer) baselines as the ones discussed in the weaknesses?\n1. Regarding my concern on the results of Table 1: could you please elaborate on why it would not be more fair to perform the evaluation on a dataset of samples whose unmodified versions have not been used to set up the initial memory of CB2M?\n1. Regarding my comments on the weaknesses for Figure 3: how would the curve shown for CB2M look vis-a-vis that of a vanilla CBM in which multiple groups are randomly intervened on? I am trying to fully understand how these results are unexpected or different to those seen on CBMs and CEMs in previous works (e.g., Shin et al. and Chauhan et al., both works cited on this paper).\n1. Could you please elaborate on the importance of the validation set size and the number of test-time interventions before evaluation on the results presented in this paper?\n1. Could you please elaborate on the hyperparameter selection process for the experiments in this paper (see weakness above)?\n1. Do you have a sense of how sensitive CB2Ms are to their hyperparameters? Are there any good strategies to select these hyperparameters?\n1. Similar to the question above, could you elaborate on how important the memory size is for this model? Are there any ablations on how memory size affects the results? It is unreasonable to assume that one can have a boundless memory for CB2M; therefore, fully understanding this question is essential.\n1. Similarly, how is the inference wall-clock performance affected by introducing the memory banks at inference time? I imagine there will be a hit but fully understanding how significant this hit is before this method becomes intractable is absolutely crucial for understanding its weaknesses and strengths.\n1. Just to confirm: is it the case that during the evaluation of CB2M the memory is left unmodified once evaluation starts on the test set? I would expect that to be the case for this evaluation to be fair, however I could not easily find this detail in the paper.\n1. For the results of Table 4, how many test interventions are needed for CB2M to achieve the observed results? I could not find this detail easily.\n\nBesides these questions, I found the following typos/minor errors that, if fixed, could improve the quality of the current manuscript:\n1. Page 4: \"Thus, with the ability to handle task (i)...\" seems to be missing something before the enumeration \"(i)\" begins.\n1. Page 8: \"improve a model via on the detected mistakes\" should probably be \"improve a model via the detected mistakes\"\n1. Page 8: Missing space in \"improvements.Even\"\n1. nit on Page 8: closing quotation is used for the beginning of \"full\" instead of the opening quotation (`` in LaTeX)\n1. Page 8: period used instead of comma in \"...the distribution shift. indicating...\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2489/Reviewer_fRVj"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2489/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798910978,
        "cdate": 1698798910978,
        "tmdate": 1700663428309,
        "mdate": 1700663428309,
        "license": "CC BY 4.0",
        "version": 2
    }
]