[
    {
        "id": "zZTQwfoOTf",
        "forum": "LfDUzzQa3g",
        "replyto": "LfDUzzQa3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
        ],
        "content": {
            "summary": {
                "value": "The authors proposed a new speech tokenization for semantic modeling in this paper. Previous methods usually use k-means to discrete semantic representation, leading to information loss. Inspired by the audio codecs which reconstruct the raw audio, RepCodec encodes speech semantic representation from Hubert or data2vec to a set of vector quantization codebooks and reconstructs them by a decoder. The experiments demonstrate the superior performance of RepCodec in speech understanding and generation. Many detailed experiments also evaluate the performance in different configurations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The motivation and description of the proposed method are very clear and easy to understand. The experiment is sufficient and demonstrates the effectiveness of RepCodec."
            },
            "weaknesses": {
                "value": "1. In the introduction, the term \"semantic quality\" is mentioned. For clarity and the benefit of readers, could you provide a definition on what \"semantic quality\" encompasses?\n2. In previous works such as VALLE, AuidoLM, and PolySpeech mentioned in this paper, they all use the k-means clustering method to obtain semantic tokens. For the speech generation task (TTS, VC), the semantic information is important in discrete tokens, but the other \nencoded information such as speaker timbre is harmful to the task. Given the importance of speaker timbre information for downstream speech generation tasks, it would be of great value if a verification for speaker information of the encoded features is incorporated."
            },
            "questions": {
                "value": "1. Section II.Speech Tokenization. \"However, the discretization step of k-means discards plenty of information of the speech\u201c. Can you give some examples so that readers can better understand the limitations of k-means?\n2. The first paragraph in Sectiion III. \"In AudioLM (Borsos et al., 2023), the WER is dramatically increased from 2.5% to 6.0% by using the discrete tokens of k-means from w2v-BERT XL\". What does this number 2.5% refer to? \n3. In Equation 5, some symbols are not defined. \n4. One question not related to the proposed method.  Why does the performance gap change among different speech encoders after VQ and K-means quantization compared with the original speech representations, especially whisper.  In other words, what kind of representations are suitable for clustering?\n5. Can you provide some samples of the speech resynthesis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_gJKh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698142020247,
        "cdate": 1698142020247,
        "tmdate": 1699635938655,
        "mdate": 1699635938655,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ddLmeEcp5y",
        "forum": "LfDUzzQa3g",
        "replyto": "LfDUzzQa3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit"
        ],
        "content": {
            "summary": {
                "value": "In this paper, a speech representation code called RepCodec is introduced for semantic speech tokenization. It is seamlessly integrated into an end-to-end framework."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. RepCodec demonstrates promising results in both ASR and unit-to-speech resynthesis compared to the clustering method.\n2. The discovery that PMNI can deviate from performance is intriguing."
            },
            "weaknesses": {
                "value": "1. Overall, this paper lacks novelty, as compared to SouldStream, it simply replaces the input from raw waveform with SSL representations.\n2. Some parts of the details in this paper are confusing:\n    * The difference in bar height in the encoder and decoder parts in Figure 1 is confusing because neither sampling nor dimension reduction is applied.\n    * Equation (5) lacks sufficient explanation. I am unsure of its correctness as neither ${\\overset{\\sim}{n_k}}$ nor $\\mathbf{e}_{i}$ is adequately defined or explained. \n    * Shouldn't equation (7) be\n$ F^* = \\arg\\max_F p(\\mathbf{y}|\\mathbf{s}) = \\arg\\max_F \\prod_{i=1}^{m} p(y_i|y_{<i}, \\mathbf{s}) $?\n\n3. It would be better to include a more in-depth analysis of the weights ($\\lambda_{r}$, $\\lambda_{q}$) of reconstruction loss and quantization loss."
            },
            "questions": {
                "value": "3. Is WER a common metric for speech resynthesis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_6oit",
                    "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698140574,
        "cdate": 1698698140574,
        "tmdate": 1700710304341,
        "mdate": 1700710304341,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wNNyGJg74u",
        "forum": "LfDUzzQa3g",
        "replyto": "LfDUzzQa3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces RepCodec, a speech representation codec designed for semantic speech tokenization. It applies VQVAE to the representations from pretrained speech encoders to learn audio semantic tokens. The authors demonstrate the superiority of their proposed method over other discrete speech representation techniques, as evidenced by improved WER scores on ASR and speech resynthesis tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The authors demonstrate the superiority of their proposed method over other discrete speech representation techniques in terms of the WER scores on both ASR and speech resynthesis tasks.\n* The authors analyze the issue with the quality measure of semantic tokens based on their similarity to ground truth phonemes, while illustrating that the reconstruction loss of their proposed method exhibits a higher correlation."
            },
            "weaknesses": {
                "value": "* Insufficient evaluation metrics. The research predominantly relies on WER as the principal evaluation metric for the performance of semantic speech tokens. To make a compelling case for the proposed method's superiority, it's essential to include other the evaluation metrics such as speaker similarity, F0 error, or mean-opinion score in the speech resynthesis experiments.\n* Limited exploration of core downstream tasks. While semantic tokens are integral to token-based language modeling of speech, the paper's experiments are primarily focused on ASR and speech resynthesis. It lacks empirical investigations into other vital application tasks such as language modeling of audio, text-to-speech, speech-to-speech translation, or conditional modeling of acoustic tokens given the semantic tokens."
            },
            "questions": {
                "value": "I have concerns regarding the lack of evaluation results, as mentioned in the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission131/Reviewer_wBo4"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698826007327,
        "cdate": 1698826007327,
        "tmdate": 1699635938486,
        "mdate": 1699635938486,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1gV47tjQkU",
        "forum": "LfDUzzQa3g",
        "replyto": "LfDUzzQa3g",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission131/Reviewer_XznZ"
        ],
        "content": {
            "summary": {
                "value": "A novel RepCodec, a speech representation codec for semantic speech tokenization, has been introduced. RepCodec utilizes a vector quantization codebook to reconstruct speech representations from speech encoders like HuBERT or data2vec. RepCodec significantly outperforms the widely used k-means clustering approach in both speech understanding and generation tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is great in its clarity and well-structured organization. Its proposed approach is lauded for its simplicity and effectiveness. The comprehensive nature of the experiments conducted further strengthens the paper's credibility. Based on these positive aspects, it is recommended for publication at the conference."
            },
            "weaknesses": {
                "value": "The simplicity and effectiveness of the proposed approach are commendable. While there are no significant weaknesses to highlight, it would be intriguing to see the application of RepCodec in the context of zero-shot Text-to-Speech (TTS) systems, such as Vall-E. Exploring its potential in this domain could provide valuable insights and possibly further advancements in speech-processing technology.\n\nThe idea of SpeechTokenizer (SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models) has some similarities, could you please elaborate more regarding the difference? If possible, adding some baseline numbers using https://github.com/ZhangXInFD/SpeechTokenizer would add value to this paper."
            },
            "questions": {
                "value": "See comment in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission131/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829783650,
        "cdate": 1698829783650,
        "tmdate": 1699635938411,
        "mdate": 1699635938411,
        "license": "CC BY 4.0",
        "version": 2
    }
]