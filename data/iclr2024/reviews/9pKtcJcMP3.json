[
    {
        "id": "MJEdoyogFt",
        "forum": "9pKtcJcMP3",
        "replyto": "9pKtcJcMP3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
        ],
        "content": {
            "summary": {
                "value": "The authors describe an algorithm for video language planning (VLP) that takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. VLP leverages (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP is evaluated on simulated and real-world robotic platforms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Overall well-written paper, which is easy to follow\n2. Great to see real-world evaluation despite the simplicity of tasks"
            },
            "weaknesses": {
                "value": "1. One of the key contributions of VLP is the use of text-to-video models as dynamics models. But authors experiments and baselines are not convincing about the value of using these models as dynamics models. \n* Authors use relatively simple tasks to evaluate VLP. Why not use a text-based LLM or something like this: https://arxiv.org/abs/2106.00188 as a dynamics model instead of the text-to-video model given the simplicity of the evaluation tasks? Would have been great to see such a baseline to really understand the edge that video dynamic models provide.\n * It would have also been great to see VLP being applied to more complex tasks e.g., procedural planning (https://arxiv.org/abs/1907.01172). Generating a walk-through plan for a procedural task is a convincing use case for video dynamics model as compared to say an LLM-based dynamics model.\n2. It is also not clear which use-cases will really require VLP as compared to existing LLM-based planning models or more generalized agent architectures such as GATO (https://arxiv.org/abs/2205.06175) or decision transformers. To that end, it would have been useful to see a broader set of baselines in the paper. Some baselines to consider -- RAP: https://arxiv.org/abs/2305.14992 (uses MCTS + LLM dynamics model similar to VLP's tree search+dynamics model), Visual Language Planner: https://arxiv.org/pdf/2304.09179.pdf (learns a multimodal dynamics model and policy by finetuning an LLM), VIMA: https://arxiv.org/pdf/2210.03094.pdf (no dynamics model but multimodal planner, could truly bring out the value of video-based dynamics via such a comparison?)\n* Authors also say HiP is the closest to their work in terms of an approach but do not provide a performance comparison in their evaluation. Why? Would love to see it.\n3. VLP's inference is slow.  Given that VLP primarily focuses on robotic applications right now, it is unclear whether VLP is suitable for real world deployment. Slow runtime should also be reported as limitations in the main paper (rather than in the appendix).\n4. It is unclear if the authors are considering open-sourcing the code and the models. Without that the reproducibility and value of the work for the community reduces. I encourage the authors to discuss open-sourcing plans in their rebuttal."
            },
            "questions": {
                "value": "- Unclear how long of a video does the video models produce. \n- Authors say that they train separate models per domain but it is unclear what is a domain. Is tabletop manipulation in sim and real world considered same or different domain?\n- In table. 1, why not ablate VLP without the video dynamics model? Given my concerns in the weakness section, would have loved to see this ablation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_AYpb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733410447,
        "cdate": 1698733410447,
        "tmdate": 1699636455935,
        "mdate": 1699636455935,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "we64bdap7v",
        "forum": "9pKtcJcMP3",
        "replyto": "9pKtcJcMP3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_4qts"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of long-horizon visual planning tasks by utilizing the robust generative capabilities of large vision-language models (VLM) and text-to-video models. The authors introduce the Video Language Planning (VLP) algorithm, which takes both a visual observation and a natural language goal as inputs and subsequently processes a series of instructions, video frames, and low-level controls. The effectiveness of their proposed heuristic value function and tree search procedure is well demonstrated through extensive long-horizon robot manipulation tasks across three hardware platforms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The long-horizon tasks pose significant challenges and matter in the field of robot manipulation. The proposed VLP algorithm has the potential to significantly improve the success rate of these tasks, as demonstrated in both simulated and real-world experiments in this paper\n- Collecting large-scale robot manipulation datasets can be a costly endeavor. VLP, on the other hand, harnesses the impressive generative and generalization capabilities of the latest VLMs and text-to-video models that are pre-trained on Internet-scale data. This approach can serve as valuable inspiration for other researchers looking to tackle challenging robot manipulation tasks without the need for expensive data collection.\n- The experimental results regarding the relationship between execution accuracy and planning budget provide valuable insights into the efficiency of the proposed VLP algorithm."
            },
            "weaknesses": {
                "value": "The primary concern regarding this work is its potential for reproduction and adaptation to, e.g., other hardware platforms and low-cost budgets.\n- While the study tests the VLP algorithm on three hardware platforms, they are either relatively simple (i.e., having just one end effector) or self-designed. There is a question as to whether the well-trained VLP model could generalize or adapt quickly to other popular platforms. This limitation may reduce the paper's overall impact.\n- While video model inference is computationally expensive, its scalability is also a concern."
            },
            "questions": {
                "value": "- Table 1 clearly illustrates that the Value Function utilized in VLP significantly enhances result accuracy. However, in many robotic tasks, it is common to have multi-modal policies. For instance, in the \"make a horizontal line\" task shown in Figure 2, there could be multiple ways to manipulate objects, resulting in various possible remaining steps, especially for images that are far from the final goal. It would be helpful to see whether the PaLM-E model can be fine-tuned to accommodate multi-modal trajectories.\n- While the paper properly discusses a few limitations, the limitation of the task horizon is unclear in this paper. In the appendix, the authors provided both VLP and other baselines with 1500 timesteps to complete a task. Does this imply that VLP may not be suitable for handling longer-horizon tasks? For example, a long-distance task, such as moving an object over a long distance (e.g., 10 meters) from one location to another.\n- In the introduction, the authors emphasize the potential benefits of VLP when working with incomplete videos that lack corresponding language labels. It would be helpful to see more detailed descriptions and accompanying experimental results to further demonstrate this capability, which are not found in this paper.\n- The authors have highlighted the issue of overestimation with the Value Function. It would be interesting to see if out-of-distribution images or goals might also contribute to this problem. If so, could the application of certain offline reinforcement learning algorithms, such as CQL, potentially offer a solution for addressing this issue?\n- Minor issues that may need careful proofreading:\n  - Sec. 2: a image goal-conditioned -> an\n  - as planning submodules Sec. 2.1 -> in Sec. 2.1"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815840654,
        "cdate": 1698815840654,
        "tmdate": 1699636455829,
        "mdate": 1699636455829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3d7alSwxMU",
        "forum": "9pKtcJcMP3",
        "replyto": "9pKtcJcMP3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_uBLi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_uBLi"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a general-purpose framework solving for robotics tasks in which planning is done using a video generative model, and then using the sampled video as a target, a low-level controller selects actions to be executed by the robot in order to reproduce the video. To improve the quality of long-term video generation, the authors propose a hierarchical approach in which a list of commands are generated in text-format in addition to the video, and together optimized to minimize an estimate of how close the task is to being completed. The authors demonstrate impressive performance for multiple synthetic and real-world robotics tasks. Importantly, due to the use of internet-scale pre-training for the vision-language model (VLM) and video generative model, the approach generalizes well in a zero-shot setting to novel lighting conditions, objects, and tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "A great challenge for the machine learning community is to determine how recent advances in generative modeling can be used to advance robotics. I think the method proposed in this paper makes great strides towards this end. Both vision and language foundation models are combined to produce an impressive model-based goal-conditioned planner for robotics. Additionally, I found the paper very well written; I'm not an expert in robotics, but I found the paper easy and interesting to read. The discussion of the limitations at the end is especially insightful, as well as the visuals of video generative model failures in the appendix. The method is stated clearly and its efficacy is backed up with substantial qualitative and quantitative results."
            },
            "weaknesses": {
                "value": "I think the main weakness of the current submission is that it does not include measurements of runtime for the proposed approach, for instance in Table 3. Planning in video space seems very expensive which could limit how well this approach could be used for robotics tasks that need to be executed quickly. Of course, these runtimes can always be improved, but it would be helpful to see what the runtime is with today's hardware.\n\nRelatedly, I'm curious if the authors have considered optimizing only the text sequence with planning in Algorithm 1, then following with per-clip video generation? It seems much more efficient to me to search through text instead of pixels. Based on Table 3, it seems there are large gains to be had from a more exhaustive search, which I'm guessing is much easier to do if the need to render pixels is eliminated."
            },
            "questions": {
                "value": "In addition to the two main points mentioned in the weaknesses section, I have a few other minor questions:\n\n- Section 2.2 \"Vision-Language Models as Heuristic Functions.\" - is this model trained with regression, or the tokenized representation of the number of steps with the standard language modeling objective? If it's with the language modeling objective, do the authors choose the mode of the distribution over predicted number of steps when using the heuristic to plan in Algorithm 1?\n- Section 2.3 \"Replanning\" - is there a way to make this re-planning rate dynamic? Ideally, it would be possible to sense when reality has diverged from the video plan in a significant way that is not recoverable.\n- A.4 \"Video Models\" - what's the frequency of these videos, e.g. how many seconds does 16 frames correspond to?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699220487187,
        "cdate": 1699220487187,
        "tmdate": 1699636455736,
        "mdate": 1699636455736,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "klXykK3FMt",
        "forum": "9pKtcJcMP3",
        "replyto": "9pKtcJcMP3",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a framework for planning in the space of videos and language by leveraging recent text-to-video models (over vision-language models (VLMs) that are trained on static images) for incorporating scene dynamics and object motions. They take as input an initial observation and a long-horizon language instruction and output a plan in the form of video and language specifications, by first prompting a VLM for language actions and rolling out a language-conditioned video model whose outputs are subsequently assessed by the VLM again for favourability towards task progress using a learned heuristic."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper is easy to follow and well-structured with the main contributions listed clearly in the introduction alongwith grounding in related works and building blocks that make up this method (VLM, a video prediction model, and an inverse-dynamics model).\n2) The authors present strong model performance compared to 4 baseline methods on object rearrangement tasks, as well as deploy their method on multiple real-robot platforms."
            },
            "weaknesses": {
                "value": "While the tasks chosen for this paper are claimed to be long-horizon, they are not challenging enough to showcase a big leap in this realm of tasks. For example, when grouping blocks by color, while the task may seem long-horizon given there are large number of blocks on the table to manipulate, there is no dependency between subgoal successes/failures. This makes the task solvable without any need to retain information for long horizons. Hence, I believe the chosen suite of tasks does not evaluate the model adequately for solving long-horizon tasks."
            },
            "questions": {
                "value": "Are there any limitations induced by video prediction models, which as the authors identified can generate out-of-manifold frames during long-horizon video prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4737/Reviewer_MAiq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4737/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699258372231,
        "cdate": 1699258372231,
        "tmdate": 1699636455655,
        "mdate": 1699636455655,
        "license": "CC BY 4.0",
        "version": 2
    }
]