[
    {
        "id": "0ziTjbAwLt",
        "forum": "esh9JYzmTq",
        "replyto": "esh9JYzmTq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
        ],
        "content": {
            "summary": {
                "value": "Reinforcement learning (RL) contains unique challenges in fixing its reproducibility problem. Current displays of evaluation take attention away from other important factors such as model overfitting and experimental design. RL researchers have developed various reliability evaluation metrics to understand the strengths and weaknesses of each RL algorithm, but these metrics do not take out-of-distribution observations into account. The authors propose time series analysis tools to measure model robustness under the presence of distribution shift. They apply these analytical tools in both single-agent and multi-agent environments to show the effect of introducing distribution shifts during test time."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I appreciate the plot showing the flaw of solely relying on point estimates to perform model evaluation."
            },
            "weaknesses": {
                "value": "The authors don't list potential weaknesses of their current method in the main text."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Reviewer_okG3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729380271,
        "cdate": 1698729380271,
        "tmdate": 1699636420169,
        "mdate": 1699636420169,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9kxykQDlvi",
        "forum": "esh9JYzmTq",
        "replyto": "esh9JYzmTq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the evaluation of RL algorithms against distribution shifts.\nIn particular, it proposes the usage of evaluation techniques from the time series literature to take into account changes in the environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The paper is written in an intuitive way, which would help the adoption of the proposed methodology by the RL community.\n\n- The evaluation of RL algorithms is certainly an important issue"
            },
            "weaknesses": {
                "value": "- It did not come across to me what is the precise problem the paper is trying to solve. The problem formulation is somewhat vague and relies on artificial examples that make it difficult to connect it with the application of RL algorithms.\n\n- The methodology proposed is somewhat scattered. It is unclear how this evaluation methodology would be applied. Perhaps the paper could include a pseudo-code or a flowchart to ground all the steps of the proposed methodology.\n\n- Although the paper provides several examples, it does not provide a proper interpretation of the results. For example, in Figure 5, which agent is more robust? In Figure 3, what is the conclusion regarding the performance of A2C and PPO? Should we favor one of them in practice?\n\n- The related work section only lists the contribution of related papers but does not provide a description of how this paper distinguishes from those."
            },
            "questions": {
                "value": "1. Could you describe more formally the assumption that \"the trained agents achieved a clear trend in performance\"? In particular, how is this related to the convergence of an RL agent? Does it mean the agent has reached an optimal performance? Furthermore, as the assumption does not consider a distribution shift, does it mean the agent continues to update its behavior?\n2. Could you comment on the connections from this methodology with non-stationary MDPs?\n3. Considering the change in the dynamics of the environment, I think we cannot always conclude that the change in performance is due to the agent's behavior. For instance, in some cases, although the dynamics change, the optimal policy may remain the same. Could this methodology help identify if the agent is underperforming?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Reviewer_8p4f"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827009196,
        "cdate": 1698827009196,
        "tmdate": 1699636420058,
        "mdate": 1699636420058,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "idBXRQ2QCQ",
        "forum": "esh9JYzmTq",
        "replyto": "esh9JYzmTq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a set of evaluation methods that measure the robustness of Reinforcement Learning (RL) algorithms under distribution shifts. The paper argues to account for performance over time while the agent is acting in its environment. The authors recommend time series analysis as a method of observational RL evaluation and show that the unique properties of RL and simulated dynamic environments supports their additional assumptions needed to measure the causal impact in their experimental evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Detailed background of the various causal inference topics is provided. Even though most of the information can be moved to the appendix, this amount of information makes the paper easy to read for someone new to the field."
            },
            "weaknesses": {
                "value": "1. Lack of novelty: The paper does not have a novel contribution. The idea of using simulators to perform interventional analysis is not new [1-5]. \n\nThough they talk about the focus in the paper being on \"adversarial attacks on images (Atari game observations) and agent switching in multi-agent environments\", there is nothing that is specific to the adversarial nature of the distribution shifts. The paper as is will be applicable if distribution shifts happen due to any other factor.\n\n2. Verbose description: The setup is unnecessary and made complex to justify the simple idea of using a simulator to perform interventions. Simple things like an average of sampled data is dedicated a definition and equation (eq. 1), which I think is not needed. Only Section 4.2 is something that talks about a new approach, everything before that is motivation or background. \n\n3. Insufficient experimental analysis: Experimental evaluation is not sufficient. Figures 3 and 4 are not analyzed in detail, and the inferences from these experiments are not explained properly. Even in the appendix, only the plots are added without any analysis.\n\n[1] Lee et al., SCALE: Causal Learning and Discovery of Robot Manipulation Skills using Simulation. CoRL 2023.\n\n[2] Verma et al., Learning Causal Models of Autonomous Agents using Interventions. KEPS 2021.\n\n[3] Lee et al., Causal Reasoning in Simulation for Structure and Transfer Learning of Robot Manipulation Policies. ICRA 2021.\n\n[4] Ahmed et al., CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. ICLR 2021.\n\n[5] Verma et al., Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems. KEPS 2023."
            },
            "questions": {
                "value": "1. How would you comment on the related literature showing that simulators can be used for interventions? How is your idea new compared to them? My failure to understand this is the biggest reason for my score. Maybe I am missing something, and I would appreciate it if you could comment on it. \n\n2. Is the choice of adversarial nature of distribution shift important to the ideas presented in the paper? If we perform an interventional analysis on this paper and make the distribution shifts non-adversarial (something simply changed in the environment), then would the analysis you present still hold? If not, why?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4448/Reviewer_mUDk",
                    "ICLR.cc/2024/Conference/Submission4448/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4448/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699145773551,
        "cdate": 1699145773551,
        "tmdate": 1700724499675,
        "mdate": 1700724499675,
        "license": "CC BY 4.0",
        "version": 2
    }
]