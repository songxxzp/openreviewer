[
    {
        "id": "dY4TbMco41",
        "forum": "EAkjVCtRO2",
        "replyto": "EAkjVCtRO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_pvUL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_pvUL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new forecasting method by combining hidden Markov models and recurrent neural networks. The training procedure is inspired by vector quantized variational autoencoders, including the latent space and the emission laws parts. This method is computationally efficient and outperforms the SOTA baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation of this paper is good. The authors show the comprehensive details of training and network architectures.\n\n- The experiments and evaluation are convincing. The authors test on multiple stationary and non-stationary datasets."
            },
            "weaknesses": {
                "value": "The writing of this paper can be improved. On Page 2, there is a relatively large blank space, which can be optimized. Also, there are many typos and grammatical issues in this paper. Some of the issues are listed below.\n- On page 6, Section 3.1.3, \u201c...where T stand for\u2026\u201d, \u201cstand\u201d should be \u201cstands\u201d.\n- On Page 8, in the caption of Table 1, \u201c...their associated MASE\u2026\u201d, \u201cMASE\u201d should be \u201cMASEs\u201d. \n- On Page 8, Section 3.2.2, the second equation should be MAE.\n- On Page 8, Section 3.2.3, \u201cthe accuracy of our model reaches state-of-the-art standards and provide uncertainty quantification.\u201d, the subject of \"provide\" is not the accuracy."
            },
            "questions": {
                "value": "In Figure 1, for Hidden States Trajectory, e.g., $\\hat{x}_{t+1}^i=2$, are those \u201c2,2,1,1,3,...,3\u201d fixed or flexible to adjust in your implementation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Reviewer_pvUL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698283672035,
        "cdate": 1698283672035,
        "tmdate": 1699636660956,
        "mdate": 1699636660956,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i8pFceWF5h",
        "forum": "EAkjVCtRO2",
        "replyto": "EAkjVCtRO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_YnSQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_YnSQ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a forecasting model that combines discrete state space hidden Markov models with recurrent neural networks. The model is trained in a similar way as VQ-VAE. Experiments on several datasets show that the proposed method outperforms other SOTA methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "(1) Introducing finite state HMM for forecasting is interesting. \n\n(2) The formulation of the model and the learning method is sound. \n\n(3) The experiment results are good."
            },
            "weaknesses": {
                "value": "(1) The two-stage training appears unnecessary in the context of time series. It may lead to sub-optimal results. \n\n(2) More details should be provided on training the model with discrete latents in the VAE framework, i.e., how the discreteness is handled. \n\n(3) There is only one baseline model based on Transformer. I suspect there are many variants for forecasting."
            },
            "questions": {
                "value": "(1) How well does your method work on long context forecasting problem? \n\n(2) Do you employ straight-through trick or Gumbel trick in training your model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698731485585,
        "cdate": 1698731485585,
        "tmdate": 1699636660838,
        "mdate": 1699636660838,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dPM3PprsDd",
        "forum": "EAkjVCtRO2",
        "replyto": "EAkjVCtRO2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_9MfL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6114/Reviewer_9MfL"
        ],
        "content": {
            "summary": {
                "value": "The authors present a new algorithm for time-series prediction based on VQ-VAEs and RNNs, with separate emission models for each quantized hidden state. The authors evaluate their algorithm on a number of datasets, showing comparable results with SOTA algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The authors evaluated their algorithm against multiple other algorithms on multiple datasets, with multi-seed comparison + grid search.\n* As far as I am aware, combining VQ-VAEs and RNNs in this specific way (latent-conditioned observation model) has not been explored before."
            },
            "weaknesses": {
                "value": "* While the emprical results are nice, it would further strengthen the paper if the authors could perform ablation experiments to uncover/provide a better intuition about why their algorithm performs well against others."
            },
            "questions": {
                "value": "* The author's new algorithm performs almost similarly to previous SOTA Transformer-based model PatchTST/64. However, it's unclear from my first reading why one would prefer one over the other. Is it easier to train/better runtime etc.?\n* It's nice that the authors performed a grid search over learning rates and batch size for the other algorithms. I think the paper would be further strengthened if the authors conducted a hyperparameter search also over network size where it make sense (similar to the grid search that the authors performed over hidden size for their network)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6114/Reviewer_9MfL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6114/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772404747,
        "cdate": 1698772404747,
        "tmdate": 1699636660716,
        "mdate": 1699636660716,
        "license": "CC BY 4.0",
        "version": 2
    }
]