[
    {
        "id": "AJS9RXcQzU",
        "forum": "7zxGHwe7Vw",
        "replyto": "7zxGHwe7Vw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_6dEL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_6dEL"
        ],
        "content": {
            "summary": {
                "value": "This paper considers semi-supervised learning in the federated learning framework, focusing on scenarios where only the server possesses limited labeled data, while clients hold unlabeled data. \nThe proposed approach introduces a label contrastive loss in addition to the standard cross-entropy loss for server updates. \nThe server embeddings are then communicated to unlabeled clients to generate pseudo-labels for client supervised learning. \nEmpirical evaluations are conducted under both IID and non-IID settings to compare the performance of the proposed method against existing approaches."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's presentation of the proposed method is straightforward and easy to follow."
            },
            "weaknesses": {
                "value": "- Method:\n\n    - In the introduction (P2), the authors highlight the potential drawback of some existing works, citing \"heavy traffic for data communication\". However, the proposed method still necessitates the communication of embeddings of all server data to all active clients in each communication round. While the cost is lower compared to the aforementioned existing methods, the communication overhead remains considerable.\n    \n    - The paper claims novelty in the use of the label contrastive loss. However, the experiment does not explicitly demonstrate how this loss contributes to performance improvement. An ablation study could be beneficial to gauge the impact on method performance if the label contrastive loss were omitted or replaced by alternative loss functions. The current description underscores the novelty of the proposed method.\n\n    \n    - Too many hyper-parameters. In the local training (Section 3.3) step, various components contributing to the combined loss function. Parameters such as the threshold value, hyper-parameters for the Beta distribution, strong augmentation method, and size of the mixed dataset, among others, are introduced. However, the paper does not clarify how these hyper-parameters are selected or their potential effects on model performance.\n\n- Experiment:\n    - The experiment results in Table 1 for CIFAR100 with 10000 anchor data are puzzling, as the performance of the semi-supervised learning method markedly surpasses that of the supervised learning method in the IID case. This anomaly warrants clarification.\n\n\n    - The description for Figure 3 is unclear. It is not apparent what the lines represent. Additionally, it is ambiguous whether pseudo-label accuracy and classification accuracy are averaged over all datasets or pertain to a single client. Furthermore, the legends for the two datasets are inconsistent.\n    \n\n- Other issues:\n    - There is a notable absence of related work such as [1] -- [4]. These works should be compared to or at least discussed in the related work.\n\n    - Including a descriptive table to highlight the advantages of the proposed method over existing approaches would enhance the paper.\n\n\n    - The paper needs to be proofread more carefully. Instances of notation inconsistency, key notation typos, and repeated reference entries should be addressed. For instance:\n\n        - On P3, the paragraph discussing Federated Learning uses two separate notations, $G^t$ and $L_i^{t}$, to represent global and local models, respectively. These notations are inconsistent with the rest of the manuscript where the weight 'w' is used to denote different models.\n\n        - The notation for the unlabeled set in the paragraph on Federated Semi-Supervised Learning (P3) is clearly incorrect.\n\n        - The format of the bibliography entries is inconsistent, with some entries including URLs while others do not.\n\n        - There are repeated reference entries for FedMatch (Jeong et al., 2020).\n    \n\nReferences:\n\n[1] FedCon: A contrastive framework for federated semi-supervised learning\n\n[2] Federated semi-supervised medical image classification via inter-client relation matching\n\n[3] Dynamic bank learning for semi-supervised federated image diagnosis with class imbalance\n\n[4] Towards unbiased training in federated open-world semi-supervised learning\n\n[5] Rethinking semi-supervised federated learning: How to co-train fully-labeled and fully-unlabeled client imaging data"
            },
            "questions": {
                "value": "Please refer to the questions in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698204671849,
        "cdate": 1698204671849,
        "tmdate": 1699636296950,
        "mdate": 1699636296950,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9m7kaTV28O",
        "forum": "7zxGHwe7Vw",
        "replyto": "7zxGHwe7Vw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_LCsM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_LCsM"
        ],
        "content": {
            "summary": {
                "value": "This work aims at tackling label corruption in FL, called Federated Semi-Supervised Learning, where the server maintains a limited amount of labeled data with unlabelled data on the client side. To this end, the authors provide a double-head structure paired with a contrastive loss. Some experiments are conducted to verify the efficacy of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The studied problem is interesting and promising."
            },
            "weaknesses": {
                "value": "1. The claimed main novelty is the label contraction loss that maximizes the cosine similarity for samples with the same label while minimizing the cosine similarity for samples with different labels. However, this method has been proposed as supervised contrastive learning [1]. Other modules of the proposed method exhibit limited novelty, such as pseudo-labeling and mix-up.\n\n2. The authors seemed to overlook many outstanding works in the literature [2,3]. For instance, the authors involved one method as the baseline method. Moreover, many related works are not considered baseline methods [4].\n\n3. The experimental results are confusing. The proposed method seems to outperform the method with supervised information (See Table 1). This makes the results not convincing.\n\n[1] Supervised Contrastive Learning. Khosla et al. 2020\n\n[2] Federated Semi-Supervised Learning with Inter-Client Consistency & Disjoint Learning. Jeong et al. 2020\n\n[3] FedCon: A Contrastive Framework for Federated Semi-Supervised Learning. Long et al. 2021\n\n[4] pFedKnow, FedIL, and FedSiam"
            },
            "questions": {
                "value": "Suggestions:\n\n1. The paper is hard to follow, I suggest the author improve the writing.\n2 Figure 1 provides limited information. For instance, the left figure illustrates the pipeline of FSSL while overlooking the details of the method proposed in this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663913276,
        "cdate": 1698663913276,
        "tmdate": 1699636296876,
        "mdate": 1699636296876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nwkEkYWTN0",
        "forum": "7zxGHwe7Vw",
        "replyto": "7zxGHwe7Vw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_wVdB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_wVdB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces \"FedAnchor,\" an innovative Federated Semi-supervised Learning method tailored for the label-at-server scenario. FedAnchor adopts a unique double-head structure, integrating a label contrastive loss based on cosine similarity. This approach optimizes the use of limited labeled anchor data on the server and generates high-quality pseudo-labels, addressing issues like confirmation bias and overfitting common in pseudo-labeling approaches. Besides, the paper conducts extensive experiments across three diverse datasets, demonstrating that FedAnchor outperforms state-of-the-art methods in terms of both convergence rate and model accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1) The proposed label contrastive loss is interesting. Although this new loss is essentially a combination of existing contrastive loss (Info NCE Loss) and the cosine similarity metric, it can be seen from the formula as well as the provided figure that it does bring the same category representations closer and different category representations further apart in the latent feature space. Thus, despite the limited novelty in loss design, I still endorse this as an important contribution.\n2) This paper provides a good introduction of the new FSSL method, FedAnchor. In particular, the methodology section has a nice flow and well summarizes the training of FedAnchor proceed on the server and clients at each communication round. The included pseudocode of FedAnchor is easy to understand.\n3) FedAnchor addresses an important problem of FSSL research work: the low convergence rate. The experimental results show that FedAnchor significantly prevails the SOTA FSSL baselines in terms of convergence."
            },
            "weaknesses": {
                "value": "1) The key concern about the paper is the number of SOTA FSSL methods used for comparisons is insufficient. In paper, FedAnchor was compared with supervised learning and SemiFL, while the only meaningful baseline is SemiFL. The finding that a single SOTA FSSL method does not perform as well as fedAnchor is not enough to prove that FedAnchor can outperform all SOTA FSSL methods. Moreover, FedAnchor does not seem to consistently outperform SemiFL. It can be seen in Table 1 that FedAnchor's results on CIFAR100 could be worse than SemiFL when 10000 anchor data was given.\n2) Another concern is whether it makes sense in federated learning for a client to download the latent representations of the anchor data from the server to generate pseudo-labels. Federated learning methods often put data privacy at the first priority, so it may be not practical to disclose the complete latent representations of the data on server to each client. Moreover, according to the pseudo-labelling procedure defined in the paper, the obtained latent representations of the anchor data have to be compared with the latent representation of each local data. Under this setting, when the server data and the client's data scale up to a large amount, the effort of pseudo-labelling will be significant. Therefore, I doubt if FedAnchor can be applied to real scenarios.\n3) Furthermore, the method section mentions that FedAnchor adopts mixup during the local training on clients. The mixup operation seems to be a crucial trick to improve the accuracy of FedAnchor as shown by the results of Table 1. Why are there no additional ablation studies in the paper on the effect of mixup on FedAnchor convergence and accuracy? Can FedAnchor's performance still be ahead of SemiFL when mixup is not available?\n\nMinor Comment\n1) The introduction and background sections are a bit lengthy. Since FedAnchor corresponds to federated semi-supervised learning rather than federated learning, the introduction to federated learning and semi-supervised learning should be shortened.\n2) The font sizes of the legends and axis scales in the figures are too small to read.\n3) In equation 9, it should be \u201c(x_{fix}, y^{fix})\u201d instead of \u201c(x_{mix}, y^{fix})\u201d if my understanding about the method is not wrong."
            },
            "questions": {
                "value": "please respond to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835125460,
        "cdate": 1698835125460,
        "tmdate": 1699636296801,
        "mdate": 1699636296801,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QwEiRs1PYh",
        "forum": "7zxGHwe7Vw",
        "replyto": "7zxGHwe7Vw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_WkFS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3446/Reviewer_WkFS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces FedAnchor, a novel method for Federated Semi-Supervised Learning (FSSL), which operates under the label-at-server scenario, where only a limited amount of labels are hosted by the server and local clients contain only unlabeled data. This method employs a double-head structure and a label contrastive loss to improve training with limited labeled data at the server and unlabeled client data. FedAnchor aims to reduce issues such as confirmation bias and overfitting, which are common in pseudo-labeling. The method has been tested on three datasets and shows superior performance over current leading methods in both convergence rate and accuracy."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The considered scenario is novel and practical since the labels from local clients are not trustworthy and can be modeled as missing or noisy.\n2. Using anchor points can significantly reduce the confirmation bias induced by pseudo-labeling."
            },
            "weaknesses": {
                "value": "1. The true contribution of this paper needs to be more clear. Generating pseudo-labels by comparing the similarities between the model representations of unlabeled data and label anchor data (or class prototypes) is not entirely new, e.g., [1--3]. FedSSL is also studied in [4].\n\n2. The paper seems to borrow FixMatch and MixMatch, which is fine. But the contributions in addition to using both algorithm in FL need to be clarified.\n\n3. An important baseline is missing. [4] should be compared in experiments.\n\n\n[1] Cleannet: Transfer learning for scalable image classifier training with label noise.\n\n[2] Prototypical Pseudo Label Denoising and Target Structure Learning for Domain Adaptive Semantic Segmentation.\n\n[3] Learning from Noisy Data with Robust Representation Learning.\n\n[4] FedCon: A contrastive framework for federated semi-supervised learning."
            },
            "questions": {
                "value": "Please see Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3446/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699134426748,
        "cdate": 1699134426748,
        "tmdate": 1699636296712,
        "mdate": 1699636296712,
        "license": "CC BY 4.0",
        "version": 2
    }
]