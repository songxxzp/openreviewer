[
    {
        "id": "hUYGrY3Lue",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_vzpZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_vzpZ"
        ],
        "forum": "1nfqABOIwQ",
        "replyto": "1nfqABOIwQ",
        "content": {
            "summary": {
                "value": "The authors propose a discriminator and a warm-up strategy to solve noisy labeling in RLHF."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Noisy labeling is an important problem in RLHF. \n2. The authors propose two techniques a discriminator and a warm-up technique to solve the problem.\n3. The authors also provide ablation studies for the two techniques"
            },
            "weaknesses": {
                "value": "1. For the discriminator part, the bound in the theory is not well analyzed. How large is the constant in the squared term $O(\\rho^2)$? Does it affect the value of the bound if the constant is large?  Would it be possible to quantitatively visualize some of the cases and the corresponding bound values in experiments?\n2. There are some hyperparamters $\\alpha, \\beta$ in the bound for the discriminator. These hyperparameters are somehow vague and make the connection between the theory and practive loose. A concrete analysis of these hyperparameters and how they affect the final results should be included."
            },
            "questions": {
                "value": "See the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697380027110,
        "cdate": 1697380027110,
        "tmdate": 1699636207346,
        "mdate": 1699636207346,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "d8g4VG8tJt",
        "forum": "1nfqABOIwQ",
        "replyto": "1nfqABOIwQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_z4ZA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_z4ZA"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces RIME, an algorithm to increase the robustness of Preference-based Reinforcement Learning (PbRL). RIME adds a preference discriminator used to filter out (or even flip) really noise labels. Additionally, RIME introduces a warm-start method to better transition from the unsupervised exploration phase of PbRL methods to the proper reward training phase.\n\nExperiments show RIME outperforming a thorough selection of baseline when the labelling error is high."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Both theoretical contributions (bounding the divergence of a preference discriminator and warm-start) are interesting and useful for the community. In particular, the soft transition between the unsupervised phase and the reward training phase that warm-start induces, seems to me more logical than simply reseting the reward.\n* The experimental section is very thorough, comparing (and beating) most recent baselines when the noise of the mistake labeller is high. Equally the ablations serve to understand the behaviour of RIME, in particular the fact that RIME does not show negative effects when training on clean labels."
            },
            "weaknesses": {
                "value": "* _W1_: The paper does not feature a user study with actual human labellers. This is particularly relevant as ultimately the gains in robustness are meant to make PbRL more usable by humans.\n* _W2_: There is no composition of RIME with other methods, but the contributions of RIME are surely applicable to SURF or RUNE.\n* _W3_: RIME uses a uniform sampling schedule rather than an uncertainty based one to select the initial trajectories to query. But such choice is not explained, nor ablated, and other methods (like PEBBLE) use the uncertainty-based sampling.\n\n**[[Post-rebuttal update]]**\n\nThe authors added extra experiments with actual human labellers, combined with other baselines, as well as further sampling schedules, clearing most of my weaknesses.\n\nThe remaining minor weaknesses are that RIME does not seem to always take advantage of the improvements proposed in other PbRL methods such as MRN or RUNE, and that the analyses so far have not included standard deviation across 5 or 10 runs."
            },
            "questions": {
                "value": "* _Q1_: Is the preference discriminator another module? Or is it embedded into the learnt reward function? The text seems to indicate the former, but Figure 1 points to the latter. Either way, please include a description of how the discriminator is operationalised.\n* _Q2_: Do you have an intuition as to why RIME performs approximately the same as PEBBLE in DMC's cheetah?\n* _Q3_: At what value of $\\epsilon$ does RIME stop working? At the reported $\\epsilon = 0.3$ most tasks continue to perform acceptably.\n* _Q4_: [Less important] Could you add an analysis of what proportion of labels get suppressed /flipped under different $\\epsilon$, tasks, and decay thresholds? This would be useful to better understand the role of label discrimination.\n\n**Nitpicks and suggestions (will not affect rating)**\n\n* In the literature review, consider quantifying the sentence \"huge amount of preference labels\"\n* In section 4.1, consider including the proof of Theorem 1 (at least for y=0) in the main manuscript, it is an important part of the paper.\n* In section 4.1, explicitly state that $\\alpha$ must be $\\le 0.5$, otherwise it will not be a lower-bound of the KL divergence.\n* In section 4.1, explain that the linear decay schedule for $\\beta$ will be denoted as $k$ further down the manuscript.\n* In section 5, I found the explicit research questions useful. Could you link add references to the sections with the actual experiments?\n* Could you add some videos/screenshots of the final performance of RIME with some tasks (eg. Cheetah)? \n* I found Figure 5, particularly hard to parse. I think a table with final performance and all the ablations would be easier to interpret.\n* Consider separating Figure 6 into two half-page figures. Subfigures a) and b) have very little to do with subfigures c) and d).\n* In section 5.2, I am not sure how \"RIME improves robustness whilst maintaining feedback efficiency\" follows \"Given that it takes only a few human minutes to provide 1000 preference labels\". Also note that more difficult tasks (which are ultimately the goal of PbRL) may well require more time to annotate. Think about rephrasing that sentence?\n\n\n**[[Post-rebuttal update]]**\n\nThe authors addressed all the major questions above and updated the manuscript accordingly. See the discussion for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2667/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2667/Reviewer_z4ZA",
                    "ICLR.cc/2024/Conference/Submission2667/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698771844713,
        "cdate": 1698771844713,
        "tmdate": 1700739567692,
        "mdate": 1700739567692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "jRlA8o8s5N",
        "forum": "1nfqABOIwQ",
        "replyto": "1nfqABOIwQ",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_dSpC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2667/Reviewer_dSpC"
        ],
        "content": {
            "summary": {
                "value": "The work attempts to make PbRL algorithms like PEBBLE more robust by proposing a sample-selection method, i.e. to reject feedback samples obtained from a scripted oracle (stand in for human in the loop) if the proposed method finds it too \u201cnoisy\u201c. Additionally, they argue in favor of using a \u201dwarm-start\u201c method for reward learning. For empirical evaluation they consider the case of \u201dmistake-oracle\u201c as defined in a prior work and evaluate on 6 tasks across 2 domains which have also been used in past PbRL works."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The work is straight-forward add-on method over an existing backbone PbRL algorithm and is easy to use with PbRL algorithms with PEBBLE like training paradigm.\n\nThe authors do a good job at highlighting their method  and provide enough background / information for readers. It is easy to follow (and can invite constructive criticism).\n\nI like the general theme of the work, which is to study noisy human preferences and I agree that it is an important topic for the PbRL / RLHF community (however I have concerns : see Weaknesses/Questions)."
            },
            "weaknesses": {
                "value": "1. The title of the paper says \u201cnoisy human preferences\u201d however there is no study or analysis with real humans in the loop. Is there a reason to believe that humans provide feedback as \u201cmistake oracle\u201d does? As I recall, even the authors of BPref agreed that the suggested oracles in their work may not truly reflect human preference. Do the authors have some experiments with actual humans interacting with the RIME framework giving their preferences on domains like in some past PbRL works?\n2. The work has been done entirely in the context of a specific \u201cmistake oracle\u201d as defined in a prior work (BPref [1]). I think this relates to previous point, while the works like PEBBLE / SURF / RUNE / MRN were done in the context of perfect teachers and that BPref was done to promote better scripted teachers : I am surprised that authors did not investigate other scripted models for noisy teachers. For example, Noise could have been conditioned on state / trajectory etc. While the claims of the paper are on \u201crobustness\u201d and that their study on \u201cmistake-oracle\u201c is a good starting point, I do not see how improvements along a single noisy oracle justifies generality of the approach.\n3. I also feel that the experiments are quite limited and generally present information that do not answer the main claim on the robustness. \n    1. Limited Domains : For example, authors only show results on 6 tasks across 2 domains, where there is no significant performance improvements on Walker/Cheetah over baseline PEBBLE. \n    2. Limited baselines for handling noise : I would have considered experiments on the lines of Fig 6c, 6d as the most important. Figures 3, 4 generally shows that existing PbRL methods are not robust to noise which is a valid point to make. However, it is figure 6c/d which shows why RIME is a superior method to alternate solutions against label noise. The authors only have two baselines and report results on two domains with specific noise error values. I was not able to find more results in the appendix.\n4. What is the overlap of the presented method on \u201csample selection\u201d based robustness to noise and existing literature. While there may not be works in PbRL directly tackling noise, it is unclear from section 2 Related Work what are the existing approaches for robustness that are used in Machine Learning and how they may resemble the proposed work. This is especially important as PbRL reward learning is posed as a classification problem, and there is a large body of works on robustness in classification. \n5. How is warm-start something that is related to human preferences or reward learning or robustness to noise? It seems that warm-start can be applied to a learning problem and may potentially provide performance improvements. Authors can argue against if they want, but as I understand warm-start is a general add-on technique that may provide more reasonable initialization to an approximator (there are some preliminary works within the context of PbRL as well [2]). Additionally, Fig 5a,b suggests that the the performance boost with the sample-selection strategy is very limited compared to baseline PEBBLE. Infact RIME (only tau lower/upper) is worse for eps=0.1. Do the authors have results for PEBBLE  + Warm start only?\n\n\n[1] Lee, K., Smith, L., Dragan, A., & Abbeel, P. (2021). B-pref: Benchmarking preference-based reinforcement learning. arXiv preprint arXiv:2111.03026.\n\n[2] Verma, M., & Kambhampati, S. (2023). Data Driven Reward Initialization for Preference based Reinforcement Learning. arXiv preprint arXiv:2302.08733."
            },
            "questions": {
                "value": "1. Can the authors provide some insights into the rejection rate of RIME? That is, how many samples eventually are not considered by the algorithm during training. It is unclear how many \u201cnoisy\u201d samples still go through the training.\n2. The authors state that they use the uniform sampling scheme for query selection. Is this choice for all the baselines and RIME? If so, PEBBLE proposed more advanced methods of query sampling like disagreement and uncertainty based which can generate better queries. In general, I would argue that query selection can have a big impact on noise (if actual humans are providing feedback). Even in the current setup, methods like PEBBLE have favored more advanced query sampling strategies. \n3. Can the authors report reward recovery results (true episode returns) on the Metaworld tasks for which they have reported success rate? Or comment on the difference in reward recovery? \n4. Why have the authors chosen different feedback schedules for different noise values? (as in Table 8 which I think should be moved to the main text. Otherwise figures 3/4 can become misleading. For example, for Quadruped it appears that at eps = 0.2 the performance is better than at eps = 0.15, but it is because the authors provide double the total feedback.)\n\nI would also request the authors to respond to the points in \u201cWeakness\u201d section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2667/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827122075,
        "cdate": 1698827122075,
        "tmdate": 1699636207176,
        "mdate": 1699636207176,
        "license": "CC BY 4.0",
        "version": 2
    }
]