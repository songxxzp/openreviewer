[
    {
        "id": "aTLCABnTCG",
        "forum": "7ffJo4vtTY",
        "replyto": "7ffJo4vtTY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_8h4M"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_8h4M"
        ],
        "content": {
            "summary": {
                "value": "The goal of this paper is to characterize CLIP-style models in terms of their weights and hidden representations. This is motivated by these model\u2019s superior generalization ability, which suggests that their internal representations are qualitatively different than smaller models trained on less data. The work finds that robust models exhibit outlier features, that is, individual neuron activations with significantly higher magnitude than the average. The work also tries to quantify the number of distinct concepts that these models learn (relative to models either finetuned or trained from scratch on ImageNet), with results suggesting that models with CLIP-training on large datasets learn more concepts."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Motivation:** As greater and greater resources are poured into training frontier models, developing an understanding of what makes these models so much more robust than their counterparts trained on smaller datasets becomes a more and more pressing problem. The approach taken by this paper, attempting to characterize robustness, through model structure alone, is promising as a low compute way of assessing the robustness of a model and a starting point to finding a mathematical basis for robustness.  \n- **Clarity:** The work is mostly well-written and well-structured. The reviewer particularly appreciated the way that the work summarizes key takeaways in colored boxes that are easy to find. The visualizations and plots were well-thought-out and communicated their information efficiently.\n- **A strong premise:** Though the reviewer had some concerns around the way the experiments were run and the conclusions that were drawn from them (see the \"Weaknesses\" section), the premise is very interesting and feels like a strong avenue for further exploration."
            },
            "weaknesses": {
                "value": "- **Experiments:** From this reviewer\u2019s perspective, the main issues with the paper come from the experiments and the conclusions drawn from these. We outline our concerns below.\n    1. **Privileged directions experiment:** This reviewer did not understand how the privileged directions experiment connected with outlier features. To check our understanding, outlier features are a subset of activations that have substantially higher magnitude than the average activation for that input. Privileged directions on the other hand, are (a subset of?) the right singular vectors for the final linear layer. To determine whether a right singular vector $v_i$ is important to the encoder, the cosine similarity between $v_i$ and a sequence of activation vectors is computed and scaled by the corresponding singular value. This is termed the *importance* of $v_i$. It is not clear to the reviewer how \u2018importance\u2019 says anything about outlier features. The latter seems to be a property specifically related to the activation (or neuron) basis whereas the former consists of right singular vectors of the weight matrix, which are almost certainly not the activation basis. To be clear, identifying whether activations align with singular vectors with large singular value seems interesting, it just doesn\u2019t seem to be related to outlier features. Perhaps the tool one wants is closer to a measure of sparsity?\n    2. **Pruning experiment:** In Section 3, \u201cPruning non-privileged directions\u201d, the paper notes that one can prune away the smallest 80% of all singular vectors without substantial loss of accuracy. This technique is a standard method of finding a low-rank approximation of a matrix and well-studied. For large matrices, it does not seem surprising that the impact on classification is small even when using a low-rank approximation as the data itself can be approximated by a low-dimensional subspace of the ambient space. Furthermore, it is hard to tell if the results are specific to robust models when this technique is not applied to the finetuned and trained from scratch models.\n- **Lack of coverage of the multimodal aspect of the models:** Given the title, one would expect that there would be some discussion about how the multimodal aspect of CLIP-type models impacts the results. Surprisingly, this feature was never addressed. This reviewer would suggest either removing the word \u2018multimodal\u2019 from the title or adding a section to address this aspect. Further, it would be more precise to say that the work considers CLIP-type models since the experiments focus exclusively on this particular family.\n- **Experimental breadth:** Related to the previous point, it would make the work stronger if the results were expanded, either by increasing the breadth of the experiments (more models for instance) or providing some analysis of why we see the phenomena that we do. \n\n### Nitpicks\n- The abstract and introduction use the term *outlier feature* without any explanation. The term is not defined for several pages. While outlier features are known within the interpretability community, for the sake of accessibility, this reviewer would recommend putting at least an informal explanation of what this concept is the first time it is mentioned. \n- It\u2019s possible the reviewer missed it, but it seems that $d_X$ and $d_H$ are never defined in Section 3, \u201cApproach\u201d.\n- SVD is a foundational method in linear algebra. As such, there probably isn\u2019t a reason to include (3).  \n- The reviewer appreciates the validation of previous work showing trends in effective robustness (ER). It would be good if more papers did these kinds of validation experiments. On the other hand, as space is precious, this reviewer would suggest moving some of the text in this section to the appendix to focus on this work\u2019s contributions. It would seem that the main point the paper needs to make with regard to ER is that CLIP-style models stand-out for their ER relative to the same architectures finetuned on ImageNet or trained from scratch on ImageNet. This could be done more concisely."
            },
            "questions": {
                "value": "- Looking at Table 3, one sees that the ImageNet supervised models tend to often have more concepts than some of the finetuned CLIP models. Is there an explanation for why this is?\n- Figure 4 suggests that the ViT models tend to share more of the same concepts between training techniques, are there any guesses for why this is?\n- This reviewer did not understand the remark \u201cAn interesting parallel can be drawn with the work of Bondarenko et al. (2023), which found that outlier features in language models assign most of their mass to separator tokens (such as the end of sentence token).\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Reviewer_8h4M"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698553715672,
        "cdate": 1698553715672,
        "tmdate": 1700809798888,
        "mdate": 1700809798888,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Tln0NjUZYo",
        "forum": "7ffJo4vtTY",
        "replyto": "7ffJo4vtTY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_YHoY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_YHoY"
        ],
        "content": {
            "summary": {
                "value": "This paper empirically showed that robust multimodal models have outlier features and these outlier features encode more concepts. The paper analyzed the representation spaces of various multimodal models and found that more robust models have much more outlier features. What\u2019s more, by probing these outlier features of robust multimodal models, the authors find that the principled directions in them encode substantially more concepts."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is tackling an important problem in understanding robust multimodal models. Multimodal models are often found to be more robust than prior supervised models. It is not clear why that is the case. This paper provides many intriguing evidences for this observation.  \n2. The finding that robust multi-modals have more outlier features is interesting. It is another evidence that zero-shot CLIP models are much different than other non-robust models.  \n3. The analysis of the paper is thorough, including using activation kurtosis to analzye outlier features and also the use of concept probing."
            },
            "weaknesses": {
                "value": "1. It is not clear to the me, what is the reason for selecting the metric of activation kurtosis for the analysis in Section 3. What makes this metric interesting for the analysis of outlier features?   \n2. It seems section 2 is an re-evaluation of existing works on effective robustness. It would be good to summarize these results and definitions in a concise fashion."
            },
            "questions": {
                "value": "1. In table 2, why would the kurtosis of OpenAI CLIP models be much higher? It seems to be an extreme value. I am interested as to what would be the difference between OpenAI and YFCC-15M/CC-12M CLIP models.  \n2. From equation 3, the definition of privileged directions in representation space seems to be based on SVD decomposition of the classification head. Have the authors tried more involved methods, e.g. reduced rank regression? Instead of finding the low-rank approximation of W, reduced rank regression would find the low-rank approximation of WX."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698644542760,
        "cdate": 1698644542760,
        "tmdate": 1699636331069,
        "mdate": 1699636331069,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ULHA2xWeCL",
        "forum": "7ffJo4vtTY",
        "replyto": "7ffJo4vtTY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_KW5N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_KW5N"
        ],
        "content": {
            "summary": {
                "value": "This paper demonstrates the existence of outlier features and the substantial encoding of multiple concepts in robust models through the study of models like CLIP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides a systematic investigation of models like CLIP, offering compelling evidence that robust models encode outlier features and a greater variety of concepts. This research is quite interesting."
            },
            "weaknesses": {
                "value": "1, This paper appears to explain some interesting phenomena but doesn't offer methods for improving model performance. Therefore, I believe the contributions of this research may be relatively limited. As a result, I consider the overall quality of the paper to be at a borderline level.\n\n2, I believe what might be more interesting is understanding why these phenomena occur rather than merely showcasing them.\n\n3, I might have been more eager for the authors to utilize the findings in this paper to inspire some ideas for addressing unresolved problems."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Reviewer_KW5N"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698684317879,
        "cdate": 1698684317879,
        "tmdate": 1699636330960,
        "mdate": 1699636330960,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EX1kJvd7FZ",
        "forum": "7ffJo4vtTY",
        "replyto": "7ffJo4vtTY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_qBSE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3748/Reviewer_qBSE"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the features learned by robust multimodal models. This paper explores the concept of robustness in multimodal models, specifically looking at the differences between robust and non-robust models and uncovering two signatures of robustness in the representation spaces of these models. The authors find that robust models have outlier features, which are highly activated components of the representation space. These outlier features induce privileged directions in the representation space, which are important for the model's performance. The authors also find that robust models encode more unique concepts than less robust models. This leads to polysemy, where a single representation can be used to represent multiple concepts. Additionally, they demonstrate that privileged directions in the model's representation space explain the model's predictive power. The paper analyzes multiple robust multimodal models trained on various pretraining sets and backbones. Overall, this paper provides valuable insights into the nature of robustness in multimodal models and sheds light on the factors that contribute to their success."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper makes a significant contribution to the field of multimodal models by uncovering two signatures of robustness in the representation spaces of these models. This is a novel approach that has not been explored in previous research. It provides a new understanding of the features that make robust multimodal models robust. The authors identify two key features: outlier features and privileged directions. Outlier features are highly activated components of the representation space, while privileged directions are directions that are important for the model's performance. The authors show that both of these features are more prevalent in robust models than in less robust models.\n\n2. The authors analyze different robust multimodal models trained on various pretraining sets and backbones, providing a comprehensive and rigorous analysis of the factors that contribute to robustness in these models. The paper uses a variety of methods to validate its findings. In addition to using activation kurtosis and singular value decomposition (SVD) to identify outlier features and privileged directions, the authors also use concept probing to show that robust models encode more unique concepts. This provides strong evidence that the authors' findings are not just artifacts of the specific methods they used.\n\n3. The findings of this paper have practical implications for the development of robust multimodal models. The authors demonstrate that outlier features and privileged directions in the model's representation space are key factors in the model's success, which can inform the development of more robust models in the future. The authors' identification of outlier features and privileged directions suggests that these features should be preserved in model training. This could be done by using training objectives that encourage the model to learn these features, or by using regularization techniques to prevent the model from overfitting to the training data."
            },
            "weaknesses": {
                "value": "1. The paper only analyzes CLIP robust multimodal models with different backbones, which may not be representative of all possible models. This limits the generalizability of the findings. There are many other large scale multimodal models, which should be included, i.e., BLIP [1], FLAVA [2].\n\n2. The paper only focuses on robust models and does not compare them to non-robust models. This makes it difficult to determine the extent to which the findings are specific to robust models. See more questions in Questions section.\n\n3. Lack of explanation of outlier features: While the paper identifies outlier features as a key factor in robustness, it does not provide a clear explanation of what these features are or how they contribute to robustness. While the paper does discuss the practical implications of the findings, it could have gone into more detail about how these findings can be applied in practice to develop more robust multimodal models, i.e., VK-OOD [3]\n\n[1]Li, J., Li, D., Xiong, C., & Hoi, S. (2022, June). Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning (pp. 12888-12900). PMLR.\n\n[2]Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., & Kiela, D. (2022). Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 15638-15650).\n\n[3]Wang, Z., Medya, S., & Ravi, S. N. (2023). Differentiable Outlier Detection Enable Robust Deep Multimodal Analysis. arXiv preprint arXiv:2302.05608."
            },
            "questions": {
                "value": "1. What are the two signatures of robustness in the representation spaces of multimodal models, and how do they contribute to the models' success?\n\n2. How do outlier features in robust multimodal models differ from those in non-robust models, and what is their role in robustness?\n\n3. What are privileged directions in the model's representation space, and how do they explain the model's predictive power?\n\n4. In Figure 1, what are baseline models and baseline models fit training on? Directly from ImageNet? How can this accuracy compare with the multimodal fine-tuning ones?\n\n5. Figure 2 is a little bit hard to read and understand. The authors should present and well-explain the figures clearly.\n\n6. Why only use kurtosis value to determine outlier features? The model of reference work has different number of parameters, how did authors choose the same one as their work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3748/Reviewer_qBSE",
                    "ICLR.cc/2024/Conference/Submission3748/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3748/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824220231,
        "cdate": 1698824220231,
        "tmdate": 1700673857807,
        "mdate": 1700673857807,
        "license": "CC BY 4.0",
        "version": 2
    }
]