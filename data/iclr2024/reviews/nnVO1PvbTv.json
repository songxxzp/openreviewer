[
    {
        "id": "YYoD5qEUdD",
        "forum": "nnVO1PvbTv",
        "replyto": "nnVO1PvbTv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an LLM-KG integration paradigm to incorporate structural knowledge stored in KGs in LLMs reasoning, namely Think-on-Graph (ToG). ToG makes LLM serve as an agent to walk on KGs by iteratively searching and pruning relations and entities from KGs. Experiments show that ToG could enhance the LLM\u2019s reasoning capabilities and achieve SOTA on 6 datasets. ToG also exhibits knowledge traceability and correctability to improve KG quality."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. Good written paper, very easy to read.\n2. Strong experimental performance, achieving SOTA on 6 datasets without training.\n3. The motivation is strong and clear, incorporating external knowledge (KGs) would be an important problem to enhance LLMs."
            },
            "weaknesses": {
                "value": "1. I believe it would be interesting to see the ToG performance when encountering the knowledge conflict between external KG knowledge and parametric knowledge stored in LLMs, which is an aspect to test the robustness of ToG.\n2. I am concerned that ToG would bring too many intermediate steps and cause high latency in reasoning and expensive deployment, especially when using API-based black-box models like GPT-4.\n3. It would be interesting to see the performance of ToG when incorporating the KGs of low quality (sparsity, noisy, etc), since most KGs are sparse and out of date to some extent. So investigating the impact of KG quality would enhance the understanding of ToG method and its limitations. \n4. I would encourage authors to present experiment results on small LLMs, such as 7B and 13B."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_LtPr"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678960952,
        "cdate": 1698678960952,
        "tmdate": 1699636165357,
        "mdate": 1699636165357,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WJZA3ZPYzn",
        "forum": "nnVO1PvbTv",
        "replyto": "nnVO1PvbTv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_K4jf"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel approach called Think-on-Graph (ToG) to synergize the LLMs and KGs for reasoning. ToG enables LLMs as agents to iteratively execute searches on KGs to discover promising reasoning paths, which are then used to guide the LLMs to generate accurate answers. ToG is a general framework that can be applied to various LLMs and KGs. Extensive experiments on several datasets show that ToG can significantly improve the reasoning performance of LLMs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-presented and easy to follow. The authors provide a clear motivation and a good introduction to the problem.\n\n2. The proposed framework can be easily plugged into existing LLMs and KGs without incurring additional training costs.\n\n3. Extensive experiments on several datasets show that ToG can significantly improve the reasoning performance of LLMs."
            },
            "weaknesses": {
                "value": "1. The computational cost of ToG is relatively high. The searching process of ToG involves multiple LLM calls, which may be costly and limit its practical applicability in some settings. \n\n2. Some details are inconsistent in the paper. For example, in the approach introduction section, ToG selects the next step triples/relations based on current expended reasoning paths. However, in the prompt illustrated in G.3, I cannot find the current reasoning paths used for the pruning process.\n\n3. I have concerns that ToG might not understand the meaning of relations well and generalize to different KGs. The relations defined in KGs are usually in diverse formats. For example, the relations in Freebase are defined in a hierarchical format, while the relations in Yago have clearer semantics. If the relations do not reveal the clear semantics, the searching process of ToG might be misled."
            },
            "questions": {
                "value": "1. Can the authors discuss the cost of ToG in detail? What is the average number of calls for the reasoning? What is the overall price for the ChatGPT/GPT-4 API calls?\n\n2. Can the authors explain the inconsistency I discussed above and present a clear illustration of the whole reasoning process?\n\n3. Can the authors explain how ToG can generalize to different KGs? How LLMs in ToG understand the meaning of relations in different KGs without additional training?\n\n4. Can other search algorithms be used for ToG? For example, depth-first search, breadth-first search, A* search, etc.\n\n5. Can ToG be used to solve complex reasoning problems that cannot be solved by path-based reasoning? For example, in GrailQA, there are questions like: \"How many TV programs has Bob Boyett created?\". This question needs to count the KG structures to get the answer. Besides, there are other complex questions requiring intersection, union, and negation operations. Can ToG be used to solve these kinds of questions? Maybe a limitations section can be added to the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698713553927,
        "cdate": 1698713553927,
        "tmdate": 1699636165267,
        "mdate": 1699636165267,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dZ5muK4j3x",
        "forum": "nnVO1PvbTv",
        "replyto": "nnVO1PvbTv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_ezze"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes ToG (Think-on-Graph) for deep, responsible, and efficient LLM reasoning with knowledge graphs with a new paradigm of \"LLM\u00d7KG.\" The ToG is with the searching and pruning procedures to conduct deep reasoning. Empirical results on five KBQA datasets with extensive experiments justify the effectiveness of the proposed ToG method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is rather solid and detailed from a technical perspective.\n\nThe figures are clear and easy to understand.\n\nThe solution of LLM\u00d7KG is reasonable and new to the LLM community.\n\nThe depth and width of Toc are clearly investigated in the ablation study.\n\nThe limitations of the proposed method are extensively discussed in Appendix A.\n\nSeveral technical details, case studies, and evaluation results are also elaborated on in the Appendix."
            },
            "weaknesses": {
                "value": "The writing of the paper can be largely improved. \n\nBesides, the mathematical notations and equations can be improved to be clearer.\n \nIt would be better to summarize the frequently used notations in one table or sentence.\n\nThe empirical performance of ToG is not consistently the best, which is outperformed by the \"prior finetune SOTA\" in some cases of Tab. 1. Although not requiring training is a major benefit of ToG, its reasoning power is not fully convincing enough. I would suggest the paper make a further discussion and explanation for that.\n\nBesides, the important baseline, \"Prior Prompting SOTA\" in Tab.1, is not sufficiently evaluated and compared. It would be much better and more convincing to fill in the blanks in Tab.1. \n\nThe paper mentions the hallucination problem many times and uses it as the motivation of ToG. However, the hallucination problem is not sufficiently studied. As far as I can see, there is only one preliminary analysis of error is provided in Appendix D.2.\n\nThe paper is empirically driven and lacks in-depth analysis, whether from methodological or theoretical perspectives."
            },
            "questions": {
                "value": "What is the running-time efficiency of ToG?\n\nHow do the KGs used in the experiment part solve the limitation of out-of-date knowledge?\n\nThe beam search with pruning adopted by ToG is quite relevant to the progressive reasoning methods equipped with learnable search and pruning mechanisms, which also come from the KG areas (e.g., AdaProp [1] and Astarnet [2]). I would suggest the paper have a discussion with these relevant works. In addition, is it possible to achieve a kind of learnable pruning as an enhancement of ToG?\n\n[1] Zhang et al. AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. KDD 2023.\n\n[2] Zhu et al. A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs. NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698738700083,
        "cdate": 1698738700083,
        "tmdate": 1699636165122,
        "mdate": 1699636165122,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1nkAf1hlOy",
        "forum": "nnVO1PvbTv",
        "replyto": "nnVO1PvbTv",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
        ],
        "content": {
            "summary": {
                "value": "This work aims at improving the integration of knowledge graphs (KGs) in LLMs. The authors propose a method called Think-on-Graph (ToG) that performs beam-search on knowledge graphs, keeping track and exploring reasoning paths. Specifically, they use a \"search\" operation backed by the KG and a \"prune\" operation backed by the LLM. Through the experiments on benchmarks of KBQA/open QA/etc, the authors find an advantage of ToG in several of them compared to prior work. They also perform analyses on the effect of individual components of ToG like the selection of KG, search depth, pruning method, etc."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper proposes an intuitive and novel method in LLM-KG integration. The experiments are performed on a variety of datasets. The performance of the method is overall positive. The analyses on each component of the method is extensive. The writing of the paper is overall clear."
            },
            "weaknesses": {
                "value": "The reliance on very strong (production-level) LLMs and the choice of baselines. The authors explored the use of Llama-2-chat (70b), ChatGPT, and GPT-4 as the LLM in the ToG method. From Table 1 and Table 2, it seems that the strength of the LLM is essential to the method (Llama worse than ChatGPT, ChatGPT much worse than GPT-4). And the performance advantage on 6 out of 9 benchmarks is only observed with GPT-4. Additionally, though the authors show GPT-4 benefits from ToG compared to non-KG prompting (e.g., CoT), stronger prompting methods targeting for compositionality may be investigated, for example, self-ask [1]. Web search and vanilla retrieval augmentation methods can also be investigated [2].\n\nEfficiency of the method. The process of beam search and pruning with LLMs can be costly. An extensive comparison on decoding time and cost across all methods should be performed and discussed. \n\n[1] Press et al. 2022. Measuring and Narrowing the Compositionality Gap in Language Models. https://arxiv.org/abs/2210.03350\n[2] Kasai et al. 2022. RealTime QA: What's the Answer Right Now? https://arxiv.org/abs/2207.13332"
            },
            "questions": {
                "value": "Please see the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2328/Reviewer_nifq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2328/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699671885619,
        "cdate": 1699671885619,
        "tmdate": 1699671885619,
        "mdate": 1699671885619,
        "license": "CC BY 4.0",
        "version": 2
    }
]