[
    {
        "id": "21KzZlfvB6",
        "forum": "8giiPtg6rw",
        "replyto": "8giiPtg6rw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_nfcV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_nfcV"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors focus on improving the robustness of deep models without access to the training data. Specifically, given a trained model, the authors first synthesize images and adopt a soft label loss to finetune the model. With such finetuning, the model's robustness can be enhanced."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n\n2. The problem is very interesting, which enhances the model's robustness without the training data. It might be useful in some cases.\n\n3. The authors have conducted extensive experiments to validate the effectiveness."
            },
            "weaknesses": {
                "value": "1. It is not clear how you synthesize the samples. Especially, how can you adopt Eq (8) to generate the samples?\n\n2. It is not clear why the authors can adopt synthetic data to improve the model robustness while the images from other domains cannot. In my opinion, the synthetic data is also from different domains.\n\n3. Apart from adversarial training, there are also some defense techniques which does not need training data. For instance, random transforms the input data before feeding them into the model [1]. Also, I am curious if the purifier trained on open datasets, such as ImageNet, can effectively eliminate the adversarial perturbation. There might be other ways that do not need training data to defend against adversarial attacks.\n\n\n[1] Xie et al. Mitigating Adversarial Effects Through Randomization. ICLR 2018."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675203773,
        "cdate": 1698675203773,
        "tmdate": 1699636263435,
        "mdate": 1699636263435,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "il4eux4MZG",
        "forum": "8giiPtg6rw",
        "replyto": "8giiPtg6rw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_F6PX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_F6PX"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a data-free defense against adversarial (evasion) attacks for image classifiers. In cases where availability of the training data of the target model is unavailable, authors propose generating an auxillary dataset composed of synthetic images. These synthetic images are generated using the information captured within the pretrained model, and generated in a way that maximizes diversity. The paper further proposes a training method to make the most out of this synthetic data, which uses the soft predictions of the target model as supervision. This training method enforces the model to learn a flat loss landscape, which promotes adversarial robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Authors do a great job explaining their method, including qualitative results/visualizations wherever possible to convey important points regarding their method. For example, I particularly liked the usage of a toy example to demonstrate how using dynamic loss weights during synthesis process increases diversity of synthetic data. Overall, the writing quality and presentation is great.\n2. In my opinion, the methods proposed by authors are simple, intuitive, and effective. These methods include\n    - the dynamic loss weighting for synthesis loss to increase diversity,\n    - using a regularization term (term 3 in L_DFSshield) to encourage flatness of loss surface, and\n    - using a refinement strategy on top of gradients to further enforce flatness of loss surface."
            },
            "weaknesses": {
                "value": "1. The explanation of the gradient refinement strategy seems rushed. The authors say that targeting a smoother loss landscape will make it more likely that there is alignment between minima achieved using real and synthetic data. Then, the authors describe the design of their gradient refinement strategy. There is no connection presented between the initial idea (smoother loss landscape) and the proposed method to implement/enforce said idea (gradient refinement). As a result, it is not clear how the gradient refinement strategy will lead to smoother loss landscape.\n2. Furthermore, there is already a term in the training objective (3rd term) that enforces smoother loss landscape. Then, how does the gradient refinement strategy and the third loss term differ in what they are trying to achieve? The distinciton between the two is not clear.\n3. Discussion in the results section is not thorough. Authors provide speculative reasoning behind certain observations, without any support in the form of theoretical/empirical results or references to relevant prior works. In my opinion, such speculative reasoning does not add much value to the paper. For example, in section 5.3. (larger datasets), authors comment that prior works are unable to take advantange of large model capacity due to lack of diversity in synthetic samples generated by them. There are several ways this could have been backed up with results. Off the top of my head, here are few methods: (1) using tSNE plots; (2) performing k-means clustering and measuring sum of squared residuals within clusters; (3) fitting a GMM to the synthetic data generated by different methods and comparing the resulting covariance matrices. I strongly suggest the authors to provide supporting evidence to any speculative claims in the results section.\n4. There are several glaring issues in Table 3, that makes it hard to trust the numbers presented in this table.\n    a. SVHN/DFARD/ResNet-20: A_pgd (weaker attack) is lower than A_aa (stronger attack)\n    b. SVHN/DaST/ResNet-50: A_pgd is lower than A_aa\n    c. Cifar10/DFARD/WRN28-10: A_clean is lower than A_pgd\n5. The authors claim that their work is the \"first data-free adversarial defense\". This is clearly not true as one of the papers that authors cite (Nayak, 2022) proposes a similar method, ie DAD. The authors acknowledge that the only difference between their method and DAD is that DAD assumes availability of a an auxiliary dataset which is from the same domain as the training dataset of the target model. Irrespective, DAD does not require access to the original dataset, making it a data-free defense.\n6. Continuing from the previous point, authors do not compare their method with DAD. I understand the benefit of adapting other data-free methods to the adversarial training regime and comparing against them. But this is not more useful than comparing against a method that already tackles the exact same problem as the one studied in this paper (ie, no adaptation needed). This comparison is crucial for the paper.\n7. Another method that provides a data-free way for improving adversarial defense is the TTE method [b]. The authors neither cite this method nor compare against it. Overall, it is critical to include DAD and TTE as baselines.\n8. Authors do not properly explore adaptive attacks. If a defense flattens/smoothens the loss landscape, it makes gradient based attacks harder to converge (and as result, less effective). AutoAttack circumvents this issue to some extent by including a black-box attack, but this is not enough to establish the true robustness of such defenses in my opinion. Using a latent space attack [a] that doesn't rely on the (flattened) outputs of the final layer will be a more effective attack than something like PGD.\n9. Continuing my point regarding adaptive attacks, there are no results using an attack that targets all the loss terms used during training collectively. Based on my understanding, the authors perform attacks using the cross-entropy loss only. Performing an exploration regarding how the attack effectiveness changes using different combinations of the training loss terms is important in terms of developing an adaptive attack.\n\n**References**\n\n[a] Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. Adversarial manipulation of deep representations. International Conference on Learning Representations, 2016.\n\n[b] P\u00e9rez, Juan C., et al. \"Enhancing adversarial robustness via test-time transformation ensembling.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
            },
            "questions": {
                "value": "1. In case of pre-training with Derma, why is adversarial training with organC better than doing so with Derma itself (figure 1, right)? Authors comment that this occurs in rare cases without any further explanation. However 1 out of 4 is not rare. Where is this conclusion coming from? Are there additional datapoints including dataset combinations other than the ones used in the paper? What can be the reason behind this phenomenon?\n2. In figure 3, for fixed (b) and dynamic (c) coefficient methods, how often do blue points appear in red space and vice versa? If this occurs more often in (c) than (b) due to increased diversity in (c), wouldn't this make it harder for the classifier to learn discriminative features using points generated with (c)? Implying that diversification is counter productive?\n3. In Table 2, for Derma => DFME, why are the A_clean and A_pgd numbers the same?\n4. Can you please explain the issues in Table 3 (listed in Weaknesses section)?\n5. How does the proposed defense compare against DAD and TTE?\n6. How does the proposed defense fare against adaptive attacks (see description in Weaknesses section)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698746151723,
        "cdate": 1698746151723,
        "tmdate": 1699636263329,
        "mdate": 1699636263329,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3qpZm9U4Ln",
        "forum": "8giiPtg6rw",
        "replyto": "8giiPtg6rw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data-free adversarial training method, DataFreeShield, which creates a synthetic dataset and performs adversarial training on the synthetic dataset to obtain a robust model. Experiments show that the proposed method outperforms several baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper explores a novel, realistic scenario-based approach to adversarial training.\n2. The motivation of the entire framework is clear.\n3. This paper is richly designed with experiments."
            },
            "weaknesses": {
                "value": "1. Although this setup is interesting, we have doubts about its actual performance. Compared with the adversarial training model, the adversarial robustness obtained in Table 3 is very low and difficult to use in practice. Especially on CIFAR100, the robust accuracy of ResNet-20 under AA is only 5.97, and the clean accuracy is significantly lower than the normal model (60%+).\n\n2. Please analyze the time complexity of the compared methods, which is important for practical applications.\n\n3. How to extract robust knowledge from clean images is not clearly expressed in this paper. The adversarial robustness of previous work relies on pre-trained robust models, but why can adversarial robustness be obtained using only loss constraints? If this is the case, can it be used in any adversarial training? We believe that this section needs to be described in detail.\n\n4. What is the relationship between adversarial robustness and the amount of generated data?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3160/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq",
                    "ICLR.cc/2024/Conference/Submission3160/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751541993,
        "cdate": 1698751541993,
        "tmdate": 1700638402270,
        "mdate": 1700638402270,
        "license": "CC BY 4.0",
        "version": 2
    }
]