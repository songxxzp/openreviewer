[
    {
        "id": "Q7BgZtIbbU",
        "forum": "yNJEyP4Jv2",
        "replyto": "yNJEyP4Jv2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically analyzes the adversarial attacks against latent diffusion models (LDM) for the mitigation of unauthorized usage of images. It considers three subgoals including the forward process, the reverse process, and the fine-tuning process. It proposes to use the same adversarial target for the forward and reverse processes to facilitate the attack. Experiments show it outperforms baselines and is robust against super-resolution-based defense."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper studies protecting images from being used by stable diffusion models without authorization, based on adversarial perturbations. This is an important research problem.\n\n2. Different from existing empirical studies, this paper proposes a theoretical framework to help understand and improve adversarial attacks.\n\n3. Experimental results show the proposed method can outperform existing baselines."
            },
            "weaknesses": {
                "value": "1. Not clear if this method needs white-box access to the subject LDM. That is, do the adversarial attackers use the same network used by infringers? Are the adversarial examples generated on one model/method still effective on different or unknown models/methods?\n\n2. Only one defense method is evaluated. Are the adversarial samples robust to other transformations such as compression or adding Gaussian noises? Also, no adaptive defense is evaluated. If the infringers know about this adversarial attack, can they adaptively mitigate the adversarial effects?\n\n3. From my understanding, Liang & Wu, 2023 and Salman et al., 2023 are not just \"targeting the VAE\" as claimed in this paper. They attacked the UNet as well.\n\n4. Many issues in the writing. \n\n    4.1. On Page 2, \"serve as a means to\" -> \"mean\". \n\n    4.2. In Section 2.2, it says \"As shown in Figure 1 adversarial attacks create an adversarial example that seems almost the same as real examples\". However, Figure 1 only contains the generated images by the infringers instead of the adversarial examples as indicated by the title.\n\n    4.3. The references to figures and tables are incorrect such as \"Figure 5.3\", \"Table 5.2\", \"Table 5.3\", etc. In the ablation study, the caption of the table is \"Figure 5\".\n\n    4.4. Some math symbols are not defined where they first appear. For example, It would be better to mention that $\\phi$ in Section 2.1 means the VAE. I suggest to use $\\mathcal{E}\\_{\\phi}$ or $\\mathcal{E}$ consistently. What does $\\sqrt{\\bar{\\alpha\\_t}}$ (the line below equation 5) mean? The text below Equation 10, for \"$q\\_{\\phi}(v\\_t | x')$ and $q\\_{\\phi}(v\\_t | x')$\", the second one should be $q\\_{\\phi}(v\\_t | x)$. It would be better to briefly explain the N, M, and K in  Algorithm 2.\n\n    4.5 The citation of SR in section E.2 is wrong. \"Salman et al., 2023\" -> \"Mustafa et al. 2019\"."
            },
            "questions": {
                "value": "1. Could you explain why the equation 4 holds? Why do we need to use $q$ to express the left $p$?\n\n2. Can the adversarial examples be effective for different or unknown models/methods? \n\n3. According to section E.1, the target $\\mathcal{T}$ in Equation 15 and 16 is defined as $\\mathcal{E}(x^{\\mathcal{T}})$. I can understand for the $\\mathcal{L}\\_{vae}^{\\mathcal{T}}$, it's meaningful to encourage $\\mathcal{E}(x')$ to be close to $\\mathcal{E}(x^{\\mathcal{T}})$. However, for the UNet part, what's the rationale to encourage the predicted noise at each timestep to be close to $\\mathcal{E}(x^{\\mathcal{T}})$? Because I think the final output $z_0$ should be close to $\\mathcal{E}(x^{\\mathcal{T}})$, but not the intermediate predicted Gaussian noise."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Reviewer_WV9g"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698086302397,
        "cdate": 1698086302397,
        "tmdate": 1699636438758,
        "mdate": 1699636438758,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uIhyIKoSHb",
        "forum": "yNJEyP4Jv2",
        "replyto": "yNJEyP4Jv2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_19tS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_19tS"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a theoretical framework for understanding adversarial attacks on latent diffusion models. Based on this framework, the paper proposes a novel and efficient adversarial attack method that exploits a unified target to guide the attack process in both the forward and reverse passes of latent diffusion models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper focuses on curbing the misuse of powerful LDM-based generative models for unauthorized real individual imagery editing, which is an important topic for securing privacy.\n\n2. The theoretical foundation behind adversarial attacks on diffusion models is built, which contributes to the understanding of the behaviors of adversarial attacks."
            },
            "weaknesses": {
                "value": "1. More thorough examination accounting for a wider range of generative techniques could further validate the method's real-world utility and limitations.  While the proposed attack focuses on the prevalent LDM framework, its generalization to other powerful generative paradigms like SDXL, DALL-E, and Deep Floyd remains untested. \n\n\n2. A more powerful baseline of PhotoGuard, i.e. Diffusion Attack is not compared to. This comparison could help gauge the true leadership of the new method. Without including this more powerful adversarial technique, the paper's claims about the proposed attack outperforming the current approaches remains uncertain.\n\n3. The authors assert a memory-efficient design but do not provide details to support this claim. Further explanation or experimental evaluation of memory usage compared to alternative approaches would help validate the proposed method's efficiency advantages."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672544427,
        "cdate": 1698672544427,
        "tmdate": 1699636438677,
        "mdate": 1699636438677,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rc8L3lVtjO",
        "forum": "yNJEyP4Jv2",
        "replyto": "yNJEyP4Jv2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_oH1F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_oH1F"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed at a theoretical framework for adversarial attacks on Latent Diffusion Model (LDM).  The key to the theory is formulating an objective to minimize the expectation of the likelihoods conditioned on adversarial samples, of which the two terms implemented within the LDM explains the adversarial attack on the VAE and on the noise predictor, respectively. In addition, a new adversarial attack combining the two types of adversarial attacks are proposed."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. Adversarial attacks on LDM is an interesting and practical problem.\n2. Various experiments are conducted."
            },
            "weaknesses": {
                "value": "The proposed theoretical framework is not sufficiently innovative. In addition, the methodology exists many errors and the experimental verification are not sound. Specifically,\n\n1. The key formulation of minimizing the conditional likelihood is trivial. The given theoretical proof is complicated. Actually, the likelihood equivalent to the KL divergence has been well-know. From this perspective, the proof is somehow trivial.\n\n2. There exists many wrong equations.   \na. In Eq. (5), the left term q(v_t|x) should be equal to the integral of the right term. Similar issue in Eq. (8).  \nb. In the first paragraph of Sect 3.2, q(v_t|v_{0:t-1} is mistakenly formulated.  \nc. In the last paragraph of Page 4, z_{t-1} is mistakenly formulated.  \nd. In Eq. (3), the sum in terms of z is mistakenly formulated given the expectation.   \ne. For Eq. (3), (4), (9)\u2026, p()|x=x\u2019 or p()|x\u2019 is inappropriate, which should be put as the subscript or p(|x=x\u2019).  \nf. The reformulation of z as v is unnecessary.  \n\n3. From the empirical results (Table 1), the strategy of combining two adversarial attacks does not perform significantly better than the Eq. (16). This raises doubt about the effectiveness of the newly proposed attack method.\n\nMinor:\nSome references are wrongly denoted, e.g. Figure 5.3."
            },
            "questions": {
                "value": "1. The proposed theoretical framework is not sufficiently innovative. \n\n2. The methodology exists many errors\n\n3. The experimental verification are not sound"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698727271099,
        "cdate": 1698727271099,
        "tmdate": 1699636438596,
        "mdate": 1699636438596,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hgoSKVHZ92",
        "forum": "yNJEyP4Jv2",
        "replyto": "yNJEyP4Jv2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_8G7v"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a method for improving adversarial attacks on Latent Diffusion Models (LDMs). The purpose is to generate adversarial examples preventing the LDMs from generating high-quality copies of the original images in order to protect the integrity of digital content. The authors mathematically introduce a theoretical framework formulating three sub-goals that existing adversarial attacks aim to achieve. This framework exploits a unified target to guide the adversarial attack both in the forward and the reverse process of LDM. The authors implement an attack method jointly optimizing three sub-goals, demonstrating that their method outperforms current attacks generally. The experiments are focused on the attacks on training pipelines, including SDEdit and LoRA."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe author conducted extensive mathematical derivations, providing mathematical explanations for the existing methods. \n\n2.\tThe experiments show that this method outperforms the baseline in most criteria and succeeded in attacking LoRA and SDEdit with Stable Diffusion v1.5."
            },
            "weaknesses": {
                "value": "1.\tThe only backbone model used in the experiments is Stable Diffusion v1.5. \nThere are plenty of more recent LDMs, such as Stable Diffusion v2.1 [1] and DeepFloyd/IF [2]. Will this method perform well in more advanced LDMs?\n\n2.\tThe pseudo-code of algorithm 1 seems redundant and demonstrates nothing. \nIt literally equals to its description: \u201cTo optimize J_{ft}, we first finetune the model on x and obtain the finetuned model \u03b8(x). Then, we minimize J_{ft} over x\u2032 on the finetuned model \u03b8(x).\u201d\n\n\n3.\tThe target image adopted by Liang & Wu (2023), visualized in \u201cFigure E.1\u201d (Is it mislabeled in Figure 8?), is the only target image used in the experiments. Did the authors try using different target images? Will the target image affect the effectiveness of this method?\n\n4.\tThere are issues in the document layout. The labels of figures are mismatched to those in the texts. \n\n5.\tThe offset problem needs to be clarified.\nThe authors claim that \u201cThe result in Figure 5.3 implies that offset takes place in 30% - 55% of pixel pairs in \u0394_z_t and \u0394_\u03b5_\u03b8, which means that maximizing J_q pulls \u0394_z_t to a different direction of \u0394_\u03b5_\u03b8 and interferes the maximization of J_p.\u201d Could the authors further explain it and Figure 3 in Section 4.1?\n\n[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\n\n[2] Mikhail Konstantinov, Alex Shonenkov, Daria Bakshandaeva, and Ksenia Ivanova. Deepfloyd. https://github.com/deep-floyd/IF, 2023."
            },
            "questions": {
                "value": "1.\tWill this method perform well in more advanced LDMs like Stable Diffusion v2.1 and DeepFloyd/IF?\n\n2.\tWill the target image affect the effectiveness of this method? Would the authors use other images as targets and test the effectiveness?\n\n3.\tIs that a typo in \u201cIn this tractable form, z\u2032_t and z_t sampled from q_\u03c6(v_t|x\u2032) and q_\u03c6(v_t|x\u2032), respectively\u201d below Equation 10?\n\n\n4.\tHow does maximizing J_q pull \u0394_z_t to a different direction of \u0394_\u03b5_\u03b8 and interfere the maximization of J_p? Could the authors further explain Figure 3 in Section 4.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827908361,
        "cdate": 1698827908361,
        "tmdate": 1699636438490,
        "mdate": 1699636438490,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vkl2nPj24f",
        "forum": "yNJEyP4Jv2",
        "replyto": "yNJEyP4Jv2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
        ],
        "content": {
            "summary": {
                "value": "Since diffusion and other advanced generative models have been used to replicate artworks and create fake content, a line work proposes a defense mechanism that adds a kind of adversarial perturbation to the protected images to prevent the adversary from fine-tuning their model on the images. This work proposes a more thorough theoretical formulation of the problem compared to the prior work and relies on this formulation to build an empirically stronger attack."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "### Significance\n\nI believe that this paper addresses an important problem with a widespread impact on both the technical community as well as society at large. The empirical results show a convincing improvement over the prior works on two different fine-tuning methods and two datasets.\n\nI believe that introducing the target image for the adversarial objective and hence conducting a targeted attack instead of an untargeted one have a large effect on the empirical success of the attack."
            },
            "weaknesses": {
                "value": "### Correctness and clarity of the theoretical results\n\nThe paper formulates an adversarial optimization problem particularly tailored for the latent diffusion models (LDM). The analysis guides the algorithm design to some degree (more on this later). However, due to the lack of clarity and various approximations being introduced without proper justification, the theoretical results become less convincing. I will compile all my questions and concerns from Section 3 and 4 in place:\n\n1. I am not sure what the sum $\\sum_z$ is over in Eq. (3). The expectation is already over $z$ so I am a bit confused about the summation. My guess is that the sum is over all the latent variables in the diffusion process (different $z$\u2019s in different steps). Is this correct?\n2. If my previous understanding is correct, my next question is why should the adversary care about the latent variables in the intermediate steps of the diffusion process instead of, say, the final step of the inverse process before the decoder?\n3. Based on the text, Eq. (3) should be equivalent to $\\mathbb E_{z \\sim p_{\\theta}(z|x)}[- \\log p_\\theta(z|x')]$. My question is that a slightly different formula $\\mathbb E_{z \\sim p_{\\theta}(z|x')}[- \\log p_\\theta(z|x) + \\log p_{\\theta}(z|x')]$ also seems appropriate (swapping order in the KL-divergence). Why should we prefer one to the other?\n4. Section 3.2 uses the notation $\\mathcal N(\\mathcal E(x), \\sigma_\\phi)$ instead of $\\mathcal N(f_{\\mathcal E}(x), \\sigma_{\\mathcal E})$ from Section 2.1. Do they refer to the same quantity?\n5. In the last paragraph of page 4, the Monte Carlo method must be used to estimate the mean of $p_\\theta(z_{t-1}|x)$, but I cannot find where the mean is actually used. It does not seem to appear in Eq. (10) or in Appendix A.1. I also have the same question for the variance of $p_\\theta(z_{t-1}|x)$ mentioned in the first paragraph of page 5.\n6. Related to the previous question, it is mentioned that \u201cthe variance of $z_{t-1}$ is estimated by sampling and optimizing over multiple $z_{t-1}$.\u201d It is very unclear what \u201csampling\u201d and \u201coptimizing\u201d refer to here.\n7. I do not quite see the purpose of Proposition 1. It acts as either a definition or an assumption to me. The last sentence \u201cone can sample $x \\sim w(x)$ from $p_{\\theta(x)}(x)$\u201d is also very unclear. Is the assumption that the true distribution is exactly the same as the distribution of outputs of the fine-tuned LDM?\n8. $x^{(eval)}$ is mentioned in Section 3.4 but was never defined.\n9. In Eq. (11), should both of the $\\theta(x)$\u2019s be $\\theta(x')$ instead? Otherwise, $x'$ has no effect on the fine-tuning process of the LDM.\n10. Section 4.1 is very convoluted (see details below).\n\n### Issues with the offset problem and Section 4.1\n\n**Comment #1**: I do not largely understand the purpose of the \u201coffset\u201d problem in Section 4.1. In my understanding, most of the discussion around the offset can be concluded by simply expanding the second term on the first line of Eq. (13):\n\n$$\n\\sum_{t \\ge 1}\\mathbb E_{z_t,z'_t} || \\Delta z_t +  \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}} \\Delta \\epsilon ||_2^2 \n$$\n\n$$\n= \\sum_{t\\ge 1}\\mathbb E_{z_t,z'_t} ||\\Delta z_t||_2^2 + || \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\Delta\\epsilon ||_2^2 + \\frac{2\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\Delta z_t^\\top\\Delta\\epsilon\n$$\n\nSo the problem that prevents optimizing just the norm of $\\Delta z_t$ and the norm of $\\Delta \\epsilon_\\theta$ directly is the last term in the equation above (the dot product or the cosine similarity). I might be missing something here so please correct me if I\u2019m wrong.\n\n**Comment #2**: It is also unclear to me how the last line of Eq. (13) is reached and what approximation is used.\n\n**Comment #3**: In theory, there is nothing preventing one from optimizing Eq. (13) as is. The issue seems to be empirical, but I cannot find the empirical results showing the failure of optimizing Eq. (13) directly and not using the target trick.\n\n**Comment #4**: The authors \u201clet *offset rate* be the ratio of pixels where the vector $\\Delta z_t$ and $\\Delta \\epsilon_\\theta$ have different signs.\u201d If my understanding of the cosine similarity above is correct, this seems unnecessary and imprecise given that the cosine similarity is the exact way to quantify this.\n\n**Comment #5**: In the first paragraph of page 7, it is mentioned that \u201cmeanwhile, since the original goal is to maximize the mode of the vector sum of\u2026\u201d I think instead of \u201cmode,\u201d it should be \u201cmagnitude\u201d or the Euclidean norm?\n\n### Empirical contribution\n\n1. After inspecting the generated samples in Figure 11-15, my hypothesis is that the major factor contributing to the empirical result is the target pattern and the usage of the targeted attack. The pattern is clearly visible on the generated images when this defense is used, and this pattern hurts the similarity scores. This raises the question of whether the contribution comes from the theoretical formulation and optimization of the three objectives or the target. I would like to see an ablation study on this finding: (1) the proposed optimization + untargeted and (2) the prior attacks + targeted.\n2. The choice of the target $\\mathcal T$ is ambiguous. While the target pattern is shown in the Appendix, there is no justification for why such a pattern is picked over others and whether other patterns have been experimented with.\n\nOverall, I believe that the paper can have a great empirical contribution, but it seems to be clouded by the theoretical analysis which appears much weaker to me."
            },
            "questions": {
                "value": "1. What are the approximations made on the fourth line of Eq. (22) and in Eq. (23)?\n2. Why are MS-SSIM and CLIP-SIM used as metrics for SDEdit whereas CLIP-IQA score is used for LoRA? The authors allude to this briefly, but it still largely remains unclear to me.\n3. The similarity metrics used in experiments seem to focus on the low-level textured detail rather than the style (please correct if this is not accurate). I am wondering if a better metric is the one that measures \u201cstyle similarity\u201d between the trained and the generated images. This might align better with the artwork and the copyright examples.\n4. For the results reported in Table 1, how many samples or runs are they averaged over? Based on the experiment setup, 100 images are used for training the model in total for each dataset, and they are grouped in a subset of 20. So my understanding is that there are 100/20 = 5 runs where 100 images are generated in each run? Is this correct?\n5. The fine-tuning hyperparameters for LoRA are mentioned in Section 5.1. Does the LoRA fine-tuning during the attack and the testing share the same hyperparameters? What happens when they are different (e.g., the adversary spends iterations during fine-tuning, etc.)? Can the proposed protection generalize?\n6. Have the authors considered any \u201cadaptive attack\u201d where the adversary tries to remove the injected perturbation on the images (e.g., denoising, potentially via another diffusion model) before using them for fine-tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4599/Reviewer_FuAb"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4599/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827987092,
        "cdate": 1698827987092,
        "tmdate": 1699636438359,
        "mdate": 1699636438359,
        "license": "CC BY 4.0",
        "version": 2
    }
]