[
    {
        "id": "DISxvLUses",
        "forum": "rfz3K3yyU4",
        "replyto": "rfz3K3yyU4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_1dmy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_1dmy"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a model poisoning data reconstruction attack on language models by first recovering intermediate activations."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- the topic of research of attacks on federated learning of language models is especially important given the recent surge of privacy concerns and large language models\n- authors provide comparison with recent state of the art methods\n- qualitative illustration of recoveries help understand better the relationship between ROUGE metrics and the reconstruction quality"
            },
            "weaknesses": {
                "value": "Major. \n\nThe authors do not specify the threat model nor what their attack does in a high-level fashion, they only detail the specifics of the optimization tricks used but never the high-level flow: what do they observe ? what can they do/change ? etc. Considering the fact that authors seem to manipulate the architecture of the model, changing dimensions of layers and switching activation functions, the reviewer is guessing that the authors are doing model poisoning (\"Malicious attacks\"). But nowhere do the authors explain the general steps of the attack. Even when presenting related works, the positioning of this work with respect to the state of the art is not clear (especially wrt LAMP and Wang et al. 2023). Referencing Balunovic, Figure 1 or section 4 are not enough for readers to quickly understand the method's general principle and especially the setting in which the authors work.\n\nThe authors' contribution of \"identify[ing] an issue with the gradient based attack: the gradient will be averaged in the context of large batch sizes and long sentences, thereby diluting ...\" is not novel as do the many works on optimization-based data reconstruction attacks cited by the authors attest (DLG, GradInv, etc.). The contribution of targeting intermediate feature maps is not very novel either (see i.e. [1])\n\nThe authors' tone is too colloquial. A salient instance of this is the use of the \"Revolutionary\" adjective (or subtle/clever page 5) to qualify the authors' method. This qualifier is subjective. For instance the reviewer highly disagrees that the author's method is revolutionary. A scientific article should aim at remaining as neutral and objective as possible.\n\nThe authors describe their model poisoning attack as \"subtle\" (page 5), specifically they highlight a limitations of the work of Wang et al. 2023: the fact that Wang et al.'s attack is easy to detect. The reviewer does not understand how changing the model's architecture by adding orders of magnitude more dimensions and switching to non sparse activations is \"subtle\" or even more subtle than Wang's.\n\nIt is not clear to the reviewer why is the optimization objective of using the cosine similarity between recovered intermediate feature and Pooler input is even possible. Isn't the input of the Pooler layer exactly what we want to recover ?\n\nThe reviewer, in spite of being very familiar with the attack on gradients' literature and FL, has troubles understanding what the authors did therefore 1. the strong reject assessment 2. the short review and 3. the lack of comments on the results and their interpretation.\n\n\n[1] Kariyappa, Sanjay, et al. \"Cocktail party attack: Breaking aggregation-based privacy in federated learning using independent component analysis.\" International Conference on Machine Learning. PMLR, 2023."
            },
            "questions": {
                "value": "The reviewer encourages the authors to:\n- add a paragraph on the threat model and a bird's eye view illustration of the method's in the setting described (more high-level than Figure 1)\n- position their work precisely with respect to the related work specifically wrt LAMP and Wang et al. 2023: what are the innovations ?\n- rework the text by 1. making it clearer what the method does and 2. removing most of the subjective statements related to the quality of the present work\n- answer question on the optimization objective (see above)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Reviewer_1dmy"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698334432269,
        "cdate": 1698334432269,
        "tmdate": 1699636343419,
        "mdate": 1699636343419,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LZ7rp9GpCQ",
        "forum": "rfz3K3yyU4",
        "replyto": "rfz3K3yyU4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_eYy2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_eYy2"
        ],
        "content": {
            "summary": {
                "value": "Privacy attacks could extract sensitive information from large language models (LLM) in federated learning (FL), either with limited batch size or being detectable. The authors aim to recover texts in various batch-size settings yet challenging to detect with the constraint of LLM  equipped with a unique Pooler layer.\n\nThe solution is to recover the intermediate feature to provide enhanced supervisory information.  This Pooler layer captures a comprehensive representation of the input text. A two-layer neural network-based reconstruction technique is used to retrieve the inputs destined for this layer meticulously. The method provides a continuous supervisory signal, offering additional feature-level guidance that assists optimization-based attacks. \n\nBy combining gradient inversion and prior knowledge, the proposed approach achieves better results on different datasets, tailored with different batch sizes (e.g., 1,2,4,8)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The idea works well in success rate and improves existing attacks. Also, it is absorbing from my perspective.\n+ Extract a unique and concise problem to solve in LLM privacy.\n+ The first to suggest utilizing intermediate features as continuous supervised signals."
            },
            "weaknesses": {
                "value": "- The presentation should be improved. [see Question 3, Question 4, Question 5, Question 6]\n- Experimental settings. [see Question 7]\n- Require more clear discussion/comparison on related works. [see Question 1, Question 2]"
            },
            "questions": {
                "value": "1. Could the authors illustrate more related works on intermediate features? For example, [HKHG+] proposes to ''examine the representations in intermediate feature maps ... ''.\nThe authors claim, \"The first to suggest utilizing intermediate features...\".  I feel a little confused about the \"the first\" here. Could the authors elaborate on it?\n\n2. In Section 2.2, \"Nonetheless, numerous studies have highlighted the risks associated with textual information.\" Could the authors explain more about the conclusions/findings that have been studied? What are explicit risks specific to LLM textual information? Given my understanding of the abstract, the authors target solving hurdles of attacks (\"extracting sensitive data from LLM in federated learning\"). Is any additional challenge introduced in the *federated LLM* compared with LLM/FL?\n\n3. What is \"[CLS]\" in the introduction?\n\n4. Could the authors detail the security model? In federated learning, clients and a server exist in common. For the proposed attack, who is the adversary, and what is the adversary's ability? What is the assumption of all participants in FL? If the adversary uses the intermediate features, does it mean the adversary has more knowledge (i.e., weaker security assumption) than previous attacks? What is the explicate attacking goal?\n\n5. Many notations in Section 3 are missing. For example, what is the hat in the equation 3? In Equation 2, $B$ is suddenly used. What are \"(3)\" and $\\otimes 3$ in Equation 5?\n\n6. Section 4 provides many optimization signals. Could the authors bridge the experimental findings and theoretical conclusions in (Wang et al., 2023)? Could the authors give a high-level walkthrough of various optimizations?\n\n7. How do authors compare with previous arts in the experiments? For example, Table 1 shows better results with different batch sizes. In the abstract, the authors point out that previous works have limited batch size. Continuously, could the authors explain more about why the previous works become worse when enlarging the batch size?\n\n\n[HKHG+] Enhancing Adversarial Example Transferability with an Intermediate Level Attack. Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim (ICCV' 2019)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Reviewer_eYy2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698570823124,
        "cdate": 1698570823124,
        "tmdate": 1699636343304,
        "mdate": 1699636343304,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xIQe8mCNBk",
        "forum": "rfz3K3yyU4",
        "replyto": "rfz3K3yyU4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_GEoJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_GEoJ"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on data extraction attack against large language model in the federated learning setting. The authors propose a novel attack by leveraging the input of the Pooler layer of language models to offer additional feature-level guidance that effectively assists optimization-based attacks. Evaluations on benchmark text classifcation datasets demonstrate the effectiveness of the proposed method with different batch sizes and models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- important research topic\n- novel attack methodology\n- well-structured paper"
            },
            "weaknesses": {
                "value": "- only evaluate on text classification\n- lacking cost analysis\n- possible countermeasures are needed"
            },
            "questions": {
                "value": "- The authors demonstrate the effectiveness of the proposed method on text classification tasks. However, it is unclear how well it would perform on other types of tasks.\n\n- I appreciate the authors' effort on presenting the superior performance of the attack. In addition, a cost analysis (time and resource) would be good to understand the trade-off on different attacks.\n\n- This paper does a great job in presenting a powerful attack. The authors are suggested to discuss (and better evaluate) possible countermeasures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717070197,
        "cdate": 1698717070197,
        "tmdate": 1699636343226,
        "mdate": 1699636343226,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OWnKKd2srb",
        "forum": "rfz3K3yyU4",
        "replyto": "rfz3K3yyU4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_7zQ4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_7zQ4"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new attack method to enhance the text recovery rate of language models under the Federated Learning setting. It is based on two techniques: (1) leveraging gradient data and prior knowledge to extract sensitive information (Zhu et al., 2019; Deng et al., 2021; Balunovic et al., 2022; Gupta et al., 2022), and (2) Two-Layer Neural Network-Based Reconstruction (Wang et al., 2023), whose results will be used as the prior knowledge. By combining these two techniques together, the proposed method tries to address existing challenges in enhancing the recovery rate of text in larger batch-size settings while being hard to detect and defend against. This paper compares the proposed method with existing baseline methods and proves its superiority."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper studies a very interesting problem, which is the attacks on language models under the Federated Learning setting.\n2. The proposed methods achieve better results than existing baselines."
            },
            "weaknesses": {
                "value": "1. The proposed method is based on the existing method and applies it to the language model under the federated learning setting, which fits the setting of the existing method well. The contribution is limited.\n2. This paper does not solve the batch size issue efficiently. Since the batch size (i.e., 8) that the proposed method can work well on is still very small compared to common settings for batch sizes.\n3. This paper does not demonstrate how existing defense methods work to defend against the proposed attack, or in other words, how the proposed attack performs against the defense methods."
            },
            "questions": {
                "value": "1. Does this method require the attacker to know the attacked model's structure, such as whether it has a Pooler layer or not, as a priori?\n2. In the text domain, what kind of information is considered private? For instance, if there's a phrase 'this food is \u2026 ,' and then 'delicious' is recovered, is this considered private information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3849/Reviewer_7zQ4"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814150154,
        "cdate": 1698814150154,
        "tmdate": 1699636343137,
        "mdate": 1699636343137,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tUCfZYhxew",
        "forum": "rfz3K3yyU4",
        "replyto": "rfz3K3yyU4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_tBfV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3849/Reviewer_tBfV"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the gradient inversion attack, that is to reconstruct the data from the model gradient. The paper focuses on the attack for language models such as BERT. The key idea is to recover the intermediate feature, the input before the pool layer. Then, it applies a previous gradient-matching attack but adds feature matching to the loss. It empirically shows that with this additional feature matching loss, the recovery accuracy can be improved."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The presentation and clarity are generally great.\n2. The studied problem is to validate an important privacy vulnerability, which motivates the study of privacy.\n3. The evaluation is systematic. The proposed methods together with baselines are evaluated cross 3 benchmark datasets and various batch sizes."
            },
            "weaknesses": {
                "value": "My main concern is that the proposed attack only works with some constraints: certain types of activation, a large enough hidden dimension of the Pooler layer, and random initialization for most variables in the Pooler layer. These constraints seem to deviate from the popular design in the usage of large language models.\n\nIn Table 1, \"Ours\" is evaluated with two different activations in the architecture, SELU and $x^3+x^2$. Then, it is not clear to me what architecture was used for baseline evaluation. Should each baseline also have two rows corresponding to two activations?\n\nIt is not well explained why each unique $x_i$ can be reconstructed by Equation 5&6 illustrated on page 4. Especially, when batch size $B$ is large enough, if I understand it correctly, it is possible that there are multiple solutions for $x_i$."
            },
            "questions": {
                "value": "Please see the \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3849/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698831832947,
        "cdate": 1698831832947,
        "tmdate": 1699636343038,
        "mdate": 1699636343038,
        "license": "CC BY 4.0",
        "version": 2
    }
]