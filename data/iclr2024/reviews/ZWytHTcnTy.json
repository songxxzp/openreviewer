[
    {
        "id": "PGGCuBrKA1",
        "forum": "ZWytHTcnTy",
        "replyto": "ZWytHTcnTy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_sjbY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_sjbY"
        ],
        "content": {
            "summary": {
                "value": "This manuscript presents a CAT-Seg model for open-vocabulary semantic segmentation. They utilize the cosine similarity to compute the initial cost between image and text embeddings and refine it with a cost aggregation stage. As the core stage, CAT-Seg decomposes it into spatial and class aggregation, which enables the end-to-end fine-tuning for open-vocabulary semantic segmentation. Experiments on in-domain dataset, including ADE20K, PASCAL VOC, PASCAL-Context, and multi-domain MESS dataset demonstrate that CAT-Seg can well-process the unseen class."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "#1 Transferring the cost aggregation from image-to-image correlation to image-to-text correspondence. The extended multi-modality cost aggregation addresses the spatial relations in images, the permutations invariance of classes and the variable number of classes that occurred in inference.\n\n#2 Consider the challenges of variable number of categories and the unordered input arrangement, the class aggregation enables the handling of sequences of arbitrary length and the permutation to input classes."
            },
            "weaknesses": {
                "value": "#1 Though the authors state that class aggregation is permutation invariance, it is not clearly presented how the unordered input arrangement is solved? In Eq.(3), it only performs several transformer blocks and cannot prove that the function is permutation invariance. This statement also has not been verified in the experiment. \n\n#2 Lacking the evaluation on the upsamping decoder. The authors conduct further aggregation with light-weight convolution layers, but do not further verify it in the ablation."
            },
            "questions": {
                "value": "Firstly, the author should make it clear why Eq.(3) can address the permutation invariance to the inputs. The reviewer suggest the author provide the visualization to the permuation invariance so that it can be understoodble to the reader. \nSecondly, the impact of upsmapler selection can be conducted in the ablation study part as the upsampler is critical to high-resolution semantic segmentation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761333917,
        "cdate": 1698761333917,
        "tmdate": 1699636169929,
        "mdate": 1699636169929,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ohkaj4hQVO",
        "forum": "ZWytHTcnTy",
        "replyto": "ZWytHTcnTy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_qcKD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_qcKD"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles open-vocabulary semantic segmentation by leveraging the cost volumes. Specifically, the correlations are obtained from text and visual information, and the spatial and class correlations are optimized, in order to refine the coarse correlation map to the finer ones, such that fine segmentation predictions can be made. Decent performance has been achieved on popular benchmarks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The overall idea is straightforward and easy to understand.\n\n2. The motivation is simple and clear: obtain finer predictions by refining the coarse correlations"
            },
            "weaknesses": {
                "value": "The main weakness lies in the incremental contribution. The optimization of correlations from coarse to fine has been extensively explored in the field of semantic correspondence [1][2][3][4]. Furthermore, similar concepts have been applied to the community of segmentation [5][6].\n\n\nDrawing from my experience, the success of optimizing dense correlations from coarse to fine in segmentation  [5][6] has demonstrated the effectiveness of this approach. \n\nThe difference between few-shot segmentation and open-vocabulary segmentation lies in the source of the correlation. In few-shot segmentation, the correlation is obtained from query and support images, while in open-vocabulary seg it is from the text-image pair. But the later processing pipelines are identical. Therefore, applying it to zero-shot segmentation is certainly feasible. \n\nMoreover, the proposed spatial and class correlation modeling actually together behave like the 4-D convolutions that have been leveraged and well exploited in the aforementioned methods.  \n\n\nConsidering the widely verified techniques employed in this paper, the method itself lacks sufficient technical contribution when compared to the aforementioned methods. Therefore, without a major improvement/change regarding the core idea of the method, I may maintain my negative rating.\n\n\n\nReferences:\n\n[1]: TransforMatcher: Match-to-Match Attention for Semantic Correspondence. CVPR 2022\n\n[2] Learning to Compose Hypercolumns for Visual Correspondence. ECCV 2020\n\n[3] CATs: Cost Aggregation Transformers for Visual Correspondence. Nips 2021\n\n[4] CATs++: Boosting Cost Aggregation with Convolutions and Transformers. TPAMI 2022\n\n[5]: Hypercorrelation Squeeze for Few-Shot Segmentation. ICCV 2021\n\n[6]: Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation. ECCV 2022"
            },
            "questions": {
                "value": "Please respond to my questions in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699358757867,
        "cdate": 1699358757867,
        "tmdate": 1699636169868,
        "mdate": 1699636169868,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "m6VmLobnKD",
        "forum": "ZWytHTcnTy",
        "replyto": "ZWytHTcnTy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_3c8N"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2373/Reviewer_3c8N"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors adopt a cost-aggregation mechanism to improve the open-vocabulary segmentation task. To be specific, the authors conduct the spatial- and channel-level aggregation based on the cost maps rather than the raw feature embedding from the CLIP or other open-set models, which help the model gain finer details from the cost map and optimize this cost transportation problem by the model. And they reach competitive segmentation performances on open-vocabulary settings among several benchmarks.."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Competitive open-vocabulary segmentation performances are obtained.\n2. The writing of this paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. My main concern of this paper lies in the novelty. It seems that this paper applies a cost-map optimization method to model the relationship between the image embeddings and test embeddings, which have already been studied widely, such as OTA problem.\n2. I think the interpretation of this paper about the cost-map is some kind of poor. Like, the authors do not demonstrate the learned embeddings of the cost-map or relationships between the image embeddings and text embeddings. It is hard to know why applying a cost-optimization following the feature embedding can improve the open-vocabulary segmentation task, assigning more accurate text labels to image pixels embeddings?\n3. The reason why the authors choose different transformer blocks to implement the spatial- and channel-aggregation is not well presented and such ablative studies are missing.\n4. The decoder upsampling layer proposed by this paper has already been applied as a common layer for the segmentation community and the inspiration of this part is less novel."
            },
            "questions": {
                "value": "1. In think the author should compare their method with other related cost-optimization papers on the segmentation task or the open-vocabulary task.\n\nFor example,  Learning object-language alignments for open-vocabulary object detection.\n\nAnd more important thing is that this paper shares much similar idea with paper, CATs: Cost Aggregation Transformers for Visual Correspondence NeurIPS 2021. The authors should really explain more in-depth difference between these two papers. Otherwise, I think the novelty of this paper dose not meet the standard of this conference.\n\n2. The visual demonstration of the cost map between image embeddings and text embedding should be provided to help us capture the point that cost-map here indeed does something.\n\n3. Some ablative studies are missing as I mentioned above.\n\n4. The introduction of decoding layer in this paper should be well demonstrate the motivation and the difference with existing works."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2373/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2373/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2373/Reviewer_3c8N"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2373/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699398746140,
        "cdate": 1699398746140,
        "tmdate": 1699636169800,
        "mdate": 1699636169800,
        "license": "CC BY 4.0",
        "version": 2
    }
]