[
    {
        "id": "IAct9y1gmF",
        "forum": "9n9q0R9Gyw",
        "replyto": "9n9q0R9Gyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_aEzM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_aEzM"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a Retrieval-Augmented Text-to-3D generation framework that uses an empirical distribution of retrieved 3D assets. The method incorporates geometric information from retrieved 3D assets based on the text prompt from existing 3D asset datasets to improve the geometric quality and consistency of the generated 3D shapes. The study introduces two techniques: Lightweight Adaptation, addressing biases in camera viewpoints, and Delta Distillation, targeting 3D content artifacts."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The study proposes an approach to Text-to-3D generation that leverages retrieved 3D assets, potentially improving the quality and geometric consistency of generated objects.\n2. The introduction of Lightweight Adaptation and Delta Distillation techniques aims to reduce biases from camera viewpoints and diminish artifacts in the 3D content, which have been challenging in previous models."
            },
            "weaknesses": {
                "value": "1. The experiments provided seem to be limited and do not fully substantiate the claims made. The supplementary materials only provided the videos of the generated objects with the same text prompts as in the paper.\n2. The generation diversity could be an issue, for example in Figure 15, the three particles of the generated corgi are very similar.\n3. The idea of Lightweight Adaptation is not new and the idea of Delta Distillation is mainly adopted from the Delta denoising score[1]. \n4. The presentation of the methodology part is not clear and intuitive, it seems the author tried to overcomplicate the narratives.\n5. The authors have chosen not to disclose specific details about how they transform the retrieved 3D assets into the 3D shape representations \\theta to acquire the Asset-based distribution. This process could be time and computationally intensive.\n\n\n\n[1] Hertz, Amir, Kfir Aberman, and Daniel Cohen-Or. \"Delta denoising score.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
            },
            "questions": {
                "value": "1. In section 4.1, how do you transform the retrieved mesh into the 3D representations?\n2. What about the efficiency of this method? It seems to require a lot of time and computational resources to generate a single object from a text prompt."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2402/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2402/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2402/Reviewer_aEzM"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755762036,
        "cdate": 1698755762036,
        "tmdate": 1699636175465,
        "mdate": 1699636175465,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x3J6OYf5G7",
        "forum": "9n9q0R9Gyw",
        "replyto": "9n9q0R9Gyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_U742"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_U742"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of Text-to-3D generation building on top of the ProlificDreamer variational score distillation (VSD) by integrating retrieved 3D assets into the model optimization procedure. In particular the VSD formulation try to match a parametric distribution of 3D shapes conditioned on a prompt to the distribution of 2D views generated by a text-to-image prior, the authors propose to integrate in the formulation another prior that relies on existing 3D assets retrieved from Objaverse to push the learned distribution of 3D shapes to be more 3D consistent and have less spurious artifacts (e.g., multiple frontal views). Besides being used explicitly in the optimization, rendering from 3D assets are also used to fine tune using LoRA the text-to-image prior to make it more aware of camera views description in the prompt. The result is a text-to-3D model that can generate more faithful 3D models that don\u2019t show multiple repetitions of frontal views. Evaluation is mostly limited to qualitative examples and user studies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The integration of the 3D assets prior into the VSD formulation is quite elegant and clearly helps to generate 3D assets with a more plausible 3D structure and artifacts free. The author provides a detailed mathematical justification for their method, explaining assumptions they consider while developing the final proposed method.\n\n+ The fine tuning of the 2D diffusion prior using 3D assets is a sound idea that clearly seems to help to build 3D awareness into the model. Many previous work did not consider improving this specific aspect of the pipeline and I found interesting the focus of the authors on this aspect."
            },
            "weaknesses": {
                "value": "## Major\n\na.**Experimental evaluation**: In my opinion the experimental evaluation of the proposed method is very weak for two main aspects:\n1. The method is compared only against the open source reimplementation of competitors, which clearly underperform compared to the results reported in the respective papers. The authors clearly report this in the paper, however I think it would be more fair to also report some qualitative comparison to other methods using the same prompts and results from the respective papers. This is what most published method have done to present their results (e.g., Dreamfusion, Magic3D, Fantasia3D). Moreover works like TextMesh and Magic3D on top of the qualitative results report user studies against the publicly available models from the Dreamfusion online gallery. The authors report a user preference study only against the (weaker) open source baseline used for their work. This is striking comparing the results for ProlifcDreamer reported by this work and the one reported in the original paper that seems to be significantly better,\n2. The paper does not incorporate any quantitative evaluation of the quality of the generated 3D assets. Unfortunately this is a trend that many recent works in the field are following but earlier published works like DreamFusion do incorporate some quantitative measurement that might help to quantify advancement on less cherry picked samples.\n\nb.**Generation might be too constrained by the retrieved assets**: The main contribution of the method is the incorporation of retrieved assets from Objaverse into the optimization objective to help create more plausible 3D models. While this clearly helps in the generation of object centric 3D models it\u2019s unclear how the proposed models would generalize to scenes generation, which the baseline ProlificDreamer seems to be able to handle up to some extent (see Fig.1 (b) from the [ProlificDreamer paper](https://arxiv.org/pdf/2305.16213.pdf)). Moreover from the current examples in the work it\u2019s unclear how much the proposed model is \u201cgenerating\u201d novel objects and how much it\u2019s just \u201ccopying and retexturing\u201d assets in Objaverse. It would have been great if the authors had included for each result also the closest Objaverse objects used during the optimization process similar to what was done in Fig. 11. Examples like \u201cA fierce tiger\u201d or \u201cA fighter aircraft\u201d in Fig. 7 might be already faithfully covered by objects in Objaverse.\n\nc. **Relying on CLIP for aligning 3D assets**: In Sec. 4.4 the authors mention that they rely on CLIP to identify which one is the \u201cfront view\u201d of the 3D assets by rendering several views and computing the cosine distance against prompts like \u201cfront/side/back view\u201d. In my opinion this has two important limitations, first of all assumes that objects have a front/back/side view while whole categories of objects don\u2019t have these properties (e.g., any symmetric object like a flower pot or a mug). Second it assumes that CLIP can be used successfully for this task, while to the best of my knowledge this has not been clearly shown before (and per my experience does not really work reliably). I would be curious to know what the experience of the authors is in using CLIP to identify camera poses. Also the fact that the models were aligned using CLIP helps to maximize the performance according to the evaluation protocol used in Fig. 9, since basically the seed shapes from objaverse were aligned to maximize the CLIP alignment computed there. If the model would just copy the objaverse example aligned with the proposed method the metric reported in Fig. 9 will be perfect but will not really measure anything useful related to the generative capabilities of the model. \n\n\n## Minor\n\nd. **Clarity**: In general I think the paper could be rewritten and made more clear. For example the English throughout the document can be improved, Fig. 9 does not have any kind of scale on the y axis, Fig. 14 caption is wrong.\n\ne. **No discussion of limitations**: the paper does not discuss limitations and failure mode of the proposed method."
            },
            "questions": {
                "value": "Can you comment on the point I raised in weakness (a)-(b)-(c) and correct any misunderstanding I might have on the contributions of the paper?\n\nAlso see weakness (d) for small suggestions on how to improve the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper contains a user study to evaluate the perceived quality of the generated assets but doesn't share many details on how the study was set up, how participants to the user study were selected and if they were compensated for the study or not."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758866498,
        "cdate": 1698758866498,
        "tmdate": 1699636175387,
        "mdate": 1699636175387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9tHPhW4NX0",
        "forum": "9n9q0R9Gyw",
        "replyto": "9n9q0R9Gyw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_3ZvE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2402/Reviewer_3ZvE"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors tackle the challenge of inconsistencies in the generated 3D scenes caused by Score Distillation Sampling (SDS) based on 2D diffusion models. To address this, the authors propose a retrieval-augmented text-to-3D generation: They utilize a particle-based variational inference framework and enhance the conventional target distribution in SDS-based techniques with an empirical distribution of retrieved 3D assets. Additionally, they introduce an adapted 2D prior model to reduce bias towards certain camera viewpoints and delta distillation to regularize artifacts in generated 3D content. Experimental results demonstrate that their method improves geometry compared to the baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is clearly written and well-motivated. Given the large-scale 3d datasets nowadays, they show how to effectively use them and overcome the inconsistency problems caused by the SDS loss.\n\n2. The qualitative results show improvements regarding the geometry compared to the baselines, while some more cases should be included."
            },
            "weaknesses": {
                "value": "I have a significant concern regarding the comprehensiveness of the experimental results, as they seem to lack crucial details:\n\na) The paper doesn't address scenarios where the text prompt is highly innovative, leading to a nearest neighbor that doesn't align well with the text prompt. How does an imperfect or conflicting retrieved object impact the final 3D generation?\n\nb) The quantitative evaluation needs strengthening. Details such as the number of scenes used in the user study and the number of questions each participant answered are essential. What's more, there is no official implementation of ProlificDreamer currently, and the threestudio version is very unstable. The user study should cover more methods, especially those with official implementations. \n\nc) The paper should also provide more qualitative results, particularly when the quantitative data might not be as reliable. Displaying more uncurated cases and separately showcasing the geometry (normal map) would offer a more comprehensive understanding of the model\u2019s performance."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2402/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698847765726,
        "cdate": 1698847765726,
        "tmdate": 1699636175316,
        "mdate": 1699636175316,
        "license": "CC BY 4.0",
        "version": 2
    }
]