[
    {
        "id": "vTYkbs0CHm",
        "forum": "9TJDsOEaBC",
        "replyto": "9TJDsOEaBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_bF5L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_bF5L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to use Gaussian processes (GPs) to identify the optimal set under a given preference cone, reducing the sample complexity of such approaches. It relies on the uncertainty quantification features of GPs to filter points that are likely to be dominated as well as optimal ones. The query is at the least certain design, so a fully exploratory scheme. A theoretical analysis is provided as well as some empirical results on data sets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The use of preference cones is less common in the multi-objective Bayesian optimization community.\n- Existing theoretical results are extended to ordering cones."
            },
            "weaknesses": {
                "value": "- There are a lot of existing works on preference learning with BO.\n- It is not clear how to use cones for a practitioner.\n- Only discrete input spaces are considered.\n- The links to a similar method, PAL, are not detailed enough. An empirical comparison with this method is needed.\n- The empirical results are not reproducible."
            },
            "questions": {
                "value": "Related references:\n- Picheny, V. (2015). Multiobjective optimization using Gaussian process emulators via stepwise uncertainty reduction. Statistics and Computing, 25(6), 1265-1280.\n- Emmerich, M. T., Deutz, A. H., & Klinkenberg, J. W. (2011, June). Hypervolume-based expected improvement: Monotonicity properties and exact computation. In 2011 IEEE Congress of Evolutionary Computation (CEC) (pp. 2147-2154). IEEE.\n- Yang, K., Li, L., Deutz, A., Back, T., & Emmerich, M. (2016, August). Preference-based multiobjective optimization using truncated expected hypervolume improvement. In 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD) (pp. 276-281). IEEE.\n- Lepird, J. R., Owen, M. P., & Kochenderfer, M. J. (2015). Bayesian preference elicitation for multiobjective engineering design optimization. Journal of Aerospace Information Systems, 12(10), 634-645.\n- Khan, F. A., Dietrich, J. P., & Wirth, C. (2022). Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. arXiv preprint arXiv:2208.10300.\n- Garnett, R. (2023). Bayesian optimization. Cambridge University Press.\n- Svenson, J., & Santner, T. (2016). Multiobjective optimization of expensive-to-evaluate deterministic computer simulator models. Computational Statistics & Data Analysis, 94, 250-264.\n- Ignatenko, T., Kondrashov, K., Cox, M., & de Vries, B. (2021). On Preference Learning Based on Sequential Bayesian Optimization with Pairwise Comparison. arXiv preprint arXiv:2103.13192.\n- Ungredda, J., & Branke, J. (2023, July). When to Elicit Preferences in Multi-Objective Bayesian Optimization. In Proceedings of the Companion Conference on Genetic and Evolutionary Computation (pp. 1997-2003).\n- Taylor, K., Ha, H., Li, M., Chan, J., & Li, X. (2021, June). Bayesian preference learning for interactive multi-objective optimisation. In Proceedings of the Genetic and Evolutionary Computation Conference (pp. 466-475).\n- Jussi Hakanen and Joshua D Knowles. On using decision maker preferences with ParEGO.\nIn International Conference on Evolutionary Multi-Criterion Optimization, pages 282\u2013297.\nSpringer, 2017.\n- Barracosa, B., Bect, J., Baraffe, H. D., Morin, J., Fournel, J., & Vazquez, E. (2022). Bayesian multi-objective optimization for stochastic simulators: an extension of the Pareto Active Learning method. arXiv preprint arXiv:2207.03842.\n\nPage 1: It is unclear what the inclusion relation is between Pareto optimal solution and cone order optimal solution (\u201cHowever, this approach can be restrictive, as it only permits a certain set of trade-offs between objectives.\u201d and then \u201cpreference cones provide a way to bias the search toward certain regions of the Pareto front.\u201d\n\nIntroductive agricultural example: perhaps you could complement Figure 1 with an actual Pareto front to better illustrate the interest. You could add the pessimistic Pareto front defined later. Also in Figure 1, cones are parameterized with angles but later on with a matrix. \n\nCan you describe the convex optimization problem used in the pessimistic Pareto front construction?\n\nDiscuss the relation with epsilon-PAL. It should be added to the empirical comparison.\n\nIt is unclear where the cone properties appear in the theoretical results.\n\nCould you provide timings? Progress curves rather than just fixed snapshots? Random search should be added as a baseline.\n\nAs I understand, the noise hyperparameter is not learned by the GP? What is multi-output covariance kernel used? Is it the same across all compared methods? Is the learning strategy shared among all methods (e.g., fixed hyperparameters)?\nState of the art EHVI is proposed by Daulton, S., Balandat, M., & Bakshy, E. (2020). Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization. Advances in Neural Information Processing Systems, 33, 9851-9864.\nCould you add an example with more than 2 objectives? Too many details are missing to reproduce the experiments: code used, number of samples, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698314508764,
        "cdate": 1698314508764,
        "tmdate": 1699637154105,
        "mdate": 1699637154105,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wpvt7eW8ZZ",
        "forum": "9TJDsOEaBC",
        "replyto": "9TJDsOEaBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_ahkf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_ahkf"
        ],
        "content": {
            "summary": {
                "value": "Vector optimization formulates a multi-objective optimization problem in a way that expresses a user's preference to trade off the different objectives.\nThe study leverages the machinery of Bayesian optimization (BayesOpt) for the task of vector optimization, where the objective functions are assumed to be black boxes.\nThe authors propose a sample-efficient policy that explores the solution space using as few queries as possible.\nUsing the smoothness assumption made by the Gaussian process (GP), the paper also proves a PAC-learning guarantee for the proposed algorithm.\nThe experiments show that this algorithm is more effective at maximizing the success rate (according to the PAC criterion) than a wide range of baselines, while keeping the number of queries low."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The problem studied in this paper is motivated well.\nVector optimization seems like an elegant way for a user to flexibly express their preference over multiple objectives.\nUsing BayesOpt to tackle this problem when objectives are expensive to query seems like a natural solution.\nThe proposed algorithm is proven to have good theoretical guarantee, where the algorithm will return an approximate Pareto-optimal (in the context of vector optimization) with high probability in at most some specified number of queries.\nThe experiments are convincing in showing that the proposed method is competitive against many baselines in the multi-objective optimization literature."
            },
            "weaknesses": {
                "value": "I find the background and problem definition a bit hard to follow.\nThe authors can consider prioritizing intuitive understanding over exposition of all the math.\nThe same goes for the algorithm itself; perhaps add a diagram on the procedure the policy goes through in Algorithm 1.\n\nThe paper does a good job comparing the proposed algorithm against state-of-the-art multi-objective BayesOpt policies.\nHowever, from what I understand, only one algorithm from the vector optimization literature, Na\u00efve Elimination, is included.\nIt could be worth including other (possibly sample-inefficient) algorithms for a more complete comparison.\n\nThe experiments are set up in a way that the GP always has access to the correct hyperparameters (obtained via training on the entire data set).\nIn many real-life settings, we don't have access to the correct hyperparameters, or even good priors for them.\nThe paper could benefit from studying the effects of the GP having the wrong hyperparameters on the performance of the algorithm."
            },
            "questions": {
                "value": "- In Definition 1, my understanding is that the second condition (ii) specifies that $x \\in P$ is not dominated by another by more than $2 \\epsilon$.\nWhat does the first condition (i) say?\nPerhaps the the background of vector optimization could benefit from more descriptive discussions.\n- The authors noted that once a point is added to the Pareto set, it will not be removed.\nThis doesn't match my intuition well; isn't it possible that as we learn more about the objectives, we realize that some of the points already added aren't non-dominated?\n- Could the authors comment on the possible difficulties of extending the proposed algorithm to continuous setting?\nI imagine the challenge lies in the discarding and Pareto identification phases.\nCan we try to discard and identify dominated and non-dominated regions, respectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Reviewer_ahkf"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698594244543,
        "cdate": 1698594244543,
        "tmdate": 1699637153977,
        "mdate": 1699637153977,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "f1hguyHuOu",
        "forum": "9TJDsOEaBC",
        "replyto": "9TJDsOEaBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_XrYo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_XrYo"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a generalization of multi-objective Bayesian optimization.\nSpecifically, the partial ordering is defined by a convex cone.\nThe partial ordering induced by coordinate-wise comparisons used in the literature of multi-objective Bayesian optimization is a special case where the convex cone is the nonnegative orthant.\nNext, the paper proposes an $(\\epsilon, \\delta)$-probably approximately  correct algorithm, which finds an approximate Pareto set with high probability.\nTheoretically, the authors present an upper bound on the number of iterations finding an $(\\epsilon, \\delta)$-PAC Pareto set.\nEmpirical evaluations on a few low dimensional functions demonstrates its superior sample complexity compared to the naive elimination (Ararat and Tekin, 2023), a recent algorithm proposed in the setting of stochastic bandits."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper introduces to the BO community the concept of partial ordering induced by a convex cone, which I think is a useful generalization and may be beneficial to certain BO applications.\n- A theoretical analysis on the sample complexity is presented, which shows that the algorithm finds an $\\epsilon$-approximate Pareto set with high probability $1 - \\delta$."
            },
            "weaknesses": {
                "value": "- The experiments are done on very small datasets. The largest dimension is $4$ and all tasks have two objectives.\n- At this point, the method in the paper is restricted to discrete domains $\\mathcal{X}$, which limits the application of the method: the constant $\\beta_t$ in the algorithm depends on the cardinality of $\\mathcal{X}$, the theorem statements assume finite domains, and the evaluation metrics needs a finite cardinality $|\\mathcal{X}|$ as well. I would assume this is fixable by extending the results in the paper, but additional empirical evaluations are required.\n- The naive elimination is a theoretical construct in the bandit setting, which does not exploit the correlation in the GP model. A more meaningful baseline for comparison is other multi-objective BO algorithms. For example, as the angle $\\theta$ changes, how do the Pareto precision and Pareto recall change comparing with regular multi-objective BO algorithms (which are designed specifically for a particular definition of Pareto optimality)? Another helpful experiment is to plot metrics w.r.t. the number of queries. This allows us to visually check the convergence. From the current tables, it is hard to tell if they are fully converged or not.\n- The experimental setup is non-standard. For example, \"for each dataset, we learn the kernel hyperparameters by training on the entire dataset\". However, the hyperparameters in BO are typically learned as more queries are added to the training data. Have the author tried the latter more commonly used setting?\n\nMinor comments:\n- The following notations need more explicit definitions in the main text: the hyperrectangles $R_t(x)$, their diameters $\\omega_t(x)$, the information gain $\\gamma_t$ and the constant $\\eta$.\n- In the definition of the polyhedral cone, it should be $C = \\\\{\\mathbf{x} \\in \\mathbb{R}^M: W \\mathbf{x} \\geq 0\\\\}$ and $W$ should be $N \\times M$. The cone is defined in the output space, not in the domain."
            },
            "questions": {
                "value": "- Theorem 1 and Theorem 2 need to add an extra technical assumption on the cone $C$. Otherwise the bounds may be vacuous in certain cases. For example, $C = \\\\{(x_1, x_2) \\in \\mathbb{R}^2: x_1 = x_2\\\\}$ is a well-defined polyhedral ordering cone. However $d(1) = \\infty$ in this case and thus both bounds become vacuous.\n- Can you share more intuition on Definition 4? My intuition is that $\\mathbf{u}^*$ points to the \"center\" of the cone.\n- In line 3 of Algorithm 4, why $+ C$ and $- C$ are on both sides of the intersection? Shouldn't $(\\mathbf{R}_t(\\mathbf{x}) + \\epsilon \\mathbf{u}^* + C) \\cap \\mathbf{R}_t(\\mathbf{x}^\\prime)$ be already sufficient?\n- The evaluation metrics PA, PR and PP need the ground truth Pareto set $P^*$. How are the ground truth Pareto sets computed?\n- Branin and Currin are continuous functions defined on continuous domains. Does the experiment in Table 1 discretize their domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9167/Reviewer_XrYo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698716731199,
        "cdate": 1698716731199,
        "tmdate": 1699637153871,
        "mdate": 1699637153871,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ItX4ayZzsm",
        "forum": "9TJDsOEaBC",
        "replyto": "9TJDsOEaBC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_vz76"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9167/Reviewer_vz76"
        ],
        "content": {
            "summary": {
                "value": "This work proposes vector optimization for multiple-objective optimization using confidence intervals built from Gaussian Processes. The proposed method called VOGP allows users to convey objective preferences through ordering cones while performing efficient sampling by exploiting the smoothness of the objective function, resulting in a more effective optimization process that requires fewer evaluations. Both theoretical guarantee and experimental results are demonstrated to consolidate the claims."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "proposes a vector optimization based on Gaussian processes."
            },
            "weaknesses": {
                "value": "There might be more applications illustrated."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9167/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698783519880,
        "cdate": 1698783519880,
        "tmdate": 1699637153763,
        "mdate": 1699637153763,
        "license": "CC BY 4.0",
        "version": 2
    }
]