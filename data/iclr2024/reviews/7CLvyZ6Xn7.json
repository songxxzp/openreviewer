[
    {
        "id": "p7XBwSjvxX",
        "forum": "7CLvyZ6Xn7",
        "replyto": "7CLvyZ6Xn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_DPcy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_DPcy"
        ],
        "content": {
            "summary": {
                "value": "This work addresses the problem of few shot domain adaptation for 3D shape generation in which a pre-trained source model is adapted to perform 3D shape generation on a target domain given only a few target training samples. A pairwise relative distance regularization is proposed at both the feature and shape level to help prevent the adapted model from overfitting to the few training samples. Additionally, 2D silhouettes are only needed instead of ground truth 3D shapes for learning the geometry distribution of the target domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) Proposes an approach to the unexplored problem of few shot domain adaptation for 3D shape generation. This is an interesting problem as 3D generative models are particularly difficult to learn from limited training data. Since many categories are wildly disparate in the number of training samples, domain adaptation is a sensible approach to tackling this training sample imbalance.\n\n2) The paper is well written and easy to follow.\n\n3) Reasonable baselines are proposed to compare the method against, and are shown to be outperformed by the proposed method qualitatively and quantitatively. In terms of generation quality and diversity, the qualitative samples seem to show greater diversity in terms of geometry and more realistic texture adaptation in most cases."
            },
            "weaknesses": {
                "value": "1) $S_{geo}^{s}$, $S_{geo}^{t}$, $S_{tex}^{s}$, $S_{tex}^{t}$ are never defined in any section of the paper, instead they are only defined in Figure 2. These should also be defined in the paper so it is easier to make sense of Eqs. 1-2 & Eqs. 9-10.\n\n2) In the \u201ccars -> trucks\u201d examples in Fig. 3, it looks like the back window of a car gets painted on to the bed of the truck, suggesting a potential limitation of the approach in which it may not be properly learning to adapt textures for all mappings between source and target domains. This can also be seen in Fig. 1, where the proposed approach has a window as a truck bed while the directly fine tuned models have actual truck beds.\n\n3) While the method does show strong results for adapting source models to a target domain, the only two source domains the method is evaluated on is the Car and Chair categories from ShapeNet. It would be nice to see some results on adapting source models from other categories.\n\n4) It would be good to also include the quantitative ablation (Table 4 in supplemental) in the main paper which better demonstrates how the losses affect generation quality and diversity across an entire dataset rather than just the few samples shown in the qualitative ablation in Fig. 7."
            },
            "questions": {
                "value": "1) Does the batch size of 4 mentioned in the setup correspond to $N$ describing the number of sampled geometry and texture codes? If so, how does the choice of $N$ affect results, as a batch of samples seem to be used in modeling the geometry and texture pairwise similarity distributions of source and target domains at each training iteration. Is 4 samples really enough to model such distributions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1282/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1282/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1282/Reviewer_DPcy"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625167673,
        "cdate": 1698625167673,
        "tmdate": 1699636055152,
        "mdate": 1699636055152,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "R0A23feDOj",
        "forum": "7CLvyZ6Xn7",
        "replyto": "7CLvyZ6Xn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_KzHF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_KzHF"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a few-shot 3D shape generation method that adapts a pre-trained 3D generative model to the domain of the provided few examples (e.g, 10 shapes of trucks). Since the data is limited, directly fine-tuning the model on the few examples can lead to severe overfitting. To address the issue, the paper proposes an additional regularization loss that perserves the pairwise distances between the samples at feature-level and shape-level. This additional regularization loss is the main technical contribution of the paper. The authors evaluate the method on ShapeNet cars and chairs categories and show the improvement over baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The presentation of the paper is quite clear, making it easy to understand.\n- The paper addresses a specific problem and solves it in an simple and effective approach."
            },
            "weaknesses": {
                "value": "- My biggest concern is that the gap between the source and target domains seems small, making the problem less interesting. For examples, trucks seem to be only slightly off the distribution of common cars. Is the source model not able to generate trucks at all? In particularly, the source datasets should be made more clear. In page 7 experiment setup, are the types of cars in the target datasets (e.g., trucks) also included in the source datasets? If not, then could you show some nearest neighbors in the source datasets? This would help understanding the gap between source and target datasets.\n- The paper claims to be the first few-shot 3D shape generation, but there are indeed 3D generation methods pre deep learning era that only need a few input shapes, e.g. [1]. The first \"few-shot 3D adaptation\" could be more accurate.\n- I don't think being able to train only on the silhouettes can be claimed as an advantage (it seems like so in the paper's tone), as it basically ignores the texture and assumes the texture distribution the same as source domain.\n- I'm not super familiar with the few-shot domain adaptation literature so I cannot really judge the technical novelty. It seems to be an adaptation of methods from image domain, but I'll seek to other reviewers examination on this.\n- One particular technical point that I doubted is Eqn. 10, where the masks of 2D rendered shapes are used for texture features. How could the rendered masks be used for the feature maps with the correct alignment? You can render from any angle and the resulting masks would be different. I don't get the rational for masking features this way.\n\n\n\n[1] Fit and diverse: set evolution for inspiring 3D shape galleries, SIGGRAPH 2012."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698212831,
        "cdate": 1698698212831,
        "tmdate": 1699636055008,
        "mdate": 1699636055008,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RqMDvCd88C",
        "forum": "7CLvyZ6Xn7",
        "replyto": "7CLvyZ6Xn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_4te8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_4te8"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a recently develop 3D model generator known as Get3D to synthesize images of 3D objects that will be helpful in domain adaptation. The synthesized images are evaluated using standard metrics. The problem is also set in a few short setting. Fidelity to shape and texture is emphasized."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Good looking pictures\nFew shot setting\nComparison with two other methods."
            },
            "weaknesses": {
                "value": "Experiments are not complete. Since domain adaptation is the motivation, the effectiveness of synthesized data for domain adaptation using Office-Data (Saenko, et al ECCV 2010), or DomainNet data should be evaluated. The loss functions used for texture should be compared with Style-GAN and diffusion-based approaches. It is not clear how effective this approach will be for synthesizing objects at different poses."
            },
            "questions": {
                "value": "Conduct experiments on Office Data to validate the usefulness of synthetic data for domain adaptation task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791441380,
        "cdate": 1698791441380,
        "tmdate": 1699636054905,
        "mdate": 1699636054905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6PjLaK0zFj",
        "forum": "7CLvyZ6Xn7",
        "replyto": "7CLvyZ6Xn7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_xeiS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1282/Reviewer_xeiS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for few-shot 3D shape generation. In particular, they introduce a domain adaptation approach to transfer a pretrained 3D generative model to a similar target domain. The key idea for domain adaptation part is to maintain the relative distances between generated samples at both feature and shape level as similar to that in the source domain. The proposed method only requires silhouettes during training. The method is implemented based on GET3D and show effectivenss on generating new samples with diverse shapes but similar textures with the source domain."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper investigates an interesting problem of few-shot 3D shape generation and is the earliest work working on this direction. This may shed new light on transferring pretrained 3D generative models to other domains where very limited data is available.\n\n- The generated shapes look promising in term of shape quality and diversity. While the geometry can achieve variations in the target domain, the texture appearance can maintain similar distribution with the source domain.\n\n- The exposition of the paper is clear and informativie."
            },
            "weaknesses": {
                "value": "- Despite the good results in the paper, the domain gap between the source and target domain is relatively small. The current domain transfer is limited to the same category, e.g. within cars or within chairs. This limits the application of the proposed approach.\n\n- Though the proposed method claims that it can be applied to other network architectures, I am not fully convinced. The current network design is highly coupled with that of GET3D. To support the claim, additional experiments of using other generative models are required."
            },
            "questions": {
                "value": "- For Equation (4) and (5), it is very reasonable to use cosine similarity and softmax function to compute the relative distance between the binary masks. Since masks are binary, there may exist simpler but more effective methods to compute the relative distances. For example, have you tried to compute based on the features from the common regions of the two masks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1282/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698794833644,
        "cdate": 1698794833644,
        "tmdate": 1699636054829,
        "mdate": 1699636054829,
        "license": "CC BY 4.0",
        "version": 2
    }
]