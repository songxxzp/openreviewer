[
    {
        "id": "Wu1rcCkTX5",
        "forum": "HC0msxE3sf",
        "replyto": "HC0msxE3sf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_TgUR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_TgUR"
        ],
        "content": {
            "summary": {
                "value": "The authors discuss connections between classic emergent communication in Lewis signalling games and Beta-VAEs.\n\nIn some traditional EC works, a speaker and listener must coordinate such that the listener can reconstruction a speaker's \"target\" observation, given communication. In many ways, this mirrors classic reconstruction training. Prior works have often found that the resulting communication from such training is often \"unhumanlike\" in several ways, including ZLA and HAS metrics. This work argues that such undesireable properties are likely a result of implicit priors that most EC works encode. \n\nBy connecting EC to Beta-VAE methods, the authors uncover theoretical interpretations of different terms in EC and open up the important directions for experiments (such as varying prior distributions or Beta).\n\nIn experiments, the authors show that, by using a learnable prior in training agents, they appear to achieve greater separation of EC into \"word-like\" units."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## Originality\nI'm am somewhat torn about the originality of this work. On the one hand, I think connection EC literature to other theoretically-rich approaches like beta-VAE is a very good idea. On the other hand, the authors note that some prior literature appears to have considered a generalization of this problem (\"Section 6: Tucker et al. defined a communication game... based on VIB, which is known as a generalization of beta-VAE.\"), which makes me think this work is not proposing novel ideas.\n\n## Quality\nOverall, the work seems quite careful and sound in discussing the mathematical underpinnings of many EC methods.\n\n## Clarity\nSome aspects of this paper were quite clear (in particular, the introduction and conclusion are very good), but I found other aspects harder to read. I suspect this is somewhat due to having a fair amount of notation is not immediately interpretable without remembering definitions from earlier pages (e.g., \"monkey typing model\" or n_bou).\n\n## Significance\nI think this work falls within an important (and significant) field of connecting EC to other training methods and objects. I remain somewhat confused about the relationship to prior art, however, so I am unsure of the significance of this individual work."
            },
            "weaknesses": {
                "value": "Overall, I like aspects of this work, but there are a few important unresolved questions or weaknesses that I would want to see addressed before accepting, in particular about relations to prior art.\n\n## Relation to prior art\nThe authors do a good job noting related prior literature, but I remain somewhat confused by the position of this paper relative to such literature. In particular, the authors write that:\n\n> Moreover, Resnick et al. (2020) explicitly formulated the objective as ELBO, though it is not directly applicable to this paper...\n\n> Tucker et al. (2022) defined a communication game called VQ-VIB, based on Variational Information Bottleneck (VIB, Alemi et al., 2017) which is known as a generalization of beta-VAE. Also, Chaabouni et al. (2021) formalized a color naming game with a similar motivation.\n\nIf prior art has used the same formulation and considered a generalization of the problem this paper is considering, what are the contributions of this paper? Honestly, I suspect there are many unique contributions made in this paper, but the contrast relative to prior art should be made much more obvious. Even just adding a sentence at the end of each related works section saying, e.g., \"While Tucker et al., Alemi et al., and Chaabouni et al. consider similar frameworks to us, we introduce novel metrics and results\" or something to that effect would help a lot. Ideally, the authors would run experiments comparing to Resnick's method.\n\n## Presentation of results\nI found the results somewhat difficult to read. Figure 2 contains the main results, and with enough flipping between pages, I could eventually figure out how to interpret them, but generally I encourage authors to make figures more self-contained. For example, listing a baseline as BL1 is not as informative as using a name/label that actually describes characteristics of the baseline (e.g., conventional + entropy).\n\n## Why did segments become more meaningful.\n\nThe analysis in this section, while addressing a very important question, is slightly unsatisfying. First, parts of the writing are very casual (e.g., The receiver must be surprised several times...\"), whereas in reality the receiver just needs to receive, over multiple timesteps, enough bits to reconstruct the input. It is unclear what it means to \"be surprised\" as a binary term.\n\nSecond, I question the fundamental conclusion of this paragraph. The authors appear to suggest that the competing terms for entropy and reconstruction are what give rise to word boundaries. In other words, communication wants to often be predictable (because of the entropy term), which creates word-like clumps. However, as the authors note, the speaker needs to communicate some information in at least some timesteps to convey the meaning to the listener. Is there any mathematical basis, given the training terms used, for why that information should be concentrated in just a few timesteps (which would match word-like clumps) as opposed to evenly distributed across time? For example, in a simple four-timestep case conveying 4 bits, is there any advantage (as measured by decreased loss) to transmitting [2 bits, 0 bits, 2 bits, 0 bits] vs. [1 bit, 1 bit, 1 bit, 1 bit]?\n\n## Minor:\nAppendix D would greatly benefit from a little bit more text explaining what the graphs present. There are also some sentences that need editing (e.g., \"threshold is set to 0.25.\")"
            },
            "questions": {
                "value": "1. In the Weaknesses section, I raised questions about why an entropy term would actually increase word segmentation. To repeat it here: is there any mathematical reason that the losses used during training should concentrate surprisal in just a few timesteps (which would induce word-like clumps) instead of spreading the surprisal loss more evenly across time?\n\n2. I struggled to understand Figure 5. What is it depicting? What do the legend entries/different lines correspond to?\n\n3. Just a clarifying question about the results for Criterion C3: the authors' proposed method is worse than baselines, correct? I recognize that topsim values for the proposed method improved generally, but for the narrow metric of the difference between topsim values, there is a decrease, right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698420061826,
        "cdate": 1698420061826,
        "tmdate": 1699636083659,
        "mdate": 1699636083659,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SwkBBeNfyL",
        "forum": "HC0msxE3sf",
        "replyto": "HC0msxE3sf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_E7HT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_E7HT"
        ],
        "content": {
            "summary": {
                "value": "This paper reanalyzes an emergent communication-signalling game in terms of\na VAE.  Within this analysis, the optimization of a signalling game is using an\n\"implicit prior\" which leads to statistical properties of the emerging language\nwhich do not match human languages.  Introducing linguistically-inspired priors\ninto signalling game by way of the VAE framework improves the resulting\nemergent languages' statistical properties (i.e., adhering Zipf's Law of\nAbbreviation and Harris's Articulation Scheme more closely)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- (major) The paper aims at re-analyzing a common EC setting in a more\n  formalized way, yielding the potential for theoretical insights that would\n  not otherwise be possible.\n- (major) Furthermore, I think this analysis is largely in the correct\n  direction with analyzing the signalling game as a VAE, looking at inductive\n  biases, and tying in linguistic concepts like Zipf's Law of Abbreviation and\n  Harris's Articulation Scheme (although this wide scope is also a bit of\n  concern; cf. \"Weaknesses\").\n- (minor) The experiments partially satisfy HAS which is known to hold for\n  human languages."
            },
            "weaknesses": {
                "value": "- (major) A critical part of the paper is the \"prior\" within a VAE or\n  signalling game, but I did not get a concrete sense of what this prior\n  actually is in the context of a signalling game with neural network-based\n  agents (I expand on this in \"Questions\").  As a result, it makes me unsure\n  how well the theoretical claims actually apply to a real setup.\n- (major) The paper, I think, tried to do too much, and ends up not spending\n  enough time on the core claims, namely, the signalling game can be\n  re-analyzed as a VAE.  I think the paper would benefit greatly from cutting\n  away all but the essential claims and going through those more slowly and\n  thoroughly.\n    - For example, this shows up in the experiments which seem more concerned\n      with evaluating the existence of ZLA/HAS in the newly proposed setting\n      rather than establishing empirically that the signalling game behaves\n      like a VAE.\n- (minor) The notation and the proofs are not very clear, and it made it\n  slow/difficult to work through the equations."
            },
            "questions": {
                "value": "- What exactly is the \"prior\" in the emergent language game?  I understand that\n  it is implicit, but does that mean that is embedded in the objective function\n  (i.e., the $D_\\text{KL}$ term is constant)?  Or is it instead the case that\n  the sender's architectural biases represent the prior?\n- In addition to the theoretical analysis, what else can the authors point to\n  to support the claim that an EC signalling game is analogous to a VAE?\n\n\n### Other comments\n\n- If the authors are assuming a REINFORCE objective for the signalling game,\n  that should be mentioned earlier than Sec 4.1.\n- What is $A_t$ in Eq 3?\n- In Sec 2.2, I do not think the section compositionality is relevant or\n  important; it should removed, in that case.\n- Before Eq 5, what is $\\mathcal A^*$?  Is it supposed to be a Kleene star?\n- What exactly is the uniform prior?  Is it just a constant probability mass\n  over every possible sequence?  If so, how do we know that is the \"implicit\n  prior\" and not something like a uniform _unigram_ prior instead, for example.\n\n\n- Sec 3.1: It is not clear to me how (9) is derived from (2).  I looked at Section\n  B.1, but it was very unclear what was happening because rather than starting\n  with (2) and going to (9), it talks about \"transforming\" different sides of\n  the equation.\n  - It would also be helpful to give an indication of what from Schulman et al.\n    (2015) is being applied (i.e., the what the \"stochastic computation\n    approach\" is).\n  - As a result, I'm not convinced that the reconstruction game, absent\n    modifications to the traditional object (e.g., length penalty), assumes\n    a uniform prior of messages.\n  - It seems like the $P(m) = \\mathbb E_{x\\sim{}P_\\text{obj}}[S(m|x)]$ should be the\n    prior over messages.  I very well might be misunderstanding something here\n    due to terminology.  Am I conflating here that \"prior\" as the distribution\n    of messages the receiver produces given the distribution over inputs with\n    \"prior\" in the sense of our objective function which we are optimizing\n    against (in which case \"prior\" does not refer to anything concrete in the\n    EC environment but rather only to the optimization process by analogy to\n    a VAE's optimization)?\n- Sec 3.2: what is a \"heuristic variant of [a] VAE\"?\n- Sec 3.4:\n  - The very first paragraph of this section, I think, is glossing over\n    critical question in the paper: what is the connection between the \"prior\"\n    and the actual EC setup.  I understand that the EC setup is analogous to\n    a VAE, but what exactly is the analog of the VAE's prior?\n  - I think the use of \"approximately\" is dangerous when trying to make\n    theoretical claims; I understand that it is unavoidable in something as\n    messy as EC, but it still needs to be accompanied by some justification in\n    order to keep the theoretical claims strong.\n\n\n### Minor notes\n\n- \"they are not\" reproduced emergent lanuaguages   has awkward phrasing\n- Right after Eq. (1), it should be $\\log(|\\mathcal A| - 1)\\ge0$ in the case\n  that $|\\mathcal A| = 2$.\n- Sec 3.3:\n  - what is $\\mathcal M$ -- the set of all messages? \n  - What is $\\mathcal A$, again?\n  - Eq 15 limit notation here would be more appropriate"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Reviewer_E7HT"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698505789101,
        "cdate": 1698505789101,
        "tmdate": 1700604944014,
        "mdate": 1700604944014,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iqUWRUGLzG",
        "forum": "HC0msxE3sf",
        "replyto": "HC0msxE3sf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_knBD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_knBD"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new perspective on Lewis\u2019s signaling game as beta-VAE and reformulates the game\u2019s objective as ELBO. Based on this modification, it analyzes the influence of the implicit prior function on the properties of word lengths and segmentation of the emergent languages. It also shows that a learned prior distribution of the emergent languages can help evolve a language following Zipf\u2019s law and Harris\u2019s articulation scheme while the previous conventional objectives do not encourage meaningful segments."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The originality of this paper is good. The authors propose a generative point of view of the signaling game and analyze the possible causing factors of the current problems of the emerging less meaningful linguistic properties using the conventional objectives. This can provide a fresh study framework for emergent communication. The rigorous formalization and mathematical equations can integrate previous designs of regularizers and help with future objective design, offering a valuable contribution to the field.\n\n2. The quality of the experiments and analysis is good. They compare different baselines controlling different priors of the objectives. The properties of word lengths, segments, and compositionality are carefully checked. \n\n3. This paper is of good clarity. It is easy to follow the argument of this paper."
            },
            "weaknesses": {
                "value": "No obvious weaknesses."
            },
            "questions": {
                "value": "Based on the current formulation, it seems that the distractors on the receiver\u2019s side are not considered. How would you incorporate the context of the distractors and their corresponding influences [1,2] into the prior design? \n\n[1] Lazaridou, Angeliki, Alexander Peysakhovich, and Marco Baroni. \"Multi-agent cooperation and the emergence of (natural) language.\" arXiv preprint arXiv:1612.07182 (2016).\n\n[2] Evtimova, Katrina, et al. \"Emergent communication in a multi-modal, multi-step referential game.\" arXiv preprint arXiv:1705.10369 (2017)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1551/Reviewer_knBD"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698624222969,
        "cdate": 1698624222969,
        "tmdate": 1699636083495,
        "mdate": 1699636083495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p0T6HjGlvr",
        "forum": "HC0msxE3sf",
        "replyto": "HC0msxE3sf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_unmd"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1551/Reviewer_unmd"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to reframe the conventional Lewis's signaling game within the context of beta-VAE and ELBO, with a focus on the impact of prior distributions on emergent languages. The authors argue that selecting appropriate prior distributions can lead to the emergence of more natural language segments, while the conventional prior may hinder adherence to linguistic properties like Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS).\n\nThe weak points of this paper include:\n(1) The paper is hard to read. The theoretical section includes symbols and equations without full explanation.\n(2) The experiments are weak. The compared methods lack descriptions, and the performance improvement is not well explained.\n(3) The studied problem lacks of enough audience."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The author well introduces the problem, which is well motivated.\n2. The authors provide a deteailed proof in supplementary material."
            },
            "weaknesses": {
                "value": "(1) The paper is hard to read. The theoretical section includes symbols and equations without full explanation.\n(2) The experiments are weak. The compared methods lack descriptions, and the performance improvement is not well explained.\n(3) The studied problem lacks audience in the community."
            },
            "questions": {
                "value": "I suggest the authors address my concerns mentioned in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1551/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699122831107,
        "cdate": 1699122831107,
        "tmdate": 1699636083372,
        "mdate": 1699636083372,
        "license": "CC BY 4.0",
        "version": 2
    }
]