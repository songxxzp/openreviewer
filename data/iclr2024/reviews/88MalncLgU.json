[
    {
        "id": "6Y5IPVRkeY",
        "forum": "88MalncLgU",
        "replyto": "88MalncLgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_DCst"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_DCst"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on the problem of out-of-distribution explanations, which means in the explainability tasks of graph neural networks, the highlighted explanation subgraph\u2019s distribution differs from the training data. The existing evaluation metrics such as faithfulness or fidelity score couldn\u2019t evaluate the explanation well due to the OOD issue. The author proposed GInX-Eval to better evaluate the explainers by retraining the GNN model and showed its great evaluation performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper has a great impact on the domain of the XAIG. It addresses a common concern about how the OOD problem affect the performance of the commonly used faithfulness metric.\n2. Figure 2 shows the effectiveness of the proposed methods greatly.\n3. The claims in section 4 are easy to follow and good to refer to.\n4. This paper has a good presentation and is easy to follow. \n5. The experiments are solid and sufficient."
            },
            "weaknesses": {
                "value": "1. GInX-Eval has to treat the pre-trained model as a white box because it needs to retrain the model during the whole procedure. However, the pre-trained to-be-explained model is not always a white box, especially in real-life applications. The training dataset may not be able to be accessed, or the training cost is high, even the model itself may be not accessible. So, this approach is not easy to apply.\n2. The methodology itself is not novel enough. Remove and retrain is not new in the machine learning community, eg: \u201cSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim, 2019, A Benchmark for Interpretability Methods in Deep Neural Networks\u201d.\n3. The contributions are over-claimed. Some previous work have also addressed the OOD problem, eg: \n\n[1] \u201cJunfeng Fang, Xiang Wang, An Zhang, Zemin Liu, Xiangnan He, and Tat-Seng Chua. 2023. Cooperative Explanations of Graph Neural Networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM '23). Association for Computing Machinery, New York, NY, USA, 616\u2013624. https://doi.org/10.1145/3539597.3570378\"\n\n[2] \u201cJ Fang, W Liu, A Zhang, X Wang, X He, K Wang, TS Chua. On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective \u201d\n\n[3] \u201cJiaxing Zhang, Dongsheng Luo, Hua Wei. 2023. MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. SIGKDD\u201923\u201d\n\n[4] \"Ying-Xin Wu, Xiang Wang, An Zhang, Xia Hu, Fuli Feng, Xiangnan He and Tat-Seng Chua, 2022, Deconfounding to Explanation Evaluation in Graph Neural Networks.\u201d"
            },
            "questions": {
                "value": "Comments:\n1. \u201cThe highlighted explanation subgraph\u2019s distribution is different from the training data.\u201d Why is different and what\u2019s the nature where this difference comes from?\n2. Why did the explanations\u2019 distribution shift to a better side but not a worse side? For example: the prediction label is 50. A good explanation prediction should be 50. A bad explanation prediction should be 20. However, due to the  OOD, the explanation prediction shifts. Why a bad prediction would shift from 20 to 45 and cause an incorrect high faithfulness score, instead of shifts from 20 to 5? As it\u2019s claimed: \u201cHowever, this edge masking strategy creates Out-Of-Distribution (OOD) graph inputs, so it is unclear if a high faithfulness score comes from the fact that the edge is important or from the distribution shift induced by the edge removal (section 1 paragraph 1)\u201d.\n3. There are two removal strategies: \u201chard\u201d and \u201csoft\u201d removal strategies. I wonder is there any difference between them toward the GNN output? If the outputs f(G_e_hard) and f(G_e_soft) are different, what\u2019s the reason for that? \n4. There should be many hyper-parameters to tune for the evaluated explainer methods, eg: size regularization and temperature in GNNExplainer/PGExplainer. How do you set them and have you tuned them to the best? It would be good to include these details in the main text or supplementary and motion them in the main text since this paper emphasizes on the experiments.\n5. In Figure 1, what\u2019s the random seed for the random baseline, and how many times the experiments are repeated? For AUC evaluation, how do you compute the AUC score? Specifically, for other explainers, we could have an edge weight vector as the explanation and compute the AUC with the ground truth. But for a random baseline, how to decide the weight of the edge?\n6. The GInX-Eval is computined via retraining, and finally evaluating the quality of the explanation of the original on the original pretrained GNN model. However, the GNN behavior would change during retraining. For example: GNN model f_a is trained on the complete training dataset, it could predict the classification according to the explanation sub-graph. But GNN model f_a is trained on the training dataset which frop 50% edges in each graph. If the explanation sub-graphs are already dropped, how could f_b predicts the graphs into correct classifications? Would the behavior of the retrained GNN models change and how would it affect the accuracy evaluation? Thus, the experiments are not fully convincing. It would be good to make some clarify.\n\n\nTypos:\n1. In section 2, \u201cSolving the OOD problem\u201d should be \u201cSolving the OOD Problem\u201d.\n2. In section 3, \u201cEdge removal strategies\u201d, \u201cPrior work\u201d should be \u201cEdge Removal Strategies\u201d and \u201cPrior Work\u201d to be consistent with \u201cOut-Of-Distribution Explanations\u201d\n3. In section 4, \u201cExperimental setting\u201d should be \u201cExperimental Setting\u201d.\n4. In the \u201cExperimental setting\u201d section, \u201cWe test two \u2026, because they score high on \u2026\u201d: should it be \u201cbecause their scores are high on\u2026\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698430381560,
        "cdate": 1698430381560,
        "tmdate": 1699636332721,
        "mdate": 1699636332721,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c7JSEzSQja",
        "forum": "88MalncLgU",
        "replyto": "88MalncLgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the evaluation method of explanatory techniques for GNNs. It argues that the faithfulness measure commonly used in the GNN explainability research area suffers from the out-of-distribution (OOD) problem where removing uninformative edges can decrease accuracy because they lead to the OOD. To tackle this problem it proposes GInX-Eval that evaluates explanatory techniques according to the decrease in test accuracy on GNNs retrained by using training data in which the highly-ranked edges are subtracted. It empirically shows that the faithfulness score is inconsistence with accuracy and decreases by removing even the uninformative edges, whereas GInX-Eval does not suffer from removing the uninformative edges. The results based on GInX-Eval indicate that some explanatory techniques like gradient-based methods have not good performance whereas others such as GNNExplainer and D4Explainer can provide good explanations of GNN predictions, which are consistent with the results of previous works."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, this paper is well-organized and clearly written. This paper clearly proves the problem of the faithfulness measure widely used in the GNN explainability research community by using carefully designed experiments. The proposed measure, GInX-Eval, can overcome the OOD problem from which the faithfulness measure suffers, by observing the test accuracy on GNNs retrained by using the training data. The evaluation based on GInX-Eval is consistent with the results of the previous works."
            },
            "weaknesses": {
                "value": "Though GInX-Eval is designed so that it can be applied to graph data and it provides good contributions to the graph learning research area, the idea of evaluating explanatory techniques by retraining the prediction methods has already been proposed in previous works such as Hooker et al (2018).\n\nAdditionally, there are several drawbacks to readability:\n- In 3.3.1 GINX SCORE, the description of \"top-k edges\" is confusing because t is already used as the fraction of the ordered edge set.\n- In equation 3, the superscript for G\\G_e^t is used without explanation despite the superscript is not used in equation 2.\n- It is very hard for readers to distinguish different colors used in Figures. Some efforts are required for readability such as using different marks.\n- Several references such as Faber et al, Hooker et al, Hsieh et al, and Hu et al lack names of conferences or years of publishing."
            },
            "questions": {
                "value": "What is the difficulty of applying the idea of retraining to the evaluation of explanatory techniques for GNNs compared to those for CNNs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698481955698,
        "cdate": 1698481955698,
        "tmdate": 1699636332641,
        "mdate": 1699636332641,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1KTGEriV2g",
        "forum": "88MalncLgU",
        "replyto": "88MalncLgU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new evaluation procedure for graph neural network (GNN) explanations called GInX-Eval. The authors argue that current evaluation metrics have limitations, particularly in evaluating out-of-distribution explanations. GInX-Eval addresses this issue by measuring the informativeness of removed edges and the correctness of explanatory edge ordering. The authors also introduce a new dataset for evaluating GNN explanations and demonstrate the effectiveness of GInX-Eval through experiments on this dataset. Overall, the paper's contributions include a new evaluation metric for GNN explanations, a new dataset for evaluation, and experimental results demonstrating the effectiveness of GInX-Eval."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Proposes a novel evaluation metric, GInX-Eval, that measures the informativeness of removed edges and the correctness of explanatory edge ordering.\n- Addresses an important issue in current evaluation metrics, namely the problem of out-of-distribution explanations.\n- Clear and well-organized writing that makes it easy to follow the authors' arguments and contributions."
            },
            "weaknesses": {
                "value": "1. Certain aspects of the design are not intuitively clear. Specifically, the rationale behind Equation 4 is not well-explained. Elaborating on the underlying intuition would aid in understanding its relevance and function within the model.\n2. The terms \"hard selection\" and \"soft selection\" are used without formal definitions. Providing precise mathematical formulas for these concepts would clarify their meaning and implementation in the context of the proposed method.\n3. A major concern with GINX-EVAL is that it necessitates the re-training of the evaluated model. This process alters the original model, potentially leading to explanations that do not accurately reflect the model's decision-making process in its original state.\n4. The utility of edge ranking as a metric is questionable. It assumes that the importance of individual edges correlates directly with subgraph importance, an assumption that may not hold true in all cases. Further justification or alternative metrics should be considered.\n5. The range of GNN backbones tested is somewhat limited. Incorporating more diverse architectures, such as GCN, would provide a more comprehensive evaluation of the proposed method's effectiveness across different models.\n\nIn summary, while the paper introduces an intriguing approach for GNN evaluation, there are several areas where clarity and methodological rigor could be improved. Addressing these concerns would significantly enhance the paper's contribution and applicability."
            },
            "questions": {
                "value": "In weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3765/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX",
                    "ICLR.cc/2024/Conference/Submission3765/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698846183472,
        "cdate": 1698846183472,
        "tmdate": 1700646949053,
        "mdate": 1700646949053,
        "license": "CC BY 4.0",
        "version": 2
    }
]