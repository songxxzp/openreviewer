[
    {
        "id": "h4wl9MvtZo",
        "forum": "nKYTiJhhAu",
        "replyto": "nKYTiJhhAu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_3z1f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_3z1f"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new method to perform online data selection. The proposed method aims to improve the worst-class performance and maintain its overall performance in the meantime. The method can precisely evaluate the performance of classes and put larger weights on the losses of poor-performance class samples. Experiments show some improvement with respect to the worst-class accuracy compared with several baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper identifies an important and interesting problem in existing methods where the worse-class performance is overlooked.\n2. This paper presents a simple solution to solve the complex max-min optimization in Eq. (3).\n3. This paper conducts extensive experiments and ablation studies to validate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Regarding the class-irreducible loss, it is not well justified that the model $\\phi_c$ can be a good approximation of $\\theta_t^{(c)}$.\n2. Regarding the class-irreducible loss, training a separate model $\\phi_c$ for each class can be computationally prohibited on large datasets. Datasets with larger class space such as CIFAR-100 and ImageNet are missing in the experiments.\n3. In Eq. (6-7), the meaning of $c$ and $y$, and their relation, need further clarification.\n4. Why can the proposed method prevent from selecting datapoints with noisy labels?\n5. What if a clean validation set is not accessible?\n6. Some related works are missing from discussion and comparison, such as [1,2]\n\n[1] Heteroskedastic and imbalanced deep learning with adaptive regularization\n[2] Robust long-tailed learning under label noise"
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698310240477,
        "cdate": 1698310240477,
        "tmdate": 1699636184582,
        "mdate": 1699636184582,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "b2EvH12h6B",
        "forum": "nKYTiJhhAu",
        "replyto": "nKYTiJhhAu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_CKWX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_CKWX"
        ],
        "content": {
            "summary": {
                "value": "This work proposes an online batch selection algorithm called REDUCR to preserve the worst-class generalization performance.\nExtensive experiments on multiple datasets show the superiority of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clear presentation and easy-to-follow writing.\n- Extensive evaluation on multiple datasets with two tasks."
            },
            "weaknesses": {
                "value": "Unclear motivation\n- Clothing1M is not a proper dataset to evaluate the effect of batch selection in worst-case accuracy, since it contains noisy labels as well. The performance drop of other baselines may be due to label noise other than class imbalance.\n- Loss-based batch selection baselines (e.g., Loshchilov et al.(2015)) prefer to select high loss examples. Then, they will automatically select the worst-class example first as it exhibits higher loss (i.e., worse generalized). \n- I think why these baselines fail on Clothing1M is due to the label noise, since noisy examples tend to exhibit higher loss so that easy to be selected [a][b].\n\n[a] Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels. NeurIPS , 2018\n\n[b] Meta-Query-Net: Resolving Purity-Informativeness Dilemma in Open-set Active Learning. NeurIPS, 2022\n\n\nLess practicality\n- Although the author provides an efficiency analysis with respect to training steps (in Fig 3), I think this algorithm might be less practical since it takes time to select batch b_t from B_t by solving the minimax problem at every time step t.\n- The author should provide GPU time analysis compared to random batch selection to convince the practicability of this algorithm."
            },
            "questions": {
                "value": "How to select batch b_t exactly? All the formulations for selection scores are for a single datapoint, as the authors assume the small batch to a single data point in Sec 4.2. Could you elaborate on how the \u201cbatch\u201d selection exactly works (line 6 in Alg 1)? With batch selection, I think the selection should consider the relationship between examples to minimize Eq. (3), so the selection algorithm should be different from the single point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698828799282,
        "cdate": 1698828799282,
        "tmdate": 1699636184484,
        "mdate": 1699636184484,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mB2khkJRWU",
        "forum": "nKYTiJhhAu",
        "replyto": "nKYTiJhhAu",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_sdUG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2482/Reviewer_sdUG"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose an approach called REDUCR for online batch selection problem. REDUCR improves existing online batch selection approach RHO-Loss by directly optimizing the worst-class generalization performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper is written well and easy to follow. All the figures and tables are of high-quality.\n+ A comprehensive discussion with related works has been provided.\n+ Empirical studies show the proposed approach can achieve superior worst-class test accuracy, though this result is not surprising since the proposed approach directly optimizes the worst-class generalization performance."
            },
            "weaknesses": {
                "value": "- The contribution and novelty of this paper are limited. Compared with an existing work RHO-Loss, the only difference is that the proposed approach directly optimizes the worst-class generalization performance, while RHO-Loss optimizes the average generalization performance. Other aspects (e.g. techniques for inducing selection scores and approximating class-irreducible loss model) are the same.\n- It is not clear why the model induced from Eq. (8) can approximate the so-called class-irreducible loss model. They are totally different models from my perspective.\n- The proposed approach improves worst-class test accuracy, but sacrifices the overall average test accuracy."
            },
            "questions": {
                "value": "- The contribution and novelty of this paper are limited. Compared with an existing work RHO-Loss, the only difference is that the proposed approach directly optimizes the worst-class generalization performance, while RHO-Loss optimizes the average generalization performance. Other aspects (e.g. techniques for inducing selection scores and approximating class-irreducible loss model) are the same.\n- It is not clear why the model induced from Eq. (8) can approximate the so-called class-irreducible loss model. They are totally different models from my perspective.\n- The proposed approach improves worst-class test accuracy, but sacrifices the overall average test accuracy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2482/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2482/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2482/Reviewer_sdUG"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2482/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699446300589,
        "cdate": 1699446300589,
        "tmdate": 1699636184398,
        "mdate": 1699636184398,
        "license": "CC BY 4.0",
        "version": 2
    }
]