[
    {
        "id": "2AVmKnM8pm",
        "forum": "LY1eOfqU16",
        "replyto": "LY1eOfqU16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_UWvQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_UWvQ"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a solution to the challenge of algorithmic fairness in AI models when only partially annotated sensitive attributes are available, a common real-world scenario. Particularly, it proposes a method that utilizes soft pseudo-labels derived from attribute classifier output probabilities to mitigate bias. Additionally, the proposed methodology takes into account the uncertainty in attribute classifiers to further enhance the model\u2019s performance in terms of fairness. The introduced approach is evaluated on two datasets, namely Adult (tabular data) and CelebA (visual data)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper addresses a highly important problem in the field of AI fairness, providing a practical solution for mitigating bias with limited access to protected attribute labels, a constraint that often occurs in real-world applications. The proposed approach is simple and effective on multiple data modalities (i.e., tabular and visual data)."
            },
            "weaknesses": {
                "value": "1. The related works section is rather brief and could benefit from a more comprehensive overview of the existing literature within this domain. It also lacks a clear depiction of the limitations of previous approaches, which the proposed method aims to address. In general, this section fails to provide a clear view of the relevant works.\n\n2. Regarding the methodology, limited details are provided for the auxiliary models f_z and f_d) (see Questions/Requests for more details)\n\n3. The evaluation of the proposed method is not sufficient. Only two datasets are employed (Adult and CelebA), the evaluation protocol does not allow for assessing the overall performance of the compared methods (i.e., the trade-off between accuracy and fairness), and several state-of-the-art approaches are not considered in the comparison. (see Questions/Requests for more details).\n\n4. There are several typos in the manuscript that should be fixed."
            },
            "questions": {
                "value": "1. It is not clear how the auxiliary model trained on only 5% of the data can learn the desired information. Does the size of the entire dataset affect the performance of the method? Have the authors investigated the possibility of introducing other types of biases when training the auxiliary models on such limited data? Considering the crucial role of the auxiliary models in the proposed method, the authors should provide an analysis pertaining to models. \n\n2. Figure 1 shows that the proposed approach is fairer but less accurate compared to the baselines. Is the overall performance (i.e., considering both utility and fairness) of the proposed method higher than the performance of the competitive approaches? How does the hyperparameter \\lambda affect the results? \n\n3. Regarding the visual data, there are widely applied benchmarks (see [1]), such as Colored-MNIST, UTKFACE, and CelebA (with isBlonde and HeavyMakeup targets) for assessing fairness-aware approaches. In order to provide a comprehensive evaluation of the proposed method, the authors should follow this evaluation setup and consider the state-of-the-art approaches in the comparison [1,2,3].\n\n4. How does the proposed method perform under different levels of bias in the training data?\n\n5. How does the proposed method perform for multiclass protected attributes (e.g., race)?\n\n[1] Hong, Y., & Yang, E. (2021). Unbiased classification through bias-contrastive and bias-balanced learning. Advances in Neural Information Processing Systems, 34, 26449-26461.\n[2] Barbano, C. A., Dufumier, B., Tartaglione, E., Grangetto, M., & Gori, P. (2022). Unbiased supervised contrastive learning. arXiv preprint arXiv:2211.05568.\n[3] Sarridis, I., Koutlis, C., Papadopoulos, S., & Diou, C. (2023). FLAC: Fairness-Aware Representation Learning by Suppressing Attribute-Class Associations. arXiv preprint arXiv:2304.14252."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7328/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7328/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7328/Reviewer_UWvQ"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698155392239,
        "cdate": 1698155392239,
        "tmdate": 1699636876206,
        "mdate": 1699636876206,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "6dNMx1XOX9",
        "forum": "LY1eOfqU16",
        "replyto": "LY1eOfqU16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_dNWk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_dNWk"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a fairness-aware regularization method that makes use of the soft labels of attribute classifiers to alleviate the information of a group-unlabeled dataset. In addition, the effect of the uncertainty of the attribute classifier is also taken into consideration. This method can be used to learn fairness-aware models when only partial information about sensitive attributes is available."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper studies the problem of fairness-aware learning under partially sensitive attribute information. Different from conventional methods which rely on an attribute classifier as a proxy model to infer \"hard\" pseudo labels, this paper proposes a soft labeling method with a novel perspective. The algorithm part is described in detail and logically clear."
            },
            "weaknesses": {
                "value": "1. The presentation of the paper should be improved. The motivation of the paper in the introduction is not clear. The introduction part is not well organized.\n2. The datasets used in the experiment were too small to be convincing.\n3. The number of comparison algorithms in the experimental part is small.\n4. Some of the references cited in the paper are outdated or not directly relevant to the proposed method."
            },
            "questions": {
                "value": "1. The setup of this method is based on binary classification. Can it be extended to multi-classification?\n2. This paper emphasizes the study of the problem of fairness-aware learning under partially sensitive attribute information. Could you give a specific definition of \u201cpartially\u201d?\n3. For the experimental results, in terms of fairness, the soft regularization method in this paper consistently achieves lower bias across varying proportions of labeled and unlabeled samples. It is beneficial to analyze the reasons for the bias."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698666911279,
        "cdate": 1698666911279,
        "tmdate": 1699636876086,
        "mdate": 1699636876086,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Np6fJtUcIb",
        "forum": "LY1eOfqU16",
        "replyto": "LY1eOfqU16",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_jukW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7328/Reviewer_jukW"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the challenge of algorithmic fairness when sensitive attributes are partially known. Traditional methods use an attribute classifier to generate \"hard\" pseudo labels for fairness-aware model optimization. The authors propose a new regularization method that employs \"soft\" pseudo labels based on the attribute classifier's output probabilities, aligned with fairness criteria definitions. They also explore how uncertainty in attribute classifier parameters\u2014due to limited sensitive attribute data\u2014affects outcomes. Experiments on the Adult and CelebA datasets show that their method outperforms existing baselines in terms of utility and fairness, demonstrating the importance of considering uncertainty."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper proposes to use soft pseudo labels to create a new fair regularization term for fair machine learning. The idea is simple yet efficient. \n2. Experiments are conducted on both tabular and image datasets."
            },
            "weaknesses": {
                "value": "1. Using a proxy model trained on labeled sensitive attributes to predict sensitive attributes is not new [R1]. Using a proxy model to obtain soft sensitive attributes is also not new [R2]. Although these works mainly focus on evaluating fairness, they can be directly extended to learning fair models with standard fair training algorithms. \n2. The sensitive attributes predicted by proxy models are biased [R3], so the soundness of this approach needs more discussion.\n3. Only \"Equalized Odds\" is considered, which is insufficient.\n4. The numbers in Table 1 may not be convincing. Even with 5% labeled data, the task smiling can still achieve an accuracy of 85% with an Equalized Odds of 0.05.\n\n[R1] Evaluating fairness of machine learning models under uncertain and incomplete information. FAccT 2021.\n\n[R2] Fairness under unawareness: Assessing disparity when protected class is unobserved. FAT 2019.\n\n[R3] Weak Proxies are Sufficient and Preferable for Fairness with Missing Sensitive Attributes. ICML 2023."
            },
            "questions": {
                "value": "Please address the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7328/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821477913,
        "cdate": 1698821477913,
        "tmdate": 1699636875961,
        "mdate": 1699636875961,
        "license": "CC BY 4.0",
        "version": 2
    }
]