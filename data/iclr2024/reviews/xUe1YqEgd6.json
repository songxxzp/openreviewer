[
    {
        "id": "ca47NZ4dCi",
        "forum": "xUe1YqEgd6",
        "replyto": "xUe1YqEgd6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_po8i"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_po8i"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a novel method for motion segmentation that, for the loss, combines a 12-parameter quadratic motion model with B-Splines to model long-term temporal consistency and object motion evolution.\nThe model trains an optical flow segmentation network to receive a stack of flow frames up to potentially the whole video. The method is unsupervised and trained on synthetic FlyingThings3D data. The network is then applied to 4 benchmark datasets (DAVIS 2016, FBMS, SegTrackv2 and DAVIS-2017 motion), showing strong results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- (S1) The paper includes qualitative examples of the model's performance in a video format, which strengthens the claims and showcases performance.\n - (S2) The quantitative results are strong, and the model is compared to a number of baselines on several benchmarks, which are well discussed.\n - (S3) The ablations, though not extensive, confirm the inclusion of the main loss components.\n - (S4) The proposed combination of B-splines and a 12-parameter quadratic motion model appears novel and interesting."
            },
            "weaknesses": {
                "value": "The central weakness of the paper is the need for more critical details and more argumentation for the approximations made, particularly in the methods section. The methods section focuses a lot on the ELBO aspect of the argumentation. How B-splines are used to model motion parameters and how they feature in the implementation are only described in passing prose, which is not sufficient given that it is the \"key\" idea of the paper. In detail:\n\n - (W1) The paper needs to clearly describe the procedure for B-Spline estimation that is required to perform the update of the loss. Concretely, the argmin of eq. 9 is not described. Moreover, more details are required to state and explain how B-splines are used to model the 12 parameters of the motion model. This is important to both understanding the paper and being able to reproduce the main proposal in the paper.\n - (W2) More details about equation 5 are required. Why is this a reasonable statement? It seems that optical flow is modelled as a set of independent Laplace variables. This is not discussed and might require some motivation. It is also not clear what the \"assumption above\" refers to. \n - (W3) Similarly, it would also be beneficial to provide more details on why loss in eq. 7 is a reasonable approximation of KL terms of ELBO. This is not discussed. \n - (W4) Equations 6, 8, and 9 might have the wrong sign. The loss is minimized (Eq. 9); however, the loss in eq. 6 is negative because the sum includes terms that are all positive, and the sign is then flipped. This would then increase the l1 error between parametric estimates and input flow, which seems to be the opposite of what is desired.\n - (W5) Given that both terms of ELBO (eq 4.) are approximated, and their signs seem to have flipped (as written in the paper), it is not clear if the resulting loss function of eq. 8 is still a lower bound."
            },
            "questions": {
                "value": "- Ye et al. [A] similarly use B-splines for modelling motion and deformations and similarly evaluate segmentation tasks on, e.g. DAVIS. It would be interesting to discuss more the differences in the approaches and provide some comparisons.\n\n- Ponimatkin et al. [B] present a method that reasons about full video as well as jointly considering the whole sequence. It would be good to discuss the differences in the approaches.\n\n- The paper claims the method is fast at inference time. Does this include the estimation of optical flow?\n\nAs the results are good and the idea _seems_ interesting and novel, I am willing to improve my rating following the rebuttal, with more details about the method (W1-W4) provided.\n\nAdditionally, the reference format does not follow the template/guidelines,  which should be addressed (this was ignored for the purposes of providing the review).\n\n---\n### References\n - [A] \"Deformable Sprites for Unsupervised Video Decomposition\", Ye et al. CVPR 2022\n - [B] \"A Simple and Powerful Global Optimization for Unsupervised Video Object Segmentation\" Ponimatkin et al. WACV 2023"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5245/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5245/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5245/Reviewer_po8i"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698665361217,
        "cdate": 1698665361217,
        "tmdate": 1699636523703,
        "mdate": 1699636523703,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WIRge0kqs1",
        "forum": "xUe1YqEgd6",
        "replyto": "xUe1YqEgd6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_S5Mv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_S5Mv"
        ],
        "content": {
            "summary": {
                "value": "The model tackles the problem of motion segmentation, which performs pixel-level segmentation on video frames by motion. The motion is first estimated using optical flow, then a stack of optical flow frames is used as input for the model, and the model outputs multiple segmentation masks. To achieve this, the proposed method fits optical flow within each segment using a 12-parameter quadratic motion model and utilizes B-spline to handle the motion's temporal changes. A specific loss function is designed for this purpose, taking into account temporal consistency. This method is trained on synthetic data (FlyingThings3D) and doesn't require ground truth annotation. The results of the model's performance are reported on three standard motion segmentation datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The proposed method is fully unsupervised, which is a big advantage in motion segmentation.\n\nThe model is able to output multiple region segmentation, in contrast to the majority of motion segmentation methods that only output the foreground segmentation.\n\nInterpreting the motion segmentation loss as Evidence Lower Bound problem is novel and interesting.\n\nThe proposed method is lightweight and fast, which is an advantage over other temporal processing methods."
            },
            "weaknesses": {
                "value": "1. \u2018Motion segmentation methods often proceed frame by frame\u2019 is not entirely accurate. Earlier methods, especially before the era of deep learning, were designed to process entire videos. This is supported by a rich body of literature. Additionally, recent methods, such as \"Deformable Sprites for Unsupervised Video Decomposition\" (CVPR 2022), also operate on the entire video.\n\n2. The paper mentions that it's the \"first fully unsupervised\" method to incorporate long-term temporal consistency, which is not true. Many non-single image methods, especially those based on layers, e.g. \"Layered segmentation and optical flow estimation over time\" (CVPR 2012), inherently have long-term temporal consistency built in.\n\n3. The paper suggests that relying solely on optical flow is an advantage. However, optical flow is estimated on image pairs, so the whole process always has images as input. Image intensity provides crucial information for object segmentation, so not being able to use images is actually a drawback, not an advantage.\n\n4. The statement claiming, \"We have not only\u2026 but also achieved some kind of tracking,\" is vague. The FBMS-59 dataset is explicitly designed for segmenting and tracking each instance. If the proposed method operates on video sequences and performs temporal association, it would be important to be evaluated under the original settings.\n\n5. The paper predominantly compares with single-frame methods (e.g., EM, MoSeg, DivA, CIS) that do not consider temporal consistency. So the proposed method is expected to yield higher results when using an entire video batch. An exception is the comparison with OCLR. OCLR performs better while the paper argues that OCLR is not fully unsupervised since it trains on synthetic data with labels. Still, the proposed method also trains on synthetic data, where labels are typically sufficient and readily available, and not using the labels is no longer an advantage. The performance of the proposed method is notably worse than OCLR, especially considering that it only compares with the 'flow-only' version of OCLR. If the paper aims to assert the advantage of not using labels, it should directly train on generic real data. See (2.) in Questions.\n\n6. The proposed method is essentially an extension of ST-MS, claiming to introduce temporal consistency over the entire video sequence. However, if I understand it right, the temporal consistency in equation (7) remains constrained to neighboring consecutive frames, similar to ST-MS. The primary distinction lies in using B-spline to model long-term motion changes and a different network architecture. \n\n7. The incorporation of transformers is only valuable if it leads to significant improvements, which do not appear to be impressive (particularly on FBMS-59).\n\n8.  The paper does not discuss any limitations or potential failure modes of the proposed method.\n\n9. Details such as training speed, hardware requirements for training, training batch size, and the size of the training dataset are not provided in the paper. These details are essential information for reproduction."
            },
            "questions": {
                "value": "1. How is K decided? Is it possible to show results with different K's?\n\n2. Why does the method train on flyingthings3D if it is unsupervised? What does the result look like if trained on DAVIS, Segtrack-V2 and FBMS-59, or the union of them?\n\n3. In Table 1, using more frames leads to worse results, which is counterintuitive. \n\n4. In Figure 3, the motorcycle and rider fly together and should share the same motion pattern. Why are they segmented into different regions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698721285538,
        "cdate": 1698721285538,
        "tmdate": 1699636523627,
        "mdate": 1699636523627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Bh6ao3ehtJ",
        "forum": "xUe1YqEgd6",
        "replyto": "xUe1YqEgd6",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_Wuh7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5245/Reviewer_Wuh7"
        ],
        "content": {
            "summary": {
                "value": "The paper presents an approach for a long-term motion segmentation problem in videos. The work assumes a readily availably optical flow as an input data. Subsequently, a parameterized motion model that seems to decompose optical flow as linear combination of several estimates and a temporal consistency term is used for motion segmentation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Although I am not sure I fully understand the motion model, decomposing the optical flow in some rate of change space seems to be a good idea. I suggest the authors to look into the ideas of \"Slow feature Analysis\"\n- Experimental results show improvement due to the approach."
            },
            "weaknesses": {
                "value": "The problem formulation and the suggested solution is a little bit difficult to understand. Hence, I struggled to identify its strength/weakness. Please see questions for more details. I recommend re-writing of the Section 3."
            },
            "questions": {
                "value": "**Motion model**.\n- What exactly is the motion model. It seems to be some kind of linear decomposition of the optical flow. is that correct? If so please clarify and denote all notation in Eq(2), for instance what is $n$? \n- Can you include label for the axis in Figure 2? I assume, the x-axis, represent frame index while y-axis is are the linear coefficients\n\n**The loss function and general model**.\n-  The loss function is a little bit difficult to understand. As far as my understanding goes it is unsupervised model that attempts to estimate the data model $p(x)$ via ELBO. A couple of questions here\n      1. What is the model used to estimate the likelihood $p(x|z)$ in eq(5), it is not clear what kind of parametric model or the kind of estimator used?\n      2. what is the form of the priori $p(z, \\theta)$ ? it is not quite clear. In eq(7), a regularize that penalizes difference in subsequent motion model predictions is added, but is not clear how this relates to $KL$ between posterior and prior. \n\n- The above questions are particularly important, because the framing of the problem as ELBO estimation leads to a generative model with recognition capacity. In such a case, we can discuss further on how capable either the recognition part (posterior estimation) or reconstruction part are depending on the application. I am not sure if the work presented such a model. In my opinion, the work is best described as optical-flow based motion estimation model with regularization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5245/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698918370873,
        "cdate": 1698918370873,
        "tmdate": 1699636523545,
        "mdate": 1699636523545,
        "license": "CC BY 4.0",
        "version": 2
    }
]