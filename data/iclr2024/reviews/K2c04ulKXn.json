[
    {
        "id": "YXzOQQGKW9",
        "forum": "K2c04ulKXn",
        "replyto": "K2c04ulKXn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_DxUx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_DxUx"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates an important but unexplored problem in contrastive learning for time series applications: the challenge of bad positive pairs. The study reveals two distinct types of time series pairs that violate the positive pair assumption, leading to suboptimal representations learned from contrastive learning. To address this special challenge, the paper introduces a simple yet effective algorithm: dynamic bad pair mining (DBPM). The key idea of DBPM is to track the training loss of individual time series pair, and use historical information to identify and suppress bad positive pairs. DBPM has the advantage of being a lightweight plug-in that does not require trainable parameters to enhance existing methods. Extensive evaluations validate DBPM's effectiveness in addressing the challenge, and its integration consistently boosts the performance of SOTA methods."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Significance: As far as I know, this study is the first attempt to examine the bad positive pair problem, which is clearly important yet has not been thoroughly examined. The main findings regarding noisy/faulty alignment will certainly provide valuable insights to future researchers working on contrastive learning for time series applications. The proposed DBPM is a simple plug-in that is easy to implement in existing methods, thus holding potential for broad application impact.\n\nNovelty: The study and proposed methods are novel. First, it's smart to connect the unexplored issue of bad positive pairs in self-supervised learning with the more familiar problem of label errors in supervised learning. Second, the design of DBPM aligns well with empirical observations, and its dynamic identification techniques make the algorithm more reliable. It is also a plus that the method does not have any trainable parameters, and introduces minimal memory overheads to the model.\n\nQuality: The submission is of high quality. The authors present a detailed theoretical analysis of noisy/faulty alignment, supported by empirical evidence using synthetic data. The evaluation is quite comprehensive, encompassing four large time series datasets and various tasks, compared with popular baselines. Experimental results are impressive and validate the proposed method\u2019s effectiveness.  \n\nClarity: The paper is well organized and clearly written. The motivation is clearly illustrated and is convincing. The problem and evaluation are clearly defined. The tables and figures are well made and informative. The appendix is well-structured and contains sufficient information. Overall, the paper is very exhaustive, but is still quite clear and easy to follow.\n\nReproducibility: The submission included an anonymized github link to the source code. Therefore, I believe this submission contains sufficient information that helps in reproducibility."
            },
            "weaknesses": {
                "value": "1. The identification may not be accurate during the early stages of training when the model hasn't been well trained. It is possible that these errors propagate through the training process and accumulate. \n\n2. Since hard samples that are beneficial may also experience large losses, they could potentially be misidentified as faulty pairs. Could the authors explain how to avoid this problem? \n\n3. In Eq3, it is unclear why the mean loss is calculated using e-1 epochs instead of e epochs (the current epoch). Could the authors explain the rationale behind this design?"
            },
            "questions": {
                "value": "Apart from positive pairs, did the authors consider negative pairs? Would it be possible to extend the proposed methods to account for false negatives (e.g. similar time series but treated as negative samples)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697560046781,
        "cdate": 1697560046781,
        "tmdate": 1699636193423,
        "mdate": 1699636193423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "umk3ZBspWP",
        "forum": "K2c04ulKXn",
        "replyto": "K2c04ulKXn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_Cugc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_Cugc"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates issues in time series contrastive learning caused by \"bad\" positive pairs violating the assumption of shared semantic information between views. It identifies two types of bad pairs: noisy positive pairs mainly sharing noise patterns when original signal is noisy, and faulty positive pairs where data augmentation alters important temporal patterns, resulting in views no longer sharing meaning. Through analysis and observing a simulated dataset, the paper shows these bad pairs degrade representations via noisy alignment or faulty alignment during training. To address this, the authors propose a Dynamic Bad Pair Mining (DBPM) algorithm that tracks training loss of each pair over time to identify potential bad pairs based on statistics of historical losses. DBPM estimates weights to suppress identified bad pairs, mitigating their detrimental impacts. Experiments show integrating DBPM into various contrastive learning frameworks consistently improves linear evaluation performance on time series benchmarks. Controlled tests also demonstrate DBPM provides greater robustness against injected bad pairs versus baseline contrastive learning. Overall, this paper provides first investigation of the bad pair problem in time series contrastive learning and proposes the DBPM solution to reliably identify and suppress bad pairs as a simple plug-in enhancing existing methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Identifies and provides the first study of an important issue in time series contrastive learning - the problem of bad positive pairs - that has not been thoroughly explored before. Provides both theoretical analysis and empirical observations on simulated data to demonstrate the detrimental effects of noisy and faulty positive pairs on representation learning.\n\n- Proposes a simple solution (DBPM) that is model-agnostic and can work as a plug-in to boost various existing contrastive learning methods for time series.\n\n- Evaluation on multiple real-world time series benchmarks demonstrates clear and consistent improvements from integrating DBPM across different datasets and models. Additional experiments validate DBPM's ability to confer greater robustness against injected bad pairs compared to baseline contrastive learning.\n\n- Thorough ablation studies analyze the effects of different design choices and hyperparameters\n\n- The code and simulated dataset will be made publicly available to facilitate future research"
            },
            "weaknesses": {
                "value": "1. The identification of bad pairs is based on heuristically set thresholds. Could more principled statistical methods could be explored to automatically determine optimal thresholds?\n2. All evaluations were studied with the InfoNCE loss, does this approach generally work for other losses, for e.g., the supervised contrastive (SupCon) loss Khosla et. al, NeurIPS 2020."
            },
            "questions": {
                "value": "1. Did the authors conduct ablation experiments to distinguish the individual impacts of noisy versus faulty positive pairs on the model's performance? It would be insightful to determine if specific datasets are predominantly influenced by one over the other. Is it possible to isolate certain features and then forecast based on them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2562/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2562/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2562/Reviewer_Cugc"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699046225572,
        "cdate": 1699046225572,
        "tmdate": 1699636193212,
        "mdate": 1699636193212,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "L0Pi34ZZbo",
        "forum": "K2c04ulKXn",
        "replyto": "K2c04ulKXn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_iore"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_iore"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the challenges of time series contrastive learning, specifically focusing on two types of detrimental positive pairs that can degrade the quality of time series representations. Initially, the authors articulate the characteristics of these pairs and elucidate how their presence can adversely impact the contrastive learning process. To address this issue, the paper introduces the DBPM algorithm, designed to detect and mitigate these harmful pairs. DBPM employs a memory module to monitor the training behavior of each pair throughout the learning process, enabling the identification of potential negative pairs. Once identified, these pairs are assigned a lower weight in the loss function to diminish their influence. The efficacy of this approach is substantiated through empirical testing on multiple datasets, demonstrating its effectiveness in enhancing time series contrastive learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper examines a largely underexplored yet practically significant problem with wide-ranging applications. It addresses the issues of noisy and faulty positive pairs, which are common in real-world scenarios and can adversely affect the efficacy of contrastive learning processes.\n\n2. The structure and presentation of this paper are commendably clear and accessible. The method proposed is articulated well, and the empirical evaluation provided is thorough and comprehensive."
            },
            "weaknesses": {
                "value": "1. The paper zeroes in on contrastive learning techniques specific to time series data. A key issue it tackles is the identification of noisy and faulty positive pairs. However, the authors could further clarify why these types of pairs pose a unique challenge within the context of time series, distinguishing them from other data types.\n\n2. There is a potential need for the authors to address the assumptions underlying their model, particularly the noisy alignment assumption that $\\xi_i \\gg z_i$. This assumption might be overly stringent for real-world scenarios where noise typically represents a smaller fraction of the overall signal.\n\n3. The methodology presented, while clear and direct, does not seem to offer substantial technical innovation. It primarily revolves around monitoring historical training statistics and down-weighting samples with elevated contrastive losses. Additionally, the empirical results, though solid, do not demonstrate a marked improvement over existing baselines."
            },
            "questions": {
                "value": "Please see the strengths and weaknesses sections."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699242856131,
        "cdate": 1699242856131,
        "tmdate": 1699636192896,
        "mdate": 1699636192896,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EzCGb7c13o",
        "forum": "K2c04ulKXn",
        "replyto": "K2c04ulKXn",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_gEVT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2562/Reviewer_gEVT"
        ],
        "content": {
            "summary": {
                "value": "This paper explores the challenges faced by contrastive learning in the context of time series data due to the presence of noisy and faulty positive pairs. It introduces a novel Dynamic Bad Pair Mining (DBPM) algorithm that dynamically identifies and mitigates the influence of these bad positive pairs, aiming to enhance the performance of existing contrastive learning methods. The paper demonstrates the effectiveness of DBPM through extensive experiments on a suite of real-world time series datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. **Novel Problem Identification:** The paper introduces a previously unrecognized issue in time series contrastive learning\u2014the challenge of 'bad' positive pairs. The authors convincingly illustrate two variants of such pairs: noisy and faulty, which arise due to augmentation on certain data inputs. The paper provides a persuasive and logical explanation for the emergence of these pairs, marking a significant observation for the time series contrastive learning community.\n\n2. **Empirical Observations:** Through empirical research on a simulated dataset, the authors observed that noisy positive pairs exhibit very low training loss, whereas faulty positive pairs show high training loss. Remarkably, both types display consistently low loss variances. Hence, on a plane plotting mean against variance of training loss, these two classes stand out as outliers from the bulk of good positive pairs. This finding is not only intriguing and insightful but also interpretable and plausible.\n\n3. **Algorithm Development:** Building upon these observations, the authors suggest utilizing the statistics (mean and variance) of training losses of positive pairs to flag the bad ones. This process includes a memory module that records the loss of all positive pairs over several epochs, followed by an algorithm that then downweights the bad pairs identified through this mechanism.\n\n4. **Presentation Clarity:** The paper is well-structured and articulate in detailing the problem, the proposed resolution, and the experimental framework. The inclusion of illustrative figures and tables effectively contributes to the reader's comprehension of the concepts and validates the significance of the proposed approach."
            },
            "weaknesses": {
                "value": "1. **Ablation Study Limitations:** The presented ablation studies are notably limited, which constrains the understanding of each component's unique effects on the overall system performance. Particularly, the roles of the memory module and the transformation module have not been analyzed independently. Distinct investigations into how each module contributes to the suppression of bad pairs, the efficiency of the learning process, and the final model accuracy would greatly enhance the transparency of the results. \n\n2. **Algorithmic Complexity:** The discussion on the algorithm\u2019s additional computational complexity is mentioned but lacks depth. A detailed exploration of the computational costs, including an assessment of trade-offs, would be instructive. Further analysis on the scaling behavior, particularly concerning large datasets and the associated runtime and memory implications in comparison to baseline methods, would greatly benefit the paper.\n\n3. **Parameter Sensitivity Analysis:** The method for selecting the hyperparameters critical for identifying bad positive pairs (specifically, the two $\\beta$ parameters) seems to be heuristic-based. A thorough sensitivity analysis of these hyperparameters, with an emphasis on how they influence model performance, could substantiate the robustness of the paper. Although the preset heuristics for setting thresholds perform adequately, an adaptive strategy for threshold selection could potentially be more effective than manual tuning of the \u03b2 values."
            },
            "questions": {
                "value": "What are other potential applications (e.g. forecasting) where DBPM could be beneficial? Would be good to see evaluation beyond just classification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2562/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699513701170,
        "cdate": 1699513701170,
        "tmdate": 1699636192739,
        "mdate": 1699636192739,
        "license": "CC BY 4.0",
        "version": 2
    }
]