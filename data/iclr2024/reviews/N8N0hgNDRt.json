[
    {
        "id": "GTHpbnPEfO",
        "forum": "N8N0hgNDRt",
        "replyto": "N8N0hgNDRt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
        ],
        "content": {
            "summary": {
                "value": "The authors aim to bridge the noticeable performance gap of open-access LLMs in solving complex mathematical problems. The paper introduces a framework that includes (i) a diverse dataset of math problems generated through transformations such as forward-backward reasoning and self-verification (MetaMathQA) and (ii) open-access LLMs (llama series) fine-tuned on MetaMathQA. Experiments on benchmark datasets demonstrate clear and impressive gains with MetaMath over other open LLMs. Additionally, the authors conduct insightful analyses, highlighting the role of question diversity in enhancing LLM performance."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- **Novel Approach:** The paper introduces a unique data augmentation strategy for mathematical reasoning. The MetaMath framework is generic and can be easily extended to other numerical reasoning datasets.\n\n- **Rich and Comprehensive Analysis:** The analysis is rich and comprehensive, offering numerous insights into data augmentation and the fine-tuning of LLMs for reasoning tasks."
            },
            "weaknesses": {
                "value": "- **Potential for Benchmark Hacking:** Given the experimental setup, there is a slight risk that the proposed approach could lead to benchmark hacking.\n\n- **Dependence on High-Quality Initial Questions:** Given that both datasets used have extensive training data available, the performance of the proposed method in the absence of high-quality initial questions available for mutation remains uncertain.\n\nTo some extent, both the weaknesses can be addressed by doing 0-shot evaluation on some other datasets like DROP (https://allenai.org/data/drop)"
            },
            "questions": {
                "value": "In Table 3, MetaMath finetuning always begins with the AnsAug split, right? Do the authors have any thoughts on what would happen if we start training from (say) SV or FOBAR?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_mqn7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789148173,
        "cdate": 1698789148173,
        "tmdate": 1699636428876,
        "mdate": 1699636428876,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "elmaSv3gZx",
        "forum": "N8N0hgNDRt",
        "replyto": "N8N0hgNDRt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_AmB6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_AmB6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to fine-tune smaller open-source LLMs (LIama) based on data augmentation from large closed-source LLMs (GPT-3.5). A set of data augmentation techniques are employed: answer augmentation, question bootstrapping by rephrasing, and backward reasoning, including self-verification and FOBAR. The data augmentation is applied to the GSM8K and MATH datasets. The augmented MetaMathQA dataset is then used to fine-tune the LIama model series. \n\nExperiments on the fine-tuned 7B, 13B, and 70B LIama models demonstrate significant improvements over various baselines. The authors also made insightful analyses regarding how the perplexity and diversity of the training data affect performance, the reversal mathematical ability, reasoning paths with incorrect answers, as well as data quantity."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed MetaMathQA dataset will be a very valuable contribution to the community.\n2. The proposed data augmentation techniques achieve good performances compared to various baselines.\n3. The authors made insightful analyses regarding different factors affecting the performance of such small LM fine-tuning. This analysis will not only contribute to the specific topic of mathematical reasoning but also will help the general direction of small LM fine-tuning as well."
            },
            "weaknesses": {
                "value": "1. Some baseline approaches to compare are missing, e.g., [1, 2] and code-based LLMs like [3]\n2. The ablation study is not comprehensive enough. Only the 7B model is tested. Table 3 is confusing - should add a line breaker between SFT and MetaMath. \n\n[1] MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning, Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, 2023\n\n[2] Platypus: Quick, Cheap, and Powerful Refinement of LLMs, Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz, 2023\n\n[3] Code Llama: Open Foundation Models for Code, Rozi\u00e8re et al., 2023"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816549753,
        "cdate": 1698816549753,
        "tmdate": 1699636428807,
        "mdate": 1699636428807,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yqffZEG15C",
        "forum": "N8N0hgNDRt",
        "replyto": "N8N0hgNDRt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a method for data augmentation to train LLMs for improving mathematical reasoning.\nThe authors combine several existing techniques such as question re-writing, self-verification, forward-backward reasoning, and answer augmentation to create a larger dataset called MetaMathQA.\nThe paper shows that this dataset can be distilled back into the model resulting in a fine-tuned model that outperforms several baselines on two benchmarks of mathematical reasoning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed approach for bootstrapping seems sound and also results in better mathematical reasoning performance through thorough experimentation\n- The authors also perform ablations that show that all of the bootstrapping techniques help improve performance\n- The paper is well presented and easy to follow"
            },
            "weaknesses": {
                "value": "- The major weakness I see is the lack of novelty. The paper in essence combines existing methods for bootsrapping. \n\nNevertheless, I feel that the empirical findings of the paper would be interesting to the community and therefore vote for acceptance"
            },
            "questions": {
                "value": "- It is interesting that even reasoning paths with incorrect answers can be useful. Do you try to train using both correct and incorrect reasoning paths? Does this perform better than just correct?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4520/Reviewer_pSoC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024698471,
        "cdate": 1699024698471,
        "tmdate": 1700598664203,
        "mdate": 1700598664203,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4n1pZRnYQX",
        "forum": "N8N0hgNDRt",
        "replyto": "N8N0hgNDRt",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_nyhB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4520/Reviewer_nyhB"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes MetaMath, a fine-tuned language model specializing in mathematical reasoning. The proposed method includes bootstrapping mathematical questions by rewriting them from multiple perspectives to create the new dataset MetaMathQA. The LLaMA-2 models are then fine-tuned on the MetaMathQA dataset. Experimental results on two popular benchmarks, GSM8K and MATH, show that MetaMath significantly outperforms other open-source large language models. The authors also introduce the concept of question diversity when creating the MetaMathQA dataset, which is important in reasoning directions, and highlight that backward reasoning questions are very helpful for large language models in understanding mathematical knowledge without memorization."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method of bootstrapping mathematical questions by rewriting them from multiple perspectives is novel.\n2. The authors construct a new dataset, MetaMathQA, by combining forward and backward mathematical questions with augmented answers. This dataset could help the community with advancing progress in mathematical reasoning.\n3. The experiments are pretty extensive in that they have compared to a lot of models/approaches. (Although there are clear weaknesses in the experiments, will discuss in the weaknesses.)\n4. The paper is well-organized and clearly written, making it easy to understand the motivation behind the proposal, the method, the dataset construction, and the experiments conducted."
            },
            "weaknesses": {
                "value": "1. It is unclear how the proposed bootstrapping approach generalizes to other types of multi-hop reasoning problems.\n2. The ablation of the method is not rigorously done.  It is unclear if we keep increasing the number of AnsAug, we can get similar improvement."
            },
            "questions": {
                "value": "I think it is necessary to show that increasing AnsAug to 395K cannot further increase the performance in order to prove the point made in the paper. I understand that this experiment can be costly, so doing this in a small scale to show the trend is good enough. I would love to see a curve on the accuracy vs. # of AnsAug and a curve on the accuracy vs # of a mixed of different augmentations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4520/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699202394932,
        "cdate": 1699202394932,
        "tmdate": 1699636428579,
        "mdate": 1699636428579,
        "license": "CC BY 4.0",
        "version": 2
    }
]