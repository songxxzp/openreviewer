[
    {
        "id": "Sb5jATn86P",
        "forum": "0IaTFNJner",
        "replyto": "0IaTFNJner",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission668/Reviewer_eFdv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission668/Reviewer_eFdv"
        ],
        "content": {
            "summary": {
                "value": "This paper observes scaling the embedding dimensionality does not lead to satisfactory improvement in recommendation models and claims this is due to the dimensional collapse problem where the learned embedding vector spans only on lower dimensional subspace. The paper further claims such problem will propagate by feature interaction module when a feature embedding interacts with collapsed embeddings and regularization could mitigate collapse but harms the performance. Thus this paper proposes an alternative approach to address the embedding collapse issue, namely to replace single embedding in the original model with multiple embeddings. The paper shows the proposed approach lead to higher \"information abundance\" which is ratio between sum of absolute singular values and the largest singular value, indicate the \"spreadness\"/concentration of the singular value distribution. The paper also applied the proposed approach on several recommender models on two datasets, shows a absolute improvement of AUC on 1e-3 ~ 1e-4 level, when scaling the number of multi-embeddings. \n\ninitial recommendation: weak reject, for reasons please see the weak points."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The claim about the dimensional collapsing problem when scaling the dimensionality of feature embeddings in recommender system is reasonably supported in the paper and sound.\n* The analysis about the trade off between feature embedding collapsing and the regularization level of feature interaction is interesting and reasonable. \n* The paper is easy to follow"
            },
            "weaknesses": {
                "value": "* The computation of \"information abundance\" for multi-embedding setting is not clearly defined in the paper. As show in the Figure 1.b for features with low predictive power would have embedding with low \"information abundance\" ratio, and scaling the dimensionality would further lower the ratio. Thus if the ratio for multi embedding is computed by averaging over the small embeddings, the lower ratios for some small embeddings will be compensated. This possibility makes the proposed abundance ratio less reliable. \n* The idea of multi-facet embedding or polysemy embedding has been studied quite extensively in the past. From network embedding (Liu et al. Is a single vector enough? exploring node polysemy for network embedding) to recommender systems (Weston et al. Nonlinear latent factorization by embedding multiple user interests). However, non of the related work on multi-embedding has been discussed in the paper.\n* The finding 1 was claimed to \"applicable to general recommendation models instead of only the models with sub-embeddings\" without being linked to any evidences. Also, I personally find find 1 is much a theory rather than a \"law\"."
            },
            "questions": {
                "value": "* How is the \"information abundance\" for multi-embeddings computed? \n* How would the proposed multi-embedding approach be positioned in the literature?\n* Please point out the evidence for the claim quoted in the third weak point."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698736644154,
        "cdate": 1698736644154,
        "tmdate": 1699635994167,
        "mdate": 1699635994167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "9kPJm1DFwm",
        "forum": "0IaTFNJner",
        "replyto": "0IaTFNJner",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses challenges in scaling up recommendation models, identifying a phenomenon called \"embedding collapse\" while enlarging models. The authors introduce information abundance as a metric to measure and evaluate collapse. They show that embedding matrix reside mostly in a low-dimensional subspace in scaled up models. The study analyzes feature interactions in models, noting they reduce overfitting but can also exacerbate embedding collapse. To tackle this, the authors introduce a multi-embedding design, scaling independent embedding sets and integrating specific interaction modules. This approach claims to improve scalability across various recommendation models. Main contributions:\n- Highlight non-scalability in recommendation models and define the \"embedding collapse\" phenomenon.\n- Empirical and theoretical analysis reveals the dual impact of feature interaction on scalability.\n- Introduction of the multi-embedding design to achieve scalability improvements for recommendation models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Originality:\n- \u2018Information Abundance' as a quantitative novel measure to measure the embedding layer collapse.\n- The 'Interaction-Collapse Law' and the two sided effect of feature interaction process helps improve the understanding of embeddings' behavior in recommendation systems.\n\nQuality:\n- The authors have detailed exploration of embeddings and their behavior, particularly in the context of information collapse with rigorous visualizations. \n\nSignificance:\n- Broad Implications for Recommendation Systems: Given the ubiquitous nature of recommendation systems in today's digital platforms, insights into their workings, particularly regarding embeddings, have widespread implications.\n- Potential for Future Research: Introducing novel concepts and metrics invariably opens the door for future studies, both to validate and to build upon these ideas. The 'Interaction-Collapse Law', for instance, may become a focal point in subsequent research."
            },
            "weaknesses": {
                "value": "1. Insufficient Empirical Validation on Large-Scale Data: The authors have shown with empirical evidences that large scale recommendation models scale poorly. However it is a common knowledge that large scale models are inherently data hungry to achieve better model convergence. This is an important premise that the paper relies on, it would good if authors can follow up to prove/disprove this as additional data points in this paper. The experiments seem to be on same amount of training data on scaled up models which does not unlock the full power of the scaled up model\n\n2. Studies on Collapse and its effect on scalability and overfitting seem to be limited to Single Embedding studies. To make a stronger case about the proposed method, it would be great if authors can provide discussion on the same for proposed Multi Embedding Design."
            },
            "questions": {
                "value": "1. How do you justify the experiment setup when we know that bigger models are inherently data hungry and need more training data to achieve full convergence?\n\n2. From the experiment results shown, the single embedding and multi embedding cases show marginal improvement in AUC values. Can authors provide more context on the significance of the improvements here and if they are well outside the noise region?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Reviewer_DK5H"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801912398,
        "cdate": 1698801912398,
        "tmdate": 1699635994074,
        "mdate": 1699635994074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IlRLbALEdX",
        "forum": "0IaTFNJner",
        "replyto": "0IaTFNJner",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission668/Reviewer_JdNi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission668/Reviewer_JdNi"
        ],
        "content": {
            "summary": {
                "value": "This paper suggests that the embedding collapse phenomenon restricts the scalability of existing recommendation models. Empirical and theoretical analysis show that interaction with collapsed embeddings constrains embedding learning. Also, this paper proposes a multi-embedding design incorporating embedding-set-specific interaction modules to capture diverse patterns and reduce collapse."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "S1. This paper provides empirical and theoretical analysis of the embedding collapse phenomenon.\n\nS2. This paper provides information abundance for quantifying the degree of collapse for such matrices with low-rank tendencies."
            },
            "weaknesses": {
                "value": "W1. The novelty of this paper seems to be limited. The method of dividing the single embedding into multi-embedding sets is similar to DMRL[1] for disentangled representation learning. DMRL divides the feature representation of each modality into k chunks. As a result, the features of different factors are entangled.\n\nW2. The motivation is not completely solid. The reason for increasing the embedding size of the model is inappropriate.\n\nW3. The experimental results of the paper are insufficient. When the embedding size was scaled up through multi-embedding, the experimental results show that performance increases. However, the performance improvement is marginal.\n\n[1] Disentangled Multimodal Representation Learning for Recommendation, IEEE\u201922"
            },
            "questions": {
                "value": "Depending on the model and dataset, the model will have an appropriate embedding size. Therefore, it seems reasonable that the performance will drop if the embedding size deviates from the proper value. Then. do we need to scale up the embedding size for the same dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698822478538,
        "cdate": 1698822478538,
        "tmdate": 1699635993963,
        "mdate": 1699635993963,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "nWhcrWyPwG",
        "forum": "0IaTFNJner",
        "replyto": "0IaTFNJner",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
        ],
        "content": {
            "summary": {
                "value": "This paper studies recommendation model performance when scaling up the embedding layers of the model. The paper identifies a phenomenon of embedding collapse, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical experiments on FFM and DCNv2 and theoretical analysis on FM, the paper shows that the feature interaction process of recommendation models leads to embedding collapse and thus limits the model scalability. The paper also performed empirical experiments on regularized DCNv2 and DNN which led to less collapsed embeddings, but the model performance got worse. The paper proposes multi-embedding, which leads to better performance when scaling up the embedding layers. Experiments demonstrate that this proposed design provides consistent scalability for various recommendation models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality: \n- The paper investigates the enlarged embedding layers of recommendation models and identifies a phenomenon of embedding collapse, wherein the embedding matrix tends to reside in a low-dimensional subspace. The discovery is novel as far as I know.\n- The paper proposed information abundance to measure the degree of collapse for embedding matrices.\n\nQuality:\n- The paper is well-written. It starts with a novel finding of embedding collapse when increasing embedding dimension which might lead to poor scalability, performs empirical experiments and theoretical analysis to show that it is caused by feature interaction, and proposes a solution to increase scalability.\n\nSignificance:\n- The scaling law of recommendation models is an important topic in both academia and industry. This paper investigates the scaling law of embedding layers and shows why naively increasing embedding dim is not sufficient for scalability. The paper proposed multi-embedding which could help alleviate the phenomenon of embedding collapse and improve scalability."
            },
            "weaknesses": {
                "value": "- In section 3, the paper proposes Information Abundance to measure the degree of collapse of embedding matrices. As the paper focuses on the scaling law of embedding layers, the paper should discuss whether Information Abundance is a fair metric when comparing embedding matrices of different dimension sizes.\n- In section 4.2, the paper uses regularized DCNv2 as an example to show that suppressing feature interaction is insufficient for scalability. It is unclear to me why feature interaction in regularized DCNv2 is suppressed.\n- The performance of multi-embedding lacks ablation study. One example is that in Figure 7 (right), we can feed all 4 small embeddings into a single feature interaction layer, and test its performance against the proposed approach. We can also test the Information Abundance of such design versus the proposed multi-embedding design.\n- It would be interesting to test the scaling law of embedding layers with increased feature interaction complexity. This can help us better understand whether the embedding collapse phenomenon is caused by insufficient feature interaction complexity."
            },
            "questions": {
                "value": "My questions are listed in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission668/Reviewer_9BGA"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission668/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698829848014,
        "cdate": 1698829848014,
        "tmdate": 1699708302074,
        "mdate": 1699708302074,
        "license": "CC BY 4.0",
        "version": 2
    }
]