[
    {
        "id": "Kr9ARuLU1A",
        "forum": "XK7kyCVjqr",
        "replyto": "XK7kyCVjqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_okeA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_okeA"
        ],
        "content": {
            "summary": {
                "value": "They propose a semi-supervised code translation method, SPACoder, that leverages snippet training, static analysis, and compilation to generate synthetic parallel code with enhanced alignment in a scalable way, and improves code translation by curriculum learning based on the alignment level of training instances. SPACoder can be generalized to multiple languages and various models with little overhead."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The curriculum learning improves the generation's performance\n- They propose some methods for generating synthetic codes"
            },
            "weaknesses": {
                "value": "- The novelty of this paper is limited. The synthetic generation and alignment-ascending curriculum learning seems simple and straightforward. \n- The discussion of the baseline is not enough. I mean, I cannot get what the contribution this paper achieved.\n- during the generation of synthetic code, can you generate the snippet-level alignment?"
            },
            "questions": {
                "value": "- See above.\n- During selecting the synthetic code, why not run the code and compare the returned results directly\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6285/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6285/Reviewer_okeA",
                    "ICLR.cc/2024/Conference/Submission6285/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697933953961,
        "cdate": 1697933953961,
        "tmdate": 1700557732640,
        "mdate": 1700557732640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0dzULZOUcQ",
        "forum": "XK7kyCVjqr",
        "replyto": "XK7kyCVjqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_jLZb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_jLZb"
        ],
        "content": {
            "summary": {
                "value": "Current neural code translation approaches are of two kinds.\n\n1. Those that rely on unsupervised \u201cback translation\u201d and/or denoising auto-encoding. These methods do not learn alignment between languages in a supervised fashion, and sometimes produce low-quality translations that translate token-by-token with incomplete understanding of target language semantics (the authors of this paper refer to this as \u201cshallow translation\u201d).\n\n2. Those that rely on supervised fine-tuning on parallel aligned data. The problem here is that high quality parallel code data is very hard to come by.\n\nThis paper works with the second family of approaches, and attempts to solve the insufficient data problem by proposing a method to generate high quality parallel aligned data for supervised fine-tuning.\n\nThe method is conceptually simple: take a small quantity of existing parallel data and train a model $f_G$ on it. Then take a large amount of monolingual data and pass each function through the model $f_G$. Filter out incorrect translations by matching function arguments and return types (a static analysis filter), and checking if it compiles (compilation filter).\n\nInstead of applying all the filters together, the authors propose a curriculum learning framework, where they first train the model on the *unfiltered* data, and train the model on progressively more and more filtered data.\n\nThe evaluation is of two kinds - a) they show that a model trained on their parallel data performs better than the **same** model trained on parallel data from other sources, b) they show that a model trained on their parallel data with a curriculum learning approach performs better than **other** existing models."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Conceptually simple approach.\n1. Very relevant problem with good impact. It's true that back-translation suffers from \"shallow translation\", and supervised fine-tuning suffers from data scarcity.\n1. It is clear from the evaluation that the synthetic parallel data is of good quality, and there is a clear benefit to using this data for fine-tuning for the downstream task of translation."
            },
            "weaknesses": {
                "value": "I think overall this paper is a good contribution to the field and should be published. However, I think there are several places where there is a lack of precision and clarity in the writing/terminology. There are also some non-intuitive concepts / details that are skimmed over, and could benefit from some elaboration. The paper would be much easier to read if these were fixed. These are listed in the Questions section."
            },
            "questions": {
                "value": "**Unclear/Imprecise terminology:**\n\n1. Section 2 - \u201cthe paper focuses on function-level code translation\u201d. This confused me because the final evaluation is on computational accuracy, which cannot be evaluated at the function level. I think you mean to say that the parallel aligned data is generated at a function level, but the technique is evaluated on full source files?\n\n1. The word \u201csnippet\u201d is vague and is used to mean different things in different places. For example, in Section 2.1 - \u201c...takes as input a code snippet $x$\u201d - here, snippet means *function*, presumably. But then in Section 2.1.1, there is a contrast between \u201csnippet level\u201d and \u201cfunction level\u201d, suggesting that snippets are smaller than functions. Could you please define what a snippet is, somewhere early on?\n\n1. What is \u201cSPACoder-function\u201d (Table 2 and Section 4.1)? Is it BT, STAT, COMP or AND? Or is it *all* of them, but in a curriculum learning setup?\n\n1. When you use the terminology \u201cSPACoder-PLBART\u201d or \u201cSPACoder-T5\u201d, there are actually two models involved here - the original *generator* model used to produce the synthetic parallel data, and the *base model* that you\u2019re fine-tuning. I assume \u201cSPACoder-PLBart\u201d refers to a model where the generator *as well as* the base model are PLBART? Would be nice if this was clarified.\n\n**Concepts that need more elaboration:**\n\n1. When doing static analysis on function signatures, how do you match types between different languages? Like int[] in Java and vector< int > in C.\n\n1. If you are operating at the function level, how do you apply a compilation filter? In C/C++, you can compile individual functions without linking, but not in Java. And Python code is not compiled, just interpreted. It would be nice if this was clarified.\n\n1.  Let us say that the generator takes code from Language A and converts it to Language B. Then while fine-tuning, do you train on [B, A] samples, or [A, B] samples or both? In other words, while fine-tuning, which is the source language and which is the target language? This is also related to my next comment below.\n\n1. Section 2.2 - \u201cWithout the selector, the generation is reduced to plain back-translation\u201d. I\u2019m having difficulty understanding why this is true. According to me, this would only be true if 1) the generator takes code from Language A and converts it to Language B, but you fine-tune another model on [B, A] samples. 2) both the generator and the fine-tuning model are *trained together* (back translation relies on this kind of joint improvement of the forward and the backward model). Could you please clarify this terminology?\n\n1. Typically, curriculum learning starts with **easy** examples and moves to **difficult** examples. Here, it seems like it starts with **low-quality** examples and moves to **high-quality** examples, which is non-intuitive ([low-quality ~ easy] and [high-quality ~ difficult]?). After a little thought, I think I understand why this is set up like this, but it is an important subtlety and should be clarified.\n\n**Typos / Bad phrasing:**\n\nSection 2.2 - \u201cwe denote the synthetic parallel data from cross-lingual static analysis as STAT and COMP, respectively\u201d - this line needs fixing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698784994329,
        "cdate": 1698784994329,
        "tmdate": 1699636689306,
        "mdate": 1699636689306,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JWVwE5OW29",
        "forum": "XK7kyCVjqr",
        "replyto": "XK7kyCVjqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_9b25"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_9b25"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a method called SPACoder for improving the translation of source code from one programming language to another. The paper argues that one of the main challenges in training neural code translation models is the limited availability of parallel code data in different languages. SPACoder addresses this issue through a semi-supervised approach that first prompts the pre-trained models to translate code from a language to another and then select the better aligned snippets to further train the model. The paper proposed to apply static analysis, and compilation to select the synthetic parallel code examples with better alignment. It also employs curriculum learning based on alignment levels to enhance code translation. SPACoder is versatile, applicable to multiple programming languages and various models with minimal additional overhead. Experimental results demonstrate that SPACoder significantly enhances code translation performance in languages like C++, Java, Python, and C, outperforming state-of-the-art methods, even with a small number of annotated training instances, such as improving C translation by up to 43%."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The overall workflow of SPACoder is intuitive and straightforward to implement yet achieves improvements over existing approaches. It proposes to use static analysis and compilation to estimate the alignment of the parallel data, which alleviates the burden of the selection process that heavily relies on execution correctness.\n\n- The application of curriculum learning in translation is reasonable since the direct learning of the alignment among several different programming languages is difficult for the model to learn and generalize.\n\n- I like the overall presentation. For instance, the examples in Figures 1, 3, and 4 clearly demonstrate the weaknesses of the previous code translation model and the improved performance of SPACoder. The related work section is also very well-written."
            },
            "weaknesses": {
                "value": "- The comparison to TransCoder-ST is not well controlled, and the explanation regarding the comparison results requires further explanation. To me, the most relevant baseline to SPACoder is TransCoder-ST, where both share the high-level idea of firstly generating the translation by the model itself, then selecting better-aligned data with some estimation and finally reinforcing the model\u2019s prediction towards these better-aligned samples while avoiding those misaligned. The main novelty of SPACoder lies in (1) it proposes to use the light-weight static analysis to replace the dynamic correctness as the selection strategy, (2) it proposes to eventually increase the learning difficulty for the model. However, there are the following issues when comparing to Transcoder-ST.\n   1. First, a strictly controlled comparison is missing where SPACoder should be, similar to TransCoder-ST, initialized from the vanilla TransCoder, and such a SPACoder-TransCoder could isolate the comparison between static analysis + curriculum learning vs. dynamic analysis.\n\n  2. It is not clear why SPACoder-PLBART keeps loosing to TransCoder-ST in Computation Accuracy, and it seems the effectiveness of SPACoder largely depends on the quality of the pre-trained checkpoints. However, it is strange that the vanilla PLBART is comparable or better than CodeT5 across Py2Java, CPP2Py, Java2Py, in computation accuracy, and Py2CPP, Py2Java, CPP2Py in CodeBLEU, while such trends are completely reversed when the model is further trained with SPACoder strategy. I would urge the authors to analyze the weaknesses of SPACoder-PLBART and explain the reversed trends in detail.\n\n3. The ablation study doesn\u2019t support the effectiveness of curriculum learning:\n   - The result for BT + STAT + COMP + AND is missing.\n   - To show the effectiveness of curriculum learning, results for rearranging the training stages should be shown.\n   - It\u2019d be even better to show the result of training only on the AND dataset for the same amount of total computation as curriculum learning and compare the two results.\n\n- Given the unstable performance of the SPACoder I mentioned above, I would like to see more results using larger models. As the inconsistent improvement SPACoder brought to PLBART and CodeT5, I would encourage the authors to extend their variant set to more models. Besides the TransCoder version I mentioned above, the codeT5-large, and the codet5+ family of varied decoder sizes might be worth trying to illustrate the generalizability and the effectiveness of SPACoder.\n\n- It is not clear why compilation mostly hurts the performance. In the ablation study of Table-5, it is not explained why compilation mostly hurts the model\u2019s performance during the curriculum learning. This is a bit counterintuitive, since compilation should be able to help filtering out those useless pairs and removing them could make the models focus on predicting at least compiled code. This downgrade of performance requires further analysis.\n\n- It\u2019s not clear how much more total computation that SPAcoder takes when compared to previous models. Considering the curriculum learning during training, SPAcoder might have been trained on the finetuning dataset for more epochs when compared to previous models."
            },
            "questions": {
                "value": "- It\u2019d be great to see a comparison of the total computation in the main results.\n- It\u2019d be great to see a thorough ablation study of the effectiveness of curriculum learning.\n- It\u2019d be great to see experiments demonstrating the efficiency/scalability of Static Analysis filtering compared to Test Case filtering.\n- What is the version of CodeT5? Small? Base? Please specify."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6285/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6285/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6285/Reviewer_9b25"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698801180434,
        "cdate": 1698801180434,
        "tmdate": 1700692777423,
        "mdate": 1700692777423,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8f2bP1Ln0a",
        "forum": "XK7kyCVjqr",
        "replyto": "XK7kyCVjqr",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_zW1g"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6285/Reviewer_zW1g"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a technique for preparing synthetic parallel data to train code translation model. The key idea is to leverage sampling of parallel data from a base model, but then leverages AST analysis and compilation check to filter out low quality data to produce higher quality synthetic data to help with train the model. \n\nThe paper takes advantages of curriculum training, starting from snippet level alignments then to function alignment data. The evaluation shows that the SPACoder improves performance of both PLBART and CodeT5, and curriculum helps improvements of the overall model performance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper's main contribution is the idea of using AST similarity to filter synthetic data to improve its quality for code translation task. \n2. The use of curriculum learning to help bootstrap the training \n3. Comprehensive experiments comparing against both zero-shot and finetuned baselines.\n\nI believe the idea of leveraging AST similarity is quite novel for the given task --- at least for the languages considered, their AST structural similarity is indeed a signal could help with improving dataset quality. While I doubt this technique would be directly available for training true low-resource languages (e.g., translation from Java to DSLs like Halide) given their AST difference, I think this technique could still inspire researchers to consider invariants on AST, or even on control flow graph level that can be used to enhance data similarity. Given that this paper did a good job finding such AST invariants and engineering it to solve code translation task, I think this paper deserves attention from the community."
            },
            "weaknesses": {
                "value": "The paper lacks some comparison with newer public models (StarCoder, CodeLLama etc), or maybe closed source commercial model like GPT-3.5. While such comparisons may seem like \"comparing apple to pear\" due to their differences in model size and corpus, I believe they are necessary if the authors want to show that SPACoder is truly advancing the problem of code translation. For larger langauge models, they often make much less compilation or runtime errors, and many of the problems appear in smaller models like CodeT5 would disappear. If that's the case, the improvement using AST augmentation would be smaller, given that their main goal is to reduce syntax and simple run-time errors.\n\nThe authors argue the effectiveness of the technique on \"low-resource\" language C. While this is true for the given dataset that parallel C data is much smaller, the community won't agree C is a true low-resource language given that C has the largest size in many public pretraining dataset (e.g., the Stack). If the author truly wants to demonstrate the performance of SPACoder on a resource language, some DSL would be a good target.\n\nThe paper also missed an experiment to compare no-curriculum vs curriculum training in terms of BT -> STAT -> COMP -> AND. What would the model performance would be like if you directly finetune PLBart or CodeT5 on AND data without other steps? This would explain whether curriculum  or the dataset matter more."
            },
            "questions": {
                "value": "I would like authors answer questions related to comparison with LLMs with zero-shot or few-shot experiments, and explain how non-curriculum training would affect the result."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6285/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818907952,
        "cdate": 1698818907952,
        "tmdate": 1699636689036,
        "mdate": 1699636689036,
        "license": "CC BY 4.0",
        "version": 2
    }
]