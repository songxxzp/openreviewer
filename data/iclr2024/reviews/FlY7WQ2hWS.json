[
    {
        "id": "K8b2W34Uje",
        "forum": "FlY7WQ2hWS",
        "replyto": "FlY7WQ2hWS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_EksS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_EksS"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Incentive-Aware Federated Learning (IAFL), an FL algorithm which is generally applicable to varying measures of client contribution such as participation rate or local update steps. To incentive client contribution, IAFL takes a personalized FL approach where the server shares higher-quality model updates with clients with higher contribution. Additionally, the paper ensures that all clients, despite limited contribution, are able to reach the optimal model by stochastically synchronizing client models with a common reference model. The paper shows that IAFL outperforms various FL baselines in terms of IPR in several heterogeneous settings."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This work appears novel in the sense that it personalizes the outcome of each round to individual clients, whereas earlier approaches attempt to produce a single global model that is compatible with multiple clients' incentives.\n\nAs mentioned in the paper, it is applicable to settings where earlier incentive-aware FL works are not, such as partial participation and lack of server-side data."
            },
            "weaknesses": {
                "value": "The behavior of IAFL is not clearly explained in the experiments. \n- What is client contribution here? Is it number of local updates? How is this set / varied across clients / time?\n- How is incentivization determined? Do you compare a locally trained model to the (fully trained) server model?\n\n6.1: \" We measure the performance of a model using the test loss and the test accuracy, denoting them as IPR_loss and IPR_accu, respectively.\" This doesn't make sense to me. Doesn't IPR_accu (Table 1) refer to the fraction of clients who are \"incentived\" to participate, and not an accuracy metric?\n\nBased on the results in Table 3 it is surprising to see that IAFL achieves much better accuracy than non-IR methods. However, shouldn't the other methods have an advantage when comparing raw accuracy, as they distribute a high-quality model to all clients without considering incentives? What exactly does this accuracy metric refer to?"
            },
            "questions": {
                "value": "Reading through this paper, I assumed that client contribution is not being adjusted in response to the rewards. Please clarify if this is inaccurate.\n\nAssuming contribution is participation rate, wouldn't there already be a disadvantage to partial participation if the server broadcasts an update (rather than the updated model) to the participating clients, as the local model could become desynchronized? Or does the paper assume the server is sending an updated model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Reviewer_EksS"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698755789401,
        "cdate": 1698755789401,
        "tmdate": 1699636568023,
        "mdate": 1699636568023,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ofVi4ET1qO",
        "forum": "FlY7WQ2hWS",
        "replyto": "FlY7WQ2hWS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_xzxw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_xzxw"
        ],
        "content": {
            "summary": {
                "value": "In federated learning, each client contributes the gradient updates computed with its own local data and then shared with the center. The center aggregates the updates from clients to update the model, and then share the model to the clients to start the next round. In this process, selfish clients may get the most up-to-date model by free-riding, and hence hurt the overall performance of the system. Prior work proposes to incentivize the clients with monetary transfers, while this work focuses on designing an incentive mechanism to share the model in a way such that the more a client contributes, the better model it will receive.\n\nThe proposed mechanism includes two key features to incentive each client to contribute more:\n* Sharing the model updates from a subset of the other clients, the size of the subset is proportional to this client\u2019s contribution;\n* With some probability, give the client the most up-to-date model to prevent the client\u2019s local model being too off.\n\nTheoretical results:\n* All clients have strictly positive incentive to contribute more\n* Each client is better off to participate in the federated learning (individually rational)\n* The bound on the performance loss of the client models (against the optimal benchmark not suffering from any free-ride challenge), which converges to zero with additional assumptions.\n\nExperiments:\n* Partition training data to simulate the distributed data in the federated learning setting\n* Evaluate the percentage of clients where the IR condition is respected\n* Evaluate how the hyperparameters influence the performance of client models at different contribution levels."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Very interesting idea to an important problem\n* Solid results"
            },
            "weaknesses": {
                "value": "* The incentive guarantee is weak in the sense that only a positive incentive is guaranteed, which might not be enough when the clients do suffer certain costs to contribute more to the center. When the cost is higher than the incentive, one may still not contribute 100% effort in the federated learning.\n* The tradeoff between the strength of incentive and the loss of (center) model accuracy is not established, which might be more important in practice. In particular, if I understand correctly, this work assumes all clients contribute 100% of its effort given the constructed incentives. Hence the performance loss of the center model is considered as zero and not measured. However, I can see at least two reasons for the clients to not contribute at its full capacity:\n  * Contribution already exceeds the threshold parameter $p_\\mathsf{ceil}$\n  * When maintaining a certain level of incentive is necessary, one may have to limit the sharing of the gradients, i.e., sufficiently low $\\kappa$ and $q$. In this case, the performance loss of the center model should emerge as the client model might be quite off from the center model and lead to low quality of the gradient updates from local models.\n* I would suggest the authors to at least discuss the above limitations"
            },
            "questions": {
                "value": "* What is the tradeoff between the strength of incentive and the loss of (center) model accuracy?\n* Is there a fairness concern that for small clients, even if they contribute to their best, they still cannot receive a high quality model? Yet the large clients only need to contribute above some threshold to receive the best model?\n* It seems to me that the small clients may have incentive to cooperate to pretend as one big client to receive a better model without a significant cost overhead? Will this lead to exchange platforms where one can first send their gradient updates to the platform, then the platform aggregates the gradient updates from many small clients together, and finally pretend as one big client to cheat in the proposed incentive mechanism? (maybe good to call out the limitation of incentive mechanisms without monetary transfers)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5535/Reviewer_xzxw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814424373,
        "cdate": 1698814424373,
        "tmdate": 1699636567902,
        "mdate": 1699636567902,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8XBnpo1O4s",
        "forum": "FlY7WQ2hWS",
        "replyto": "FlY7WQ2hWS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_dsGp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_dsGp"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes an incentive-aware federated learning algorithm that encourages client contribution by training-time rewards. Concretely, the authors propose a local reward scheme to ensure that a higher-contributing client receives a better final model."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The scope of the experiment is extensive. The authors experiment with different data partition methods, different metrics for measuring incentives, and benchmark against various baselines."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "The problem this paper studies is interesting. However, it could be that I'm missing something, in Theorem 1 and Theorem 2, does convergence speed become faster as the number of agents $N$ grows? It would be helpful to simplify the bound and make the dependence on $N$ explicit. Does adding more clients lead to a faster convergence rate? I would happily increase my score if the question is addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699022610925,
        "cdate": 1699022610925,
        "tmdate": 1699636567803,
        "mdate": 1699636567803,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "wXZcV47Y45",
        "forum": "FlY7WQ2hWS",
        "replyto": "FlY7WQ2hWS",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_LXR6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5535/Reviewer_LXR6"
        ],
        "content": {
            "summary": {
                "value": "This paper studies incentive mechanism for federated learning. Existing works in this direction typically incentive clients via post-training monetary rewards. The authors argue that, clients may anticipate timely rewards during FL process, and may decide to quit when not being properly incentivized. Moreover, monetary rewards may be infeasible in some situations, e.g., when revenue is unclear or budget is limited. Therefore, the authors propose a new formulation, where the clients are reward during the FL process in the form of global model updates of varying quality, depending on the contribution of each client. The authors derive a convergence guarantee of the proposed method, where the convergence rate of each client depends on its reward rate $\\gamma_{i,t}$."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of providing incentives during the FL process instead of postponing to the end of FL is novel and well-motivated."
            },
            "weaknesses": {
                "value": "1. What it means for a client to be incentivized is not well-defined in this paper.\n\nFrom Proposition 1, it seems as long as the gradient of client $i$'s utility w.r.t. its contribution $p_{i}$ is higher than that under standard FL mechanism, we say the client $i$ is incentivized. It is not clear why we, as the designer of the mechanism, cares about whether the gradient of utility for each client is higher than what the client gets under a standard FL mechanism. Instead, a more natural goal is to incentivize the clients to contribute to FL using their full capacity in order to get the best learning outcome. \n\nIn this regard, the intrinsic cost $c_{i}$ of each client also plays an important role, i.e., it is possible that the cost value is high, such that we end up with a negative gradient of the utility (contributing more leads to even lower utility). Therefore, a rational client will decide to contribute $p=0$ in this case, which affects the convergence of the FL process.\n\n2. Current convergence analysis over-simplifies the effect of contribution level on local gradients\n\nDue to the simplification of the \"contribution measurement\" mentioned in Section 4.2, the current convergence result given in Theorem 2 is independent from client's behavior model. Currently, the only place that contribution level of a client plays a role in the convergence result, is the reward rate $\\gamma_{i,t}$ (the quality of the global model that the server decides to give to this client). However, the contribution level should also affect the quality of the local gradient that client provides to the server, e.g., lower contribution means computing the local gradient using smaller portion of its local data (Other than simply saying the client will always faithfully compute the full local gradient w.r.t. the given global model). \n\nIn the extreme case mentioned in my first comment, where the client decides to make zero contribution, then the server will not get the local gradient from this client. However, in the current analysis, the authors assume that the server can always get the local gradients of all the clients no matter what, which does not seem to be reasonable."
            },
            "questions": {
                "value": "Can the authors elaborate on, in Theorem 1, why $o(1/\\gamma_{i,T}^{\\prime})$ suffices to make the convergence hold? Where did the $(T+\\alpha)^{2}$ in the numerator go?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5535/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699150460083,
        "cdate": 1699150460083,
        "tmdate": 1699636567708,
        "mdate": 1699636567708,
        "license": "CC BY 4.0",
        "version": 2
    }
]