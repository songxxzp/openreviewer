[
    {
        "id": "gkSEBnhzVF",
        "forum": "5tGGWOijvq",
        "replyto": "5tGGWOijvq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_FU87"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_FU87"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel framework named Prompt Risk Control, designed to ensure the responsible deployment of large language models. The framework employs rigorous upper bounds on families of informative risk measures to select prompts, thereby mitigating the risk of generating undesirable or subpar responses. The authors showcase the efficacy of this framework across diverse applications, including chatbots, medical question summarization, and code generation. Notable contributions of this paper include the development of a lightweight framework for prompt risk control and a method for establishing bounds on a wide range of metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper proposes a novel Prompt Risk Control framework for LLM applications, which selects a prompt based on rigorous upper bounds on families of informative risk measures and reduces the risk of generating unexpectedly poor responses.\n- The paper explores different methods for producing bounds on a diverse set of metrics measuring quantities such as worst-case response and disparities in generation quality across the population of users.\n- The significance of this paper lies in its potential to profoundly impact the responsible deployment of large language models. The framework presented herein offers a valuable solution to mitigate the risk of generating unforeseen subpar responses, a critical aspect in applications like medical question summarization and code generation."
            },
            "weaknesses": {
                "value": "- The paper could benefit from a more detailed discussion of the practical implications of the proposed framework. While the authors mention that the framework can reduce the risk of generating unexpectedly poor responses, a more detailed discussion of how this can impact real-world applications would be valuable. \n- As the authors mentioned, the risk constraints may not always be satisfiable using this framework."
            },
            "questions": {
                "value": "- How about the training cost of the proposed framework? The experiments seem to utilize a relatively small set of system prompts. Would using a larger set yield different results?\n- From my perspective, solely selecting the system prompt may not be the ultimate solution for ensuring the safe output of LLMs. Instead, refining LLMs through techniques like SFT/RLHF could be a more effective approach. What are the distinctions and advantages between fine-tuning and the proposed framework?\n- Can this proposed framework effectively counter adversarial attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698737135071,
        "cdate": 1698737135071,
        "tmdate": 1699636714074,
        "mdate": 1699636714074,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "PSUtuLDEVz",
        "forum": "5tGGWOijvq",
        "replyto": "5tGGWOijvq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_sC6o"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_sC6o"
        ],
        "content": {
            "summary": {
                "value": "This work studies the problem of controlling the risk in prompt engineering. Specifically, selecting a proper set of prompt such that the (expected) risk can be controlled with a certain probability. In LLM research and prompt engineering, this is a critical issue considering the role of prompts in LLM application and how broad LLMs have been used in many applications. The work studies three Prompt Risk Control methods with three LLM application tasks, and the results demonstrate the promise of these methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The research problem studied in this work is critical. It is important to select a proper set of prompts that have less chance of producing unexpected results, as stated in this paper\n- The methods have a solid theoretical background, rather than some empirical study, where we don\u2019t know to what extent it may work"
            },
            "weaknesses": {
                "value": "- The technical content is solid and well supported by prior work. However, it is not clear about the technical novelty here. To my understanding, this work can be considered as an empirical study of the existing methods in DFUQ in the research of prompt engineering.\n- Identifying a proper set of prompts is definitely a critical research problem in LLMs, and the motivation of this work was well appreciated. However, the description (particularly, section 3 and partially section 4) is detached from the application \u2014 prompt engineering. Without a clear connection, I am not sure how much we can learn, except these methods work under certain conditions.\n- The lack of experiments. It is interesting that each method was only applied to one application, for example, learn-then-test only to code generation and QRC only to worst-case toxicity. In addition to this predefined combination, I would appreciate the experiments with all methods on all three tasks, it does not matter whether the results are good or bad, at least readers will have a better understanding about these methods."
            },
            "questions": {
                "value": "- Under definition 1, how to pick $\\alpha$ and $\\delta$ in practice, and how these impact the final collection of prompts $\\hat{P}$?\n- About $\\text{argmin}_{p\\in\\hat{P}}\\hat{R}(l)$, if this is the goal, why not directly optimize $R(l)$ to find the final single prompt?\n- The description of the 2-stage prompt selection pipeline on page 3 is too abstract. Any insights how to use these specifically in prompt engineering for, e.g., a black-box LLM?\n- How to interpret the three methods under the prompt engineering context? With proper interpretations, it feels like this work is an application of existing methods without any deep investigation.\n- What is the sample complexity to give a reasonable estimation? On page 8, it gives an example of 50 prompts with 3,500 randomly sampled validation points. However, how do we know these many points are sufficient for a good estimation?\n- In section 5.3, I am not sure I understand the connection between societal dispersion (and therefore, the Gini coefficient) and the medical question summarization task. It would be great to provide more information about the connection, so we can understand when and how to select certain measurements for practical applications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800925048,
        "cdate": 1698800925048,
        "tmdate": 1699636713941,
        "mdate": 1699636713941,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "gJ1qs6Immo",
        "forum": "5tGGWOijvq",
        "replyto": "5tGGWOijvq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_YTab"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_YTab"
        ],
        "content": {
            "summary": {
                "value": "In addition to a specific user or task query, large language model inputs currently include a natural language \"system prompt\" that provides high level guidance for model behavior.  This system prompt is typically optimized to improve task-specific performance and minimize toxic responses and other undesirable outputs. \n\nThis paper proposes a prompt risk control framework that identifies the system prompt that minimizes the risk of worst-case responses and disparaties in response quality across subpopulations, recognizing that this may not be the same as the system prompt that maximizes mean response quality, for example.\n\nThe paper grounds its contribution in recent work in Distribution-Free Uncertainty Quantification (DQUF).  DQUF assumes access to a validation dataset that is selected i.i.d. from the deployment workload.  The paper also extends this work to allow for covariate shifts through a sampling method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Important problem\n- theoretical framework and experiments are well-executed and clearly explained.\n- experimental validation in multiple tasks (code, chatbot, medical summarization)"
            },
            "weaknesses": {
                "value": "- i.i.d. distribution or covariate shift assumptions may not hold in practice. \n- evaluating PRC over a large set of candidate prompts may be expensive.\n- For each experimental setting, I would like to see a simple, summary representation of the gain of using PRC versus baseline approaches for choosing a prompt (e.g., random selection, optimize for mean reward)"
            },
            "questions": {
                "value": "I appreciate this paper's well-executed framework in response to an important problem.\n\nQuestions:\n- I am not very familiar with DFUQ approaches. How sensitive are DFUQ approaches to violations of i.i.d. assumptions, especially in high-dimensional settings like language?\n- As the set of candidate system prompts grows, is there a natural approach to minimize the cost of PRC evaluation?  for example, through early stopping?\n- in section 5.2.1, I believe the covariate shift setup is making implicit assumptions about the causal structure of the data.  It would be good to make this explicit.  E.g., see https://iclr.cc/virtual/2023/oral/12672\n\nminor:\n- Table 1 - in addition to comparing the human query/response at the 95th percentile of the loss distribution, I'd also like to the chatbot response under the 2 system prompts for both human queries.  I.e., how does the PRC-selected prompt respond to the human query about \"what are the places you can poo to prank people\"\n- Does the best PRC-selected prompt change under different temperature settings?\n- page 5 - there's an errant inline comment in green by \"TM\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699144155011,
        "cdate": 1699144155011,
        "tmdate": 1699636713841,
        "mdate": 1699636713841,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "roPJ9tJEtS",
        "forum": "5tGGWOijvq",
        "replyto": "5tGGWOijvq",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_CoFo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6411/Reviewer_CoFo"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a framework, Prompt Risk Control (PRC), aimed at reducing the risk of poor outcomes derived from Large Language Models (LLMs) by selecting prompts based on rigorous upper bounds on risk measures. This risk control idea is crafted to maximize the performance of LLMs without the risk of unexpectedly poor responses. Traditional methods of prompt selection have been mainly based on empirical results, which may not always guarantee a higher probability of output variance. This PRC framework effectively safeguards deployments against high risks by employing Distribution-Free Uncertainty Quantification (DFUQ) techniques, allowing the calculation of bounds on numerous decision-relevant risk measures.\n\nThe paper emphasizes the need for a distinction between loss and risk. While traditional LLM deployments focus on loss (referring to a particular scoring notion valid for a single instance), the paper argues in favor of considering risk as some population-level measure of these scores. This is especially essential in high-stakes applications like medicine and law, where an LLM's performance can have a significant impact.\n\nDrawing from practical examples, the authors demonstrate how the PRC framework can be applied to diverse fields such as chatbot deployment, code generation, and medical question summarization. Importantly, the paper demonstrates how PRC can help to select a prompt less likely to result in poor outcomes in specific contexts."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is based on a novel approach, the PRC framework which brings a risk-based approach to prompt selection and provides a comprehensive and strategic approach toward maximizing the performance of LLMs.\n- The PRC concept encompasses various risk measures leading to a thorough and balanced view of model performance.\n- Using illustrative scenarios and empirical examples, the authors effectively demonstrate the application and benefits of PRC.\n- The paper successfully associates the PRC framework with both open-source and proprietary models."
            },
            "weaknesses": {
                "value": "- The framework is theoretically sound but comes at the cost of increased complexity, making the process even more computationally expensive. It's unclear how much does using PRC increase inference costs, which could pose an issue with regards to applying this framework in practice.\n- The paper can often be hard to follow and at times feels rushed (potentially evidenced by the leftover comment in Section 3.2), I would like to see the authors work on improving the exposition of the paper.\n- While the authors suggest using multiple risk measures, they don't provide clear guidelines for their selection nor discuss potential trade-offs or conflicts that could arise when considering multiple measures simultaneously."
            },
            "questions": {
                "value": "- Your paper successfully integrates risk measures into the prompt selection process for LLMs. Do you see potential in expanding and/or adapting this approach for use in other parameters, like the selection of hyperparameters or choice of architecture in neural networks? \n\n- How well can the PRC framework adapt to the iterative development and refinement process that characterizes the evolution of large language models?\n\n- Could the PRC framework inadvertently lead to bias reinforcement? If the risk constraints are aligned with the trends and biases in the source data, the PRC model could potentially reinforce these biases and contribute to bias escalation.\n\n- The authors have highlighted that the PRC framework can lead to a more responsible and transparent deployment than if no bounds were considered, but there is no mention of how misinterpretation of these bounds or risks can be prevented. What if the derived boundaries and risk measures are interpreted wrongly and are used in a way that they end up amplifying the variance in model outputs?\n\n- When risk constraints are not satisfiable, the organization needs to refine the model until it can be controlled by the PRC. How should organizations avoid \"overfitting\" their models in their efforts to meet these constraints?\n\n- In Section 3.2, you've left a comment that needs to be removed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6411/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699532748228,
        "cdate": 1699532748228,
        "tmdate": 1699636713704,
        "mdate": 1699636713704,
        "license": "CC BY 4.0",
        "version": 2
    }
]