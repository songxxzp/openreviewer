[
    {
        "id": "rFgcfYjNlU",
        "forum": "hDzjO41IOO",
        "replyto": "hDzjO41IOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_3wW3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_3wW3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel approach for inverse problem solving via diffusion models via a finer approximation of the posterior density than in prior work. The approximation leverages Tweedie's formula to estimate the posterior mean and covariance and use it find a Gaussian distribution that is close (in the sense of KL-divergence). The resulting algorithm is optimal for Gaussian data distributions, which is further supported by synthetic experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Leveraging higher order information from Tweedie's formula is an original contribution which comes with clear improvements in the accuracy of the posterior approximation. \n- Tuning the weight/step size of the conditional score is a notoriously difficult problem that can easily lead to instability when done wrong. This paper obviates the need for such tuning, which has potential positive impact on designing more reliable solvers with less tuning.\n- Overall, I find the paper well-written and fairly easy to follow.\n- The technique has excellent performance on the syntethic experiments, providing support for the theoretical claims."
            },
            "weaknesses": {
                "value": "- The experiments are insufficient to verify the practicality of the method. Beyond the synthetic examples, no quantitative comparison is provided for actual image reconstruction problems (inpainting, SR). I recommend performing experiments in the same setting as DPS or $\\Pi GDM$ in the original papers and compare the following metrics: PSNR, SSIM, LPIPS, FID. The proposed method has greatly increased compute cost, and therefore it would only make practical sense if sample quality is demonstrated to be significantly better than in competing techniques.\n- Qualitative comparison seem to show that reconstruction quality is poor. There are many inconsistencies and artifacts on generated samples both for inpainting and SR (Figures 3 and 4). State-of-the-art diffusion samplers should be able to provide higher quality reconstructions visually, for instance take a look at samples generated by DPS in the original paper. I believe DPS is not set up properly for the experiments as the reconstructions are clearly not consistent with the observations. I recommend tuning the step size for DPS on a small validation set to optimize LPIPS/PSNR.\n- Claims such as \"having on-par restoration results with $\\Pi GDM$\" and \"we found our approach more robust across various problems\" are not supported by the paper in its current form."
            },
            "questions": {
                "value": "- How does the proposed technique compare quantitatively (both perception and distortion metrics)  to prior work on the image datasets?\n- What has been done to ensure fair comparison with competing techniques in terms of hyperparameter tuning?\n- What exactly justifies the increased compute cost of the proposed sampler beyond the theoretical justification? In other words, in what setting would one choose the proposed sampler over others, such as DPS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Reviewer_3wW3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697793866973,
        "cdate": 1697793866973,
        "tmdate": 1700590418829,
        "mdate": 1700590418829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8o4OkLNpOZ",
        "forum": "hDzjO41IOO",
        "replyto": "hDzjO41IOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_ixot"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_ixot"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed a new method to solve the posterior sampling by the means of diffusion models. Compared to prior works like DPS, the authors also tried to approximate the posterior score function which is theoretically intractable, but they considered a higher order moment and approximated p(x_0 | x_t) as a Gaussian distribution, instead of just the posterior mean, with statistical guarantee. Just like DPS, the technique proposed by the authors can be plugged into several types of diffusion models such as DDPM, DDIM. Finally, the authors did lots of experiments including toy cases and image restoration problems based on pre-trained score estimator, and showed better image quality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The presentation of this paper is very clear and the reference list is also very complete. The writing quality is high. \n2. The experiments in this paper is quite solid. The presentation of the generated images by the proposed model as well as several baseline models is very clear."
            },
            "weaknesses": {
                "value": "1. It would be better to show us some numerical results under some image metrics like PSNR or FID of the generated images, with the results of baseline methods added, so that the audience will know whether the image quality really increases numerically. \n2. In Proposition 3, the authors assumed that the initial distribution p_{data} is Gaussian, which is understandable since p(x_0 | x_t) is approximated by using a posterior Gaussian. However, I would say the assumption is very strong and impractical. It would be better if the authors can figure out a way to propose a better theoretical result with weaker assumptions. \n3. Compared with DPS, the authors added a posterior covariance matrix, which makes the contribution quite incremental. Unless the authors can show that the added covariance can greatly improve the performance compared to DPS, or I afraid the contribution is not enough for an \"accept\" to a more and more competitive ML conference."
            },
            "questions": {
                "value": "1. Can you explain in which aspects, the added posterior covariance matrix is important for the conditional diffusion models?\n2. Can you add some numerical results on the image restoration tasks as I said in the previous section? Thanks very much."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698698235866,
        "cdate": 1698698235866,
        "tmdate": 1699636969975,
        "mdate": 1699636969975,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c4AcziqDcF",
        "forum": "hDzjO41IOO",
        "replyto": "hDzjO41IOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_uiGE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_uiGE"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a conditional sampler of diffusion models for solving Bayesian linear problems. This conditional sampler explores the second-order Tweedies formula, which extends beyond the prior works that only make use of the first order information. The proposed approach essentially forms a Gaussian approximation to the intermediate posterior distributions (along the generation steps), and eliminates the need of choosing a time-dependent variance schedule required by prior works. This approximation is exact when the true data density is indeed Gaussian and the score function is learned perfectly (and when taking the full gradient); when it not the case, the authors prove an error bound. \nThe method is evaluated and compared to prior works on a synthetic example (which reveals a superior performance in terms of the wasserstein distance metric), and real image restoration tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I've updated my score from 5 to 8 after the rebuttal. \n\n------\nThe paper is very well-written and easy to follow. The proposed method is a very elegant and simple way to extend and improve upon the prior works (namely DPS and $\\Pi$GDM)  along this line. The most practical point is that it (potentially) removes the procedure  to select a time-dependent step-size hyperparameters. The synthetic study also demonstrates that the proposed method can better capture the true posterior. Lastly, the theoretical analysis should be helpful for analyzing other similar algorithms in this space."
            },
            "weaknesses": {
                "value": "I have identified two significant limitations:\n\n1. Practical Usability and Compute Cost:\n   One major limitation I've identified is the insufficient discussion of practical usability, particularly in terms of the associated compute cost. Although the authors briefly mentioned the drawback that \"TMPD is slower, as each iteration costs more memory and compute...\" in the concluding section, I believe that this point requires more emphasis to address its impact on real-world applications. \n\nTo improve the clarify of writing, I recommend moving a summary of the discussion on computation cost comparison and cheaper approximations from Appendix E.1 into the main text. This change would provide readers with a clearer understanding of the computational trade-offs and make the paper more accessible to a broader audience. \n\nOn the practical side, I doubt if this second-order approximation can be practical without resorting to low-rank approximation or other proxy approaches (while the authors assert the computation is not expensive for inpainting but that cannot be generalized to other tasks). \n\n2. Limited Empirical Evaluation:\n   Another significant limitation is the limited empirical evaluation presented in the paper. While the paper showcases samples generated by the proposed method and other competing methods for inpainting and super-resolution tasks, it is not immediately evident how the performance of the proposed method differs from the alternatives. The paper lacks a comprehensive summary of the qualitative differences observed in these samples. To address this limitation, I suggest incorporating quantitative metrics, such as FID or IS."
            },
            "questions": {
                "value": "- Page 3, bottom part, \"the central to our method\" is this a typo? this phrase is not clear to me.\n\n- Theorem 1: can yo unpack how \"we can bound the approximation error of our SDE solely due to approximations made for the likelihood.\"? The bound in the theorem depends on H, and the property of Phi (which characterizes the behavior of how true data density deviates from a Gaussian). Where does the approximation to the likelihood enter in to this upper bound?\n\n- On page 8 \"our chosen hyperparameter \u03b7 = 1.0 (Song et al., 2021a)\". Have you defined $\\eta$? \n\n- On page 9: \"explore more efficient sampling techniques, such as ensemble methods or low rank approximations\" can you expand on how ensemble methods can be useful here? \n\n- On page 9 \"On the positive side, we found our approach more robust across various problems due to the stabilising effect of better approximations.\", similar to my comment in Weakness 2, I don't see how this point is supported other than the synthetic results. \n\n - On page 18 the compute cost: what is N? The number of data points to generate?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7905/Reviewer_uiGE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698700548554,
        "cdate": 1698700548554,
        "tmdate": 1700506553774,
        "mdate": 1700506553774,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0hfcd0DClr",
        "forum": "hDzjO41IOO",
        "replyto": "hDzjO41IOO",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_NB1r"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7905/Reviewer_NB1r"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on developing a sampler for solving inverse problems. While prior samplers use a dirac delta function to approximate the posterior, the authors use a Gaussian distribution with mean and covariance estimated using Tweedie\u2019s formula. The key idea is to take the gradients of the score in estimating the covariance. This approximation is shown to be optimal when the data distribution is Gaussian. Experiments on denoising, inpainting, and super-resolution are conducted to support the claims. The results are compared with two baselines: DPS and piGDM."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. While most of the prior works assume zero covariance while approximating $p_{0|t}$, the authors use Gaussian distribution to better approximate the posterior with a way to compute the covariance matrix. \n2. The proposed pre-conditioning helps remove the time dependent step size in prior works."
            },
            "weaknesses": {
                "value": "1. The theoretical analysis is a simple extension of Chen et al. [2]. \n2. The authors use the results from Girsanov\u2019s theorem. But there is no proof whether the condition to apply Girsanov holds in this case. \n3. The experimental results do not seem correct. In fact, I suspect the baseline results, e.g. DPS results are wrong. I have tested many of the baselines myself and the results are not as bad as what the authors have shown in this paper, zoom in Fig 3, Fig 7 for instance to see the artifacts. \n4. Page 9: \u201cThe ability to handle measurement noise also gives TMPD the potential to address certain real-world applications, such as CT and MRI imaging with Gaussian noise\u201d I am not sure how TMPD handles the noise. The results are not as good as the results reported in the baseline papers, piGDM and DPS in a similar setting. Also, it is slower than piGDM."
            },
            "questions": {
                "value": "1. Missing citation: The score is trained using a denoising score-matching objective [1].\n2. What is $C_{0|n}$ in the last paragraph on Page 4?\n3. In equation 9, it seems like the first gradient in the right hand side is over the entire function, not just the mean, which is not correct. I ask the authors to put the brackets appropriately to make it clear. \n4. What is the intuition behind the specific family of data distribution considered in Theorem 1? Is it the setting where the authors could prove something? Or does it capture any meaningful data distributions?\n5. Section 6.2: The largest standard deviation of the added noise is 0.01 and the baselines still perform poorly as per the experiments reported in this paper. However, baselines such as DPS perform much better even with higher noise level (sigma = 0.05).\n6. Super-resolution results of the proposed method TMPD in Fig 4 are equally bad with the unwanted color artifacts. Maybe take more measurements and compare. \n7. Typo in the first equation on Page 18. Missing gradient\n8. Typo on pp 14 in KL divergence. Missing gradient \n9. What is the scale of M in Theorem 1? Is the dependence on M optimal?\n\n**Reproducibility**\n\nThere are enough details present in the paper to reproduce the results.  \n\n**References**\n\n[1] Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674\n\n[2] Chen et al. \u201cSampling is as easy as learning the score\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7905/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821466118,
        "cdate": 1698821466118,
        "tmdate": 1699636969714,
        "mdate": 1699636969714,
        "license": "CC BY 4.0",
        "version": 2
    }
]