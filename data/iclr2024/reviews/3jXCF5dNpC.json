[
    {
        "id": "ZFb3SYPCll",
        "forum": "3jXCF5dNpC",
        "replyto": "3jXCF5dNpC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_ZszC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_ZszC"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a simple prompting strategy RE2 which re-reads the question multiple times. The authors demonstrate the effectiveness of the RE2 on a set of reasoning benchmarks either in the vanilla setting or in combination with other techniques including CoT, PS, PAL and self-consistency. They also conducted ablation studies on the times of re-reading, complexity of questions, and different re-reading instructions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The biggest strength of the paper to me is the simplicity of the method, making it easily adoptable by the wide research community.\n2. The paper is very comprehensive in reasoning datasets covered, models evaluated on, baselines compared against and ablations conducted.\n3. The results are mostly positive against the baselines for all the datasets and models studied."
            },
            "weaknesses": {
                "value": "1. The gains are more pronounced in weaker models (davinci-003 vs ChatGPT, Llama-2-13B vs 70B). This raises the question of the scaling behavior of the proposed RE2 method.\n2. For ARC tasks evaluated on ChatGPT, RE2 shows negative or neutral-ish results in both the vanilla and CoT settings. This is concerning, and worth more investigations to understand why.\n3. A lot of the gains in the paper are within the range of 2%, and it is unclear whether these results are just noise since the paper didn\u2019t provide any way to quantify the standard deviations."
            },
            "questions": {
                "value": "1. Typo in the last sentence of the abstract: \u201cthough-eliciting prompting\u2026\u201d.\n2. The claim of \u201cLLMs to understand the input in a bidirectional manner\u201d is misleading: it is unclear to me where the bidirectional attention from the model comes from. Neither did the authors explain what exactly they mean by \u201cbidirectional\u201d.\n3. The authors claim that LLMs gain deeper insights/understanding with RE2. However this claim is not supported by any evidence at all. It can be totally misleading. For instance, re-reading 3 times is not better than 2 times. It is possible that in pretraining corpus, there is such data which resembles the re-reading 2 times behavior, giving an edge to RE2.\n4. Ideally, RE2 should work beyond Reasoning given that humans don\u2019t do re-reading on reasoning tasks. Covering tasks beyond Reasoning would certainly make the paper much stronger."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698728021262,
        "cdate": 1698728021262,
        "tmdate": 1699636036331,
        "mdate": 1699636036331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "q7GWERue9h",
        "forum": "3jXCF5dNpC",
        "replyto": "3jXCF5dNpC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_hwHe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_hwHe"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RE2, a simple modification to improve the reasoning ability of large language models. As claimed in the paper, the existing decoder-only model can not well capture the back-and-forth interactions between different stages during reasoning. The authors simply repeat the question first before solving it. In this way, as claimed, earlier tokens can be aware of later tokens in the question. This approach are evaluated in several benchmarks including arithmetic, commonsense and symbolic. Many ablation studies are done to support the effectiveness of proposed RE2."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- To enable back-and-forth interaction during reasoning in large language models is a reasonable motivation, since a single-pass forward process in decoder-only architecture may not be sufficient for the complex reasoning process.\n\n- The experiments are well designed and complementary, supporting the proposed repeated question prompts from several perspectives.\n\n-  The paper is well organized and very easy to follow. I enjoy reading the paper."
            },
            "weaknesses": {
                "value": "- The authors connect the repeating question prompts with human's thinking process, which is a casual argument without justification to back this up. It is hard to be convinced this is how and why the repeated prompts help.\n\n- Repeating the question needs a question assumed to be there. It seems not to be generalizable for many other scenarios where it is not simply a Q-A setting, such as a multi-round conversation. Instead, approaches like chain-of-thoughts are in the solving stage, i.e., they can be used in any scenario.\n\n- In figure 2,  RE2 makes the low-complexity questions (<=3) worse in the GSM benchmark. However, the other arithmetic benchmarks (except GSM) in table1 5 6 are mostly in low complexity too. These two results are contradict. Why is this the case?"
            },
            "questions": {
                "value": "- For most datasets in table1 and table2, it seems RE2 improves vanilla more than CoT, but the case is the opposite in the symbolic reasoning. Is there any interpretation of this difference? \n\n- From table1 and table2, RE2 would almost always improve davinci-003 but seems pretty random in ChatGPT-vanilla (half better, half worse). Why do they behave in such a different way?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1103/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1103/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1103/Reviewer_hwHe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699078405098,
        "cdate": 1699078405098,
        "tmdate": 1699636036259,
        "mdate": 1699636036259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LyhmnbjA1M",
        "forum": "3jXCF5dNpC",
        "replyto": "3jXCF5dNpC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_KNMq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1103/Reviewer_KNMq"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a simple yet interesting prompt, in which the question is repeated.  Experiments conducted on a series of reasoning benchmarks serve to underscore the effectiveness and generality of the proposed prompt."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The prompt proposed in the paper is interesting and simple enough, and demonstrated to be able to effectively improve the reasoning performance of LLMs. The presentation is clear and easy to follow."
            },
            "weaknesses": {
                "value": "The experiments conducted in the paper mainly compare the proposed method with vanilla COT with backbones ChatGPT and davinci-003 (Llama-2 is used for another reasoning task). But there have been lots of COT prompts recently, and other LLMs, which have not been evaluated in the paper. Even for the conducted experiments, the proposed method is not always useful for performance improvement, which can not fully support the theoretical analysis in the paper. To me, it's more suitable for a demonstration paper."
            },
            "questions": {
                "value": "In the experiment, why the backbone LLMs were divided into two groups for two sets of reasoning tasks, i.e. ChatGPT and davincci-003 for commonsense reasoning and symbolic reasoning, and Llama-2 for arithmetic reasoning? I think you should compare the competing methods with different LLMs in the same group of reasoniing tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1103/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699368720308,
        "cdate": 1699368720308,
        "tmdate": 1699636036188,
        "mdate": 1699636036188,
        "license": "CC BY 4.0",
        "version": 2
    }
]