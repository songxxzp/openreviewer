[
    {
        "id": "S3Min56HN5",
        "forum": "OEL4FJMg1b",
        "replyto": "OEL4FJMg1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_8eDU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_8eDU"
        ],
        "content": {
            "summary": {
                "value": "This paper presents DragonDiffusion - a novel framework to enable drag-style image editing with diffusion models.\nTo this end, new techniques are presented, including (1) using an energy function to guide the editing and (2) a memory bank for editing consistency.\nQualitative and quantitative experiments show the merits of DragonDiffusion."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of enabling more precise and interactive image editing with diffusion models is an attractive topic. I believe that this work will attract both academic and community interest.\n- Sufficient experiments have been conducted to compare with DragGAN-related methods, showing the merits of the methods (e.g., DragGAN can not edit based on a reference image).\n- The ablation study demonstrates the effectiveness of the framework design.\n- Generally, the paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. My primary concern about this paper is **whether ICLR is a suitable venue**.  \nI believe this paper would be more fitting for a Computer Vision conference (e.g., CVPR, ICCV, ECCV, SIGGRAPH).  \nWhile I don't intend to downplay the contribution of this paper (in fact, I appreciate it), I find it challenging to identify a precise description for this paper within the context of ICLR.  \nPerhaps, \"representation learning for application in Computer Vision\", but given that there is no \"representation learning\" happening, I am not sure. Thus, my initial rating is \"marginally below the acceptance threshold\".\n\n1. My other complaints are mainly about *writing* (but it is not the main reasons for my decision). Authors can use it to improve paper' clarity.\n- In Abstract, \"... they often lack the ability to precisely edit the generated or real images.\" -> I think this should tone down to \"interactively\" as \"precisely\" might have a broad meaning (e.g., precise in terms of pose, shape, etc.). In a broader meaning of precision, I see existing works can also achieve \"precise\" image editing (e.g., ControlNet [1]).\n- Introduction, first paragraph: (Similar to above) While I see DragonDiffusion has clearly advanced in interactive image editing, I think it'd be more comprehensive to mention seminal works aiming to perform more precise image editing (e.g., [1-4]... you name it). Alternatively, authors can briefly discuss these works in Section 2.3.\n- Section 2.3, \"InstructPix2Pix retrain diffusion models..\" -> \"InstructPix2Pix finetunes diffusion models...\"\n- Section 2.3, \"However, text-guided image editing is coarse.\" Could you add a sentence explaining why \"coarse\" is a bad thing?\n- Section 2 though Section 3.1 all use $x_{T}$, then suddenly Section 3.2 uses $z_{T}$. As far as I understand, the authors intend to use Latent Diffusion (Stable Diffusion), which is $z_{T}$. Then, could authors revise Section 2.1 (and other related parts in Section 2-3), so it is made sure that we have mentioned $z_{T}$ before?\n\n**Reference**:  \n[1] Zhang et al., Adding Conditional Control to Text-to-Image Diffusion Models, ICCV, 2023.  \n[2] Couairon et al., DiffEdit: Diffusion-based semantic image editing with mask guidance, ICLR, 2023.  \n[3] Nguyen et al., Visual Instruction Inversion: Image Editing via Visual Prompting, NeurIPS, 2023.  \n[4] Epstein et al., Diffusion Self-Guidance for Controllable Image Generation, NeurIPS, 2023."
            },
            "questions": {
                "value": "As both DragonDiffusion and Self-Guidance [4] use (1) an energy function and (2) modify the attention layer to perform edits, could the author elaborate further on the differences between them? I also think it would be great if the author could compare them to Self-Guidance (as they can also resize objects, move objects, etc.)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Reviewer_8eDU"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698340421166,
        "cdate": 1698340421166,
        "tmdate": 1700675360182,
        "mdate": 1700675360182,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IWaJDx3VqG",
        "forum": "OEL4FJMg1b",
        "replyto": "OEL4FJMg1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_dwdi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_dwdi"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a method of drag-style image manipulation with Diffusion Model. They addressed this through the guidance based on feature matching and also conducted comparisons with previous studies such as DragGAN and DragDiffusion. The effects of using layers of various scales were analyzed, and attempts were made to preserve the content of the original image using visual cross attention. In addition, various applications such as object moving, object resizing, appearance replacing, and object pasting were also demonstrated."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The training time is short and FID score is better compared to DragGAN and DragDiffusion\n- Many applications are conducted like object moving, object resizing, appearance replacing, and object pasting"
            },
            "weaknesses": {
                "value": "- The problem targeted by this paper is not clear. Therefore, it is unclear why diffusion feature matching, drag-style editing, memory bank, and visual cross-attention strategy were introduced, giving an incremental feel.\n- If the paper is focused on the problem of drag-style image manipulation, more experimental results should be presented. For example, it is unclear why the FID score is higher compared to DragGAN. There is no related ablation study for that part.\n- Despite the introduction of the visual cross-attention strategy, it feels like that the identity or content of the image is not sufficiently preserved.\n- The choices of hyper-parameters seems heuristic. The experiment from multiple combination of hyper-parameter set could be helpful to address this issue."
            },
            "questions": {
                "value": "- Just as selecting the feature layer or combining information from the layer is important, I know that at which diffusion time the guidance is given also has a significant impact. Were there any related experiments on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Reviewer_dwdi",
                    "ICLR.cc/2024/Conference/Submission1354/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698520281497,
        "cdate": 1698520281497,
        "tmdate": 1700202017117,
        "mdate": 1700202017117,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "XpyXPgmHjY",
        "forum": "OEL4FJMg1b",
        "replyto": "OEL4FJMg1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_Fbvf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_Fbvf"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces DragonDiffusion, an image editing method that allows for Drag-style manipulation on Diffusion models. By utilizing feature correspondence, this approach transforms image editing into gradient guidance. It incorporates multi-scale guidance that takes into account both semantic and geometric alignment, as well as visual cross-attention for consistency. The proposed method demonstrates promising performance across a range of image editing tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The energy motivation that originates from classifier guidance is interesting. It motivates the design of the energy function for correspondence in diffusion models.\n- The visualization figure vividly demonstrates the editing effect."
            },
            "weaknesses": {
                "value": "- The clarity of how the memory bank is meaningful is not evident in this draft. As the memory bank is proposed as a contribution, the authors should provide a more comprehensive ablation study, including both quantitative and qualitative analysis.\n- How the energy design makes it works is not clear, the authors should provide more details numerical studies.\n- The inference time is too slow, approximately 15.93 in Table 1, which makes the solution incomparable with dragGAN."
            },
            "questions": {
                "value": "as weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698670701033,
        "cdate": 1698670701033,
        "tmdate": 1699636062635,
        "mdate": 1699636062635,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "S2mj6GAEzP",
        "forum": "OEL4FJMg1b",
        "replyto": "OEL4FJMg1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_bwVZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_bwVZ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method enabling user 'dragging' style motion control image editing, similar to DragGAN but with diffusion models. To achieve this, DDIM inversion is performed first on the input image, while the intermediate features from the UNet are saved. During the forward image editing pass, starting from the DDIM-inverted noise, at each diffusion step, three terms are calculated: 1. A cosine similarity score between the dragging patch of the DDIM inversion features and current generation features to gain local consistency, 2. A cosine similarity score between the mean features of the dragging patch of the DDIM inversion features and current generation features to gain global appearance consistency, and 3. Similarity between the unchanged features. The final score gradient is calculated by perturbing the original gradient with the gradient of a weighted sum of these similarities constraints to zt."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The task of user-defined handles is challenging and well-motivated -- \u2013 supported by various applications shown in the paper.\n* Evaluations were done with reasonable metrics and against SOTA methods, and decent improvements can be observed, especially the efficiency compared with DragDiffusion. Nice qualitative results are shown.\n* The method has significantly less complexity comparing with prior works, but seems to work well."
            },
            "weaknesses": {
                "value": "* Compared with prior (and concurrent) works such as DragGAN and DragDiffusion, way too few samples are shown. The paper and supplementary do not present enough challenging and diverse qualitative samples and comparisons.\n* The ablation is a bit incomplete. E.g. it will be nice to see some ablations on the usefulness of S_global.\n* Some flickering still happens in the no-change areas, e.g. clouds in the sun example and background in the apple example. If this is because of the balance between different losses, some ablation could be very helpful.\n* The identity preservation also seems a bit off, e.g. patterns of the apple. However, I think it is a relatively minor issue as other works also cannot completely fix this issue."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Reviewer_bwVZ"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734492632,
        "cdate": 1698734492632,
        "tmdate": 1699636062545,
        "mdate": 1699636062545,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "K4t0iStGBy",
        "forum": "OEL4FJMg1b",
        "replyto": "OEL4FJMg1b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_vgvS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1354/Reviewer_vgvS"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles a suite of image editing task (dragging editing in particular) via gradient guidance to drive the sampling from the inversed latent towards the editing target. To ensure consistency between the editing output and the original input, the KV values are cached to form a memory bank that retains the semantics of the original image. The gradient guidance formulation can deal with multiple tasks like dragging edit, object removal, object resizing, appearance replacing and object pasting. The method is mainly compared with prior dragging-based image editing approaches and shows improved quality. The results on other image tasks are also quite impressive."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- While gradient guidance has been explored extensively, using this idea as a general approach to accomplish multiple image editing tasks is cool. The qualitative results as shown in Figure 12 is stunning. And, all of these are achieved without the use of any auxiliary model.\n\n- Caching the memory bank for improved image information preservation is a useful technique.\n\n- It is welcome to report the detailed model inference time as shown in Table 1. \n\n- The proposed method works well for real images, while the editing on real images is usually challenging for many prior methods."
            },
            "weaknesses": {
                "value": "- First of all, the paper proposes to use gradient guidance sampling for a bunch of tasks, but the paper writing and the experiments mainly focus on dragging-based editing. This will narrow down the scope of the paper quite a lot. It is suggested to formulate the paper as a general solution and equally treat multiple tasks. \n\n- Also, the experiment is not thorough enough. It is suggested to conduct comparisons on other tasks besides dragging edit. For example, for object pasting, it is suggested to compare against the work \"paint by example\". For appearance replacement, it is suggested to compare against \"Diffusion Self-Guidance for Controllable Image Generation\" and \"Null-text Inversion\".\n\n- There are some typos in the paper. For example, Equation 2 is not correct. \n\n- There is no limitation analysis for the proposed method."
            },
            "questions": {
                "value": "I'm glad to see more comprehensive comparison against more approaches as aforementioned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1354/Reviewer_vgvS"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1354/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698835228347,
        "cdate": 1698835228347,
        "tmdate": 1699636062456,
        "mdate": 1699636062456,
        "license": "CC BY 4.0",
        "version": 2
    }
]