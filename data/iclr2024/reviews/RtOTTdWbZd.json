[
    {
        "id": "3OVwkVi3Xc",
        "forum": "RtOTTdWbZd",
        "replyto": "RtOTTdWbZd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_59is"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_59is"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Advantage-Induced Policy Alignment (APA), a method that applies advantage weighted regression (AWR) to Language Models (LLMs) alignment tasks and replaces the KL-divergence with a squared error loss function."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written. The analysis and discussion are insightful and contribute to a better understanding of the proposed method.\nExperiments on multiple datasets and LLMs demonstrate significant improvements over AWR."
            },
            "weaknesses": {
                "value": "The primary concern lies in the novelty and motivation of the proposed method. It seems that the objective represents a minor adjustment to previous work, where KL divergence (p log(p/q)) is replaced by a new f-divergence (p log^2(p/q)). The motivation behind this change remains unclear. The authors claim that APA no longer requires estimating the importance ratio, but it is worth noting that equation (9), which contains log(\u03c0_theta / \u03c0_init), still necessitates calculating this ratio. The authors have pointed out discrepancies between the original target and the final loss of AWR as well as the changing of \u03c0_old in the online case as motivating factors for APA. However, it appears that APA still faces these issues. Another potential motivation could be sample efficiency, but the authors have not explained why using the new f-divergence is more sample-efficient.\n\n## Theoretical Results:\nThe authors claim to provide a comparison among APA, PPO, and AWR theoretically in the introduction, but only the upper bound of APA is presented in Theorem 1. This leaves readers unable to derive the theoretical advantages of the proposed method. Additionally, the assumption for explaining why Z(s) approximates to 1 is deemed overly strong.\n\n## Experiments:\nGiven that both APA and DPO [1] are derived from the optimal policy and are non-RL methods, it is suggested that APA should be compared with more existing works, such as DPO. \n\nRecent research [2] has highlighted the significance of considering various divergences, each offering its unique balance between alignment and diversity trade-offs. Given that the core contribution of this paper revolves around the introduction of a novel f-divergence for distribution matching, it is imperative that the authors conduct a comparative analysis by pitting the new f-divergence against commonly used f-divergences, such as reverse KL, total variation (TV), and Jensen-Shannon (JS) divergences.\nFigure 2 suggests that PPO may not have converged due to a large KL penalty.\n\n## Typos:\nIn the formation of the PPO loss, \\hat{Adv} should be \"adv.\"\n[1] Rafailov R, Sharma A, Mitchell E, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" ICML Workshop MFPL, 2023.\n[2] Go D, Korbak T, Kruszewski G, et al. Aligning language models with preferences through f-divergence minimization[J]. International Conference on Machine Learning. PMLR, 2023."
            },
            "questions": {
                "value": "Recent research [3] argues that the KL penalty can be removed, and the authors themselves find that PPO without KL penalty converges to a higher reward compared to APA, as shown in Figure 7. Does this imply that PPO outperforms APA when using smaller or no KL penalty terms? It is important to note that the reviewer does not believe that the higher cost of KL divergence is the issue.\n\n[3] Gao L, Schulman J, Hilton J. \"Scaling laws for reward model overoptimization.\" International Conference on Machine Learning. PMLR, 2023: 10835-10866."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698575143915,
        "cdate": 1698575143915,
        "tmdate": 1699637052495,
        "mdate": 1699637052495,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x1WzWUJf3k",
        "forum": "RtOTTdWbZd",
        "replyto": "RtOTTdWbZd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_2o5E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_2o5E"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an alternative approach to the PPO algorithm for RLHF. The proposed approach has a clear mathematical motivation, where the key idea is to replace the KL-regularizer with a squared-error of the log-probability. Empirical results show that the proposed algorithms consistently outperform PPO in some problems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1 The presentation and intuition is clear, following the theoretical solution of KL-regularized optimization problem with several reasonable modifications. \n\n2 Some of the empirical results consistently demonstrate that the proposed algorithm is promising in terms of stability, sample efficiency, and also reward optimization. The considered datasets are also standard in RLHF literature.\n\nOverall, I feel that the authors have proposed a promising alternative approach to the PPO algorithm. And it would be interesting to see if further industry-level models are aligned with the proposed algorithm in the future. However, the paper is still not ready for publication. I have a couple of questions in the weakness part."
            },
            "weaknesses": {
                "value": "1 While I can understand the mathematical derivations along the line and it is great to see that the loss of APA is provably convergent, I am curious why the square loss is better than the KL-divergence (it is because of the guarantee provided in theorem 1?). In a more general sense, as the new loss function can be viewed as a different f-divergence, and we know that with a f-divergence as the regularizer, we can also obtain another variants of (3), do these f-divergences work better than KL? I am not sure whether there are studies in the literature of PPO but I do see a work considering this modification in DPO [1]. Can you comment on this or provide more intuitions why the squared error works better than the KL and how does it compare with general f-divergence? I think a more systematic and comprehensive comparisons among all these common f-divergence choices could largely improve the paper.\n\n2 It seems that the modification also applies to the DRL scenarios beyond the LLMs. But as far as I know, the prominent approach in research and industry is still the KL-based PPO. I am wondering whether this is due to the special case of LLMs or this modification could also outperform PPO in classical applications.\n\n3 After looking at the dataset in the huggingface, the HH-RLHF dataset used in this paper indeed only contains the helpful part. This is fine as multi-objective alignments could be more complicated and is not the focus of this paper but I think you may explicitly mention this somewhere. \n\n4 Why do you choose to finetune only the last two layers (on the HH dataset in appendix C)? It seems that with the used GPT-J-6B reward model, we might get much higher reward with PPO [2] (although it does depends on the implementation and starting checkpoint). I am not sure whether this is because you only choose to finetune the last two layers but it seems that in practice, we tends to use low-rank adapter instead of freezing most of the layers. \n\n5 In figure 6, the PPO does not work normally as the reward seems to be decrease from the beginning.  I understand that PPO often drops suddenly but this could be mitigated by using larger KL penalty and also smaller learning rate and less epochs for each iteration. I think the PPO is not tuned to the best performance so far and using the default parameters in trlx is not enough.\n\nA minor typo: I think the state space of the LLM should be the product of the token spaces instead of the union.\n\n[1] Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints\n\n[2] RRHF: Rank Responses to Align Language Models with Human Feedback without tears"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636634715,
        "cdate": 1698636634715,
        "tmdate": 1699637052369,
        "mdate": 1699637052369,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "s7TuWPl77I",
        "forum": "RtOTTdWbZd",
        "replyto": "RtOTTdWbZd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_qc78"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_qc78"
        ],
        "content": {
            "summary": {
                "value": "This work presents Advantage Induced Policy-Alignment (APA) for language model finetuning. This work first identifies a source of instability and optimization target mismatch in Advantage Weighted Regression (AWR) and addresses it with APA by minimizing a different f-divergence rather than the KL. They show on multiple RLHF tasks that APA leads to more stable and sample-efficient learning when compared to online RL (PPO) and AWR."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Good presentation of the idea in a clear way. The paper is well written and easy to follow.\n- Experimentation. Multiple tasks are evaluated with a good discussion/comparison between the RLHF tradeoff of KL from SFT model as well as reward optimization."
            },
            "weaknesses": {
                "value": "Minor comments: \n- In the main text, there is a reference to a connection to soft-q learning but it seems like only the f-divergence interpretation is discussed.\n- For the 125M parameter experiments, it seems like PPO is not properly tuned to optimize for the reward."
            },
            "questions": {
                "value": "How important was it to estimate a good critic? Was there anything else that was needed to learn a better A rather than having a separate model? This seems to be an interesting angle to experiment some more since the provable guarantees also relies on having a good advantage function for $\\pi_{old}$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698765090530,
        "cdate": 1698765090530,
        "tmdate": 1699637052251,
        "mdate": 1699637052251,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rPKC7oZ6GJ",
        "forum": "RtOTTdWbZd",
        "replyto": "RtOTTdWbZd",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_GAYy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8437/Reviewer_GAYy"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new algorithm for fine-tuning language models with RLHF. The algorithm, APA, uses a squared error loss function that aligns the output policy of the language model with a target policy based on the estimated advantages. The authors claim that APA has advantages over existing RLHF methods, such as PPO and AWR, in terms of sample efficiency, stability, and hyperparameter sensitivity. Besides, the paper provides theoretical justification for APA. The proposed algorithm is evaluated on two language tasks: StackExchange question answering and HH dataset, and outperforms existing methods such as PPO and AWR in terms of sample efficiency, stability, and KL control."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The research objective is clear and the paper is well-motivated. I do notice that PPO is sometimes unstable for training language models, and the model performance may drop with the training goes on. The proposed algorithm is simple and seems effective on the evaluation tasks. I appreciate the authors' effort in providing theoretical justification for the design of the loss function and the convergence of the algorithm."
            },
            "weaknesses": {
                "value": "The major reason I tend to reject is the scope of evaluate tasks. As RLHF (with PPO) has been well verified on ChatGPT, which has great generalization ability, PPO can be utilized to train language models in scale. In addition to training language models, PPO has stand the test of time in many other area such as robotics control. I believe that is why researchers use PPO to fine-tune many large language models. However, the evaluation tasks in this paper locate in a specific domain. I think it is not enough to be an alternative to PPO.\n\nBesides, I also have some minor concerns:\n1. The paper does not provide much discussion on the choice of the hyperparameter $\\lambda$, which controls the trade-off between the expected advantage and the KL divergence. Besides, as this paper proposes an optimization algorithm, the ablation study is required.\n2. There seems lacking an analyze the qualitative differences between the outputs of different algorithms, or the potential harms or biases of the reward models."
            },
            "questions": {
                "value": "My major concerns have been listed in 'Weakness' section. Here are some other questions:\n\n1. Can APA generalize to other language tasks or domains? \n2. What do you think are important for a RL algorithm in RLHF training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8437/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8437/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8437/Reviewer_GAYy"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8437/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699085970541,
        "cdate": 1699085970541,
        "tmdate": 1700665118076,
        "mdate": 1700665118076,
        "license": "CC BY 4.0",
        "version": 2
    }
]