[
    {
        "id": "jeamltWAwj",
        "forum": "kaGA40pfFY",
        "replyto": "kaGA40pfFY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_58ny"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_58ny"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Rationality of Thought as a new method to prompt LLMs into a rational thinking process. In addition, they propose a new benchmark inspired by problems that have been used to study cognitive biases in humans. Their methods improve the accuracy of LLMs on their new benchmark and on existing reasoning benchmarks (but to a lesser degree)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed method is interesting, clean, and well-motivated.\n\nThe paper is well-written and easy to follow.\n\nThe authors propose a new and interesting benchmark.\n\nThe obtained results are promising."
            },
            "weaknesses": {
                "value": "The authors write that each type of bias has 16 questions in their data set. Were all of these problems taken from prior literature or does this also include novel problems created by the authors? My main concern about the current state of the paper is regarding this issue. Many of the original cognitive bias problems (e.g., the Linda problem) are presumably heavily featured in the training data. To ensure that we are probing the reasoning capabilities of LLMs, it would be important to include novel (i.e., rephrased) versions of the original problems. While the current set does not include any such problems, I am unlikely to increase my score further. Maybe this is already done but the information is just not provided. In this case, the authors should describe how the rephrasing was done.\n\nImportant references to prior work are missing. For example, Binz & Schulz (2023) already studied some of the biases included in the present benchmark. In addition, the present paper takes quite a one-sided view in claiming that LLMs are becoming increasingly similar to human reasoning. There are many examples where this is not the case, see for instance Ullman (2023) or Mitchell & Krakauer (2023).\n\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120.\n\nUllman, T. (2023). Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.\n\nMitchell, M., & Krakauer, D. C. (2023). The debate over understanding in AI\u2019s large language models. Proceedings of the National Academy of Sciences, 120(13), e2215907120.\n\nThe visual presentation could be improved in places. In particular, while Figure 1 gives a good intuitive understanding of the approach, the reader has to go to the SI to see the exact RoT prompt. This is unfortunate as this is the heart of the paper. Furthermore, Table 1 does not add any value in my opinion. A good solution would be to therefore replace Table 1 with Table 5 from the SI or similar.\n\nMinor:\n\nFormatting is a bit weird in places, especially spaces are often used wrongly/inconsistently.\n\nEquation on page 3:\n* is not numbered.\n* p_rot does not appear.\n* \\in \\mathcal{D} should be in the subscript (also for the equations later in the paper).\n* the equation implies that there is some optimization happening but that is not the case to my understanding (maybe I am missing something here).\n\nFigure 3: legend and text are way too small to be readable. To allow for larger font sizes, subplots a and c could be removed given that they display redundant information with b and d.\n\nThe conclusion is not a conclusion as it is followed by a related work section.\n\nPage 9: the authors say \u201cResearch by Alaina ...\u201d but then cite another author (maybe first and surnames are exchanged here)."
            },
            "questions": {
                "value": "The authors use zero-shot CoT for the experiments in section 3 and few-shot CoT for the experiments in section 4. Why was this choice made? It would have been easy to also use zero-shot CoT for the experiments in section 4 (especially if the claim is that RoT is nice because it does not require example solutions). Related to that, the fact that zero-shot CoT actually decreases performance on the cognitive bias dataset is surprising. Any idea why this is the case?\n\nGiven that the improvements were not so great in GPT3.5, would you say that RoT prompting is an emergent ability?\n\nFinally, I was wondering whether all the steps in RoT prompting are needed or whether some of them could be removed without a loss of performance. I am not asking the authors to run additional ablations but I think this is still interesting to think about."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Reviewer_58ny"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698401925882,
        "cdate": 1698401925882,
        "tmdate": 1700918421798,
        "mdate": 1700918421798,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fLJ2F4bcrh",
        "forum": "kaGA40pfFY",
        "replyto": "kaGA40pfFY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_QJRm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_QJRm"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a prompt engineering technique, called RoT (Rationality of Thought), whose goal is to align with the underlying logic of human cognition. The paper claims that RoT can assist large language models (LLMs) to reduce cognitive biases. Compared with Chain-of-Thought and other reasoning-enhancement techniques, RoT does not need additional manual annotations. RoT integrates various methods to improve reasoning of LLMs (e.g. thought chains, self-reflection, and expert knowledge) into a unified theoretical framework rooted in cognitive psychology. As part of the study, the paper has created a cognitive bias dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper proposes a new prompt engineering technique, RoT designed to avoid cognitive bias. RoT include steps: identification, decomposition, reflection, calculation, and evaluation, among other markers of rational thought. \n\n2. When applied RoT to GPT-3.5-turbo and GPT-4 on the cognitive bias test set, accuracy improvements are 1.5% and 18.7%, respectively. \n\n3. The paper collects a cognitive bias test dataset including 29 such biases."
            },
            "weaknesses": {
                "value": "1. There does not seem to be convincing evidence that Rational of Thought method can improve arithmetic reasoning and common sense reasoning tasks. Specifically improvements on GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021),AQUA-RAT (Ling et al., 2017) , and ARC (Clark et al., 2018) over CoT are +0.4, -0.6, +4.8, -0.3, +0.7. Other than AQUA-RAT, the performance degradations and improvements are all very small. However, the paper does not dive deep into AQUA-RAT performance improvement.\n\nAs shown in the paper \"Large Language Models Cannot Self-Correct Reasoning Yet\", it is not clear RoT can improve reasoning when compared with CoT. \n\n2. Despite connecting RoT to human cognition, there is little insight on why RoT can improve cognitive biases."
            },
            "questions": {
                "value": "Now there are a large number of prompt engineering techniques that decompose problems into sub-problems. The paper should comment and hopefully compare to some of the most notable ones, such as Least to most prompting, tree of thoughts, etc.\n\nBesides the GPT family models, the authors are encouraged to study Anthropic Claude and open source models such as LLaMA2. This would be very important to see whether the techniques can also apply to other LLMs.\n\nThe evaluation seems to focus on multiple choices. LLMs are known to have position bias. Does the result change if you change the ordering of the answer choices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698537790205,
        "cdate": 1698537790205,
        "tmdate": 1699636123331,
        "mdate": 1699636123331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EXP5El547a",
        "forum": "kaGA40pfFY",
        "replyto": "kaGA40pfFY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_Zkyz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_Zkyz"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Rationality of Thought, a prompting technique for reducing cognitive biases in LLMs. Based on a diverse collection of psychology papers, the authors compose a dataset of 464 questions reflecting 29 cognitive bias types. The RoT prompt improves performance of GPT-3.4 and -4 on this set of questions, and transfers effectively to other LLM benchmarks."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The approach is well grounded in studies of human psychology. \n- The authors expend significant effort on summarizing the cognitive bias types in prior psychology work.\n- The constructed dataset is valuable for assessing the cognitive biases of general-purpose AI agents.\n- The RoT prompt is simple and effective. It improves the performance of LLMs significantly, outperforming chain-of-thought."
            },
            "weaknesses": {
                "value": "- The data collection process is insufficiently detailed. See questions. \n- The prompt search process is not transparent. See questions\n- Missing ablation studies. \n- The authors do not discuss the limitations of this approach (e.g., time/#tokens, generalizability)."
            },
            "questions": {
                "value": "- Does the dataset have a train and a test set? If yes, are the results in figure 2 on the train or test set? If no, did you search for a prompt to overfit GPT-4 on the dataset?\n- The questions are collected from \"multiple authoritative psychological works\"? Are they the same as those from which you compose the list 93 common cognitive biases? If not, you must cite them explicitly. \n- What are the limitations of the dataset? Is it also biased in some way (e.g., representing only Western culture, focusing on a majority group)?\n- How did you come up with the prompt? Are these steps of solving a problem proposed prior psychology work?\n- For a fair comparison, can you also provide the results of CoT zeroshot in table 3? It is possible to put the average cost of each prompting technique as a column?\n- Can you conduct an ablation study to demonstrate the importance of each step in RoT? \n- Step 3 of the prompt encourages \"probability calculations, Bayesian methods, and other data analysis techniques\". It is a very specific request. Do you have evidence that the models actually follow this instruction? How do they behave when the solution of a problem does not require one of those techniques? \n- Do you use the same prompt for each evaluation domain? Are there any customizations?\n- Do you specify all the RoT steps all at once or input one step, wait for GPT to respond and then input the next step?\n- It would be more interesting to try this method on tasks on which the Direct-zeroshot performance is low (e.g., code generation?). Except GSM8K, performance of GPT-4 on other tasks is already strong."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1926/Reviewer_Zkyz"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698815027273,
        "cdate": 1698815027273,
        "tmdate": 1700636192954,
        "mdate": 1700636192954,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AZpYzC9KEa",
        "forum": "kaGA40pfFY",
        "replyto": "kaGA40pfFY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_BFjh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1926/Reviewer_BFjh"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new prompt for LLM called \"Rationality of Thought\" especially designed to extract more rational answers from input queries. The prompt is evaluated on a proposed cognitive bias dataset as well as a number of reasoning datasets and is shown to outperform both direct prompting as well as the existing \"chain-of-thought\" prompting."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper is easy to read and the overall presentation is clear.\n\nThe contribution of this work -- the RoT prompt, seems novel and works especially well for super large model like GPT-4."
            },
            "weaknesses": {
                "value": "I think the only missing element is an ablation study. Given the rather wordy prompt, one wonders which part of it contributes the most to improving answer quality. \n\nAlthough it is in the appendix, I am not sure what role Table 4 plays in this work. Where is this algorithm used?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1926/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849377521,
        "cdate": 1698849377521,
        "tmdate": 1699636123176,
        "mdate": 1699636123176,
        "license": "CC BY 4.0",
        "version": 2
    }
]