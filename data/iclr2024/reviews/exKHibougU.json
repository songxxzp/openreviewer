[
    {
        "id": "1BDSDpzPWa",
        "forum": "exKHibougU",
        "replyto": "exKHibougU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a new text-to-video generation pipeline called LLM-grounded Video Diffusion (LVD). In particular, it first uses LLM to generate the layouts of the video and then uses the generated layout to guide a pre-trained video diffusion model. The whole process does not update the weights of both the LLM and video diffusion model. Besides, the authors show that LLMs\u2019 can generate spatiotemporal dynamics aligned with text prompts in a few-shot setting. Qualitative results and quantitative results show that LVD generates higher quality videos that also align more with text."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Overall, the paper is well-organized and easy to follow. The figures and tables are informative.\n\n- The finding that LLMs can generate good spatiotemporal dynamics with only three examples is interesting and well supported by the experiments. The exploration of physical properties contained in LLM is also inspiring and deserves further research. \n \n- The results generated by LVD are promising compared to the baseline, ModelScope."
            },
            "weaknesses": {
                "value": "- The idea of using LLM to generate layout is already explored in LayoutGPT (Feng et al., 2023) and LMD (Lian et al., 2023). LMD also adapts in a training-free manner. It is beneficial for the authors to include a more detailed comparison.\n\n- The technical contribution is limited. The first layout generation part is similar to LMD, and the second part is a straightforward application of other training-free layout-to-image methods in the video domain."
            },
            "questions": {
                "value": "- From Table 3, we can see that LVD improves the video quality. What causes the improvement?\n\n- Is LLM able to generate reliable layouts using text without direction information, such as \u201ca walking dog\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_ZTWf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808278835,
        "cdate": 1698808278835,
        "tmdate": 1699636163279,
        "mdate": 1699636163279,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xbIDQ3ilfA",
        "forum": "exKHibougU",
        "replyto": "exKHibougU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
        ],
        "content": {
            "summary": {
                "value": "Grounded Text-to-image generation has been studied by several papers recently. However, text-to-video geneartion with layout control is still unexplored. This paper tackles this task by proposing a training-free method by adding layout information through adjusting the attention maps of the diffusion UNet. Speficically, this paper first utilizes LLMs (GPT-4) to generate a multi-frame object layouts, then designs a layout-grounded video generator that encourages the cross-attention map to concentrate more on the bounding box areas. Extensive experiments for spatiotemporal dynamics evaluation have demonstrated the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is clearly written and easy to follow.\n\n- The proposed method is training-free, which avoid the need for costly training with image/video data.\n\n- Using LLM-generated layouts for videos is relatively unexplored. And it's natural to use the knowledge embedded in LLMs to general layouts for downstream video generation."
            },
            "weaknesses": {
                "value": "- Even though the proposed method is training free, it takes longer time during inference to generate videos due to the optimization steps needed for the energy function.\n\n- Training-free layout control already exists in previous literatures [1, 2]. Therefore, the design of the energy function and backward guidance is not that novel. \n\n- Ablation study of the model design is not given (e.g., number of DSL guidance steps, energy function design).\n\n\n[1] Chen, Minghao, Iro Laina, and Andrea Vedaldi. \"Training-free layout control with cross-attention guidance.\" arXiv preprint arXiv:2304.03373 (2023)\n\n[2] Lian, Long, et al. \"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.\" arXiv e-prints (2023): arXiv-2305"
            },
            "questions": {
                "value": "- Could the authors provide some reasoning why they report video-text similarity metric in Bain et al., 2021? It would be nice to also report CLIPScore, since its widely reported in other text-to-video generation baseilnes.\n\n- The examples provided in the paper are with non-overlapping bounding boxes. Will the proposed method work well with overlapping layouts?\n\n- If there are multiple objects, is the final energy function summing over the energy function corresponding to each object?\n\n- It seems that the background areas of the generated images with proposed method are quite static (Fig1, 7, 8, 9). Is this because the model encourages static background, or becuase the visualized images happens to have relatively static background? \n\n- Based on my understanding, another concurrent work, VideoDirectorGPT [1], is also for text-to-video generation with layout guidance. Even though the technical routes are different from this paper, it would be nice to have some discussions and comparison in the related work section. \n\n[1] Lin, Han, et al. \"Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning.\" arXiv preprint arXiv:2309.15091 (2023)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_34cZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698823445926,
        "cdate": 1698823445926,
        "tmdate": 1699636163209,
        "mdate": 1699636163209,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DkaqDWnnxV",
        "forum": "exKHibougU",
        "replyto": "exKHibougU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel approach to text-conditioned video generation that seeks to address the limitations of current models, which struggle with complex spatiotemporal prompts and often produce videos with restricted or incorrect motion patterns. The key contribution is the LLM-grounded Video Diffusion (LVD) model that separates the video generation task into two steps: (1) using a Large Language Model (LLM) to generate dynamic scene layouts (DSLs) from text inputs, and (2) using these layouts to guide a diffusion model in generating the video. The approach is described as training-free and can be integrated with existing video diffusion models that allow for classifier guidance. Moreover, they introduce a benchmark for evaluating the alignment between input prompts and generated videos."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposal of a training-free approach presents a pipeline that is well-suited for the application of off-the-shelf LLMs and diffusion models. Its simplicity yet effectiveness stands out as a notable strength.\n- The discovery that LLMs can generate spatiotemporal layouts from text with only a limited number of in-context examples is noteworthy. It highlights the potential for a straightforward integration of LLM reasoning into text-to-video tasks."
            },
            "weaknesses": {
                "value": "- The idea of guidance via energy functions and cross-attention maps seems to be basically derived from BoxDiff (Xie et al., 2023;) and Chen et al. 2023a;. It is unclear how much of this work is based on previous research and how much is new. Since they are dealing with video generation using layouts, it would have been nice to see the authors' contribution in extending to the temporal axis, but this is not evident, which is disappointing.\n- I am concerned that the scale of the sample size for the proposed DSL benchmark may be too small to conduct a sufficiently robust evaluation.\n- The paper's contribution appears to lack novelty. There is existing work in text-to-image generation that has already established the capability of LLMs to create layouts, and this research seems to merely extend that to assess whether the same capability applies to temporal understanding. I didn't perceive any novel ideas stemming from the temporal aspect of the problem that would distinguish this work significantly from its predecessors.\n- The paper seems to lack a detailed analysis or ablation studies concerning the prompts given to the LLM for generating Dynamic Scene Layouts (DSLs). Such investigations are crucial to understand how different prompts affect the LLM's output and the subsequent quality of the video generation. Further exploration in this area could significantly bolster the robustness of the presented approach.\n- The paper's current framework could indeed benefit from additional ablation studies or analytical experiments to demonstrate the effectiveness of using DSLs for training-free guidance of text-to-video diffusion models. Moreover, a theoretical explanation of why this particular approach is effective would be valuable. It's important for the research to not only present the method but also to thoroughly validate and explain why certain choices were made and how they contribute to the overall performance and reliability of the model."
            },
            "questions": {
                "value": "- Can the authors elaborate on how the model performs with ambiguous or complex text prompts that might yield multiple valid interpretations in terms of spatial and temporal dynamics?\n- Could the authors discuss any observed limitations or failure modes of the LVD approach, particularly in cases where the LLM might generate less accurate DSLs?\n- (Minor point) Typographical error in Section 4, second paragraph. The sentence in question should indeed conclude with \"feature map\" instead of \"feature ma.\" A revision is recommended for accuracy and clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2307/Reviewer_6SgA",
                    "ICLR.cc/2024/Conference/Submission2307/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2307/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698874944551,
        "cdate": 1698874944551,
        "tmdate": 1700548666916,
        "mdate": 1700548666916,
        "license": "CC BY 4.0",
        "version": 2
    }
]