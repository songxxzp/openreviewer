[
    {
        "id": "neyFV0WvE8",
        "forum": "sFyTZEqmUY",
        "replyto": "sFyTZEqmUY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_ihUZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_ihUZ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a universal simulator (UniSim) that aims to simulate how humans and agents interact with the world. The proposed framework combines various types of datasets, including internet text-image pairs and robotics data, with the motivation that existing datasets are useful along different axes. The paper uses a video diffusion model as an interactive simulator of the world. UniSim can simulate both high-level instructions and low-level control, which show zero-shot transferability to real-world scenarios, addressing the sim-to-real transferability problem. The authors highlight the potential for UniSim to be used in broader applications, such as video captioning and rare event detection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- This is an interesting paper that presents some exciting results.\n- The paper is well organized and well-written."
            },
            "weaknesses": {
                "value": "- It would be nice if the paper delved more into the limitations of the models. The paper has shown that exciting results can be obtained, but it's useful for the community to know the limits of the generalization capabilities, especially if people want to use this in the future for various applications. \n- For reproducibility, it would be helpful if the authors could release the code and some example pre-trained checkpoints."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698732580272,
        "cdate": 1698732580272,
        "tmdate": 1699636211375,
        "mdate": 1699636211375,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DBZbFbd302",
        "forum": "sFyTZEqmUY",
        "replyto": "sFyTZEqmUY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
        ],
        "content": {
            "summary": {
                "value": "This paper presents UniSim, a video prediction and generative model aiming for serving as a universal simulator of diverse scenarios conditioned on input language-described actions. It devotes a big effort in combining dataset with different modalities and information axes, trained a unified generative model, and shows the trained model can be used for downstream policy learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- Very cool and impressive research direction and proposed method\n- Huge effort devoted in unifying multiple large scale datasets\n- Experiments demonstrated effectiveness for downstream policy learning"
            },
            "weaknesses": {
                "value": "I think the paper presents a very important step towards learning a universal video predictive world model. One of my questions is, the shown demo looks like generally still in distribution, in terms of generalization across different embodiment: the generated video contaiing robot are very similar to robotic dataset, and in more complex scenes training using human videos the model seems only handling human hands. How does it work in those complex scenes when the model is commanded to predict outcomes given a robot action input?\nAlso, when it comes to low level control input, the paper seems only handling delta motion in the cartesion space. Does it handle more general end-effector action in SE3 space? (joint space seems out of reach for this family of method since it's not observable) Is it true that for predicting outcomes conditioned on robot action, the robot arm needs to be visible in the first place?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_b2Jg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782698771,
        "cdate": 1698782698771,
        "tmdate": 1699636211287,
        "mdate": 1699636211287,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2qaG3iXNkU",
        "forum": "sFyTZEqmUY",
        "replyto": "sFyTZEqmUY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
        ],
        "content": {
            "summary": {
                "value": "- The paper presents a video diffusion model that does conditional next (few) frame prediction. It conditions on previous frames and either a text description of the video, or more granular actions (robotic movements, or camera angles). The focus is on its use in robotics contexts.\n- The novelty is in the mix of data trained on. Rather than focusing on a single environment or even single action space, the model (UniSim) is trained jointly on 14 common datasets, from the text-image LAION dataset (often used for image generation), to the Something-somethingV2 video dataset (often used for video classification). Significant compute is used (512x TPUs)\n- A limited ablation is conducted on how previous observations should be conditioned upon.\n- Three use cases are explored:\n1) A separate vision-language model is first trained to predict language instruction and actions, given a start and end observation, on a robotics control task in the Language Table environment. It is then finetuned using simulated trajectories from data synthetically generated by UniSim (longer to those in the original dataset).\n2) A separate policy model is first trained via BC on the Language Table environment, then finetuned with RL using simulated trajectories from data synthetically generated by UniSim (itself trained on Language Table data).\n3) A separate video-to-caption model is found to benefit when finetuned on data synthetically generated by UniSim, for producing captions on ActivityNet.\n\n___\nFollowing the rebuttal, I upgrade my ratings as follows: soundness 2$\\to$3, overall rating 5$\\to$8, and confidence 4$\\to$5. The main remaining weakness is that the mixture of datasets and modalities (a key contribution of the work) appears to be of limited benefit on the tasks assessed by the paper. But there are enough positives in the paper for me to downweight this issue."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper will undoubtedly draw a lot of attention and excitement from researchers working in several areas, including RL, robotics, large models, and diffusion models.\n- It represents a major effort in training a cross-domain model, with emphasis on its use for robotic control.\n- I welcome this kind of larger scale-up work being submitted to an academic conference, since a recent trend has seen similar works restricted to industry-lab preprints.\n- The first few pages motivating the work are quite inspiring.\n- Effort has been made to explore a range of use cases.\n- Overall it represents a very promising direction towards foundational models for control."
            },
            "weaknesses": {
                "value": "I expect that this paper will comfortably clear the bar for acceptance. However, there are two main issues I believe should first be addressed. I've set my score relatively low because of these, but anticipate increasing it following a revised version.\n\n1) Whilst it's difficult to accuse the paper of overclaiming in any specific place, the writing and framing risk feeling a little showy. The title is very general, and applies to any paper on model-based RL for the real-world rather than something specific to this paper, and naming the method a \"universal simulator\" feels grandiose. (Happy to collect other revewiers' opinions on this.) The connection between POMDP's and action-conditioned video generation is more-or-less assumed by any world model paper (e.g. [1]), and shouldn't be highlighted as a main contribution of the paper.\n2) One of the recurring claims throughout the paper is that the major novelty is UniSim's \"orchestration of diverse datasets, each providing a different aspect of the overall experience\" into a single model. Yet no hard evidence is given for this combination being important -- aside from two vague figures in Appendix E. At a minimum, it would be important to train a version of UniSim on say, datasets from the Language Table environment _only_, and report numbers for when synthetic data was generated from this, in Table 2 and 3. This would help support the claim that dataset diversity is valuable.\n\nOther issues (in decreasing priority)\n- I think it'd be useful to investigate how entwined the effect of actions is with the dataset distribution. For example, could camera commands (zoom in etc) successfully be applied to kitchen scenes as in Figure 3? The fact that the name of the dataset had to be included as part of the action during training, makes me suspect actions may not be able to generalise well to new kinds of video. This would not be a dealbreaker for the paper's acceptance, but is important to show readers how general this data mixing is.\n- A lack of strong baselines might be expected for this kind of scale-up work. But in their absence, ablations become more important, to verify that the various components of the model were all necessary. The paper only presents a brief study of which frames to condition on.\n- The model section is poorly written. The use of $\\mathcal{T}$ is (I think) slightly misleading -- usually the transition fn of a POMDP is defined as operating on the states, $\\mathcal{T}(s_t,a_t) \\to s_{t+1}$, and there is a separate emission function producing the observations, $p(o_t|s_t)$. Eq. 1 implicitly combines these -- I might recommend renaming it $f$ or $g$ or whatever. I didn't follow why $o_l$ notation needed to be introduced, since it's immediately unrolled into $[o_t, o_{t+1}]$ and never referred to again. I also didn't understand why the model conditions on the noised, rather than clean, previous observations. It's said the last four frames are concatenated from $o_{t-1}$, which confused me -- does $o_{t-1}$ represent four frames, or should it read $o_{t-1:t-4}$ or similar?\n- It's a shame to give the model details only in the Appendix C, as I believe many readers would be interested in them. I hope some of these can be shifted to the main body, particularly key details around the diffusion architecture (such as the core and super-resolution modules) and the amount of compute required.\n- Any algorithmic or model novelty is light (more or less straightforward video diffusion).\n- The two main experiments were conducted on environments that were within the training distribution of UniSim. It would have been more impressive to investigate the performance on new environments.\n- The wordy description of all datasets in 2.1, I felt was much better summarized by Table 5 in the Appendix (perhaps with the addition of a column explaining how an action space is defined and handled), and might be swapped. (Optional!)\n\nMinor issues/questions\n- Appendix says 1M steps on 512 TPUs with batchsize 256 -- this seemed a low ratio of training updates to available compute. Did performance saturate beyond this?\n- What was the wall clock time of the model training?\n- How many parameters were in the model?\n- Will the model weights be open-sourced?\n\n[1] Transformers are Sample-Efficient World Models"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_kkjZ"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698947816457,
        "cdate": 1698947816457,
        "tmdate": 1700754985809,
        "mdate": 1700754985809,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ihrdy34PtF",
        "forum": "sFyTZEqmUY",
        "replyto": "sFyTZEqmUY",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose to learn a universal simulator (UniSim) of real-world interaction through generative modeling (a diffusion model for outputting the next frame given the previous frame and the input actions). They achieve so by careful orchestration of diverse datasets, which are rich along completely different axes (e.g., some videos have object-level diversity, some have densely labeled language instructions, and some have scene-level diversity). They show applications of the proposed simulator such as training long-horizon embodied planners and low-level object manipulators."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Reasonably scalable approach to collect training data for the proposed simulator \n+ The use of diffusion models to fuse different aspects of the diverse datasets with decent results is impressive\n+ Particularly the sim-to-real transfer is a promising direction for using the proposed real-world simulator."
            },
            "weaknesses": {
                "value": "While this work shows great promise in a range of downstream applications. I believe it might need more experimental evidence to support the claim that it can simulate low-level actions well. Specifically, section 4.2 only shows results for a relatively simple object (mostly blocks) re-arrangement (without grasping, e.g.) on a table. What about grasping objects, pulling objects (e.g., opening a drawer), etc? It will give us insights as to how fine-grained the controls are supported by the proposed simulator, even if it cannot simulate low-level actions perfectly."
            },
            "questions": {
                "value": "See \u201cweaknesses\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2698/Reviewer_ZLpe"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2698/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698999340626,
        "cdate": 1698999340626,
        "tmdate": 1699636211128,
        "mdate": 1699636211128,
        "license": "CC BY 4.0",
        "version": 2
    }
]