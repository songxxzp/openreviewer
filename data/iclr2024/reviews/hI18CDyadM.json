[
    {
        "id": "53RSL4PPu1",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_S2zK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_S2zK"
        ],
        "forum": "hI18CDyadM",
        "replyto": "hI18CDyadM",
        "content": {
            "summary": {
                "value": "This paper proposes a local motion deblurring Transformer with adaptive window pruning, which only deblur the active (blurry) windows. The windows are classified as active/inactive according to the predicted bluriness confidence score."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Overall, I think this paper has a novel idea and achieves good results in terms of performance and efficiency. I tend to accept this paper due to following reasons.\n\n1. adaptive window pruning that saves computation on unnecessary attention windows.\n\n2. bluriness confidence prediction that works well for both local motion prediction and global motion prediction\n\n3. annotated local blur masks on ReLoBlur\n\n4. well-designed experiments, well-presented figures and well-written paper."
            },
            "weaknesses": {
                "value": "Below are some concerns and suggestions.\n\n1. Since the confidence predictor only uses MLP layers. How many pixels did you shift? Is the feature shift necessary to enlarge the receptive field of the neighbourhood? \n\n2. What is the mask prediction accuracy on validation set?\n\n3. How did you decide the border when annotating the masks for blurry moving objects?\n\n4. If a patch is always abandoned, how is it processed? What layers it will be passed into during inference?\n\n5. It could be better to provide the results of two special cases (masks are all-ones/all-zeros) in the tables as a reference.\n\n6. Why did you only report real-world results on large images? Do you have a chart for comparison on PSNR/FLOPs/runtime under different image resolutions.\n\n7. The key of this method is the adaptive window pruning. It is better to provide an ablation study with the rest tricks as a baseline (i.e., no window pruning in training and testing).\n\n\nMinor:\n\n1. Unfinished paragraph in page 2.\n\n2. For figure 1, I think it might be better to visualise the attention window borders, for example, by adding solid lines to show the windows.\n\n3. The summarised contributions are a bit overlapped (point 1 and point 2). I think it's better to claim adaptive window pruning and bluriness confidence prediction as two contributions.\n\n4. I think there is no need to distinguish between AdaWPT-F and AdaWPT-P. Just be simple and united. The name of AdaWPT is enough.\n\n5. Comparison in Figure 6 is not visible.\n\n6. Is there an example on a globally clear image (e.g., a still scene). Contrary to Figure 7, will the decision map be all zeros?\n\n7. Are there other similar works in image restoration/deblurring? Are there some connections between this method and some \"blur kernel prediction + restoration\" methods (e.g. Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution)?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697371750705,
        "cdate": 1697371750705,
        "tmdate": 1699636469643,
        "mdate": 1699636469643,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r3rtBP02tp",
        "forum": "hI18CDyadM",
        "replyto": "hI18CDyadM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
        ],
        "content": {
            "summary": {
                "value": "The presented work delves into an interesting research problem: local motion deblurring. It introduces a novel approach known as LMD-ViT, constructed using \"Adaptive Window Pruning Transformer blocks (AdaWPT).\" AdaWPT selectively prunes unnecessary windows, focusing powerful yet computationally intensive Transformer operations solely on essential windows. This strategy not only achieves effective deblurring but also preserves the sharp regions, preventing distortion. Moreover, their method significantly accelerates inference speed. Additionally, they have provided annotated blur masks for the ReLoBlur dataset. The utility of this approach is showcased on both local and global datasets, where it demonstrates substantial performance improvements on local motion deblurring (the ReLoBlur dataset) and competes favorably with baseline methods in the realm of global deblurring."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1.\tThis paper addressed problems of single image local motion deblurring, which is very essential in today\u2019s photography industry. The presented method is the first to apply sparse ViT in single image deblurring and may inspire the community to enhance image quality locally.\n2.\tThe proposed pruning strategy including the supervised confidence predictor, the differential decision layer and pruning losses are reasonable and practical. It combines window pruning strategy with Transformer layers, only allowing blurred regions to go through deblurring operations,  resulting in not only proficient deblurring but also the preservation of sharp image regions.\n3.\tThe quantitative and perceptual deblurring performances are obvious compared to baseline methods.\n4.\tThe presented method derives a balance between local motion deblurring performance and inference speed, as shown in the ablation study and experiments. The proposed method reduced FLOPs and the inference time largely without deblurring performances dropping on local deblurring data.\n5.\tThe authors provided annotated blur masks for the ReLoBlur dataset, enhancing the resources available to the research community."
            },
            "weaknesses": {
                "value": "1.\tThe authors did not mention whether the presented method LMD-ViT requires blur mask annotation during inference. This is crucial because if the method does require blur masks before inference, it would be helpful to provide instructions on how to generate them beforehand and assess their practicality.\n2.\tThe proposed method uses Gumble-Softmax as the decision layer in training and Softmax in inference. The equivalence of the two techniques in training and inference is not discussed.\n3.\tIn the user study experiment, the absence of an explanation regarding the camera equipment used is notable. This is important because when images from the same camera share a common source, the blurriness often exhibits a consistent pattern. Therefore, including images from the same camera would allow us to assess the proposed method's robustness.\n4.\tSome references are missing, like \u201cWindow-based multi-head self-attention\u201d in page 2, and \u201cLeFF\u201d in Section 2.4.1."
            },
            "questions": {
                "value": "please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Reviewer_SKAv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698726082242,
        "cdate": 1698726082242,
        "tmdate": 1699636469560,
        "mdate": 1699636469560,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aAoyznWwwz",
        "forum": "hI18CDyadM",
        "replyto": "hI18CDyadM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
        ],
        "content": {
            "summary": {
                "value": "The paper attempts to tackle local motion deblurring. Existing deblurring literature mostly focuses on general deblurring (without specifying global or local regions). This could be a disadvantage in computation and generalization in scenarios where only a small part of a high-resolution image is blurred. Therefore, this work proposes a transformer-based network called LMD-ViT. The authors make use of adaptive window pruning and blurriness confidence predictor in the transformer blocks to ensure the network focuses on local regions with blurs. Quantitative and qualitative results are presented on the ReLoBlur and GoPro datasets. The effectiveness of different design choices is analyzed."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "An adaptive window pruning strategy is adopted to focus the network computation on localized regions affected by blur and speed up the Transformer layers.\n\nA carefully annotated local blur mask is proposed for the ReLoBlur dataset to improve the performance of local deblurring methods."
            },
            "weaknesses": {
                "value": "The organization of the paper can be improved.\n\n1) The methodology (Sec. 2) consists of too many (unnecessary) acronyms. Moreover, there are some inconsistencies when citing previous works (for example, LBAG (Li et al., 2023), LBFMG (Li et al., 2023), etc.). It would be better for the submission would strongly benefit from polishing the writing.\n\nThe settings of the experiments need more explanation.\n\n2) It is not clear why the GoPro dataset is used for training along with the ReLoBlur training set. In previous works, such as LBAG, only the ReLoBlur dataset is used (see Table 4).\n\nThe novelty of the submission needs to be clarified.\n\n3) It would be better to discuss the differences between LBAG and Uformer. Compared to LBAG, it simply substitutes the CNN architecture with a Transformer. All the other modules, including sparse ViT, W-MSA (Window-based multi-head self-attention), and LeFF (locally- enhanced feed-forward layer), have been introduced in previous deblurring works.\n\nThe fairness of the experiments.\n\n4) The transformer baselines, such as Restormer and Uformer-B, are not trained with the local blur masks, which are deployed during their training by the proposed methods. This makes the comparison in Table 1 unfair.\n\nUnclear parts.\n\n5) Please explain in detail how the authors manually annotate the blur masks for the ReLoBlur dataset.\n\n6) The baseline results reported in Table 1 are higher than those in their original papers, e.g., LBAG. It would be better to give the reasons and more details when introducing Table 1. The baseline results reported in Table 1 are higher than those in their original papers, e.g., LBAG for 34.85 dB.\n\nTypo: \nTable table 4 in Appendix C.\nDecision map in Figure 7."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4860/Reviewer_ePoq"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4860/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698760530276,
        "cdate": 1698760530276,
        "tmdate": 1699636469403,
        "mdate": 1699636469403,
        "license": "CC BY 4.0",
        "version": 2
    }
]