[
    {
        "id": "qWQSktq8Jh",
        "forum": "5jxtlpla15",
        "replyto": "5jxtlpla15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_FmYQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_FmYQ"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel method for open vocabulary segmentation. Without the need for training,  it leverages diffusion models to generate examples for the categories uses the clip/dino to extract prototypes, and uses the prototypes to make segmetation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This work is novel and interesting.  It provides a new idea to tackle open vocabulary segmentation. \n2. The motivation is clear writing is easy to understand. \n3. Thanks to the generalization ability of SD, CLIP, and DINO models, the proposed methods show a strong generalization ability for \"zero-shot\" tasks."
            },
            "weaknesses": {
                "value": "1. The definition of \"zero-shot\". As the authors use diffusion models to generate images for the potential categories, I suggest the authors not claim \"zero-shot\". Because using SD to generate images is somehow equivalent to collecting the target images from the internet. The categories are no longer \"unseen\".  From my perspective, \"open-vocabulary\" is acceptable but \"zero-shot\" is not. \n\n2. Burdens of generating a large support set.  Although this work does not require training, generating and storing the support set might be heavy when the category list becomes large."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697894136903,
        "cdate": 1697894136903,
        "tmdate": 1699636592202,
        "mdate": 1699636592202,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xk7umRDUcU",
        "forum": "5jxtlpla15",
        "replyto": "5jxtlpla15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_zXW9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_zXW9"
        ],
        "content": {
            "summary": {
                "value": "The paper describes OVDiff, a model that uses text-to-image diffusion models for open-vocabulary segmentation.\n\nThe basic approach is to: \na) use text queries with a text-to-image model to produce sample images that constitute a support set.\nb) unsupervised instance segmentation is used (e.g., CutLER) along with cross-attention maps of the diffusion model to distinguish between foreground and background in the support set images.\nc) from the support set, prototypes are learned for the class, instance and parts. Both the object and the background are used for positive and negative prototypes.\nd) finally, a segmentation map is obtained by comparing dense image feature to prototypes using cosine similarity.\n\nExperiments are performed with several image encoders; DINO, MAE, SD (stable diffusion) and CLIP, and on several datasets; PASCAL VOC, Pascal Context, and COCO-object. Several ablations are performed, e.g., combining image features outperforms any individual feature."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Overall the paper is well written and clearly lays out the approach and the experiments. \n\nThe proposed method relies on off-the-shelf pretrained components, and it is relatively straightforward. \n\nExperiments explore a few interesting ablations, e.g., the contribution from the background components, the distinction between stuff and things, etc."
            },
            "weaknesses": {
                "value": "Several related missing references: \nLearning to Detect and Segment for Open Vocabulary Object Detection, T. Wang, N. Li, CVPR 2023\nSemantic-SAM: Segment and Recognize Anything at Any Granularity, Feng Li et al., https://arxiv.org/pdf/2307.04767.pdf\n\nThe paper does not describe results on LVIS, commonly used for open set segmentation. \n\nIn distinguishing between stuff and things, the authors describe asking ChatGPT. It was unclear to me whether this was necessary for the paper as it was a relatively small number of classes and the results contained some errors. It's possible that better prompting could have produced more accurate results. \n\nThe results shown in Fig. 5 show a few issues w/ OVDiff; e.g., small false positive \"corgi\" patches, issues w/ the donut image, a small false positive patch of \"bus\". \n\nEditing Comments\np. 6: As the approach does note require --> As the approach does not require\np. 8: Though sometimes region --> Though sometimes the region?\np. 8: not fully align with whole --> not fully align with the whole?"
            },
            "questions": {
                "value": "a) Please clarify the overall novelty and contribution of the paper.\nb) In the text-to-image methodology, it seems that only single-class prompts are used (e.g., \"a good picture of a cat\" or \"a good picture of a dog\") rather than more complex queries that could provide more shape information when segmenting. Does this limitation impact performance?\nc) Comment on whether it would be useful to benchmark on LVIS?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817141732,
        "cdate": 1698817141732,
        "tmdate": 1699636592094,
        "mdate": 1699636592094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mJzuk1Zgta",
        "forum": "5jxtlpla15",
        "replyto": "5jxtlpla15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_kQ2p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_kQ2p"
        ],
        "content": {
            "summary": {
                "value": "This work proposes to leverage the generative text-to-image diffusion models to enhance open-vocabulary segmentation. The proposed method OVDiff synthesizes support image sets from category names and collect the representative prototypes for each category. The segmentation is performed by comparing a target image with the prototypes."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The proposed idea of using text-to-image generated samples as support set  to perform the image-image feature comparison seems novel.\n* OVDiff achieves the state-of-the-art on VOC, Context and Object benchmarks.\n* OVDiff also exhibits reasonable segmentation on the in-the-wild examples."
            },
            "weaknesses": {
                "value": "* The background segmentation requires a pre-computation and the use of external module CutLER (Wang et al. 2023). \n\n* It seems requiring a careful curation and parameter control to achieve the accurate foreground/background segmentation and to collect the good representative prototypes.\n\n* As the synthesized images are mostly object-centric, it is not clear whether the method can still work on large images with multiple fine-grained objects. \n\n* When evaluated on the context-59 and ADE-150 datasets with more fine-grained objects, OVDiff performs worse than some of the recent SOTA methods.\n\n* While running speed is not a main benchmark in open-vocabulary segmentation, the proposed pipeline of image synthesis, prototype collection, background computation clustering seems involving quite a bit of computation."
            },
            "questions": {
                "value": "* Would OVDiff run faster or slower than the SOTA methods?\n* Please see Weaknesses section for other questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698961352520,
        "cdate": 1698961352520,
        "tmdate": 1699636591932,
        "mdate": 1699636591932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LNM5IijvvM",
        "forum": "5jxtlpla15",
        "replyto": "5jxtlpla15",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
        ],
        "content": {
            "summary": {
                "value": "This paper present OVDiff, a novel method that leverages the generative properties of text-to-image diffusion models for open-vocabulary segmentation. The proposed method shows good results on PASCAL VOC."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method achieve SOTA performance on challenging benchmarks\n2. Figures in the paper are clear and easy to follow"
            },
            "weaknesses": {
                "value": "1. The paper mentions the use of diffusion to generate images and extract the corresponding feature prototypes. However, this approach may introduce bias due to the potentially limited diversity of the generated images, leading to biased results. Generating a larger number of images to address this issue would result in a significant increase in time, as creating a single image with diffusion methods is time-consuming, requiring at least 2-3 seconds even when accelerated by methods like Denoising Diffusion Implicit Models (DDIM).\n\n2. The core insight of your study is not immediately clear. Could you succinctly summarize the key findings and the experimental evidence that supports them? The method section could be simplified for better readability; currently, the intertwining of motivation within the methodological steps detracts from a clear understanding.\nFurthermore, there is a discrepancy between the subtitles in Figure 1 and the corresponding method section headings, which disrupts the flow for the reader. The methods section itself seems overly intricate, resembling a layered application of preexisting large models and techniques from other studies, which dilutes the novelty of your work.\n\n3. I am concerned about the complexity of the process and would like to know detailed information on the time required to process a single image, including the computational costs needed for the entire procedure from image generation to final segmentation results. Additionally, how does this compare to other methods? A report on the processing times for alternative methods is also necessary for a thorough comparison."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5674/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5674/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5674/Reviewer_PKyW"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5674/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699471032125,
        "cdate": 1699471032125,
        "tmdate": 1699636591818,
        "mdate": 1699636591818,
        "license": "CC BY 4.0",
        "version": 2
    }
]