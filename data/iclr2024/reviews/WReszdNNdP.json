[
    {
        "id": "gG8k10bxSi",
        "forum": "WReszdNNdP",
        "replyto": "WReszdNNdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_wS2P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_wS2P"
        ],
        "content": {
            "summary": {
                "value": "The manuscript delineates a simple method, termed as BOWLL, which is devised as a baseline for the evaluation of open world lifelong learning - a conjunction of open-set recognition, active learning, and continual learning. The BOWLL exhibits an innovative usage of the Batch Normalization layer - a commonly-used component of neural networks,  along with Out-of-Distribution Detection module, Active Query module, Memory Buffer, and Pseudo Data that endow the proposed method with competitive performance for open world lifelong learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S1. The paper's significance is underscored by its motivation to facilitate future research in this domain.\n\nS2. The BOWLL model's novelty is encapsulated in its innovative usage of the Batch Normalization layer, along with Out-of-Distribution Detection module, Active Query module, Memory Buffer, and Pseudo Data.\n\nS3. The evaluation is comprehensive, with comparisons to baseline methods and ablation study providing a compelling demonstration of the promising performance of the BOWLL method.\n\nS4. The clarity of the manuscript enhances accessibility for readers, facilitating a straightforward understanding of the proposed approach."
            },
            "weaknesses": {
                "value": "W1.  Although the paper provides a comprehensive explanation of the methodology, further technical insights regarding the implementation and each module within the BOWLL method would be beneficial.\n\nW2. The paper falls short in providing a detailed analysis of the limitations of the proposed BOWLL, a factor which could be significant for future research and practical applications.\n\nW3. The computational complexity of the BOWLL algorithm, especially for those selection and replacement strategies, which could be a concern for large-scale datasets or practical applications, is not discussed in the manuscript.\n\nW4.  Although the paper employs sound-good methodology and achieves competitive performance,  further efforts regarding the technical innovation and methodological novelty would be beneficial.\n\nW5. The manuscript could delve deeper into the Continual Train Step, a factor which could be pivotal for understanding the pipeline of open-world lifelong learning.\n\nW6. A more detailed exposition of the datasets used in the evaluation, including their characteristics and potential biases, would enrich the manuscript."
            },
            "questions": {
                "value": "C1. How does the method balance the data in the memory buffer and pseudo-images?\n\nC2. What is the formulation of $R_{TV}()$ and $R_{l_2}()$ respectively in Eq. (7)?\n\nC3. What is the meaning of $\\beta$ in the evaluation metric LCA?\n\nC4. Haven't the model used those discarded data?\n\nC5. What is the relationship between open-world learning and open-world lifelong learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698607276210,
        "cdate": 1698607276210,
        "tmdate": 1699636048163,
        "mdate": 1699636048163,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "2zCbss5tJW",
        "forum": "WReszdNNdP",
        "replyto": "WReszdNNdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses open-world continual learning, an emerging research area, and suggests leveraging Bayesian Network statistics to enhance various phases of open-world learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Applying BN statistics for OOD detection, active learning, and continual learning is a novel and unified approach.\n2. The significance of the problem is notable.\n3. It surpasses a strong baseline, GDUMB."
            },
            "weaknesses": {
                "value": "I believe this paper might overstate its contributions for the following reasons:\n\n1. It seems to focus solely on the class-incremental learning scenario in continual learning, despite claiming to address various types of continual learning settings. How about, for example, Task-incremental learning [1]? \n\n2. The paper claims that BOWLL can achieve OOD detection, active learning, and continual learning, but I only see a comparison in final and LCA performance in table 2. This falls short of adequately demonstrating the model's superiority in all three objectives.\n\n[1]: Continual learning of a mixed sequence of similar and dissimilar tasks. Ke et al., NeurIPS 2020"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1218/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1218/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698628897539,
        "cdate": 1698628897539,
        "tmdate": 1699636048095,
        "mdate": 1699636048095,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TJySpF3Ay8",
        "forum": "WReszdNNdP",
        "replyto": "WReszdNNdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_BmgJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_BmgJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a baseline method for open-world lifelong methods that relies on the idea of batch normalization. The method relies on three important components: 1) detection of out-of-distribution examples using batch-norm statistics, 2) active querying of remaining examples by using batch-norm statistics, 3) continual training using both example replay and generated pseudo-examples that also rely on information coming from batch-norm statistics. Experiments are run on three benchmark datasets, and compared to strategies such as joint learning, funetuning and GDUMB, using metrics such as backward transfer and accuracy."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper intends to tackle the very important problem of open-world lifelong learning by exploiting a simple yet effective strategy of batch-normalization statistics. These statistics are exploited in several parts of the learning process, including discarding OOD examples, actively selecting most effective examples, and actually learning from these selected examples in a continual learning setting. \n- Experiments show that the proposed baseline is quite competitive, in particular in terms of backward transfer (Table 2)\n- The paper is well-written and easy to follow. The components of the solution are clearly explained, and the diagram in Fig. 1 is very self-explanatory."
            },
            "weaknesses": {
                "value": "- The main weakness that I see in the paper is the limitation of the experiments. I would have expected more robust experiments in more varied datasets, and a larger number of datasets and tasks. \n- Similarly, I would have expected more comparisons with other SOTA methods that, although not originally open-world learning, perhaps could be slightly modified for the sake of comparison."
            },
            "questions": {
                "value": "- Table 2 shows quite a remarkable good performance of the proposed method in the case of backward transfer, which is a very challenging problem in continual learning, and is difficult to achieve. Could you provide more insights as to why this would be the case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698718424796,
        "cdate": 1698718424796,
        "tmdate": 1699636048005,
        "mdate": 1699636048005,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0UDvLJHIoV",
        "forum": "WReszdNNdP",
        "replyto": "WReszdNNdP",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_ELi2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1218/Reviewer_ELi2"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors introduce the first monolithic baseline for open world lifelong learning, which remedies the lack of well suited baselines for evaluation. Particularly, the simple batch normalization technique is repurposed for 3 subtasks in lifelong learning: open-set recognition, active learning and continual learning. Through extensive empirical evaluation, the resulting approach proves simple yet highly effective to maintain past knowledge, selectively focus on informative data, and accelerate future learning. The proposed method also compares favorably to other related baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- A simple and reliable baseline is always valuable, especially for the less-studied open world lifelong learning area. The method seems competitive on the benchmarked datasets.\n- The unified use of the batch norm statistics for the 3 components in lifelong learning is interesting and promising. The ablation in Table 3 of the appendix is nice, indicating the involved components are indispensable."
            },
            "weaknesses": {
                "value": "- One main concern of this paper is the missing analysis for some components of the proposed lifelong learner (see questions below)."
            },
            "questions": {
                "value": "- The image synthesis method based on Deep Inversion seems interesting. All it's doing is to generate class-conditioned pseudo-images using past representations (the running mean and variance from the batch normalization layers). How much cost will such image synthesis incur? How faithful are the generated images? Why not opt for feature synthesis which seems natural and efficient given the maintained feature mean and variance?\n- For active query, the acquisition function is designed using entropy weighted with sample similarity. How important is such weighting? Is this the best way to strike a good tradeoff between exploration and similarity? Any other formulations for ablation/comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777037458,
        "cdate": 1698777037458,
        "tmdate": 1699636047935,
        "mdate": 1699636047935,
        "license": "CC BY 4.0",
        "version": 2
    }
]