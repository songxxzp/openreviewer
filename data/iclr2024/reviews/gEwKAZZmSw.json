[
    {
        "id": "NLCBbtuxhP",
        "forum": "gEwKAZZmSw",
        "replyto": "gEwKAZZmSw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_PsQT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_PsQT"
        ],
        "content": {
            "summary": {
                "value": "This manuscript proposes a sampling algorithm to eliminate computation during forward and/or BP. More specifically, a variance-controlled adaptive sampling (VCAS) method is designed to accelerate BP by computing unbiased stochastic gradients.\n\nThe effectiveness of the VCAS is justified by pre-training and fine-tuning tasks in both vision and NLP domains. Some ablation studies are also included to discuss the effects of hyper-parameters."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* This manuscript is well-structured, with a clearly explained methodology section.\n* The manuscript evaluates the effectiveness of the VCAS on pre-training and fine-tuning tasks in both vision and NLP domains.\n* An ablation study is also included."
            },
            "weaknesses": {
                "value": "1. Limited literature review. This manuscript did not carefully examine the relevant papers, and some closely related papers were omitted from the discussion, e.g., [1, 2, 3] and the related work in [1].\n2. Limited baseline evaluations. Close baseline methods e.g., [2, 3] should be considered.\n3. Additional hyper-parameters and insufficient ablation studies. The ablation study on one task, i.e., fine-tuning BERT-base on MNLI, is insufficient to justify the insensitivity of these hyper-parameters.\n4. Clarity. Some design choices in Section 5 are just given and have no explanation. For example, how to derive the provided equation from the mentioned zeroth-order algorithm? \n5. The claim on the unbiased stochastic gradient needs to be more careful and some theoretical justifications should be provided.\n\n### Reference\n1. SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks at the Edge\n2. Back Razor: Memory-Efficient Transfer Learning by Self-Sparsified Backpropagation\n3. Sparse weight activation training"
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Reviewer_PsQT"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681861080,
        "cdate": 1698681861080,
        "tmdate": 1700631974504,
        "mdate": 1700631974504,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "0Fum3cRgiS",
        "forum": "gEwKAZZmSw",
        "replyto": "gEwKAZZmSw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_Dd1x"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_Dd1x"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a variance-controlled adaptive sampling (VCAS) method for accelerating the back-propagation of deep neural network training. VCAS computes unbiased, variance controlled gradients for both activations and network weights. By sampling both data samples and tokens in each datum in a layer-wise, fine-grained manner, VCAS can drastically reduce the computation in the back-propagation process without introducing too much variance overhead. With the similar FLOPs reduction, VCAS better optimizes the target model compared with prior loss-based and gradient-based sampling methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This work introduces a fine-grained strategy that 1) increasingly removes data samples when back-propagating to the input layer, and 2) samples tokens in each data sample when computing gradients for network weights. This fine-grained strategy allows high FLOPs reduction with controlled variance.\n\n- The sampling ratios are adaptively adjusted according to the estimated variance. As training progresses, the network may require changing sampling ratios, and the adaptive sampling ratios can better satisfy this need.\n\n- Compared with prior methods, the proposed method, VCAS, can better simulate exact back-propagation, leading to better optimized loss and better evaluation performance."
            },
            "weaknesses": {
                "value": "- Training time reduction: When comparing with baselines, this work uses FLOPs as the efficiency indicator. However, FLOP reduction may not directly translate into wall-clock time reduction due to various factors like parallel computation efficiency. It is suggested to also list the wall-clock training time of each method for a more straightforward comparison.\n\n- Insights on sampling ratio updates: In Section 7 this work has discussed the design choices that determine the sampling ratio updates. For better comprehension, it may be useful to include a figure that shows how $s$ and $\\nu_l$ changes as training progresses.\n\n- Figure/Table clarity: Figure 2 seems to lack some more detailed explanation. In Table 1, it is not clear which numbers should be bolded. For example, for ViT-base fine-tuning on CIFAR-100, UB seems to be highlighted for the highest eval accuracy, but for ViT-large fine-tuning on CIFAR-100, UB seems to be highlighted for the lowest train loss? Also for Table 1, how significant is the relative improvement over baselines?\n\n- Limitations: It is suggested to include some detailed discussion on the limitations (e.g., applicable model architectures, base optimizer, dataset) of the proposed method. In this paper, only Transformer-based architectures and Adam-like optimization algorithms are tested. It is not clear whether we can extrapolate the conclusion to other settings."
            },
            "questions": {
                "value": "- It is not directly clear to me whether the weight gradient sampling of VCAS is applicable to convolution neural networks (CNN). In principle, convolution is yet another linear operator, but I\u2019m not sure how to perform this sampling in a convolutional layer. Similarly, can VCAS be applied when optimizing a recurrent neural network (RNN)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Reviewer_Dd1x"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698701468856,
        "cdate": 1698701468856,
        "tmdate": 1700355449216,
        "mdate": 1700355449216,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8AW0IQm6oN",
        "forum": "gEwKAZZmSw",
        "replyto": "gEwKAZZmSw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_d6vC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_d6vC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes Variance-Controlled Adaptive Sampling (VCAS), which performs an approximated stochastic gradient with an adaptive sampling rate. Based on the insight that gradients are sparse after learning has progressed to some extent, the authors improve the efficiency of learning by computing only a few selected gradients through adaptive sampling. The proposed method approximates the exact backpropagation values well in BERT and ViT training."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "VCAS performs backpropagation 20-50% faster, while following the loss curve of true full backpropagation with low variance. VCAS has an adaptive sampling rate, which allows for efficient sample selection based on learning loss and per layer. The idea is simple and highly applicable."
            },
            "weaknesses": {
                "value": "Comparing accuracy under the same amount of FLOPs reduction makes it difficult to understand its effectiveness compared to a metric like time to reach target accuracy[1]. As a result, it is unknown how VCAS will perform under a 50% or greater reduction.\n\n\n[1] Mindermann, S., Brauner, J. M., Razzak, M. T., Sharma, M., Kirsch, A., Xu, W., ... & Gal, Y. (2022, June). Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning (pp. 15630-15649). PMLR."
            },
            "questions": {
                "value": "I would like to see more detail in Table 2 or 3. What is the relationship between Final Eval Accuracy and FLOPs reduction? For example, is the recommending FLOPs reduction ratio for VCAS around 40%?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Reviewer_d6vC"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698762855504,
        "cdate": 1698762855504,
        "tmdate": 1699636274024,
        "mdate": 1699636274024,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bcgPtXnW8T",
        "forum": "gEwKAZZmSw",
        "replyto": "gEwKAZZmSw",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_MWbo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3259/Reviewer_MWbo"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a sampling method for back propagation with controlled variance and self-adaptive sample ratios, named VCAS. It computes an approximate stochastic gradient by applying finegrained sampling to gradually remove samples and tokens during backpropagation. VCAS have similar variance as well accuracy with exact back propagation, while seems to reduce the training cost significantly."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. I like it very much that the authors test the proposed algorithm on multiple fine-tuning and pre-training tasks in both vision and\nnatural language domains. Typically, papers in this domain would use small scale datasets such as MNIST or CIFAR 10/100.\n\n2. The ablation studies on a few hyperparameters are very important. I see only very few of the papers in this domain have done this before."
            },
            "weaknesses": {
                "value": "I think the authors can improve the paper in the following ways:\n\n1. I believe adding an algorithm table with detailed steps would make the paper more clear. \n\n2. The authors report the Final Train Loss / Final Eval Acc.(%) / FLOPs reduction ratio(%). However, I'd like to know the actual reduction in training time as these sampling methods might introduce overhead in computation. It would be helpful if the authors can report a time table for training on these datasets.\n\nTo be frank, I feel not many papers actually do this but it can be interesting to see that the actual training time might not be reduced at all, or at least not much as expected given a certain sampling ratio.\n\n3. The paper lacks discussions of related paper. For example, https://arxiv.org/pdf/2104.13114.pdf also considers the importance sampling problem by sampling data points proportionally to the loss, instead of norm of gradient.\n\nFor another example, https://arxiv.org/pdf/2306.10728.pdf also proposes adaptively sampling methods for dynamically selecting data points for mini-batch. I'd love to see the authors discussed more about these papers.\n\n4. Can the authors be more specific in terms of the notations? Adding a table of notation would be very helpful. For example, what is $h^{(l)}$ below:\n\n$$\\nabla_{Z^{(l-1)}}=h^{(l)}\\left(\\nabla_{Z^{(l)}} ; Z^{(l-1)}, \\theta^{(l)}\\right)$$"
            },
            "questions": {
                "value": "1. Although the proposed VCAS algorithm seems promising compared with SB and UB, I'd like to know the actual reduction in training time as these sampling methods might introduce overhead in computation. It would be helpful if the authors can report a time table for training on these datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3259/Reviewer_MWbo"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3259/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699314410582,
        "cdate": 1699314410582,
        "tmdate": 1700620704572,
        "mdate": 1700620704572,
        "license": "CC BY 4.0",
        "version": 2
    }
]