[
    {
        "id": "HxT2Arxtsy",
        "forum": "PbpJnyewVM",
        "replyto": "PbpJnyewVM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_gYxn"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_gYxn"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces Preference Optimal Transport (POT), which aims to establish a correspondence between the same human preference in different reinforcement learning tasks. It solves a question: if a human user prefers trajectory x in the source domain, what trajectory y would the same user prefer in the target domain? Specifically, POT aligns two sets of trajectories by solving an optimal transport matrix under Gromov-Wasserstein distance. POT also incorporates uncertainty in human preferences by using distributional rewards."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Clear identification of the problem (user preference transfer between RL task domains)\n2. Clear presentation of the POT algorithm."
            },
            "weaknesses": {
                "value": "In Table 1, we can see that scripted labels are in general providing better results than transferred labels, which is expected. Therefore, transferred labels should only be used as a substitute when scripted labels are expensive. The paper has yet to discuss applicable scenarios of using transferred labels - when should we compromise success rate for cheaper labels?"
            },
            "questions": {
                "value": "Please respond to the weakness above. If is addressed appropriately, I am willing to improve my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612392204,
        "cdate": 1698612392204,
        "tmdate": 1699636573629,
        "mdate": 1699636573629,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NNxjggRUWD",
        "forum": "PbpJnyewVM",
        "replyto": "PbpJnyewVM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_43na"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_43na"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an algorithm for zero-shot preference-based RL, aiming to address the human preference-guided RL challenge in a transfer learning context. Specifically, the authors utilize optimal transport theory to transfer human preference labels from one RL task to another. This transfer procedure involves computing a coupling matrix using the Gromov-Wasserstein distance, and this matrix yields the transferred preference labels. Additionally, the reward function is modeled as a distribution, facilitating learning from noisy labels and is refined using a process that employs Gaussian distributions instead of scalar rewards. The proposed approach is assessed on robot manipulation tasks in the Meta-World and Robomimic environments. Comparative experiments against baselines and thorough ablation studies further validate the design decisions presented in the paper."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Overall, this paper is well-written, and the proposed idea is presented comprehensively.\n- The method proposed uses the Gromov-Wasserstein distance to learn a coupling and subsequently transfers the preference label. This approach allows for the creation of a reward function for a target task using preference labels from source tasks, without the need for preference labels specific to the target task.\n- The authors have enhanced the robustness of the RL objective by introducing Gaussian noise.\n- Comprehensive experimental results are provided to validate the proposed method."
            },
            "weaknesses": {
                "value": "- The contribution could be the weakness of this work. It seems that the proposed method is rather incremental since the authors incorporate the original preference-based reinforcement with off-the-shelf Gromov-Wasserstein distance method. In addition, the uncertainty module also utilizes the existing reparameterization trick. Since the Gromov-Wasserstein distance is a concrete way to measure the difference between different distributions, is it possible for the authors can provide generalization bounds (even in the toy example)? \n- Some RL tasks can be difficult to generalize. The authors haven't clarified how their optimal transport technique might be effective across such different scenarios. \n- The authors may better justify the usage of preference in RL settings. The simulation environments used in this work are not designed to validate human preference, they are more suitable for goal-conditioned RL. The RL task displayed in this work are reply on the end-effector of the robot arm. The authors may want to clarify what is the underlying benefit of transferring human performance.\n- It seems that the total number of trajectories is 4, and the authors use a K-means clustering to separate them further. I am wondering about the necessity of doing this. Are you labeling the preference among clusters or within clusters?"
            },
            "questions": {
                "value": "- Task Similarity: It appears that different tasks represent variations of the same RL task, with only differing goals. This seems a restrictive setting. Can the authors discuss the method's versatility?\n\n- Preference Matrix Clarification: Including the original preference matrix would enhance comprehension. Consider the source samples matrix:\n\u200b[[/, 1, 0], [0, /, 0], [1, 1, /]], and using this and the coupling matrix: [[1/6, 1/6, 0], [1/6, 1/6, 0], [0, 0, 1/3]]. Could the authors detail the preference transfer?\n\n- Preference Transfer Properties: What are the inherent properties of the proposed method? A deeper analysis using the aforementioned matrices could shed light on this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733570748,
        "cdate": 1698733570748,
        "tmdate": 1699636573528,
        "mdate": 1699636573528,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zICPcIQAf4",
        "forum": "PbpJnyewVM",
        "replyto": "PbpJnyewVM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_Lako"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_Lako"
        ],
        "content": {
            "summary": {
                "value": "In Preference-based Reinforcement Learning, matching rewards with human intentions typically demands a significant amount of labels provided by humans. Moreover, the costly preference data from previous tasks often isn't reusable for future tasks, leading to repeated labeling processes. This paper introduces a zero-shot cross-task preference-based RL method that employs preference labels from labeled data to deduce labels for other data, thus bypassing the need for additional human input. The authors employ the Gromov-Wasserstein distance to map trajectory distributions across tasks. Yet, relying solely on these transferred labels could lead to potentially imprecise reward functions. Addressing this, we present the Robust Preference Transformer. It estimates both the average and variance of rewards by representing them as Gaussian distributions. The proposed methodology, when tested on Meta-World and Robomimic, demonstrates superior performance compared with baselines."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "I don't feel the simple addition of labels can work. Let me provide a toy example. If the authors pointed out my error, I would be glad to change my score.\n\nAssume the pair-wise labels of source $\\{x_1, x_2, x_3\\}$ is $Z_s = \\[\\[/, 0, 0\\], \\[1, /, 1\\], \\[1, 0, /\\]\\]$.\n\nLet's assume the target is exactly the same as the source, i.e., $\\{y_1 = x_1, y_2 = x_2, y_3 = x_3\\}$. Then the transportation matrix is $\\boldsymbol T = \\[\\[1/3, 0, 0\\], \\[0, 1/3, 0\\], \\[0, 0, 1/3\\]\\]$\n\nLet's say we want to compute the label of $(y_2, y_3)$\n\n$\\boldsymbol A^{23} = \\[\\[0, 0, 0\\], \\[0, 0, 1/9\\], \\[0, 0, 0\\]\\] $\n\n$z(y_2, y_3)=\\frac{1}{9}z(x_2, x_3)=1/9$, while we know the ground truth is $z(y_2, y_3)=z(x_2, x_3)=1$.\n\nIn addition, some notations are ambiguous, e.g., $\\mathcal S$ for state space and source task at the same time."
            },
            "questions": {
                "value": "* In Figure 3b, why is RPT+POT even better than Oracle PT? If it is due to the variance, error bars should be provided in all figures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811563153,
        "cdate": 1698811563153,
        "tmdate": 1699636573426,
        "mdate": 1699636573426,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UKzhivR27Y",
        "forum": "PbpJnyewVM",
        "replyto": "PbpJnyewVM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_g2DT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5573/Reviewer_g2DT"
        ],
        "content": {
            "summary": {
                "value": "This paper first introduces a zero-shot cross-task transfer algorithm Preference Optimal Transport (POT) for preference-based offline reinforcement learning. The trajectories between the source and target tasks are aligned via the optimal transport method and generates pseudo preference labels based on the alignment matrix. Additionally, the paper introduces the Robust Preference Transformer (RPT) to model the uncertainty of preference labels, enabling robust learning in the presence of transfer noise. Experimental results demonstrate that the proposed algorithm exhibits significant advantages in both zero-shot and few-shot preference."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper addresses a highly meaningful scenario that could have a positive impact in practical applications. \n\n2. The writing logic of the paper is very clear, and it is almost devoid of difficulty in understanding."
            },
            "weaknesses": {
                "value": "1. There are issues with the problem formulation in the paper. The assumption of identical action space alone is not sufficient to guarantee the alignment of trajectories and preference labels between source and target tasks. The fundamental reasons for the success of Preference Optimal Transport (POT) are not adequately explained.\n2. Transferring the preference labels from the source task to the target task undoubtedly involves negative noise or uncertainty. This problem becomes even more severe in the context of zero-shot learning, where there is no corrective information available. While the paper acknowledges modeling uncertainty as variance, it doesn't eliminate this negative impact on the downstream tasks, which is unreasonable. On the contrary, the success of the target task appears to depend on such uncertainty since RPT+POT > PT+POT."
            },
            "questions": {
                "value": "1. The paper proposes that uncertainty should approach a predefined value $\\mu$ during training. What will happen if the uncertainty is directly set to $\\mu$ without training?\n2. Unclear expression\uff1a$\\mathcal{S}$ represents state space and source task simultaneously and the expression of $\\mathcal{T}$ is also a little confusing in **problem setting**; PT+Dis and PT+Sim in experiments.\n3. What is value of $u_i$ and $v_j$ and does the calculation of $A$ need the value of $u_i,v_j$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5573/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825281138,
        "cdate": 1698825281138,
        "tmdate": 1699636573309,
        "mdate": 1699636573309,
        "license": "CC BY 4.0",
        "version": 2
    }
]