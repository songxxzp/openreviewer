[
    {
        "id": "dDQQ2oPhKs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_YEgK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_YEgK"
        ],
        "forum": "MiMxv6ijvC",
        "replyto": "MiMxv6ijvC",
        "content": {
            "summary": {
                "value": "The authors proposes an architecture for computer vision: a self-attention layer on top of a convolutional network. They claim this is novel, and evaluate on MNIST, Fashion MNIST, STL-10 and Tiny-ImageNet. Their network performs roughly on par with a typical ResNet (better on STL, worse on all other datasets), but has less parameters than a ResNet 50."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "**Originality**: Putting a Self-Attention Layer on top of a CNN is not novel, and has been proposed many times before (e.g. even in the original Vision Transformer )\n\n**Quality**: The empirical work is poor: Previous work on the same topic is not cited, some prior work is mis-attributed. While the work claims to be targeted towards low-data regimes, it compares only to baselines known to be data-hungry. It does not compare to comparable architectures. There are no error bars even for small scale experiments.\n\n**Clarity:** The work leaves open many questions: the manuscript not explain deviations from standard designs, and talks more about datasets that are well known in the community instead of architectural decisions. Their main contribution is described in a single line short sentence (see \"Weaknesses\" below).  Numbers in the results are misrepresented to be significant even though they are actually worse.\n\n**Significance:** As far as can be seen from the empirical results, the work is of minor significance."
            },
            "weaknesses": {
                "value": "Novelty: Variants of this architecture have been proposed many times over. See e.g. the \"Related Work\" section in the Vision Transformer paper by Dosovitskiy et al. 2021 for some links\n\nMisleading presentations: Table 1 has all of the results for the proposed method boldened. This is usually done to indicate significantly better results, or at the very least BETTER results as comparable methods. The method proposed here is oftentimes performing worse than the competitors, and the use of boldface is misleading, as it is not in line with the norms of the community. \n\nClarity: The architectural decisions are extremely unclear. No time is spent to give intutions, or even ablations for the concrete choices made. For example:\n* The bottleneck blocks used in this work seem fairly similar to ResNet Bottlenet blocks, but have an additional Convolution at the end. No explanation is given for this, it would be nice to have one.\n* CAReNeT attention is merely described as \"an attention mechanism operating in parallel across both grid and window patterns\". I do not understand what that means -- What is a window pattern, what does it mean to be parallel across grids and windows? This is the paper's main contribution, so a lot more explanation should be devoted to it.\n* The \"Residual Blocks\" in the Figure 1 seem to always be downsampling. Why is that? It would be nice if Figure 1 could also point out intermediate resolutions to better understand when & by how much the resolution is reduced.\n\nSignificance: Variations of this model were proposed many times before. It is unclear if there is anything special about this version. The experiments do not compare to similar architectures from the literature (again: see the ViT paper for references), so it's unclear if it performs better than those. Table 1 show nice results on STL-10, but the paper does not investigate why this is. Why does CAReNeT perform so well here? Is this due to attention, or due to something else in the architecture? What other applications could benefit from this?\n\nSmaller remarks:\n\n\"The fusion of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) Khan et al. (2022) presents a groundbreaking approach to image classification\"   ==> Khan et al., did not introduce ViT's, nor the fusion of CNNs and ViTs. Both where done by A. Dosovijtsky et al., (2021) (or references therein), please correct this citation."
            },
            "questions": {
                "value": "I have no questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7399/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697452588658,
        "cdate": 1697452588658,
        "tmdate": 1699636886669,
        "mdate": 1699636886669,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EqBFf5WbU3",
        "forum": "MiMxv6ijvC",
        "replyto": "MiMxv6ijvC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_qugU"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_qugU"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new network design named CAReNet that incorporates convolutional layers, attention mechanisms,, and residual connections. They focus on network-specific training strategies and the conducted experiment results show the relative improvements to some modern network designs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors carefully review the different elements of the proposed network and reassemble them into a more compact network.\n2. The experiments on STL10, Mnist, Tiny-ImageNet demonstrates the effectiveness and its efficiency in terms of parameters."
            },
            "weaknesses": {
                "value": "1.Limited novelty. While the paper brings forward a new model design, its foundation lies in leveraging existing techniques. As a result, its contribution is incremental and not enough for establishing a distinct and well-designed model architecture.\n\n2.The experiment comparisons are not convincing enough. The performances are similar to ResNet18 on most datasets and does not show significant superiority. Moreover, the compared methods do not include more modern architectures, such as Swin-T [1], ResNeXt [2], ConvNext [3]. \n\n3.Scaling potential. To comprehensively evaluate the network performance, the experiments should be included to show it can be extended to large-scale dataset, such as ImageNet-1k. Demonstrations of its adaptability to broader downstream vision tasks would also be beneficial.\n\n4.Lacking ablations. The paper could be further strengthened with an ablation study. This would offer clarity on how specific components of the proposed design contribute to performance enhancements.\n\n[1] Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\"\u00a0Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[2] Xie, Saining, et al. \"Aggregated residual transformations for deep neural networks.\"\u00a0Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[3] Liu, Zhuang, et al. \"A convnet for the 2020s.\"\u00a0Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
            },
            "questions": {
                "value": "Please consider to answer the above questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7399/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698653781371,
        "cdate": 1698653781371,
        "tmdate": 1699636886529,
        "mdate": 1699636886529,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "riYpw1QQPh",
        "forum": "MiMxv6ijvC",
        "replyto": "MiMxv6ijvC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_CQ8z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_CQ8z"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new architecture named CARENet for the tasks of image classification in the low-data regime. They propose an architecture and claim that their architecture design leads to smaller models which are parameter efficient, and perform well when trained from scratch on datasets in the low-data regime."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The authors have well-framed and well-motivated their problem in the Introduction of making models that work well for low-data regimes. They also partially well-motivate how their problem formulations are important as opposed to pre-training models on large datasets."
            },
            "weaknesses": {
                "value": "- The authors mention that:\n\n> While these Transformer-based models have shown remarkable promise, they are not without their\nchallenges, particularly when it comes to computational efficiency and the ability to scale. \n\nI believe that the authors start talking about addressing a different problem than what they propose in the introduction. We know that ConvNets scale well especially shown with BiT [1] and other works however it has also been shown that Transformer-based models are comparatively easier to scale, and more memory memory-efficient [2] (especially see Figure 12). Transformer-based models do have their challenges and the authors identify most of them well, however, I think the way they start talking about these problems is a weakness or if this is what they meant, they should also reflect this in their experiments.\n\n- A huge weakness of this paper is that the authors only present a new architecture: this new architecture uses fairly popular and standard methods of building architecture and is not something novel at all. They also introduce a CARENet block which is comprised of a bottleneck block followed by an attention mechanism operating in parallel across both grid and window patterns, however, this is also rather standard, and (shifted) window/local attention has been immensely popularized by [3], and approaches similar to their CARENet block have been used multiple times, popularly in [4]. I believe the construction of their architecture or their method itself is not novel.\n\n- The authors loosely mention these aspects while explaining their architecture:\n\n>  offering enhanced feature extraction capabilities\n\n>  designed to bolster information flow\n\n> empower the model to learn profounder representations\n\n> refine the spatial hierarchy of the feature maps\n\nNot only the benefits they propose are not written down succinctly and clearly but are also not benefits that come with \"their\" work or their way of putting together these architectures, these are rather fairly popular and standard approaches to building models.\n\n- The method itself might not come across as novel or does not present any important theoretical insight in the formulation of the network. In these cases, one might look toward the paper in this case at the very least explain how such a small change should lead to better properties or in general for applying some method to a unique context, and the only way to show this due to lack of the earlier 2, I feel should be results which in this case should be well compared and contrasted with other methods and not leave questions in the mind of a reader. However, the authors do neither of these.\n\n- I thoroughly disagree with the authors on this,\n\n> In this study, we leverage a diverse set of benchmark datasets to rigorously evaluate the performance and robustness of our proposed models.\n\nTheir experiments are not indeed diverse or large in number, they evaluate on STL10, Fashion Mnist, Mnist, and Tiny-Imagenet which is not a diverse set of datasets.\n\n- The authors present that their model is superior or at par with other models trained without extra data however this is not true, and they only reported the performance of some models. For instance, in the case of Tiny Imagenet, there are more than 10 different models [5] trained on the same amount of data that perform better than CARENet but these are not even cited or compared in the paper. Considering this, their results get severely diminished with the lack of clarity around STL-10 results (see questions) and the lack of proper comparisons for Tiny ImageNet. Given this I would also suggest changing,\n\n> The presented results furnish an in-depth comparison among several cutting-edge neural networks\narchitectures\n\nMaybe parameter efficiency could indeed be something you work toward,\n\n> The Tiny-Imagenet dataset results present an interesting paradigm. MaxVit outperforms other architectures with a 58.28% accuracy but CAReNet, with nearly half the model size of MaxVit, closely\nfollows with an accuracy of 54.4%\n\nHowever, still, the results need to be well-explained and compared with SoTA models. The authors should modify or instate a new problem statement if they are indeed trying to work toward parameter efficiency.\n\n- The authors mention that they build a robust architecture,\n\n> Across the board, on datasets such as Fashion Mnist and Mnist, CAReNet maintains an enviable performance, with accuracies nearing 95% and surpassing 99% respectively. Such consistent achievements, despite its smaller model footprint, underscore the robustness of CAReNet\u2019s design.\n\nHowever, the experiments are not diverse enough and do not span multiple tasks or multiple kinds of dataset settings to state that the model design is in fact robust.\n\n- With multiple modern models to the same means,\n\n> Signifying a groundbreaking stride in efficiency\nand performance, CAReNet not only outpaces ResNet50 by achieving a lead of\n2.61% on Tiny-Imagenet\n\nI disagree with the authors belief that an improvement over ResNet50 signifies a \"groundbreaking stride\".\n\n[1] Kolesnikov, Alexander, et al. \"Big transfer (bit): General visual representation learning.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16. Springer International Publishing, 2020.\n\n[2] Smith, Samuel L., et al. \"ConvNets Match Vision Transformers at Scale.\" arXiv preprint arXiv:2310.16764 (2023).\n\n[3] Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[4] Tu, Zhengzhong, et al. \"Maxvit: Multi-axis vision transformer.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2022.\n\n[5] https://paperswithcode.com/sota/image-classification-on-tiny-imagenet-1"
            },
            "questions": {
                "value": "- There seems to be some mistake with the title, \"CARENET : A NOVEL ARCHITECTURE FOR LOW DATA REGIME MIXING CONVOLUTIONS AND ATTENTION **CONFERENCE SUBMISSIONS**\"\n- I would recommend the authors follow the problem that they clearly define for the related works, they include SimCLR in their related works section which does present a solution for having less labeled data however this is not the problem that the authors define. However, the authors do not mention how SimCLR or any self-supervised learning algorithms solve a different problem and that their problem formulation is different than these. I think this could be fixed with a rewrite of related works and organizing it better.\n- I would recommend the authors reorganize their methods section as well, the components they talk about or fairly common components, and more importantly they should not need to go into such depth into each component in the main text especially when an understanding of these components is not crucial for the reader to understand how your method is novel.\n- I do not understand how \"Model size (MB)\" has anything to do with the problem you define and why it would show up in the main table in the paper?\n- Could the authors clarify how they use STL-10, their method indicates that their approach is designed only for labeled data, do they simply use the labeled subset of STL-10 and how are other models compared for STL-10?\n- I assume in Table 1 none of the models use extra data?\n- I could not help but wonder why authors left out CIFAR-100 from their comparisons while it is supposed to be more challenging than MNISt and Fashion-MNIST?\n- I do not think the work shows this aspect\n\n> This work focuses on the importance of architecture-specific training strategies\n\nthere is no talk at all about specific new training strategies or training strategies that are modified for this architecture and there are also no experiments to show this?\n\n### Minor formatting issues:\n\n- These\n\n> including VGG Simonyan & Zisserman (2014) and ResNet He et al.\n(2015)\n\n> Squeeze-and-Excitation (SE) layers Hu et al. (2018)\n\n> Transformers Vaswani et al. (2017)\n\namong others shouldn't use in-text citations.\n\n- The number of parameters should for this scale of models be in millions for readability.\n- There seems to be a typo with \"Tiny-Imagenet (ours)\" which in this context indicates that this paper introduced TI."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7399/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7399/Reviewer_CQ8z",
                    "ICLR.cc/2024/Conference/Submission7399/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7399/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799715200,
        "cdate": 1698799715200,
        "tmdate": 1700008146099,
        "mdate": 1700008146099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oodwZbgcy8",
        "forum": "MiMxv6ijvC",
        "replyto": "MiMxv6ijvC",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_LKri"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7399/Reviewer_LKri"
        ],
        "content": {
            "summary": {
                "value": "The paper presented aims to integrate the concept of a Convolutional ResNet with an Attention network to attain comparable recognition accuracy using fewer parameters. The proposed layer, referred to as CAReNet, is incorporated into the final convolutional block. Specifically, CAReNet utilizes an Attention block in both grid and window configurations to execute the task. The overall performance of the model is compared with that of ResNet, VGG, Max Vit, and CoatNet in terms of top-1 accuracy using small datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "Investigating the convolutional-based attention network might look interesting, but I think the paper is not ready for publication yet. Please see my comments below."
            },
            "weaknesses": {
                "value": "While have utmost respect for the work submitted and I hope my comments will assist the authors in fortifying their paper: \n\n\n-\tThe motivation for integrating Convolution with Attention within the proposed method requires clearer articulation. The current manuscript does not sufficiently convey the intuition or the rationale for this combination. \n\n-\tThe literature review appears to be incomplete, lacking references to several pertinent studies. For instance, a notable omission is the Convolutional vision Transformer (CvT). I recommend the authors expand this section to provide a more comprehensive background. \n\n-\tWhile it is acknowledged that the addition of an attention layer to convolution blocks can expedite convergence in certain image classification tasks, this does not directly demonstrate the efficiency of the proposed method from a parameter-count perspective. A more detailed analysis is required to substantiate the method's effectiveness. \n\n\n-\tThe clarity of the paper's presentation needs improvement. For instance, the captions of figures and tables lack essential details, making it difficult to fully understand the results presented. \n\n-\tThe evaluation of the model on only three small datasets does not provide a robust validation of its capabilities. A more extensive evaluation, including additional and larger datasets, would be more convincing. \n\n\n\n-\tThe current evaluation presented in Table 3 does not convincingly demonstrate the model's effectiveness. For example, the marginal improvement over ResNet18 on the MNIST dataset, despite a higher parameter count, calls into question the practical benefits of the proposed method. \n\n-\tThe paper would benefit greatly from an ablation study, particularly one that investigates the impact of removing the CAReNet block, to discern its actual contribution to the model's performance. \n\n-\tFor work of this nature, it is crucial to assess performance on larger-scale datasets to ensure the model's effectiveness and generalizability. \n\nI recommend addressing these points to provide a more compelling argument for the proposed method and its potential impact on the field."
            },
            "questions": {
                "value": "In the limitations section, numerous questions were raised; however, there still remain some unclear points in the paper. For example, it is quite surprising that VGG16, with ten times the number of parameters, only achieves 10% accuracy on the STL10 dataset. This result is counterintuitive, considering VGG16's established performance on various image recognition tasks. I strongly suggest the authors rigorously investigate this anomaly and discuss whether there might be an error in the reported results or if there are underlying factors that could explain this unexpected outcome. A more detailed explanation would greatly enhance the credibility and scholarly rigor of the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7399/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699254454464,
        "cdate": 1699254454464,
        "tmdate": 1699636886325,
        "mdate": 1699636886325,
        "license": "CC BY 4.0",
        "version": 2
    }
]