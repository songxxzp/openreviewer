[
    {
        "id": "SLV3HyghPl",
        "forum": "LXiG2WqKXR",
        "replyto": "LXiG2WqKXR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_vFed"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_vFed"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on improving the generalization capabilities of text-based reinforcement learning (TBRL) agents through an interactive environment called STARLING. This environment uses large language models (LLMs) like GPT-3 and an interactive fiction game engine (Inform7) to automatically generate text-based pre-trained games. These games help TBRL agents to hone specific skills and improve their performance in various tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-structured and easy to follow."
            },
            "weaknesses": {
                "value": "1. The paper's focus on using LLMs to generate configuration files for a specific game engine may not offer broad insights applicable to other related work.\n\n2. The paper's use of text-based pre-trained games may not generalize well to other forms of reinforcement learning (RL) tasks. Moreover, high-performance LLMs could potentially solve text-based tasks directly, questioning the need for pre-training to acquire skills.\n\n3. The lack of comparative experiments with other pre-training methods makes it difficult to ascertain the advantages of using LLMs for designing pre-trained games."
            },
            "questions": {
                "value": "I have no further questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Reviewer_vFed"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8892/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698304709938,
        "cdate": 1698304709938,
        "tmdate": 1699637118795,
        "mdate": 1699637118795,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "z8I0u2E2Xg",
        "forum": "LXiG2WqKXR",
        "replyto": "LXiG2WqKXR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_2y7E"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_2y7E"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose an LLM-aided method that generates text-based game environments, that are similar to some target domain text-based games, to pre-train RL agents. The authors show that compared to agents trained on target games from scratch, RL agents pre-trained on the generated games could benefit from having a better initialization, either learn to solve games more efficient, or to achieve more scores. \n\nThe authors use three different text-based game suites, namely ScienceWorld, TextWorld Commonsense, and Zork 1. In which, ScienceWorld and TWC consists of synthetically generated games, Zork 1 is human-authored game that is significantly more difficult."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* Interesting topic: I find the overall idea of leveraging large foundation models (e.g., GPT-x) in helping downstream tasks (especially interactive tasks) thrilling. Text-based games are interactive environments where state information can be represented in pure text, and thus they can be a good testbed to study LLM applications in an interactive setting. \n\n* Interesting approach: There are some other works in the field that aim to use LLMs 1) as an actor (i.e., given a piece of text describing the state, one queries an LLM to suggest the next action); or 2) as a critic (ask an LLM to review a trajectory in a hindsight, and use the resulting feedback or score as additional training signal to improve the agent). Different from existing work, here, the authors propose to leverage LLMs in a data augmentation manner. In such way, we can harvest knowledge from the LLMs and represent these knowledge as interactable environments (e.g., how to cook pasta). This is where I'm the most excited because in comparison with extracting static knowledge from LLMs (e.g., `cooking pasta requires a pan`), this approach generates environments that an agent could really interact with, through exploration, trial an error, the agent could learn such knowledge in an arguably more grounded way. In this work, the downstream task is to solve text-based games, it is also natural to augment the data in the form of games. \n\n* Human score: I appreciate the authors evaluated the generated games with human annotators."
            },
            "weaknesses": {
                "value": "Please see below in the questions section."
            },
            "questions": {
                "value": "### Concerns and Questions\n1. The current version of the paper (mainly Section 2) lacks the clarity on how exactly the games are being generated. From my understanding, not all steps described in Figure 2 (left) is performed automatically. For instance, how exactly the authors turn JSON files into Inform7 code, is that done by the LLM or the authors manually? On a related note, the authors mention `We compile this code using the Glulx...` but to my understanding Glulx is just an interpreter, not a compiler. Could the authors clarify this?\n2. I like the authors decompose text-based games as trees of `skills`. To make the statistics of generated games more clear, I'd suggest the authors to provide more information on the diversity of the skills, and how these skills relate to skills appear in ScienceWorld/TWC/Zork? The authors hypothesize that RL agents may have adapted skills learned from pre-training games to target games, but there is insufficient evidence on why/how this is the case. \n3. Related to the previous point, how exactly do the authors obtain `game ideas`, or the `seeds`? This could be quite important because the seeds may have big impact on the distribution shift between generated games and the target games. I.e., if the distributions are close enough, it makes sense that the augmented games could be helpful. But otherwise, there might be other reasons why they are helpful. \n4. Is there a specific reason why the variance in the figures (e.g., Figure 4) are so low? It seems all the 3 agents with different seeds learn in a very similar way?\n5. Figure 6 is a bit confusing, I don't understand why in the example on the right the agent gets 35 and 39, but in the left figure the scores at the end is 15. \n6. In Figure 5 (mid and right), it seems the vanilla baseline end up getting higher training scores, but in Table 3, STARLING achieved better evaluation scores. Is that because vanilla baseline is overfitting? The author say in Section 3.3.2 that `We can see that the STARLING agent gets a boost in performance both in the scores achieved and the moves taken compared to the vanilla TBRL.` But this is true only at the beginning of the training. \n7. In addition to my concern that the current version of the submission has insufficient clarity on game generation and insufficient analysis on the generated games, I feel the provided results are not strong enough. The vanilla TBRL seems to be a weak baseline, but even that, STARLING is not outperforming TBRL by a large margin. For instance, on Zork1, given enough episodes, vanilla TBRL could reach the same score as STARLING, given the fact that STARLING requires an extra 100 episodes of pre-training. \n8. I have a mixed feeling of raising this point, but in some recent works such as [SwiftSage](https://arxiv.org/abs/2305.17390), agents could achieve 0.97 test score on ScienceWorld task 1-1, compared to 0.04 here. I fully understand that these are totally different approach (maybe even orthogonal) and is less appropriate to be compared directly, but I encourage the authors to think how the data augmentation approach could be used to facilitate modern and much stronger baseline systems (than an RNN-based RL agent). Related to my previous concern, I am a bit hesitant when I see results of task 1-1 in Table 2, comparing 0.04 with 0. \n\n### Typos and minor things\n1. There are two TextWorld bib entries (page 11). The correct one is Cote et al.\n2. At the bottom of page 3, there are two sentences with repeating info: ` If the user approves the game metadata in the JSON file, we write and compile the Inform7 code based on the JSON file into an Inform7 game for a given game idea. If the user approves the game-related data in the JSON file, we write and compile the Inform7 code based on the JSON file into an Inform7 game for a given game idea. `\n3. In Section 3.3: `We can the pre-trained TBRL ...`\n4. In page 7, under Table 3: `... (fina a living thing ) ...`\n5. Figure 6 caption: `... in the art gallery withing the first few episodes.`\n6. Please use a larger font size in all figures. \n\n### Suggested citations\n1. [ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games](https://arxiv.org/abs/2305.14879). In this work, Wang et al. also explore to generate text-based games with the help from LLMs. The approach and goal are not exactly the same as in here, but still worths discussing at least in the related work section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8892/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698426723828,
        "cdate": 1698426723828,
        "tmdate": 1699637118621,
        "mdate": 1699637118621,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TVxvN7ta7h",
        "forum": "LXiG2WqKXR",
        "replyto": "LXiG2WqKXR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_yam5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_yam5"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors propose STARLING, an interactive text-based environment that leverages large language models (LLMs), such as GPT-3, to automatically generate text-based games. These games are created based on a seed set of game ideas and are designed to help RL agents acquire specific skills. The generated games involve tasks like cooking pasta, boiling water, and more, requiring RL agents to perform a sequence of actions to achieve a goal.\n\nThey evaluated STARLING on three environments: ScienceWorld, TWC, and Zork1. In all these environments, STARLING showed\nenhanced skills in the target environment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. It presents an interesting pipeline to leverage LLM to pre-train RL. They prove that STARLING outperforms the baseline RL agent by utilizing the skills learned using LLM and boosting the performance and generalization capabilities to reach the goal of the target environments: ScienceWorld, TWC, and Zork1.\n\n2. They promise to release the 100 games generated as a part of this paper, and the scripts to generate game-related files based on a set of game ideas and LLM. I think these prompts will be helpful to RL agent's training and test."
            },
            "weaknesses": {
                "value": "1. Concern about overlapping between training and test datasets. B/c LLMs were trained on many web texts which include text adventure game transcripts. When you test your pre-trained model on some existing games, will this cause data leaking?\n\n2. Limited Comparison. Only compare the model with a vanilla text-based RL agent. You use LLM, so whether an LLM-based agent will be worth comparing to?\n\n3. Scalability and Complexity: Compared to the vanilla text-based RL agents, what is the  Scalability and Complexity of STARLING?\n\n4. Experiments: I only found some limited comparisons under 100 episodes. I did not see the results after converging."
            },
            "questions": {
                "value": "See weakness. \n\n1. how to make sure the LLM games/skills you use for pre-training has no overlapping or similar scenario of test games?\n\n2. Compared to an LLM-based agent, what will be the results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8892/Reviewer_yam5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8892/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773832547,
        "cdate": 1698773832547,
        "tmdate": 1699637118502,
        "mdate": 1699637118502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "k7xCoPP4eE",
        "forum": "LXiG2WqKXR",
        "replyto": "LXiG2WqKXR",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_Fptt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8892/Reviewer_Fptt"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces STARLING, a self-supervised training of text-based reinforcement learning agent with large language models. Specifically, the authors propose to utilize a LLM (GPT3) to and an interactive fiction game engine (Inform7) to generate text games based on some seed ideas. This is motivated by the current limitations of text-based games for RLM studies. including simple games lacking complexity, and being domain-specific without generalizable capability. The generated games are used to pre-tran text-based RLM agents before evaluating on downstream tasks including ScienceWorld, TWC, and Zork1. Results show enhanced skills in the target environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. This paper introduces a method to augment pre-training data for text-based games. The data may mitigate previous limitations such as low complexity and domain-specific.\n2. The augmented data may improve model performance on downstream tasks."
            },
            "weaknesses": {
                "value": "1. There are many details and analysis missing. Please see the questions section below.\n2. Some key claims are not justified. The paper is motivated to generate data with high game complexity and generalized skills to other situations. However, from the evaluation and discussion, STARLING \"lack navigational complexity that elicits skills such as planning\", and \"involves fewer sequences of actions\" (although specifically designed to have a minimum number of actions). These cannot show the benefits of the pre-training."
            },
            "questions": {
                "value": "1. In Section 3.3., what skills are learned \"using LLM\"? My understanding is that you only use the LLM to generate games, while the agents are trained from scratch (according to Section 3.1)\n2. why did you select these 4 tasks out of 30 tasks from SCIENCEWORLD?\n3. Did you consider the 25 evaluation games as the development set?\n4. How did humans perform the study? Were they instructed to complete the tasks directly?\n5. Why would pre-training games may have influenced the performance of STARLING in the later episodes of tasks 1-1 and 5-2\"? Why would the performance comparison between the vanilla TBRL and STARLING vary dramatically among the four tasks? If \"STARLING\" really learned to avoid failure states better, why would the performance drop? And can you clarify what you mean by \"avoid failure states\"?\n6. For ZORK1, does the result suggest that pre-training helps with initialization, but in the end perform similarly to the baseline? What are the results comparisons in the test set?\n\n\nSuggestion: \n1. Please use \\citet to cite references\n2. Please double check your writing. There are many typos and errors that make it really hard to follow the paper. For example, there are sentence repetitions at the bottom of page 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8892/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698800453912,
        "cdate": 1698800453912,
        "tmdate": 1699637118368,
        "mdate": 1699637118368,
        "license": "CC BY 4.0",
        "version": 2
    }
]