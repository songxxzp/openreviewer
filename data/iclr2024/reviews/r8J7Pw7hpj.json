[
    {
        "id": "boJs2L3KCV",
        "forum": "r8J7Pw7hpj",
        "replyto": "r8J7Pw7hpj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_uiiy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_uiiy"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a new algorithm for multi-objective reinforcement learning. The main idea is to construct a Pareto oracle which can be queried iteratively to shrink the search space of Pareto optimal points. The authors provide theoretical analyses of the convergence properties of the proposed algorithm and experiment with three standard multi-objective optimization environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Multi-objective reinforcement learning is an important problem to study. This paper adds a new method to the collection of MORL algorithms. The proposed Iterated pareto Referent Optimization (IPRO) algorithm is interesting in its three phases and is relevant to the community.\n\n2. I appreciate the authors providing theoretical analyses of IPRO, though I have some questions about their presentations. Please see my questions in the Questions section."
            },
            "weaknesses": {
                "value": "1. There are some important baselines that need to be included, for example [1] and [2]. Furthermore, comparisons with GPI-LS did not demonstrate any improvements. I think that the authors should provide more discussion on this. It is hard to see why one would want to use IPRO over an existing method.\n\n[1]: Yang, Runzhe, Xingyuan Sun, and Karthik Narasimhan. \"A generalized algorithm for multi-objective reinforcement learning and policy adaptation.\" Advances in neural information processing systems 32 (2019).\n\n[2]: Abels, Axel, et al. \"Dynamic weights in multi-objective deep reinforcement learning.\" International conference on machine learning. PMLR, 2019.\n\n2. The presentation of the paper could use a major improvement. There are several places where definitions are either missing or unclear, making the paper hard to understand. The IPRO algorithm is described in words only in the main paper. I think Algorithms 1 and 2 in the Appendix should be included in the main paper to improve the clarity. Please see my comments in the next sections about other places for clarification.\n\n3. This may be a point I did not understand entirely due to the presentation issue. In Section 3.2, in describing the design of a Pareto oracle, the authors mentioned using GGF-DQN to optimize the objective function in Equation (1). My question is: what is the benefit of utilizing another multi-objective algorithm as a subroutine in IPRO compared to directly using it to optimize the multi-objective function?"
            },
            "questions": {
                "value": "1. In Theorem 4.1, does one need to place some assumptions on the Pareto oracle to claim this result?\n\n2. IPRO needs to find the ideal and nadir vectors for initialization. In practice, even the reduced single-objective RL problem might not be solved to optimality. How does this fact impact the performance of IPRO?\n\n3. In the definition of Achievement scalarizing functions (Section 2), please explain how $s$ becomes $s_r$ depending on $r$.\n\n4. Please define what a target region is. It is first mentioned in the paragraph about Achievement scalarizing functions in Section 2.\n\n5. In the Problem setup (Section 2), please define the expected return value $v^{\\pi}$ rigorously."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697672131791,
        "cdate": 1697672131791,
        "tmdate": 1699636325982,
        "mdate": 1699636325982,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AylSkxcRox",
        "forum": "r8J7Pw7hpj",
        "replyto": "r8J7Pw7hpj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_3Frz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_3Frz"
        ],
        "content": {
            "summary": {
                "value": "This paper provides a framework for learning the Pareto front in multi-objective MDPs. The framework decomposes the learning problem into a series of single-objective problems, where each problem is solved by a Pareto oracle. Specifically, the IPRO algorithm iteratively proposes reference points to a Pareto oracle and gets new Pareto optimal points which trim sections from the search space. The algorithm is shown to converge to the Pareto front asymptotically. The paper also contains experiments that validate that the algorithm leads to a close approximation to the true Pareto front."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The problem of learning the Pareto front in MOMDPs is general and is important in practice.\n\n- The proposed algorithm is natural and novel.\n\n- The theoretical guarantee looks sound."
            },
            "weaknesses": {
                "value": "- The presentation is not good enough. The major contribution of this paper is the algorithmic framework IPRO and the guarantee Theorem 4.2. However, the paper introduces IRPO on page 6 while stating Theorem 4.2 on page 8. For example, Section 3.2.1 - 3.2.2 is irrelevant to the main result and should be presented later. Moreover, the absence of the algorithm box of IPRO hinders my understanding.\n\n- See the question part."
            },
            "questions": {
                "value": "1. How do you select the reference point $l$ from the set of lower points $L$ if there are many candidates?\n\n2. Since the convergence is only asymptotic, how do you show the advantage of your method theoretically? For example, one can optimize a weighted sum utility each time, and the weights are sampled uniformly. This method can also converge to the Pareto front asymptotically.\n\n3. Following 2, is it possible to characterize the convergence rate?\n\n4. What is the big difference between a weak Pareto oracle and an approximate Pareto oracle in practice? Is it only for theoretical rigorousness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3694/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3694/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3694/Reviewer_3Frz"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698803564204,
        "cdate": 1698803564204,
        "tmdate": 1699636325883,
        "mdate": 1699636325883,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "frR1AZ86Zq",
        "forum": "r8J7Pw7hpj",
        "replyto": "r8J7Pw7hpj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_iyma"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_iyma"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a way to discover the Pareto front of Multi-objective RL (MORL) problems. The authors use the augmented Chebyshev scalarisation function as the achievement scalarising function (ASF), converting a multi-objective problem into a single-objective one. They introduced the Pareto oracles (the weak and the approximate ones) into MORL. With the Pareto oracles in hand, they propose Iterated Pareto Referent Optimization (IPRO), which learns the Pareto front in 3 phases: (i) The main loop consists of iteratively updating the Pareto front by interacting with the Pareto oracle. (ii) The return value from the Pareto oracle can be used to update the Pareto front while maintaining the lower set L and upper set U. Finally, they theoretically give an upper bound on the approximation error and a guarantee to converge to a \\tau-Pareto front. The proposed algorithm is then evaluated on multiple benchmark MORL environments, including DST, Minecraft and MO-Reacher."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper introduces a new concept called Pareto oracle into MORL, which can be implemented with modifications of single-objective RL algorithms, and thereby proposes IPRO, a new MORL framework that iteratively finds the Pareto optimal policies with the help of Pareto oracles. \n\n- This paper then provides upper bounds on the true approximation error at each time step and gives a convergence guarantee of their proposed algorithm IPRO."
            },
            "weaknesses": {
                "value": "- My main concern is that the proposed IPRO framework completely ignores one core problem of RL (and hence MORL) -- sample efficiency. Specifically, IPRO completely abstracts away the \"learning from online interactions\" aspect of RL through the use of Pareto oracles, which are assumed to be capable of directly returning a Pareto optimal policy given any reference point. However, this is actually the fundamental challenge of RL (and surely MORL as well). If the complexity of \"learning from online interactions\" is completely encapsulated into an oracle, then the problem would simply degenerate to a typical Multi-Objective Optimization (MOO) problem and then there is no need for reinforcement learning (That also manifests why the experimental results are all shown in terms of iterations, not in number of samples). With that said, the IPRO appears more like just an MOO algorithm rather than an MORL method (as it is agnostic to the specific problem structure of RL).\n\n- Several algorithmic components are stated without much explanation and hence rather confusing. For example:\n    - Regarding the practical implementation of a Pareto oracle, a Pareto oracle could be achieved by taking the augmented Chebyshev scalarisation function as the objective under any off-the-shelf RL algorithm (e.g., DQN, A2C, etc). Notably, the augmented Chebyshev function depends not only on the reference point but also  $\\lambda$ and $\\rho$, which are the weight vector and the augmentation coefficient that determine the required improvement of each dimension. The choices of $\\lambda$ and $\\rho$ are critical in determining which Pareto optimal solution we are looking for (similar to the preference vector in linear scalarization). However, in IPRO, it is not explained (either in the main text of the pseudo code in Appendix) how these parameters are determined in each iteration. \n    - IPRO, how shall we construct the sets $L$ and $U$?\n    - What has been done in the Pruning phase? And how does Pruning uncover additional Pareto optimal solutions?\n    - How is the memory-based policy implemented? Is there any special neural network architecture involved?\n\n- The experimental results do not appear strong enough to demonstrate the performance of IPRO. Several MORL benchmark methods are missing in the experiments. Just to name a few:\n    - PGMORL (Xu et al., ICML 2020) uses an evolutionary approach to search for the Pareto front and shall be a good baseline for IPRO.\n    - Envelope Q-learning (Yang et al., NeurIPS 2019) and (Abels et al., ICML 2019) provide single-network solutions to approach the convex coverage set.\n    - More recently, (Basaklar et al., ICLR 2023) and (Hung et al., ICLR 2023) also provide single-network solutions to MORL.\n\nMoreover, the experimental results are all reported in terms of iteration, which does not reflect the actual sample efficiency of the algorithms.\n\n- The presentation could be improved in several places. For example, in Section 2 and Section 3, while the preliminaries and definitions are mostly fairly standard definitions and concepts in MORL, I do find the description to be somewhat lengthy and hence a bit hard to read.  \n\n\n\n[References]\n1. Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik, \u201cPrediction-guided multi-objective reinforcement learning for continuous robot control,\u201d ICML 2020.\n2. Runzhe Yang, Xingyuan Sun, and Karthik Narasimhan, \u201cA generalized algorithm for multi-objective reinforcement learning and policy adaptation,\u201d NeurIPS 2019.\n3. Axel Abels, Diederik Roijers, Tom Lenaerts, Ann Now\u00b4e, and Denis Steckelmacher, \u201cDynamic weights in multi-objective deep reinforcement learning,\u201d ICML 2019\n4. Toygun Basaklar, Suat Gumussoy, Umit Ogras, \u201cPD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm,\u201d ICLR 2023.\n5. Wei Hung, Bo Kai Huang, Ping-Chun Hsieh, Xi Liu, \u201cQ-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots,\u201d ICLR 2023."
            },
            "questions": {
                "value": "Some additional detailed questions:\n\n- What are the definitions of \u201cnadir\u201d $v^n$ and \u201cideal\u201d vector $v^i$ in Figures 1 and 2? (They seem to be some uniform lower bound and upper bound of all possible return vectors?)\n\n- What is the reason behind the design of a* in eq (3)? Is this design theoretically grounded?\n\n- I notice that in Fig 3(e), the coverage of DQN as Pareto oracle is generally higher than the other two while the hypervolume of DQN as Pareto oracle is the lowest in Fig 3(b). Why is that the case?\n\n- What is the main usage of Section 3.2.1? Is the stochastic stationary policy used in the implementation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699205182918,
        "cdate": 1699205182918,
        "tmdate": 1699636325793,
        "mdate": 1699636325793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yTqKF5KN5Q",
        "forum": "r8J7Pw7hpj",
        "replyto": "r8J7Pw7hpj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_WEZT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3694/Reviewer_WEZT"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes IRPO, which transforms multi-objective optimization problems into a series of single-objective optimization problems. It iteratively selects reference points and simplifies the search space based on the solutions optimized by RL. The effectiveness of the method is demonstrated both experimentally and theoretically in this paper."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1, The idea presented in this paper, as far as I know, is innovative. The approach of decomposing the problem from the perspective of hypervolume and solving it gradually is inspiring to me. I believe this paper has the potential to become a work with long-term impact."
            },
            "weaknesses": {
                "value": "1. Despite the novelty of the idea, the major concern is that the experimental evidence in the paper is not sufficient to demonstrate the advantages of the method compared to other MORL algorithms. First, the benchmarks used are relatively simple, and there are no experiments on complex multi-objective tasks, such as the classic MO-MuJoCo tasks. Additionally, there is a lack of comparisons with relevant MORL baselines, such as Envelope [1], PGMORL [2], and Q-Pensieve [3].\n2. Further improvements are needed in the presentation of the paper. For instance,  the definitions in Chapter 4 could be introduced in Chapters 2 and 3, rather than gradually unfolding them within later sections.\n3. I strongly recommend the authors to include pseudocode in the main text.\n\n\n---\n[1]: A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation\n[2]: Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control\n[3]: Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots"
            },
            "questions": {
                "value": "1. What are the advantages of IRPO compared to directly using RL for multi-objective optimization? For example, I could scalarize the multi-objective rewards directly and optimize them. I could use Envelope [1] to train one policy to solve all tasks, or get a set of policies through PGMORL [2].\n2. Could the authors provide a performance comparison between IRPO and other MORL algorithms on MO-MuJoCo?\n3. What do \"occupancy measure\" and \"occupancies\" mean in Section 3.2.1? An introduction is lacking.\n4. How long does the RL optimization process before pruning take? More detailed training specifics are needed.\n\nIf the authors can address my concerns, I am willing to increase my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3694/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699633304055,
        "cdate": 1699633304055,
        "tmdate": 1699636325709,
        "mdate": 1699636325709,
        "license": "CC BY 4.0",
        "version": 2
    }
]