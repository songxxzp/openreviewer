[
    {
        "id": "GUBKzQ9evq",
        "forum": "NL6bspkWft",
        "replyto": "NL6bspkWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on open-vocabulary 3D instance segmentation. They introduce a new pipeline, namely, OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene understanding at the instance level. The OpenIns3D framework employs a \u201cMask Snap-Lookup\u201d scheme. The \u201cMask\u201d module learns class-agnostic mask proposals in 3D point clouds. The \u201cSnap\u201d module generates synthetic scene-level images at multiple scales and leverages 2D vision language models to extract interesting objects. The \u201cLookup\u201d module searches through the outcomes of \u201cSnap\u201d with the help of Mask2Pixel maps, which contain the precise correspondence between 3D masks and synthetic images, to assign category names to the proposed masks. This 2D input-free and flexible approach achieves state-of-the-art results on a wide range of indoor and outdoor datasets by a large margin."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Open-vocabulary instance segmentation is an important research topic in 3D scene understanding, this paper proposed a novel method to tackle this problem.\n\n- They propose a novel Mask-Snap-Lookup scheme, which distills knowledge from 2D foundation models to the 3D masked point clouds.\n\n- Their method outperforms the existing baseline approaches on several benchmarks, which demonstrates the effectiveness of their proposed method.\n\n- The paper writing is clear and easy to follow."
            },
            "weaknesses": {
                "value": "- The mask proposal module is adapted from an existing instance segmentation model Mask3D. Although the authors remove the class-specific information, essentially Mask3D is trained with a close set of categories. Hence the mask proposal module is not class-agnostic and the whole system is not an open-vocabulary system, as this system cannot handle the irregular point cloud clusters that don't belong to the indoor object classes. To evaluate this, I suggest the authors adapt their system to outdoor driving scenarios such as nuScenes, to see whether their approach can generate mask proposals of objects on a road such as traffic cones and barriers.\n\n- The authors need to compare with CLIP^2 [1] in Table 1, which can also generate open-vocabulary instance segmentation results.\n\n[1] Zeng et al. CLIP$^2$: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data. CVPR 2023."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1445/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1445/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1445/Reviewer_3Uq7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698226845405,
        "cdate": 1698226845405,
        "tmdate": 1699636073099,
        "mdate": 1699636073099,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4Srgc3SqvW",
        "forum": "NL6bspkWft",
        "replyto": "NL6bspkWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_A44f"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel pipeline called OpenIns3D, which consists of three core steps: mask, snap, and lookup, eliminating the need for 2D image inputs and enabling 3D open-vocabulary scene understanding at the instance level.  This approach not only requires less rendering time and inference time but also achieves much stronger results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The experimental results are impressive. The OpenIns3D achieves superior quantitative results compared with other methods.\n2. The idea is interesting. The authors propose a novel framework that can achieve 3D open-vocabulary scene understanding without 2D images. \n2. This paper is well-written and maintains a smooth flow. The whole pipeline is easy to understand."
            },
            "weaknesses": {
                "value": "1. As the method consists of multiple steps, the authors should provide more training details for all steps in the main text or appendix.\n2. Will the performance of 2D Open-world Detector influence the performance of the OpenIns3D? The authors seems not to provide experimental results in ablation study.\n3. Although the authors propose a 3D open-vocabulary scene understanding without 2D images, this method still needs well-prepared point clouds. It seems that 3D point clouds are also difficult to obtain in real world."
            },
            "questions": {
                "value": "1. Input queries shown in experiments are a few words. The authors should provide additional experimental results on processing real whole sentences.\n2. The authors should provide visual comparisons with SOTA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698836737588,
        "cdate": 1698836737588,
        "tmdate": 1699636073037,
        "mdate": 1699636073037,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VjMYd2xRwF",
        "forum": "NL6bspkWft",
        "replyto": "NL6bspkWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_wuwY"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for open-vocabulary instance segmentation. It consists of three main steps, first around a 3D point cloud of a scene, several virtual cameras are placed (all pointed inwards) to record several synthetic images of the scene (snap). Some 2D open-vocabulary segmentation or detection approach is then applied to these images to find the sought after objects. In parallel, a class agnostic variant of Mask3D is used to extract object proposals (mask). Finally, the obtained class agnostic masks are matched to the obtained open vocabulary instances to assign them to a class (lookup). The key difference to previous methods is that this approach does not require aligned RGB images to be present in the data, but rather relies on synthetically created images to be fed into a 2D vision-language model. Compared to previous open vocabulary methods evaluated on ScanNetV2, S3DIS, and STPLS3D, the method achieves better performances."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The writing of the paper is easy to follow.\n- The paper tackles the interesting task of OV point cloud instance segmentation.\n- The scores compared to some baselines look promising, even without the use of 2D images."
            },
            "weaknesses": {
                "value": "- At its core, the method is built on top of a somewhat flawed assumption. How can we obtain RGB point clouds, without actually having aligned RGB images? Of course there might be LiDAR point clouds without aligned RGB images, but at that point we can also not create synthetic RGB images from an uncolored point cloud, to feed into a 2D model expecting RGB images. While I still see some potential benefit, like being able to render novel images that are more focused on certain objects or better suited for the downstream model, this aspect is not explored here. I can't really imagine another case, where we have RGB point clouds, but for some reason we had to delete each and every underlying RGB frame used for colorization. As such I don't see the true merit of this approach.\n\n- Ignoring the above issue, the \"mask\" module is not novel, it's very similar to the one used in OpenMask3D with some mask filtering from SAM, the \"lookup\" module is fairly simple matching and not super interesting as far as I can tell. Leaving the \"snap\" module to be the core novelty of this approach. The way it is described initially: \"Multiple synthetic scene-level images are generated with calibrated and optimized camera\nposes and intrinsic parameters.\" gives the impression that the camera poses and intrinsics might somehow be optimized on a scene by scene basis, to create the best possible synthetic images for a given scene, resulting in the best scores. However, I understand this module simply places a predefined number of cameras around the scene, looking at the center and optimizing the focal length according to some heuristics. The slightly more involved \"local enforced lookup\" is mostly explained in the supplementary, raising the question what the real novelty of this paper is supposed to be? Finally, the rendered images are still just points splatted to the camera? According to \"Challenge 4\" there is a domain gap between projected and natural images, which I agree with, but as far as I can tell, the paper does nothing to truly bridge this gap, apart from aiming virtual cameras in a certain way?\n\n- The training of Mask3D for class mask predictions can have a significant effect on how truly \"open vocabulary\" the downstream method really is. The way all of these methods are presented is that one can easily find objects as long as we can describe them. This method relies on Mask3D being able to create class agnostic masks for the objects though. To thus evaluate how well it generalized to novel classes, Mask3D should not be trained on these classes. Now I'm assuming that Mask3D was trained to exactly create proposals for the classes also evaluated on, which makes the whole evaluation questionable. The same weakness is actually also a problem in the evaluation on OpenMask3D. While the paper does show some qualitative results in the teaser figure, this does not really prove the overall generalization. To properly show this, it could have been a great opportunity to simply evaluate on ScanNet200, given this should not require any novel training.\n\n- In general the lack of evaluation on a larger set of classes such as ScanNet200 is a clear weakness in my opinion. In fact, if you can provide such results in the rebuttal, without retraining (which should thus be fast), I would be happy to change my opinion. This would also allow a direct comparison with OpenMask3D. And yes, while one might claim that this was not published prior to the ICLR deadline, the paper clearly acknowledges OpenMask3D exists by citing it and even contains a figure about it. As such a simple comparison would be more than fair.\n\n- An actual ablation about how real images compare to synthetic images for this would have been very valuable. An interesting setup could have been to use a set of real images from the dataset and compare it to rendered synthetic images using the same camera pose and intrinsics.\n\n- Certain parts of the text make it seem like it's a bad thing to use a pre-trained network such as 3DETR or DETR, clearly stating that MPM is trained from scratch (start of chapter 5). This feels a bit hypocritical given that this whole method relies on pretrained 2D VL models trained on huge amounts of data. In general I must sadly say the text seems to contain a lot of \"fluff\" for me, introducing new fancy names for all kinds of stuff."
            },
            "questions": {
                "value": "- I think that the references to Table 5 and 6 in the text are swapped.\n\n- The introduction states \"and point clouds generated by photogrammetry frequently lack depth maps\", how is this relevant here? And is a point projected to an image not a sparse depth map?\n\n- As suggested above, an actual evaluation on ScanNet200 would make a major difference."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698849446608,
        "cdate": 1698849446608,
        "tmdate": 1699636072952,
        "mdate": 1699636072952,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7hReWjFoEU",
        "forum": "NL6bspkWft",
        "replyto": "NL6bspkWft",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_rEPE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1445/Reviewer_rEPE"
        ],
        "content": {
            "summary": {
                "value": "OpenIns3D, a RGB-independent framework, addresses point cloud-based instance segmentation through the introduction of a multiple stages approach. Firstly, it generates mask proposals based on point clouds, rendering scene images following a specified strategy. Subsequently, 2D vision language models are employed to extract objects. Finally, the assignment process between 2D and 3D segments is applied. Experimental results underscore that OpenIns3D substantially outperforms previous baseline models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed framework focuses on an RGB-agnostic setting and achieves precise 3D instance segmentation through a multi-stage approach.\n\n- The framework primarily relies on 3D proposals, establishing connections between 2D and 3D segments, and subsequently employs filtering operations that effectively leverage large-scale 2D vision models.\n\n- Experimental results, when compared to those presented in previous papers, clearly illustrate a remarkable enhancement in performance."
            },
            "weaknesses": {
                "value": "- The Mask Proposal Module is trainable using IoU as a form of supervision. It is strongly recommended to include comprehensive training details in the main draft of the paper.\n- Further clarification is needed regarding the adjustment of camera parameters in the Camera Intrinsic Calibration process.\n- It is advisable to incorporate a comparative analysis that includes segmentation results obtained from multiple pseudo-projected images. Given that your method heavily relies on prior knowledge from 2D models, solely comparing it with point-based methods may not provide a fully equitable evaluation.\n- The performance gain from 2D models or framework design should be separately discussed."
            },
            "questions": {
                "value": "- Please provide a detailed explanation of the several modules proposed in the paper, especially in addressing Weaknesses 1 and 2.\n\n- Please include further comparisons with image-based segmentation. This can be achieved by projecting the point cloud onto different cameras and then unprojecting it back to the initial point clouds. Such additional analysis would help demonstrate the performance of the proposed method and offer valuable insights into why it may perform better or worse compared to pure image-based segmentation, and the gain from 2D models."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1445/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053972884,
        "cdate": 1699053972884,
        "tmdate": 1699636072892,
        "mdate": 1699636072892,
        "license": "CC BY 4.0",
        "version": 2
    }
]