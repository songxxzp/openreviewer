[
    {
        "id": "4Q2PgMd6WQ",
        "forum": "BtmB8WrPSp",
        "replyto": "BtmB8WrPSp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for creating sparse adversarial perturbations. The authors evaluate the approach comparing with existing sparse image-specific attacks, against model robust to $\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ perturbations."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "+ the attacks seem to be run correctly in the evaluation\n+ tested against robust models\n+ interesting approach to achieve sparse perturbations"
            },
            "weaknesses": {
                "value": "- experimental evaluation should be improved\n- contributions are not fully supported by the experimental evidence and should be clarified"
            },
            "questions": {
                "value": "Overall, the paper is easy to read and well written. The proposed contribution is significant, however the claims should be supported better by the experimental evidence.\n\n**Experimental evaluation should be improved.** The authors claim the approach is explained with image classification as an example, but the approach should be applicable to any kind of data. This is inconsistent with how the method is evaluated. In fact, the authors write in the introduction:\n\n> For image inputs, we consider the pixel sparsity, which is more meaningful than feature sparsity and consistent with existing works (Croce & Hein, 2019c; Croce et al., 2022). That is, a pixel is considered perturbed if any of its channel is perturbed, and sparse perturbation means few pixels are perturbed.\n\nSo this means that a value of perturbation equal to x corresponds to x pixels changed, but each pixel might contain up to three features. This is written only in the introduction, which makes the evaluation metrics used later for the experiments unclear. \nMoreover, it would be interesting to see the results of this method without this additional constraint. The approach can be still developed, simply by creating a mask for every channel. However, removing this limit would make the attack comparable with many other white-box sparse attacks, including:\n\n* EAD https://arxiv.org/abs/1709.04114\n* VFGA https://arxiv.org/abs/2011.12423\n* PDPGD https://arxiv.org/abs/2106.01538\n* BB https://arxiv.org/abs/1907.01003\n* FMN https://arxiv.org/abs/2102.12827\n\n**Unclear difference with SAIF.** The authors state that the attack method is similar to the SAIF attack (beginning of sect. 4.1). However, they don't explain clearly what the difference is and what they add to this similar attack to make it perform better. This should be discussed in sect. 4.1"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3339/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3339/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3339/Reviewer_RqW9"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697961764269,
        "cdate": 1697961764269,
        "tmdate": 1699636283342,
        "mdate": 1699636283342,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "o8E6wBzxWT",
        "forum": "BtmB8WrPSp",
        "replyto": "BtmB8WrPSp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_DCtb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_DCtb"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors investigate the performance of Sparse-PGD, an l0 attack for crafting adversarial examples. Specifically, the authors note how little attention there has been on evaluating the robustness of machine learning models based on l0 threat models. To this end, the authors propose an attack that is specifically optimized for this threat model, borrowing ideas from SAIF. Their method, Sparse-PGD, is built from a magnitude tensor and a sparsity mask, whose design attempt to tackle known problems in l0-based optimization with convergence and gradient explosion. In their evaluation, they compare their attacks against a variety of other attacks and (adversarially trained) models and demonstrate compelling results. The paper concludes with an ablation study on varies components of their attack and a brief experiment on adversarial training."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Significance/Originality**- $\\ell_0$-based attacks have received much less attention than other $\\ell_p$ threat models, but represents more realistic threat models in many domains such as network data\n\n**Quality**- Explorations on adversarial training and a broad set of baselines gives a good measure of attack performance.\n\n**Clarity**- Background is well-written, gives a good summary of the field of AML and the various threat models, making it appealing to a broader audience"
            },
            "weaknesses": {
                "value": "* Optimization is unclear - Section 4.1 requires additional details. Arguments are made concerning when a relaxation is necessary (i.e., through a projection), yet later it is claimed that the relaxation exhibits deficiencies, so the original optimization is used instead. After reading 4.1, it is unclear what optimization sPGD actually entails and what is used in the evaluation.\n* Evaluation methodology - There are many important details are not present in the evaluation and necessary plots are missing (see questions for details)\n* Contribution of attacks introduced in this work is unclear - It does not seem appropriate to add Sparse-RS as part of sAA, given that Sparse-RS is used verbatim from prior work. The evaluation should only include the contributions made in this work.\n* Incomplete characterization of l0-based attacks - JSMA (Papernot, 2016) is not mentioned or evaluated against, even though it is the first l0-based attack"
            },
            "questions": {
                "value": "Thank you for your contribution to ICLR. It was an interesting read. Below, I summarize some of my main questions concerning this work.\n\n1. Section 4.1 can be confusing at times - Section 4.1 should be revisited, given that there seem to be inconsistencies in the motivation of certain decisions and the optimization itself is unclear. Specifically: (a) for updating the magnitude tensor, are p and delta the same variable? (b) Why is the l2-norm of the loss taken in (5)? (c) For updating the sparsity mask, what is gradient ascent performed on? (d) it is unclear what, \"Since elements in m are 0 or 1, we use sigmoid to normalize elements in m-tilde to be 0 or 1\" is trying to say; aren't elements in m in [0, 1] because of (6)? (e) the argument that projection on the binary set Sm is discarded because coordinate descent is suboptimal is unclear; why is such a projection introduced to be later argued as suboptimal and thus discarded? (In fact, this observation is stated twice) (f) it is unclear where the projection onto the binary set Sm is used in gp and why it is used in tandem with gp-tilde if gp exhibits both non-convergence and gradient explosions, and (g) there are many terms that are co-dependent with other terms throughout 4.1--it is challenging to understand precisely what are the main ingredients of Sparse-PGD, why they matter, and what decisions influenced their design.\n\n2. Evaluation could be clearer - While I appreciate the extensive evaluation, it does not appear to disclose sufficient information to measure the performance of sAA. Specifically, (a) a distortion vs accuracy curve should be plotted, so that we can understand the performance curves of sAA against baselines. Reporting the final results at a fixed norm boundary is not readily indicative of attack performance, given that are are many values of k a defender would consider to be \"adversarial\", (b) when attacking against adversarially trained models, perturbations must stay within the threat model. That is, it should be made clear that, when attacking an l-infinity-based model, the l0 perturbations also do not exceed, e.g., 8/255. Otherwise, it is not clear to me what insights are to be drawn from attacking a model whose threat model is violated, (c) mixing threat models does not seem sound. It is not clear why black-box attacks are compared to white-box attacks, etc. White-box threat models should only be compared to white-box attacks, and likewise for black-box attacks.\n\n3. Attack configuration does not seem fair - It is not clear to me why Sparse-RS is included within sAA when it is used verbatim from prior work. So that readers can understand the core contributions of this work, comparisons against baselines should only be evaluated against the introduced attacks. Moreover, the JSMA (Papernot, 2016) is one of the first l0-based attacks to be introduced in the literature. It is unclear to me why this not compared against in this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698782686252,
        "cdate": 1698782686252,
        "tmdate": 1699636283259,
        "mdate": 1699636283259,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cnxjJunaGT",
        "forum": "BtmB8WrPSp",
        "replyto": "BtmB8WrPSp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_DbyG"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a variant of PGD for $\\ell_0$-bounded adversarial perturbations, named Sparse-PGD (sPGD), which jointly optimizes a dense perturbation and a sparsity mask. Then, sPGD, on different loss functions and with two alternative formulations, is used to form, together with an existing black-box attack, Sparse-AutoAttack (sAA), which aims at extending the AutoAttack to the $\\ell_0$-threat model. In the experiments on CIFAR-10 and CIFAR-100, leveraging its multiple components, sAA improves upon the robustness evaluation of existing attacks. Finally, sPGD is used in adversarial training to achieve SOTA $\\ell_0$-robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Adapting PGD to optimize $\\ell_0$-bounded attacks is a challenging task, and sPGD is shown to often outperform existing attacks, especially white-box ones. Moreover it can be integrated into the adversarial training framework.\n\n- Extending AA to the $\\ell_0$-threat model would be important, and sPGD might be a promising step in such direction."
            },
            "weaknesses": {
                "value": "- While sAA seems effective (Table 1), there are some concerns in my opinion: first, according to Fig. 1a, the attacks notably benefit from more iterations. In particular, Sparse-RS shows significant improvements between 3k and 10k iterations for all models, which means that the results reported in Table 1 might be suboptimal. Second, in Tables 6, 7 and 8, CS alone appears to be better than sAA on the models robust to $\\ell_0$-attacks: while CS is evaluated on a subset of points only, an improvement of more than 3% (Table 6) seems significant to hint to the fact that, even on the full test set, the results of sAA might be improved. Finally, in most cases the robust accuracy of the best individual attack (either RS or sPGD) is quite higher (2-3%) than their worst-case, i.e. sAA, which suggests that each attack is suboptimal.\n\n- The budget of iterations of the attacks is not justified: looking at Fig. 1a it seems that more iterations would significantly improve the results, especially for RS. If I understand it correctly, sPGD is used for 20 runs ({1 CE, 9 targeted CE} x {projected, unprojected}) each of 300 iterations, for total 6k iterations (each consisting in one forward and one backward pass of the network). However, only 3k queries (forward pass only) are used for RS, which seems unbalanced given that RS provides better results for $\\ell_\\infty$- and (especially) $\\ell_0$-adversarially trained models.\n\n- The claim that no prior works proposed adversarial training for the $\\ell_0$-threat model is imprecise, see e.g. [Croce & Hein (2019)](https://openaccess.thecvf.com/content_ICCV_2019/papers/Croce_Sparse_and_Imperceivable_Adversarial_Attacks_ICCV_2019_paper.pdf). Moreover, the cost of using 100 iterations of sPGD in adversarial training seem very large. Finally, the sAT and sTRADES would need to be added to Fig. 1a, to see how the effect of more queries in RS on the achieved accuracy (see previous points)."
            },
            "questions": {
                "value": "The main concerns are detailed above. As minor point, it would be interesting to have some evaluation on ImageNet models.\n\nOverall, I like the idea of extending AA to the $\\ell_0$-threat model, but the current results do not convincingly support how the paper proposes to build sAA (e.g. how significantly would the results improve with 2x iterations to every attack, of 4x to RS?). Similarly, the effectiveness of adversarial training with sPGD should be tested more thoroughly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698855269998,
        "cdate": 1698855269998,
        "tmdate": 1699636283186,
        "mdate": 1699636283186,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SlZOTWkEe2",
        "forum": "BtmB8WrPSp",
        "replyto": "BtmB8WrPSp",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_Ngmx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3339/Reviewer_Ngmx"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a \ufeff\ufeffeffective and efficient attack called \ufeffsparse-PGD (sPGD) to generate sparse adversarial perturbations bounded by l_{0} norm, which achieves better performance with \ufeffa small number of iterations. Sparse-AutoAttack (sAA) is presented\ufeff, which is the ensemble of the white-box sPGD and another black-box sparse attack, for reliable robustness evaluation against l_{0} bounded perturbations. \ufeffFurthermore, adversarial training is conducted against l_{0} bounded sparse perturbations. The model trained with the proposed attack is superior to other \ufeffsparse attacks regarding robustness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The attacks are evaluated under different norms and limited iterations for fair comparison.\n+ The white-box and black-box are combined \ufefffor comprehensive robustness evaluation.\n+ The impacts of \ufeff\ufeffIteration Number and Sparsity Level are considered and analyzed."
            },
            "weaknesses": {
                "value": "- Following \ufeffSparse Adversarial and Interpretable Attack Framework (SAIF) [1], which adopts \ufeffa magnitude tensor and sparsity mask same as this paper, the authors further \ufeffdiscard the projection to the binary set when calculating the gradient and use the unprojected gradient to update \ufeffthe magnitude tensor p. \ufeffSparse-AutoAttack (sAA) part has extended the work of \ufeffAutoAttack (AA) [2,3], and the reason for discarding \ufeffthe adaptive step size, momentum and difference of logits ratio (DLR) loss function should be further explained clearly. The paper appears to offer limited new perspectives on the attack process and lacks a notable degree of technical innovation.\n- The authors claim that \u201c\ufeffWe are the first to conduct adversarial training against l_{0} bounded perturbations.\u201d However, related work had also conducted similar experiments [4].\n- This paper has emphasized the contribution of \ufeffcomputational complexity and efficiency but lacks corresponding analysis for \ufeffcomputational complexity and query budgets for comparison.\n- In Table 1 in experimental part, \ufeffRS attack outperforms sPGD_{CE+T} for l_{\u221e} models while more analysis is required.\n- The performance analysis in Subsection 5.1 is not well-organized for clarity.\n- Many parameters in this paper need to be pre-defined. For example, \u2018the current sparsity mask remains unchanged for three consecutive iterations, the continuous alternative fm will be randomly reinitialized for better exploration. \u2018 Why three consecutive iterations? Will choosing a different number affect the results?  What is \\alpha and \\beta? Will \\alpha and \\beta affect the value of \u2018three iterations\u2019? Also for a small \\lambda, it is unclear about how small the \\lambda should be.\n- How do you set up the budget for each attack method to compute the robust accuracy so the comparison is fair?\n\nReferences\n\u00a0\n[1] \ufeffTooba Imtiaz, Morgan Kohler, Jared Miller, Zifeng Wang, Mario Sznaier, Octavia Camps, and Jennifer Dy. Saif: Sparse adversarial and interpretable attack framework. arXiv preprint arXiv:2212.07495, 2022.\n[2] \ufeffFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an en- semble of diverse parameter-free attacks. In International conference on machine learning, pp. 2206\u20132216. PMLR, 2020.\n[3] \ufeffFrancesco Croce and Matthias Hein. Mind the box: l_1-apgd for sparse adversarial attacks on image classifiers. In International Conference on Machine Learning, pp. 2201\u20132211. PMLR, 2021.\n[4] \ufeffFrancesco Croce and Matthias Hein. Sparse and imperceivable adversarial attacks. In\u00a0Proceedings of the IEEE/CVF international conference on computer vision. 2019"
            },
            "questions": {
                "value": "Pls see the Section Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3339/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699081600506,
        "cdate": 1699081600506,
        "tmdate": 1699636283113,
        "mdate": 1699636283113,
        "license": "CC BY 4.0",
        "version": 2
    }
]