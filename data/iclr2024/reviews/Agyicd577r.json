[
    {
        "id": "iLcwXqwqR9",
        "forum": "Agyicd577r",
        "replyto": "Agyicd577r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an efficient prompting technique, BatchPrompt, which batches the input samples into a single prompt to improve the token utilization. While simply batching samples leads to a significant performance drop, this paper introduces Batch Permutation and Ensembling (BPE) and Self-reflection-guided Early Stopping (SEAS) to maintain the generation quality. BPE permutes the data order in each batch and uses majority voting to get the final prediction. SEAS allows early stopping of voting when LLM is confident about the sample. Experiments on some language understanding tasks show BPE+SEAS boosts BatchPrompt performance to be competitive with single-data prompting while using far fewer tokens and API calls."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The idea of BatchPrompt is simple and practical. Using ensemble and early stopping techniques BPE and SEAS to improve performance is novel to me.\n- The paper is clearly written and easy to follow.\n- The work focuses on the important problem of improving the prompting efficiency of LLM inference."
            },
            "weaknesses": {
                "value": "- The proposed method adds some hyperparameters like batch size and voting rounds for configuration. More analysis could be provided on computational efficiency tradeoffs and how to determine the good hyperparameters\n- The experiments are conducted on language understanding tasks. It would be helpful to evaluate the method on more diverse tasks, e.g. reasoning, knowledge-intensive QA, and creative writing."
            },
            "questions": {
                "value": "- How to determine good hyperparameters like batch size and voting rounds for BatchPrompt? More analysis could be provided on computational efficiency tradeoffs.\n- It seems that gpt-3.5-turbo suffers from performance degradation when using BatchPrompt, while gpt-4 does not. Is this caused by the model scale? I believe it is helpful to add an analysis of BatchPrompt on the LLaMA series with different model sizes.\n- It would be helpful evaluate the method on more diversed tasks, e.g. reasoning, knowledge-intensive, creative writing tasks. Does the type or difficulty of the instruction affect the performance of BatchPrompt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Reviewer_Nbfh"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698672517538,
        "cdate": 1698672517538,
        "tmdate": 1699635963214,
        "mdate": 1699635963214,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Szd7rw2sZI",
        "forum": "Agyicd577r",
        "replyto": "Agyicd577r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes a method for batching prompts.  Larger batch size generally improve throughput, but degrade performance.  This paper introduced some suggestions (voting rounds and SEAS) to reduce the performance degradation."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper advocates the use of batching for prompting, and may be successful in setting a new trend in that direction."
            },
            "weaknesses": {
                "value": "I worry about running so many experiments.  The plots in Figure 3 suggest that there are patterns to the results, but even so, if we run lots and lots of experiments and report the best values, the best value could be the result of randomness.\n\nOn the other hand, to make the case for trends, we may need to run even more experiments over more benchmarks, models, batch sizes and so on.\n\nIt would be nice to fit some kind of smooth regression to the results to help with interpretation.  Can you say how performance depends on batch size, voting rounds and model?  An ANOVA would help address concerns above with running so many experiments."
            },
            "questions": {
                "value": "Can you say more clearly up front that large batches improve throughput, but would degrade performance.  To address performance, you introduce voting rounds and SEAS.  This should also be stated clearly in the conclusions.\n\nThe discussion of the results should address the comments above about interpretation.  The ablation studies show that voting rounds are effective.  But it is hard to see the relation between batch size and performance.  It looks like batch size still reduces performance, even with voting rounds and SEAS.  Is that right?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission360/Reviewer_pc53"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698685039246,
        "cdate": 1698685039246,
        "tmdate": 1699635963120,
        "mdate": 1699635963120,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uRDowjKCav",
        "forum": "Agyicd577r",
        "replyto": "Agyicd577r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission360/Reviewer_7rFs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission360/Reviewer_7rFs"
        ],
        "content": {
            "summary": {
                "value": "For NLP tasks where each data point for inference is not necessarily lengthy, the token count for instructions and few-shot examples in the prompt may be considerably larger than that of the data point, resulting in lower token-resource utilization. This paper try to alleviate the preceding problem by batching multiple data points into a single prompt, a prompting strategy we refer to as \u201cBatchPrompt\u201d. This strategy increases the \u201cdensity\u201d of data points, which in turn leads to improved token utilization, which shows promising fulture."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tBatchprompt could highly improve token-resource utilization\n2.\tBPE could effectively It can effectively reduce the error rate caused by the different position in a batch.\n3.\tSEAS could effectively reduce the amount of unnecessary calculations"
            },
            "weaknesses": {
                "value": "1.\tIt seems that each item in the new batch (with only one prompt) could not be computed parallelly as original. Whether it will increase the time cost? It might be better to add time and flops metrics in the experiments.\n2.\tI think the \u201cbatchprompt\u201d could be used in both training and test phases, right?\n3.\tIn BPE, the weight for confidence is directly 1. What about to generate the weights scores directly by the LLM without whether confident?"
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698980757306,
        "cdate": 1698980757306,
        "tmdate": 1699635963047,
        "mdate": 1699635963047,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1zHz9hxnoO",
        "forum": "Agyicd577r",
        "replyto": "Agyicd577r",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission360/Reviewer_PJJX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission360/Reviewer_PJJX"
        ],
        "content": {
            "summary": {
                "value": "The paper describes a method to improve resource utilization by increasing the 'density' of user query tokens using batching the queries. User queries exhibit lower token utilization compared to the system prompts and/or few shot examples that goes with the query. Authors point out that this is not cost efficient and the 'batchprompt' method requires less LLM calls and better user query token utilization (saving the overall numbers of tokens in a batch which in effect is more cost-efficient). \n\nBatch prompting makes the LLM generation task n times harder for batch size n since the LLM needs to generate n outputs corresponding the n packed queries. Authors conduct experiments and show that this significantly degrades performance, and the order of the packed queries also significantly impact the performance. \n\nAuthors develop a batch permutation and ensembling method (to utilize voting from repeated permutations) - this slightly increases the token count and increases the LLM calls (still much less compared to single prompt inference) however improves the performance. Further improvement is realized with 'self reflection guided early stopping (SEAS) scheme) where the generator is also asked to provide the confidence of the result and using rules, the system stops the voting procedure early.\n\nAuthors have performed experiments on 3 datasets (yes/no question answering, entailment, paraphrase detection) and shown that with a batch size of 32, and using BPE and SEAS. the accuracies on 3 datasets do not degrade (improve slightly)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Authors propose a robust method that uses larger batch size, more voting rounds (eg. 5+) and a self-reflection guided early stopping approach.\n\n- The early stopping method also uses a pruning strategy to prune away confident predictions leaving fewer/harder samples for later\nrounds. In the process, the harder samples might also become easier to predict, due to smaller effective batch size in later rounds. \n\n- via experiments, authors show that voting is most successful when the baseline performance is strong (for example, gpt4 vs. gpt3.5)"
            },
            "weaknesses": {
                "value": "- Authors chose small number of tasks (only 3 simple tasks (yes/no QnA, paraphrase detection and entailment detection) -> these tasks may be too easy for gpt3.5 and gpt4 systems \n\n- Results are shown using few experiments (~300 dataset queries each for the 3 datasets); typically a validation on more tasks and more datasets would have helped get a more confident understanding of the approach. \n\n- this is a nice applied research paper with good results and a principled approach for improving cost efficiency, however there are many variables to unpack (quality and length of tasks, mixing different types of instructions, performance on novel datasets not seen by the LLMs, solving position bias discrepancy via BPE with more experiments and results, role of prompt variations on the results, etc)"
            },
            "questions": {
                "value": "- All tasks are very short answer type tasks, using tasks that generate longer answers might be very hard to experiment using the batchprompt approach. Couldn't see a discussion on this topic in the paper. Thoughts?\n\n- it is not clear how this system would be used with all its advantages in a deployment scenario - batching real world prompts with very different instructions might have unpredictable behavior, any thoughts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission360/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698983813806,
        "cdate": 1698983813806,
        "tmdate": 1699635962985,
        "mdate": 1699635962985,
        "license": "CC BY 4.0",
        "version": 2
    }
]