[
    {
        "id": "lL4aVeQUuI",
        "forum": "1op5YGZu8X",
        "replyto": "1op5YGZu8X",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
        ],
        "content": {
            "summary": {
                "value": "This paper applied neural tagent kernel techniques into adversarial training setting and proves that adversarial trained DNN can be approximated by a linearized DNN. For square loss it reveals a AT degreneration phenomena so that explains robust overfitting. The paper designed an algorithm AdvNTK based on the theoretical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I like the idea of analyzing robust overfitting from the time-dependent regularizer matrix that derived from adversarial NTK."
            },
            "weaknesses": {
                "value": "Adversarial trained DNN can be approximated by linearized DNN only when the perturbation size considered is small, however, the paper seems not give a quantitive result regarding how small it should be. I understand that the author using attack learning rate for a total of time S. Yet it\u2019s unclear how large the attack rate is, as if the attack learning rate is infinitesimal, then the perturbation size is so small, and it\u2019s unclear for me whether such adversarial training algorithm has any generalization guarantee in terms of robustness.\n\nMoreover, I\u2019m not sure why it\u2019s interesting to study adversarial training within the NTK regimes, as in [1] show when the network is close to initialization, there\u2019s no robust network at all.\n\nIn the proof of theorem 2 the dependency of polyt seems odd to me, as polyt depends on the norm of W_t, which can go to infinity. I understand that for neural tagent kernel the network width does not move far from initialization so you can argue norm of W_t is bounded,  as theorem Lemma C.5, yet this theorem\u2019s proof also depends on polyt. Therefore the current presentation of omiting dependency on the norm of W_t, nor of x_t seems to hide something in the proof.\n\nWhat\u2019s the relationship between the experiment vs. theorems? How does the experiment validate theorems? The robust accuracy of both SVHN and CIFAR10 seems extremely lower than normal adversarial training. Is it because of the network architecture? Why cannot use standard architecture such as ResNet that people commonly use in practice? Can the author provide any simple experiment to confirm the AT degeneration phenomenon? In fact, it\u2019s unclear to me the difference between this wording vs robust overfitting, and if they are the same thing then there\u2019s no need to use a different paraphrase.\n\nIn terms of the writing, I think the author should put more discussion on the theorems, explain, and provide intuition.\n\n[1] Wang, Y., Ullah, E., Mianjy, P., & Arora, R. (2022). Adversarial robustness is at odds with lazy training.\u00a0Advances in Neural Information Processing Systems,\u00a035, 6505-6516."
            },
            "questions": {
                "value": "This paper focuses on regression setting with squared loss. I\u2019m wondering if the idea can be generalized to classification setting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_edb8"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698508067446,
        "cdate": 1698508067446,
        "tmdate": 1700607341939,
        "mdate": 1700607341939,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "c2yWCY59S8",
        "forum": "1op5YGZu8X",
        "replyto": "1op5YGZu8X",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
        ],
        "content": {
            "summary": {
                "value": "\u2022\tThis paper theoretically explores the robust overfitting of Adversarial Training (AT). Specifically, it demonstrates that Deep Neural Networks (DNNs) trained with AT can be represented from the perspective of Neural Tangent Kernel (NTK). The paper further formulates the dynamics of Adversarial Training. Based on this formulation, it explains the reasons behind the occurrence of robust overfitting and proposes an algorithm, termed ADV-NTK, that can prevent robust overfitting in a manner akin to early stopping."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\u2022\tThe paper tackles a critical and pressing issue: Overfitting in Adversarial Training.\n\n\u2022\tThe contribution of paper is based on a theoretically solid proof. They also offer a detailed and meticulous explanation enhancing the understandings."
            },
            "weaknesses": {
                "value": "\u2022\tThe paper is written based on the logical flow of formulation and comparably not focusing on the motivation of the paper. For better readability, it seems to be a need for more appeal on how important task the paper tries to solve and what is the contributions of the paper.\n\n\u2022\tIn context of DNN-NTK, they replace the constrained-spaces condition with an additional learning rate term to control the strength of adversarial examples. (in Equation 7) However, unlike many attack mechanisms, it doesn't efficiently find a significant direction of attack. Despite this, does believe that the derived formula's proof is still not too loose?\n\n\u2022\tThe additional experiments are needed. In specific, it would be better to empirically verify 1) whether theoretically proven properties actually happen similarly in real-world dataset and 2) how much the robust overfitting problem has been addressed with the proposed Adv-NTK. For instance, tracking the performance (or loss) trend across iterations between vanilla AT and Adv-NTK."
            },
            "questions": {
                "value": "\u2022\tIn the above part.\n\n[Overall]\n\u2022\tIf the authors provide further experiments on Adv-NTK and supplementary explanations for the justification for learning rates, I agree that this paper is accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_4ePw"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634558860,
        "cdate": 1698634558860,
        "tmdate": 1700576076686,
        "mdate": 1700576076686,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1rvf6xFXfQ",
        "forum": "1op5YGZu8X",
        "replyto": "1op5YGZu8X",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of robust overfitting in adversarial training using the Neural Tangent Kernel (NTK). First, the authors extend the theoretical framework of NTK to adversarial training, introducing an adversarial regularization kernel and demonstrating that adversarially trained DNNs can be well approximated by their linearized DNNs. They then derive the closed-form dynamics of adversarial training for the linearized DNN and uncover a phenomenon called adversarial training degeneration: prolonged adversarial training leads to the degradation of the wide DNN to a state similar to that of a DNN with normal training, which results in robust overfitting. Based on these theoretical findings, the authors propose an adversarial training algorithm called Adv-NTK for infinite-width DNNs, and experimental results show that it can enhance the robustness of infinite-width DNNs to a level comparable to that of finite-width DNNs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper provides a linear model-level explanation of the adversarial training degeneration phenomenon. The designed Adv-NTK algorithm enables infinite-width NTK models to achieve robustness similar to MLPs through adversarial training."
            },
            "weaknesses": {
                "value": "1: This paper focuses on linearized models but lacks a detailed explanation of why linearization approximation is applicable. Theorem 1 demonstrates the convergence of two kernels for initialization. However, the properties that remain constant over time appear to be submerged in Appendix C.2. It is recommended that the authors include an informal presentation of the results from Appendix C.2 between Theorems 1 and 2 and provide corresponding discussions.\n\n2: Section 5.1 lacks a discussion regarding the impact of $\\eta S$ on the adversarial training degeneration phenomenon."
            },
            "questions": {
                "value": "Are the results in Section 4 of this paper valid for adversarial training of any intensity? Intuitively, if the intensity of adversarial training is too high, some of the results in Chapter 4 may not be valid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_s7R7",
                    "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2578/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779880075,
        "cdate": 1698779880075,
        "tmdate": 1700511518349,
        "mdate": 1700511518349,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HKwygi9PSy",
        "forum": "1op5YGZu8X",
        "replyto": "1op5YGZu8X",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the issue of robust overfitting in deep neural networks (DNNs) during adversarial training (AT). It extends neural tangent kernel (NTK) theory to explain this phenomenon for infinite width deep networks, showing that a wide DNN under AT can behave like a linearized DNN, leading to AT degeneration over time (assuming squared loss). To address this, the paper introduces Adv-NTK, an novel AT algorithm for infinite-width DNNs, designed to enhance network robustness. The effectiveness of Adv-NTK is demonstrated through experiments on real-world datasets, establishing its real potential in improving DNN robustness against adversarial attacks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper attempts to tackle an important problem of overfitting in adversarial training from a theoretical perspective, which should be relevant to the broader community.\n\nThe empirical validation of the Adv-NTK algorithm using real-world datasets like SVHN and CIFAR-10 enhances the quality of the paper. This empirical approach ensures that the theoretical findings are not only sound in theory but also applicable and effective in real-world scenarios."
            },
            "weaknesses": {
                "value": "Its not clearly under what conditions the small step size can lead to linearization of the adversarial training of DDN.\n\nIn the proof, it does not try to find efficient direction, does that make any difference on the proof ?"
            },
            "questions": {
                "value": "It would improve the paper if bound on the learning rate can be provided with respect to its training (or maybe some empirical results towards that)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_gmGK"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2578/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699639975158,
        "cdate": 1699639975158,
        "tmdate": 1699639975158,
        "mdate": 1699639975158,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ACEACARbJP",
        "forum": "1op5YGZu8X",
        "replyto": "1op5YGZu8X",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo"
        ],
        "content": {
            "summary": {
                "value": "This paper studied the robust overfitting issue in adversarial training. Specifically, the author studied the training dynamics of adversarial training under the NTK regime. Theoretical results show that after long training the term that captures the robustness will fade away and the trained network will degenerate to the network with standard training. The author further proposed an algorithm for adversarial training under the NTK regime and conducted experiment. Results show that the proposed algorithm outperforms standard adversarial training and vanilla NTK."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper provides a novel theoretical view in robust overfitting in adversarial training, and the theoretical analysis gives an intuition on why adversarial training fails on a simplified regime (NTK).\n2. The paper further proposed an algorithm applicable to NTK models. Experiment results aligns with the theoretical conclusion in the paper."
            },
            "weaknesses": {
                "value": "My concerns are listed as the follows:\n\n1. The proposed algorithm can only work under NTK regime. Though the corresponding results can serve as an empirical evidence of the theoretical conclusion, it seems that the proposed algorithm has limited practical application.\n2. The assumption in the paper seems too strong comparing with practical setting. The author are encouraged to consider more practical setting like GD and cross entropy loss. \n3. In the studied setting, the scale of pertubation is also related to the norm of $\\partial_x \\mathcal{L}$, rather than depending on $S$, which introduces a discrepancy between the setting studied and real-world setting.\n4. Some missing references in convergence of DNN: [1], [2], [3]\n\n[1]Li, Yuanzhi, and Yingyu Liang. \"Learning overparameterized neural networks via stochastic gradient descent on structured data.\" Advances in neural information processing systems 31 (2018).\n\n[2]Zou, Difan, et al. \"Gradient descent optimizes over-parameterized deep ReLU networks.\" Machine learning 109 (2020): 467-492.\n\n[3]Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. \"A convergence theory for deep learning via over-parameterization.\" International conference on machine learning. PMLR, 2019."
            },
            "questions": {
                "value": "Please refer to the \"weakness\" section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2578/Reviewer_S6eo",
                    "ICLR.cc/2024/Conference/Submission2578/Senior_Area_Chairs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2578/-/Official_Review"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699655755473,
        "cdate": 1699655755473,
        "tmdate": 1700502343141,
        "mdate": 1700502343141,
        "license": "CC BY 4.0",
        "version": 2
    }
]