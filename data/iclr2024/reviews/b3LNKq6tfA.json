[
    {
        "id": "lpa1zDpHk2",
        "forum": "b3LNKq6tfA",
        "replyto": "b3LNKq6tfA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_f5JC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_f5JC"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes ViCT, a multimodal Transformer model to reverse engineer UI code from screenshots. An actor-critic framework is used to train the model, addressing the problem of non-differentiable rendering. The model shows superior results on two novel synthetic UI-to-Code datasets, RUID and RUID-Large. A novel metric, htmlBLUE, is proposed to better compare html code."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The paper works on an interesting problem of automated reverse engineering of HTML/CSS code from UI screenshots.\n2. The authors propose to formulate the task as a Reinforcement Learning problem to tackle the problem of non-differentiable web rendering."
            },
            "weaknesses": {
                "value": "- The paper is not clearly written. Some sections are hard to follow (e.g. Section 3.3). \n- Some parts of the paper are inconsistent. \n  - In Section 4.1, the authors claim to test InstructBLIP[1] as a baseline, but I could not find it in the experimental results.\n  - In Section 4.1, the authors mention an experiment \"identifying the number of distinct shapes\", which is absent in the paper.\n- The main and only datasets the authors use for evaluation are fully synthesized. The UIs in the dataset only contain three types of elements, Rectangle, Ellipse and Button. From the examples in Figure 3, I find them quite unrealistic and do not resemble real-world web UIs, which shadows the effectiveness and practical applicability of the model in genuine scenarios.\n- Important details on dataset construction and algorithm design are missing (see Questions). \n- Experiments are limited. \n  - Missing baselines, e.g. Pix2Struct [2].\n  - The models are only evaluated on two synthetic datasets. Can you run experiments on other datasets, such as the dataset of pix2code [3]?\n  - \"DiT-LLaMA\" is missing in Figure 3.\n\n(Minor)\n- In Section 1, \n> In this paper, we take the first step towards reverse-engineering a UI screenshot, i.e., generating an HTML/CSS code that can reproduce the image.\n\nThere are prior works on UI-to-Code tasks, such as Pix2Struct[2] and pix2code[3], as you mentioned in Related Works. Do you mean you are the first to directly generate runable UI code without any postprocessing from images?\n\n- Some typos, e.g. a missing period at the end of Section 2.\n\n[1] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung and Steven Hoi. \"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.\" arXiv preprint arXiv:2305.06500. 2023.\n\n[2] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang and Kristina Toutanova. \"Pix2struct: Screenshot parsing as pretraining for visual language understanding.\" International Conference on Machine Learning. PMLR. 2023.\n\n[3] Tony Beltramelli. \"pix2code: Generating code from a graphical user interface screenshot.\" Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems. 2018."
            },
            "questions": {
                "value": "1. Please provide more information on the construction of the datasets, RUID and RUID-Large. How do you generate the DOM trees? Which CSS styles do you use as attributes?\n\n2. Please explain the design of the critic model. Is it trained on complete prediction-source pairs and used to estimate values on individual tokens? Additionally, it seems that the critic model only takes visual positional information into account, i.e. IoU. How does the model learn the attributes of the HTML elements, e.g. colors?\n\n3. Please further justify the use of htmlBLEU. In your experiments, you compare htmlBLEU to BLEU with the rendered pixel Mean Squared Error as a standard. Does that mean using MSE as a metric of visual similarity is a better choice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Reviewer_f5JC"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697707340095,
        "cdate": 1697707340095,
        "tmdate": 1699636796793,
        "mdate": 1699636796793,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "8qFNpBSf2r",
        "forum": "b3LNKq6tfA",
        "replyto": "b3LNKq6tfA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_TJDi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_TJDi"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel methodology for retrieving HTML/CSS code from screenshots, by stacking a vision encoder, that parses images as sequence of tokens, to a language decoder, that produce the code itself.\nTo train their netowork, the authors generate a dataset of simple HTML pages with elements and styles, thus enabling large-scale data generation.\nTo fine-tune the model, the authors rely on a RL algorithm that tries to maximise the similarity between the renders, formalized as a four-class approach. This formulation is differentiable, thus the method can be fine-tuned with gradient descent.\nAlso, the authors propose the htmlBLEU metric that emphasizes relevant common pieces of HTML/CSS.\nResults show that current state of the art creates allucinations, unable to produce similar results to the ground truth.\nThe authors clarify that this is a proof of concept, and more must be done to get higher-quality results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is original, as it presents an interesrting problem that can be solved through transformers.\n2. The state of the art is not able to re-create the same results as the proposal.\n3. Interesting technique for generating HTML synthetic data."
            },
            "weaknesses": {
                "value": "**Why RL?** I understand that it is not possible, given the render, to propagate gradients to the tokens. However, for the same reason, it is not clear from the paper why this is not a problem when optimising the RL policy. The authors should better explain the passage in 3.3, as now it is very confusing to understand.\n\n**Missing ablation study.** The RL algorithm is given some fixed rewards. How the results changes by varying them? And how these values have been chosen?\n\n**Confusion around the htmlBLEU** While the authors write a generic description of the metric, it would be easier for readers to read an algorithm. Also, the proposed metric does not score too different results with respect to BLEU.\n\n**Synthetic data might be harder to parse than real webpages.** While the introduction of the RUID dataset (and its creation) are very interesting and useful, I argue if the randomness of the approach could generate many samples that are very hard to transform to code, thus impeding the improvement of performance at training time."
            },
            "questions": {
                "value": "1. Can the author better explain why they use RL?\n2. Can the author provide a better explanation of the htmlBLEU metric?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697734773437,
        "cdate": 1697734773437,
        "tmdate": 1699636796661,
        "mdate": 1699636796661,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C4tj81Ru62",
        "forum": "b3LNKq6tfA",
        "replyto": "b3LNKq6tfA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_N1mE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_N1mE"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors propose a framework to process the screenshots of UI and generate related codes based on the LLM decoder. To solve the problem of inefficiency of rendering, a visual critic without rendering (ViCR) module is introduced to predict visual discrepancy of original and generated UI codes. Also, the paper created two synthetic datasets for training and evaluating. An additional metric, named htmlBLEU score, has been developed to evaluate the UI-to-code performance. The proposed method outperforms previous baseline."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The experimental results are good, demonstrating the effectiveness of proposed method."
            },
            "weaknesses": {
                "value": "The method is incremental in terms of scientific research value, just simply modifying the normal pattern of inserting vision encoder into language models. The proposed framework is effective in tackling the UI-to-code generation, but not such a fundamental research in representation learning from my perspective."
            },
            "questions": {
                "value": "The paper claims ViCR has no rendering during fine-tuning, but the training objective is based on IoU between reverse-engineered images and the original UI screenshot. So how to acquire the reverse-engineered images without rendering?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Reviewer_N1mE"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698650017276,
        "cdate": 1698650017276,
        "tmdate": 1699636796563,
        "mdate": 1699636796563,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Wq4dZAVupc",
        "forum": "b3LNKq6tfA",
        "replyto": "b3LNKq6tfA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_o7Uy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6863/Reviewer_o7Uy"
        ],
        "content": {
            "summary": {
                "value": "This paper provides ViCT (Vision-Code Transformer), an UI-conditioned code generation model that is fine-tuned with reinforcement learning (RL). More specifically, ViCT takes an UI image as input and generates HTML. ViCT consists of vision foundations models (e.g., ViT and DiT) for encoding images and Large Language Models (LLMs) for generating code. To further align ViCT with the visual similarity between an input UI image and an UI image rendered by generated code, this paper provides ViCR (Visual Critic without Rendering), a reward model for RL fine-tuning. To demonstrate the proof of concept, this paper builds RUID (Random UI Dataset), a new dataset for UI to code generation, that includes about 50K pair of UI image and HTML. With the dataset, this paper shows that ViCT provides comparable performance and fine-tuning with ViCR can further improves the performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- S1. The main idea of fine-tuning an image-conditioned text generation model with a reward model and reinforcement learning is very interesting. Even though the concept of an image-conditioned code generation was proposed before, using foundation models (DiT and Llama) and fine-tuning the model with RL (Policy Gradient method) seems novel.\n\n- S2. To demonstrate the proof of concept, this paper builds a new dataset for UI to code generation, which contains about 50K pairs of UI and HTML (RUID-Large, Random UI Dataset)."
            },
            "weaknesses": {
                "value": "- W1. Overall architecture of the proposed method (ViCT) seems reasonable. However, I am not sure that the design choice for the reward modeling and RL fine-tuning is effective. The overall method is similar to Reinforcement Learning with Human Feedback (RLHF), a recent prevailing method for LLM alignment. In RLHF, the reward model (RM) is usually modeled by relative feedback (preference or superiority) over a pair of inputs. Also, the prevalent RL algorithm is Proximal Policy Optimization (PPO) rather than vanilla Policy Gradient (PG). It would be better to provide some considerations on these design choices. And, it would be much better to provide a comparison between ViCR (absolute feedback + PG) and RLHF methods (relative feedback + PPO).\n\n- W2. I am not sure how effectively ViCR models an intermediate reward in Eq 2. According to Eq 2., \\hat{q_theta}(w_t^s), a value function for the token w_t^s is used. Can the reward model (ViCR) estimate the value for an intermediate token in partially generated code?"
            },
            "questions": {
                "value": "- Q1. Regarding W1, how does the reward model (ViCR) perform? Since this paper models ViCR as classification of visual similarity (very low, low, high and very high), it will be better to provide classification accuracy.\n\n- Q2. Regarding W1, how does the learning curve (e.g., IoU over learning steps) look like? It will help readers to understand the learning dynamics in RL fine-tuning of ViCT.\n\n- Q3. Regarding W2, how does the token-level reward model perform?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6863/Reviewer_o7Uy"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6863/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699107950614,
        "cdate": 1699107950614,
        "tmdate": 1699636796453,
        "mdate": 1699636796453,
        "license": "CC BY 4.0",
        "version": 2
    }
]