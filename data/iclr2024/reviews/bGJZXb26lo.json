[
    {
        "id": "NqkRQ5YhUY",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_FNUA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_FNUA"
        ],
        "content": {
            "summary": {
                "value": "DITTO is a world-model-based approach for imitation learning. DITTO uses an intrinsic reward to learn to imitate expert trajectories. The intrinsic reward adopted is a distance between the states reached by the learned policy and the ones reached by the expert policy in the expert data, evaluated on the latent space trajectories of the world model. This strategy allows learning on-policy (in the world model's imagination)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* **Motivation**: the approach is well-motivated, as covariate shift and learning from high-dimensional demonstrations are well-established problems. The authors also report theoretical results from the literature, providing appropriate references, to theoretically justify their approach."
            },
            "weaknesses": {
                "value": "* **Novelty**: the approach novelty is limited, as it combines the idea of performing imitation learning through some form of state matching/inverse reinforcement learning, which is not novel [1], with the idea of using world models to learn behaviour in imagination for high-dimensional inputs, which has become more and more popular in the last few years. \n* **Evaluation**: the results are limited to 6 environments (1 is in the Appendix). The authors compare baselines in terms of obtained rewards. In the first place, this choice is arguable given that none of the approaches is optimizing to maximize rewards. In the second place, it is not clear why some approaches perform better than the Expert. In general, I would expect that well-behaved approaches' performance would obtain similar performance to the expert and not outperform it. Some examples of how to evaluate similarity to the expert are provided in [1].\n\n[1] f-IRL: Inverse Reinforcement Learning via State Marginal Matching, Ni et al, 2020"
            },
            "questions": {
                "value": "* The authors use `s` both for indicating the POMDP and the world model latent states. These spaces can be completely different, both in dimensionality and in the information they contain, and thus it can be misleading to indicate them with the same letter. I recommend the authors to update their use of notation.\n* The presentation could be improved in several ways. There are some minor typos (see Questions section of the review), Figure 1 is not very informative (could show better how the world model and the expert trajectories are employed), Figure 3 has an unusual layout with two plots being much larger than the others, and the related work contains equations that are not completely relevant (e.g. Equation 4).\n* In order for the work to be accepted, I think the evaluation should be improved and better reflect the motivating scenario. I suggest the authors complete the story with more experiments and baselines. I also recommend the authors look into more adequate metrics of comparison that better support the motivation of this work, e.g. that show that DITTO reduces covariance shift (also see comments in Weaknesses)  \n\nSome typos/writing suggestions:\n* there is a repetition in the abstract: `in the latent space of a learned world model` appears twice in two consecutive sentences\n* typo in the introduction: `the both the strong`\n* in Figure 1 caption : `the intrinsic reward 8`"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5272/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5272/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5272/Reviewer_FNUA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697917801601,
        "cdate": 1697917801601,
        "tmdate": 1699636527038,
        "mdate": 1699636527038,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uZtNWovfu1",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_aexK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_aexK"
        ],
        "content": {
            "summary": {
                "value": "This paper studies offline imitation learning from pixels, which is actually an interesting and appealing direction. As a result, the authors propose an algorithm dubbed DITTO, which optimizes the distance measurement defined in the latent space of the learned world model. The authors propose to match the latent state of the learner and the expert. Instead of the minimization over the expert dataset, the authors match the latent state of the learner and the expert at the same time-step. The whole procedure of DITTO is composed of two phases, (a) train the world model in the demonstration datasets (b) the authors encode expert demonstrations into the world model latent space, and use the on-policy reinforcement learning algorithm to optimize the intrinsic reward. They conduct several experiments on pixel-based tasks, including discrete control tasks like Atari, and continuous control tasks like BipedalWalker."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- the studied topic is interesting and important to the community\n\n- the reproducibility of this paper is kind of good, and I believe the results provided in this paper are reproducible\n\n- this paper is easy to follow and understand"
            },
            "weaknesses": {
                "value": "However, I also have some concerns about this paper. Please refer to the following comments.\n\n- (major) This paper does not have a clear and friendly structure for the readers. Many of the key formulas (e.g., ELBO objective) are placed in the Appendix, while the authors spend many spaces on related work part. The authors state that they theoretically show that their formulation induces a divergence bound between the expert and the learner. However, no formal theorem/lemma/proposition is presented in the paper. The core theoretical result in Eq 6 is based on the previous conclusion. Also, what is the purpose and the role of Appendix A? What conclusion can be derived based on corollary A.1?\n\n- (major) The evaluations are limited. The authors only conduct experiments on 5 Atari tasks and 1 continuous control task. I do not think these are sufficient to show the generality and the effectiveness of their method. More experiments on both the discrete control domain and the continuous control domain can definitely make this paper stronger.\n\n- (major) potential issues with the baselines. The authors only compare their proposed DITTO method against some comparatively weak baselines like BC, D-GAIL, etc. As far as the reviewer can tell, there are some imitation learning methods that can achieve quite good performance on the Atari games, e.g., IQ-learn [1]. So, how does DITTO compete against it? Another recent offline imitation learning algorithm OTR [2] computes rewards using the optimal transport, so how does DITTO compete against OTR? Intuitively, we can choose not to use the world model and instead the optimal transport to calculate the rewards for the downstream RL algorithm. It is unclear whether DITTO can beat OTR in this scenario.\n\n[1] Iq-learn: Inverse soft-q learning for imitation. NeurIPS.\n\n[2] Optimal transport for offline imitation learning. ICLR.\n\n- (major) It is unclear how different choices of the base algorithm affect the learning efficiency and the final performance. The authors only adopt simple actor-critic architecture with the actor being updated using the REINFORCE algorithm and the critic being updated with the $\\lambda$-return. A valuable and necessary study is, how different base algorithms affect the performance of DITTO.\n\nBased on the above considerations, I think this paper is under the acceptance bar of this venue.\n\n- (minor) The author state in Section 2.1 that \n> These methods are effective when low-dimensional proprioceptive states and actions are available but have not yet demonstrated strong performance in high-dimensional observation environments\n\nI do not think it is difficult to extend these algorithms into the pixel-based variants, do you think these methods can have poor performance in the pixel-based tasks, given that we introduce an encoder to these algorithms?"
            },
            "questions": {
                "value": "- Can the authors also compare the computation cost of their method against baselines?\n\n- No limitations are stated in this paper. It is important to acknowledge both the advantages of your method and disadvantages of your method"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698915364641,
        "cdate": 1698915364641,
        "tmdate": 1699636526958,
        "mdate": 1699636526958,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mkdGaG7unr",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_nJqY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_nJqY"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an offline imitation learning method that imitates expert in the latent state space. The paper shows that the latent space of a learned world model can provide measure of state divergence, and measure imitation performance without access to an extrinsic reward function. The method first trains a world model, and then optimizes the RL agent by minimizing the latent state distance between the learned policy and the expert. The paper evaluates DITTO on challenging Atari environments from pixels, and shows that it outperforms existing offline imitation learning methods and achieves expert performance with minimal data. The paper also adapts two baseline methods, BC and GAIL, to the world model setting, and demonstrates that they benefit from the model-based approach but are still inferior to DITTO."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The novel intrinsic reward based on the latent distance is interesting, and the paper presents that maximizing this reward induces imitation learning and bounds the extrinsic reward difference. I appreciate authors' effort to justify the effectiveness of the heuristic reward in Appendix A. Besides, the paper provides a clear definition of the problem and the motivation seems correct."
            },
            "weaknesses": {
                "value": "1. A significant issue I concern is the contribution of the study. DITTO is built based on Dreamer [1], which also trains policy in the latent space. The difference is that Dreamer does not imitate the offline data. Besides, the proposed contributions in the Introduction section, i.e., D-BC and D-GAIL are straightforward.\n2. The literature review is inadequate. For example, there lacks a discussion about works within offline imitation learning domain, e.g., [2,3]. \n3. The paper is not well-organized. Some important details are missed in the main body, e.g., the training objective of the world model.\n4. Some offline model-based RL baselines are missed [4]. Besides, some IL baselines that handles high dimensional state should be considered, such as AIRL [5] and DICE [6].\n\n---\n\n### References above\n- [1] Dream to Control: Learning Behaviors by Latent Imagination. Hafner et al.\n- [2] Offline imitation learning with a misspecified simulator. Jiang et al.\n- [3] Curriculum Offline Imitation Learning. Liu et al.\n- [4] Offline model-based adaptable policy learning. Chen et al.\n- [5] Learning Robust Rewards with Adversarial Inverse Reinforcement Learning. Fu et al.\n- [6] Imitation Learning via Off-Policy Distribution Matching. Kostrikov et al."
            },
            "questions": {
                "value": "The paper does not provide any ablation studies or experiments to evaluate the impact of different components or design choices of DITTO. For example, how does changing the reward function, the distance measure, or the horizon length affect the results? How does DITTO compare with other model-based reinforcement learning methods that do not use imitation learning? How does DITTO perform on different types of environments, such as continuous control or navigation tasks?\n\nI feel that the paper lacks analysis or discussion on the quality and robustness of the learned world model. How well does the world model capture the dynamics of the true environment? And how does DITTO handle situations where the world model fails to generate realistic or consistent latent trajectories?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698946038850,
        "cdate": 1698946038850,
        "tmdate": 1699636526835,
        "mdate": 1699636526835,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "WN3FhTpuMI",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_d4u9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_d4u9"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces DITTO, a novel imitation learning algorithm that leverages a learned world model for offline training in a latent space. DITTO mitigates covariate shift and provides a measure of state divergence, enhancing the evaluation and optimization of imitation performance. Empirical results demonstrate that DITTO outperforms state-of-the-art methods in offline settings on challenging Atari environments from pixels, and the paper provides theoretical guarantees on the quality of imitation and generalization capability."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "-\tThe paper leverages world model learning to address common problems in imitation learning, showing how the latent space of a learned world model provides a rich representation of environment dynamics for imitation learning.\n-\tThe paper introduces extensions of baseline imitation learning algorithms to the world model setting, demonstrating that DITTO achieves state-of-the-art performance on challenging Atari environments."
            },
            "weaknesses": {
                "value": "1. The novelty of the method seems limited, which looks like a simple combination of Dreamer and GAIL / BC.\n2. The experiment setup is not convincing, it is too simple and all results are only evaluated on a small set of simple tasks.\n3. There is some problems about the theoretical part."
            },
            "questions": {
                "value": "1.\tIt is a little confusing that you prove a bound for the reward function given in Eq. (7) but actually use Eq. (8). The reason stated in the paper is that Eq. (7) is computationally expensive, but there is no clear relation between the formulations of Eq. (7) and Eq. (8). Could you please explain how you design the distance function in Eq. (8)?\n2.\tFollowing this, it will be more convincing if you train DITTO with Eq. (7) in a toy environment, which does not require much computing resources.\n3.\tIn Fig. 2, the latent distance is defined by $1-r_{int}$, where $r_{int}$ is also defined by yourself. It might be unfair to compare this indicator. What are the results of more common distance measures? Or could you give reasons why this measure is better than other measures?\n4.\tIs there any ablation study for different reward functions? If training with Eq. (8) is exactly better than with other common distance measures, it can also partly answer Question 3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "1.\tIt is a little confusing that you prove a bound for the reward function given in Eq. (7) but actually use Eq. (8). The reason stated in the paper is that Eq. (7) is computationally expensive, but there is no clear relation between the formulations of Eq. (7) and Eq. (8). Could you please explain how you design the distance function in Eq. (8)?\n2.\tFollowing this, it will be more convincing if you train DITTO with Eq. (7) in a toy environment, which does not require much computing resources.\n3.\tIn Fig. 2, the latent distance is defined by $1-r_{int}$, where $r_{int}$ is also defined by yourself. It might be unfair to compare this indicator. What are the results of more common distance measures? Or could you give reasons why this measure is better than other measures?\n4.\tIs there any ablation study for different reward functions? If training with Eq. (8) is exactly better than with other common distance measures, it can also partly answer Question 3."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699140522628,
        "cdate": 1699140522628,
        "tmdate": 1699636526723,
        "mdate": 1699636526723,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Ps1rwcmzuG",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_dH9k"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_dH9k"
        ],
        "content": {
            "summary": {
                "value": "This submission introduces an offline imitation learning algorithm, DITTO, consisting of a two-part training process: a world model trained on demonstrations of any quality and a policy using intrinsic rewards. The method is tested in an offline setting on Atari games, showing better performance than baselines, including Behavior Cloning and Generative Adversarial Imitation Learning."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The empirical results of DITTO are generally good.\n* The method addresses an important problem in imitation learning and proposes an interesting solution by using a world model and intrinsic rewards.\n* The submission is mostly well-written, clear, and easy to follow."
            },
            "weaknesses": {
                "value": "* More experiments are needed to establish the superiority of DITTO.\n* The discussions and comparisons on the related works are not sufficient, especially for offline imitation learning and imitation learning from pixels.\n  - Offline imitation learning:\n    - Offline imitation learning with a misspecified simulator. NeurIPS 2020\n    - SMODICE: Versatile Offline Imitation Learning via State Occupancy Matching. ICML 2022\n    - Discriminator-weighted offline imitation learning from suboptimal demonstrations. ICML 2022\n  - Imitation learning from pixels:\n    - Domain-Robust Visual Imitation Learning with Mutual Information Constraints. ICLR 2021\n    - Imitation Learning from Pixel-Level Demonstrations by HashReward. AAMAS 2021\n    - Visual Imitation Learning with Patch Rewards. ICLR 2023\n* The submission does not support the claim that it addresses the problem of covariate shift with sufficient evidence.\n* The impact of the quality of the world model on final imitation learning performance needs further analysis."
            },
            "questions": {
                "value": "Referring to the weakness:\n* How is the covariate shift problem embodied in the offline imitation learning problem? More empirical studies may be needed to verify the conclusion of the submission.\n* In Figure 3, why does DITTO exceed average expert performance in 3/5 environments, and is this desirable in offline imitation learning?\n* Comparison with existing model-based offline RL algorithms using similar datasets would be informative."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699497657431,
        "cdate": 1699497657431,
        "tmdate": 1699636526627,
        "mdate": 1699636526627,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g3nS2HrAo5",
        "forum": "bGJZXb26lo",
        "replyto": "bGJZXb26lo",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_gyqq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5272/Reviewer_gyqq"
        ],
        "content": {
            "summary": {
                "value": "In this paper, an offline imitation learning method is proposed. The key idea lies in learning the environment model with expert demonstrations, and further doing reinforcement learning (RL) in the learned model. The proposed approach is tested under Atari games with pixel inputs, verifying its effectiveness under these environments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation of the paper is reasonable: reducing the cost of doing online RL by learning offline in the model.\n\n- As far as I know, although model-based RL has a long history, the strategy of training environment model first has not been used in imitation learning."
            },
            "weaknesses": {
                "value": "- I am in wonder why the proposed method can handle the distribution shift issue in imitation learning. Please refer to Q1 below. \n- The theoretical discussions in Sec. 3.1 are confusing to me. Please refer to Q2 below. \n- The experimental results are not quite sufficient. There are several Atari games chosen in the experiments, but the reason for choosing them remain unclear. Why not report the results on more games? Furthermore, it is difficult to understand what kind of internal feature space that has been learned by the model. In my view, if learning the world model can address the distribution shift issue, one plausible reason is that the learned model feature space has strong generalization ability. While this remains unclear to me due to the missing of the analysis on the learned feature space."
            },
            "questions": {
                "value": "- As described in the paper, one of the central challenges for imitation learning is that the learner can face significant different data distribution to expert's, since her policy can be very different from which the expert uses. While if we use expert demonstrations only to learn the environment model, why can't this model still be biased? If we don't allow the learner to interact with the environment, how can we build accurate on situations that the expert seldom faces? \n\n- I don't understand why Eq. 5 and Eq.6 are correlated. Furthermore, the \"adaptation error\" term in Eq. 6 captures the error of model learning, which is crucial for understanding the proposed approach. While further analysis is lacking in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5272/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699537668508,
        "cdate": 1699537668508,
        "tmdate": 1699636526496,
        "mdate": 1699636526496,
        "license": "CC BY 4.0",
        "version": 2
    }
]