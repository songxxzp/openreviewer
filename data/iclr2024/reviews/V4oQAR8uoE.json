[
    {
        "id": "vtdd7HU0Ip",
        "forum": "V4oQAR8uoE",
        "replyto": "V4oQAR8uoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_7Ywq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_7Ywq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new defense framework, Rapid, designed to counter textual adversarial attacks. Rapid initially uses an adversarial example detector embedded within the victim model to identify potential adversarial examples. These flagged inputs are then further disturbed using adversarial attacks to neutralize the attacker's original perturbation, effectively restoring the adversarial example to a benign state. Lastly, a pseudo-similarity filtering strategy is implemented to select the restored examples, enhancing performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Rapid leverages adversarial attacks to counterattack adversaries and inject safe perturbations, aiming to distract the model from malicious perturbations."
            },
            "weaknesses": {
                "value": "1. Lacking evaluation on adaptive attack scenarios. The manuscript assumes a non-adaptive attacker without knowledge of the defense mechanism. However, modern attackers are becoming increasingly sophisticated and may attempt to circumvent defenses by targeting their individual components. Adding experiments where the adversarial detector and other defense modules are explicitly known to the attacker could demonstrate Rapid's effectiveness against such adaptive attacks.\n\n2. While the joint training of the adversarial detector and victim model is an efficient approach, the manuscript could further analyze how this multi-task learning impacts the model's performance on natural examples. Specifically, it merits exploration of whether optimizing the additional loss terms introduced by Rapid has any detrimental effects on the core NLP capabilities that the model was originally designed for.\n\n3. Lacking necessary ablation studies. The training losses listed in eq. 3 employs an adversarial training objective item. It is not clear if the defensive capability of Rapid is sourced from the existing adversarial training technique. Benchmarking against well-known adversarial training baselines would help evaluate the extent to which Rapid exceeds such prior work.\n\n4. The paper asserts that Rapid outperforms existing methods in efficiency, as it only executes a Perturbation Defocusing step on potential AEs. However, the PD process, which involves adversarial attacks, appears to be time-consuming. To substantiate the claim of efficiency, the authors should provide experimental validation demonstrating cost efficiency on a dataset comprised of 50% clean samples and 50% successful AEs."
            },
            "questions": {
                "value": "Please refer to the Weaknesses sec."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1263/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698590479285,
        "cdate": 1698590479285,
        "tmdate": 1699636053029,
        "mdate": 1699636053029,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "SUZUEycGpr",
        "forum": "V4oQAR8uoE",
        "replyto": "V4oQAR8uoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_EQRG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_EQRG"
        ],
        "content": {
            "summary": {
                "value": "Recent studies show language models are vulnerable to adversarial attacks. Current defence techniques struggle to repair semantics, limiting practical utility. A novel approach called Reactive Perturbation Defocusing (Rapid) uses an adversarial detector to identify pseudo-labels and leverage attackers to repair semantics. Experimental results show Rapid's effectiveness in various attack"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The method is clearly presented and easy to follow\n+ The demonstration is available"
            },
            "weaknesses": {
                "value": "- The code is unavailable\n- The novelty of work is limited\n- Some designs need more justification. For instance, why do you train adversaries in adversarial training?"
            },
            "questions": {
                "value": "1. Some well-known related textual adversarial attacks should be discussed, e.g., [1], [2], [3]. \n2. The novelty of the proposed method is hard to judge. It seems like it leverages another adversaries to change the attacked sentence to the sentence with the original semantics.\n3. Some method designs require more justifications. For instance, in adversarial defense detection, it is unclear to me why do the authors train the adversaries.\n4. In pseudo-similarity supervision, do you have a formal definition of the semantics of the sample? or is it just the label?\n\n[1] Morris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., & Qi, Y. (2020). Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909.\n\n[2] Li, J., Ji, S., Du, T., Li, B., & Wang, T. (2018). Textbugger: Generating adversarial text against real-world applications. arXiv preprint arXiv:1812.05271.\n\n[3] Boucher, N., Shumailov, I., Anderson, R., & Papernot, N. (2022, May). Bad characters: Imperceptible nlp attacks. In 2022 IEEE Symposium on Security and Privacy (SP) (pp. 1987-2004). IEEE."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1263/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698805349192,
        "cdate": 1698805349192,
        "tmdate": 1699636052921,
        "mdate": 1699636052921,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aCijDYJN3o",
        "forum": "V4oQAR8uoE",
        "replyto": "V4oQAR8uoE",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_WkfB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1263/Reviewer_WkfB"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes Rapid, a method to correct adversarial perturbations in textual adversarial examples.\n\nRapid is a reactive adversarial defense, doesn't defend all inputs. It pre-detects adversaries using an adversarial detector which is jointly trained with a standard classifier. Once adversarial input is identified, safe perturbations are added (perturbation defocusing). The perturbation introduced by the adversarial attacker is considered \u2018safe\u2019 since it does not alter the semantics of the adversarial input. *The safe perturbations are also added by an adversarial attack method.*\n\nThe paper shows that recent adversarial defense RS&V [A] cannot model the semantic differences in adversarial and repaired examples. Moreover, prior work is unable to efficiently pre-detect adversaries before the defense process. These approaches indiscriminately treat all input texts.\n\n[A] Xiaosen Wang, Yifeng Xiong, and Kun He. Detecting textual adversarial examples through randomized substitution and vote. In UAI, volume 180 of Proceedings of Machine Learning Research, pp. 2056\u20132065. PMLR, 2022b.\n\nExperiments are performed on BERT and DeBERTa models on SST2, Amazon, Yahoo! and AGNews text classification datasets.\nMultiple baseline defense methods like DISP, FGWS and RS&V are compared against Rapid."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper shows experimental results which improve upon prior defense works like DISP, FGWS and RS&V.\nThree research questions study the behavior of the defense method in detail.\nThe working demo illustrates the inputs and outputs of the system well."
            },
            "weaknesses": {
                "value": "The use of adversarial attackers to repair adversarial perturbations by Rapid is a bit counterintuitive to me. I do not fully understand how Rapid can correct attacks generated by PWWS when it also internally uses PWWS (Table 2). Either the method section doesn't clearly convey the idea, or it's due to my unfamiliarity with prior work."
            },
            "questions": {
                "value": "1. (Typo) Section 7 \u201calailable\u201d\n\n2. (Clarification) Section 1 \u201cThe examples repaired by Rapid are well-maintained\u201d. A clarification regarding what well-maintained means will be helpful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1263/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1263/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1263/Reviewer_WkfB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1263/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811148800,
        "cdate": 1698811148800,
        "tmdate": 1699636052830,
        "mdate": 1699636052830,
        "license": "CC BY 4.0",
        "version": 2
    }
]