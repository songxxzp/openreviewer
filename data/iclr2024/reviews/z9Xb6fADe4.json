[
    {
        "id": "ojVxIURAJC",
        "forum": "z9Xb6fADe4",
        "replyto": "z9Xb6fADe4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_NYw3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_NYw3"
        ],
        "content": {
            "summary": {
                "value": "This paper considers using deep reinforcement learning to design pushback rate for mixed-mode runways. Based on the congestion information, a controller decides the pushback rate (from 0 to 4). The reward is a combination of how fast aircraft leave the gate and the time it spends waiting to take off."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ Optimizing aircraft traffic within an airport is a complex and important problem. \n+ It's easy to understand what the paper is trying to do and the problem makes sense. \n+ Some numerical improvement are seen against an uncontrolled policy."
            },
            "weaknesses": {
                "value": "- It's not clear that deep reinforcement learning is the right tool to use here. Since the decision is centralized, and there are important safety constraints, a rolling horizon approach (e.g., a MPC) may do better. \n- The learning problem setup is also fairly standard and it's hard to see innovations in that regard. As the authors point out, a rate would be more natural for the traffic controllers. Looking at how this can be directly learned rather than taking a discretization approach as currently done in the paper would be interesting."
            },
            "questions": {
                "value": "- In current practice, is the pushback process entirely unmetered? Or would an aircraft need the clearance from air traffic control to pushback? If the latter is true, the ATC would be implicitly doing some optimization right?\n- Does the methods in the paper use the fact that a runway is mixed use? I'm not sure I see that in the algorithm. Also, for a large airport like Changi, are the runways mixed use?\n- Sometimes a plane pushes back to free up a gate such that a landed aircraft can use it. So holding planes at gates is not exactly zero cost."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6595/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6595/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6595/Reviewer_NYw3"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698686388769,
        "cdate": 1698686388769,
        "tmdate": 1699636750633,
        "mdate": 1699636750633,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "uA3R5kK7At",
        "forum": "z9Xb6fADe4",
        "replyto": "z9Xb6fADe4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_WotT"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_WotT"
        ],
        "content": {
            "summary": {
                "value": "This research introduces a Deep Reinforcement Learning (DRL) strategy for Departure Metering (DM) during mixed-mode runway operations, focusing on optimizing pushback timings to alleviate airside congestion. By framing the DM challenge as a markov decision process and utilizing Singapore Changi Airport data, the study simulates airside activities to assess DM policies. The DRL agent uses spatial-temporal event graphs to detect airside hotspots and adjusts pushback rates dynamically. Compared to other methods, the DRL approach proves superior, especially under high traffic. Findings indicate notable reductions in taxi times and a 27% fuel savings at Singapore Changi Airport."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The author employs an \"airside event graph\", inspired by temporal constraint networks, to effectively represent the state of airside traffic. This novel representation captures spatial-temporal movements of aircraft, offering a sophisticated modeling approach.\n2. It considers various types of conflicts (e.g., following, crossings, and head-on conflicts) that can occur during taxiing. This comprehensive approach ensures that the proposed solution addresses a wide range of operational challenges.\n3. The paper doesn't just propose a solution, but also quantitatively evaluates its impact, as seen in the results section. This rigorous evaluation approach, including fuel consumption analysis, provides tangible evidence of the proposed method's efficacy."
            },
            "weaknesses": {
                "value": "1. The model primarily validates using the Singapore Changi Airport scenario, questioning its adaptability. The research falls short in demonstrating generalization across varied environments.\n2. The \"hotspots\" paradigm is ambiguously defined. Such lack of clarity in state representation can lead to convoluted state spaces and suboptimal policies in reinforcement learning.\n3. The reward structure is inadequately elaborated. Given its importance in shaping agent behavior in reinforcement learning, its cursory treatment raises concerns about potential biases and pitfalls.\n4. The action space, narrowed to pushback rate control, oversimplifies the complexity of airside operations, missing out on capturing nuanced dynamics.\n5. The comparative evaluation against baselines lacks depth and rigor, failing to provide a comprehensive assessment against state-of-the-art methods."
            },
            "questions": {
                "value": "1. How does your approach compare with existing MARL algorithms in terms of efficiency and efficacy? Are there benchmark comparisons against state-of-the-art multi-agent methods to validate the superiority of your approach?\n2. How do agents in the model communicate during airside operations?\n3. Reward shaping and credit assignment are critical in multi-agent settings. How do you ensure that individual agents receive appropriate credit for their actions to promote cooperative behavior? Are there specific reward shaping techniques employed to foster collaborative actions?\n4. How does the method explore the joint action space and how scalable is it with increasing dimensions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Looks fine."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698754922839,
        "cdate": 1698754922839,
        "tmdate": 1699636750530,
        "mdate": 1699636750530,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zEbRJHecXx",
        "forum": "z9Xb6fADe4",
        "replyto": "z9Xb6fADe4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_f91p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_f91p"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel RL-based approach for Departure Metering (DM). This is achieved by introducing domain-specific state/action representations and together with a PPO-based RL agent for this task.\n\nEmpirically, the authors show that the proposed method outperforms other methods and ablations on a simulation based on Singapore Changi Airport surface movement data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "I am not an expert in the field of air traffic control, but the domain-inspired choices in the proposed MDP representation are well motivated. The authors encode specific domain knowledge to achieve good performance.\n\nThe paper is well written, although in some parts the narration feels a bit too slow (while in others the authors gloss over a few details)."
            },
            "weaknesses": {
                "value": "In my opinion, the main weaknesses of this work lie in (i) the extreme specificity of the methodology, (ii) the lack of baselines and/or experiments to validate the proposed architectural innovations, and (iii) more generally, the relevance to the broader ICLR community."
            },
            "questions": {
                "value": "(i) The extreme specificity of the methodology:\nAir traffic control is a relevant problem. However, the proposed architecture seems to be extremely tailored for this one specific application. How generalizable are these methods beyond the this application? It'd be nice to see experiments on a more diverse set of problems.\n\n\n(ii) The lack of baselines and/or experiments to validate the proposed innovations:\nThe set of baselines is extremely limited. The authors should provide additional RL-based approaches from literature or simply by implementing sensible alternative approaches to the problem.\n\nArguably, the major contribution of this work is the representations of MDP elements and the authors do a good job at motivating the reasoning behind their choices. It would be interesting to test how agnostic the proposed framework is to the choice of RL algorithm."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818271041,
        "cdate": 1698818271041,
        "tmdate": 1699636750361,
        "mdate": 1699636750361,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lwzvABGjSX",
        "forum": "z9Xb6fADe4",
        "replyto": "z9Xb6fADe4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_n3Ra"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6595/Reviewer_n3Ra"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method based on Deep Reinforcement Learning (DRL) to address the airport takeoff control problem on mixed-mode runways. The core idea of the article includes formalizing the takeoff control problem as a Markov Decision Process (MDP), using a spatio-temporal event graph to characterize the traffic density in hotspots, and adopting a continuous deferral rate action space instead of the traditional binary open/close control method. In addition, the reward function aims to encourage high taxi speeds and runway utilization, and utilizes the Proximal Policy Optimization (PPO) algorithm to train agents. This method was evaluated on simulated traffic data from Singapore Changi Airport, and the results show that the DRL strategy reduces taxi delays, fuel consumption, and conflicts compared to baseline strategies. The article emphasizes the applicability and research characteristics of its application."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Novel state representation using event graph captures airside congestion well\n\n- Pushback rate action space is more practical than on/off metering\n\n- Significant taxi delay and fuel burn reduction in experiments\n\n- Outperforms other approaches like tabu search and baseline DRL\n\n- Evaluated on realistic traffic scenarios from Singapore Airport"
            },
            "weaknesses": {
                "value": "1) Unclear how method would transfer or scale to other airports  \n2) This paper focus on the application and no new algorithms is provided. \n3\uff09Lack of comparison of other optimization and planning algorithms."
            },
            "questions": {
                "value": "- How would the policy transfer to other airports with different layouts and traffic patterns?\n\n- Could the event graph idea be used for other air traffic management tasks?\n\n- What are other ways to set the hyperparameters instead of manual tuning?\n\n- How would the policy perform with human controllers in the loop vs automation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6595/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699557427836,
        "cdate": 1699557427836,
        "tmdate": 1699636750138,
        "mdate": 1699636750138,
        "license": "CC BY 4.0",
        "version": 2
    }
]