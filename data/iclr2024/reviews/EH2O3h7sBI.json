[
    {
        "id": "2O07fbdGGk",
        "forum": "EH2O3h7sBI",
        "replyto": "EH2O3h7sBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes three steps to derive the prompt gradient projection (PGP) approach for continual learning of pre-trained models with prompt tuning. First, using the self-attention mechanism in ViT, the work deduces the gradient restriction conditions for prompt learning based on the feature and the prompt vectors. Second, to simply the solving of these conditions, the work uses a sum space of the feature and the prompt vectors and conducts Singular Value Decomposition (SVD) on this space. Finally, PGP exploits balancing plasticity and stability by rearranging the singular values split on the basis of a threshold."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Given the rising popularity of using pre-trained models for downstream tasks, this is a timely topic to study how such models can be adapted for effective CL.\n\n- Clear explanations and equations built up from scratch.\n- Reasonable experimental settings."
            },
            "weaknesses": {
                "value": "- Given that the L2P and DualPrompt backbones have been pre-trained on ImageNet, the limited performance gain on the 10-Split-TinyImageNet is a bit concerning (Table 3). Also, the gains on other dataset settings follow a similar trend. Alternatively, the work could have used a backbone trained on a different dataset altogether [1] and then evaluated its performance on the said settings. A third option would be to use other CL datasets like CUB-200, ObjectNet, etc. that have limited domain and style overlap with the ImageNet dataset.\n- The work claims that one of the major advantages of the proposed PGP method is its training time and memory cost. However, Table 2 does not report the training time of the baseline (L2P-R). As such, it is unclear what conclusions can be derived about the training time.\n- What concerns me is the scope of the proposed PGP method. While the authors use the classic L2P and DualPrompt as their baselines, it would have been more interesting to see how the proposed method complements the continual learning of more powerful pre-trained models such as vision-language models [2]. Also, the baselines considered in this paper are rather weak. Many latest methods are missing. Such as LoRA, K-Adapter, and other parameter-expansion methods.\n\nMinor comments:\n\n- Definition of V_t missing - the paper introduces it as a two part column vector (before eq. 11) without explaining what does it contain. The same for U_t (prior to eq. 10).\n\nReferences:\n\n[1] Zhou, Da-Wei et al. \u201cLearning without Forgetting for Vision-Language Models.\u201d ArXiv abs/2305.19270 (2023): n. pag.\n\n[2] Thengane, Vishal G. et al. \u201cCLIP model is an Efficient Continual Learner.\u201d ArXiv abs/2210.03114 (2022): n. pag."
            },
            "questions": {
                "value": "Please see the weaknesses. Overall, it is a good incremental contribution to the field of prompt-tuning for continual learning. However, I am worried about the limited efficacy of the proposed method as well as the scope of the reported experimental baselines. Therefore, I cannot recommend an acceptance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_LEcv",
                    "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698636716935,
        "cdate": 1698636716935,
        "tmdate": 1700544838818,
        "mdate": 1700544838818,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "YBjx4vnTnt",
        "forum": "EH2O3h7sBI",
        "replyto": "EH2O3h7sBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
        ],
        "content": {
            "summary": {
                "value": "The paper presents Prompt Gradient Projection (PGP), a novel approach that combines Prompt Tuning with Gradient Projection to address the challenge of forgetting in continual learning. Prompt Tuning reduces forgetting in class-incremental learning by selecting and updating relevant prompts based on input samples, while Gradient Projection prevents forgetting in task-incremental learning by ensuring gradient updates in orthogonal directions to old features. PGP integrates these techniques, releasing the need for task identifiers in Gradient Projection and providing theoretical guarantees against forgetting. By deducing the orthogonal condition for anti-forgetting in prompt gradients and using Singular Value Decomposition for efficient computation, PGP significantly reduces forgetting and improves accuracy in various learning settings, achieving state-of-the-art results on benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel approach, Prompt Gradient Projection (PGP), which combines Prompt Tuning and Gradient Projection to tackle the issue of forgetting in continual learning. The problem statement is well-defined, and related work is thoroughly reviewed, showcasing a strong understanding of the research landscape. The paper's strength lies in its extensive and insightful experimental studies, including ablation studies that highlight the efficiency of PGP and the influence of various factors. Furthermore, the inclusion of proofs, extensive experiments, and detailed algorithms in the appendix enhances the paper's credibility and reproducibility."
            },
            "weaknesses": {
                "value": "The clarity and readability of the current paper are areas of concern. The paper's overall structure and writing could be improved to enhance its accessibility. Several specific issues were identified:\n\nReadability and Clarity: The paper's overall readability and clarity could be enhanced. Specific instances of unclear language were noted, such as in the abstract and the last paragraph of page 3. Further revisions are needed to make these sections more lucid.\n\nFigure 2: Figure 2 was found to be unclear and challenging to follow, especially for readers seeking to grasp the method's essence. Consider revising or providing additional explanatory details to improve the comprehensibility of this figure.\n\nProof Presentation: While the proof on page 5 is well-explained up to equation 12, the subsequent section, where multiple elements are combined, is challenging to follow. Expanding on the last two paragraphs on page 5 could enhance clarity and understanding.\n\nTerminology and Consistency: In page 6, there is mention of dividing $V_t$ into $V_{t,1}$ and $V_{t,2}$\" while also referencing $V_{t,0}$\". Clarification is needed to ensure consistency and prevent potential confusion, possibly by referring back to the explanation on page 5.\n\nNotation Clarification: On page 6, there is a reference to \"$V_{t,}$\" which requires clarification to make it more understandable to the reader. Providing a clear definition or context for this notation would be beneficial.\n\nOverall, addressing these issues will significantly improve the paper's accessibility and enhance its overall quality, making it more suitable for publication."
            },
            "questions": {
                "value": "The paper raises several important points that require clarification and further exploration:\n\nReference Request: In the paragraph preceding equation 1, the statement \"If the update direction is orthogonal to the old features, it follows that $\\Delta W_t x_{t,i}=0$\" lacks a specific reference. It would be beneficial to provide a reference or additional context to support this claim and enhance the paper's credibility.\n\nTime Complexity: The paper would benefit from a more detailed discussion of the method's time complexity, along with a comparative analysis against relevant baseline methods. Understanding the computational efficiency of the proposed approach in relation to other methods is crucial for assessing its practical utility.\n\nPerformance vs. Time Complexity Trade-off: The paper mentions a maximum improvement of around 1% in reducing catastrophic forgetting in the reported experimental results. It is essential to provide a more in-depth discussion of the trade-off between the promising performance of the proposed algorithm and its associated time complexity. Explaining why this algorithm should be prioritized over other existing algorithms, considering the modest improvement, would provide valuable insights.\n\nFigure 3 Clarification: Figure 3 requires further elaboration. While it is presented that both algorithms exhibit similar patterns, with Dual-Prompt appearing to be shifted up(accuracy) or down (forgetting) by a certain scale, a more detailed explanation of this observation is necessary. Clarifying the significance of these patterns and the implications for the proposed method's effectiveness is essential.\n\nAddressing these concerns will contribute to a clearer and more comprehensive understanding of the paper's content and its contributions to the field.\n\n================================\nI appreciate the authors for offering a thorough rebuttal! \nI find this paper to be intriguing and innovative, leading me to raise my score to 8."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_epe6"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698663042084,
        "cdate": 1698663042084,
        "tmdate": 1700545903692,
        "mdate": 1700545903692,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dL2f4kGC90",
        "forum": "EH2O3h7sBI",
        "replyto": "EH2O3h7sBI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
        ],
        "content": {
            "summary": {
                "value": "The submission proposes to combine prompt-tuning based continual learning methods with gradient projection methods. The submission explains how to project the gradient of the learnable prompt tokens and prompt key such that there is no forgetting. The evaluation is performed on 3 datasets and various different continual learning setups."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Combining prompt tuning with gradient projection and therefore gaining understanding of the learnable prompts' gradient space in context of CL is novel and important\n- The gradient projection derivation is sound and clearly explained.\n- Evaluation is done on multiple datasets and settings, and the proposed method shows superiority against competitors such as L2P and DualPrompt."
            },
            "weaknesses": {
                "value": "- The comparison of prompt-tuning based CL methods with other CL methods does not seem fair to me. Prompt-tuning based methods use pretrained backbones which will give an performance edge when comparing against non-prompt-tuning based CL methods.\n\n=========== Post-rebuttal changes ========\n- The authors have resolved my concern on this particular issue."
            },
            "questions": {
                "value": "- Figure 1 is a bit unintuitive as normally, models that fill the radar chart are considered to be more powerful but in this case for FOR metrics being close to the center is better and for ACC metrics being fuller is better. Changing the FOG figure so higher is better would make the Figure more readable in my opinion.\n\n- From what I could understand, the gradients for the learnable prompt tokens and prompt keys in the prompt pool are projected. What about gradients for the classifier attached to the backbone? Are these modified as well?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission618/Reviewer_Q1LH"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission618/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698816052816,
        "cdate": 1698816052816,
        "tmdate": 1700698833470,
        "mdate": 1700698833470,
        "license": "CC BY 4.0",
        "version": 2
    }
]