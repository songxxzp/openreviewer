[
    {
        "id": "J0aYl8WDgP",
        "forum": "1OP4crhgkD",
        "replyto": "1OP4crhgkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_DBEC"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_DBEC"
        ],
        "content": {
            "summary": {
                "value": "Based on automatic subgoal generation, the authors design a sophisticated framework to perform goal generation, sub-goal assignment, and language-grounded goal-based MARL, along with effective techniques like in-context learning and self-reflection in the prompting engineering domain. The authors evaluate performance of derived policies on two benchmarks, Overcooked and MiniRTS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors compose several techniques of prompting engineering to realize subgoal generation and train goal-conditioned agents, which seems significantly improve sample efficiency. The authors well introduce multiple advancements from current LLM-agent research and provide thorough discussions about related methods.\n2. The framework the authors propose is comprehensive, though maybe too complicated, it provide a guidance of design an LLM-assisted agent.\n3. The framework, SAMA, realizes semantically useful task decomposition by generating and assign explainable subgoals, which is advantageous compared to previous goal-based methods."
            },
            "weaknesses": {
                "value": "1. The major concern is that the sophisticated design of SAMA may hinder its general use on other benchmarks. It seems that the authors create exhaustive prompts for running SAMA on these two benchmarks (Page 23-36). \n2. The accompanied concern is that the deployment of SAMA may induce high costs of calling LLMs (like OpenAI API) and training language-grounded agents. \n3. In the experiments part, the authors do not make some illustrative examples of goal generation but only provide performance curves. \n\nSome minor mistakes:\n1. In the caption of Figure 2, \"subgaol\" -> \"subgoal\". The authors should also consider unifying the term to \"subgoal\" or \"sub-goal\" as they both appear in this paper.\n2. Figure 4 (right) is too small to be read."
            },
            "questions": {
                "value": "1. Can you make a list of how many tasks are accomplished with the help of PLMs? Among them, how many can be done in offline setting and how many cannot? I think it will be better to evaluate the contributions. \n2. To hack SAMA for another environment, can you summarize how many prompts/components the practitioner should modify?\n3. How much percentage of goals and subgoals is generated in the offline manner? How many queries are needed during online training? \n4. Can you provide the financial costs and time costs of training SAMA agents?\n5. As the evaluation of Overcooked is based on self-play performance, what is the difference among those ad hoc teamwork methods (SP, FCP, and COLE)? In my opinion, the SP may be reduced to a general MAPPO algorithm. Why do ASG methods perform much worse than SP?\n6. Why do you select the MiniRTS benchmark with splitting units rather than directly evaluating on other MARL benchmarks? It seems a little wired. Meanwhile, why cannot SAMA surpass RED?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1133/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1133/Reviewer_DBEC",
                    "ICLR.cc/2024/Conference/Submission1133/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698060674125,
        "cdate": 1698060674125,
        "tmdate": 1700706752550,
        "mdate": 1700706752550,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ifae4SBWt6",
        "forum": "1OP4crhgkD",
        "replyto": "1OP4crhgkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_T35w"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_T35w"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates long-horizon, sparse reward tasks in cooperative multi-agent RL problems. The proposed method is to use the pre-trained large language model and pre-trained language-ground RL agent. The method prompts the pre-trained language model to generate potential goals, decompose the goal into sub-goals, assign sub-goals to each agent in the multi-agent setting, and replan when the pre-trained RL agents fail to achieve the goal. \n\nThe experiments are conducted on two challenging benchmarks for MARL, Overcooked and MiniRTS. The proposed method outperforms the SOTA baselines."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem of long-horizon, sparse reward tasks in multi-agent RL is super challenging and significant.\n\nIt is well-motivated to apply the large-language model, to use prior knowledge and common sense for goal generation and task planning.\n\nEmpirically, the proposed method outperforms the SOTA."
            },
            "weaknesses": {
                "value": "In general, the presentation can be improved. Since there are too many components in the pipeline, it will be better to emphasize and explain the most novel and important part in detail, rather than briefly mention each component within space limit.\n\nThe proposed method is not fully analyzed. For example, as for the reward design part, how is it accurate to determine task completion? About the pre-trained language-grounded RL agent, how does it perform the training and validation set of states and sub-goals? In the self-reflection phase, how does the task planning evolve?"
            },
            "questions": {
                "value": "Could you please clarify in Figure 5, which component of the proposed method is updated as environment steps increase? If the language-grounded RL agent is trained here, why is it called pre-trained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698346723228,
        "cdate": 1698346723228,
        "tmdate": 1699636039324,
        "mdate": 1699636039324,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GMu7ztrlUe",
        "forum": "1OP4crhgkD",
        "replyto": "1OP4crhgkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_b4Ao"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_b4Ao"
        ],
        "content": {
            "summary": {
                "value": "The authors propose Semantically Aligned task decomposition in MARL (SAMA), a method that aims to generate subgoals for MARL tasks with sparse reward signals. By taking advantage of pretrained language models, the proposed method shows to be more sample efficient during MARL training."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Generally, the paper is well written and well organized.\n\nThis paper:\n* proposes a complex method to integrate language models and MARL from a task decomposition perspective\n* shows that using prior knowledge from language models improves sample efficiency in MARL\n* provides detailed analysis regarding the language learned for the tasks"
            },
            "weaknesses": {
                "value": "* Overall, there is a big limitation from the proposed method: the approach introduced requires the prior existence of the required resources to create an accurate task manual and state action translations. This is indeed pointed by the authors in section 3.1: \"Nevertheless, linguistic task manuals still need to be made available for general multi-agent tasks. Furthermore, except for a handful of text-based tasks, typical multi-agent environment interfaces cannot provide text-based representations of environmental or agent state information and action information.\"; this makes this method very limiting\n* It is stated that the language task manual is generated from the latex code of the paper of the multi-agent task. Once again, this sounds very limiting.\n* While the proposed method shows to reduce the required samples in the tasks (Fig. 5), it requires very complex prior preprocessing to create the required text-based rules and manuals. This makes me think that it possible that the method becomes even more costly, in general, after all of this processing, despite the sample efficiency in the task.\n* From my understanding, the goal decomposition is made prior to the task, meaning that a lot of prior knowledge is required (as mentioned before). Other methods such as MASER [1] or [3] decompose the tasks on a more flexible manner, which saves a lot of potential preprocessing.\n* Throughout the paper, the authors claim several times that their method does not need to learn how to split subgoals or to generate them on the go as other methods do, reducing the sample complexity. However, the preprocessing carried needed to achieve this seems very complex and requires a lot of prior knowledge and carefuly engineered features. I wonder again whether this is really more advantageous than following the standard approaches.\n* I have concerns regarding the claims that this method addresses the credit assignment problem. Also since the authors test is environments with only two agents, this can be difficult to analyse (overcooked with 2 agents and MINIRTS with 2 agents; in related works environments with many more agents such as SMAC [2] are used).\n* In the conclusion it is stated as a limitation: \"where human rationality is immaterial or inexpressible in language or when state information is not inherently encoded as a natural language sequence.\"; Yet i believe this is a very interesting remark and would be interesting to see how a method such SAMA can be used to tackle these problems, instead of having a preset of convenient \"manuals\".\n* The authors mention throughout the paper (introduction and Fig. 2, for example) the potential for generalizability of the proposed method. However, this is not shown or further discussed, and due to the required preprocessing I fail to understand how generalization to different environments/tasks can be easily done.\n\nOverall, I think that the proposed way of integrating language models with MARL from a task decomposition perspective is interesting and can contribute to the explainability of MARL systems. However, I feel that the proposed method in this paper has several limitations and requires a very complex preprocessing. If the manuals cannot be properly generated, then it is not possible to tackle the problems. I also wonder whether the shown sample efficiency is really worth it since it needs all this preprocessing.\n\n[1] https://arxiv.org/abs/2206.10607\n\n[2] https://arxiv.org/abs/1902.04043\n\n[3] https://ieeexplore.ieee.org/document/9119863\n\n\nMinor:\n- in section 2:  \"endeavoring to parse it into N distinct sub-goals g1k, \u00b7 \u00b7 \u00b7 , gN\"; missing brackets\n- in section 3.1: \"illustrate the process in the yellow line of Figure 2\" the word figure shouldnt be in yellow; same here (purple line in Figure 2) and in the others that follow in the paper"
            },
            "questions": {
                "value": "1. To extend the proposed method to other cases, would it be possible to create a task manual and state action translation for environments that do not follow the conventions presented in this paper? For instance, in more complex environments such as SMAC [2].\n2. In section 3.1: \"For each paragraph $S^i_{para}$, we filter paragraphs for relevance and retain only those deemed relevant by at least one prompt from $Q_{rel}$.\"; how is the filtering of the relevant paragraphs done? Are they manually filtered?\n3. In overcooked, despite the method being more sample efficient during training (figure 5) we can see in figure 4-right that the testing performance of the proposed method stays below other sota methods. Is this because MARL might not be good enough for this environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1133/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1133/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1133/Reviewer_b4Ao"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698403853651,
        "cdate": 1698403853651,
        "tmdate": 1699636039244,
        "mdate": 1699636039244,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "VkDV0AAUh3",
        "forum": "1OP4crhgkD",
        "replyto": "1OP4crhgkD",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_Um1j"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1133/Reviewer_Um1j"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes the Semantically Aligned task decomposition (SAMA) framework, which aims to solve the sparse reward problem in multi-agent reinforcement learning. SAMA prompts pre-trained language models with chain-of-thought that can suggest potential goals, provide suitable goal decomposition and subgoal allocation as well as self-reflection-based replanning. Each agent's subgoal-conditioned policy is trained by the language-grounded RL method. Compared with the traditional automatic subgoal generation method, SAMA can have higher sample efficiency. This paper verifies the performance of SAMA on Overcooked and MiniRTS."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper has a clear structure, introduces the proposed method step by step, and the figures are clear and easy to understand.\n2. Experimental details are given in the appendix, which makes it easy to reproduce the experimental results. Relevant prompts are also given in the appendix, making the contribution and experimental results more convincing.\n3. The testbed chosen in this paper is very representative and challenging. The tasks in Overcooked and MiniRTS can be decomposed by common sense, and their status is relatively easy to translate into natural language. This allows the paper to better focus on how to decompose tasks and allocate subtasks.\n4. It can be seen from the experimental results that SAMA can indeed reach or exceed the performance of existing baselines, and the sample efficiency is indeed significantly higher than other baselines."
            },
            "weaknesses": {
                "value": "1. Currently, SAMA may not be applicable to an environment where human rationality is immaterial or inexpressible in language or when state information is not inherently encoded as a natural language sequence. For example, when the state space is continuous, it is difficult for SAMA to complete the state and action translation stage.\n2. PLM still has some flaws, which sometimes hinder the normal progress of the entire SAMA process. In addition, using PLM will bring additional time costs and economic costs.\n3. In some scenarios (such as Coordination Ring), although SAMA learns very quickly in the early stage, the final convergence results are still not as good as some baselines.\n4. Although in the task manual generation stage, SAMA automatically extracts critical information from the latex file or code through PLM, this is undoubtedly a relatively cumbersome process, so the cost of this stage cannot be ignored."
            },
            "questions": {
                "value": "1. Is the introduction of the self-reflection mechanism unfair to other baselines? Because other methods do not have this ability similar to \"regret.\"\n2. As can be seen from Figures 4 and 5, although SAMA has a very high sample efficiency in the early stages of training, it often converges to a local optimal solution. What is the reason for this result? Is it because the interval $k$ is too large? Do different $k$ values have different effects on the algorithm's performance?\n3. How to ensure the accuracy of PLM in the Task Manual Generation stage or the generated code snippet for assessing sub-goal completion?\n4. Is the wall time for the agent to complete a round in SAMA much different from other baselines? What is the number of tokens that need to be input to PLM in one episode?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1133/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698758040481,
        "cdate": 1698758040481,
        "tmdate": 1699636039175,
        "mdate": 1699636039175,
        "license": "CC BY 4.0",
        "version": 2
    }
]