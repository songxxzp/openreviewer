[
    {
        "id": "6vLwgmDjEe",
        "forum": "r2uhY4pXrb",
        "replyto": "r2uhY4pXrb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_Z7sX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_Z7sX"
        ],
        "content": {
            "summary": {
                "value": "The paper presents ViCo, a novel method for personalized text-to-image generation using diffusion models. This task aims to generate photorealistic images from textual descriptions without fine-tuning the original diffusion model. It utilizes an image attention module and a mask to condition the diffusion process on visual semantics. It outperforms existing models with minimal training, making it a promising solution for personalized text-to-image generation without the need for fine-tuning diffusion models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper presents an efficient mechanism to generate object masks without relying on prior annotations, simplifying foreground object isolation from the background.\n\n2. It is computationally efficient and non-parametric, reducing the influence of distracting backgrounds in training samples.\n\n3. ViCo is highly flexible and easy to deploy, as it doesn't require fine-tuning of the original diffusion model.\n\n4. The model requires no heavy preprocessing or mask annotations, making it easy to implement and use."
            },
            "weaknesses": {
                "value": "1. ViCo may have lower performance compared to fine-tuned methods, implying an ease-of-use vs. performance trade-off.\n\n2. The use of Otsu thresholding for mask binarization may slightly increase time overhead during training and inference for each sampling step. However, this is offset by shorter training time and a negligible increase in inference time."
            },
            "questions": {
                "value": "1. Can you explain in more detail how ViCo generates object masks and incorporates them into the denoising process?\n\n2. How does ViCo compare to other methods in terms of computational efficiency and parameter requirements?\n\n3. Can you explain how ViCo achieves diverse poses and appearances in recontextualization and art renditions?\n\n4. Can you clarify the limitations mentioned and how the trade-off between keeping the frozen diffusion model and not fine-tuning it affects performance?\n\n5. How does ViCo's cross-attention method for capturing object-specific semantics differ from others, and what are its advantages?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698042623762,
        "cdate": 1698042623762,
        "tmdate": 1699636394980,
        "mdate": 1699636394980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "hHn3ElJIBF",
        "forum": "r2uhY4pXrb",
        "replyto": "r2uhY4pXrb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_dE3Q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_dE3Q"
        ],
        "content": {
            "summary": {
                "value": "This work aims to achieve personalized text-to-image generation that allows users to combine inputs of texts with example images and generates an image accordingly. Existing work on this task is either computationally inefficient or sacrifices the generation quality with computation cost. The authors propose a cross-attention based mechanism, which also has the benefit of helping isolate the foreground object using attention maps, that requires only training the cross-attention layers and optimizing the placeholder embedding. The proposed method is more efficient without sacrificing the reference object's identity."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The goal of this paper is to achieve personalized text-to-image generation that is lighter-weight (and faster) than existing methods with an on par quality or even better. Quantitative and qualitative results presented seem to support this."
            },
            "weaknesses": {
                "value": "How the proposed method avoids fine-tuning the entire diffusion model for each reference object is by using cross-attention: multi-resolution features maps of the reference image, C_I^l, are used to perform cross-attention with the intermediate outputs, n_t^l, of the main denoising UNet. With cross-attention blocks, they only train these blocks for each reference object, instead of the entire diffusion model. Using cross-attention blocks in conditioned image generation to warp a source image to target [i, iv, v, vi, vii] or to preserve a reference image's identity [ii, iii] has been a popular approach. Indeed, this may be one of the first work to explore cross-attention blocks in LDMs, but I don't think this contribution seems sufficiently novel.\n\n\n[i] Bhunia, Ankan Kumar, et al. \"Person image synthesis via denoising diffusion model.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[ii] Zhu, Luyang, et al. \"TryOnDiffusion: A Tale of Two UNets.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[iii] Karras, J., Holynski, A., Wang, T. C., & Kemelmacher-Shlizerman, I. (2023). Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint arXiv:2304.06025.\n\n[iv] Mallya, Arun, Ting-Chun Wang, and Ming-Yu Liu. \"Implicit warping for animation with image sets.\" Advances in Neural Information Processing Systems 35 (2022): 22438-22450.\n\n[v] Liu, Songhua, et al. \"Dynast: Dynamic sparse transformer for exemplar-guided image generation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[vi] Ren, Yurui, et al. \"Deep image spatial transformation for person image generation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.\n\n[vii] Tseng, Hung-Yu, et al. \"Consistent View Synthesis with Pose-Guided Diffusion Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "After reading both the main paper and supplementary sections, I'm still not 100% clear of the training procedure, which also raises questions for me regarding the results presented. Currently my understanding is that training is conducted for each object separately (in Sec.3.4 it reads \"We train our model on 4-7 images with vanilla diffusion U-Net frozen...\"), and during training, the learning of cross-attention layers and placeholder text embeddings S* are performed simultaneously (as described in Sec.3.4). If my understanding is correct, the question I have is: would the model learn better if cross-attention layers are trained with all available images (from all objects, or with any larger dataset where there are many objects, each with at least 2 images), and S* is optimized for each object? I have this question because when looking at the qualitative results, I think the preservation of the reference image's identity could be further improved, e.g., in Figure 1, the Batman toy's body pose changed in Figure 1, and in Figure 4, the cat statue's face changed, the texts on the can changed, and the drawing on the clock also changed. One possibility I can think of for why the reference image's identity is not perfect is that the cross-attention layers are not fully trained, and training with a larger dataset with a wider variety of objects may help. \n\nAnother question I have is regarding where to incorporate cross-attention blocks. From Supplementary Section A, it mentions that the final design was incorporating cross-attention blocks in the decoder of the UNet. Existing work  [i, ii] that also use cross-attention incorporate it in both the encoder and decoder of the UNet. I wonder if this configuration was tried, and if yes, why it wasn't successful in this case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698630814842,
        "cdate": 1698630814842,
        "tmdate": 1699636394905,
        "mdate": 1699636394905,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "T1d5iXSpAz",
        "forum": "r2uhY4pXrb",
        "replyto": "r2uhY4pXrb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_cbrM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_cbrM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for customizing text-to-image generation models, which requires less number of parameters to be tuned and less training time compared to related works."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper proposes to introduce extra attention modules, which can introduce new concept into the diffusion process. The introduced attention modules contain much less parameters compared to the whole diffusion model, leading to more efficient fine-tuning.\n\nOnly fine-tuning introduced attention modules have the advantage of maintaining the original capability of pre-trained models, which might be important.\n\nAccording to the experiment results shown in the paper, better results are obtained compared to vanilla DreamBooth. The needed training time is also much less."
            },
            "weaknesses": {
                "value": "The major concern is on the experiments, why do the authors only use 20 unique concepts from the Textual Inversion, DreamBooth, Custom Diffusion, rather than use a union of their testing samples or directly use the DreamBench dataset proposed in DreamBooth paper?\n\nOne important related work is missing [1], which requires fine-tuning less number of parameters compared to LoRA, and maintains the original capability of the pre-trained model. As shown in the paper, the method is also very stable (please see question in next section).\n\nThe low $T_{CLIP}$ score may indicate unsatisfactory edit-ability, thus more qualitative results in terms of complicated style change are suggested.\n\nAlthough encoder-based methods are not directly related to the proposed method, comparison and discussion are strongly suggested. Especially considering the fact that encoder-based methods normally require much less fine-tuning time or are even tuning-free (although they need pre-training).  \n\n[1]. Controlling Text-to-Image Diffusion by Orthogonal Finetuning. Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Sch\u00f6lkopf."
            },
            "questions": {
                "value": "Have the author considered using ground-truth mask in computing the diffusion loss (with a pre-trained model like SAM[1]), or forcing the attention map to be aligned with the mask?\n\nIn DreamBooth, when the model is fine-tuned for too much iterations, the model performance may degenerate even when augmentation data and prior loss is used. Will this also happen with the proposed method? Specifically, generated results with respect to different iterations are suggested to be shown, especially when the number of training steps are very large. This result is important as related work OFT is shown to be stable even after thousands of fine-tuning steps. \n\n[1]. Segment Anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698827253960,
        "cdate": 1698827253960,
        "tmdate": 1699636394836,
        "mdate": 1699636394836,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JhZkK9162f",
        "forum": "r2uhY4pXrb",
        "replyto": "r2uhY4pXrb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_xPT1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4274/Reviewer_xPT1"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a text2image personalization method that learns the personalization text embedding and the proposed image attention. \"Image attention\" is a cross-attention module to integrate visual conditions into the denoising process for capturing object-specific semantics. A mask that is derived from the cross-attention map between reference image and text is applied to the \"image attention\" used to focus more on the object of the reference image."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The method only requires learning relatively few parameters to effectively incorporate the information from reference image for personalized text2image generation.\n- The results are very favorable compared to existing methods like DreamBooth and Textual Inversion, while having a low training time cost.\n- Good ablation study and analysis provided in the paper.\n- The paper is quite transparent and information-rich in many ways, which is good for reproducibility purposes."
            },
            "weaknesses": {
                "value": "- In Table 4, the improvement introduced by masking is not so significant. \n- The method is incapable of using multiple reference images during inference, for more robust generation.\n- Apparently, the method only works with images that have a single reference primary object."
            },
            "questions": {
                "value": "1. Do you use the same reference image for different model variations in the evaluation (especially T4)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4274/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698902242649,
        "cdate": 1698902242649,
        "tmdate": 1699636394758,
        "mdate": 1699636394758,
        "license": "CC BY 4.0",
        "version": 2
    }
]