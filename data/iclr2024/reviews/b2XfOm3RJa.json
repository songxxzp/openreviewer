[
    {
        "id": "VFHXBGFenw",
        "forum": "b2XfOm3RJa",
        "replyto": "b2XfOm3RJa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_Cn8C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_Cn8C"
        ],
        "content": {
            "summary": {
                "value": "This paper makes an important contribution to our understanding of LLMs and how they work. Using reference datasets, this paper looks at the behavior of the model under perturbations of the input data to determine paths and attention heads that are used as parts of reasoning processes. The specific perturbations they use focus on constructing counterfactual examples."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "The biggest strength of this paper is the question they are asking. While many authors are focused on improving model performance on reasoning tasks, this paper focuses on understanding the internals of the model and advancing the science of the models themselves. The paper was also very strong in its data processing and creation, which was a novel and original reuse of existing data\n\n- Originality: There are two particularly original aspects to this paper. First, the use of reasoning datasets to provide counterfactual examples for models is an original and useful idea. Second \n\n- Quality: Tests were well thought out and explained, datasets were relevant and well chosen. The use of ablation/knockout methods to really focus and prove claims about model performance was particularly nice. \n\n- Clarity: By addressing a hard, technical topic, this paper did not set itself up for success on clarity, however the paper is well written with no major flaws in style or content. \n\n- Significance: As previously stated, the significance of this paper is that it is advancing the science of how LLMs work, rather than improve their performance while punting on the basic understanding of how they work."
            },
            "weaknesses": {
                "value": "There are two basic weaknesses of this paper. First is clarity, which, as mentioned above, this is an area that it is difficult to be clear in because of the technical nature of the content.  Second, is the generality of the claims they make.\n\nRegarding the generality of the claims, the weakness of this comes from using limited data and reference models. This paper makes claims about LLMs at large, based on two example LLMs. I would like to know why these two models are representative for LLMs at large and why results from these two models are expected to generalize. Even better would be some claims about what classes of models these results are expected to apply to."
            },
            "questions": {
                "value": "Suggestions:\n\n- Please fix the citation on page 3 for HuggingFace.\n\n- I would also like to see a specific discussion section that pulls together all the results into a summary of what you learned. Right now, that content is spread across a lot of the experimental section, so I'd suggest consolidating it under its own section and highlighting the valuable lessons learned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698180204129,
        "cdate": 1698180204129,
        "tmdate": 1699636476026,
        "mdate": 1699636476026,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B5IJ6ADwWN",
        "forum": "b2XfOm3RJa",
        "replyto": "b2XfOm3RJa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates how chain-of-thought (CoT) prompting enhances reasoning capabilities in large language models (LLMs). The key findings are:\nAdjusting the few-shot examples in a CoT prompt is an effective way to generate paired inputs that elicit or suppress reasoning behaviour for analysis. Only a small fraction of attention heads, concentrated in middle and upper layers, are critical for reasoning tasks. Ablating them significantly harms performance. The authors show that some heads focus on the final answer while others attend to intermediate reasoning steps, corresponding to the two stages of CoT."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper:\n- Provides novel insights into CoT reasoning through attention-head analysis. \n- Links model components to reasoning subtasks."
            },
            "weaknesses": {
                "value": "This paper:\n- Focuses only on textual reasoning tasks, not more general capabilities.\n- Limited to analyzing attention heads, does not cover other components like MLPs.\n- Does not modify training to directly improve reasoning abilities."
            },
            "questions": {
                "value": "Do you think these findings would transfer to more open-ended generative tasks beyond QA?\n\nDid you consider any changes to the model architecture or training process to improve reasoning?\n\nCould your analysis approach help detect if a model is just memorizing vs. logically reasoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_6hxZ"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698735449659,
        "cdate": 1698735449659,
        "tmdate": 1699636475945,
        "mdate": 1699636475945,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ftjGwQ1LGB",
        "forum": "b2XfOm3RJa",
        "replyto": "b2XfOm3RJa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to explain the CoT reasoning ability of LLMs by identifying \"important\" attention heads that \"contribute the most\" to the predictions. Specifically, the paper first constructs counterfactual samples for every referential few-shot CoT sample by replacing the CoT texts in the few-shot examples with random texts generated by ChatGPT. The paper then adopts a method developed in prior work to assign an importance score for every attention head in the LLM. \n\nThe authors discover that only a small fraction of the attention heads are important to the CoT task. They also discover that attention heads have different roles: some are responsible for verifying the answer and some are used to synthesize the step-by-step behavior of CoT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Understanding the behavior of LLMs is an important topic that could lead to more robust and trustworthy deep-learning models. This paper focuses on demystifying the chain of thought behavior, which is a practically useful and widely studied phenomenon of LLMs. The paper focuses mainly on identifying important attention heads, which could lead to a better understanding of the attention mechanism employed by Transformer models. Interesting observations regarding the attention patterns and different roles of every attention head have been made."
            },
            "weaknesses": {
                "value": "My primary concern is that the methodology used in the paper is not tailored to understanding CoT behaviors. Specifically, the method used to identify \"important\" attention heads is adopted from prior work (Wang et al. (2023) cited in the paper). On the method side, the only task-specific design is how to construct the counterfactual sample $x_c$ given a reference sample $x_r$, which is done by replacing the CoT part in the few-shot example prompt with some randomly generated text (by ChatGPT). It would need more justification why the important attention heads identified by $x_c$ generated in this way contribute to the CoT behavior since (i) it is possible that a (simple) adversarial change in the prompt could significantly decrease the accuracy; (ii) in $x_c$, since the CoT reasoning demonstrations are removed, the LLM would by default not using CoT, which explains the drop in the accuracy; (iii) it would be nice to design the counterfactual example in some other ways, e.g., add incorrect (but still relevant) CoT demonstrations.\n\nAdditionally, the score/importance of every attention head is scored by the accuracy drop when substituting its output with the corresponding activations generated by the counterfactual examples. Since $x_r$ and $x_c$ could differ significantly, the paper fails to justify whether replacing the activations directly will have some deteriorating effects on the LLM, since it completely \"block\" the information flow. This makes it harder to justify the conclusions made in the paper."
            },
            "questions": {
                "value": "The proposed method does not seem to have specific designs for understanding CoT.\n\nWill directly replacing the activations of certain attention heads have deteriorating effects on the LLM?\n\nPlease refer to the weakness section for more information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_bDfK"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698741287526,
        "cdate": 1698741287526,
        "tmdate": 1699636475864,
        "mdate": 1699636475864,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cJG20XUIbV",
        "forum": "b2XfOm3RJa",
        "replyto": "b2XfOm3RJa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
        ],
        "content": {
            "summary": {
                "value": "The reasoning processes of the LLaMA2-7B and Qwen-7B models are interpreted using the path patching method (initially introduced in [1], which is an interoperability method rooted in causal intervention), in the context of using few-shots prompts. The evaluation is grounded on three benchmarks: StrategyQA, AQuA, and CSQA (which have all been introduced in previous publications). They find that only a small number of attention heads are responsible for reasoning, and they also find that they are located in specific locations in the model's architecture.\n\nThis represents a solid effort to interpret large language models using the path patching method to date and is the first paper where the chain-of-thoughts method is interpreted using the path patching method. \n\n[1] https://openreview.net/pdf?id=NpsVSN6o4ul"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Carrying out this piece of research requires handling several different technical aspects (carefully constructing the datasets to allow counterfactual evaluation, applying the path patching method, and evaluating the effects of knocking out different attention heads), which the authors seem to largely have done well, including paying attention to subtle issues like the choice of right metric (section 4.4.2).\n\nTheir findings highlight interesting behaviors that the attention heads display, which is summarized in section 4.3, for example: \"_Analogically,\nthe function of Head 18.30 and 13.16 corresponds to the two stages of the chain-of-thought (CoT) process: firstly think step-by-step to get intermediate thoughts, then answer the question based on these thoughts._\"         \n\nI think such research will become more widely spread in the future in order to understand the reasoning processes of attention-based models better, and as such, it is very timely work."
            },
            "weaknesses": {
                "value": "- the presentation is at times unclear (see the questions section). I found it quite hard to read and had to spend some time understanding their methodology\n- the literature review section could be more comprehensive: a number of other articles on interpretability rooted in causal interventions exist on models of similar sizes, such as [2,3], the latter also using a 7B model but not being cited. I would recommend the authors contrast the existing approaches in the \"_Related Work_\" section or an appendix to that section, such as from [3] -but there are also other articles- with their own and comment upon similarities and differences so that the reader is well-informed of how their methods compete with existing ones.\n- the improvements are nice but somewhat incremental since a single in-context learning technique is analyzed on just two models.\n- their methodology might also be improved by the use of diagrams to show in a single glance all the relevant information\n- it's somewhat strange that Appendix B (\"_Reference Data Examples_\") is empty; why include it in that case?\n\n\n[2] https://arxiv.org/pdf/2305.00586.pdf     \n[3] https://arxiv.org/pdf/2305.08809.pdf"
            },
            "questions": {
                "value": "Tables 1, 2 / 4, 5 are unclear and suffer from a number of issues:\n- Table 1, 2: A reader might at first be confused whether what is shown is the complete few-shot that is supplied to the model that is to be tested (e.g., LLaMA2) to facilitate in-context learning - or if what follows after the \"A\" is the answer of LLaMA2/Qwen?\nThat the former is correct transpires indirectly only from the text: \"_The outputs of LLaMA-7B and Qwen-7B on the counterfactual data are shown in Table 4 and 5, respectively._\" I would recommend adding at least a caption here, explaining more clearly what can be seen, without having to look in the text for the meaning of the table.\n- In Table 1,2, the last question is \"_Can Reiki be stored in a bottle?_\", which led me to believe that this is the question the model should have answered (once with the correct in-context learning text and once with the modified/colored text as indicated from table 2).     \nBut a look in the appendix at Table 4 suddenly reveals a different last question, \"_Would a rabbi worship martyrs Ranavalona I killed?_\" (all else being the same), which I found confusing. Can the authors explain this?\n- minor formatting: \"Q\" and \"A\" from Table 1,2 are set in bold but not in Table 4,5, which does not aid readability.\n- since these tables seem an essential part of the paper, as the capture the methodology, it is tiring to go back and forth between Tables 1,2 and the Appendix; I would propose to move all Tables to the main body to aid readability.\n\nIn the section \"Counterfactual data generation\" you say: \"_x_c is generated by partially editing x_r with the purpose of altering the model\u2019s reasoning effects. In order to suppress the reasoning capability of the model while maintaining equal lengths for counterfactual data and   reference data, we replaced the evidence in x_r with irrelevant sentences describing natural scenery, which were randomly generated\nusing ChatGPT-3.5._\"\nNo mention of a specific dataset is being made. Shouldn't the change that you made account for the specific structure of the dataset? I am guessing if some dataset actually deals with natural scenery, then inserting random natural scenery descriptions might not achieve the desired counterfactual effect. Some statement should be made here that what is randomly included matches the dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "(not applicable)"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4909/Reviewer_wEdy"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4909/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773486636,
        "cdate": 1698773486636,
        "tmdate": 1699636475792,
        "mdate": 1699636475792,
        "license": "CC BY 4.0",
        "version": 2
    }
]