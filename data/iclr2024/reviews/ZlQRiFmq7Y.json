[
    {
        "id": "gTETvOfQqq",
        "forum": "ZlQRiFmq7Y",
        "replyto": "ZlQRiFmq7Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_LxNZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_LxNZ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces an innovative approach called Vocabulary Disentangled Retrieval (VDR) for learning disentangled representations using natural language as a supervisory signal. VDR employs a bi-encoder architecture that is trained on data-text pairs, where a disentanglement head maps dense representations into a disentangled space. This space is structured such that each dimension correlates with a token from the BERT tokenizer's vocabulary, creating an interpretable, token-level representation of data.\n\nThe authors present two novel sparsification techniques to refine these disentangled representations: top-k sparsification and nonparametric sparsification using normalized binary bag-of-words. These methods are designed to maintain the interpretability of the representations while enhancing retrieval performance.\n\nExtensive experiments across 15 benchmark datasets, encompassing text-to-text and text-to-image retrieval tasks, demonstrate that VDR significantly outperforms comparable bi-encoder retrievers and existing baselines.\n\nBy utilizing sparse embeddings and a vocabulary-defined space without complex or computationally intensive methods, the paper's proposed approach provides a more interpretable and efficient solution for embedding-based retrieval across various domains."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The approach provides interpretable representations that are robust for retrieval tasks, though not necessarily state-of-the-art. The quality of the work is reflected in its thorough experimental setup, with evaluations that cover both document and image retrieval tasks.\n\nThe clarity of the paper is another strength, as the paper is well-written and easy to understand. This clarity extends to the simplicity and ease of implementing the proposed retrievers, which also generalize to different applications such as text-to-text and text-to-image retrieval tasks.\n\nIn terms of significance, the model demonstrates a strong performance in zero-shot retrieval tasks. The VDR-bag of words, in particular, is competitive with the more complex and parametric VDR-k, offering better latency and making it appealing for low-resource applications. The encouraging results on BEIR benchmark and cross-modal benchmarks underscore the impact of the work, suggesting that the method could be further improved by integrating it with other orthogonal approaches."
            },
            "weaknesses": {
                "value": "In my opinion, the biggest concern with this paper is that its main innovation being the use of vocabulary as a basis for disentanglement, which is not convincingly demonstrated to enhance explainability or offers significant improvements.\n\nThe focus of the methodology and experiments appears to be on training effective retrievers using disentangled representation in the vocabulary space. However, this seems to diverge from the initial motivation, which emphasizes using natural language to guide disentanglement learning. This inconsistency complicates the narrative flow and understanding of the paper\u2019s objectives.\n\nIn terms of empirical evaluation, while the model demonstrates improved performance in text-to-text and cross-modal retrieval tasks, there is a lack of analysis on non-retrieval tasks. This omission limits the understanding of how the disentanglement contributes to broader applications beyond retrieval."
            },
            "questions": {
                "value": "The authors use elu1p activation as a replacement for ReLU. Do you have experimental results that motivate this decision?\n\nDo the authors have any intuition as to why their approach works better than DPR? It is not clear to me why dense representations would perform much worse than the disentangled representations based on the vocabulary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698956881539,
        "cdate": 1698956881539,
        "tmdate": 1699636623398,
        "mdate": 1699636623398,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "7yHVHy55g0",
        "forum": "ZlQRiFmq7Y",
        "replyto": "ZlQRiFmq7Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_LJaD"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_LJaD"
        ],
        "content": {
            "summary": {
                "value": "This work introduces Vocabulary Disentangled Retrieval (VDR), a method that aims to take a retrieval-based approach to achieve disentangled representation learning. VDR supports parametric and non-parametric inference. The latter option does not require a neural network during inference, decreasing the computation speed, making it suitable for low-resource scenarios. The method is tested on text-to-text retrieval and cross-model retrieval tasks. The baselines are split into primary and advanced baselines, and VDR outperforms many of the primary baselines in terms of NDCG and Recall. A human evaluation on 20 images, and a case study with 6 images is presented to evaluate the disentanglement of VDR."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed method, VDR, supports nonparametric inference, which increases the inference efficiency. This makes this method an interesting option in lower resource settings (given budget for training).\n- The authors address an important problem, and their method could be used to help users interpret the results of retrieval system."
            },
            "weaknesses": {
                "value": "- I am mostly concerned about the evaluation methods used, specifically: \n\n(1) The retrieval baselines are split into primary baselines and advanced baselines. The authors position themselves in the primary group, arguing that their method does not need advanced techniques as in the advanced baseline group. I am not sure how 'advanced' is defined here, and thus whether this is a a fair distinction. This is important, as VDR outperforms the primary baselines, but does not necessarily outperform the advanced baselines. \n\n(2) One of the most important aspects of VDR is the disentangled representation. This is evaluated with a small case study of only 6 images (and some more in the appendix) and a small scale human evaluation. Except for the small size of the data samples, it is also not clear how the images were selected. Are these cherry picked, or random? There is also little information about the setup of the human evaluation, making it difficult to judge these results. Given that this is such an important part of the paper, I would have expected a larger scale evaluation.\n\n- The paper makes a couple of unsubstantiated claims, most notably:\n\n(1) Page 2: \"In practice, individuals employ natural language to interpret and distinguish objects.\" There is no source to back up this claim, and it is also not entirely clear what the authors mean by this, for example, babies can distinguish objects, before they can speak?\n\n(2) The definition of Information Retrieval on page 3 is very narrow. It does not follow the broader definition given in Manning, 2009, which the authors cite. The other source, Zhao et al., 2022, is a survey paper on dense text retrieval methods, which is also only a tiny fraction of the entire field of Information Retrieval.\n\n- Minor detail: the authors cite Kingma & Ba (Adam) for AdamW, instead of Loshchilov & Hutter."
            },
            "questions": {
                "value": "- The current work focusses on disentanglement, and I am wondering what the authors consider to be the difference between disentanglement and grounding in this case?\n\n- The authors claim that VDR outperforms the baselines with a significant margin, can the authors give the results of a significance test that backs up this claim?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5882/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5882/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5882/Reviewer_LJaD"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699314621574,
        "cdate": 1699314621574,
        "tmdate": 1699636623299,
        "mdate": 1699636623299,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aOkpRFU6z1",
        "forum": "ZlQRiFmq7Y",
        "replyto": "ZlQRiFmq7Y",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_fuH7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5882/Reviewer_fuH7"
        ],
        "content": {
            "summary": {
                "value": "1. The paper presents a novel approach for learning disentangled representations for text-to-text and text-to-image retrieval by leveraging an LM vocabulary as the distinct units of latent representation.\n\n2. Concretely, the paper proposes casting both the query representation and the passage representations to a |V| dimensional space, with each value in the vector indicating the token's relevance to the input. This is then passed through a gating function to obtain the final (sparse) representation for each input. The models are trained end to end in a contrastive learning framework.\n\n3. In order to extend the approach for multimodal retrieval scenarions, the authors introduce the following modifications:\n\n3.1 Leveraging elu1p + top-k instead of ReLU to control for the number of sparse tokens in the representation and ensure the number of activated units are deterministic.\n\n3.2 A non-parametric entry loss between the BoW representation of the query and the encoded representation of the data, thereby encouraging the model to avoid learning irrelevant co-activations.\n\n3.3 In order to encourage activation of tokens that are in-frequent, the authors propose a contrastive mask, that activates a fixed fraction of activations within a batch, but discards their contribution while computing similarity between the positive samples; thereby encouraging the units to participate in the loss.\n\n4. The proposed method demonstrates strong performance on both text-to-text retrieval (when trained on MS MARCO and tested on BEIR), as well as multimodal retrieval (when trained on YFCC15M and tested on ImageNet, COCO Captions, and Flickr30k).\n\n4.1 In addition to that, for text only retrieval, leveraging a BOW representation of the query (thereby removing the need for a query encoder) achieves good retrieval performance while being up to 10x faster.\n\n4.2 For the scenario of image disentanglement, based on human evaluation, for 92% cases the annotators found the disentangled representations to be a satisfactory representation of the image, compared to 85% when using a state-of-the-art image captioning model"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is well presented, I quite enjoyed reading it. The authors do a good job of presenting the approach in a clear manner. The idea of using the vocabulary from the tokenizer of a pre-trained model as the discrete latent units is quite neat. \n2. The proposed modifications (w.r.t the elu1p + top-k activation, the non parametric entry loss and the contrastive mask) are well motivated.\n3. The paper presents strong results for both text-to-text and text-to-image retrieval. For text-to-text, the results using non-parametric inference are particularly surprising and impressive, and potentially is generally applicable for tasks requiring fast inference. For the text-to-image retrieval scenario, both human evaluation as well as the qualitative analysis presented does establish the core premise of the paper of being able to leverage a text vocabulary for learning discrete latent representations."
            },
            "weaknesses": {
                "value": "1. Since the latent representations presented are tied to an underlying vocabulary, that inherently limits the kind of representations that the model can potentially learn. Because most of the modern day tokenizers are learned based on statistical properties, especially for models with larger vocabulary sizes (where each unit itself may not represent a meaningful entity), the representations learned may not be interpretable. The authors do showcase this limitation in one of the failure cases in the Appendix (w.r.t the tokenization of the word Giraffe).\n\n2. The human evaluation setup for BLIP captioning is artificially detrimental to the image-captioning model. Specifically, the evaluators extracted 5 tokens from the caption to best reflect the image. However, the criterion of using top-k discrete tokens is a limitation of the proposed method and thus should not be imposed on the image captioning model. At the very least, using the complete caption should also be used (potentially as an upper bound for the task of which set of tokens better describe the image)."
            },
            "questions": {
                "value": "1. How stable is the proposed approach to increasing the vocabulary size ? Concretely, would the performance be impacted if the underlying text encoder was XLM-Roberta (with a vocabulary of 250k)? \n2. On a similar line of thought, what do the authors think about the feasibility of being able to decouple the vocabulary of the latent vector and the vocabulary used by the query encoder ? Concretely, would it be possible to have an XLM-Roberta encoder with the latent vectors being represented by a BERT vocabulary ?\n3. For the text-to-text retrieval scenario, does the non-parametric inference still work without using the non-parametric entry loss ? \n4. Would it be possible to elaborate more on how the analysis for Figure 4 (b) was done ? Is it considering only the patch embeddings for the max pooling operation in the DST head ? \n\nMinor Typographic issues\nAbstract: Our approach employ a bi-encoder -> our method employs a bi-encoder\nAbstract: Moreover, The result -> Moreover, the result\nPage 2: real-world data are -> real world data is\nPage 4: to the tokens exist -> to the tokens that exist"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5882/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699328726318,
        "cdate": 1699328726318,
        "tmdate": 1699636623191,
        "mdate": 1699636623191,
        "license": "CC BY 4.0",
        "version": 2
    }
]