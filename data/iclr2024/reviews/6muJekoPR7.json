[
    {
        "id": "twNnV69wil",
        "forum": "6muJekoPR7",
        "replyto": "6muJekoPR7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_P9fg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_P9fg"
        ],
        "content": {
            "summary": {
                "value": "The paper discusses the challenge of implementing prompt-based backdoor attacks via few-shot prompt-tuning due to issues like imbalanced poisoned datasets and overfitting. To address this, the authors propose TrojFSL, a method that comprises three modules aimed at executing backdoor attacks in a few-shot prompt-tuning setting. TrojFSL reportedly improves the Attack Success Rate (ASR) and Clean Data Accuracy (CDA) significantly across various PLMs and downstream tasks compared to previous methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper addresses a novel and significant issue in the field of backdoor attacks in NLP. It addresses the challenges of backdoor design in few-shop prompt-pruning, like the imbalanced poisoned dataset and overfitting issue.\n- This paper provides extensive evaluation results.\n- Overall, this paper is easy to follow."
            },
            "weaknesses": {
                "value": "- This paper only considers syntactic triggers. However, the generality of the proposed method with respect to different trigger types remains underexplored. If the method is indeed trigger-agnostic, it is imperative that an evaluation is conducted to demonstrate its effectiveness across a broader spectrum of triggers.\n\n- The authors claim that the target class is susceptible to receiving a larger number of input samples compared to other non-target classes, subsequently leading to a low CDA. Intuitively, this can be balanced by setting the poisoning ratio $\\alpha$. Setting an appropriate poisoning ratio can achieve a good CDA and ASR.\n\n- The paper falls short in elucidating some of the experimental settings, particularly when benchmarking TrojFSL against previous works. The absence of a detailed experimental setup undermines the reproducibility and the clarity of comparative analysis. Additionally, there is a noted inconsistency with the results of the referenced paper [1], where BadPrompt reportedly attains a 100% ASR with merely two poisoning examples on SST-2. An explanation of this discrepancy, along with a thorough delineation of the experimental setup, would bolster the comparative narrative.\n\n- The discussion on defense strategies is somewhat not enough. While the authors posit that RAP and ONION are ineffectual against TrojFSL when utilizing invisible syntactic triggers, the efficacy of these defenses under alternative trigger patterns employed by TrojFSL remains unexplored. Moreover, the consideration of elementary adaptive defenses, such as rephrasing the input, could offer a more comprehensive insight into the defense landscape against the proposed attack.\n\n[1] Cai, Xiangrui, et al. \"Badprompt: Backdoor attacks on continuous prompts.\" Advances in Neural Information Processing Systems 35 (2022): 37068-37080."
            },
            "questions": {
                "value": "- Is the attack trigger-agnostic?\n- Why does the performance of previous works (e.g.,  BadPrompt), as reported in this paper, not align with the results presented in the original paper?\n- Is TrojFSL effective in adaptive defense mechanisms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698029215713,
        "cdate": 1698029215713,
        "tmdate": 1699636412424,
        "mdate": 1699636412424,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aSMjtVuzs4",
        "forum": "6muJekoPR7",
        "replyto": "6muJekoPR7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_x2RS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_x2RS"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a backdoor attack against LLMs for few-shot learning. In particular, a training loss for backdoor learning is proposed with weight-balancing, token masking, and trigger-attention optimization. The proposed method exhibits high ASR on various datasets for various models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The paper is generally well-written.\n\n* The design of the method is well-motivated."
            },
            "weaknesses": {
                "value": "* Lack of comparison with baselines.\n\nThe proposed method is only compared with the baselines on one dataset for one model. The performance of the baseline is clearly worse than the results reported in the original paper (e.g. for PPT).\n\n* Omission of existing works.\n\nThe backdoor attack in [1] does not require changes to the PLM. It also requires no access to the PLM, which is more practical than the proposed method against state-of-the-art LLMs. So the statement that \"no prior prompt-based backdoor can be implemented via few-shot prompt-tuning with frozen PLMs\" in the paper is incorrect.\n\n[1] Wang et al, DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models, 2023.\n\n* The experiments regarding weight balancing should be reconsidered.\n\nCurrently, the experiments are conducted on binary classification tasks which are simple for reweighting. It is more informative (and convincing) to consider SST-5 with more classes and show that the intuition behind weight-balancing holds.\n\n* Other Incorrect statements.\n\nFor example, \" the adversary must collect some input samples belonging to the non-target classes and change their labels to the target class\" is incorrect, given there is a clean-label backdoor attack [2] and a handcrafted backdoor attack [3].\n\n[2] Turner et al, Clean-Label Backdoor Attacks, 2020.\n[3] Hong et al, Handcrafted Backdoors in Deep Neural Networks, 2021.\n\n* Minor issues\n\nThere are typos in the captions of Tables 4 and 5 regarding the dataset."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698270549160,
        "cdate": 1698270549160,
        "tmdate": 1699636412331,
        "mdate": 1699636412331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "QIVCkMs2zV",
        "forum": "6muJekoPR7",
        "replyto": "6muJekoPR7",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_oqR6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4392/Reviewer_oqR6"
        ],
        "content": {
            "summary": {
                "value": "The author introduces TrojFSL, a prompt-tuning technique for conducting backdoor attacks, comprising three modules: balanced poison learning, selective token poisoning, and Trojan-triggered attention. Experiments across various downstream tasks demonstrate that TrojFSL significantly outperforms previous works in terms of attack success rate."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The motivation behind this study is clear, and the writing is articulate.\n- The research on few-shot backdoor attacks in the context of large language models holds significant real-world relevance.\n- The paper identifies limitations in prior works, providing valuable insights for subsequent attack designs."
            },
            "weaknesses": {
                "value": "- The limitations in technical innovation. While the paper outlines existing issues faced by few-shot attacks, such as an imbalanced poisoned dataset, overfitting, and lack of attention awareness, the proposed methods appear to be a combination of various tricks without offering new technical insights. While the effectiveness of the technique is acknowledged, it lacks novelty in terms of technical approaches.\n- Excessive hyperparameters requiring adjustment. The three enhancement strategies introduced in the paper require varying degrees of hyperparameters. For instance, adjusting $\\beta$ and $\\lambda$ in the balanced dataset, controlling token mask $\\gamma$ to mitigate overfitting, and managing attention loss updates for attention awareness. The introduction of numerous parameters complicates the tuning process, making practical application challenging.\n- The effectiveness of the proposed method in a black-box setting is not addressed. Current large language models are typically accessed through APIs, raising questions about whether the proposed technique can achieve efficient few-shot attacks in a black-box scenario. If not, it is essential to provide necessary discussion and explanations.\n- Lack of comprehensive ablation experiment results. Table 6 only presents linear combination results of different strategies. More diverse combinations should be provided to help readers understand the specific effects of each strategy or their combinations. Since the proposed method involves multiple hyperparameters, it is crucial to conduct ablation experiments on more representative datasets to explore the impact of these parameters."
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4392/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4392/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4392/Reviewer_oqR6"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4392/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698671873388,
        "cdate": 1698671873388,
        "tmdate": 1699636412250,
        "mdate": 1699636412250,
        "license": "CC BY 4.0",
        "version": 2
    }
]