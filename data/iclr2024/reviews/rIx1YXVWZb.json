[
    {
        "id": "O0mjSDChA0",
        "forum": "rIx1YXVWZb",
        "replyto": "rIx1YXVWZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the interpretability of Transformers. The authors focus on the 5-digit number addition task and analyze how a one-layer Transformer model finishes this task. To understand the model clearly, they propose a mathematical framework for integer addition, consisting of five tasks: Base Add, Make Carry 1, Make Sum 9, Use Carry 1, and Use Sum 9. The first three tasks can be independently executed for each digit pair, representing the sum of two digits modulo 10, checking for carry, and determining if the addition results in 9, respectively. The last two tasks chain operations across digits, respectively denoting adding the previous column's carry to the sum of the current digit pairs, and propagating a carry when Make Sum 9 and Use Carry 1 are true. The authors then analyze the one-layer Transformer model under this framework during the training phase and testing phase. More precisely, during the training phase, the authors investigate the training loss for Base Add (BA), Use Carry 1 (UC1), and Use Sum 9 (US9) three tasks. According to the experimental results,   US9 is the most complicated, especially in the case where more than one column carry occurs (e.g. 445+555=1000) and BA, UC1 two tasks are highly correlated. During the testing phase, the authors use ablation experiments to evaluate each attention head and conclude that for different digit pairs, the model uses slightly different algorithms."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "- The paper advances in the direction of opening the black box of Transformers, which is a very important topic as Transformers are being applied in an increasing number of domains. \n\n- The authors decompose integer addition into several subtasks and investigate the loss of each task during the training. This might provide some inspiration for future improvements in deep learning for math.\n\n- The paper is well-organized. The basic idea is clean and easy to follow."
            },
            "weaknesses": {
                "value": "- One experimental flaw is that the test accuracy of the model is not provided. In addition, it is also worthwhile to explore using the trained model directly for the addition of integers with more digits.\n\n- In the integer addition task, digit 0 should be treated as a special case, since intuitively, when humans perform integer calculations, the more zeros there are, the easier the calculation becomes. In other words, the digit 0 requires special attention. So it might be interesting to include Make Sum 0 and Use Sum 0 in the mathematical framework."
            },
            "questions": {
                "value": "(1) What's the performance of the trained model on the test data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_ExS5"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637712886,
        "cdate": 1698637712886,
        "tmdate": 1699636831615,
        "mdate": 1699636831615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "lExhXVmZCt",
        "forum": "rIx1YXVWZb",
        "replyto": "rIx1YXVWZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
        ],
        "content": {
            "summary": {
                "value": "This paper delves into the intricacies of a one-layer Transformer model trained for integer addition, emphasizing the importance of understanding machine learning models for safety and ethical considerations. The study uncovers that the model breaks down the addition task into parallel, digit-specific streams, using different algorithms for various digit positions. Interestingly, the model starts its calculations later but completes them swiftly. A unique use case with a high loss is pinpointed and elaborated upon."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This is a technically solid, moderate to high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations."
            },
            "weaknesses": {
                "value": "1. The adaptability of the methodology in this paper is limited, as it only applies to a one-layer Transformer model. Perhaps further analysis on two-layers or even more complex models would be beneficial. Moreover, the study solely focuses on integer addition, making it challenging to extend to other operations like subtraction or multiplication.\n2. The writing of this paper is not comprehensive. For instance, the descriptions for Figure 4 and 8 are difficult to comprehend.\n3. The experiments conducted are not exhaustive. In the \"Prediction Analysis\" section, the authors failed to provide a specific metric and results compared to baselines."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7065/Reviewer_sWU9"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698824766950,
        "cdate": 1698824766950,
        "tmdate": 1699636831502,
        "mdate": 1699636831502,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LsiisUjwEp",
        "forum": "rIx1YXVWZb",
        "replyto": "rIx1YXVWZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_f5y1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_f5y1"
        ],
        "content": {
            "summary": {
                "value": "Very interesting paper that focuses on explaining the \"inner workings\" of the foundational model of Transformer. While the use-case demonstrated (integer addition with a single layer transformer) is simplistic, the idea is novel and the visualisations are meaningful and make sense for better trust and confidence in how a transformer model works for the AI community."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Transformer model focus - no doubt an important model in the current AI landscape. Solid mathematical explainations and interpretation of the model working, the attention visualisations shown are very interesting and the model training loss curve which shows how a transformer trains individual digits semi-independently was promising to see."
            },
            "weaknesses": {
                "value": "No major weakness other than the paper applying the framework of explainability to a simple problem (integer addition). Though, this is well the strength as well of the paper as it makes the model easier to interpret and understand."
            },
            "questions": {
                "value": "Solid theoretical framework in the paper, good interpretation and visualisations - no further questions from this reviewer. The paper is very well written, easy to understand and the method is clear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698965626523,
        "cdate": 1698965626523,
        "tmdate": 1699636831380,
        "mdate": 1699636831380,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "3wXj2eYVyz",
        "forum": "rIx1YXVWZb",
        "replyto": "rIx1YXVWZb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7065/Reviewer_wEos"
        ],
        "content": {
            "summary": {
                "value": "This paper attempts to reveal the internal working mechanism of Transformers for integer addition tasks."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "\u2022 Study an important problem of transformer models' application in numerical computation tasks."
            },
            "weaknesses": {
                "value": "\u2022 The analysis framework and analysis lacks mathematical rigidity. \n\t\u2022 The conclusion is not well established based on rigorous mathematical framework. \n\t\u2022 The paper does not fully utilize/choose the most relevant aspect of transformers for addition tasks."
            },
            "questions": {
                "value": "1. Page 2, a latex error: \"d_e-dimensional embeddings\"\n\t2. Page 3, section 3, paragraph 3, \"detailing identified circuits\", is this a typo? Or which \"identified\" circuits? What is the \"identified\" process?\n\t3. Page  3, section 3, paragraph 4, \"techniques in works like symbolically \u2026 \", a grammar error?\n\t4. Page 3, section 3, paragraph 7, \"Surveys like  overview techniques\u2026\", missing citations after \"Surveys like\"?\n\t5.  Page 4, section 4, paragraph 3, \"Fig. 2 shows \u2026 semi-indendently\u2026\", what is the loss per digit is being plot in Figure 2? \n\t6. Page 4, section 4, paragraph 4, \"Transformer models always process text from left to right\u2026\", this is not true. It is just an artifact of GPT-style attention masking. For example, we can do config the attention mask to enable full order attention over the two addends and generate the outputs in all kinds of order, e.g. from the tens digit to higher value digits, from the middle digit to two ends, and so on. We can also do non-autogression generation, e.g. incremental masking output generation. \n\t7. Page 4, figure 3 caption \"..After the question is fully revealed (at layer 11)..\", by \"layer 11\" do you mean the 11th row? To avoid ambiguity, it is better to number the attention matrix and refer to them the row or column number across the paper.  Also what are the sub-figures of 0.0, 0.1, 0.2? Different heads? What are the labels? \n\t8.   Page 4-5, section 5, please clarify whether the  \"mathematical framework\" is  for characterizing (grouping) addition data instances-digits only? Or is there a link to the loss? If so, please formulate the framework and what kind of mathematical hypotheses this framework can verify formally in mathematical terms?  Also please detail the loss on each digits formally. Also please detail the statistics of your training and valid datasets in terms of your classification of digits in your framework. \n\t9. Page 4-6, please detail how the loss is being average. Are they per digits or per digit average?\n\t10. Page 6, please introduce or define or describe phase 1, 2, 3 formally?  \n\t11. Page 7, section 7, \"During model prediction we overrode \u2026 the model memory (residual stream)\u2026\", please detail the approach formally? Are your conclusions/assertions based on checking the attention scores?  Please discuss explicitly with formal treatment. Otherwise, the plain English language analysis in Section 7 is difficult to follow and justify. Also formally define \"independent of every other digit\", \"most impact on loss\" and define it based on measure statistics during model inference time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7065/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699353481880,
        "cdate": 1699353481880,
        "tmdate": 1699636831229,
        "mdate": 1699636831229,
        "license": "CC BY 4.0",
        "version": 2
    }
]