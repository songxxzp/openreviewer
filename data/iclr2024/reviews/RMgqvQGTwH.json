[
    {
        "id": "bLwQF0aiix",
        "forum": "RMgqvQGTwH",
        "replyto": "RMgqvQGTwH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_CW8P"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_CW8P"
        ],
        "content": {
            "summary": {
                "value": "The paper studied the hybrid RL setting where the agent has access to both online and offline dataset. A natural policy gradient based algorithm is proposed with provable guarantee on the sample complexity. The sample complexity bound showed that the approach, in theory, can achieve the best of both worlds. The paper further provides simulation studies on combinatory lock environment, where the proposed algorithm showed advantage over baseline methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The hybrid RL setting has a quickly growing literature and this paper by considering a natural policy gradient algorithm is a significant contribution to the literature. The algorithm has sound theoretical guarantee."
            },
            "weaknesses": {
                "value": "In general, the discussions on the theoretical results have to be more comprehensive and more coherent. \n \nThe paper mentioned in the abstract that the sample complexity bound is best-of-both-worlds. However, I did not see any discussion on the existing bound for pure online and pure offline bound. Many results in the offline literature, when a pessimism algorithm is applied, has a sample-complexity bound depending on the single-policy concentrability instead of the all-policy. it is clearly not the best in the world of pure-offline learning.\n\nThe NPG coverage condition and the Bellman error transfer coefficient do not seem to match. Could NPG coverage condition be weakened such that it reflects the dependence on the value function class? In the current form, if $\\nu$ is generated by some policy on the environment, then $C_{off, \\pi^e} \\leq C_{jpg, \\pi^e}$. \n\nCould the authors comment on the technical contribution? By quickly going through the appendix, I don't see significant technical contributions except following the analysis in Song et al. (2022) and the previous natural policy gradient literature.\n\nOne can image that the algorithm performance should heavily depend on the coverability of the offline dataset. I suggest the authors test the effects of offline distribution $\\nu$. Does the a better coverage provide better performance?"
            },
            "questions": {
                "value": "The authors proposed a parameterized policy class. What is a sufficient condition on the policy class to ensure the same sample complexity bound in Theorem 1?\n\nAs indicated by the Theorem 1, the algorithm should benefits more from the offline dataset when Bellman completeness is not satisfied. Has this been observed in the simulation studies? It seems that the HNPG always has the dominate performance regardless of the Bellman completeness? Do the authors have any comments on this point? I believe this is critical regarding how the theoretical results help us understand the real-world performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698678464521,
        "cdate": 1698678464521,
        "tmdate": 1699636408467,
        "mdate": 1699636408467,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iD4R3KnBC2",
        "forum": "RMgqvQGTwH",
        "replyto": "RMgqvQGTwH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_mti6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_mti6"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a hybrid RL method based on NPG that uses both on-policy data and offline data. The algorithm enjoys theoretical guarantees for the best of both the offline RL setting and on-policy RL setting. Empirical performance also shows that the proposed hybrid RL algorithm outperforms the existing algorithms in challenging scenarios."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Clarify: the paper is well written. The motivation is justified clearly. The algorithm design and theoretical guarantees are also explained clearly. \n\nSignificance and quality: the results are quite significant since it is able to achieve the best of the both world theoretical guarantees, with empirical performance improvements in challenging scenarios too."
            },
            "weaknesses": {
                "value": "I don't see major weaknesses. Just one question about Figure 2: it is hard to differentiate the curves generated by different approaches, thus difficult to see the message from this figure. I suggest the authors improve the presentation of Figure 2."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808076224,
        "cdate": 1698808076224,
        "tmdate": 1699636408393,
        "mdate": 1699636408393,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CjHAcYlh9y",
        "forum": "RMgqvQGTwH",
        "replyto": "RMgqvQGTwH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_HGgg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_HGgg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new hybrid reinforcement learning (RL) algorithm called Hybrid Actor-Critic (HAC) that combines on-policy and off-policy learning. The proposed algorithm uses compatible function approximation and a hybrid loss using both online and offline data to achieve better theoretical guarantees. The authors present a theoretical analysis to show that it has both offline and on-policy performance guarantees with a small Bellman error and still has the on-policy performance gurantee when the Bellman error is large. In practice, the algorithm is tested on rich-observation and exploration-challenging environments and is shown to outperform hybrid RL baselines like RLPD."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Theoretical analysis: The paper provides a thorough theoretical analysis of the proposed algorithm, demonstrating that it achieves nice theoretical guarantees when offline RL-specific assumptions hold, while maintaining the guarantees of on-policy natural policy gradient (NPG) regardless of the validity of the offline RL assumptions.\n2. Empirical results: The authors demonstrate the effectiveness of their approach on challenging rich-observation environments, outperforming state-of-the-art hybrid RL baselines. This showcases the practical benefits of combining on-policy and off-policy learning."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed method is limited. The algorithm is a combination of natural actor-critic and hybrid-RL [1], and it needs to be clarified what is the technical challenge in combining them.\n\n2. I am also concerned with the claim that the proposed method achieves the best of both worlds. While it is nice to have guarantees without the Bellman completeness assumption, other strong assumptions are still required. The realizability for any $Q^\\pi$ is a strong assumption, and the value-based method usually only requires the realizability of $Q^*$. More importantly, the assumption on NPG coverage is also strong since it is an all-policy coverage assumption. Offline RL only needs single-policy coverage assumption as in Definition 3, and it can be proved that $C_{off,\\pi^e}^2 \\leq C_{npg,\\pi^e}$ [1,2]. This invalidates the claim that the proposed method achieves the best of both worlds since when the offline assumption does not hold, the online guarantee is lost, too. I expect an exploration mechanism so that we can still achieve low regret when the offline data has insufficient coverage.\n\n[1] Song, Yuda, et al. \"Hybrid rl: Using both offline and online data can make rl efficient.\" arXiv preprint arXiv:2210.06718 (2022).\n\n[2] Xie, Tengyang, et al. \"Bellman-consistent pessimism for offline reinforcement learning.\" Advances in neural information processing systems 34 (2021): 6683-6694."
            },
            "questions": {
                "value": "1. What is the technical challenge in combining natural actor critic and hybrid RL?\n\n2. When the offline guarantee does not hold, how can we still retain the on-policy guarantee?\n\n3. Can you provide more insights on the superior performance of the proposed method on the comblock environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4364/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4364/Reviewer_HGgg",
                    "ICLR.cc/2024/Conference/Submission4364/Senior_Area_Chairs"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832822064,
        "cdate": 1698832822064,
        "tmdate": 1700672913726,
        "mdate": 1700672913726,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "GWLFVWkjds",
        "forum": "RMgqvQGTwH",
        "replyto": "RMgqvQGTwH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_M4D3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4364/Reviewer_M4D3"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on hybrid RL, where the agent has offline data and is able to interact with the environment simultaneously. The novel algorithms introduced in this paper integrate the TD error from offline data and the estimation error coming from online datasets into the policy evaluation loss. These approaches distinguish itself from the prior works that directly merge offline and online data, followed by applying an off-policy method. The theoretical result shows that the optimality gap of the proposed method can be upper bounded without Bellman Completeness or with Bellman Completeness and offline data coverage guarantee. The experiments are conducted in a continuous Comblock environment, which necessitates accurate multi-step actions to attain the final optimal reward. The results reveal that this method outperforms TRPO, a purely on-policy method, as well as RLPD, a hybrid off-policy actor-critic method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed algorithms in the hybrid RL setting are interesting and novel.\n- The theoretical guarantee established for the two distinct cases of Bellman Completeness offers significant insights. To my knowledge, this paper is the first that provides an algorithm with best-of-both-worlds type guarantees for hybrid RL (so that the assumption about Bellman Completeness can be relaxed).\n- The proposed algorithms empirically outperform other hybrid RL baseline like RLPD in hard exploration problems (continuous and image-based Comblock).\n- The paper is very well-written and easy to follow."
            },
            "weaknesses": {
                "value": "I did not spot any particular major weakness in this paper. That said, there are a few places that could be further improved:\n- In Algorithm 3, HNPG solves the squared loss regression with a linear critic. The requirement of a linear critic is probably mainly for theoretical analysis and could be relaxed to more general function classes (e.g., neural function approximation). A further discussion (possibly with some experiments) could make the proposed algorithm more impactful. \n- Regarding Definition 3, it is a bit difficult to get a sense of how large the transfer coefficient could be (as it is the maximum taken over all stationary policies and the whole function class) and under what condition of $\\nu$ would $C_{off}$ be bounded or small.\n- As the proposed HNPG focuses on the hybrid RL problems, readers would probably expect at least a bit of experimental comparison on those more mainstream RL benchmarks like D4RL (as done in several recent hybrid RL papers, e.g., RLPD and Cal-QL) or hard exploration problems in Atari, e.g., Montezuma's Revenge as in (Song et al., 2022)."
            },
            "questions": {
                "value": "Some detailed questions:\n- The concept and the terminology of \u201cNPG coverage\u201d in Definition 2 could be further elaborated on. Specifically, Definition 2 is defined w.r.t. some comparator policy, which is not necessarily related to NPG.\n- To calculate the loss in Eq. (1), one would need to obtain an unbiased estimate $y$, which could be obtained via Monte Carlo sampling. On the other hand, it would probably be better to get $y$ with the help of bootstrapping. Would this still preserve a similar sub-optimality guarantee as in Theorem 1?\n- Regarding Definition 2, how to ensure a finite $C_npg$ without the assumption on the positivity of the reset distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4364/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024390090,
        "cdate": 1699024390090,
        "tmdate": 1699636408260,
        "mdate": 1699636408260,
        "license": "CC BY 4.0",
        "version": 2
    }
]