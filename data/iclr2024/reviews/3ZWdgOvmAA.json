[
    {
        "id": "7WJ2BLwbP8",
        "forum": "3ZWdgOvmAA",
        "replyto": "3ZWdgOvmAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_8pKb"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_8pKb"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a logit-based KD method through the reconstruction of instance-level distributions. The proposed LumiNet emphasizes the perception alignment instead of the traditional logit output. Through this framework, the student model can learns the inter-class and intra-class relationship from the teacher model. Extensive experiment results prove that the proposed method shows competitive performance in both image classification and detection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow. \n2. This paper conducts extensive experiments in both image recognition and detection."
            },
            "weaknesses": {
                "value": "1. Capturing inter-instance and inter-class relationship to boost knowledge distillation is not new to me. For example, the idea of [1] is quite similar with this paper. I want to see more explanations during rebuttal period. \n[1] Multi-Level Logit Distillation (CVPR2023)\n2.  Missed baseline TAKD and DML in experiment section."
            },
            "questions": {
                "value": "See weakness. I think the authors should have more explanations on the novelty of their method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6523/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6523/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6523/Reviewer_8pKb"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698070325679,
        "cdate": 1698070325679,
        "tmdate": 1699636733387,
        "mdate": 1699636733387,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "dp92j01p6K",
        "forum": "3ZWdgOvmAA",
        "replyto": "3ZWdgOvmAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_tkRa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_tkRa"
        ],
        "content": {
            "summary": {
                "value": "In this work, the authors use statistic matrixes to recalibrate logits for knowledge distillation, named LumiNet.  LumiNet focuses on inter-class relationships and enables the student model to learn a richer breadth of knowledge. Both teacher and student models are mapped onto the statistic matrixes, with the student\u2019s goal being to minimize representational discrepancies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Distillation is an important topic to our community, the proposed method is simple.\n2. The write-up is easy to understand."
            },
            "weaknesses": {
                "value": "1. My major concern is the generalization, given that distillation is a well-defined topic, but this method seems like doesn't work well in heterogeneous architecture settings. I know it's a logit-based method, but the authors claimed to bridge this gap. Maybe the authors can combine feature-based loss in their method to see if their proposed method does work or not.\n\n2. More state-of-the-art methods should be compared in this work. e.g. [1]\n\n\n[1] Liu, Dongyang, Meina Kan, Shiguang Shan, and Xilin Chen. \"Function-consistent feature distillation.\" (ICLR 2023)"
            },
            "questions": {
                "value": "If this method get any benefit when combined with feature-based loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698638198549,
        "cdate": 1698638198549,
        "tmdate": 1699636733285,
        "mdate": 1699636733285,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "TRWuXsJ1nO",
        "forum": "3ZWdgOvmAA",
        "replyto": "3ZWdgOvmAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_7mGH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_7mGH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a new method for logit-based knowledge distillation. The authors define a perception matrix to recalibrate logits and use the recalibrated logits to perform knowledge."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The method looks more effective than existing logit-based knowledge distillation techniques on various vision tasks.  The results of more complex object detection tasks are also provided. \n\n- The method can improve performance without introducing extra parameters and latency."
            },
            "weaknesses": {
                "value": "- The improvements over DKD overall are not very significant. The proposed method and DKD share the general idea of modeling inter-instance relations to improve knowledge distillation. The method's performance improvements over DKD are limited in several cases, and it is as efficient as DKD. While it is good to see a solid improvement over DKD, the similarity between the two methods indeed limits the paper's contribution.\n\n- The method is not evaluated on larger/stronger models. Will the method work well on popular vision Transformer architectures? Will the improvement be less significant if stronger teacher models are considered? \n\n- I am not totally convinced that holistic methods like DKD \"sometimes fail to wholly capture the essence of the teacher\u2019s knowledge\" but the proposed method can resolve this issue. What is \"the essence of the teacher\u2019s knowledge\" here? In the paper, I can only find some intuitive analysis and overall performance to support the proposed method. Why the transformation in Eq.3 can solve this issue? Can you provide more specific analyses on why and when the existing methods fail and why the proposed method is better?"
            },
            "questions": {
                "value": "Overall, I think it is a solid contribution to logit-based knowledge distillation. However, the core idea of the proposed method is not completely new and the improvement is not that significant. The paper also didn't provide an in-depth analysis of why the method is better.   I think it is an okay paper but not a strong/impressive contribution to the community. Therefore, I would like to rate this paper as \"marginally below the acceptance threshold\". \n\nA minor issue: \"... based on context and past encounters Johnson (2021)\" -> \"... based on context and past encounters (Johnson, 2021)\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699026380645,
        "cdate": 1699026380645,
        "tmdate": 1699636733168,
        "mdate": 1699636733168,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NremubfEVP",
        "forum": "3ZWdgOvmAA",
        "replyto": "3ZWdgOvmAA",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_3qnL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6523/Reviewer_3qnL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new logit-based distillation method to distill a teacher model with only predicted logits. The proposed method, LumiNet,  can reconstruct more granular inter-class relationships, enabling the student model to learn a richer breadth of knowledge. The proposed LumiNet is evaluated on CIFAR-100, ImageNet, and MSCOCO, revealing its competitive performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is clear, and the method is easy to follow.\n2. LumiNet makes a simple but effective modification to logit KD, which calibrates the mean and variance of each class."
            },
            "weaknesses": {
                "value": "1. A strong baseline [1] should be discussed and compared. It seems that [1] also tries to learn more granular relationships, which achieves better performance than LumiNet in many benchmark results. If LumiNet can directly outperform [1] or further improve performance based on [1] should be discussed. \n2. The experiments are performed on Conv-based methods. It will be convincing that LumiNet can show its advantages over previous works on ViTs.\n\n[1] Multi-level Logit Distillation, https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6523/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699114489683,
        "cdate": 1699114489683,
        "tmdate": 1699636733044,
        "mdate": 1699636733044,
        "license": "CC BY 4.0",
        "version": 2
    }
]