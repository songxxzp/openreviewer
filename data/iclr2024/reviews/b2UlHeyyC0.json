[
    {
        "id": "BcKhFx73tn",
        "forum": "b2UlHeyyC0",
        "replyto": "b2UlHeyyC0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_YY52"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_YY52"
        ],
        "content": {
            "summary": {
                "value": "The paper studies a method to use image-text pretrained model (e.g., CLIP) for fine-grained classification for which CLIP might not have (enough) data. It proposes a method that retrieves relevant data from an external memory, which contains data outside the fine-grained classification dataset. After retrieval, it trains a transformer atop of CLIP to fuse CLIP's features and features of retrieved images. It reports improved fine-grained recognition tasks in experiments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Retrieval based augmentation is a recent technique for improving performance of downstream tasks using pretrained models.\n- Discussion of search methods (e.g., uni-modal search, cross-modal search) is comprehensive."
            },
            "weaknesses": {
                "value": "There are several concerns related to weaknesses. The paper is hard to follow.\n\n\n- In Introduction, the paper writes \"Our hypothesis is that this disparity stems from the fact that it is hard to align the image and text modalities\". It is not clear why it happens w.r.t \"it is hard to align the image and text modalities\". Can authors clarify? \n\n- Following the above, the abstract mentions that \"fine-grained entities which are rare\". The first paragraph also uses examples to explain rare concepts. Having concepts rare seems like a different reason from \"being hard to align image and text modalities\". Which reason is more reasonable? Can authors explain and clarify?\n\n- The sentence is unclear -- \"One caveat that we identify in this approach is that initial captions are augmented within their modality only, hence limiting the potential added-value brought by the retrieved items.\" Can authors clarify?\n\n- The sentence is unclear -- \"However, when crossing modalities, these representations are less successful in identifying suitable matches, such as finding the text with the closest representation to a query image representation.\" Can authors clarify? What message does this sentence deliver?\n\n- The sentence is unclear -- \"Through this process, we successfully transform the image and text representations into multi-modal versions, which significantly simplifies their alignment\". Can authors clarify?\n\n- The paper studies different search methods as shown in Figure 2. However, it does not discuss computation cost, complexity, etc. It seems that computing features of images and texts can be very computationally expensive. The paper misses important details in methods.\n\n- The paper uses a dataset called WebLI and explains that it is a private dataset. However, how to access the private dataset? Does it mean that authors own the private dataset (indicating a leakage of author identities)? How to fairly compare methods if authors use a private dataset? Authors do not discuss ethical issues w.r.t the private dataset. This is a concern.\n\n- When discussing \"Is the performance boost merely due to additional training?\", the paper uses ResNet backbone and learns a transformer. Given that transformer might be better than convnets, it is questionable to claim using a transformer is a novel technique. Can authors discuss results if using a transformer backbone in CLIP along with transformer head for fine-grained recognition?"
            },
            "questions": {
                "value": "Questions are in the weaknesses. I encourage the authors to address them in rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper uses a private dataset but does not discuss potential ethical issues."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698568477833,
        "cdate": 1698568477833,
        "tmdate": 1699636525284,
        "mdate": 1699636525284,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "C9p6MTB1i4",
        "forum": "b2UlHeyyC0",
        "replyto": "b2UlHeyyC0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_G4iu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_G4iu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes RECO, a method to improve the embeddings produced by vision-text encoders like CLIP. For a given image query, the proposed method first finds a set of similar images, with their accompanying texts. Then, the retrieved texts are embedded and fused with the embedding of the query image, to produce a better representation. For text queries, the process is the same, but it first retrieves similar texts and it uses the embeddings of their associated images to improve the embedding of the text query.\nThe authors evaluate their method on 6 image classification benchmarks, on the OVEN benchmark and on the Text-to-Image and Image-to-Text retrieval tasks from Flickr30k and MS COCO."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* The paper presents extensive quantitative results to assess the performance improvements achieved by using RECO on top of different vision-text encoders like CLIP and LiT-L16L, as well as reporting results for different tasks: zero-shot image classification, OVEN task and Text-to-Image and Image-to-Text retrieval. Moreover, the authors report results of strong and adequate baselines. From the results, it is clear that RECO improves CLIP embeddings for all tasks.\n\n* The paper has a strong section on \u201cDesign choice analyses\u201d, which does further experiments to show that the specific configuration used by RECO (unimodal search + cross modal fusion) is the best of all the options. Additionally, this section also evaluates the effects of using a different memory bank during inference than the one used during training, the effect of the number of retrieved elements and validates that the improvement does not come from an increased capacity of the model."
            },
            "weaknesses": {
                "value": "* The main weakness of the paper is that improvement in performance of using RECO changes significantly with the dataset used as the memory bank. The best results are obtained using a non-public dataset (WebLI), for which the authors do not provide any instructions on how to reproduce it.\n\n* Some tables do not report results on the Dogs dataset, it would be better to add them since this dataset is used in the main results Table."
            },
            "questions": {
                "value": "* Why do the results for Text-to-Image retrieval improve more than Image-to-Text?\n* Why is WebLI a better memory bank than LAION? Is the number of images, better alignment between images-text, better captions...?\n* Would a model trained with WebLI perform well using LAION during inference?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775545526,
        "cdate": 1698775545526,
        "tmdate": 1699636525172,
        "mdate": 1699636525172,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "zhbqi9MpuV",
        "forum": "b2UlHeyyC0",
        "replyto": "b2UlHeyyC0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_Smu2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5261/Reviewer_Smu2"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces retrieval-enhanced contrastive training (RECO), a method designed to enhance the performance of visual-text models on fine-grained recognition tasks. Specifically, RECO refines the model's embeddings with cross-modal information retrieved from a large external image-text pair dataset. The proposed method outperforms the original CLIP or LiT models in 11 challenging fine-grained tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The authors have thoroughly investigated various designs for retrieval enhancement, emphasizing the importance of combining uni-modal search and cross-modal fusion.\n2. The proposed RECO employs a light-weight, single-layer transformer encoder for fusion, without significantly increasing the number of parameters.\n3. They achieve significant improvements on several fine-grained recognition datasets."
            },
            "weaknesses": {
                "value": "1. This method relies on a large-scale dataset of image-text pairs as external knowledge. However, if the image-text pairs are noisy, the retrieved cross-modal information may be inaccurate, potentially undermining the final performance.\n2. The uni-modal search process seems to have a significant overhead (in terms of computation and IO access) during inference, since it has to perform retrieval from a large number of image-text pairs.\n3. While this method enhances performance on fine-grained tasks, how does it affect the accuracy of recognizing common generic concepts? Can it be applied to common visual recognition tasks?"
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5261/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5261/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5261/Reviewer_Smu2"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5261/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699283067163,
        "cdate": 1699283067163,
        "tmdate": 1699636525082,
        "mdate": 1699636525082,
        "license": "CC BY 4.0",
        "version": 2
    }
]