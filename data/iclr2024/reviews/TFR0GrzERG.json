[
    {
        "id": "W5wcIQ7d5D",
        "forum": "TFR0GrzERG",
        "replyto": "TFR0GrzERG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_9ENX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_9ENX"
        ],
        "content": {
            "summary": {
                "value": "The authors study how task descriptions affect the in-context learning abilities of transformer models. To do so, they devise a synthetic experiment setting in which the information provided by the task description is controllable. They find that task descriptions can suppress the model of learning from examples and that they can increase the lower bound of in-context learning performance."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "The proposed task and experimental setup were clean.\n\nHow task descriptions affect in-context LLMs would indeed be a worthy question to study."
            },
            "weaknesses": {
                "value": "The authors claim (at multiple points in the text) that their work \u201ccontributes to a deeper understanding of the in-context learning mechanism in LLMs.\u201d However, this is not true as none of the experiments involves an LLM. Instead, their results only study in-context learning in a transformer-based model. The claim of improving our understanding of LLMs is misleading. \n\nFurthermore, the authors state that our understanding of in-context learning in the meta-learning setting (which they study) is limited. I don\u2019t think that this is true. We have a pretty good theoretical understanding of in-context learning in meta-learned neural networks: they implement (under ideal conditions) Bayesian inference for the distribution of tasks they were trained on (Ortega et al. 2019). Given this correspondence, I don\u2019t find the main results from the present paper surprising. If the mutual information between the task and its description is high, no hidden variable has to be inferred, leading to high performance and no learning from samples. If it is low, the hidden variables have to be inferred to make good predictions, and that is easier with more samples. \n\nOrtega, Pedro A., et al. \"Meta-learning of sequential strategies.\" arXiv preprint arXiv:1905.03030 (2019).\n\nThe notation is all over the place. This starts with the equations on page 1 for which the meaning of y and r is not provided. Letters are used inconsistently. q_{\\theta} is not defined. Figure 1 suddenly uses r to denote targets. In general, given these issues, I had a hard time following the description provided in the formulation and motivation section, even though I think I understood the method from the general framing. \n\nImportant details about the training task distribution are missing. How are the parameters sampled?\n\nMinor:\n\nThere seems to be a leftover from editing: \"There are several grammar errors and issues with the clarity in the given passage. I will provide a corrected version below, with changes and suggestions highlighted in brackets.\"\n\nFigure 3A: \n* no axis labels, and hence impossible to interpret. \n* mentioned after Figure 3B in the text.\n\nThe writing is ungrammatical in places and should be double-checked. Examples:\n* The conclusions of synthetic experiments are still held.\n* Those observation is well align with the findings on the above synthetic experiment.\n* As the task label prediction can also reflect whether the model understand what the task is."
            },
            "questions": {
                "value": "In the introduction, the authors claim that task descriptions with minimal information can impair in-context learning performance because they hinder the model\u2019s ability to learn from in-context examples. I don\u2019t think that this is correct given the presented results (Figure 1). \n\nFigure 2: I don\u2019t understand why performance for medium task information is worse than performance for low task information. Shouldn\u2019t more information always be better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698401577917,
        "cdate": 1698401577917,
        "tmdate": 1699636158232,
        "mdate": 1699636158232,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "AyMsNUNuZR",
        "forum": "TFR0GrzERG",
        "replyto": "TFR0GrzERG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_DYTw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_DYTw"
        ],
        "content": {
            "summary": {
                "value": "This work explores the relationship between a task description and number of in-context examples, and particularly its effect on in-context learning. They implemented a synthetic task that involves solving algebraic equations and trained a standard transformer architecture on this synthetic dataset in which each task example contains both a task description, in-context examples, and a query example with which to evaluate on. Since its a controlled, synthetic task, they were able to control the amount of information about the task the task description will supply. They found an inflection point in which, at a certain threshold of task description information, the accuracy will dramatically rise with respect to the amount of information in the description. Additionally, the model accuracy increases linearly with respect to number of in-context examples before the phase transition but does not increase after the phase transition. The authors also analyzed the attention ratios between the task description and in-context examples, the impact on adding a loss in which the task is predicted, and then checking if their results generalize to a more common natural language dataset (CoFe).\n\nI think this is a very technically solid work that could mainly be improved by improving the clarity (see my suggestions). I've rated the paper a 6 for now, but would be happy to re-evaluate the score if the clarity is improved."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Good careful and methodical experimental design that combines both synthetic, controlled experiments and checks findings on a larger dataset. \n\n*  The work employs multiple experimental measures/approaches that are intuitive and effectively support their hypotheses."
            },
            "weaknesses": {
                "value": "* Clarity: For Figure 3A, is the heatmap reflective of performance profile when adding the task prediction loss? The caption led me to believe that I'd be comparing two sets of data, one with task prediction loss and one without. \n\n* Clarity: It seems Figure 3B would be better suited to be a part of Figure 2 and 3A for Figure 4 (if it is indeed a figure representing performance profile after adding the task prediction loss). Maybe this would make the figures have too many panels or introduce size constraints, but I encourage the authors to think about how each figure can have a consistent theme with panels that are related and support each other. \n\n* Clarity: For Figure 4, the y-axis is the accuracy in predicting the task, right? If so, maybe label it as \"task prediction accuracy.\" I had some brief confusion on whether this meant accuracy in performing the current task correctly or predicting what the current task is. Also would be good to label the color bar in Figure 4C. \n\n\n* Interpretation: The interpretation of the task description suppressing the model's in-context learning ability puzzles me. To me, it seems like, when the task description does not have enough information, the model relies on the examples to learn the task. But when the task description has enough information, the model does not need to rely on examples because it has already learned the task, so additional examples don't lead to performance gains. In both cases, the task description is not necessarily hindering the model's in-context learning ability, but maybe is the \"preferred\" method of learning the task over examples. Maybe this is what the authors meant when they said \"suppress\", but to me, suppression of in-context learning would mean that the model is unable to learn the task using more in-context examples. \n\n* Related to above: the results actually remind me of work in cognitive science on teaching with language vs demonstrations (Sumers et al. 2023). Teaching with language can be more effective because a language description of a task can effectively transmit abstract concepts needed to perform the task, whereas demonstrations give specific instances of concepts needed to perform the task and requires the learner to infer the abstract concepts given the demonstration. I'd be interested to hear the authors' thoughts on if their results can be interpreted in this light. \n\nReferences:\n\nSumers, T. R., Ho, M. K., Hawkins, R. D., & Griffiths, T. L. (2023). Show or Tell? Exploring when (and why) teaching with language outperforms demonstration. Cognition, 232, 105326."
            },
            "questions": {
                "value": "* The authors mention real world scenarios as a limitation. What specifically about real world scenarios could make things different? It would be good to have specific examples. One I could think of: if a task requires grounding in another non-linguistic domain (e.g. utilizing images or motor programs for robots), then the task description can supply the abstractions needed to perform the task, but the in-context examples will be needed to ground those abstractions in the new domain. In this case, the learning dynamics after the phase transition may be different from the current results, because more in-context learning examples may still help the model even after the task description has enough information."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Reviewer_DYTw",
                    "ICLR.cc/2024/Conference/Submission2250/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698757471568,
        "cdate": 1698757471568,
        "tmdate": 1700669302402,
        "mdate": 1700669302402,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CTmBjnDNEl",
        "forum": "TFR0GrzERG",
        "replyto": "TFR0GrzERG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_YT61"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_YT61"
        ],
        "content": {
            "summary": {
                "value": "The authors investigate how task decripitions impact in-context learning (ICL) by conducting experiments on synthetic datasets. They find a phase transition in terms of the amount of information in task decriptions: (i) information below a threshold can hinder transformers from performing ICL; (ii) information above the threshold will promote ICL. Authors further try to interpret the phase (i) by showing that the task decriptions decrease the attention ratio of ICL examples. They also interpret phase (ii) by claiming rich task descriptions allow model to learn a good representation of the task. The authors also propose that using predicting the task label as an auxilliary task can improve ICL. Finally, the authors partially verify the previous discoveries on the real-world dataset CoFE."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. I personally like the problem of how task information impacts ICL and I appreciate the idea of conducting synthetic experiments to approach the problem.\n\n2. The illustrations are nice."
            },
            "weaknesses": {
                "value": "1. Some claims are not fully verfied or explained. (Will list them in the question section.)\n\n2. I cannot see the difference between section 6 (Discussion) and section 8 (Conclusion).\n\nMinor points:\n\n3. The first paragraph on page 4 is irrelevant."
            },
            "questions": {
                "value": "1. For Figure 2A, why the blue curve achieves the best performance after the threshold?  In my understanding, blue curve stands for the least number of ICL expamples.\n\n2. There seems to be 3 phases in Figure 2A so there should be 2 transitions. All the transition I mention in the comment is the latter one. I think the word transition refers different ones in different contexts, which makes me a bit confused. (For instance, the \"phase transition\" in the comment for Figure 2B in section 5.1 seems to be different from the transition in the 2nd paragraph in page 2.)\n\n3. I don't see the reason behind the sharp transition in Figure 2A and the provided interpretations for both phase (i) and (ii) fail to explain that. Is the sharp transition caused by the lack of points used in the plot?\n\n\n4. Phase (i) needs more detailed explaination as increasing the number of ICL examples barely improves ICL. \n\n5. Section 5.2 gives the interpretation for phase (ii) by saying \"Higher information of task description will increase the lower bound of performance.\" What is the lower bound here? Should it be stated in a more formal way?\n\n6. The Figure 1C gives the i/o of the transformer model. Should the figure also include the output for the task prediction as eq(7) and eq(6) are using the same model $f$.\n\n7. Is the first sentence in the 3rd paragraph of section 5.3 finished? (\"As the task label prediction can also reflect whether the model understand what the task is.\")\n\n8. Figure 3A suggests that there is also a similar phase transition for the **accuracy gain**. Does it have any explaination? \n\n9. Section 2 paragraph 2 says\"However, despite these valuable contributions, all these explorations tend to overlook the influence of task descriptions on the in- context learning process. \" Is that true? For the linear regression ICL papers, I don't think they have such task decriptions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Reviewer_YT61"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698781983209,
        "cdate": 1698781983209,
        "tmdate": 1699636158080,
        "mdate": 1699636158080,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "EXBzkVxZMM",
        "forum": "TFR0GrzERG",
        "replyto": "TFR0GrzERG",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_hvVV"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2250/Reviewer_hvVV"
        ],
        "content": {
            "summary": {
                "value": "**Update after rebuttal:**\nI want to thank the authors for making many small improvements to the paper. Ideally, the paper would have been in this state when submitting already. Thank you also for running the ablations and pointing out that the task description is always helpful and never detrimental. To me personally the paper is approaching the acceptance threshold but still lies slightly below it. I have increased my score to reflect that. As I stated in my original review, I very much like the idea of creating synthetic tasks that have both in-context examples and a task-description that is \"understood\" by the model and can be modified in terms of its information content. I personally suggest to spend a bit more time to produce a strong and impactful paper that will stand the test of time - I think the potential for this is there. In the meantime, the current manuscript could be very suitable for an ICLR workshop for instance. Ultimately, I will not strongly argue against acceptance if the majority of reviewers thinks the paper is now ready, but to me personally the bar for ICLR has not quite been reached yet.\n\nShould the paper get rejected, I personally think the most important areas to focus on are: more experimental evaluation (add one or two more tasks, increase the scale of tasks, think about ways to perform experiments at LLM scale (is there a way without having to train an LLM)), and working towards generality of the findings (do we get similar shapes for the usefulness of more task information across many tasks, or is the shape very task dependent?). Additionally, even with the many small improvements, the paper could use another pass to further improve the writing in terms of clarity and conciseness, and the figures are now OK but can also be polished a bit further. \n\n\n**Summary:**\nThe paper investigates the role of two different sources of task information in in-context learning: (1) examples from the task (such as input-label pairs), and (2) general task descriptors (additional context that is informative about the task). The second type of information has received considerably less attention in the systematic analysis of in-context learning. The aim of this paper is to address this by designing a (relatively) small set of modular arithmetic tasks that are parameterized such that they are easily enumerable (leading to a unique task id, given their parameters). Additionally, a task-description can be constructed such that it provides a controllable (and quantifiable!) amount of information about the task parameters. This is implemented by providing lower and upper bounds for the task parameters as the task description; by widening or tightening the bounds, the amount of information provided can be easily manipulated. Taking all of this together allows to conduct precise experiments where the amount of task information available in the context can be controlled by: (1) the number of in-context examples, and (2) the amount of task information provided by the task description (which is prefixed in-context before the examples). The paper finds that providing no task information leads to the network performance depending strongly on the number of in-context examples, whereas providing full task information means accuracy is almost independent of the number of examples. Finally providing non-zero but less than (near-)maximal task information seems to be detrimental to in-context learning, leading to poor accuracy regardless of the number of examples in the context. Additionally the paper studies the influence of predicting the task id (the parameters of the task) as an auxiliary task on the previously obtained results; and performs an experiment inspired by the synthetic tasks using a non-synthetic LLM benchmark dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* Very timely problem. Understanding the role of side-information for in-context learning is much less understood and studied compared to understanding the influence of in-context examples (particularly from a meta-learning / sequential prediction viewpoint).\n* I really like the idea of constructing synthetic tasks where the amount of task-relevant side information in the context can be controlled and quantified precisely. This is a refreshingly different approach to yet another heuristic and ad-hoc method for improving in-context learning in an intransparent fashion.\n* Forcing the network to explicitly predict the task might lead to the network \u201cpaying more attention\u201d to the side information encoded in the task description in the context, and thus lead to improved overall performance. Testing this hypothesis is a really good idea.\n* A limitations section."
            },
            "weaknesses": {
                "value": "* At the beginning of page 4 there seems to be a leftover comment from a co-author: \u201cThere are several grammar errors and issues with clarity in the given passage.\u201d. Unfortunately I think that this applies to the whole paper except for Sections 1 and 2. The current paper is very cumbersome to chew through, and seems as if it was drafted and maybe reorganized multiple times in a hurry. Many conference papers have a paragraph here or there that is a bit rushed due to deadline pressure, but the current manuscript needs a major revision and rewrite in terms of clarity and readability. I personally suggest in the future to not give in too much to deadline pressure, if a paper is clearly not ready yet, take the time to polish it and turn it into the strongest possible version. This is doubly unfortunate because I really like the idea behind the main experiment; but it is currently not in a presentable state. I have left many concrete passages for improvement under \u2018Questions\u2019.\n* Besides clarity, I do have some concerns regarding the main experiment and its interpretation. For Figure 2: the task info (mutual info) is given in the task description by a lower and an upper bound. I assume these values can only be integers and the mutual info can thus only be changed in discrete steps. The results in Fig. 2 A, C look like the task description is only helpful for the setting with maximal mutual information (which means the upper and lower bounds are equal), but is not helpful in any other setting. The main text describing the results of the figure says \u201cafter this information threshold the accuracy grows rapidly with information gain\u201c and the paper suggests similar, somewhat \u201cgradual\u201d, results on multiple occasions: it looks to me that this is incorrect; if the information in the task description is not maximal it seems to be detrimental regarding of the amount of information, and only when it is maximal does it actually help (i.e. only for a single value of the x-axis in Fig 2A, corresponding to the rightmost column in Fig 2C). Is this correct? If yes, I think this needs an explanation (why can the network not use partial info about the task; is this a general phenomenon or is it only for modular arithmetic, etc.).\n* Figure 2: what are the maximally possible values for the lower and upper bound in the task description? Is it possible that for minimal task information the intervals are so large that they always have the same value (and the network easily learns to ignore these constant values in its input)? Whereas for non-zero but less than maximal task information these values change and act as detrimental noise that interferes with in-context learning? If yes, this could explain why for no task information we see non-trivial accuracy, but the kind of interference that we then see for non-zero task information seems less mysterious.\n* Important ablations are missing: \n    * No task description during training, i.e. only in-context examples. Gives a baseline performance.\n    * No in-context examples, only task description with maximal info during training (sanity check: should lead to non-trivial accuracy, otherwise the network seems to be unable to use the task description). \n    * No in-context examples, only task descriptions with low to medium task info during training (sanity check: should lead to worse but not detrimental performance; otherwise it indicates that the network cannot use any non-maximal-info task description and a redesign of the task is required).\n\n\n**Verdict:** I was very excited to read this paper after the intro. The idea of controlling and quantifying the side-information in the context and study the impact on in-context learning is really good and interesting. The ablation with learning the task via an auxiliary loss is also very interesting. Unfortunately the paper in its current state does not hold up to its promises. The paper overall feels rushed and is a tedious read. Some important questions regarding the experimental setup are open (see questions below). Finally there are some concerns that the results shown do not quite match the interpretations in the paper (see weaknesses above). It is still unclear to me whether the results are a bit surprising (non-zero, but non-maximal task info seems to be very detrimental to in-context learning) or whether there is a simpler explanation that is harder to see because the task description only influences a single datapoint (when it is maximal). This needs to be cleared up, and requires some further investigation and ablations. While it is possible to tighten up the paper in terms of clarity and writing, I would suggest to come up with another synthetic task where task info can be manipulated even more fine-grained (some sort of injecting more or less noise into the description) to see whether the \u201cphase transition\u201d is generally this sharp or whether this is a quirk of modular arithmetic. Giving an answer to this would make for a potentially very strong paper. Overall I do not think that the current paper is ready for publication, but I want to encourage the authors to pursue what I believe to be a great main idea. I am happy to read the other reviews and hear the authors\u2019 response before reaching a final conclusion."
            },
            "questions": {
                "value": "* Last paragraph on P1: point out what x, y, and r is.\n\n* Notation: for a typical in-context set of examples $\\{x_i, y_i, r_i \\}_{i=1}^l, I assume that $r_i = r ~\\forall i$, meaning all in-context examples have the same task index? After reading 4.2, I now understand that x and y are \u201cinputs\u201d and r is the \u201clabel\u201d and not the task index. This makes sense, but really needs to be clarified in the intro.\n\n* Typo: a few times in the paper: \u201cthe in-context learning\u201d -> \u201cin-context learning\u201d, e.g. understanding in-context learning, or driving in-context learning.\n\n* What is $r_a, r_b$ in Fig 1B?\n\n* The Transformer used (24 layers, 8 attention heads) seems fairly large (or rather deep) for this task, how was the architecture chosen?\n\n* I believe the index for the sum in Eq. 6 should start at i=2? Otherwise for i=1 the same x,y,r pair is used twice. Essentially the same for Eq. 7.\n\n* End of Section 4.2: \u201cWe train the model for 20w steps\u201d - 20 million steps?\n\n* Is there any tokenization for the modular arithmetic task? If yes, which one, and does it ensure that two numbers from different parameters (e.g. a, and b in the task description) are never grouped into a single token?\n\n* Fig. 2 needs improvements: What are the units on the x-axis (bits? nats?)? What are the thin colored lines in 2A (number of examples?)? Panel B: better to state the value of task info in addition to saying before and after phase transition. Panel B and C: replace \u201cExample number\u201d with \u201cNumber of examples\u201d. Panel C: units on the x axis, numerical values on x- and y-axis ticks.\n\n* **Important:** How is the network trained for the results in Fig. 2? Does the number of examples and amount of task information vary across training for a single model, or is a single model trained for one value of task information and one value of number of examples?\n\n* Page 6, paragraph titled \u201cHigher information of task description will increase the lower bound of performance.\u201d - what figure/result does this paragraph refer to? It seems to be left over from a figure that has been dropped?\n\n* **Important:** Page 7: how exactly was the extra loss for task prediction added? Did the model have to predict both the answer to the query and the correct task (otherwise how could the accuracy gain in Fig 3A be computed)? If yes, how were the two loss functions combined / weighted?\n\n* Fig 3 needs similar improvement in terms of labeling as Fig 2.\n\n* It took me literally 3 minutes to understand what data Fig 3A is showing. It seemed so unrelated to its description in the main text that I first thought that by accident the wrong figure ended up in the main text. The colorbar needs a label with a quantity that is mathematically defined in the main text. The plot itself needs an informative title.\n\n* Fig 4 needs similar improvements in terms of labeling as Fig 2 and 3. Axes need units, axes ticks need numerical values, plots need informative titles, and legends need to say what variable the colors indicate - the list of logarithms in Fig 4B is incomprehensible. Colorbars need a label too to say what quantity they are encoding.\n\n* **Important:** Essentially all line plots in the paper. Since lines show the mean over 5 runs, please also show some indication of variance (e.g. +/- std-deviation shaded areas). This is important for claiming significant difference between certain settings. Just because the means are different does not mean that the difference is statistically significant.\n\n* **Important:** Sec. 5.4: I am not familiar with the CoFE dataset, and the current description in the paper does not really help with that. Please provide a better description (ideally with an illustration and examples) - if there is not enough space in the main paper, push it into the appendix. It is OK to leave out details and refer to the original publication, but readers should not need to read the publication to get the main gist of the dataset and tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2250/Reviewer_hvVV"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2250/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698799677473,
        "cdate": 1698799677473,
        "tmdate": 1700751421422,
        "mdate": 1700751421422,
        "license": "CC BY 4.0",
        "version": 2
    }
]