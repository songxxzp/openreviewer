[
    {
        "id": "JUozWVTxI0",
        "forum": "p4eG8rCa0b",
        "replyto": "p4eG8rCa0b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
        ],
        "content": {
            "summary": {
                "value": "This article proposes two methods to develop a diffusion model that merges the capability to manipulate the three-dimensional positioning of objects with the application of disentangled global stylistic semantics from exemplar images onto the objects.\n\nSpecifically, the paper introduces depth disentanglement training, which makes the model realize the  3D relative positioning of multiple objects by disentangling the salient object depth and the background object depth for the fusion of the condition during training.  In the meantime, this work presents a technique called soft guidance, which imposes the mask information into cross-attention mechanism to facilitate apply global semantics onto targeted regions without specific local localization cues."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The paper is written in a clear and coherent style, presenting ideas in a manner that is easily comprehensible. Additionally, most figures in the paper effectively visualize and reinforce the concepts discussed.\n\n2. As stated in the paper, this work is the first to leverage the disentaglement of images to salient object depth and impainted (unseen) background depth map for training the relative depth aware diffusion model.  The soft guidance technique is also novel for applying the global semantics to specific localizations.\n\n3. The adequate experiment supports the effectiveness of the model. Both the qualitative and quantitative comparisons demonstrate that the model can control the relative placement of the objects and the effectively prevent the concept bleeding."
            },
            "weaknesses": {
                "value": "1. Correct me if I have misunderstood. I'm confused about the details of the soft guidance. I understand that the work wants to leverage the mask map to selectively impose the foreground embedding and background embedding for the cross-attention mechanism. However, I'm uncertain as to whether this process should take place during the computation of similarity or subsequent to it. I would appreciate it if the author could elucidate the specific dimensions and computational details related to both the cross-attention mechanism and the soft guidance technique, to enhance reader comprehension. Please refer to the questions for additional context on my confusion.\n\n2. Apart from the standard metrics used for evaluating generative models, I wonder if there exist specific metrics that can accurately assess the model's capability to control the three-dimensional placement of objects and localize global semantics, as these are the primary objectives of this study. While the Mean Absolute Error (MAE) between the ground truth depth map and the depth maps derived from the generated images may offer some insight into the model\u2019s proficiency in 3D object placement, I am curious about how we might effectively gauge its ability to localize global semantics. Could there be other metrics or methods of evaluation that address this second capability?"
            },
            "questions": {
                "value": "1. In my understanding, the size of S is $i\\times j$, where i is the number of queries, and j is the number of keys; the size of $W_K \\dot y_{full}$ is $j \\times C$ and the size of $W_Q \\dot z_t$ is $ i \\times C$. However, I am puzzled as to why $j$ needs to be greater than $2N$.\n\n2. Additionally, I am uncertain about whether the mask should be applied along the dimension of $C$. It perplexes me that the mask is utilized on the calculated similarity rather than during the actual computation of similarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_FnjA"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698612300166,
        "cdate": 1698612300166,
        "tmdate": 1699636445621,
        "mdate": 1699636445621,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eXlL6m7tJ3",
        "forum": "p4eG8rCa0b",
        "replyto": "p4eG8rCa0b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_iDhm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a method for controllable text-to-image generation. The method learns auxiliary modules on top of a pre-trained Stable diffusion model, and introduces a novel training scheme to facilitate compositional image synthesis given two depth images that represent the foreground and background. Further, foreground and background styles are controlled by separate images thanks to a localized cross-attention mechanism. The qualitative and quantitative experiments demonstrate that the proposed method outperforms several baselines in terms of image quality, image-text alignment and foreground-background disentanglement."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper studies the composition of control signals in controllable text-to-image diffusion. Unlike previous approach which takes a single control image, the proposed method allows the conditioning on two depth images. This provides a means to separately control foreground and background image content. The method also enables localized control of image styles using exemplar images. To the best of my knowledge, compositional image generation remains a challenging problem, and this paper demonstrates one feasible solution to the problem by engineering pre-trained diffusion models.\n\n- The paper presents a novel training scheme to instill depth awareness into the diffusion model. Training of the method relies on RGB images and their foreground / background depth maps which are not readily available. To this end, the paper introduces a simple strategy to create synthetic training data from single-view image datasets. This is key to the success of the proposed method and may be of interest to the image synthesis community in a broader context.\n\n- The method allows localized control of image styles using exemplar images. The key idea is to limit the extent of cross-attention so that tokens representing an exemplar image only contribute to a local region in the image. Many works have used attention maps to localize objects or control their shapes and appearance. This paper for the first time uses attention maps to control (local) image styles.\n\n- The experiments demonstrate superior qualitative and quantitative results. The proposed method outperforms several strong baselines in image quality and image-text alignment while supporting broader applications."
            },
            "weaknesses": {
                "value": "- Calling the model \"depth-aware\" is misleading. I would rather say it learns to compose two spatial layouts and generate a coherent image. Using the teaser figure as an example, the cat can appear either in front of or behind the cake given the same depth maps, and similarly, the castle can either occlude or be occluded by the mountain. In other words, the exact ordering of objects is not induced by the depth maps. This phenomenon is likely because the depth produced by MiDaS is scale and shift invariant (i.e., it is not metric depth, and background can appear closer than foreground).\n\n- Since all that matters is generating a coherent image, I would imagine that other types of spatial conditions (e.g., segmentation masks for both foreground and background) can work equally well if used for training. I encourage the authors to test this hypothesis, and design additional ablation experiments to fully reveal the behavior of their model."
            },
            "questions": {
                "value": "- The illustration of soft guidance in Figure 2 is confusing. I personally prefer color coding of the attention maps to highlight the regions influenced by different tokens.\n\n- Details about the reconstruction experiments (Figure 6) are lacking. It is unclear from the text what is the exact evaluation procedure. Also the MAE values reported in Table 2 is not meaningful, again because MiDaS does not predict metric depth. Please include qualitative comparison between the input and reconstructed depth maps."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695826964,
        "cdate": 1698695826964,
        "tmdate": 1699636445525,
        "mdate": 1699636445525,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "bWwlBMZynd",
        "forum": "p4eG8rCa0b",
        "replyto": "p4eG8rCa0b",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
        ],
        "content": {
            "summary": {
                "value": "The authors propose the Compose and Conquer (CnC) network that achieves 3D object placement and successfully integrates global styles and local conditions. To this end, the authors first propose depth disentanglement training (DDT) which disentangles the foreground and background depth and processes them with independent layers before fusing them together. Moreover, the paper also involves a novel soft guidance block that efficiently combines global and local conditions. Thorough qualitative and quantitative evaluations demonstrate the design and the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The strengths of the proposed paper can be summarized as:\n1. The paper is well-written and easy to follow\n2. The proposed DDT and soft guidance modules are novel and effective, demonstrated by both qualitative and quantitative results\n3. Evaluations are comprehensive and showcase better results than existing SOTA methods"
            },
            "weaknesses": {
                "value": "The weaknesses of the proposed paper can be summarized as:\n1. Type of conditions. (1) Is the model capable of applying different types of conditions? (2) Is the model capable of applying two different conditions simultaneously while recognizing the 3D relations? I am rather interested in these different situations especially considering that the authors only employ depth in the submitted paper. More examples or scenarios would be appreciated.\n2. Image triplets. There is no visualization of the prepared image triplets for training. I am curious regarding the quality of foreground image, background image and foreground mask. It's also especially important to analyze the inpainted background image and how it would negatively affect the training and final outcomes.\n3. Qualitative results. (1) What are the prompts for examples in Figure 3? (2) Through the visualization in Figure 3, 4 and 5, it's interesting to see that the final generated images do not fully reflect the foreground depth condition. Meanwhile, the background depth map is often ignored through the qualitative results.\n4. No limitations and societal impacts are discussed in the submission."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4652/Reviewer_taqR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4652/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699425680654,
        "cdate": 1699425680654,
        "tmdate": 1700613307013,
        "mdate": 1700613307013,
        "license": "CC BY 4.0",
        "version": 2
    }
]