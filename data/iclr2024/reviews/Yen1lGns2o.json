[
    {
        "id": "UryWVVBzLM",
        "forum": "Yen1lGns2o",
        "replyto": "Yen1lGns2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates self-supervised representation learning from video with a specific focus on the data distributions. In particular, the paper questions the needs of using large-internet scale image datasets and propose instead to learn representation by watching few long videos.\n\nThe paper makes two mains contributions:\n-\tThe WTtour datasets which composed by 10 long-videos \n-\tDORA, a self-supervised representation learning approach that learns to represent and track object in a video at the same time.\n\nThe paper evaluates the learned representations on various downstream tasks including ImageNet linear probing, Pascal VOC unsupervised object discovery, MS-COCO/ADE20K object detection/segmentation and DAVIS-2017 for video understanding. DORA pretrained on WTours demonstrates strong performances on ADE20K and MS-COCO"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper explores the pretraining of visual representation using few long videos which is an original and novel empirical setting.\n\n- They propose a new datasets WTours which could be of interest to the representation learning community.\n\n-  Dora obtains reasonable performance when fine-tuning the performances on ADE-20K and MS-COCO."
            },
            "weaknesses": {
                "value": "- Performance on ImageNet linear probing seems low. While I understand that video-pretrained models have a disadvantage over image-pretrained model as they can\u2019t be pretrain \u2018in-distribution\u2019 with respect to imagenet. However, ImageNet is a standard vision task. It is important to understand why there is such a gap between image and video models on this evaluation.\n\n- The DINO baseline is trained for 100 epochs only which is not the default setting. Additionally, DINO paper reports a performance of 61.8 with a VIT.S/16 on Davis while the paper reports of 54.6 for the same method. I would encourage the authors to report what are the performances of the DINO released models as those models are available.\n\n- DORA shares some similarity with the VITTO. Both approaches learn from video and use an 'unsupervised' pooling mechanism. However, DORA seems to underperform VITTO on the ADE20K and MS-COCO tasks. \n\n- Missing comparison with more recent baselines. It would be nice to add comparison with DINOv2 and a weakly-supervised baseline  OpenCLIP, which are both state-of-art methods."
            },
            "questions": {
                "value": "I like the motivation and the novel exploration of the paper. However, I think the experimental evaluation could be improved to better support the claims of the paper. \n\nFirst, I think comparing with state-of-art image-baseline such as DINOv2 and CLIP on the different tasks would really highlight the importance of video pretraining. Second, I think it would be useful to discuss in depth the relation with the VITTO approach. Finally, the current approach currently falls short of image-pretrained model on ImageNet. It would be nice if the author could discuss this limitation in the manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_GQ4C"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763183113,
        "cdate": 1698763183113,
        "tmdate": 1700508085799,
        "mdate": 1700508085799,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "HgpHHR9IeZ",
        "forum": "Yen1lGns2o",
        "replyto": "Yen1lGns2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_krCr"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a method for self-supervised learning of image representation models. It is based on a similar scheme to DINO (Caron et. al. 2021) that learns image representation by distilling a moving average teacher model's representation of the global image view to the representation of a student model's on multiple local views. The novel way of achieving this in this work is to use the feature correspondence provided in hours-long walking tour videos to provide tracking, which can identify the location of different objects in any video frame without annotation. This object-centric way of generating local views seems to lead to good learned representation, which is examined in the experimental section.\n\nIn total, the proposed method does not need curated image or video data for self-supervised learning and can learn effective representation from hours-long walking tour videos. The proposed multi-object masking approach is shown to be significant in learning effective visual representation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "+ The fact that this method does not use curated data is a big plus for me. The way of collecting this type of data seems to be easily scalable, as it can be from walking tours or vehicle-mounted cameras. \n\n+ Using correspondence-based tracking to provide localization is sound and practical in representation learning. \n\n+ The experimental results show the model trained with the proposed method on the walking tour videos can outperform strong baselines trained on curated datasets such as ImageNet and Kinetics-400."
            },
            "weaknesses": {
                "value": "- One minor issue I have about the presentation is the introduction of the tracking module. The tracker is not learned, and it is only used to provide object locations. I would like further discussion on the potential use of the corresponding information. Also, a comparison on the effect of using different types of unsupervised trackers would also help strengthen the work as the major idea seems to be not dependent on a certain type of tracker."
            },
            "questions": {
                "value": "I have the following questions after reading the text:\n\n1. In Eq. (8) the learning is done on a single frame. Because the tracking has already provided corresponding between locations across multiple frames, is there a certain consideration to not use views from multiple frames in this loss function?\n\n2. The authors have presented 10 walking tour videos. The results in Table 5 suggest training on one video already achieves similar accuracy obtained by training on all videos. Does this mean one video is sufficient? Is there some point in further scaling up the training data? I would like to see a discussion on this topic."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698814058417,
        "cdate": 1698814058417,
        "tmdate": 1699636163895,
        "mdate": 1699636163895,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vfRXQvOmOn",
        "forum": "Yen1lGns2o",
        "replyto": "Yen1lGns2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed a new perspective on self-supervised learning (SSL). Instead of pretraining models on ImageNet-like object-centric datasets, the paper pretrained the models on egocentric videos (\u201cWalking Tours\u201d dataset) which depict numbers of objects and are comparable with human learning. Compared with other video datasets for SSL, Walking Tour dataset had more objects and classes and more gradual shifts in lightness. To pretrain on Walking Tours, the paper proposed a novel SSL method, based on DINO, to first discover objects and then track objects, named DoRA. In every batch, DoRA randomly sampled 8 frames temporally separated by 1 second, discovered objects in the first frame, and tracked them over the following 7 frames. In the default setting, objects are tracked by cross-attention in the multi-object tracker, which leads to spatially overlapping. The paper then proposed to establish object-patch correspondences using the Sinkhorn-Knopp algorithm to deal with the problem. After finding separated objects, the input video clip in the student branch would be masked to contrast with the clip in the teacher branch. Experiments on dense prediction tasks show that DoRA on Walking tours shows comparable performance with other SSL methods pretrained on ImageNet."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThis paper proposed a new pretraining method on egocentric videos which are uncurated and comparable with human learning. As these videos do not involve human annotation, they can be easily obtained, leading to a promising way of SSL.\n2.\tThe proposed method is simple and neat. DoRA provides an intuitive but effective way to learn from frames that contain multiple objects."
            },
            "weaknesses": {
                "value": "1.\tThe paper mainly talks about how we can learn discriminative representations from egocentric videos, whereas what kind of egocentric videos are suitable for DoRA is not deeply discussed. It would contribute more to the community if we knew what properties a video should have to be worth learning.\n2.\tA good SSL method should be scalable, not only on the dataset but also on the model structure. It would be better for authors to show more results on larger ViTs.\n3.\tSome minor writing problems. (1) in Sec. 4 \u201cDiscovering objects with multi-head attention\u201d, $\\widetilde{Q}$, $\\widetilde{K_t}$ are only defined in Fig.3 and are not defined in text. (2) In Fig.3 (Left), the input should be $X_t^{o_i}$"
            },
            "questions": {
                "value": "1. In Table 4, why does DoRA perform worse when using WT_all than when using WT_Venice?\n2. DoRA shows inferior performance on ImageNet linear probe (LP) but superior performance on dense prediction tasks, would it perform better than other contrastive methods on the ImageNet fine-tuning task? Like MAE [1], lower on LP but higher on fine-tuning.\n\n[1] He, Kaiming, et al. \"Masked autoencoders are scalable vision learners.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_CACu"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698832001681,
        "cdate": 1698832001681,
        "tmdate": 1699636163814,
        "mdate": 1699636163814,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LfLYBJv4GP",
        "forum": "Yen1lGns2o",
        "replyto": "Yen1lGns2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs"
        ],
        "content": {
            "summary": {
                "value": "The paper proposed a dataset *Walking Tours (WT)* consisting of high definition street-walk videos of 23 hours total length, and DoRA, a multi-object-tracking-inspired framework to learn visual representation from different views of the same objects in adjacent frames. The proposed method largely follows the DINO framework, with the local crops in DINO replaced with a tracking of objects in a video. The method is tested on several mainstream visual tasks, including image detection and segmentation, video object segmentation, object tracking, image classification and object discovery. Compared to its baseline DINO, the proposed method outperforms it clearly when using the same WT dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The motivation to learn visual representation using of the appearance variation of object along time is sound and is probably worth exploring in the future as an additional source of information in self-supervised visual learning.\n\n* The proposed architecture to utilize temporal information by tracking the same object in different frames is novel, and the visualization can justify that the proposed method is working as expected."
            },
            "weaknesses": {
                "value": "* **Scalability concern.** Although the proposed method does outperform DINO on all reported experiments in controlled experiments (i.e., with the proposed WT dataset), the results with even WT_{all} is still not clearly better than DINO with ImageNet-1k. This leaves it unknown whether the WT dataset will eventually outperform ImageNet-1k (or the even larger ones like ImageNet-22k and LVD-142M) with the dataset at a reasonable scale. Also the experiments are mostly on ViT-S, which is relatively small compared to the well-known works in the field (which usually report at least ViT-B), so it is also hard to tell whether the proposed method scale well with the model size.\n\n* **Significantly worse results on image classification.** I have noticed that the image classification results of WT-pretrained models are lower than ImageNet-1k-pretrained by a fairly large margin (45LP / 36KNN on WT vs. 72LP / 70KNN on ImageNet). Although one can argue that this is because of the domain gap between WT and IN, I would consider the accuracy difference large enough to require some formal justification (e.g., run WT and IN pretrained models on a 3rd classification dataset like iNaturalist or Places) to conclude that the WT-pretrained models are not significantly weaker on image classification tasks.\n\n* **The potential privacy and safety issues of the dataset.** Also see *Details Of Ethics Concerns*. As the paper claims the dataset as a main contribution, I would expect more efforts in assessing the privacy and safety issues in the dataset (e.g., How many clear faces are detected and what are their resolutions? How many harmful scenes are detected to the best effort of the authors?) and clarifying the legal issues and usage restrictions of the dataset (e.g., Is it possible that some videos are taken down upon the request from people appearing in them? Is their usage in some jurisdictions not allowed / limited to non-commercial only? What are some possible negative effects if the models remember the private information in it? What are the possible effects of some common mitigations, like blurring the faces in Google street view?)"
            },
            "questions": {
                "value": "* In addition to training epochs, it would be kind to also mention the actual training time as a more practical measurement of the training cost.\n\n* In Appendix D, are there any other differences between DoRA without tracking and DINO other than the crop generation method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper proposes a new dataset consisting of 23 hours of UHD (4k) videos filmed on the public streets which may contain personal information like high resolution faces of strangers and audio recordings of the nearby people talking (very likely) without their consent. Although the videos are not filmed by the authors themselves and are in CC-BY licenses on YouTube according to the paper, I'm concerned that it needs a careful discussion regarding the compliance issues or restrictions of using them for machine learning purposes (or even posting them on YouTube in the first place) in different jurisdictions.\n\nThere is also no assessment or mitigation about the potentially harmful scenes (e.g., violent, harassing, criminal) in the proposed dataset."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_DfLs",
                    "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698839111545,
        "cdate": 1698839111545,
        "tmdate": 1700712460331,
        "mdate": 1700712460331,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ytDhdexGqR",
        "forum": "Yen1lGns2o",
        "replyto": "Yen1lGns2o",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
        ],
        "content": {
            "summary": {
                "value": "The authors consider how both (a) data and (b) method can improve training an image encoder in a self-supervised manner. Regarding data, they introduce an open-source dataset containing long, first-person videos, and propose several advantages of this over curated image (and video) datasets. Given this dataset contribution, the authors propose a self-supervised method which tracks objects to act as a signal for a classical multi-view self-supervised learning (SSL) loss. This method leverages some key properties from the dataset, specifically that the videos have natural scene transitions and are of a person-person view. Instead of using off-the-shelf object-trackers or optical flow to establish correspondence, the authors use the attention-map between the [CLS] token from a selection of heads and the patch-embeddings. Optimal transport is used to establish unique object-patch correspondence (i.e. non overlapping patches) and then given this multi-view correspondence, a technique based on DINO is used.\n\nOverall the authors show that for many downstream tasks, their method (DORA) pre-trained on (even one) video where the number of frames is comparable to imagenet-1K (IN-1K) achieves better performance than DINO pretrained on IN-1k. In comparison when DINO is pretrained on the same data, the peformance is worse than IN-1K which suggests it is the unique coupling of dataset and training-method which helps the authors achieve SOTA results"
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and the supplementary work provides a good level of detail and convincing ablations and visualisations\n- The contribution of the open-source dataset (10 videos) to replicate this work is very useful for the community\n- The statistical analysis of the contributed dataset also useful\n- This is a nice use-case of Sinkhorn\u2013Knopp to avoid having to use non end-to-end approaches like optical-flow or off-the-shelf object-detectors and the motivation which shows overlapping spatial regions when linearly projecting the attention-map (instead) is a good argument for its use\n- Overall, the results with DORA are very impressive"
            },
            "weaknesses": {
                "value": "- I'm a bit confused about Table 4: Video Object Segmentation (DAVIS-2017). In the DINO paper they report ViT-S/16 with INet getting 61.8, 60.2, 63.4 respectively (their Table 5), however your Table 4 reports DINO as getting 59.4, 57.4, 61.4 what accounts for this difference?"
            },
            "questions": {
                "value": "- What is the stability like when using SK, aside from the entropy regularisation is some annealing schedule needed that transforms the coupling matrix from soft to hard gradually?\n- Is there any intuition why the k-attention maps obtained by projecting the attention map are spatially overlapping? Is there any possibility to use a simple heuristic to avoid it that can be ablated with SK?\n- What is the training cost of using SK for every forward-pass like this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2314/Reviewer_YYAs"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2314/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699009476598,
        "cdate": 1699009476598,
        "tmdate": 1700577071014,
        "mdate": 1700577071014,
        "license": "CC BY 4.0",
        "version": 2
    }
]