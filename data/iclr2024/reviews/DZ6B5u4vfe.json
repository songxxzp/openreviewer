[
    {
        "id": "cHtkyU5bx7",
        "forum": "DZ6B5u4vfe",
        "replyto": "DZ6B5u4vfe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_WxVH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_WxVH"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a study about the relation between LLMs and humans. The motivation is that the instruction-tuned LLMs carry out human instructions better (seems closer to humans). Analysis on both brain activities  shows a closer alignment from LLMs after instruction tuning. The authors also found that the world knowledge and model size are strongly correlated with brain alignment."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The authors examined two famous and commonly used families of instruction tuned models and find a consistent phenomenon. They also observed the gradual increase in brain score during instruction tuning.\n\n2. The authors studied the models\u2019 fit to both human neural and behavioral data."
            },
            "weaknesses": {
                "value": "1. It remains unclear in the whole passage that which \u201cinternal representations\u201d from LLMs are used, which makes it difficult to reproduce the results.\n\n2. The \u201cworld knowledge\u201d part in the BBH dataset is different from the knowledge required in MMLU. The formal consists subsets such as Sports Understanding, Movie Recommendation, and Causal Judgement; While the latter is mainly about disciplinary knowledge such as Anatomy and College Physics. This makes the key term, \u201cworld knowledge\u201d, much ambiguous. What knowledge are considered \u201cworld knowledge\u201d? Are there any difference between factual, general and disciplinary knowledge?\n\n3. The authors tried to study the effect of world knowledge and model size separately in Section 4.2. However, the two factors are deeply intertwined, given that larger LLMs tend to outperform smaller ones in knowledge-related question answering. The results in Figure 2 also show that model size has even stronger and more significant effect on the brain score. As a result, it cannot be concluded that \u201cworld knowledge\u201d is a key contributor to the increase in brain score. Instead, it can be just another indirect effect of the larger model size.\n\n4. The authors use the performance on MMLU and BBH to represent the models\u2019 capability of \u201cworld knowledge\u201d. However, performance on these benchmarks is affected not only by the quantity of knowledge that the models possess, but also by their ability to follow instructions. Thus, a higher performance on MMLU and BBH doesn\u2019t necessarily mean that the model has more world knowledge, and the correlation between benchmark scores and brain scores does not necessarily show a link between world knowledge and the fit to human neural data.\n\n5. The authors use the correlation between model per-token perplexity and human reading time to represent the behavioral fit. However, they pointed out in Section 6.2 that this approach is controversial when applied to large Transformer-based models. Thus, the choice of this approach is confusing. Why not use other ways to test the behavioral fit?\n\n6. It is counter-intuitive that factual, domain knowledge can contribute the higher human fit in general reading. In fact, many questions in MMLU are difficult even for most people (e.g., Anatomy, Astronomy, College Physics, ...), and are not going to be retrieved during story reading. It is confusing why the authors choose MMLU as an aspect of the \u201cworld knowledge\u201d, and how this can guide Neuroscience research in human language understanding.\n\n7. The three fMRI datasets are in different settings, i.e., reading sentence by sentence, listening the whole passage, and reading word by word, which could bring different activation patterns in the human brain. However, the authors did not discuss the difference between them."
            },
            "questions": {
                "value": "See the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698830940712,
        "cdate": 1698830940712,
        "tmdate": 1699637164566,
        "mdate": 1699637164566,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sFD4UNdzNe",
        "forum": "DZ6B5u4vfe",
        "replyto": "DZ6B5u4vfe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
        ],
        "content": {
            "summary": {
                "value": "The paper explores the impact of instruction-tuning on large language models (LLMs) to determine their alignment with the human brain in terms of brain and behavioral alignment. Experimental results from two renowned LLM families indicate that instruction-tuning improves brain alignment by 6.2%, with world knowledge and model size being the primary contributors. However, instruction-tuning does not have a similar effect on behavioral alignment. The authors emphasize the importance of integrating world knowledge in future LLM developments."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper focuses on the instruction tuning of LLMs, exploring the neuroscience behind language models. This unique perspective advances the understanding of LLMs in the context of human cognition.\n\nThe experiment design is intuitive and relatively easy to follow. The experiments are extensive, including 3 datasets for brain and behavioral alignment, respectively."
            },
            "weaknesses": {
                "value": "Lack of comparative analysis with other tuning techniques such as reinforcement learning from human feedback (RLHF).\n\nThe investigation of behavioral alignment is limited. A more comprehensive exploration could offer insights into the discrepancy between brain and behavioral alignments and its implications for LLM development and application.\n\nOther alignment measure methods may be considered to increase the reliability of the results, such as:\n\nJiaang, Li, et al. \"Structural Similarities Between Language Models and Neural Response Measurements.\" arXiv preprint arXiv:2306.01930 (2023).\n\nLiu, Xu, et al. \"Coupling Artificial Neurons in BERT and Biological Neurons in the Human Brain.\" arXiv preprint arXiv:2303.14871 (2023)."
            },
            "questions": {
                "value": "What is the computational cost of this study? 33B model is quite large, are there any quantization techniques used like LoRA?\n\nMay need proofreading: Table 3 and Table 4 in the Appendix have the same caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission9249/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9249/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission9249/Reviewer_KJEe"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837823549,
        "cdate": 1698837823549,
        "tmdate": 1699637164447,
        "mdate": 1699637164447,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rT0R39WKAw",
        "forum": "DZ6B5u4vfe",
        "replyto": "DZ6B5u4vfe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_fAu2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_fAu2"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the effect of instruction-tuning in the alignment between LLMs\u2019 representations and human language processing. The authors use two types of human data: brain activity patterns (brain alignment) and reading times (behavioral alignment). The brain alignment is defined as the extent to which a linear regression model predicts brain activity patterns using the LLMs\u2019 representations. The behavioral alignment is defined as a correlation between LLM perplexity and human reading time for each word. Through the experiments across 25 vanilla and finetuned models from the T5 and LLaMA families, the authors conclude that instruction tuning improves brain alignment, (2) the performance on the world knowledge-related tasks and model size are correlated with brain alignment, and (3) instruction-tuning and other examined factors are not correlated with behavioral alignment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Using 25 models and two benchmarking datasets covering various task categories, the authors perform detailed analysis between LLM representations and human brain and behavioral data.\n- The discussion includes implications both for NLP and neurosciences along with the literature review, which can encourage interdisciplinary research across both fields.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- The authors use models from just two families, T5 and LLaMA. Looking at Figure 2, it seems that LLaMA models do not show a significant correlation between brain alignment and MMLU score, BBH world knowledge, and model size. The results would be more convincing if the authors could use a few more families such as GPT.\n- Concerning the tasks related to world knowledge, it appears that these tasks may simply exhibit greater linguistic diversity compared to the other tasks examined. The concept of world knowledge seems somewhat ambiguous, and any clarification could be insightful. For instance, would similar results be observed if more language understanding tasks were added? I was unable to determine how the BBH tasks are categorized into \"language understanding\" and \"world knowledge.\"\n\nI think that expanding the experiments to address these points could lead to more reliable results."
            },
            "questions": {
                "value": "- How did the authors determine the category classification for the BBH tasks?\n- Is it possible to provide a more detailed analysis regarding world knowledge? Any discussion and additional analysis would be appreciated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698838239568,
        "cdate": 1698838239568,
        "tmdate": 1699637164339,
        "mdate": 1699637164339,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "p0KUMoF6gG",
        "forum": "DZ6B5u4vfe",
        "replyto": "DZ6B5u4vfe",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_co7p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission9249/Reviewer_co7p"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the correlation between instruction-tuned LLMs and human similarity in the field of neuroscience by examining brain alignment and behavioral alignment. The authors evaluate 25 LLMs on a reading task to identify the effects of instruction tuning LLMs in terms of human language processing. Instruction turned LLMs have higher brain scores than vanilla LLMs, and further analyzed the properties of LLMs that contribute high alignment and found out that the model size and world knowledge are correlated to brain alignments. For the behavioral alignment, there was no correlation between per-word LLM perplexity and per-word human reading times."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "This paper explains why instruction-tuned LLMs perform better than vanilla LLMs from a neuroscience perspective by measuring brain scores."
            },
            "weaknesses": {
                "value": "- This paper appears to be a replication of [1,2], specifically focusing on instruction-tuned models. It lacks novelty and originality.\n    - Increasing the model size and integrating world knowledge (using a larger training dataset) are not surprising new discoveries for improving language modeling.\n    - Additionally, this paper measures the correlation between world knowledge tasks and brain alignment.\n    - This paper should demonstrate the effects of contributing factors separately (world knowledge and model size). The plots seem to be dependent on model size.\n- The current version of the paper requires further improvement.\n    - It lacks details for readers without a background in neuroscience.\n        - How is the brain score computed for each model? Does it compute the hidden state of every layer?\n    - In Section 4.1, the last paragraph seems to be located too early, making it difficult to understand before explaining the dataset.\n    - In Figure 3A, shouldn't the language stimuli be labeled as Futrell2018?\n    - Figure 3B appears to be an empty plot.\n\n\n[1] Schrimpf, Martin, et al. \"The neural architecture of language: Integrative modeling converges on predictive processing.\"\u00a0*Proceedings of the National Academy of Sciences*\u00a0118.45 (2021): e2105646118. \\\n[2] Oh, Byung-Doh, and William Schuler. \"Why does surprisal from larger transformer-based language models provide a poorer fit to human reading times?.\"\u00a0*Transactions of the Association for Computational Linguistics*\u00a011 (2023): 336-350."
            },
            "questions": {
                "value": "- How is the 'No Instruction' model trained in Figure 1D? The Alpaca instruction dataset is formed with both non-empty input fields (instruction, input, output) and empty input fields (instruction, output). Did you only use non-empty input fields and remove the instruction in those cases?\n- What aspect do you believe instruction tuning contributes to the correlation between world knowledge and brain alignment?\n- In Figure 2, it appears that there are different correlations for each dataset (Pereira2018, Blank2014, and Wehbe2014). Why is Blank2014's correlation so much lower compared to the other two?\n- How is word perplexity measured? Could you provide an example of input and the corresponding NWP loss?\n    - Since Flan-T5 models are encoder-decoder models, I'm not sure how they are measured differently from decoder-only models. Were the same inputs passed into both the encoder and the decoder?\n    - Did the vanilla LLMs also show no correlation?\n- Why is there a performance drop when Flan-T5 is fine-tuned on instruction tuning datasets (Alpaca, GPT4ALL, ShareGPT) as seen in Table 5 (Flan-T5-XL results)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission9249/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699243055584,
        "cdate": 1699243055584,
        "tmdate": 1699637164245,
        "mdate": 1699637164245,
        "license": "CC BY 4.0",
        "version": 2
    }
]