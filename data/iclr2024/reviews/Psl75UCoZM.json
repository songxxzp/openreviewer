[
    {
        "id": "F899EMDr1f",
        "forum": "Psl75UCoZM",
        "replyto": "Psl75UCoZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_4daR"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a point cloud forecasting-based world model for autonomous driving. The model first tokenizes point clouds into discrete BEV tokens (codebook/vocabulary) following VQVAE and UltraLiDAR (Xiong et al., 2023). Then tokens are decoded to reconstruct the point clouds with an implicit representation depth rendering branch and a classical coarse voxel reconstruction branch. MaskGIT is further leveraged to a discrete diffusion model, with different masking conditions and history information (the classifier-free diffuision) guidance applied, to realize the future prediction ability. The point cloud forecasting method is evaluated on three datasets and achieves state-of-the-art results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The presentation of the paper is good, especially in the methodology part. Symbols and figures are clear and helpful for understanding.\n- The model architecture is detailed in the appendix. The authors also provide details besides the model structure, such as the K-means clustering strategy to solve codebook collapse and LayerNorm to stabilize training, which are valuable empirical findings for future research.\n- MaskGIT with diffusion is interesting. It could probably be applied to other tasks as well."
            },
            "weaknesses": {
                "value": "- The reviewer is confused about the motivation of discrete tokenization and masked image modeling.\n  -  The proposed method adopts a VQVAE-like model to capture the complex 3D world, as mentioned in the introduction challenge (i). Classic BEV (the method in BEVFusion, BEVFormer, etc.,) can also realize this ability IMO. This undermines the motivation to use discrete tokenization and the necessity to use a discrete diffusion model in the after.\n  -  Table 4 presents the ablation of the discrete diffusion algorithm. The motivation to use MaskGIT seems its parallel decoding strategy. How about the masked image modeling? What will the results or inference time be like if no masked image modeling (or even MaskGIT) is applied?\n  -  The intuition of using different masking strategies for world model training comes from the robotics field. It would be valuable if ablations on this could be presented as well.\n  -  In light of the above points, a naive baseline should be simple diffusion modeling with simple BEV features.\n- The gain of point cloud forecasting mostly comes from CFG, which involves past poses and actions. Without this, the results are close to 4D-Occ. \n  - Taking the current action as input is reasonable for a world model, but much information about history could make the prediction rely heavily on the past. If the prediction horizon is longer, such a long history is also weird as poses at the very beginning are intuitively unhelpful. \n  - As this task implicitly involves ego-planning, this could lead to causal confusion though impressive results are obtained under the open-loop scenario. The authors have stated that combining the world modeling approach with model-based RL is a future direction, yet, it is important to demonstrate its effectiveness for the decision-making task."
            },
            "questions": {
                "value": "- In the introduction, the task definition of point cloud forecasting is 'to predict future point cloud observations given past observations and future ego vehicle poses'. In which paper, the future ego vehicle pose is provided?\n- Why not report full results under all metrics in ablation study tables?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698675425390,
        "cdate": 1698675425390,
        "tmdate": 1699636226394,
        "mdate": 1699636226394,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4PwVINe278",
        "forum": "Psl75UCoZM",
        "replyto": "Psl75UCoZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
        ],
        "content": {
            "summary": {
                "value": "The study focuses on the development of unsupervised world models to enhance an autonomous agent's understanding of its environment. Though world models are a form of sequence modeling, their adoption in robotic applications like autonomous driving hasn't scaled as rapidly as language models such as GPT. Two primary challenges identified are the complex nature of observation spaces and the need for a scalable generative model. To address these, the research proposes a novel approach: (1) Tokenization of Sensor Observations. (2) Prediction using Discrete Diffusion. Applying this method to point cloud observations (which play a crucial role in autonomous driving) showed a significant improvement. The proposed model reduced the Chamfer distance by more than 65% for a 1-second prediction and over 50% for a 3-second prediction on major datasets like NuScenes, KITTI Odometry, and Argoverse2."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The contribution is clear and important to the autonomous driving society. Developing a driving world model is recognized as a critical step for scene understanding and decision-making.\n\n2. This paper is well-written and easy to follow. The figures are intuitive and informative. \n\n3. The experimental results are surprisingly good, which improves a lot over existing SOTA methods."
            },
            "weaknesses": {
                "value": "1. The metrics for evaluating the performance of the world model may not be reasonable enough. For point cloud, most of the points describe the background, which is usually static and irrelevant to the downstream task. The prediction of the motion of dynamic objects is more important. Maybe the author can also report the comparison results for dynamic objects or show the advantage of using such a model for some downstream tasks.\n\n2. The conclusion says \u201cOne particularly exciting aspect of our approach is that it is broadly applicable to many domains. We hope that future work will combine our world modeling approach with model-based reinforcement learning to improve the decision-making capabilities of autonomous agents.\u201d  It is unclear to me what the advantage of using such a world model is for MBRL, for example, compared to object segmentation and tracking pipelines.\n\n3. There are many complex modules in the pipeline, including VQ-VAE, diffusion model, neural feature grid, and transformer. Not sure if it is easy to reproduce the results and extend it to other datasets or tasks."
            },
            "questions": {
                "value": "1. Could the authors elaborate more on \u201cusing past agent history as CFG conditioning improves world modeling\u201d. What agent history is used here and how is it used? Does it introduce additional knowledge and make the comparison unfair?\n\n2. Since the proposed method introduces a world model, could the authors demonstrate some qualitative examples of different future predictions with different actions? A longer horizon could be helpful to check the consistency of generated frames. I wonder how diverse the future prediction is and how accurate the prediction matches the given action."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_amws"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698709114584,
        "cdate": 1698709114584,
        "tmdate": 1700445967714,
        "mdate": 1700445967714,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1TgRv4Osxm",
        "forum": "Psl75UCoZM",
        "replyto": "Psl75UCoZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use a VQ-VAE+diffusion approach (similar to certain image diffusion pipelines) for the task of learning point-cloud world models for autonomous driving. They design task-specific encoder and decoder architectures to encode point-cloud observations as a sequence of discrete tokens, apply an interleaved spatial-temporal transformer and a discrete diffusion model to predict discrete codes for future frames, and decode with a model based on neural occupancy representations. The proposed model compares favorably to SOTA baselines for the task on standard metrics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The proposed approach gives strong empirical performance. The architecture takes advantage of structure in the problem at several points in useful and interesting ways (in particular the combination of localized neural occupancy and BEV tokenization is quite interesting, and seems novel). I think the backbone (transformer+discrete diffusion) is comparatively less novel, but this is the first time I've seen it applied to the autonomous driving setting and it is interesting to see that it still gives strong performance.\n\nSeparately the authors propose several improvements to MaskGIT. These modifications seem to be crucial for the performance of their algorithm, but it would be interesting to see if these improvements generalize to the original image-based setting or to other discrete diffusion settings (though doing this on anything other than a toy problem would probably be outside of the scope of this paper).\n\nThe authors also identify an issue with standard point-cloud prediction metrics and propose a simple modification. Though this part may be less relevant to the ICLR community, it is an important observation for the self-driving community and should not be overlooked."
            },
            "weaknesses": {
                "value": "The proposed architecture is highly specific to point-cloud occupancy prediction (the novelty lies largely in the encoder and decoder, which are task-specific architectures).\n\nThe introduction/related work are somewhat intermixed, which is fine, but leads to confusing presentation. It's not entirely clear from the introduction/related work how the proposed method relates to MaskGIT, and although MaskGIT is heavily referenced it is never clearly described. Background of discrete diffusion could also be better described.\n\nAblations of differences to MaskGIT are good, but it would be useful to also present ablations of the other task-specific model components (encoder/decoder, and maybe also the BEV token grouping?) as compared to general-purpose versions.\n\nMinor note: point cloud prediction visualizations are a little difficult to parse - I'm not sure how they could be improved but it's quite difficult to analyze results/see what's changed between two different images, both in Fig. 1 and Fig. 5."
            },
            "questions": {
                "value": "- It would be interesting to conduct a more thorough analysis of the modifications to MaskGIT, perhaps on a simpler discrete diffusion problem.\n - Why is L1 median reported inside the ROI but L1 mean reported for the full scene?\n - How important are the novel task-specific encoder and decoder (including the rendering and reconstruction losses on the encoder) to the final performance?\n - It's mentioned that the model is relatively small; how much do the results change when scaling the model (up/down)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2827/Reviewer_2kSt"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698833309192,
        "cdate": 1698833309192,
        "tmdate": 1699636226243,
        "mdate": 1699636226243,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "g71pJI9SrH",
        "forum": "Psl75UCoZM",
        "replyto": "Psl75UCoZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_9NMX"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a state-of-the-art world model for driving data. Among several major contributions, the paper describes a new way to tokenize point clouds using a VQVAE combined with a PointNet; the paper also proposes a combination of generative masked modeling and discrete diffusion for learning a world model. The proposed method is tested on three commonly used lidar datasets and is shown to achieve state-of-the-art on 1s and 3s time horizon prediction."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "1. The paper proposes a tokenizer for point clouds, which could have major applications across robotics.\n2. The combination of MaskGIT with discrete diffusion and classifier-free guidance is novel. The idea of both decoding and denoising tokens is very interesting.\n3. The proposed model outperforms prior state-of-the-art by a large margin.\n4. The methods section is clear even though it proposes several novel models and losses."
            },
            "weaknesses": {
                "value": "The unnumbered first equation in Section 3 should be explained better.\n\nMinor:\n* It is not fully clear to me what \u201cSE(3) ego poses\u201d mean.\n* Figure 1 and 5 might be easier to read if you zoom in on the circled areas."
            },
            "questions": {
                "value": "1. \u201cWe hope that future work will combine our world modeling approach with model-based reinforcement learning to improve the decision making capabilities of autonomous agents.\u201d \u2013 Are you planning to release your code?\n2. What hardware is required to train your model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "10: strong accept, should be highlighted at the conference"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699024143538,
        "cdate": 1699024143538,
        "tmdate": 1699636226171,
        "mdate": 1699636226171,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rArc0WbxO5",
        "forum": "Psl75UCoZM",
        "replyto": "Psl75UCoZM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2827/Reviewer_2hr7"
        ],
        "content": {
            "summary": {
                "value": "The work presents a groundbreaking technique for learning world models in an unsupervised manner, with a particular application to autonomous driving. It addresses the complexity of interpreting unstructured sensor data by implementing a Vector Quantized Variational AutoEncoder (VQ-VAE) to tokenize this data, followed by the prediction of future states through a discrete diffusion process. This technique modifies the Masked Generative Image Transformer (MaskGIT) into a discrete diffusion model, which leads to a substantial increase in prediction accuracy. The proposed approach stands out for its ability to tokenize sensor inputs and utilize a spatio-temporal Transformer for the efficient decoding of future states, which has demonstrated an improvement in prediction accuracy over existing methods on autonomous driving datasets. The model achieves a significant reduction in prediction errors and also shows competence in generating both precise short-term forecasts and diverse long-term predictions, thereby holding great promise for the application of GPT-like learning paradigms in robotics."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper introduces a novel approach by combining VQ-VAE tokenization with a discrete diffusion process, which is kind of innovating. The idea of simplifying the observation space and tokenizing the observation space makes it much easier to model the complex observation space that are usually the case for self-driving.The proposed method's improvement is demonstrated through rigorous experimental validation, showing significant improvements in prediction accuracy over existing methods.The reduction in Chamfer distance for both short-term and long-term predictions indicates a high-quality advancement in the field of point cloud predictions. The paper is also well-structured, with a clear exposition of the methodology, which includes tokenization of sensor data and the subsequent prediction process."
            },
            "weaknesses": {
                "value": "The paper mostly addresses the prediction of near term future states but it is not clear if we go much further, how would the accuracy be? With a diffusion model, the inference could be slow, so this model may not be suitable for use on board but mostly would be useful for simulations and other tasks that don't require real time feedback or predictions. This may limit the application of this approach."
            },
            "questions": {
                "value": "How is the result if we predict much further, like 9s? In other dataset, like WOMD, the prediction horizon tends to be slightly longer so it would be great to know if the performance would drop significantly if we predict much further away states. Secondly, could you also share some insights on how this model could exactly be integrated within modern self-driving systems and work with other modules such as planning? How would noise in perception, like Lidar affect the prediction accuracy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2827/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699258125735,
        "cdate": 1699258125735,
        "tmdate": 1699636226082,
        "mdate": 1699636226082,
        "license": "CC BY 4.0",
        "version": 2
    }
]