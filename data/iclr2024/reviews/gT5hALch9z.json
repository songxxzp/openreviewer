[
    {
        "id": "anXr4mswBa",
        "forum": "gT5hALch9z",
        "replyto": "gT5hALch9z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_gf5T"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_gf5T"
        ],
        "content": {
            "summary": {
                "value": "The authors created a dataset for evaluating improper behavior of LLMs and the trade-off between safety and helpfulness objectives.\nThey report on experiments with different fine-tuning variants of LLAMAs, showing some unsurprising results (there is a trade-off between safe behavior and helpful behavior, the amount of labeled data matters) and some less obvious results (asking the model to provide an opinion yields safer answers and asking to answer a question to provide instructions; finding the right balance between safety and helpfulness is difficult)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Experiments seem sound and done carefully. Not all results are trivial and are thus worthy of being reported in a paper. The dataset could be useful for further evaluation of alignment and safety methodologies."
            },
            "weaknesses": {
                "value": "The claims on page 2 need to be redone. The ones that are shown are (a) not novel enough (e.g. there is a tension between the two objectives) and (b) redundant (2 and 3 are different ways of stating 1). \n\nHowever, there are real contributions (such as the creation of the dataset and the observed experimental results) which should be highlighted in the claims.\n\nAlthough the experiments may help to adjust current LLMs in a slightly safer way, this may all be useless if the overall LLM with RLHF or safety fine-tuning is very easy to break with a little bit of appropriately chosen fine-tuning, e.g. see Qi et al \"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!\", arXiv:2310.03693"
            },
            "questions": {
                "value": "How do the authors put their work in the perspective of the results from Qi et al 2023 (arXiv:2310.03693)?\nIs it even worthwhile pursuing this kind of avenue if fine-tuning for safety can be completely broken so easily?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698514803302,
        "cdate": 1698514803302,
        "tmdate": 1699636223437,
        "mdate": 1699636223437,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "fuJraLl0rr",
        "forum": "gT5hALch9z",
        "replyto": "gT5hALch9z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_yQ2D"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_yQ2D"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the safety concerns of large language models (LLMs) that focus on helpfulness rather than harmlessness in their instruction-tuning. It reveals that popular instruction-tuned models are highly unsafe, and adding just 3% safety examples can significantly improve their safety. While safety-tuning does not significantly reduce models' capabilities or helpfulness, it can lead to exaggerated safety behaviors, where models refuse perfectly safe prompts if they resemble unsafe ones. The results highlight the trade-offs between training LLMs to be helpful and harmless."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The paper addresses an important and timely direction, which is the safety concern of LLM\n+ The paper is easy to follow in general \n+ Figure 1 provides a good overview of the main focus of the paper"
            },
            "weaknesses": {
                "value": "- The collected safety dataset is too small, many are about 100.\n- The difference with related work is not deeply discussed, e.g., the reasons for not using the red teaming dataset in safety training are not convincing enough \n- There are some parts of the experiment results requiring further explanation"
            },
            "questions": {
                "value": "In general, the idea of the work is interesting, to evaluate the safety of the existing open-source LLMs and find that a small amount of safety demonstrations could improve the safety of the LLM. The manuscript is easy to follow and the idea is not hard to understand. However, I think the biggest problem the paper has is the evaluation is kind of weak, e.g., the size of the dataset is small (many are about 100) and some experiment results are not well-explained. I list the questions below.\n\n1. Section 2: Maybe the authors could define what safety is before a deeper discussion. Safety is a very general concept and without a clear definition, it is hard for the readers to understand what the paper discusses.\n\n2. Page 4: \n> We randomly selected 2,000 questions from this dataset and used GPT-3.5-turbo to generate \u201csafe\u201d responses to them.\n\nWhy do you choose ChatGPT is generate safe responses and how do you guarantee that? Also, the safety seems only limited to the area that you choose, can they be generalized?\n\n3. Section 3.3: Why not use the red teaming dataset?\n\n4. Figure 3(a): Why safety training is not effective in O-Controversial dataset?\n\n5. Page 8: \n> Models\u2019 \u201copinions\u201d do not reflect models\u2019 behavior.\n\nI don't quite get the meaning of this part.\n\n6. Page 8:\n> Too much safety can be detrimental.\n\nMaybe you could go over the dataset and get some more stronger numbers. With example only is not convincing enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681753794,
        "cdate": 1698681753794,
        "tmdate": 1699636223366,
        "mdate": 1699636223366,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xXJKAC225c",
        "forum": "gT5hALch9z",
        "replyto": "gT5hALch9z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates approaches for improving safety in large language models that have been fine-tuned to follow instructions. The authors show that popular LLMs can generate unsafe or harmful responses when given malicious prompts. They demonstrate that incorporating even small amounts of safety data during instruction tuning, such as a few hundred examples, can substantially reduce unsafe behaviors without negatively impacting performance on standard benchmarks. However, too much safety data leads to models refusing safe prompts, a problem they term exaggerated safety. The paper releases datasets and tools for evaluating safety issues in language models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Clearly frames the tension between helpfulness and harmlessness in instruction-tuned LLMs. Shows examples of popular models complying with unsafe instructions.\n- Examines the tradeoff between safety and capabilities, finding that even not too much safety data leads to exaggerated safety behaviors where models overly refuse safe prompts."
            },
            "weaknesses": {
                "value": "- The tradeoff of safety and helpfulness is already a well-known fact.\n- The safety training data is limited in scope and size. Scaling up safety data could lead to different conclusions.\n- The current models still seem susceptible to simple adversarial attacks or instructions framed differently.\n- The paper mainly echo with existing finding in literature without proposing novel methods to solve the safety helpfulness tradeoff."
            },
            "questions": {
                "value": "Have you continued experiments with scaling up the amount of safety data? If so, did you find any \"sweet spots\" where safety improves without exaggerated safety effects?\nHow resilient are the safety-tuned models to variations in the phrasing of unsafe prompts or simple adversarial attacks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2808/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2808/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2808/Reviewer_cMF9"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817084386,
        "cdate": 1698817084386,
        "tmdate": 1700875106923,
        "mdate": 1700875106923,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LfA3S5yXkU",
        "forum": "gT5hALch9z",
        "replyto": "gT5hALch9z",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the safety-tuning of LLMs. It creates a public fine-tuning dataset for safety-tuning, which is currently lacking. This dataset it presents a study on how mixing safety data and common instruction-tuning data would impact the safety and utility of resulting models. It is shown that, by mixing a small portion of safety data, the models would be much safer while do not suffer from too much utility drops."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The evaluation is comprehensive, covering a diverse set of safety evaluation datasets with focuses on different aspects of model safety. It also considers multiple approaches to evaluate the quality of the response, evaluating the utility of the models. Human studies are also involved.\n\n2. Contribution of a publicly available safety tuning dataset.\n\n3. The analysis is insightful, providing a lot of practical insights gained from safety-tuning experiments."
            },
            "weaknesses": {
                "value": "The novelty is my primary concern. It is known that aligned models like ChatGPT and Llama-2 involve instruction tuning with safety data. That's exactly what makes these models safer than unaligned ones. What the paper does is just repeat an experimental study on instruction tuning. This makes the contributions of the paper seemingly insignificant. \n\nHowever, the details of the safety tuning of ChatGPT/Llama-2 are not publicly transparent. Especially, the safety data are not publicly available, and we neither know how different design choices will impact the effectiveness. From this perspective, this work can be publicly accessible material for the public to understand and reimplement the practice."
            },
            "questions": {
                "value": "Can you explain more on the novelty of this work given my above concern?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2808/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2808/Reviewer_HxPh",
                    "ICLR.cc/2024/Conference/Submission2808/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2808/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698821582325,
        "cdate": 1698821582325,
        "tmdate": 1699793378983,
        "mdate": 1699793378983,
        "license": "CC BY 4.0",
        "version": 2
    }
]