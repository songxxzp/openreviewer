[
    {
        "id": "oVQ4yrcWkP",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
        ],
        "content": {
            "summary": {
                "value": "The paper uses the task of regression to study connections between the Information Bottleneck principle and the topology of feature space. Based on these connections, the authors introduce a regularisation scheme"
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper introduces an interesting approach to study representation learning from the point of view of the information bottleneck principle, but there are some problems in the presentation that prevent me from giving a fair evaluation of the analysis (see below).\n\nAdditionally, not being an expert in topology, I am not able to assess the soundness and value of the method introduced in section 5."
            },
            "weaknesses": {
                "value": "There are some points of the theoretical analysis that I do not understand and prevent me from giving a proper evaluation. The analysis hinges on theorem 1, but there are two aspects of it that I do not understand:\n\n1. Minimising the difference is not generally equivalent to minimising the ratio, implying that the equivalence is a result of I being the mutual information. Can the author provide details on this equivalence? Is there a difference between I and \\mathcal{I} in the proof of theorem 10?\n\n2. Both Z and Y are deterministic functions of X. What does it imply for the entropy of Z conditioned on Y? Is it only meaningful when the mapping from X to Y is one-to-many? Does your theory suffer from the problems associated with having deterministic functions, as discussed in 'ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING' by Saxe et al, ICLR 2018?\n\nAdditionally, \n\n-The accessibility of Section 4 is limited to readers with advanced knowledge in topology.\n\n-The experimental evaluation is limited to a very simple architecture (two-layer network with 100 hidden units)"
            },
            "questions": {
                "value": "See weaknesses above. In addition:\n\n1. What is the meaning of `regression representation learning'? Do you mean representation learning in a regression task?\n\n2. Are Swiss Roll and Mammoth standard jargon in topology?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1763/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1763/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1763/Reviewer_iGsx"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698747199386,
        "cdate": 1698747199386,
        "tmdate": 1699636105689,
        "mdate": 1699636105689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RgJR9P2HFY",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_awEv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_awEv"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Persistent Homology Regression Regularizer (PH-Reg) as a novel regularization strategy inspired by the Information Bottleneck (IB) principle with the goal of regularizing intermediate representation to preserve the topological structure of the (regression) targets while reducing the intrinsic dimensionality. A theoretical section relates the intrinsic dimensionality of the representation to the IB principle and generalization error, justifying the need for PH-Reg. An experimental section validates the effectiveness of the proposed method by analyzing the effect of the regularization components that encourage lower intrinsic dimension $\\mathcal{L}_d$ and preserve the topology of the target space $\\mathcal{L}_t$ on both synthetic and real-world tasks."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1) The paper draws interesting and novel connections between the Information Bottleneck principle and the topology and intrinsic dimensions of the representations with respect to the targets\n\n\n2) The experimental section covers a significant number of diverse datasets, integrating quantitative results with qualitative visualizations\n\n\n3) The authors include an estimation of the additional computational and memory cost of Ph-Reg demonstrating that the overhead of the proposed method is small compared to the cost of training large architectures."
            },
            "weaknesses": {
                "value": "## Main Concerns\n\n1) **Clarity** \n   1) The underlying assumptions used to prove the statements in Section 3 are not fully clarified. Theorem 1 implicitly restricts the considered representations to deterministic functions of $\\bf x$. Previous literature [1] has shown the benefit of using stochastic encoders, for which the result from Theorem 1 is not applicable. \n   2) Theorem 3 assumes a uniform (conditional) distribution on the manifold $\\mathcal{M}_i$ but the paper does not elaborate on under which conditions this assumption is reasonable.\n   3) Section 4 is extremely difficult to follow because of the large number of definitions. Additional intuition on the roles of the terms in $\\mathcal{L}'_d$, $\\mathcal{L}_d$, $\\mathcal{L}_t$ would greatly improve the readability.  \n\n2) **Soundess**\n   1) The definition of the optimal representation and the statement in proposition 2 seem not to be applicable to continuous $\\bf z$ and $\\bf y$ without further clarifications. The value of the (differential) entropies $\\mathcal{H}({\\bf Y}|{\\bf Z})$ and $\\mathcal{H}({\\bf Z}|{\\bf Y})$ approach $-\\infty$ whenever there exists an invertible mapping ${\\bf y}=f({\\bf z})$. As a result, I believe the proof for proposition 2 needs to be revised.\n   2) The existence of a homeomorphic mapping between $\\bf z$ and $\\bf y$ and definition 1 rely on the existence of an underlying mapping ${\\bf y} = f({\\bf x})$. In other words, this assumption seems to restrict the setting to targets $\\bf y$ that can be fully determined from $\\bf x$. This aspect is not directly discussed in the main text and the real-world experiments consider distributions in which $p({\\bf y}|{\\bf x})$ has non-negligible aleatoric uncertainty (super-resolution, depth-estimation, age estimation).\n  \n\n3) **Experiments**\n   1) No measure of standard deviation is reported, which makes it difficult to assess the significance of the reported results.\n   2) The authors mention that \"several widely used regularizers like weight decay and dropout effectively reduce the last hidden layer's intrinsic dimension\", but these simple baselines are not included in any comparison.\n\n\n\n## Minor Issues\n4)  **Presentation**\n    1) The distribution $\\mathcal{D}$ defined in theorem 2 seems to depend on the specific sample ${\\bf y}_i$ but the index $i$ is dropped in the notation.\n    2) Figure 2b is helpful in supporting section 4, but I was unable to parse the plot on the right side since there is no direct reference to it in the description.\n    3) The differences between plots (b) to (e) in Figure 3 are difficult to relate to the description in the main text since the plots appear quite similar to each other.\n    4) Section 5.2 introduces a large number of abbreviations without previous mentions.\n\n### References\n[1] Alemi, Alexander A., et al. \"Deep variational information bottleneck.\" arXiv preprint arXiv:1612.00410 (2016)."
            },
            "questions": {
                "value": "1) Can the proposed method be extended to stochastic representations? What is the benefit of considering only deterministic encoders?\n\n2) Under which conditions is the assumption used in Theorem 3 (uniformity) justified?\n\n3) Does the proposed theory address solely tasks with negligible aleatoric uncertainty? In tasks such as super-resolution, it seems that the predictive distribution $p({\\bf y}|{\\bf x})$ could map a low resolution ${\\bf x}$ into multiple possible \"correct\" outputs ${\\bf y}$. Does that mean that an \"optimal representation\" of $\\bf x$ does not exist for this task? How could a homeomorphic mapping between $\\bf y$ and $\\bf z$ exist in these settings?\n\n4) How are the values for $\\lambda_t$ and $\\lambda_d$ determined?\n\n5) How consistent are the results reported in Tables 1, 2, 3, and 4 across multiple runs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763356907,
        "cdate": 1698763356907,
        "tmdate": 1699636105586,
        "mdate": 1699636105586,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iZ6wMoS5uB",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_pU4H"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_pU4H"
        ],
        "content": {
            "summary": {
                "value": "This work considers deep learning models trained for regression from the perspective of information bottleneck and the topology of the model\u2019s latent space. The work notes that the concept of the information bottleneck (IB) imposes constraints on the relationship between the latent space of a model and the target space. Through a number of propositions and theorems, the work claims to show that (i) the latent space of a regression model should have the same intrinsic dimension as the target space and (ii) the latent space of a regression model should have the same topology as the target space. To nudge models in this direction, the work develops regularizers that encourage these properties in the latent space. Finally, the work describes both synthetic and real-data experiments that suggest that these regularizers are both helpful for building robust regressors."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- **Advancing the science of deep learning for regression:** The fact that so much research into the science of deep learning has focused exclusively on classification at the expense of other tasks (such as regression), is a weakness of the field. This work which investigates IB, intrinsic dimension, and the topology of latent space specifically for models trained to perform regression thus represents a welcome research direction. \n- **Utilizing interesting mathematics:** The paper brings together interesting mathematics (topology) with ideas about the information bottleneck. Though some of the formalism of that connection needs to be ironed out, the underlying idea is interesting and seems worth pursuing."
            },
            "weaknesses": {
                "value": "**Writing correctness and clarity:** There were a number of issues with the writing that made it more challenging to read the work than it should have been. For instance, typos such as:\n1. In the introduction, \u201cThe homeomorphic between two\u2026\u201d $\\mapsto$ \u201cThe homeomorphism between two\u2026\u201d.\n2. In the introduction, \u201c\u2026and in the topology view,\u2026\u201d $\\mapsto$ \u201c\u2026and from the topological viewpoint,\u2026\u201d\n3. In Section 5.1, \u201cIn contrast, naively lowering the intrinsic dimension ($+L\u2019_{d}$ ) performs poorly and even worse than the baseline, i.e., Tours\u201d $\\mapsto$ \"...i.e., torus.\u201d\n\nThere were also a number of mathematical statements that were made that either didn\u2019t make sense or were not true. For example:\n1. The sentence \u201cThe continuity represents the $0$th Betti number in topology\u2026\u201d is incorrect. The $0$th Betti number captures the number of connected components in the topological space but has little to do with continuity (otherwise). For instance, a set of three unique (and discrete) points in $\\mathbb{R}^3$ has $0$th Betti number $3$ and three disjoint spheres in $\\mathbb{R}^3$ also has $0$th Betti number $3$.\n2. Pretty much all topological data analysis is based on algebraic topology, so the word \u2018algebraic\u2019 in \u2018algebraic topological data analysis\u2019 is unnecessary.\n3. \u201cHowever, unlike classification, regression\u2019s target space is naturally a topology space, rich in topology information crucial for the detailed task.\u201d It isn\u2019t clear why the target space of a classification task is less naturally a topological space. Perhaps what the paper means is that there is less topological structure in the discrete target space of classification tasks. Then again, if one is only using the $0$th homology groups (as this paper does), one is not capturing any higher dimensional structure anyway.\n4. Proposition 2 concerns continuous maps between $\\mathbf{Z}$ and $\\mathbf{Y}$. What are $\\mathbf{Z}$ and $\\mathbf{Y}$? Continuous maps only make sense when one has a defined topology on the domain and target space, this doesn\u2019t seem to currently exist in the work. \n\nThere were also a few cases where notation was used that was never defined.\n1. What is $\\mathcal{Y}$ in Theorem 2?\n2. If $\\mathbf{X}$, $\\mathbf{Z}$, and $\\mathbf{Y}$ are going to be used in proofs, it should be stated what they are.\n\nFinally, the reviewer had a hard time understanding some of the figures\n1. Figure 1(b) is used to illustrate a point about intrinsic dimension in the introduction. The reviewer had trouble understanding what the task is here and what the reader is meant to see in this scatter plot. More context should probably be added.\n\n**Use of the word \u2018topological\u2019 when only $H_0$ is used:** In the cases considered in this paper, the only topological property that $H_0$ measures is the number of connected components of a space (it has been shown that persistent homology also captures geometric information like curvature, so such features may also be leaking into regularization). As such, it captures only a small fraction of the total topological characterization of the space (particularly for high-dimensional spaces). It is this reviewer\u2019s opinion that this fact should be more explicitly stated than what is currently found on the bottom of page 5. For instance, it would be more accurate to call this \u2018connectedness regularization\u2019. It is probably not necessary to actually make this shift, but it does highlight that what is being advertised in much of the work is only marginally captured by the actual algorithms. Also, the paper makes a point of noting that regression tasks have richer topology than classification tasks, but if one is only using $H_0$, the target spaces are effectively the same.\n\n**Concern about where improvements are coming from:** One interpretation of the proposed method is that by constraining topology and intrinsic dimension (even on $\\mathbf{Z}$) based on ground truth, one is imposing built-in priors to the regression model. With this extra information, it is not surprising that the new model achieves better performance. One way to test this would be to impose the same regularization on model outputs in $\\mathbf{Y}$ rather than $\\mathbf{Z}$. Are the same improvements seen? I am not sure that the results are less interesting, but the connection to IB might be less compelling."
            },
            "questions": {
                "value": "- How does this regularization method scale to higher-dimensional target spaces? \n- Is it possible to incorporate higher homology groups into the regularization terms?\n- Do intrinsic dimension estimators outside of the approach leveraged in the regularizer capture a decrease in intrinsic dimension of the latent space representations?\n- How important is it to apply these regularizers to the latent space? Would a similar effect be seen if the outputs were regularized instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698791869490,
        "cdate": 1698791869490,
        "tmdate": 1699636105476,
        "mdate": 1699636105476,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "abJZa1TxLR",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_5Zit"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_5Zit"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the IB principle for regression problems. Then, the connection between the IB principle and topological properties is established. Basing on these observations, the PH-reg regularizer is proposed to harmonize topology of Z space and target space Y and to minimize the intrinsic dimension of latent space Z. Finally, experiments with both synthetic and real-world datasets show that the proposed PH-reg brings some improvement in quality measures."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Only a few papers study application of topology to machine learning and only a few papers study differentiable topological losses.\n2. The paper is mostly well written and clear.\n3. New theoretical results are presented.\n4. Experiments show improvements in quality measures for super-resolution, depth estimation and age prediction problems.\nAblation studies are provided."
            },
            "weaknesses": {
                "value": "1. I have concerns about the IB principle. Neural networks are deterministic functions, and, thus H(Y|Z) = 0 always.\nH(Z|Y) > 0 when different Z map to the same Y. This can happen when different X map the the same Y, because for real large networks different input objects X have different embeddings Z. So, some H(Z|Y)>0 depends only on the dataset itself, not the network for realistic scenarios.\n\n2. $min_Z \\left( I(Z,X) - \\beta I(Z,Y) \\right) $ and $min_Z \\frac{I(Z,X)}{ \\beta I(Z,Y) }$ are different problems. Thus, Theorem 1 is wrong.\n\n3. The regularizer in eq. 8 is the topological loss from Moot et. al, 2020. You should explicitly mention it.\n\n4. Sometimes the narration is clumsy, ex.: \"Figure 1(a) provides a t-SNE visualization\nof the 100-dimensional feature space with a \u2019Mammoth\u2019 shape target space.\"\nHere I don't understand what does 100-d space mean and what is \"Mammoth target space\".\nAlso, I think for your case visualization with t-SNE is not a good choice since t-SNE often tears a manifold apart.\nTry to use PCA.\n\nminor:\n1. Caption to Figure 3: model' - typo\n2. In Section 3, you write \"Consider a dataset S = {xi, yi} with N samples xi , which typically is an image (xi \\in Rdx1\u00d7dx2 )\"\ncolor channels of image are missing."
            },
            "questions": {
                "value": "1. what do functions f1, f2, f3, f4 from Section 5.1 mean?\n2. I don't understand the purpose of experiments from Section 5.1.\nYou embed 3D point cloud into a high dimensional space by adding 96 extra noise coordinates.\nThen, you try to predict 3 original coordinates, right?\nSo, ideally, the encoder must learn to ignore 96 noise coordinates and learn identity mapping for the rest of them.\nWhen you enforce the topological regularizer from (Moor et. al, 2020), you enforce pairwise distances in input and target spaces to be the same. Obviously, noise coordinates are forced to be ignored. \n3. What is the target space in the age prediction problem, R ? Then, the homeomorphism from image space R is not possible.\n4. What are the target spaces and Z space for depth estimation and super-resolution problems?\n5. For experiments with DIV2K, the improvement is very small (~0.1%-0.2%). Also std. intervals are not provided.\nHave you optimized hyperparams on the validation dataset and evaluated final quality on the test set?\nOtherwise, results are not valid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698844390301,
        "cdate": 1698844390301,
        "tmdate": 1699636105396,
        "mdate": 1699636105396,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LgE843qdtS",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_5oRh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_5oRh"
        ],
        "content": {
            "summary": {
                "value": "This paper considers implementing information bottleneck (IB) principle for regression problem. To this end, authors first derive a new generalization error bound, showing that in regression scenario, the generalization error is upper bounded by a function of the conditional entropy of $Z$ given $Y$, i.e., $H(Z|Y)$, in which $Z$ is the learned feature. Given this, authors mention the connection between $H(Z|Y)$ with respect to the intrinsic dimension of $Z$. Hence, to control the generalization error, it makes sense to minimize the intrinsic dimension of $Z$.\n\nFurther, based on the optimization representation condition that $H(Y|Z)=H(Z|Y)$, authors emphasized the necessarity to add an additional topology preserving regularization terms. Experiments are conducted on three real world data to demonstrate the effectiveness of the two regularization terms."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The IB principle for regression problem is less considered. The motivation of this work is promising.\n2. The new generalization error bound in Eq.~(2) is interesting, although it still has some issues (see below)."
            },
            "weaknesses": {
                "value": "Overall, I have some concerns regarding Theorems 1, 2 and 3, especially Theorem 2. \n\n1. I have several concerns regarding the new generalization error bound in Eq.~(2).\n\n1.1 how this bound is connected to the new bound in [1], which emphasized the role of I(X;Z|Y) in classification tasks.\n\n[1] Kawaguchi, Kenji, et al. \"How Does Information Bottleneck Help Deep Learning?.\" arXiv preprint arXiv:2305.18887 (2023).\n\n1.2 Is the bound tighter or not? or does H(Z|Y) a good indicator on the generalization error in regression problems? Can you validate this point?\n\n1.3 In fact, the Theorem 2 is a bit unclear, especially the description on Q. Are there some properties that Q should have?\n\n1.4 It is unclear how to derive from Eq.(28) to Eq.(29). I understand that $(E(|z-\\bar{z}|_2))^2 \\leq E |z-\\bar{z}|_2^2$. However, I do not understand why $E |z-\\bar{z}|_2^2$ can be further bounded by $Q(H(Z|Y))$. Does $Q$ exist for general case? In fact, authors only illustrated that $Q$ exists for Gaussian case and uniform case. I am not sure if the derivation from Eq.(28) to Eq.(29) holds for general case.\n\n2. Regarding Theorem 1, it only holds for a deterministic mapping function from $X$ to $Z$, since you assume that $H(Z|X)=0$? This should be emphasized in the main paper, since majority of deep IB approaches use stochastic representations and $H(Z|X)\\neq 0$.\nActually, this also decreases the applicability of Theorem 1. \n\n3. Theorem 3 is actually a straightforward result of [Ghosh & Motani (2023), Proposition 1]. This should also be emphasized in the main paper. \n\n4. Apart from the above concerns, I also feel the two regularization terms are not novel. \nIn fact, the connection between entropy and intrinsic dimension has been carefully discussed in [Ghosh & Motani (2023)]. \nThe way that authors evaluate the intrinsic dimenion seems to be directly from Birdal et al. (2021).\nAs for another regularization term on topology perserving, it also shares rather similar to that in (Moor et al., 2020).\n\n5. Finally, regarding the experiments. It would be much helpful if authors can compare their method to other prevalent deep IB approaches. For example, nonlinear information bottleneck also considers a regression setup. Another example is the convex information bottleneck [2].\n\n[2] Rodr\u00edguez G\u00e1lvez, Borja, Ragnar Thobaben, and Mikael Skoglund. \"The convex information bottleneck lagrangian.\" Entropy 22."
            },
            "questions": {
                "value": "please see weaknesses 1, 4, and 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698886157267,
        "cdate": 1698886157267,
        "tmdate": 1699636105330,
        "mdate": 1699636105330,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pupMtYV5HL",
        "forum": "OeNcnlQPRz",
        "replyto": "OeNcnlQPRz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_7gBF"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1763/Reviewer_7gBF"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to use a regularizer PH-Reg, with aim to lower the intrinsic dimension of the feature space while trying to preserve the \"0-dimensional topologically relevant distances\", in the context of representation learning. Some arguments, favoring  reducing a quantity which could be related to the intrinsic dimension of the feature space, in terms of information bottleneck approach, are given. Some  empirical verifications of the method are described, an access to the source code for experiments for reproducibility purpose is not provided."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper tries to relate quantitative characteristics of data representations from information theory with topological data characteristics, following several recent approaches."
            },
            "weaknesses": {
                "value": "1) The proposed \"intrinsic dimension lowering\" loss term $\\mathcal{L}_d$  actually disturbs the intrinsic dimension in an unclear way, it can both increase and decrease it, since, for a given data representation $Z$, the term   $\\mathcal{L}_d$  involves the ratio of logarithms $\\log E(Z_n)/ \\log E(Y_n)$ where $Y$ is another data representation. There is no $\\log E_n(Y)$, $Y-$dependent part, in the formula for intrinsic dimension of $Z$, see eg arXiv:2306.04723 or J. M. Steele, Growth rates of euclidean minimal spanning trees with power weighted edges, The Annals of\nProbability, 16(4):1767\u20131787, 1988.  So the paper narrative concerning the lowering of intrinsic dimension is seemingly in contradiction with the reported experiments setup. \n\n2) Reproducibility check concerning the reported experiments in the manuscript (Section 5) could not be performed. No source code was made available during the review phase. \n\n3) Theoretical results reported in Section 3 seem to have strong intersection with previously  published papers, in particular, Theorem 3 from Section 3 seems to be similar to Proposition 1 from  [Ghosh & Motani (2023)], cf Appendix A.3. \n\n4) The definition of the intrinsic dimension employed in Section 3, which is the same as the definition in  [Ghosh & Motani (2023)],  is completely different from the definition of intrinsic dimension via the 0-th persistent homology, employed in Section 4. So, a priori, there might be no connection between the two quantities. \n\n3) Training details are  lacking in the description of the experiments reported in Table 1, Table 2, Table 3, Table 4.   In particular, how many iterations/epochs were used, what were the stopping criteria? It might be that the experiments, if they can be reproduced, cf above, could be explained by overfitting of the baselines, since it seems that the standard regularizers, such as weight decay, were not used. \n\n4) Standard deviations are not reported in Table 2, Table 3, Table 4. \n\n5) Only a single baseline method is employed for comparison in each case in Table 2, Table 3, Table 4. \n\n6) The loss $\\mathcal{L}_t$ employed \"to preserve topology\", has been recently shown to have the following drawbacks: firstly, this loss is not continuous, secondly, moreover, diminishing   $\\mathcal{L}_t$  can lead to bigger difference in topology, cf  arxiv2302.00136, Appendix J, Figure 10. \n\n7)  Related work section does not mention many relevant papers, eg arXiv:2106.04024, arXiv:2201.00058, arXiv:2306.04723, J. M. Steele, Growth rates of euclidean minimal spanning trees with power weighted edges, The Annals of\nProbability, 16(4):1767\u20131787, 1988.\n\n9) The writing could be improved, there are some vague statements and not very clearly defined notions. \n\nBelow are some specific remarks: \n\npage 2 : \"intrinsic dimension of the feature space\" - what is this ? From what follows a reader can guess that seemingly it is  what can be  called intrinsic dimension of the dataset points  in the feature space, is it?\n\npage 2 :  Figure 1a - The right picture with supposedly \"topology similar\" has clearly too many clusters, eg a separate leg cluster, and, on the contrary, it does not preserve topology. The preserving topology representations exist in the literature for this mammooth dataset and they look completely different. \n\npage 2 : Figure 1b \"Lower the intrinsic dimension\" picture - Is the paper saying that a good representation of the 3d mammoth for regression task should be 1dimensional? First of all, the 3d object smashed into 1d space does not seem to be a good representation for the vast majority of tasks including many other regression tasks.  Secondly, such a smashing into 1d space representation would depend heavily on the regression task, which is also usually considered to be bad for a data representation. \n\npage 2:  \"The homeomorphic between two spaces\" - should it be homeomorphism ?\n\npage 2:  \" t-SNE visualization of the 100-dimensional feature space\" - the 100d feature space is described only in Section 5.1, several pages down. It would be better to mention it here on page 3, otherwise it is impossible to understand what are the 100 dimensions mentioned here. \n\npage 2: \"we are the first to explore topology in the context of regression representation learning\"- actually, the autoencoders learn the regression task of reproducing the coordinates of the data points, so no, the topology has been heavily explored in previous works in the context of regression representation learning. \n\npage 3: \"The intrinsic dimension of the last hidden layer\" -  what is this ? From other contexts a reader can probably guess that, seemingly, it is  what can be called intrinsic dimension of the dataset points  in the last hidden layer, is it?\n\npage 3:  \"1-dimensional (Trofimov et al 2023) topologically relevant distances\" - actually the method from (Trofimov et al 2023) preserves both 0-dimensional and 1-dimensional topological features, and even arbitrary $k-$dimensional features. \n\npage 3:  \"However, unlike classification,\" - as already mentioned, the autoencoders learn the regression task of reproducing the coordinates of dataset points. \n\npage 4: \"$Dim_{ID} M_i$ is the intrinsic dimension of the manifold\"(corresponding to the distribution)   - which definition of intrinsic dimension is used here? \n\npage 5: \"is a homomorphism between ... and\" - is a homomorphism from... to ...\n\npage 5: \"that the set of points S sampled from\" - that the set of points S _is_ sampled from ?"
            },
            "questions": {
                "value": "What is the relation between the quantity called \"intrinsic dimension\" in the Theorem 3, Section 2, seemingly based on \u03f5-neighborhood intrinsic dimension and the \"intrinsic dimension\" calculated via asymptotic of the minimal spanning trees in Section 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 6,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1763/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699558912033,
        "cdate": 1699558912033,
        "tmdate": 1699636105249,
        "mdate": 1699636105249,
        "license": "CC BY 4.0",
        "version": 2
    }
]