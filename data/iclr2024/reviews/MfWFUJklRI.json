[
    {
        "id": "LI4p4TRi07",
        "forum": "MfWFUJklRI",
        "replyto": "MfWFUJklRI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_VjfH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_VjfH"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the problem of graph invariant learning (GIL), aiming to find edges or nodes that are related to label information and invariant to environmental changes. This paper proposes three principles in GIL: Sparsity, Softness, and Differentiability, which cannot be fully covered by previous GIL methods. To address this issue, this paper designs a new regularization, namely Graph Sinkhorn Attention (GSINA), based on the optimal transport theory. GSINA can control the sparsity and softness of edge attention, and therefore improve the performance of GIL. Experiments on both real and synthetic datasets validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of utilizing the GIL for optimal transport is somewhat interesting. It makes sense to move the coefficients to 1 for invariant edges and to 0 for spurious edges.\n\n2. The proposed method GSINA consistently outperforms other GIL methods."
            },
            "weaknesses": {
                "value": "1. In addition to the three principles proposed, I think there is a fourth principle, which is the completeness of the subgraph. In practice, we expect important edges to form a complete subgraph. For example, in molecular property prediction, invariant information should be related to functional groups rather than individual chemical bonds or atoms. This is the advantage of subgraph selection methods over information bottleneck methods. I am concerned about whether enforcing sparsity guarantees this principle.\n\n2. There seems to be no clear reason to replace information bottleneck with optimal transport. We can also apply the Gumbel trick to  Graph Stochastic Attention (GSAT) to ensure its sparsity. Is there any theoretical evidence to prove the effectiveness of optimal transport over information bottleneck? Additionally, we can observe from the ablation study (Table 6) that without the help of the Gumbel trick, GSINA performs similarly or even worse than GSAT. Therefore, I am concerned about the effectiveness of the optimal transport.\n\n3. The experimental results appear to be copied from other papers. But I think it would be better if this paper could provide the results for some important baselines. For example, GSAT in the graph-level OOD tasks."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698720672148,
        "cdate": 1698720672148,
        "tmdate": 1699636050197,
        "mdate": 1699636050197,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aUeHiLYPdm",
        "forum": "MfWFUJklRI",
        "replyto": "MfWFUJklRI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_8Epa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_8Epa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel graph attention mechanism called Graph Sinkhorn Attention (GSINA) for graph invariant learning (GIL). GSINA extracts sparse, soft, and differentiable invariant subgraphs from input graphs by leveraging the optimal transport theory and the Sinkhorn algorithm. The proposed method acts as a powerful regularization to improve generalization in GIL tasks. The key benefits of GSINA are that it meets the desired principles of sparsity, softness, and differentiability for invariant subgraph extraction. Experiments across synthetic and real-world benchmarks demonstrate that GSINA outperforms prior state-of-the-art GIL methods on both graph-level and node-level tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. Originality: The paper proposes a new graph attention mechanism using optimal transport to extract invariant subgraphs. This is a novel application of the Sinkhorn algorithm not explored before for invariant learning on graphs.\n\n2. Quality: The theoretical analysis explains the design principles and formulations behind the proposed approach. The experiments compare against multiple baselines over several benchmarks to demonstrate the effectiveness of the method.\n\n3. Clarity: The paper is well organized and clearly explains the background, proposed method, and experimental results. Visualizations provide some intuition about the sparse graph attention.\n\n4. Significance: The work introduces a general framework for graph invariant learning that is applicable to both node and graph tasks. It provides improvements over state-of-the-art techniques, showing promise for this approach to invariant learning on graph data."
            },
            "weaknesses": {
                "value": "1. The paper lacks an in-depth theoretical analysis of why the proposed optimal transport approach and Sinkhorn algorithm can effectively extract invariant subgraphs for graph learning tasks. More analysis connecting GSINA to invariance principles or analyzing its inductive biases could strengthen the method.\n\n2. The paper lacks computational complexity analysis of the proposed GSINA method. It is unclear how the time and space complexity scale as the size of the input graphs increases. Moreover, there is no comparison of the running time or memory usage of GSINA compared to the baseline methods. Analyzing the overhead imposed by using optimal transport and Sinkhorn could quantify the tradeoff between accuracy gains and computational costs.\n\n3. The Introduction section lacks specificity in explaining the innovative contributions of the proposed GSINA model for graph invariant learning. Additionally, the Approach section lacks adequate transition and explanation before formulating invariant subgraph extraction as an optimal transport problem. Addressing these weaknesses would enhance the clarity and logical flow of the paper's core contributions."
            },
            "questions": {
                "value": "1. The complexity analysis of GSINA is limited in this paper. Can you provide a detailed analysis of the computational overhead of GSINA compared to standard GNNs and other graph invariant learning techniques? What are the asymptotic complexities of key components like Sinkhorn attention and Gumbel trick?\n\n2. This paper argues that sparse, soft, and differentiable are design principles for graph invariant learning (GIL). However, connectivity is another important consideration for extracting meaningful and interpretable invariant subgraphs in GIL. The invariant regions should represent coherent structures and patterns rather than disjoint disconnected components. How do you think about the connectivity of invariant subgraphs? Does the proposed GSINA model take this into account?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Reviewer_8Epa"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698798856829,
        "cdate": 1698798856829,
        "tmdate": 1699636050057,
        "mdate": 1699636050057,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "go5sPG2U6Z",
        "forum": "MfWFUJklRI",
        "replyto": "MfWFUJklRI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_mLys"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_mLys"
        ],
        "content": {
            "summary": {
                "value": "This paper studies graph invariant learning to discover the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. It adopts the optimal transport theory and designs one graph attention mechanism as a powerful regularization method for graph invariant learning. The experiments show the effectiveness of the method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The strengths of this paper are listed as follows:\n- This paper focuses on one important research problem which is graph invariant learning. It is very interesting to me.\n- The writing is good in general. The motivations are clearly present. And the technical details are easy to understand.\n- The experiments show the improvements on the baselines."
            },
            "weaknesses": {
                "value": "The concerns are from the following aspects:\n- The technical contributions are a little straightforward to me since the key design for graph and graph OOD problem are not very well explained. For example, the design in section 3.1 is similar to GSAT [1] and the edge attention in section 3.2 is very similar to [2]. These differences with existing works are not very clear, which raises my concerns about novelty.\n- Some theories are not well formulated. For example, it should formally define the graph distribution by considering the non-Euclidian graph properties. But the theories have weak connections with the graph itself.\n- The experiments are not convincing enough since the authors seem to ignore some baselines (such as the graph-level and node-level method in [3]). Some of the results are confusing. For example, why some in-distribution results are lower than the OOD results.\n\n[1] Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism. ICML 2022.\n[2] Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure. NeurIPS 2022.\n[3] Out-Of-Distribution Generalization on Graphs: A Survey. ArXiv 2022."
            },
            "questions": {
                "value": "See weaknesses part above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698825016297,
        "cdate": 1698825016297,
        "tmdate": 1699636049967,
        "mdate": 1699636049967,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "67gbAOrs0a",
        "forum": "MfWFUJklRI",
        "replyto": "MfWFUJklRI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_Jtwt"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_Jtwt"
        ],
        "content": {
            "summary": {
                "value": "In the context of graph invariant learning, this paper presents the GSINA model, which leverages optimal transport and graph attention to identify invariant subgraphs while satisfying the principles of sparsity, softness, and differentiability. Graph-level and node-level experiments on both synthetic and real-world datasets are carried out to demonstrate the effectiveness of GSINA."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1)\tThe paper is skillfully written and exhibits a well-structured organization. \n(2)\tThe experiments encompass commonly-used datasets for this task and provide comprehensive comparisons with many typical methods, including CIGA and GSAT. I believe the experiments are robust and information-rich.\n(3)\tThe majority of models relying on top-k selection extract hard subgraphs, often resulting in a notable loss of information in node-level experiments. In contrast, GSINA retains all edges by assigning low attention weights to variant part, ensuring the completeness of the graph structure."
            },
            "weaknesses": {
                "value": "\uff081\uff09 Using optimal transport theory to obtain differentiable solution to top-k selection has been adopted by other existing works, so I think the method of this paper lacks novelty. \n\n\uff082\uff09 The superiority of GSINA is marginal and not consistent. For example, for Graph-level OOD generalization performances in Figure 4 and Table 5, GSINA expresses worse results.\n\n\uff083\uff09 The compared methods of different datasets are not unified. Baselines in Table2-4 are  less than Table 5.  And they ignored to compare some recently proposed methods: MoleOOD[1],  GIL[2],  Disc[3] and  GREA[4].\n\n\uff084\uff09 The interpretable performance is not enough to verify the effectiveness of GSINA since  the used attention mechanism is expected to have stronger interpretability than other models.\n\n\uff085\uff09 GALA[5] presents GALA for learning invariant graph representations without environment partitions under the proposed minimal assumptions. It is suggested to cite this paper.\n\n\n[1] Learning substructure invariance for out-of-distribution molecular representations.\n\n[2] Learning invariant graph representations for out-of-distribution generalization.\n\n[3] Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure.\n\n[4] Graph Rationalization with Environment-based Augmentations.\n\n[5] Rethinking Invariant Graph Representation Learning without Environment Partitions"
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698837087516,
        "cdate": 1698837087516,
        "tmdate": 1699636049885,
        "mdate": 1699636049885,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "E9HjoGhVO2",
        "forum": "MfWFUJklRI",
        "replyto": "MfWFUJklRI",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_mZFa"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1236/Reviewer_mZFa"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method (GSINA) for learning invariant representations when performing node or graph classification. The method is designed to provide sparse invariant features (subgraphs) like simultaneously ensuring differentiability. They draw inspiration from attention based strategies, which are differentiable, and top-k strategies, which are typically sparse but not differentiable. GSINA combines these two perspectives and uses proposes an optimal transport problem to obtain edge importances. The found invariant subgraph is then feed into the predictor to learn more generalizable features. The authors perform comprehensive experiments to demonstrate the efficacy of their method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The authors perform a very comprehensive evaluation across many datasets and tasks. Indeed, the proposed method does substantially outperform its competitors. For example, on by almost 10% with respect to CIGA on Table 4.\n\n- The proposed method appears to be well-grounded. Maximizing the mutual information between a subgraph and the label is a popular approach in both the invariant feature learning and GNN-explanation literature. Moreover, the use of Optimal Transport on top of attention makes sense given the shortcomings of existing approaches. \n\n- Their method does not seem to require extensive hyperparameter tuning (just r), which uses the validation performance. This is especially helpful for OOD settings, where one cannot assume access to OOD data."
            },
            "weaknesses": {
                "value": "- I think the novelty is a bit lacking in the approach. Individual pieces seem like combinations of existing pieces. Perhaps I missed something and the authors could clarify this? Could the authors also clarify if the softmasks learnt by GSINA do in fact coincide with the known invariant signals in the graph (as per the explanation literature)? \n\n- Runtime/ Computational Complexity: I'm concerned that this method will substantially increase the runtime relative to the vanilla model and also other invariant representation methods. Could the others please provide some runtime plots so that I can understand if this is in fact the case? \n\n- The writing of this paper needs to be polished. For example, \"The invariance optimization methods are based on the principle of invariance, which assumes the invariant property inside data, i.e. the invariant features under distribution shifts.\""
            },
            "questions": {
                "value": "Please see the weakness above, and below. \n\nOut of curiosity, I was wondering if GSINA could be applied to a pretrained GNN as a way of post-hoc improving the representations for better OOD generalization? Perhaps by enforcing consistency between the pretrained model's predictions on the original graph and the extract subgraph or through end-to-end fine tuning? \n\nAlso, can the authors also please clarify if GSINA + the GNN Predictor are trained \"end to end?\" They mention a two-stage framework, but I just wanted this clarified. Maybe adding a pytorch-style algorithm would be beneficial."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1236/Reviewer_mZFa"
                ]
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1236/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699053137744,
        "cdate": 1699053137744,
        "tmdate": 1699636049787,
        "mdate": 1699636049787,
        "license": "CC BY 4.0",
        "version": 2
    }
]