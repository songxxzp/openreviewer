[
    {
        "id": "KLyGLm3niP",
        "forum": "5e0yWSNGIc",
        "replyto": "5e0yWSNGIc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_yyyH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_yyyH"
        ],
        "content": {
            "summary": {
                "value": "The paper analyzes adversarial training in deep reinforcement learning and identifies some issues with these methods. In particular, adversarially trained value functions are shown to overestimate the optimal values, and they may give incorrect ranking for the performance of sub-optimal actions."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is generally well-written. The motivation to study adversarial training is well-explained, and the potential issues for adversarial training is very relevant to the community.\n\n- A simple but insightful example with two states and three actions is provided to analytically demonstrate the effects of the regularizer in adversarial training methods. It is shown that the regularizer takes a lower value with parameters which overestimates the optimal value and reverses the order of the values of the second and the last action.\n\n- In numerical experiments on three environments in ALE, several metrics are used to compare vanilla trained models and adversarial trained models. One way is to directly compare the optimal value function with the two training methods. Another way is to compare their normalized value estimates among the best, the second best, and the worst actions. The paper also introduced the metric of performance drop when taking a sub-optimal action in a randomly sampled p-fraction of states. All the comparisons suggest that adversarial training might incur overestimation bias for the values of the optimal action, and provide inaccuracy value estimates for sub-optimal actions."
            },
            "weaknesses": {
                "value": "- The statement of Theorem 3.4 is very informal and it is not clear what it guarantees. From the short proof in the supplementary, it seems like only one set of parameters given in Prop. 3.3 is analyzed. Since no further analysis (like gradient analysis) is done, it is not guaranteed that the regularized optimization will indeed go to the overestimation direction as suggested by the theorem. I think the authors may need to either rewrite the theorem more formally and provide a proof that the optimal solution to the regularized optimization problem indeed overestimate the values, or replace the theorem possibly by numerically showing the learning dynamics of the regularized problem.\n\n- Though overestimation of the optimal values and the incorrect relative ranking for sub-optimal actions seem to be potential issues when adversarial training is used, the paper doesn't discuss what aspects these issues might actually affect adversarial trained agents. Does the overestimation lead to performance loss? Does the incorrect ranking of sub-optimal actions affect robustness in any sense? Without more discussions on these issues, they may just be properties of adversarial training and not necessarily serious concerns.\n\n- Adversarial trained models give higher values compared with vanilla trained models, but it could be possible that vanilla trained models do not provide good value estimates either. Comparing with a more accurate estimate for optimal value may help clarifying this concern. One possibility is to compare with the average score so one may get an idea of whether the trained value function over or underestimate the values.\n\n- Some minor issues:  \n  - It's confusing when both $a_w$ and $a_|A|$ are used to refer to the same variable.\n  - The paper introduces the concept of $\\tau$-dominate but it is not used in the main paper."
            },
            "questions": {
                "value": "- Can the authors improve Theorem 3.4?\n\n- Are the issues identified in the paper connects to performance loss, robustness, or alignment with human decision?\n\n- Is it possible to compare adversarial trained and vanilla trained values with other value estimates, like the average score or some Monte Carlo methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698202547861,
        "cdate": 1698202547861,
        "tmdate": 1699636932689,
        "mdate": 1699636932689,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "UDz2cinoOn",
        "forum": "5e0yWSNGIc",
        "replyto": "5e0yWSNGIc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_JDGz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_JDGz"
        ],
        "content": {
            "summary": {
                "value": "This paper studies the effect of adversarial robustness regularization on the action gap, overestimation bias, and suboptimal action ranking accuracy of deep neural networks trained on RL tasks.  It observes that adversarially robust RL agents exhibit a larger action gap between the predicted optimal and second-best action, accompanied by reduced accuracy at ranking suboptimal actions. It demonstrates these pathologies in a simple linear model, along with deep RL agents trained on a subset of games from the Arcade Learning Environment."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper highlights a previously under-discussed pitfall of adversarial training: adversarial robustness objectives have the side effect of reducing the accuracy of the ranking of the Q-value over actions   \n  - It illustrates this pathology with an easy-to-interpret example, whose correctness is easy to verify.\n  - The analysis of the action-ranking accuracy is creative and quite interesting.\n  - The paper provides an interesting counterexample to a reasonably widely-held belief that increasing the action gap should correspond to reduced overestimation bias.\n  - The figures are for the most part quite easy to interpret and clearly convey the intended message.\n  - The experimental setup is clear and well-justified."
            },
            "weaknesses": {
                "value": "- I have some issues with the notion of adversarial robustness studied in this paper. In particular, it is unclear whether the adversarial regularizer really makes sense in an RL context, since it doesn't distinguish between the quality of the action that gets overestimated in the adversarial example. In the image classification tasks where adversarial examples were initially studied, any incorrect label is equally 'incorrect' in a sense. However, in deep RL, the particular action targeted by the adversary can have wildly different influences on the performance of the agent's policy. The authors argue that the adversarial training objective they study is of interest because it has been studied in multiple published works, however some of the cited works (for example Gleave et al.) consider a very different adversarial threat model than that discussed here.\n  - The observation that the adversarial regularizer interferes with the model's accuracy on suboptimal actions is exactly what would be expected by looking at the formula used. While the empirical evaluations and toy example are helpful to verify\n  - It is not clear whether inaccurate ranking of Q-values and overestimation bias is actually a problem in the sense that it leads to worse behaviour policies or slower training. For example, I could imagine that inaccurate estimates of suboptimal actions could be a problem if the network has not yet converged to an optimal policy. However, analysis around this phenomenon is missing from the paper.\n  - The discussion of the 'action gap' is misleading: the paper claims that \"the fact that adversarially trained deep neural policies overestimate the optimal state-action values refutes the hypothesis that increasing the action gap is the sole cause of a decrease in overestimation bias of state-action values.\" However, the experimental setup of this paper does not isolate the effect of increasing the action gap on overestimation bias. Because there are many other confounding factors that arise from the adversarial regularizer, the results from section 6.3 are consistent with a model where increasing the action gap decreases overestimation bias, but then some other effect of the adversarial regularizer independently increases the overestimation bias, overwhelming the effect of the increased action gap. In that setting, it would still be correct to say that \"the action gap is the sole cause of a decrease in overestimation bias\" in other contexts. This claim should be adjusted to state that \"an increase in the action gap of a Q-function does not uniformly reduce overestimation bias in all contexts.\"\n  - Minor: Figure 5 is difficult to read due to the small font size. In Figure 4, there are some bizarre artifacts where the Q values of the blue line occasionally drop to overlap with the red line, and it's not clear why this should happen."
            },
            "questions": {
                "value": "- Do the pathologies highlighted by this paper result in meaningful challenges for optimization? Do they slow down learning? Simply noting that a regularizer results in worse Q-value estimation does not on its own indicate that this worse Q-value estimation is necessarily a problem for performance or for learning dynamics. I would be more confident in the significance of these findings if the authors could indicate some practical examples in which they present a barrier to policy improvement.\n  - One angle that seems to be missing from this paper is whether there are other ways of enforcing adversarial robustness which avoid these pathologies, assuming that they are indeed a problem. Would a more naive approach of e.g. explicitly regularizing the Lipschitz constant of the network also encounter this pathology?\n - The three games considered are settings where I would expect adversarial robustness to be particularly at odds with Q-value accuracy, as states with small pixel distance could correspond to very different value functions. I would be interested in seeing if we see the same magnitude of trends in a different domain such as Mujoco."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698626517136,
        "cdate": 1698626517136,
        "tmdate": 1699636932543,
        "mdate": 1699636932543,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "91ZUn9lgVP",
        "forum": "5e0yWSNGIc",
        "replyto": "5e0yWSNGIc",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_KcnX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7663/Reviewer_KcnX"
        ],
        "content": {
            "summary": {
                "value": "This paper reveals that the adversarial training in RL could lead to inconsistencies and overestimations of state-action (Q) values.  The authors further show that vanilla-trained DRLs have more accurate and consistent estimations, both in theoretical analysis with a linear model and experiments with neural network approximations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-organized and easy to follow. \n2. The research problem is interesting, it aims to answer the question \"What cost does adversarial training bring in value estimation of DRL? \"\n3. The motivating linear model example well explains the angle of overestimation and wrong order ranking with adversarial training. \n4. The existing experiments clearly support the claim and the findings in the paper."
            },
            "weaknesses": {
                "value": "1. As a finding/observing paper, the \"novelty\" is not that strong. Here the \"novelty\" refers to the findings themselves. By adding a regularizer term as introduced in Definition 3.1, it is somewhat intuitive and straightforward to imagine that this regularizer keeps the peak value of the optimal action while punishing any other choices within the small neighborhoods, which could lead to the over-estimation of optimal action and reordering of non-optimal action. Therefore, it is not surprising to see the findings in the following context. \n2. Although the authors mention the effects of over-estimation of optimal action and reordering of non-optimal action from here to there in the paper, I don't see a systematic analysis and deep discussion of how they lead to a big problem for RL. \n3. As a finding/observing paper, the authors only implemented DDQN vs. SA-DDQN. This is not enough for a paper motivated by experiments. The authors are encouraged to bring more results to support their findings and claims."
            },
            "questions": {
                "value": "1. From RL's perspective, overestimating the value of optimal action shouldn't be a problem as the higher value will encourage to pick the optimal actions. Re-ordering the non-optimal actions is not a problem either as the agent will never pick them. Should we really consider these two as drawbacks of adversarial training in RL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7663/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698789704626,
        "cdate": 1698789704626,
        "tmdate": 1699636932422,
        "mdate": 1699636932422,
        "license": "CC BY 4.0",
        "version": 2
    }
]