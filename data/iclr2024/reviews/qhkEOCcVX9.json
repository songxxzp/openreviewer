[
    {
        "id": "Zvvjg0cNe8",
        "forum": "qhkEOCcVX9",
        "replyto": "qhkEOCcVX9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
        ],
        "content": {
            "summary": {
                "value": "This work introduces a benchmark, the Newborn Embodied Turing Test (NETT), and reveals the limitations of current machine learning algorithms compared to newborn brains in object segmentation tasks. However, the experimental results of this work are insufficient to validate the claimed fairness of the NETT."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThis work's main contribution is the introduction of the Newborn Embodied Turing Test (NETT) benchmark, which enables a direct comparison of the learning abilities between newborn animals and machines in object segmentation tasks."
            },
            "weaknesses": {
                "value": "1.\tThe evaluation of this proposed NETT benchmark is not comprehensive enough. Many variables were not taken into consideration during the data collection process for the biological chicks, such as their binocular vision compared to the single camera of artificial intelligence chicks. Meanwhile, biological chicks might roam around while artificial intelligence chicks do not. These unaccounted variables raise doubts about the effectiveness of the proposed benchmark. The validity of the benchmark may be compromised due to the discrepancies in sensory capabilities and behavior between biological and artificial chicks. Further refinement and consideration of these variables are necessary to ensure the reliability and validity of the benchmark.\n2.\tThe presentation of this work still needs improvement, as the figures are not in vector format. For example, Figure 3 lacks clear labels for the x and y axes, making it difficult to understand the meaning of the figure."
            },
            "questions": {
                "value": "1.\tThe evaluation of this proposed NETT benchmark is not comprehensive enough. Many variables were not taken into consideration during the data collection process for the biological chicks, such as their binocular vision compared to the single camera of artificial intelligence chicks. Meanwhile, biological chicks might roam around while artificial intelligence chicks do not. These unaccounted variables raise doubts about the effectiveness of the proposed benchmark. The validity of the benchmark may be compromised due to the discrepancies in sensory capabilities and behavior between biological and artificial chicks. Further refinement and consideration of these variables are necessary to ensure the reliability and validity of the benchmark.\n2.\tThe presentation of this work still needs improvement, as the figures are not in vector format. For example, Figure 3 lacks clear labels for the x and y axes, making it difficult to understand the meaning of the figure."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Reviewer_ELqk"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698458737216,
        "cdate": 1698458737216,
        "tmdate": 1700725617248,
        "mdate": 1700725617248,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CWA6QbLjnf",
        "forum": "qhkEOCcVX9",
        "replyto": "qhkEOCcVX9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
        ],
        "content": {
            "summary": {
                "value": "The authors aimed to put both state-of-art vision models and biological vision on similar levels of data diet. They do this by rearing newborn chicks in a very controlled visual environment where they have access to limited visual experience. By exploiting the imprinting ability of chicks, their ability to segment foreground from background is studied. Then vision models are trained with exact same data by instantiating them as the vision modules in a RL agent rewarded to go towards the imprinting object. Finally both the models and chicks are probed for their ability to segment new objects and/or new backgrounds and it was found that the models fare much worse compared to the newborn chicks. This indicates that the state-of-art models lack something that newborn chicks' visual system has."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "4 excellent"
            },
            "strengths": {
                "value": "Rarely do I see works with this novelty, quality and significance. \n\n* Studying deep learning models in a setting where the data diet is controlled is timely and needed. \n* The data collected from newborn chicks is compelling and novel. \n* The results showing that current models lack the necessary mechanisms can be very impactful. \n* The data collection procedure and analysis opens up a ton of possibilities for future work. \n* The paper is overall well written."
            },
            "weaknesses": {
                "value": "* Perhaps the biggest weakness is that the vision models here are trained in a way that is not very indicative of the state-of-the-art. Most vision models that is used in the community are usually a combination of self-supervised training and supervision with a lot of exemplars. While this might diminish how relevant this work is for the community, it does not take away from the significance of the work since the goal of the work is to study models when they get only as much visual experience as newborn chicks (i.e newborn chicks are not getting a ton of supervision). \n* PPO agents are trained for a lot of episodes (original PPO paper (Schulman, 2017) did 1M episodes) usually while this work only does 1000 episodes. This is probably okay since this setting is much simpler. But I would still train one model for much more steps to rule out strange behaviors (like grokking (Power, 2022))\n* The paper might benefit from an expanded section on the differences between birds and mammal brains, especially since the audience is likely to be more from the computational side. While I do appreciate the effort in section 1.1, it might be helpful to expand it a bit more."
            },
            "questions": {
                "value": "* Why does Figure 1 make it seem like this is a cyclic process? I don't see why it is put on a circle - step 1 does not follow step 6?\n* I wish this was called something other than a Turing test. I think most people (as far as I know) think of the imitation game when they hear Turing test while this is very different. I feel like this is significantly different enough to warrant a different. I almost feel like calling this Turing test is underselling it. Would another name that highlights the \"limited exposure to environment\" aspect of this work be better suited? Turing test says nothing about how much experience the models get to have?\n* Section 2.1 : Was the second display blank (all black) or just a background?\n* Section 2.2 : Do the RL agents need to be called \"artificial chicks\"? I think calling them \"RL agents\" or something is less confusing. \n\n\n\nImprovement to the paper -\n* All the figures need major revamp. The text is barely readable and they need to be of higher resolution. Figure 2 needs to say what \"F\" and \"N\" stand for. Figure 2 could also be made such that the backgrounds and foregrounds are easily distinguishable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5993/Reviewer_FxD2"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698695653654,
        "cdate": 1698695653654,
        "tmdate": 1699636642109,
        "mdate": 1699636642109,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NuFUZUqQvI",
        "forum": "qhkEOCcVX9",
        "replyto": "qhkEOCcVX9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_oTEc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_oTEc"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to benchmark the intelligence of embodied AI systems against that of biological ones (animals). Towards this end, they propose a new benchmark, the Newborn Embodied Turing Test (NETT). \n\nIn NETT, an agent is born into an environment with a display showing a singular rotating object. The agent must then learn to properly identify that object when presented with distractor objects or when it is placed on novel backgrounds. The biological agents are chicks that learn this skill via filial imprinting and the artificial agents are DRL agents that learn this via an filial-imprinting-like reward.\n\nThe authors find that while biological chicks are well suited to learn this task, their artificial counterparts are not. The authors consider a variety of DNN architectures and find that none perform significantly above chance."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper presents a convincing experiment that directly compares the capabilities of biological and artificial chicks. I really like how controlled the setup is. You can't perfectly control for all the differences between biological and artificial systems (as the authors note in their limitation section), but this paper does an admirable job at reducing this gap as much as possible.\n\nThe authors examine a variety of different DNN architectures with the aim to close the gap between biological and artificial chicks.\n\nThe discussion section is well-written and presents a balanced discussion that points out the limitation of the experiments presented and contains interesting pieces of information."
            },
            "weaknesses": {
                "value": "Figures in the paper are very low resolution (and possibly heavily compressed with lossy compression). This is particularly noticeable for figure 3, which is nearly illegible.\n\nThe number of trials used for training artificial chicks seems rather small at 1000. Similarly, the episode length of 1000 seems long given how small the environment is. Why were these numbers chosen? Are they similar to the number of times the biological chicks turn towards their object and how long they stay facing it?\n\nThere has been work in deep reinforcement learning algorithms that may be applicable here. One example is as DAAC and IDAAC (\"Decoupling Value and Policy for Generalization in Reinforcement Learning\", Raileanu and Fergus)."
            },
            "questions": {
                "value": "### Questions\n\n1. Did the authors perform the biological chick experiments themselves or are those directly from Woods & Woods 2021? (I was unable to get access to this paper to check myself.) If the authors performed this experiment, has it been checked by an Institutional Review Board (IRB) or similar?\n\n### Suggestions for Improvement\n\nThis paper reminded me of \"Exploring Exploration: Comparing Children with RL Agents in Unified Environments\" by Kosoy et al. These two works perform different experiments and have different goals, but the authors may want to mention this work.\n\nSome \"high-water\" marks for artificial chicks would be useful. If one was trained on the test set, how would it do? Ideally it should perfectly solve the task. If one was trained with a singular object but many different backgrounds, how would it do?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper contains experiments performed with real animals in not great conditions -- newborn chicks are confided to a small box in isolation with only a rotating object on a screen for company. While the experimental design seems to come from prior work (Woods & Woods, \"One-shot object parsing in newborn chicks\", 2021), I don't feel confident making a judgement on if this experiment is okay. \n\nI asked the authors if they performed this experiment and if they did, has it been check by an IRB or similar."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698970152044,
        "cdate": 1698970152044,
        "tmdate": 1699636642013,
        "mdate": 1699636642013,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CfKa7fQWhr",
        "forum": "qhkEOCcVX9",
        "replyto": "qhkEOCcVX9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_gdnS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5993/Reviewer_gdnS"
        ],
        "content": {
            "summary": {
                "value": "This paper conducts a comparison study on the one-shot object segmentation ability of real animals and machines. Specifically,  the authors create simulated \u2018digital twin\u2019 environments that mimic the rearing condition of real biological newborn chicks. In the simulated environments, the \u2018artificial chick\u2019 is trained via deep reinforcement learning to segment objects. The experimental results show that \u2018artificial chick\u2019 failed in the one-shot object segmentation task while biological chicks are able to solve it."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Originality and Significance:    \nThe reviewer found the idea of comparing machine and real animals' learning ability in a strictly controlled environment interesting. This line of study might help us learn more about the similarity and difference of animal brain and deep neural network. \n \n\nQuality:   \nWhile the idea of the paper is interesting, the reviewer found some claims are not well-supported by the experiments. Please see the Weakness section for more details.\n\n\nClarity:    \nThe presentation is mostly clear. However, many improvements, particularly on the figures, are needed.  Please see the Weakness section for more details."
            },
            "weaknesses": {
                "value": "1. The technical contribution of the paper is limited. Specifically, the proposed \u2018artificial chick\u2019 is a PPO agent trained with \u2018imprinting reward\u2019, which encourages the agent to move close to an object. It is not surprising that this simple baseline with a heuristic reward fails to solve the one-shot object segmentation task.   \n\n2. The reviewer found main claims in the paper are not well-supported by the experiments. Specifically, the authors claimed that \u2018\u2019... none of the algorithms learned background-invariant object representations that could generalize across novel backgrounds and viewpoints\u201d. However, the ML algorithm the authors used in the paper are just  PPO with different architectures and a heuristic reward. The reviewer thinks those simple methods well-represent the state-of-the-art one-shot object segmentation methods. There are many works on one-shot segmentations [1, 2]. Incorporating those methods in the \u2018artificial chick\u2019 might make the experiments more convincing.     \n\n3.  The reviewer has concerns on the significance of the paper. The main finding here is that the RL trained agent failed the one-shot segmentation task. It seems expected. Providing more insights and analysis of why it fails may make the paper more valuable.   \n \n4.  The presentation in the experimental section is unclear. Particularly, the text and numbers in figure 3, which show the only experimental result of the paper, are ineligible.  \n\n\n\n[1] Learning to Segment Rigid Motions from Two Frames, Yang et al., CVPR 2021.   \n[2] One-Shot Learning for Semantic Segmentation, Shaban et al., BMVC 2017"
            },
            "questions": {
                "value": "1. According to section 2, the biological chicken data is from previous work (Wood et al. 2021). However, in the abstract, the authors claim that \u201cwe raised newborn chicks in controlled environments \u2026\u201d. This seems contradicting. Could you clarify?    \n\n\n2.  In Introduction, it reads \u201cHowever, most studies with newborn subjects have produced data with a low signal-to-noise ratio,\u201d. The reviewer has difficulty following the content. Please give citations for \u2018some studies\u2019 and elaborate what is the \u2018produced data\u2019 here."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5993/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699207462456,
        "cdate": 1699207462456,
        "tmdate": 1699636641921,
        "mdate": 1699636641921,
        "license": "CC BY 4.0",
        "version": 2
    }
]