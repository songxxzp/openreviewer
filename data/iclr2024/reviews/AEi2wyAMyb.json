[
    {
        "id": "Gi95B1Rh0G",
        "forum": "AEi2wyAMyb",
        "replyto": "AEi2wyAMyb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_akKq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_akKq"
        ],
        "content": {
            "summary": {
                "value": "This paper designs a bi-level optimization method for semi-supervised learning (SSL). In the outer loop, it optimizes the pseudo-labels by minimizing the validation loss. In the inner loop, it optimizes the model parameter by empirical risk minimization. Extensive experiments validate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper is well written and easy to understand.\n- The proposed method is simple yet effective, and the introduction of bi-level optimization is novel in the SSL literature.\n- To solve the optimization problem in the outer loop, the approximation approach is sound and effective. The entire algorithmic process is simple and interesting."
            },
            "weaknesses": {
                "value": "- The convergence of the proposed method is not revealed either theoretically or empirically. I think both theoretical and empirical analysis can be done to analyze the convergence problem. Similar theoretical analysis of convergence can be found in many bi-level optimization papers, and introducing it can improve the paper. Besides, it is also important to present the convergence figure of parameters, i.e. pseudo-labels in this paper. For example, the empirical study of the convergence property can be referred to Shu et al. (2019).\n\n- Since the approach explicitly introduces the validation data sets in the training phase, it is not clear whether the comparison with the previous method is still fair. Previous methods at most used the validation set for tuning hyperparameters. Authors should explicitly discuss this issue. They should also discuss the number of validation data for the method, because I am afraid that the imparity problem gets worse with more validation data.\n\nI am willing to raise my score if my concerns can be addressed.\n\n***\nReference:\n- Shu et al., Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting, NeurIPS 2019."
            },
            "questions": {
                "value": "- Can the method converge quickly?\n- Is the comparison with previous SSL methods still fair?\n- What's the impact of the number of validation data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697555561288,
        "cdate": 1697555561288,
        "tmdate": 1699636368987,
        "mdate": 1699636368987,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "y8QKqL6BJh",
        "forum": "AEi2wyAMyb",
        "replyto": "AEi2wyAMyb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_Yzuc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_Yzuc"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Bi-level Optimization method for Pseudo-label Learning (BOPL) for Semi-Supervised Learning, which treats pseudo-labels as latent variables and formulates pseudo-labeling as a bi-level optimization problem to jointly learn the pseudo-labels and model parameters within a bi-level optimization framework. This approach simultaneously enhances the quality of pseudo-labels and the prediction consistency between labeled and unlabeled data. Experimental results validate the effectiveness of the proposed approach and show that it outperforms the state-of-the-art SSL techniques."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper defines the SSL problem as a novel bi-level optimization problem, which directly learns the pseudo-labels of unlabeled samples as latent variables through an outer optimization, updates pseudo-labels using teacher-student model, and learns the model parameters through an inner optimization.\n2. BOPL employs a pair of bi-level objectives to ensure prediction consistency between labeled and unlabeled samples and combines Interpolation Consistency Training (ICT) to facilitate model training and improve the robustness and generalizability of the model.\n3. The experiments are compared with existing SSL methods, and ablation study and sensitivity analysis are conducted. The results demonstrate the effectiveness of BOPL."
            },
            "weaknesses": {
                "value": "1. The pseudo-labels are simply initialized by the initial teacher model parameters and the update process is impacted by various factors, including the learning rate \u03b1, linear combination weight \u03b3 and the EMA based teaching models. Consequently, the update of the pseudo-labels tends to be slow. Are there alternative methods that can be used to initialize pseudo-labels to expedite the convergence of model training, such as employing warm-up with labeled data or using mean soft labels (1/num_classes)?\n2. In the Algorithm 1, the update of $\\theta^{t+1}_{S}$ in the last 3rd line is equivalent to the 7th line and it seems to be redundant.\n3. Both Meta Pseudo-Labels[1] and Meta-Semi[2] formulate SSL as a bi-level optimization problem, please explain in detail the differences between BOPL and them.\n4. In Table 1, there exists some unreasonable data, which is different from the original paper. For example, in ReMixMatch[3], the test error of 6.27\u00b10.34 for 250 labels and 5.14\u00b10.04 for 4000 labels in CIFAR-10 in original paper differs from the values presented in this paper. Is it because the experimental setup is different from the original paper? Furthermore, in BOPL, the test error for 250 labels is lower than that for 1000 labels in CIFAR-10, which seems to contradict common sense. If these results are accurate, it could imply that BOPL may not be the top-performing method under this setting. According to common expectations, SimMatch's [4] results at 1000 labels should fall within the range of 4.84 and 3.96, while BOPL's reported result of 5.12 seems less favorable by comparison.\n5. Compared with existing SSL methods, it's possible that the experiments lack some widely used and important settings, particularly in more challenging conditions. It is suggested to validate the effectiveness of the BOPL approach under the setting of 40 labels in CIFAR-10, 400 labels in CIFAR-100 and 250 labels in SVHN, following the setups of FixMatch[5] and SimMatch[4]. Meanwhile, the results of BOPL+ICT are missing from the main text, please provide additional experimental data.\n\n[1]Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V Le. Meta pseudo labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\n[2]Yulin Wang, Jiayi Guo, Shiji Song, and Gao Huang. Meta-semi: A meta-learning approach for semi-supervised learning. arXiv preprint arXiv:2007.02394, 2020.\n[3] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. ReMixMatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations (ICLR), 2020.\n[4] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semisupervised learning with similarity matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n[5] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems (NeurIPS), 2020."
            },
            "questions": {
                "value": "Please respond to the questions mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698634210166,
        "cdate": 1698634210166,
        "tmdate": 1699636368906,
        "mdate": 1699636368906,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Dqtxie0pcw",
        "forum": "AEi2wyAMyb",
        "replyto": "AEi2wyAMyb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_hAat"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4051/Reviewer_hAat"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a new method to update pseudo-labels commonly used in semi-supervised learning (SSL). The \"bi\"-level optimization refers to one objective being the training loss with pseudo-labels as targets, which is a function of both parameters and pseudo-labels, and the other objective of refining pseudo-labels. The objective for pseudo-labels is defined to be the cross entropy on the labeled dataset + entropy of model predictions on the unlabeled dataset. The authors provide a cheap approximation to the gradients of this objective, which is optimized with projected gradient descent, where the authors experiment with different projections (ReLU vs. softmax). This procedure is combined with other SSL methods and tested on common SSL datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The proposed method outperforms many other SSL algorithms on common SSL benchmark datasets. \n* Ablation studies shows that pseudo-labels benefit from ReLU projections instead of softmax which is an obvious choice for projection onto the probability simplex. It's good that the paper experiments with projection methods and found that there is a better working solution that softmax."
            },
            "weaknesses": {
                "value": "* The approximation in Eq. 14 still requires two parameter updates to compute the gradient; this must be slow.\n* Presentation could be improved. Prop. 1 is just a nice expression for the chain rule - consider combining with proposition 2 or moving it to the Appendix? I also think the description in Alg. 1 could be simplified by writing only the steps critical to the algorithm, e.g. no need to write \"compute loss L_{inner}\" which doesn't really help with understanding. Or if the authors feel that it's needed, write the expression again in the algorithm description to make it self-contained.\n* The novelty of this method seems incremental - while it's practically useful, the only real contribution is to write the parameters $\\theta_S$ as a function of $Y$ with a cross-entropy / entropy loss, from which the method naturally follows. However it's not clear why this method results in improved performance."
            },
            "questions": {
                "value": "Is the pseudo-label updates implemented using expression in Eq. (14) under Prop. 2 to update the labels? It isn't mentioned for sure whether this is used; Equation 17 suggests that the labels are being updated with the true gradients rather than the approximation form in Eq. 14. If so, why do the authors describe the approximation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4051/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4051/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4051/Reviewer_hAat"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4051/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698637228934,
        "cdate": 1698637228934,
        "tmdate": 1699636368841,
        "mdate": 1699636368841,
        "license": "CC BY 4.0",
        "version": 2
    }
]