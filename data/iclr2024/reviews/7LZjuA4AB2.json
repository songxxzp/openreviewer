[
    {
        "id": "jQ3Ro12KLS",
        "forum": "7LZjuA4AB2",
        "replyto": "7LZjuA4AB2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_Uetj"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_Uetj"
        ],
        "content": {
            "summary": {
                "value": "The paper investigates why does initialization with a pretrained model improves performance on some tasks but not on others?\nThey argue that a model that is trained from scratch cannot perform well on examples that are not in the support of the training distribution, and argue pretraining can help on such out-of-support instances.\nPretraining, however, as they show cannot help with systematic biases (spurious correlations) in the training data. By combining initialization with a pretrained model and training with a balanced dataset that is free of spurious correlations, we get best of both the worlds. \n\nThe paper is mostly easy to follow but I was not surprised by their results, i.e. it did not improve my understanding of the problem or offer a novel practical advice."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The focus of the paper is practically very relevant.\n- The presentation is somewhat easy to follow"
            },
            "weaknesses": {
                "value": "- **Unclear Contributions**. The contribution of the paper is unclear. Both theoretical and empirical contributions are mild. See next point for comment on theoretical analysis. The paper suggested to use initialization with a pretrained model and training on a balanced dataset, which is a standard practice anyway.\n- **Limited theoretical contribution.** Theoretical analysis considered a very simple setting with an intuitive result. The result is too simple to inform poor extrapolation of random initialization (or training from scratch) or better extrapolation of pretraining in practice. \n- **Vague or inconsistent definition of support**. Out-of-support and in-support are not formally defined. From their analysis and examples, the definition of out-of-support are examples with zero support (or probability) attributed by the input pdf. However, their experiments in Section 4.2 classified examples in and out-of-support using a classifier that is trained to classify between two distributions instead of estimating and using a PDF. \n- **Presentation issues**. ER is a central measure used in the paper, but is not defined. How does the y axis label of Figure 5 (right) relate to MG defined in (4)?\n- **Practical implications are not well argued**. Their takeaways may not be practically relevant. In practice, the distribution shift is likely a mix of \"out-of-support\" (whatever that means) and minority subpopulation. The takeaway of the paper that pretraining helps on out-of-support examples do not bear any practical significance in how we train and deploy a model on the target distribution."
            },
            "questions": {
                "value": "**Q1** In Figure 2 (left), if the training data only has pictures of _indoor dogs_ and _outdoor cats_, then are the examples of _outdoor dogs_ and _indoor cats_ inside or outside the support of the training distribution? \n\n**Q2** In Figure 6(b), how does finetuning a pretrained model on female only dataset (i.e. female only examples from the original dataset) compare with the other results?\n\nMinor: In figure 3, best to clarify that the cat and dog thumbnails are only placeholders and not from CIFAR-10."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698766281006,
        "cdate": 1698766281006,
        "tmdate": 1699636588654,
        "mdate": 1699636588654,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sDOdm4u3b4",
        "forum": "7LZjuA4AB2",
        "replyto": "7LZjuA4AB2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates an interesting problem of the impact of pre-training data on the model's robustness to distributional shifts. The work proposes to characterize the \"failure modes\" of the model under distributional shifts into two types\u2013poor extrapolation and dataset biases. The work then argues that pre-training can help with the first but not the second. This paper proposes two approaches to address these issues\u2013use intervention techniques at pre-training to prevent exploiting biases; and fine-tune on small, non-diverse but debiased datasets."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The problem being investigated is definitely of interest. With the emergence of foundation models, it is of growing interest to better understand the impact of pre-training data and its implications for downstream processes.\n \nThis paper is well-contextualized. Its technical structure is plausible (intuitions, motivating examples, formal analysis, generalization, empirical verification, etc.)."
            },
            "weaknesses": {
                "value": "This paper is a difficult read in general. I was quite attracted by the topic of this paper and had high hope until Theorem 1, which I could not understand after several attempts. **Many technical details are missing or inconsistent (e.g., missing key definitions, no details for important procedures, only providing references with no description at all), rendering many arguments ungrounded and hardly convincing.**\n\n- Theorem 1, key error\u2013$w_{ref}$ is undefined, which is a crucial variable. With this, I can only guess about this theorem. Assuming it is correct in its own sense, the conclusion is problematic. The theorem only gives that pre-training/initialization affects ID but not OOD. Yet, this affect can be either positive or negative,meaning pre-training/initialization does not necessarily help in every case\u2013which I think is true in practical cases. This work has then taken it for granted that pre-training WILL HELP downstream robustness and built the arguments on this as a main basis. **I think there is a major gap.** It is necessary to specify the conditions when it helps/when it hurts/and when it does not affect, which is actually the most valuable part.\n\n- Another fundamental issue for Theorem 1 is that the model is considered deterministc. Even if you use random initialization and stochastic gradient solvers, you end up with the same solution. It is natural to use a stylized model as a starting point to build the analysis. Then I would expect to see how this analysis generalizes to the case of non-convex models where there is inherent learning stochasticity. Yet, without providing anything else, the paper jumps to experiments that are all based on neural network models. **This is another major gap.** Actually, there is not really a \"pre-training\" thing for convex models\u2013regardless of the order you feed data to the model, it always converges to the same optimal solution. The resulting model solely depends on the training data and is irrelevant to initialization or the training process. \n\n- I don't understand why the proposed \"in-support shifts\" would change the classification boundary at all. Image a max-margin classifier (e.g., SVM) and a binary classification task for cat and dog images. (Note that Logistic Regression requires the same probability for both classes. You cannot directly apply it to unbalanced classification problems.) Regardless of the relative proportion for cat or dog images, the underlying distribution for cats and dogs is invariant. A proper classifier would find the decision boundary somewhere between the generating distributions for both classes. I don't see why this is considered \"not robust\". Or a more important question\u2013what is the robustness considered in this work? **The definition for the central notion of this work is not provided.**\n\n- I don't understand the splitting methods for partitioning sketch images into ID and OOD subsets w.r.t. ImageNet. The paper only describes it based on whether they \"look like\". This process actually sounds rather non-trivial. The paper refers to Appendix B 3.2 for details, **which does not exist.**\n\n- For the proposed approaches\u2013use intervention techniques at pre-training to prevent exploiting biases; and fine-tune on small, non-diverse but debiased datasets. The paper merely cites existing works for these techniques without any description. **This renders this work not self-contained and incomplete and this cannot be counted as technical contribution of this work.**\n\n- Format: Appendix is not cut from the main paper. The PDF provided for the main paper is this 32-page document."
            },
            "questions": {
                "value": "- Theorem 1, key variable $w_{ref}$ is undefined.\n\n- Appendix B 3.2, which is referred to in Page 6, does not exist.\n\n- Appendix should not be submitted under the main paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_f9zp"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698779060650,
        "cdate": 1698779060650,
        "tmdate": 1699636588513,
        "mdate": 1699636588513,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "xnFsvCP4xW",
        "forum": "7LZjuA4AB2",
        "replyto": "7LZjuA4AB2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
        ],
        "content": {
            "summary": {
                "value": "The paper proposes to study the effectiveness of model pre-training under various kinds of distribution shifts. A key insight of the paper is that pre-training can help address poor extrapolation but not dataset biases. The paper motivates this insight theoretically and demonstrates the expected behavior empirically in a variety of experimental setups. These include simulated synthetic and real-life shifts, as well as a case study on dataset de-biasing."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper addresses the very interesting topic of exploring whether pre-training helps training performant models under different types of distribution shift. This is an important frontier in the increasingly adopted pre-training fine-tuning training methodology.\n- The ideas presented seem original and the relevant literature is sufficiently discussed.\n- The later sections on developing more robust models were interesting case studies on how to apply the insights from previous sections."
            },
            "weaknesses": {
                "value": "- Although more details are presented in the Appendix, I don't think that the setup for Theorem 3.1 is presented well in the paper. It is not clear why the logistic regression assumption is needed and the proof is not referenced in the main paper. It is also unclear what $proj_{W_{ref}}$ refers to as it is never formally introduced. Is orthogonal to be interpreted mathematically or figuratively? Also \"[...] while the initialization determines how the model extends outside of $W_{ref}$\": since $W_{ref}$ is contained in $proj_{W_{ref}}$. It appears to me like both terms are influenced by $W_{ref}$, not just the first term.\n- Figure 1's legend is unreadable due to overlapping text. I was still able to get the intuition but the authors should fix this. \n- Although Figure 3 presents two examples of in-support and out-of-support shifts, there is no ablation on the shift intensity. I wonder to what extent the shift type influences shifted accuracy and how the shift intensity for a fixed type of shift would alter the experimental results. In other words: does the strength of the bias or the degree of extrapolation matter? My intuition is that this should also play a role. Connecting to this, both in-support and out-of-support shifts are also not formally defined. The descriptions given at the beginning of section 4 should be made more precise.\n- The negative correlation reported in the middle panel of Figure 5 is negative but at the same time the correlation of -0.112 is weak, suggesting that the margin gains are independent of each other rather than complimentary. \n- Overall, the take-away message from this work is a bit unclear to me. While the main paper suggests that pre-training is always desirable (with larger gains for out-of-support shifts than for in-support shifts), Appendix D.2 discusses the possibility of harmful biases being instilled into the model during pre-training. Without assumptions about which distribution to expect at test time (which is what we would want to fine-tune for), it therefore becomes impossible to understand whether a model should have been pre-trained or not. This makes it hard for this method to be applied in practice."
            },
            "questions": {
                "value": "Embedded in Weaknesses above.\n\nI am willing to increase my score as part of the discussion phase if the authors can address my concerns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_cFpN"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698818998569,
        "cdate": 1698818998569,
        "tmdate": 1699636588407,
        "mdate": 1699636588407,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vF0FkIiMDF",
        "forum": "7LZjuA4AB2",
        "replyto": "7LZjuA4AB2",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
        ],
        "content": {
            "summary": {
                "value": "The paper studies the robustness of classifiers, based on pre-training, to distribution shifts. It provides with a chacterization for distribution shifts based on whether the network is asked to extrapolate (out-of-support) or generalize in a group-balanced manner when trained with data which are label-imbalanced or are generated from spuriously correlated features (in-support). They claim that pretraining helps with the former type, but not the latter; providing some theoretical insights in a very simplified setting and performing various ablating experiments. In order to deal with the second type, they argue that pretraining needs to be combined with group-robustness methods, such as methods based on loss reweighting or data rebalancing; and that this strategy provides with complementary robustness benefits (to both proposed types). They conclude by encouraging \u201cpractitioners not to treat pre-training as a panacea for robustness\u201d."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper is easy to follow and for the most part well-written. While the paper does not propose novel methods for robustness, it leads an important discussion on the interplay of pre-trained networks with training methods for group-robustness. It provides with a novel characterization of distribution shifts and it argues about the complementary utility of the two approaches around this characterization, with some theoretical and empirical insights. The reviewer believes that such an analysis paper is needed in light of recent advances in the studied literature and that the authors have identified well that this is a topic of interest.\n\nThe reviewer appreciates the careful discussion in defining and designing in-support and out-of-support shifts, and they believe it is a potentially useful axis of discussion."
            },
            "weaknesses": {
                "value": "While their conclusion might not be incorrect, the design of several experiments is flawed and does not lead to the authors\u2019 claims. The reviewer thinks that these consist a large enough body of the paper to lean towards possible acceptance. In particular:\n\n1.  In **Section 4/Figure 3**, there are multiple variables which are being ablated at the same time. The measured effective robustness is with respect the performance of ResNet18 models, however the pretrained models, which are compared to, have various architectures. It is not clear whether the perceived robustness is due to purely transfer properties from pretraining strategies or from larger model size.\n2. Biased datasets are also susceptible to an *in-support*/*out-of-support* analysis, however one is not given at the study. Specifically, one can imagine the extreme case where some minority group probabilities go to 0, yielding them completely *out-of-support*. For example, imagine the case in **Figure 2 (right)**, where cats are also observed during the night. How does this impact the pretraining algorithms? Current analysis seems to suggest that pretraining would be able to handle the more extreme cases better, which is counter-intuitive. I suggest studying systematic generalization tasks, where some combinations of generative attributes are completely not represented in the training set, but they exist in the test set in a balanced manner. See [1,2].\n3. In **Section 5/Figure 5**, the first correlation estimate is not strong enough to indicate negative correlation of (i) (and thus complementary effects between pretraining and debiasing methods). Please provide with a confidence interval of the correlation estimate, as it seems statistically possible that it is very close to 0. On the other hand, the correlation of (ii) is poorly motivated and it can be tautologically positive, in which case it is of no logical inference value.\n4. In **Section 6**, the proposed form of curated dataset is made so that the spurious feature which is balanced during test-time is completely omitted, namely the \u201cgender\u201d attribute of CelebA. If there is no spurious correlated feature during training, then the transfer problem corresponds to an extrapolation one, for which probably a classifier with simple augmentation/regularization might just work. The construction of counterfactual data, beyond being difficult to achieve in most cases, needs to be ablated to demonstrate that it is a necessary component of the curation process.\n\nFinally, a comparatively minor concern is that the theorerical insight provided corresponds to an overly simplified setting.\n\n[1] Schott, Lukas, et al. \"Visual representation learning does not generalize strongly within the same domain.\" (2021).  \n[2] Tsirigotis, Christos, et al. \u201cGroup Robust Classification Without Any Group Information.\u201d (2023)."
            },
            "questions": {
                "value": "In **Figure 4**, what % of samples are found to be *in-support* for each of the test sets considered? How is absolute test accuracy (iid and ood) affected as we ablate this? In other words, would a test set comprised 100% of *in-support* samples achieve similar absolute accuracy as an iid test set?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5655/Reviewer_kid6"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5655/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698820772561,
        "cdate": 1698820772561,
        "tmdate": 1699636588298,
        "mdate": 1699636588298,
        "license": "CC BY 4.0",
        "version": 2
    }
]