[
    {
        "id": "QBmaxRe5oW",
        "forum": "ymR2bz0cEs",
        "replyto": "ymR2bz0cEs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_dGtW"
        ],
        "content": {
            "summary": {
                "value": "This work focused on video HOI recognition and proposed a hypersphere-based method to learn the interdependency between humans, objects, and interactions. The authors proposed several modules like CF, ISR, and BiGRU to build a new pipeline to learn complex spatio-temporal video HOIs. On three benchmarks, the proposed method was evaluated and compared with previous works and showed improvements."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "+ The complex relations within video HOIs are a meaningful problem for intelligent visual understanding, using hypersphere is an interesting attempt.\n\n+ The whole paper is written well and easy to follow."
            },
            "weaknesses": {
                "value": "- Some design choices were not well illustrated and verified, which will be detailed in the questions.\n\n- Some claims are ambiguous, please give more explanations:\n\nusually lack of ability to capture global context information: which works and why?\n\nultimately compromising their representational accuracy: what is representational accuracy? Why Euclidean cannot?"
            },
            "questions": {
                "value": "1. Why choose the hypersphere? Its pros upon Euclidean? Maybe discussions and experiments for support.\n\n2. Each class has its own hypersphere, then how to embed the relations between classes, e.g., holding and grasping? Is the current setting reasonable?\n\n3. How to handle the multi-label classifications given hyperspheres, and the long-tailed bias?\n\n4. Discussion about the temporal action localization or segmentation? And some possible comparison between this line of works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698330175598,
        "cdate": 1698330175598,
        "tmdate": 1699636339075,
        "mdate": 1699636339075,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "IxYqXb5b9l",
        "forum": "ymR2bz0cEs",
        "replyto": "ymR2bz0cEs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_NfZw"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_NfZw"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an interaction-centric hypersphere reasoning model for multi-person video HOI recognition. The design of interaction-centric hypersphere explicitly directs the learning process towards comprehending the HOI manifold structures governed by interaction classes, a hitherto unexplored domain."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The method proposed in the paper is interesting."
            },
            "weaknesses": {
                "value": "The experimental evaluation is not comprehensive. \nThe presentation for some key concepts and ideas is unclear, which needs extensive improvement."
            },
            "questions": {
                "value": "Presentation:\n1. The definition of the task is unclear. What is multi-person video HOI recognition? How to understand multi-person interaction and what is the specific expression? If it is multi-person interaction, what is the difference from the research direction of group action recognition? This work is anchored to explore multi-person interactive action recognition, so please clearly describe the task content and specific input and output.\n\nIn addition, regarding the definition of the task of this article, I have some idea until the fourth section. The previous sections do not elaborate on it, which is very good for understanding.\n\n2. Why it is called a hypersphere? What is the meaning of hypersphere and does it have any theoretical implications? Hypergraphs and graphs are different theories. In this paper, what is the difference between the concept of a hypersphere and a sphere?\n\n\u201cThe design of interaction-centric hypersphere explicitly directs the learning process towards comprehending the HOI manifold structures governed by interaction classes, a hitherto unexplored domain.\u201d This hypersphere appears to be used to predict interaction probabilities. Please explain how it differs from traditional classifiers? This explanation is necessary since this hypersphere is the key idea. In addition, there is not much comparison, description, and argumentation between manifold structures and hypersphere theories in this paper. On the contrary, other modules explain more, which makes me wonder what is the core of the paper.\n\n3. What are the HOI manifold structure, which has been mentioned several times in this paper? It is hard to understand. Is it related to Riemannian geometry? Please elaborate it. It is better to have a clear explanation.\n\n4. \u201dTo enhance the awareness of complex HOI structures in our representations, we introduce the Context Fuser (CF)...\u201d Is there any connection between complex HOI and multi-person HOI?\n\n5. \u201cTo facilitate interaction reasoning, we place the ISR module on top of the context fuser module, yielding entity representations capable of capturing interaction transition dynamics.\u201d Does entity representation represent the characteristics of human and object entities? Or does it represent the transition characteristics of the same person or object between different states?\nThis sentence confuses me a lot about what exactly it represents.\n\n6. \u201cHowever, current video HOI recognition methods do not fully explore such inherent structural nature of HOI components. Instead, they often opt for disentangled representations for each component, which may have suboptimal representation capabilities.\u201d It is recommended to visualize the problem to be solved, so that readers can understand it clearly.\n\n7. In Figures 2 and 3, it is better to replace the letters with specific features. Using a large number of letters is too unintuitive and makes it difficult for readers to understand.\n\n8. \u201cWe follow 2G-GCN to extract feature of humans and objects from backbone network\u201d.\nYou used 2G-GCN to capture features, but the input {vt}t=1T seems to be a clip. Is the input of 2G-GCN a video? The output is the characteristics of people and objects in each frame of the video? Do ZH and ZO represent the characteristics of people and objects in each frame, or the characteristics of the entire video? I'm totally confused. \n\nExperiments:\n1. Although three datasets are compared, the algorithm is not fully verified. Why the VidHOI dataset is not used for comparison? This is a well-known video-based human-object interaction dataset.\n\n2. There are no comparisons for this hypersphere module in the ablation experiments. It's the key component that needs comparative validation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698767537771,
        "cdate": 1698767537771,
        "tmdate": 1699636338985,
        "mdate": 1699636338985,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "CplKqN2hCd",
        "forum": "ymR2bz0cEs",
        "replyto": "ymR2bz0cEs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_dB2v"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_dB2v"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an interaction-centric hypersphere reasoning model for multi-person video HOI recognition. To do this, a context fuser is designed to learn the interdependencies among humans, objects, and interactions; a state reasoner model on top of context fuser is used for temporal reasoning; an interaction-centric hypersphere is used to represent the manifold structure of HOIs. \n\nThe model is flexible for multi-person or single-person videos. Experiments show the method outperforms the previous method by 22% F1 score on multi-person dataset, MPHOI-72 and the method performs similarly with existing methods on single-person dataset, Bimanual Actions and CAD-120."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper proposes an interaction-centric hypersphere representation scheme for HOI recognition learning.\n- The method achieves SOTA performance with\na huge improvement of more than 22% F1 score over existing methods."
            },
            "weaknesses": {
                "value": "- The main focus of the paper is HOI recognition for multi-person videos. In the experiment, there is only 1 multi-person dataset used for evaluation but 2 single-person datasets. Showing model performance on different multi-person datasets will help strength the claims in the paper.\n- After reading the paper, there is still a lack of proof or explanation about why an interaction-centric hypersphere will help in the task theoretically. The ablation study does not show an ablation study on it."
            },
            "questions": {
                "value": "- In the method, the context fuser and interaction state reasoner extract CLIP features for the representation. Ablation on the features is necessary to test if the feature is important or over-complex where a simple binary feature is enough. For example, in the interaction state reasoner, the two possible states \u201ccontinue\u201d and \u201cstop\u201d can be represented by binary labels or simpler features of lower dimensions compared with CLIP.\n- In 4.3.1 Model inference, during model inference, the interaction probability is predicted on each frame. It is not clear who is in interaction if there are multiple people in the video. Interaction prediction for each person is more detailed and straightforward.\n- Based on the question above, ablation studies with methods of HOI detection on images are necessary. HOI detection can detect interaction for each person. If there are multiple people, the results for comparison from the HOI detection is whether there is any interaction from all people or not.\n- In the Conclusion Sec, it mentions that the method outperforms SOTA on the multiple-people dataset but is on par with the single-person dataset. Is it because in the single-person video, there is only one person so the interaction prediction is determined by that single person? But for multi-person videos, the model does not need to predict correctly for each person to get the correct answer for the image."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698987435810,
        "cdate": 1698987435810,
        "tmdate": 1699636338918,
        "mdate": 1699636338918,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "w1x5bQF7HH",
        "forum": "ymR2bz0cEs",
        "replyto": "ymR2bz0cEs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_3Vxe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3816/Reviewer_3Vxe"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel *Context Fuser* module leveraging the strengths of the CLIP and BLIP models, incorporates an *Interaction State Reasoner* module, and introduces *Interaction Feature Loss* to address the video Human-Object Interaction (HOI) problem."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The experimental results demonstrate superior performance over other methods on MPHOI-72 and CAD-120 benchmarks."
            },
            "weaknesses": {
                "value": "1. **Unfair Comparisons and Insufficient Ablation Studies:**\nThe primary weakness of the proposed method is the unfair comparisons with other video HOI methods and the lack of thorough ablation studies. The Context Fuser employs large Vision-Language Models (VLMs), CLIP and BLIP, pre-trained on big data. While ASSIGAN and 2G-GCN do not. In Table 1, removing Context Fuser results in a notable decline in $F_1@10$. This raises suspicions that the significant performance enhancement could be largely attributed to the text-image alignment capabilities inherent in large VLMs rather than the proposed Context Fuser. The performance is below the benchmark set by Qiao et al., 2020, in the absence of the Context Fuser. The paper lacks critical ablation studies to disentangle the contributions of the VLMs and the proposed method.\n2. **Missing References:**\nThere is a relevant ICLR\u201923 paper you should refer to, Gao et al., ICLR\u201923. Gao et al, which also uses large VLMs for the HOI problem. Unlike fixed prompt used in Context Fuser, while Gao et al. delve into learnable prompts.\n>Gao, K., Chen, L., Zhang, H., Xiao, J. & Sun, Q. Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection. _ICLR_ (2023)."
            },
            "questions": {
                "value": "A deeper ablation study focusing on the efficacy of the CLIP and BLIP parts within the Context Fuser is advisable. This would ascertain whether the observed improvements stem from the newly proposed module or merely from the integration of CLIP and BLIP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3816/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699179287396,
        "cdate": 1699179287396,
        "tmdate": 1699636338826,
        "mdate": 1699636338826,
        "license": "CC BY 4.0",
        "version": 2
    }
]