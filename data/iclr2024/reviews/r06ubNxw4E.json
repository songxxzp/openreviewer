[
    {
        "id": "IeQaxNNnlu",
        "forum": "r06ubNxw4E",
        "replyto": "r06ubNxw4E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_m7N7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_m7N7"
        ],
        "content": {
            "summary": {
                "value": "Targeting at the codebook collapse problem in training deep generative models, this work incorporates evidential deep learning instead of softmax to mitigate the overconfidence issue in matching codebook elements. Experiments on various datasets show that the proposed EdVAE method mitigates codebook collapse and improves the reconstruction performance. Source codes are provided in the supplementary material."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The motivation for using evidential deep learning to mitigate the overconfidence brought by Softmax is clear and convincing.\n+ The paper is well-written and detailed math derivations are provided in the supplementary material, which makes theoretical contributions.\n+ Comprehensive experiments are conducted, which demonstrate the effectiveness of this method.\n+ Source codes are provided, which makes this work easy to reproduce."
            },
            "weaknesses": {
                "value": "- The Related Work section lacks works on evidential deep learning.\n- It seems that the coefficient \\beta in Eq.(8) is set to different values for different methods on different datasets. Is the experiment performance very sensitive to this coefficient? A hyper-parameter analysis experiment on \\beta may be useful for better evaluating the effectiveness of the method.\n- In the EDL formulation, uncertainty is calculated by u=C/S, which indicates that the uncertainty is inversely proportional to the sum of evidence. However, in your method, there is no explicit use of uncertainty. What's the reason?\n- (Minor) How to ensure the evidence learned from the codebook meaningful?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Reviewer_m7N7"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5334/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698733076313,
        "cdate": 1698733076313,
        "tmdate": 1699636536247,
        "mdate": 1699636536247,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eyPw3mTje1",
        "forum": "r06ubNxw4E",
        "replyto": "r06ubNxw4E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_QdFK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_QdFK"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a novel approach, Evidential Discrete Variational Autoencoders (EdVAE), designed to mitigate the codebook collapse issue observed in Variational Autoencoders (VAEs). To address the confirmation bias issue associated with discrete VAEs (dVAE), the authors replace the softmax function with evidential deep learning (EDL). Extensive experimental results are provided to substantiate the effectiveness of EdVAEs in comparison to dVAEs."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The authors adeptly pinpoint and tackle the confirmation bias problem induced by the softmax probabilities in dVAEs.\n- Comprehensive experimental results are showcased, underlining the efficacy of the proposed method against established benchmarks.\n- The paper is articulated well, featuring clear explanations and a coherent structure."
            },
            "weaknesses": {
                "value": "- There is a discrepancy between the method outlined in the text and its code implementation. The paper describes a two-stage sampling process: first sampling a distribution over the codebook from a Dirichlet distribution, parameterized by the output logits, and then sampling a code from this distribution. However, the code implementation directly samples the code from a categorical distribution parameterized by the output logits. Given this, it is unclear why EdVAE, which optimizes a more complex expression for KL divergence, outperforms dVAE, which directly optimizes entropy, when higher entropy is the desired outcome.\n- Beyond the computation of the KL divergence, there are two other distinctions between EdVAE and dVAE: 1. The value of \\alpha^i\n  is capped at a maximum of 20, and 2. EdVAE utilizes a fully connected (FC) layer to compute logits instead of calculating the distance to each code embedding. It would be beneficial to explore whether these modifications would enhance dVAE's performance as well.\n- The experimental results could be more compelling. The paper primarily offers quantitative results, but the improvements in MSE and FID over other methods appear to be marginal."
            },
            "questions": {
                "value": "- Can the authors clarify the inconsistency between the described method and the code implementation?\n- Could the authors elucidate why EdVAE is more effective, given that the goal is to achieve higher entropy, even though EdVAE optimizes a different metric?\n- Could the authors conduct ablation studies on alpha clamping and logits computation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5334/Reviewer_QdFK"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5334/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698808723871,
        "cdate": 1698808723871,
        "tmdate": 1699636536146,
        "mdate": 1699636536146,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "joijuy3Pun",
        "forum": "r06ubNxw4E",
        "replyto": "r06ubNxw4E",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_xKyk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5334/Reviewer_xKyk"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors proposed an original extension of discrete VAE with an evidential formulation (EdVAE) to tackle the problem of Codebook collapse. To be Specific, the authors utilize a Dirichlet instead of distribution as a distribution instead of stochastic process over the Categorical distributions that model the codebook embedding assignment to each spatial position. Extensive experiments demonstrate that the proposed methord improves the current benchmark performance."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "(1)\tThe paper is organized and clearly written.\n(2)\tIn this paper, the author attempted to utilize Dirichlet distribution to solve the problem of Codebook Collapse caused by stochasticity, which seems to be intuitively reasonable.\n(3)\tSufficient experimental results demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "(1)\tThe proposed method lacks innovation. The authors proposed to utilized Evidential Deep Learning (EDL) to tackle the problems of codebook collapse that are the combination of two proposed framework.\n(2)\tThere are few recently proposed methods in the experimental results so that I do not know whether the proposed method achieves the superior performance nowadays.\n(3)\tThe paper lacks ablation experiments, which cannot prove the effectiveness of the proposed module.\n(4)\tThe Motivation is unclear. The authors proposed to tackle the Codebook collapse problem in this paper. However, the proposed method has little relevance to the motivation. I would appreciate it if the authors could further explain the rationale for the proposed approach."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5334/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698890704361,
        "cdate": 1698890704361,
        "tmdate": 1699636536035,
        "mdate": 1699636536035,
        "license": "CC BY 4.0",
        "version": 2
    }
]