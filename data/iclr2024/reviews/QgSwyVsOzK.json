[
    {
        "id": "NLA36X0BaI",
        "forum": "QgSwyVsOzK",
        "replyto": "QgSwyVsOzK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission823/Reviewer_Ajiu"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission823/Reviewer_Ajiu"
        ],
        "content": {
            "summary": {
                "value": "This paper considers that explicit mechanism for incorporating / exploiting knowledge is needed for LLMs to conduct knowledge-intensive tasks. Although existing methods are computationally efficient, they work differently compared to the way human brain works. Borrowing ideas from neuroscience, this paper proposes to represent knowledge as functionals to construct semantic fields to hold relationships among tokens within a text. From the empirical study, the paper shows the superiority of their proposed approach on several classical QA tasks (i.e., WikiQA, SQuAD 2.0, CommonsenseQA)."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper addresses a critical issue within the domain, which is to promote the knowledge reasoning within LMs / LLMs. The idea of paper's proposed approach, which is inspired from neuroscience as stated, to treat knowledge as functionals is novel.\n2. From the empirical study the proposed approach is promising."
            },
            "weaknesses": {
                "value": "1. This paper is somewhat difficult to follow. From the introduction part, the motivation to treat knowledge as functional representation is somewhat unclear for me. I am not sure about the reason to introduce functional representation in Sec. 2.1 because it seems that Eq. (4) - (9) themselves do not contain too much about functionals. \n\n2. Although the paper claims that they aim at incorporating KGs into LLMs for knowledge-intensive task, the effectiveness of the proposed approach with LLMs is lacked in the empirical study, as well as the baselines."
            },
            "questions": {
                "value": "1. Could you explain in more details why the concept of functional is important in KasF?\n2. Could KasF be easily transferred to the use of LLMs?\n3. What is the additional computational cost brought by KasF compared to the backbone language model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission823/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission823/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission823/Reviewer_Ajiu"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698469400024,
        "cdate": 1698469400024,
        "tmdate": 1699636009769,
        "mdate": 1699636009769,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ZagUmKiSkj",
        "forum": "QgSwyVsOzK",
        "replyto": "QgSwyVsOzK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission823/Reviewer_wgCm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission823/Reviewer_wgCm"
        ],
        "content": {
            "summary": {
                "value": "In this paper, the authors address the existing gaps in knowledge-reasoning capabilities, highlighting the need for enhanced performance and sustainability. To tackle this issue, they introduce a novel approach named Knowledge as Functional representation (KasF), which leverages a dynamics-based mechanism. This mechanism is designed to simulate the semantic flow amongst tokens, thereby facilitating the process of knowledge reasoning. The authors present empirical evidence to demonstrate the superiority of KasF in capturing intricate semantic patterns, showcasing consistent improvements in accuracy while utilizing fewer parameters compared to traditional methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.The introduction of KasF stands out as a new method, utilizing a superposition of semantic fields to represent knowledge. This is achieved through the development of a dynamic mechanism, which calculates the similarity between semantic units, ensuring a more precise representation.\n\n2.KasF exhibits an impressive ability to comprehensively capture semantic features, effectively eliminating ambiguities in the representation of entities and relations. This leads to a more robust and accurate knowledge-reasoning process.\n\n3.The paper is well-written, particularly in the methods section, where the authors provide clear and concise explanations of their approach, making it accessible to readers."
            },
            "weaknesses": {
                "value": "1.Despite the complexity and innovation of the proposed method, there is a sense of lack of novelty, especially when viewed in the context of semantic compression or representation enhancements applied to knowledge reasoning tasks.\n\n2.The results presented in the experimental section are not entirely convincing, with the authors relying on outdated baseline models for comparison, which undermines the validity of their findings."
            },
            "questions": {
                "value": "1.The example provided in Section 2.1 of the methods part seems inadequate. Why not illustrate the concept with a more direct example, such as a Question-Answering (QA) scenario? The current example on semantic compression is not as intuitive and might not effectively aid in understanding the method defined in the paper.\n\n2.In Table 2, only the time cost for KasF is listed. For a comprehensive comparison, it would be beneficial to include the time costs associated with other methods as well.\n\n3.Regarding Table 4, which specific model from the GPT-3.5 series was selected for the comparison? A clarification on this would enhance the transparency of the experimental setup.\n\n4.The authors emphasize KasF's advantage over FC Linear in terms of having fewer parameters. However, it would be more logical to compare KasF with other semantic compression methods to provide a fair and accurate assessment.\n\n5.To strengthen the credibility and significance of KasF, applying it to newer knowledge reasoning datasets or integrating it with open-source large language models like LLaMa would be a valuable extension of this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744882822,
        "cdate": 1698744882822,
        "tmdate": 1699636009683,
        "mdate": 1699636009683,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "r21mQ86jdQ",
        "forum": "QgSwyVsOzK",
        "replyto": "QgSwyVsOzK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission823/Reviewer_xHsK"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission823/Reviewer_xHsK"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel representation method, \"Knowledge as the Functional (KasF)\", for knowledge reasoning tasks, inspired by the semantic field concepts in biological and linguistic study. Specifically, based on the dynamics inspiration, a sentence is treated as a semantic flow among semantic units (tokens/words). The representation is formulated as a functional, from the initial to the end token of a sentence, with a task specific objective. The authors implement the formulation with LLMs like RoBERTa, ALBERT as base models. They evaluate the proposed KasF on multiple QA benchmarks, i.e., WikiQA, SQuAD2, and CSQA. Specially, on CSQA, KasF achieves the state-of-the-art single-model performance on the blind test set."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "* This paper is well-written and easy to understand.\n* The proposed method has a clear intuition and gets nice performance on QA benchmarks."
            },
            "weaknesses": {
                "value": "I don't see clear weaknesses in this paper."
            },
            "questions": {
                "value": "* Would it be possible to adapt the approach beyond classification-style QA tasks? e.g., generative QA, reasoning on knowledge graphs, etc."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698935910197,
        "cdate": 1698935910197,
        "tmdate": 1699636009587,
        "mdate": 1699636009587,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "ERseYmHaHi",
        "forum": "QgSwyVsOzK",
        "replyto": "QgSwyVsOzK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission823/Reviewer_b9xY"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission823/Reviewer_b9xY"
        ],
        "content": {
            "summary": {
                "value": "I confess I don't really understand more than 10% of this paper but I will try to provide a summary of parts that I understood.\n\nOverall, the paper tries to provide a new mechanism to improve performance on reasoning tasks by encoding various inputs (queries, documents, etc) using \"functionals\". These functionals are somehow supposed to capture multiple meanings associated with any input word and group provide a grouped representation of words with shared semantics in context. In Section 2, the authors provide a description of their method but I don't understand any part of it since most of the definitions of mathematical symbols are either assumed or just not provided. The first task that authors initialize their method for is semantic compression, which in standard NLP, is simply the task of mapping input text (queries, etc) to simpler representations (for example, remapping the tokens with hypernyms, using distributed representation, etc). In Section 2.1, the authors are trying to do this compression for a query vector defined as $y \\in R^{D_v \\times 1}$, yet  it is unclear what D_v is. They also use a symbol $V \\in R^{n \\times D_v}$ but again it is unclear what n or V is supposed to be here. Other symbols, $z, \\gamma, P$, etc are introduced in this section but not defined. As such. by end of it, I am not sure what really is the output we are aiming for and how is it derived.\n\nSimilarly, Section 2.2, which I conclude is the main method description is mostly unreadable to me. The most I can conclude is that the authors are trying to use their method to provide a sequence to sequence mapping mechanism (from a input sequence X to output sequence Y). \n\nIn the experiment section, the authors are evaluating their method on 3 datasets - wikiQA, SQuAD and CommensenseQA. But given my lack of understanding of method section, I can't really evaluate this intelligently. The best I conclude is that the authors method outperform their baselines for both semantic compression and Reading Compression."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "It is possible with rewriting the readability of the paper can be improved bringing into focus its core contributions. Specifically I do think a method that can represent multiple distinct semantics associated with individual tokens can be useful but in its current form, I have a hard time understanding how the authors are able to achieve it."
            },
            "weaknesses": {
                "value": "The paper is unreadable. I would suggest AC either discard my review from consideration or put much less weight on my review if other reviewers are able to better understand the paper."
            },
            "questions": {
                "value": "In Section 2.1, I would like to understand what exactly is the task being performanced -- 1) what is the input (please provide an example), 2) what is the output, 3) what does symbols D_v, V, N, z means?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "1: strong reject"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698939165104,
        "cdate": 1698939165104,
        "tmdate": 1699636009524,
        "mdate": 1699636009524,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "goQM0pym63",
        "forum": "QgSwyVsOzK",
        "replyto": "QgSwyVsOzK",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission823/Reviewer_CBXQ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission823/Reviewer_CBXQ"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces Knowledge as Functional representation (KasF), a new method for knowledge reasoning in NLP that models knowledge as dynamic semantic fields. This approach outperforms traditional models on several NLP tasks by requiring fewer parameters and providing more precise knowledge encoding. The main contribution is the innovative functional representation that achieves state-of-the-art results on benchmarks such as CommonsenseQA, with the potential for more efficient and sustainable NLP models."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Innovative Knowledge Representation: The KasF model introduces a new functional representation of knowledge that enhances semantic reasoning in NLP.\n\nState-of-the-Art Results: It achieves superior performance on benchmarks such as CommonsenseQA, indicating its potential for accurately handling complex reasoning tasks.\n\nComputational Efficiency: The model's efficiency in terms of parameters used suggests it is less resource-intensive, contributing to more sustainable AI development."
            },
            "weaknesses": {
                "value": "- Complexity of the method: The KasF model's novel approach might be complex for the broader research community to understand and replicate. \n\n- While the paper shows strong empirical results, it may not thoroughly address how the model generalizes to tasks beyond those evaluated. It's unclear if the approach can be applied effectively to more reasoning tasks that are unseen in training.  \n\n- The paper might lack a deeper theoretical discussion on the limitations of the functional representation of knowledge, which would be important for future research to build upon or address its shortcomings."
            },
            "questions": {
                "value": "- How well does the KasF generalize to unseen datasets that are similar to CommonsenseQA, e.g., SocialIQA, PIQA, RiddleSense? \n- How does the KasF model integrate with large external knowledge bases, and what is the impact on its performance when external knowledge is incorporated? Is it possible to do that?\n- Is it feasible to connect KasF to the decoder-only models such as Llama?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 5,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission823/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238362876,
        "cdate": 1699238362876,
        "tmdate": 1699636009294,
        "mdate": 1699636009294,
        "license": "CC BY 4.0",
        "version": 2
    }
]