[
    {
        "id": "bJ53da3Cw7",
        "forum": "NY3wMJuaLf",
        "replyto": "NY3wMJuaLf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FedCOG, a synthetic data-assisted federated learning system, to mitigate the data heterogeneity in the training. The design mainly focused on the local training part. In the local training part, FedCOG first generates task-specific and client-specific data, and then uses knowledge distillation to train the local model. The experiment on computer vision benchmark datasets demonstrates that FedCOG performs well compared to existing FL baselines."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed method is easy to follow. Different from other synthetic-data based methods, the FedCOG proposed task-specific and client-specific data for generation, which is novel and practical.\n\n2. The paper is well-structured. The experiments include several existing FL baselines. The usage of the real-world FL multilabel dataset FLAIR is very rare in the FL literature."
            },
            "weaknesses": {
                "value": "1. I am confused about the data generation part of the reading. To my understanding, FedCOG took a learnable parameter for the data generator. What is the structure of the data generator? Does FedCOG update the weight of the data generator during the training as well? Could the author address more about how the data is generated locally?\n\n2. In the experiment part, what are the sample numbers of the synthetic data in your setup? \n\n3. The client number is so limited for the experiment related to standard datasets.\n\n4. I am concerned that none of the selected baselines is a synthetic data-based method. I see the paper cites FedGen in the related work section. Why does the author not compare with the recent synthetic data-based methods such as FedGen[1] and DynaFed[2]?\n\n[1]. Zhu, Zhuangdi et al. \u201cData-Free Knowledge Distillation for Heterogeneous Federated Learning.\u201d Proceedings of machine learning research 139 (2021): 12878-12889 .\n\n[2]. Pi, Renjie et al. \u201cDYNAFED: Tackling Client Data Heterogeneity with Global Dynamics.\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 12177-12186."
            },
            "questions": {
                "value": "1. How would the FedCOG be harmonized with FedProx? I am curious about how the FedCOG does the local proximal term in the KD-based model training?\n\n2. In Table 4, why FedProx took longer local training time compared to the FedCOG?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734162403,
        "cdate": 1698734162403,
        "tmdate": 1700505422915,
        "mdate": 1700505422915,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "kZ2jlsK9Ui",
        "forum": "NY3wMJuaLf",
        "replyto": "NY3wMJuaLf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a novel consensus scheme based on data generation to solve data heterogeneity problems in federated learning. It achieved a relatively higher accuracy on four public datasets with different degrees of heterogeneity. In the context of each individual client, the present study implemented a methodology wherein the global mode and local model which is extracted from the previous epoch were frozen. The objective was to train the generated data with the aim of optimizing the disparity between predictions made by the global model and those made by the local model, all the while mitigating any potential impact on the overall accuracy of the global model. All goals are evaluated on the generated dataset. Unlike current works focusing on the model, this paper provides a novel perspective from the local dataset. By enhancing the distribution of the local dataset, it claims to achieve better convergence. It achieved relatively higher accuracy on public datasets (FLAIR, Fashion-MNIST, CIFAR-10, and CIFAR-100) with different degrees of heterogeneity compared with federated learning (FL) algorithms like FedProx."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The novel proposed method has a low overhead on each client, making it easy to apply in general FL tasks.\n- Extensive experiments have been done to prove the advantages of the applied method.\n- The paper is well organized and it's easy to follow."
            },
            "weaknesses": {
                "value": "- As far as I know, an enormous amount of work has proved that in the vision task, introducing unbalanced label distribution will influence the performance of the global model, and according to the results of the experiments, it's possible that this empirical idea is true. For details, please refer to the detailed comment C1.\n- Evaluation is not strong enough; For details, please refer to the detailed comment C1~C3.\n- No analysis of convergence is provided. For details, please refer to the detailed comment C3.\n\nDetailed comments:\n- C1 In the experiment results, the final accuracy on CIFAR-10 is relatively low, please try some more complicated networks other than the 5-layer CNN. \n- C2 It's possible the network is not converged. To eliminate such a possibility, please provide a graph depicting the trend of convergence with the number of rounds on the server side. \n- C3 What's more, the proposed method only achieved a little improvement in accuracy, it's not sure whether it's caused by insufficient experiments, please repeat and provide mean and standard error for all results.\n- C4 We kindly request further experimentations involving the generation of datasets of varying sizes, with corresponding meticulous documentation of the associated overhead. Furthermore, if feasible, we encourage experimentation on datasets comprising high-resolution images uniformly, to enhance the comprehensiveness of the analysis.\n- C5 Please add proofs for the convergence analysis. If possible, please add a formal security analysis to your method."
            },
            "questions": {
                "value": "1. This paper introduced data distribution from other clients, will this cause privacy leakage, making it easier for the attacker to learn data information from the clients? \n\n2. Will generating new data for each client be identical to amplifying the global weight update direction collected in the last epoch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1",
                    "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698935798627,
        "cdate": 1698935798627,
        "tmdate": 1700715831094,
        "mdate": 1700715831094,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cG2gpTFzko",
        "forum": "NY3wMJuaLf",
        "replyto": "NY3wMJuaLf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_wN99"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_wN99"
        ],
        "content": {
            "summary": {
                "value": "This paper proposed FedCOG, a Federated Learning (FL) algorithm that facilitates learning via augmented data generated from the global model, which is later used for knowledge distillation between the global and client models. This scheme is compatible with most existing FL algorithms. Its effects have been empirically verified on real-world datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "\\+ This paper tackles a crucial challenge in FL which is Data heterogeneity. Their core idea of data correction for achieving global data consensus is well-motivated.\n\n\\+ Data generation by capturing the residual knowledge between the global and the client model is novel.\n\n\\+ Sensitivity analysis is well designed and conducted.\n\n\\+ This paper is clearly written. Related work is comprehensive."
            },
            "weaknesses": {
                "value": "\\- Data generation on the client step brings extra computation workload compared with classic FL or FL with data generation on the server side.\n\n\\- The method seems to be designed purely for vision tasks. Authors could discuss if the proposed method can be extended to scenarios with other input modalities, such as text inputs.\n\n\\- This paper would further benefit from theoretical derivations to interpret why generated data on the client side helps in improving global model performance.\n\n\\- Concerns on Experiments: All methods in Table 1 achieve notably lower accuracies than SOTA FL methods. I am concerned that the model arch, communication round, or optimizer setting is not well set up for appropriate comparison."
            },
            "questions": {
                "value": "\\- Since tackling data heterogeneity is the key of this paper, I suggest authors conduct more experiments on data with Dirichlet distribution by varying the hyper-parameter $\\beta$. More results with changing heterogeneities would further validate the effects of the proposed methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699025049402,
        "cdate": 1699025049402,
        "tmdate": 1699636814441,
        "mdate": 1699636814441,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FrM9o8so01",
        "forum": "NY3wMJuaLf",
        "replyto": "NY3wMJuaLf",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on federated learning (FL) in the presence of data heterogeneity. Different from the existing methods which usually consider this data heterogeneity as an inherent property and attempt to mitigate the adverse effects, this paper proposes to handle the heterogeneity by generating new data, called FedCOG. There are two key components in FedCOG, including complementary data generation and knowledge-distillation-based model training. It can be plug-and-play, and naturally compatible with standard federated learning protocols.  Extensive experiments on classical and real-world datasets proved the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method is novel, handling the data heterogeneity from the perspective of the data generation, instead of correcting the model.\n2. The proposed method can be a plug-and-play model, and it is naturally compatible with standard FL protocols."
            },
            "weaknesses": {
                "value": "1. The motivation for why we need to use data generation, instead of recent popular methods based on model correction, is somewhat not clear. When the training dataset is very large, the proposed method therefore needs to generate a large amount of data in order to achieve alignment, which is costly, then it seems like model correction is a better choice in such a scenario.  \n2. As the paper mentioned, the proposed method FedCOG has two advantages, i.e., plug-and-play and compatibility with standard FL protocols, however, the unique advantages of this data generation method, compared to previous model correction methods, are still ambiguous. Could you please elaborate more regarding them?"
            },
            "questions": {
                "value": "(see above)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo",
                    "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699506270623,
        "cdate": 1699506270623,
        "tmdate": 1700591876050,
        "mdate": 1700591876050,
        "license": "CC BY 4.0",
        "version": 2
    }
]