[
    {
        "id": "5GSR1Cv0ze",
        "forum": "f7PmO5boQ9",
        "replyto": "f7PmO5boQ9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_EKE2"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_EKE2"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a novel framework for evaluating LLMs in goal-driven, multi-agent, and multi-turn settings called DynaEval. They motivate their approach by pointing out the gap between the existing benchmarks and how LLMs will be used in the wild, which is in interaction with other humans/agents/LMs in which they try to achieve a final goal (be it a common goal or a personal one).\n\nIn the first step, they propose what they believe to be the common ground of such real-world interaction processes to create a unified definition. They call this the Interaction process of DynaEval, which mainly consists of an interaction goal and an interaction rule. They then draw a relationship between this interaction process and extensive games with perfect information (not described in the paper). Furthermore, they then enforce the anonymity and perfect information rules from EGPI on the DynaEval interaction process in order to overcome the stability and fairness issues that they identified.\n\nNext, they explain the structure of the DynaEval process implementation, which consists of a referee, a message pool, and a synchronous interaction algorithm. The authors then explain the implementation of 4 games to evaluate the LLMs with.\n\nFinally, they do experiments with to compare 4 existing LLMs in the aforementioned games under the DynaEval process. The authors claim that GPT-4 is superior in 3 out of the 4 games and ChatGPT (I'm assuming that means GPT-3.5?) is superior in the remaining one."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- Attempting to go beyond the simple static framework of evaluation for LLMs, trying to bring the evaluation setting closer to reality of how these models will be used\n- I quite liked the idea of using games to evaluate LLMs, since even though the games can be quite complex, the evaluation criteria can be pretty simple and mechanistic"
            },
            "weaknesses": {
                "value": "1. My first issue with the paper is clarity:\n\n    a. Even though EGPI seems to be a central concept in their paper, the authors do not even explain what it is. They simply brush over it by adding a citation. I wouldn't expect more than a small minority of the audience at ICLR to be familiar with this concept.\n\n    b. The DynaEval interaction process doesn't seem quite natural and it's not explained from where the \"interaction goal\" and \"interaction rule\", which are supposed to be defining characteristics of these processes, come from. If I had to guess, I would think that the \"definition\" came as a result of trying to match EGPI definition with the DynaEval process and not the other way around.\\\n\n    c. The equations (1) and (2) seem totally out of place in the paper. Honestly, I'm not sure what role they play in explaining any of the concepts. Are they just there to explain the concept of sampling, or are they there simply to provide a \"scare factor\"?\n\n2. Another point of contention is statistical significance:\n\n    a. Looking at Figure 3. Both Figures 3.a and 3.b show a set of highly overlapping distributions. The medians of the top contenders in 3.a are barely distinguishable even, however the conclusion that section 3.2 notes is that GPT-4 performs best is mode 1. It seems to me like the authors are making claims without regard to statistical significance of their results.\n    b. The same is true in Table 2: the difference between GPT-4 (8.77) and ChatGPT (8.74) seems negligible, but that is not reflected in the conclusions of the text\n\n\n3. I'm not sure of the utility of many parts of the proposal:\n\n    a. I'm not convinced that DynaEval framework itself brings anything to the table. I get the utility of using games as evaluation methods, which is a good idea depending on the implementation, but the framework itself doesn't quite bring anything to the table IMO.\n\n    b. The games seem odd, and it's not obvious what capability of the model they are trying to measure. For example, I'm not even sure it makes sense for the Code G&R and the Machine Translation games to actually be turn-based games. As I said, I do see why games can be useful, but the devil is in the details"
            },
            "questions": {
                "value": "I ended up aggregating questions in the weaknesses section as it made more sense."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817313345,
        "cdate": 1698817313345,
        "tmdate": 1699636567724,
        "mdate": 1699636567724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DrEnZeXkP4",
        "forum": "f7PmO5boQ9",
        "replyto": "f7PmO5boQ9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_U79L"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_U79L"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes DynaEval, an interaction-based evaluation framework for evaluating LLMs. DynaEval employs a referee and a message pool in the evaluation process. The authors claim that by this design, the evaluation ensures fairness and stabability. They also discuss the relation between the proposed evaluation procedure and the extensive-form games."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. This paper is very well written and easy to follow. The examples given in the paper help people to understand the procedures of DynaEval. \n2. The problem is well motivated. Indeed existing LLM evaluation frameworks rely on high-quality datasets or human judgement, therefore it is necessary to develop novel frameworks to evaluate their ability of interaction."
            },
            "weaknesses": {
                "value": "Although this paper targets an interesting problem and the presentation is great, I find that the real contribution of this paper is limited or unclear.\n\nFirst, the proposed DynaEval framework lacks a specific evaluation goal since the \"ability of interaction\" is too big. DynaEval focuses on the interaction BETWEEN LLMs, whereas I think the \"ability of interaction\" of LLMs should focus more on the interaction BETWEEN LLMs and HUMAN USERs. In other words, we care more on whether an LLM could understand user intentions and help to execute tasks.Therefore, the ability of interaction should include the ability of understanding, questioning, reasoning or even tool using. I am not sure what specific ability can DynaEval evaluate. The authors may argue that DynaEval is a general framework to evaluate all of these abilities. But I think that is not the core challenge of evaluating LLMs. \n\nSecond, I did not find anything novel in DynaEval except the referee. The paper tries to standardize the interaction process and spends many words to emphasize the fairness and stability, and hence propose a synchronous interaction algorithm which ensures anonymity and multiple indenpendent runnings. However, I think most of them are common senses in many evaluation tasks. For example, if we are evaluating several recommendation models, it is very natural to use the same test set (fairness) and run multiple times (stability). Therefore, I did not find much novelty of DynaEval.\n\nThird, the paper tries to build a close relation between DynaEval and extensive-form games with perfect information (EGPI). But I did not see necessity of doing this.  The authors claim that game theory helps to overcome fainess and stability by introducing anonymity and synchronicity. But I think this is unconvincing. Because the concepts of anonymity and synchronicity are not exlusive to game theory. In fact, EGPI can describe a very broad range of interaction scenarios, therefore it is not surprising that the procedures of DynaEval belong to EGPI. But it should be interesting if DynaEval shares some characteristics of EGPI, such as the existence of equilibrium."
            },
            "questions": {
                "value": "In the example of IDIOMS SOLITAIRE, which specific ability are you testing? From my point of view, winning LLM should have seen more idioms so that it can come up with an answer easily. But does this has anything to do with \"interaction\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698917643433,
        "cdate": 1698917643433,
        "tmdate": 1699636567625,
        "mdate": 1699636567625,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "41evnoTg9H",
        "forum": "f7PmO5boQ9",
        "replyto": "f7PmO5boQ9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_u39q"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_u39q"
        ],
        "content": {
            "summary": {
                "value": "The authors in this work propose an evaluation framework for LLMs by connecting the typically interactive use of LLMs to game theory. Leveraging this connection, the authors provide a general framework for evaluating the performance of LLMs as agents in dynamic real-world games. They construct several explicit scenarios from the framework and find relative performance that seems to be somewhat distinct from static evaluation results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The authors make an important point regarding a significant difference in the typical use case versus LLMs and the typical forms of evaluating LLM performance. They provide a simple and theoretically grounded approach to resolving this difference which they explain clearly and soundly. They demonstrate how to use this evaluation criteria with straightforward and well executed experiments. The experimental results seem to demonstrate that they capture a gap between static and dynamic use of LLMs though this result is not explicitly highlighted by the authors - see weaknesses."
            },
            "weaknesses": {
                "value": "In this work there is a hypothesis that the authors test in their experiment design which they are not explicit about. The hypothesis is that these dynamic evaluation scenarios will uncover different comparative results between LLMs than static evaluation scenarios. It seems to be that they found comparatively different results in their experiments because to my knowledge, static evaluation methods find that ChatGPT underperforms GPT-4 on translation tasks while they find the reverse on their dynamic evaluation dataset. This claim is significant, and should be both highlighted and explained substantially in the text. There needs to be an explicit and clear comparison between the dynamic evaluation results and what static evaluation methods find with an accompanying explanation.\n\nIn fact, the authors emphasize an odd point instead of this comparison to the relative performance in static evaluation. They discuss the \u201cimprovement of performance\u201d through the evaluation, even while highlighting their main results in the conclusion. While LLMs can improve performance on tasks through interaction, this observation seems completely beside the point of DynEval which is an evaluation procedure, so the absolute performance of the LLMs is not important."
            },
            "questions": {
                "value": "Is the stability condition reasonable for LLMs? Couldn\u2019t they not converge? Aren\u2019t those sorts of guarantees made for substantially similar agents? Maybe I also don\u2019t correctly understand what \u201cconverge\u201d means in this context. Some greater clarity here would help.\n\nThe source code link does not work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5534/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5534/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5534/Reviewer_u39q"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699183189337,
        "cdate": 1699183189337,
        "tmdate": 1700600935083,
        "mdate": 1700600935083,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rnJSbfo5BU",
        "forum": "f7PmO5boQ9",
        "replyto": "f7PmO5boQ9",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_ZvMp"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5534/Reviewer_ZvMp"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a dynamic interaction-based evaluation framework for evaluating LLMs named DynaEval. Inspired by game theory, the authors highlight the interaction process of LLMs is theoretically aligned with extensive games with perfect information (EGPI). The interaction process of LLMs in DynaEval is categorized into four steps: selection, interaction, recording, and circulation. The authors propose a fairness condition as letting LLMs be **anonymous** and delivering the messages **synchronously**. The authors specify a stability condition as running multiple times of interactive LLMs independently. In practice, the authors introduce a referee and message pool to implement the fairness and stability condition."
            },
            "soundness": {
                "value": "1 poor"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The evaluation of LLMs with fairness and stability guarantee is a less studied yet very important research problem for the machine learning community. The topic of this paper is highly related to the ICLR conference.\n\n- The paper is well-written and easy to read.\n\n- The intuition of relating the interaction process of LLMs to the extensive games with perfect information (EGPI) is novel, though I feel the connection fails to result in a strong technical contribution."
            },
            "weaknesses": {
                "value": "- Overall, I feel the technical contribution of this paper is limited because the method fails to come up with a practical and novel fairness condition or stability condition. The presented fairness condition and stability condition are both standard practice, with limited new insights. Anonymity, synchronicity, and independent runs are all typical practices during LLM evaluation. As the method is tackling dynamic interaction of LLMs, I do not see any specific part of the algorithm being customized to the specific property of dynamic interaction. Also, I feel the concepts of referee and message pool are not novel either.\n\n- Apart from the refee and message pool, another important concept that is less talked about in the paper is the **task rule**. The task rule itself has a lot to do with fairness and stability for the evaluation task. Regretfully, in the paper, very little details about it has been revealed.\n\n- The referee plays a very important role in the evaluation, responsible for the selection, recording, and circulation process.  The referee also takes charge of evaluating all LLMs' performance. However, for the fairness evaluation of LLMs, it is unclear whether it is possible to develop referees automatically in a supervised signal-based manner with less human-based guidance. \n\n- The discussion on the proposed evaluation method compared to existing ones is quite weak. It is unclear how the proposed one can tackle the dynamic nature of the LLMs interaction while the existing evaluation methods cannot."
            },
            "questions": {
                "value": "Please refer to the WEAKNESSES part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5534/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699409216681,
        "cdate": 1699409216681,
        "tmdate": 1699636567402,
        "mdate": 1699636567402,
        "license": "CC BY 4.0",
        "version": 2
    }
]