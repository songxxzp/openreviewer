[
    {
        "id": "jfZoa4KjcQ",
        "forum": "nNyjIMKGCH",
        "replyto": "nNyjIMKGCH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_tJUL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_tJUL"
        ],
        "content": {
            "summary": {
                "value": "This work proposes a reinforcement learning (RL) framework that utilizes the visual metric (such as IOUs) as the reward function to optimize an encoder-decoder policy network that generates token represented element bounding boxes for visual grounding, specifically in the web-UI domain. The framework is proposed to alleviate the issues of existing pixel-to-sequence works that cannot associate stronger and coherent geometrical information to their token optimization process.\nThe method is tested on mobile and desktop web UI datasets and performance gains were shown to justify their proposed method\u2019s effectiveness."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed RL framework marries the benefits from both vision and language (token generation).\n- The paper is easy to follow, and the method is well-motivated against existing works.\n- The ablation studies on tokens to optimize is justifying for the proposed method.\n- The tackled UI instruction grounding problem is an important task for modern generation AI agents."
            },
            "weaknesses": {
                "value": "- The work is a bit over-claimed by stating that the framework does not require any additional information, while it indeed still requires the ground truth bounding boxes of the elements.\n- More details on the baselines of existing grounding modules, such as GroundingDINO [1] and GLIP [2], are needed. I.e., how the inputs are handled, how the tasks are adapted for their settings, training details, etc. I\u2019m a bit skeptical about the results being that much lower than the proposed module where after all they are all using the same amount of output supervisions for training. (Afterall, the author proposed method is also not benefiting much from non-provided metadata, such as OCRs, so that explanation is not convincing.) These grounding models may have been simply under-tuned.\n- Following up on the previous point, the proposed method adopts pre-trained weights from document understanding tasks that the conventional grounding modules do not have access to. A more fair comparison is to pre-train these grounding modules at least with these document understanding data (perhaps only regressing the boxes of texts and/or contents, the actual text recognition is not so important), as their pre-training domains are far from these structural textual contents.\n- Since the framework does not fully utilize the benefits of web UI browsing tasks (see \u201cQuestions\u201d below), the framework is supposed to be generic enough to also tackle conventional grounding problems on images from, e.g., COCO. (Since the GT boxes are nevertheless still needed.) I would like to see how this method compares with existing grounding modules (even if just compared with pixel-to-sequence ones) on these more generic tasks.\n\n[1] Liu, Shilong, et al. \"Grounding dino: Marrying dino with grounded pre-training for open-set object detection.\" arXiv preprint arXiv:2303.05499 (2023).\n\n[2] Li, Liunian Harold, et al. \"Grounded language-image pre-training.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
            },
            "questions": {
                "value": "- At a first glance, I thought the model can really omit any metadata for supervisions, including even the ground truth boxes. That is to say, this may be the advantage of web UI problems as the predicted boxes may not need to be 100% correct but at least \u201ctouch\u201d the ground truth ones (and then in reality it should be clickable). In this sense, one can really design an RL framework that relies solely on \u201csuccessfully clicked\u201d the right place in the UI as the reward function, and this should be much easier to collect from the data curation point of view. Why not consider this setting as an upgraded version of the framework, and really showcase the power of RL here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Reviewer_tJUL"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698363115586,
        "cdate": 1698363115586,
        "tmdate": 1700083993640,
        "mdate": 1700083993640,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i8oYRhpFb2",
        "forum": "nNyjIMKGCH",
        "replyto": "nNyjIMKGCH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_Nz5p"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_Nz5p"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a UI instruction grounding model which is purely vision-based and requires no additional user-provided information about the UI. The task aims at locating an area in the UI screenshot (eg. a button) given a natural language instruction. To that end, the proposed model takes the UI screenshot and instruction as inputs to predict the geometric coordinates of the bounding box (coordinates of the top-left and bottom-right corners) as a sequence of tokens. To encourage the model\u2019s optimisation by prediction accuracy of the complete bounding box rather than each independent coordinate value, the authors introduce the concept of \u201ccombinational semantics\u201d to scale the loss of different coordinate tokens corresponding to the same bounding box according to its IoU with the ground-truth and update the model according to coordinates only instead of the whole sequence to be completed. Experiments were carried out on both mobile and desktop data, which demonstrates impressive improvements over existing UI instruction grounding models and verifies the effectiveness of the two proposed designs."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper is well-organised and well-motivated to build a purely vision-based UI instructing grounding model. The formulated algorithm is fairly original with a novel \u201ccombinational concept\u201d introduced, which integrated coexisting relationships between tokens in addition to sequential relationships that are commonly adopted in causal language modelling. Experiments are somewhat sufficient to demonstrate the overall performance of the proposed method and the effectiveness of independent components."
            },
            "weaknesses": {
                "value": "I'm generally fine with this paper with just a few minor concerns:\n\nThe two learning objectives in Eq.2 and Eq.3 are used in parallel, but another straightforward idea is to combine the two and scale the losses for coordinate tokens by rewards. Will this work better?\n\nAlthough the Monte Carlo sampling is commonly adopted in the RL community for computing the expectation value of rewards, I\u2019d suggest the authors briefly describe its formulation or core ideas for the paper\u2019s completeness.\n\nWhilst the presentation of the overall ideas and model designs are clear, the paper still have some formatting issues that need to be addressed carefully:\n+ broken references. In the second paragraph of the introduction \u201cAI model (\u2026) and APIs (\u2026; ?)\u201d\n+ typos. At the paragraph above the experiments section \u201cWe estimation the expectation\u2026\u201d; \u201cWe evaluate the effectiveness  \u2026 on UI UI instruction grounding\u2026\u201d\n+ In the paragraph at the end of Page 7, \u201cAs shown in Table 3, RUIG (all tokens)\u2026\u201d should it be Table 4?\n+ In the first paragraph in Sec4.3, \u201cAs shown in Table 6\u2026\u201d, I found Table 6 in the appendix but it has nothing to do with the traditional baselines"
            },
            "questions": {
                "value": "In Sec 3.1, the authors claim that some of the existing grounding methods require the bounding boxes of all UI elements as priors and that limits their generic use in practice. However, the proposed methods also need the coordinates of the bounding boxes as the labels for computing the rewards. In this case, what are the advantages of the proposed methods in terms of practice using? \n\nIn the comparisons to existing methods in Table 5, why not use web data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Reviewer_Nz5p"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639190718,
        "cdate": 1698639190718,
        "tmdate": 1699637123040,
        "mdate": 1699637123040,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "mw29LJ9O0O",
        "forum": "nNyjIMKGCH",
        "replyto": "nNyjIMKGCH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_3sqL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_3sqL"
        ],
        "content": {
            "summary": {
                "value": "The paper addresses the challenge of automating User Interface (UI) tasks through natural language instructions. The authors introduce a multimodal grounding model without the need of metadata information. This model, consisting of a visual encoder and a language decoder, is pretrained on document understanding tasks and subsequently trained to decode spatial information from UI screenshots. By adopting a \"pixel-to-sequence\" approach, it predicts geometric coordinates as a sequence of tokens. Furthermore, the authors propose a novel Reinforcement Learning-based algorithm using policy gradients. This algorithm supervises tokens jointly in a sequence, thereby enhancing spatial decoding capabilities. Through extensive experiments, it's shown that this Reinforced UI instruction Grounding (RUIG) model outperforms existing methods and holds promise as a comprehensive UI task automation API."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The proposed model requires only text instructions and screenshot images as inputs, without the need of UI metadata or additional information.\n- A policy gradients-based approach is introduced to augment the pixel-to-sequence paradigm to be aware of combinational semantics.\n- Various experiments have showcased the superior of the proposed method to surpass existing methods, even those that rely on UI metadata."
            },
            "weaknesses": {
                "value": "- It is not very clear what is the key difference between the UI task and general object grounding tasks [1][2][3]? A follow-up question is that is the proposed reinforced learning method similar to [4] ?\n- In my understanding, introducing Reinforcement Learning is the main contribution of this paper, instead of multimodal large language model, since a lot of papers have been proposed to use multimodal large language model to solve tasks during the past months. However, the authors only use half of the page to illustrate the reinforced pixel-to-sequence model. Is there no detailed introduction or well-curated design?\n- In Section 4.3, when comparing with traditional grounding approaches, it is not surprising the traditional grounding approaches and UI-tailored grounding approaches are not good at understanding UI data, since their language models have less capacity than Large Language Models. Have the recently-proposed LLM-based multimodal models [5][6][7] been tested on UI data ?\n\n\nReference:\n- [1] Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.\n- [2] Modeling context in referring expressions.\n- [3] Visual genome: Connecting language and vision using crowdsourced dense image annotations.\n- [4] Learning globally optimized object detector via policy gradient.\n- [5] Kosmos-2: Grounding Multimodal Large Language Models to the World\n- [6] Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic\n- [7] MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"
            },
            "questions": {
                "value": "I really appreciate the analysis on the limitation of vanilla pixel-to-sequence and the insight on combinational semantics of (xmin, ymin, xmax, ymax). Although the authors propose a new training objective to improve it, I wonder whether it is enough. In other words, do we need to change the mechanism of decoded process, for example, from autoregressive to parallel decoding?\n(I assume this is an open problem, and should not be considered as a weakness of this paper)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8920/Reviewer_3sqL"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751949753,
        "cdate": 1698751949753,
        "tmdate": 1699637122919,
        "mdate": 1699637122919,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "X44MUiJEpJ",
        "forum": "nNyjIMKGCH",
        "replyto": "nNyjIMKGCH",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_yPFE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8920/Reviewer_yPFE"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a novel Reinforced Instruction Visual Grounding (RIVG) model for automating UI tasks using LLMs. Following Pix2Seq, the RIVG model is built upon a multimodal architecture consisting of a visual encoder and a language decoder, which is pretrained on document understanding tasks and then fine-tuned for decoding spatial information from UI screenshots. The authors argue the limitation of the pixel-to-sequence paradigm, where the loss is not optimized towards the \"combinational semantics\", e.g. a bounding box prediction of <bbox><x1><x2><y1><y2></bbox>, the current loss implementation is treating each token separately instead of the bounding box coordinates as a whole. The authors propose a reinforcement learning-based algorithm that jointly supervises tokens in the sequence with visually semantic metrics, effectively enhancing the spatial decoding capability. Extensive experiments demonstrate that the RIVG model outperforms state-of-the-art methods and has the potential to serve as a generic UI task automation API."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The use of reinforcement learning and policy gradients for optimizing directly towards the IoU metric is an interesting and novel approach.\n- The motivation for the awareness of the combinational semantics is intriguing and addresses a limitation in the pixel-to-sequence paradigm."
            },
            "weaknesses": {
                "value": "- It is unclear if the benefit of the reward loss is due to the lack of model capacity in the LLM or if it would scale with the model size (e.g. LLaMA-2) and the model knowledge (e.g. more training data). It is nice that the authors conduct the scaling experiments in Table 6, but it is not necessarily at the scale of the recent large language models and has not undergone large-scale pretraining. Despite LLMs use the same loss at token level, which also has the \"combinational semantics\" issue, they are able to achieve complex reasoning capabilities as they scale up."
            },
            "questions": {
                "value": "- How do you compare the original Pix2Seq model, and the recent instruction-tuned multimodal models like LLaVA [1]? Can we think of them as Pix(image)2Seq(language)? -- bounding box outputs is a special case. If LLaVA is finetuned to (1) understand the text; (2) predict the bounding box for objects using datasets like COCO, would the proposed approach still be beneficial compared with SFT on these datasets? Given that this is a concurrent work, I am putting this in the questions section instead of a weakness.\n\n[1] Liu et al. Visual instruction tuning. NeurIPS 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8920/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698870026445,
        "cdate": 1698870026445,
        "tmdate": 1699637122798,
        "mdate": 1699637122798,
        "license": "CC BY 4.0",
        "version": 2
    }
]