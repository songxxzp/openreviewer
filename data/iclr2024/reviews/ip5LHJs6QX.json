[
    {
        "id": "2mZjNUj7w1",
        "forum": "ip5LHJs6QX",
        "replyto": "ip5LHJs6QX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
        ],
        "content": {
            "summary": {
                "value": "UPDATE: the rebuttal has answered many of the issues and I have reflected this in the score.\n\n\nThis paper describes an optimized deep learning architecture for vision tasks. It is related to a line of work that utilizes mixtures of transformers and CNNs or adds modulation to CNNs in order to come up with an architecture with high accuracy and low computational complexity and low latency."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "S: One strength of the paper is the relative simplicity of the architecture compared to some related works.\n\nS: Another strength seems to be good empirical results on selected vision tasks.\n\nS: Even though on the surface the work seems to be incremental improvement over VAN and FocalNets, this paper generalizes them nicely and provides a simpler alternative, which also seems to perform better. Also, any improvement in efficient models for vision tasks is naturally important."
            },
            "weaknesses": {
                "value": "W: The paper is mainly constructive and experimental in nature. In the appendix there is a tentative explanation that describes that the modulation might lose effectiveness in larger networks. Expanding on this and at the same time showing this in larger networks would make the contribution stronger.\n\nW: One of the most curious things about the paper is the ablation results in Table 5. From there it seems that replacing the modulation with just a regular residual connection (sum) has quite modest performance drop (abs perf drop 1%). Without the multiplication, unless I am mistaken, the architecture reduces to a ResNet with specific hyperparameters and two-path construction. Could the authors discuss this. I think both contributions are valuable, but I am wondering whether it is correct to attribute the performance of the architecture to the modulation since other aspects seem to play even larger role than abs. perf. of 1%. \n\nW: Continuation of the above (without mult). Would the architecture without mult be second best of the architectures compared in Table 2. If so, please add it there.\n\nW: Without mult cont: What would be the performance of the ResNet (no mult) version with the VIT-style attention layers on top?\n\nW: Since this work is most closely related to VAN and FocalNets those works should be at least briefly mentioned in the related work section as well. \n\nW: In Fig 1 and corresponding text, would it make sense to mention where nonlinearities are applied?"
            },
            "questions": {
                "value": "Q: Is Figure 1a) missing the softmax part of the transformer architecture? I think this is one of the key differences to the modulation designs.\n\nQ: In section 3.1, the FC layer seems the same as a 1x1 pointwise convolutional layer. If this is correct, it might be beneficial to mention this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8518/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_6eCe"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698498147079,
        "cdate": 1698498147079,
        "tmdate": 1700223429124,
        "mdate": 1700223429124,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Io9VBb5CpQ",
        "forum": "ip5LHJs6QX",
        "replyto": "ip5LHJs6QX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a unified convolutional-based building block, EfficientMod, which incorporates favorable properties from both convolution and attention mechanisms. Comparing the prior work shown in Figure 1(b), EfficientMod firstly fuses the FC layers from the MLP and the modulation block to achieve better efficiency, resulting in a unified block. Secondly, EfficientMod includes simplified context modeling, which employs one large-kernel depth-wise convolution layer between two linear projection layers. Extensive experiments and comparisons demonstrate the effectiveness of the proposed method across a range of different tasks (classification, detection, and segmentation)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The proposed method, EfficientMod, is remarkably simple, which could exert a significant influence when deploying a deep model on a resource-limited device.\n- The experimental results clearly showcase the effectiveness of EfficientMod in outperforming existing state-of-the-art methods across various tasks (classification, detection, and segmentation)."
            },
            "weaknesses": {
                "value": "- Examining Figure 1, and comparing (b) and (c), the proposed EfficientMod block fuses the MLP on the top and the modulation block as one unified block to improve efficiency. It is conceivable that this might limit performance when using the same number of parameters. The authors should elucidate the principles behind this design, not only from the perspective of efficiency but also in terms of representational ability.\n-  Building on the first point, it is imperative to present a comparison between (b) and (c) with the same number of parameters, both in terms of performance and efficiency, and under the same training settings. For example, a comparison between (b), only with fused MLP, and (c).\n- Could the authors discuss whether the transformer block can also benefit from the proposed method for efficiency (at least it is feasible to fuse the MLP into one unified block)?\n- In Figure 8 and Figure 9, one of the notations in the legend should be 'EfficientMod'."
            },
            "questions": {
                "value": "My main concern lies with points 1 and 2 in the weaknesses. I look forward to the authors\u2019 response."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8518/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8518/Reviewer_Q55t",
                    "ICLR.cc/2024/Conference/Submission8518/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698775703016,
        "cdate": 1698775703016,
        "tmdate": 1700691653980,
        "mdate": 1700691653980,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "okwV8G9sDS",
        "forum": "ip5LHJs6QX",
        "replyto": "ip5LHJs6QX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_fvW3"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an efficient modulation block to build efficient vision networks. The proposed EfficientMod block mainly consists of a simple context modeling design (CTX) with FC and Conv layers. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that the proposed method achieves strong performance compared with prior methods."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tThe proposed model is simple yet effective.\n2.\tThe proposed model shows strong performance on several benchmarks, including ImageNet, COCO, and ADE20K."
            },
            "weaknesses": {
                "value": "1.\tIn Table 2, there are no latency reported for state-of-the-art efficient models.\n2.\tThe proposed method seems simple and more analysis and motivations for the design are needed to understand the principal of the design choice.\n3.\tImportant baselines such as ConvNeXt and Swin Transformer are not included in the comparisons."
            },
            "questions": {
                "value": "See the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698840158638,
        "cdate": 1698840158638,
        "tmdate": 1699637064433,
        "mdate": 1699637064433,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "4zya1XQK6o",
        "forum": "ip5LHJs6QX",
        "replyto": "ip5LHJs6QX",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8518/Reviewer_xsDm"
        ],
        "content": {
            "summary": {
                "value": "This paper introduced a new model structure based on previous modulation designs to further improve the efficiency (especially inference latency) and performance. The paper revisited previous modulation designs and improved the efficiency by reducing fragmented operations and simplifying the structure. The proposed method shows better performance than previous efficient networks on ImageNet with lower latency. The improvements also transfer to detection and segmentation."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1.\tThe paper had a clear introduction to previous works and how is the proposed method motivated from these works. This makes it easier to follow the work and understand how it works.\n2.\tThere are extensive experiments on multiple tasks. And the proposed method achieves better performance and latency than previous efficient models."
            },
            "weaknesses": {
                "value": "1.\tThere are limited technical contributions in the work. This paper focuses on improving the latency of previous works. The improvements/changes from previous works are mainly engineering designs, for example, fuse multiple FC layers together, fuse multiple DWConv into a larger one, replace reshape operation with repeat. The guidance is mainly from previous works such as ShuffleNet v2, which is to reduce fragmented operations for improved latency. There are limited new insights.\n2.\tIt is not clear how much efficiency improvement does each design contribute. I suggest the author to conduct a thorough ablation study to show the impact of each structure change, and explain why it could achieve improvement.\n3.\tFig 1 is good to illustrate the difference between the proposed method and previous works. But it could be better to expand Fig 1 (c) in details when explaining the method. This makes it easier to understand the proposed structure and details.\n4.\tIn Table 1, why VAN and FocalNet results are not included? They seem to be the most relevant works.\n5.\tIn Table 2, why adding Attention even reduced the FLOPs?\n6.\tIn Table 1, are the GPU and CPU latency of different models measured on the same device?"
            },
            "questions": {
                "value": "Please see the weakness part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8518/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698899519754,
        "cdate": 1698899519754,
        "tmdate": 1699637064296,
        "mdate": 1699637064296,
        "license": "CC BY 4.0",
        "version": 2
    }
]