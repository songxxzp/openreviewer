[
    {
        "id": "4EwFTpizR2",
        "forum": "ktG8Tun1Cy",
        "replyto": "ktG8Tun1Cy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_FT6d"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_FT6d"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses the problem of text-to-3D, in which a 3D model of an object (represented by a NeRF) is produced given a text prompt describing the object. It shows that the classifier-free guidance part of the SDS loss is the main term driving the optimization of the NeRF, hence, proposing a new loss called Classifier Score Distillation (CSD). Furthermore, they also leverage the negative prompts to drive the rendered image away from low-quality region. In the experiment, the authors qualitatively show that the new CSD loss is easy to optimize as the SDS loss but bring the 3D model quality similar to the VSD (proposed in the Prolific Dreamer paper)."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is easy to understand and well-written. \n2. The qualitative results are promising. \n3. The proposed loss is simple and easy to reimplement."
            },
            "weaknesses": {
                "value": "1. The main weakness of this paper is that its reproducibility. Since the method is simple enough that I can reimplement it in the code base of SDS loss in the threestudio framework. However, I try my best to replicate every provided detail of the results are not good as shown in the paper. They are more or less like SDS, not good as VSD loss as claimed. Therefore, it would be much better if the authors do not provide their implementation to verify during the rebuttal phase, otherwise, it greatly affects their contribution.  \n2. In our reimplementation, the Janus problem is very serious. \n3. Lack of quantitative comparison with SOTA approaches such as Prolific Dreamer, Fantasia3D...."
            },
            "questions": {
                "value": "1. What negative prompts did you use?\n2. How well does the CSD loss perform without the help of negative prompts. i.e., with the Eq. (7) only?\n3. Which 81 text prompts you chose to compute CLIP R-precision, why don\u2019t you compute all the text prompts (415 text prompts) provided in the DreamFusion repo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1304/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676797777,
        "cdate": 1698676797777,
        "tmdate": 1699636057794,
        "mdate": 1699636057794,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LCi6qvQlJd",
        "forum": "ktG8Tun1Cy",
        "replyto": "ktG8Tun1Cy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_pueN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_pueN"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a text-to-3D generation model by exploring classifier-free guidance in score distillation. Experiments are conducted on several text-to-3D tasks to evaluate the proposal."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "++ The main idea is simple yet effective for text-to-3D generation.\n\n++ It is good to include an in-depth discussion about SDS in section 3.\n\n++ Lots of promising qualitative results are shown to validate the effectiveness of proposal."
            },
            "weaknesses": {
                "value": "-- According to implementation details in section 5.1, this work uses two different pre-trained text-to-image models (DeepFloyd-IF stage-I model and Stable Diffusion 2.1). So is there any reason or ablation study for this design choice?\n\nIn addition, some baselines (like ProlificDreamer) only use the pre-trained text-to-image model of Stable Diffusion. It is somewhat no fair to compare this work with other baselines using different pre-trained models.\n\n-- The evaluation of text-guided 3D generation is performed over 81 diverse text prompts from the website of DreamFusion. However, I noticed that the website of DreamFusion (https://dreamfusion3d.github.io/gallery.html) contains lots of results (more than 81 prompts). So how to choose the 81 diverse text prompts? Any screening criteria behind?\n\nMoreover, this evaluation only uses CLIP ViT-B/32 to extract text and image features, while DreamFusion uses three models (CLIP B/32, CLIP B/16, CLIP L/14) to measure CLIP R-Precision. So following DreamFusion, it is better to report more results using more CLIP models.\n\n-- The experimental results are somewhat not convincing, since the comparison of quantitative results is inadequate and more detailed experiments/baselines should be included:\n\n1) For text-guided 3D generation, Table 2 only includes two baselines, while other strong baselines (Fantasia3D and ProlificDreamer) are missing.\n\n2) Section 5.2 only mentions the computational cost of ProlificDreamer and this work. It is better to list the computational cost of each run.\n\n3) For text-guided texture synthesis, a strong baseline [A] is missing for performance comparison. Moreover, only user study is performed for this task, and I am curious to see more quantitative comparison using the CLIP score or CLIP R-Precision.\n[A] Lei J, Zhang Y, Jia K. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition[J]. Advances in Neural Information Processing Systems, 2022, 35: 30923-30936.\n\n-- I am curious to see more results by plugging the proposed CSD into more baselines (like  DreamFusion and ProlificDreamer)."
            },
            "questions": {
                "value": "Please check the details in Weaknesses section, e.g., more clarification about implementation details and more experimental results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1304/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729167493,
        "cdate": 1698729167493,
        "tmdate": 1699636057724,
        "mdate": 1699636057724,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qEnrxaps1a",
        "forum": "ktG8Tun1Cy",
        "replyto": "ktG8Tun1Cy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_8VxR"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_8VxR"
        ],
        "content": {
            "summary": {
                "value": "This manuscript introduces a novel perspective on score distillation sampling (SDS). Classifier free guidance (CFG) can be interpreted as an implicit classifier based on the diffusion model that scores how much the image corresponds to the text. Empirically, SDS adds a CFG term to its gradient to ensure that the generation corresponds to the text prompt. However, by doing so, the gradients used in SDS in practice are dominated by this CFG term. This work proposes Classifier Score Distillation (CSD) which uses solely this CFG term to provide the gradients. This paper shows that CSD alone is sufficient to guide 3D generation. Furthermore, this work uses its CSD formulation to give a new interpretation of negative prompting with CFG and proposes a new negative prompting formulation that allows for explicit weights on both the positive and the negative directions.  This paper compares CSD both qualitatively and quantitatively to numerous baselines on multiple generation tasks showing SOTA performance. This work also shows CSD on editing tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- Novel formulation of score distillation that gives an interesting new perspective.\n- CSD is general and can be used for any approaches using score distillation (text-to-3D, text-driven image editing) to improve results. It can also be seamlessly integrated into any existing score distillation approaches.\n- Thorough evaluation shows that CSD gives improvement over SDS both qualitatively and quantitatively.\n- The paper is well written, clearly motivating and explaining the intuition behind CSD."
            },
            "weaknesses": {
                "value": "Major:\n- This likely inherits the weaknesses of using a high CFG with standard SDS (I assume the following are true, but see questions for more details): less diversity of generations for a given prompt, less realistic generations, over saturated colors. [1]\n- If I understand correctly, empirically, this is not much different than using SDS with a large weight for CFG. It would be helpful to show comparisons to SDS with very large CFG weights. See questions for more details.\n\nMinor:\n- Figure 2a: It might be more clear to show both norms on the same scale. At first glance it can be confusing if you don\u2019t notice the different scales.\n- Figure 2b: Consider including CSD here. It would be interested to see higher values for w as well since DreamFusion uses w=100.\n\nReferences: [1] Wang, Zhengyi, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. \"ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation.\" arXiv preprint arXiv:2305.16213 (2023)."
            },
            "questions": {
                "value": "- Does using CSD cause images to be less \u201crealistic\u201d since it removes the prior term of the loss? I.e. the generation will adhere to the text prompt very closely, but lead to an potentially unrealistic result?\n- Similarly, how is the diversity of generations using CSD for a given prompt? I would guess that there is less diversity than SDS since higher CFG weight typically reduces diversity.\n- What are the CFG weights used in the experiments section for the SDS on the baseline methods? It is specified that the DreamFusion results were obtained from its website implying a CFG of 100, but what about for the others? The default value in ThreeStudio appears to be 100 for methods using stable diffusion and 7.5 for Prolific Dreamer. Is that what was used for the experiments? If so, it might be helpful to add experiments showing existing SDS methods with very large CFG weights (i.e. 200, 500, 1000, etc.) and see how that compares to CSD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1304/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1304/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1304/Reviewer_8VxR"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1304/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698734688035,
        "cdate": 1698734688035,
        "tmdate": 1699636057615,
        "mdate": 1699636057615,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "LwLayUpXiB",
        "forum": "ktG8Tun1Cy",
        "replyto": "ktG8Tun1Cy",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_oCeh"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1304/Reviewer_oCeh"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a new score distillation scheme for text-to-3D generation, dubbed, Classifier Score Distillation (CSD). While the original Score Distillation Sampling (SDS) from DreamFusion subtracts random noise, CSD subtracts unconditional noise estimate (or noise estimation with negative prompts). With CSD, the author shows its effectiveness in text-to-3D generation and texture synthesis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- CSD is a simple yet effective method in transferring 2D diffusion prior to the 3D scene generation or editing. In contrast to prior state-of-the-art ProlificDreamer, it does not require fine-tuning of diffusion models, which may introduce training inefficiency and instabilities. \n\n- The qualitative and quantitative results show its effectiveness compared to prior methods. Also, this work presents a relationship between Delta Denoising Score which also used subtraction of noises in image editing tasks. I believe this is also related to the noise subtraction scheme in collaborative score distillation [https://arxiv.org/abs/2307.04787] paper, which the discussion will make the paper more complete."
            },
            "weaknesses": {
                "value": "- In general, I do not see a crucial weakness of this paper as it illustrates a simple method that improves the current text-to-3D generation. I believe providing detailed hyperparameter ablation study will make  the paper more informative."
            },
            "questions": {
                "value": "- See Strengths; how the image-conditioned noise subtraction of InstructPix2Pix diffusion model in Collaborative Score Distillation paper can be related to classifier score distillation? Can Collaborative score distillation can be improved with classifier score distillation like approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1304/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772648511,
        "cdate": 1698772648511,
        "tmdate": 1699636057512,
        "mdate": 1699636057512,
        "license": "CC BY 4.0",
        "version": 2
    }
]