[
    {
        "id": "z3l8uE7E0e",
        "forum": "cG2BAbFnA4",
        "replyto": "cG2BAbFnA4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_CHdE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_CHdE"
        ],
        "content": {
            "summary": {
                "value": "This paper addresses complementary-label learning, which is a weakly supervised learning problem. The proposed method does not rely on the uniform distribution assumption nor on the ordinary-label training set. More importantly, this method is risk-consistent with theoretical guarantees. Experiments on both synthetic and real-world benchmark datasets validate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed method does not rely on the uniform distribution assumption nor an ordinary-label training set, which is more realistic.\n2. The proposed method is risk-consistent with solid theoretical guarantees.\n3. The introduction part is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The notations are confused, e.g., $\\overline{Y}$ is a set of label vectors composed of {0,1}, and $k$ denotes the number of classes in {1,2, ..., q}, however, on page 4, line 5, the authors claim that \"$k\\in\\overline{Y} $\". The notations should be carefully checked and standardized.\n2. Although the experimental results of the proposed method in the paper are better than the compared methods, there is a lack of experimental analysis provided. A thorough analysis of the experimental results would be helpful in understanding the factors contributing to the superior performance.\n3. Although this paper provides nice theoretical results, it does not explain why the proposed method performs well. Which part plays an important role, the one-versus-rest (OVR) strategy, the risk correction function, or any other techniques?\n4. The details of the constant $c_k$ in assumption 1 are missed. How to decide this constant? Will the choice of this constant affect the performance?"
            },
            "questions": {
                "value": "1. Why set the label $\\bar{y}$ as a q-dimensional vector? Intuitively, adding more complementary labels would improve the performance.\n2. The assumptions in Theorem 2 directly borrow from Theorem 10 of [1]. According to these assumptions, which functions can be used as the loss function $l$? Can you list some of these and write the explicit form of $l$ that can be used in practice?\n3. The compared method GA[2] also adopts a risk correction approach, can you explain why your CONU outperforms GA?\n\n[1] Tong Zhang. Statistical analysis of some multi-category large margin classification methods. JMLR, 2004.\n\n[2] Takashi Ishida, Gang Niu, Aditya K. Menon, and Masashi Sugiyama. Complementary-label learning for arbitrary losses and models. ICML 2019."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Reviewer_CHdE"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698676372387,
        "cdate": 1698676372387,
        "tmdate": 1699636181400,
        "mdate": 1699636181400,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "a9DSxKaJrg",
        "forum": "cG2BAbFnA4",
        "replyto": "cG2BAbFnA4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_XP12"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_XP12"
        ],
        "content": {
            "summary": {
                "value": "This manuscript utilized the OVR strategy to decompose the complementary-label learning into a set of negative-unlabeled classification problems."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This manuscript utilized the OVR strategy to decompose the complementary-label learning into a set of negative-unlabeled classification problems."
            },
            "weaknesses": {
                "value": "1. ``Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix. '' This argument is false because some cll algorithms are designed for both uniform and non-uniform distribution.\n\n2. The methodologies compared in this research are outdated, if not obsolete. I'm afraid that most cutting-edge techniques are missing.\n\n3. Lack of comparison. Because cll is a subset of pll, the methods comparison should include the most recent pll methods.\n\n4. The loss proposed for complementary label learning is uncear.\n\n5. The experiments are conducted on simple datasets, some complex dataset like cifar100, subset of webvision should be used to verify the effectness of the proposed method."
            },
            "questions": {
                "value": "1. Is this strategy of this paper is similar to a previous work of Ishida2017[1].\n\n[1] Takashi Ishida, Gang Niu, Weihua Hu, and Masashi Sugiyama. Learning from complementary\nlabels. In Advances in Neural Information Processing Systems 30, pp. 5644\u20135654, 2017."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698773787588,
        "cdate": 1698773787588,
        "tmdate": 1699636181312,
        "mdate": 1699636181312,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "B1xn1pS1t6",
        "forum": "cG2BAbFnA4",
        "replyto": "cG2BAbFnA4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_ye4F"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_ye4F"
        ],
        "content": {
            "summary": {
                "value": "This paper tackles complementary label learning by regarding the problem as multiple negative-unlabeled learning problems. This novel formulation avoids explicit assumptions on the label distribution relationship between complementary and ground-truth labels, and is risk-consistent with theoretical guarantees: A risk-consistent estimator. Empirical results validate promising performance of the proposed approach."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Solid justifications on the statistical consistency and convergence rate of the corrected risk estimator have been provided with proofs.\n2. Method that avoiding the assumptions on the transition matrix is a substantial contribution to the CLL community."
            },
            "weaknesses": {
                "value": "1. More experiments with instance-dependent CL data should be investigated, due to the practical reason mentioned in this paper.\n2. The performance of prior estimation should be evaluated in the empirical study."
            },
            "questions": {
                "value": "1. I notice some results of existing methods are much worse than the results reported in their original papers. For example, the results of NN and GA on K-MINST and F-MNIST in paper [1] are much higher than that are reported in your paper.\n2. Have you tried FORWARD [2] on CLCIFAR datasets? I notice that the results of FORWAD is pretty good on these instance-dependent CL datasets and should be involved in the comparison.\n\nRefs.\\\n[1] Chou Y T, Niu G, Lin H T, et al. Unbiased risk estimators can mislead: A case study of learning with complementary labels[C]//International Conference on Machine Learning. PMLR, 2020: 1929-1938.\\\n[2] Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary labels. In Proceedings of the 15th European Conference on Computer Vision, pp. 68\u201383, 2018."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2453/Reviewer_ye4F"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698776335577,
        "cdate": 1698776335577,
        "tmdate": 1699636181217,
        "mdate": 1699636181217,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "h6yuuIESba",
        "forum": "cG2BAbFnA4",
        "replyto": "cG2BAbFnA4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_MvNX"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2453/Reviewer_MvNX"
        ],
        "content": {
            "summary": {
                "value": "This paper points out that the existing complementary-label learning approaches have relied on some assumptions about the distribution of complementary labels, or on an ordinary-label training set, which may not be satisfied in real-world scenarios. It then proposes a risk-consistent approach that express complementary-label learning as a set of negative-unlabeled binary classification problems, using the one-versus-rest strategy. Furthermore, it introduce a risk correction approach to address overfitting problems when using complex models. It also proves the statistical consistency and convergence rate of the corrected risk estimator."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The idea of expressing complementary-label learning as a set of negative-unlabeled binary classification problems is very novel and sensible.\n2. The proposed approach doesn\u2019t rely on assumptions about the distribution of complementary labels or ordinary-label training set, which makes it more suitable for real-world scenarios.\n3. The result is promising."
            },
            "weaknesses": {
                "value": "1. The major novelty lies in the problem reformulation. The way to conduct theoretical analysis and risk correction is off-the-shelf. In this sense, the technical contribution is not very impressive.\n2. In Fig2, only the impact of inaccurate class priors over the proposed method is illustrated. How about the competitors?\n3. Some recent PU-learning methods should be reviewed in Sec.2.2, e.g.,\n[1] Beyond Myopia: Learning from Positive and Unlabeled Data through Holistic Predictive Trends. NeurIPS 2023.\n\n[2] Positive-Unlabeled Learning With Label Distribution Alignment. TPAMI 2023.\n\n[3] GradPU: Positive-Unlabeled Learning via Gradient Penalty and Positive Upweighting. AAAI 2023.\n\n[4] Dist-PU: Positive-Unlabeled Learning From a Label Distribution Perspective. CVPR 2022."
            },
            "questions": {
                "value": "A brief introduction for the compared methods to explain their respective characteristics will help understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2453/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698811706062,
        "cdate": 1698811706062,
        "tmdate": 1699636181129,
        "mdate": 1699636181129,
        "license": "CC BY 4.0",
        "version": 2
    }
]