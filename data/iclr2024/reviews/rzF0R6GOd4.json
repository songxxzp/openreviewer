[
    {
        "id": "k5qpEHunzq",
        "forum": "rzF0R6GOd4",
        "replyto": "rzF0R6GOd4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_uMrz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_uMrz"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles dynamic surface reconstruction and scene-flow estimation from multi-view RGB video with known camera parameters. It uses a global coordinate-based MLP that outputs the SDF in canonical space (t=0), which then gets converted to density via VolSDF's formula, enabling NeRF-style rendering. Training uses an RGB reconstruction loss and an Eikonal regularizer for the SDF. The method departs from prior dynamic NeRFs in its deformation model: a separate MLP is trained to predict the temporal change (derivative) of the SDF at any point in space. These changes get integrated with Runge-Kutta over time to obtain the time-dependent SDF. Furthermore, the paper shows how this SDF flow can be converted into scene flow. Experiments show that the geometry is either less noisy or more detailed than prior methods on the CMU Panoptic dataset and another existing dataset."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "Expanding the dynamic NeRF field in the direction of reconstruction/geometry is worthwhile in my opinion, as it enables more applications than novel view synthesis alone would. Correspondences are also interesting for the same reason. Thus, the problem setting is a big strength of the paper.\n\nThe method's SDF flow parametrization is novel, as far as I know. It seems generic enough to be potentially valuable for problem settings very different from the setting in the submission.\n\nThe illustrations are done well and the paper in general is written very well.\n\nThe method outperforms prior methods quantitatively. Qualitatively, it is better in at least one aspect compared to each prior method, while on par in the other aspects, and hence overall better."
            },
            "weaknesses": {
                "value": "*** Method:\n\n[Tangential Motion] The paper should more clearly state that tangential motion causes issues when converting a time-dependent SDF into scene flow.\n\n[Topology Change] The results don't show any good instances of topology change. I do not understand this claim. The best example I could find is at 0:45. Are there any other good instances? I'm also lost as to which aspect of the method helps with topology change. Please mark that more clearly.\n\n*** Experiments:\n\n[Scene Flow Evaluation] The paper doesn't show any appearance/novel view results. That's okay since it focuses on the geometry, and appearance is only used as an auxiliary to deal with the RGB input. However, I would still be curious to see video results where the appearance is taken from t=0 and propagated via the SDF flow. That would allow to visualize longer-term correspondence drift and whether tangential motion causes issues in practice. Currently, the SDF flow (and thus the derived scene flow) only cares about short-term \"geometrical\" correspondences. - This is relevant because the paper doesn't just claim that the SDF flow is a helpful parametrization but rather that it enables scene flow, i.e. correspondences. Currently, the qualitative and quantitative scene flow evaluation is only short-term, even though the method requires long-term correspondences (equation 7). The quantitative scene flow evaluation is also only a single sequence. \n\n[Integral Evaluation] How does the temporal integration scale with scene length? Presumably linearly? Is that why only 24 frames are used? How many function evaluations does the Runge-Kutta solver need for the last (24th) frame? \n\n[Weak Qualitative Geometry] Tensor4D gives detailed, noisy results and NDR is less noisy but lacks detail. The submission's results have the level of detail of Tensor4D and the low noise level of NDR. Still, none of these results are overwhelming or that impressive. (Doesn't need to be addressed in the rebuttal.)\n\n*** Paper:\n\n[Related Work] The related work section isn't that thorough. For example, the dynamic NeRF papers Fang et al. Fast dynamic radiance fields with time-aware neural voxels and Li et al. Neural 3D Video Synthesis are missing. Li et al., like NSFF, can also handle topology changes. Furthermore, shape-from-template methods aren't mentioned. Scene flow methods (e.g. Song et al. PREF: Predictability Regularized Neural Motion Fields) aren't discussed. Also, since NDR is compared to, mentioning some of the papers in the RGB-D line of work would be good, e.g. DynamicFusion, VolumeDeform, KillingFusion, OcclusionFusion. And TARS (Duggal et al. Topologically-aware deformation fields for single-view 3d reconstruction) seems relevant since it isn't restricted to blend skinning (a typo (\"blender\") in the submission). \n\n[Limitations] Please discuss limitations.\n\n*** Minor:\n\n[Input Assumptions] Since the mathematical derivation matters here, please state what assumptions go into the paragraph between equation 5 and 6, which talks about the continuity and differentiability of a time-dependent SDF. In theory, there is nothing preventing an object from appearing out of nowhere, an SDF does not inherently have any restrictions on its temporal evolution (while it does have restrictions in space, namely the Eikonal equation). In real-world cases, geometry noise (say, due to the sensor or an imperfect reconstruction) pops randomly into existence and vanishes randomly over time, which leads to discontinuities w.r.t. the time parameter. Unless point x is meant to be a Lagrangian particle rather than an Eulerian grid coordinate? Figure 2 looks Eulerian though. --- Please state the assumptions that go into that paragraph.\n\n[Wrong Argument] The argument at the end of Sec. 3.1 is that most dynamic NeRF methods have issues with topology changes. That's not the case for most dynamic NeRF methods that handle general objects since most condition the canonical model in some manner on time (e.g. HyperNeRF). An extreme case of that is Neural Scene Flow Fields. The argument only holds for D-NeRF-style methods like Nerfies or NR-NeRF. For the others, the better argument would be to say that they use density to parametrize geometry rather than SDF and hence the geometry tends to be very noisy and lack a clearly defined surface.\n\n[Labelling] Figure 2 would benefit from labelling the two ellipses with their respective timesteps (t=0 and t=6?)."
            },
            "questions": {
                "value": "I have listed my concerns in Weaknesses. The concerns under \"Minor\" don't need to be addressed in a rebuttal, except that clearing up [Input Assumptions] would help me. For all others, I'd appreciate a response. In particular, the most important concerns I have are [Topology Change] and [Scene Flow Evaluation]. If these are not addressed in a rebuttal in some form, I am against accepting the paper. I would still want the rebuttal to address the other major concerns to feel like I have a decent grasp of the submission.\n\nOverall, I lean towards reject. Even though the results are not that impressive, the paper shows nice technical contributions. If the rebuttal addresses my two main concerns well, I could increase my score to acceptable.\n\n=====\n\nPost-rebuttal justification: The rebuttal addressed all concerns very well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Reviewer_uMrz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697541391114,
        "cdate": 1697541391114,
        "tmdate": 1700570274948,
        "mdate": 1700570274948,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "USJUHH8iOO",
        "forum": "rzF0R6GOd4",
        "replyto": "rzF0R6GOd4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_hwJN"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_hwJN"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a novel representation, namely SDF Flow, for solving multi-view dynamic scene reconstruction by representing the dynamic scene as a 4-D SDF field and modelling its derivative w.r.t. time instead of the SDF value. The proposed representation has several nice properties and can be used to compute the scene flow analytically. Experimental results on public datasets have shown competitive reconstruction quality compared to state-of-the-art, and better scene flow estimation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The proposed representation is novel. I\u2019ve never seen similar representations before. The idea is clear and elegant but effective. It has the potential to open up a new paradigm for dynamic scene reconstruction and provide great insight for the community.\n\n2. The authors also revealed the relationship between SDF flow and scene flow through mathematical derivation, and have shown that the scene motion can be computed analytically by solving linear equations.\n\n3. Quantitative and qualitative experiments have shown the proposed method could achieve promising reconstruction quality. The scene flow estimation result also looks promising."
            },
            "weaknesses": {
                "value": "1. In Sec 3.3, the authors made two assumptions when deriving the computation of scene motion from SDF flow, which is justified by a 2D toy example. However, it would be great to have a more in-depth theoretical analysis and experiments to understand its convergence behaviour. Especially for assumption 2, the difference between assumed $\\Delta_S$ and real value looks quite big.\n\n2. The optimisation takes too long, which limits its practical usage."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Reviewer_hwJN"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698681711223,
        "cdate": 1698681711223,
        "tmdate": 1699636719141,
        "mdate": 1699636719141,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "Vbh5NIbjtJ",
        "forum": "rzF0R6GOd4",
        "replyto": "rzF0R6GOd4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_jfGg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_jfGg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes to predict the first-order derivative of a signed distance function (SDF) at a point, termed as SDFflow. By using SDFlow, the dynamic recontruction to recover a SDF at time t can be treated as space-time integration of the SDFflow from time t_0 to t and its starting SDF at time t_0. The authors further demonstrate how locally rigid scene-flow can be recovered least-square optimization given the locally rigid assumption for small motions. The results demonstrate the method is outperforming previous baselines using alternative shape and flow representation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The approach is technically sound. It is kinda intuitive that this representation should work. \n* From the quantitative comparison, it is clear the method is outperforming previous methods by a relatively large margin. \n* The author also provides connection of SDF flow to scene flow with a math derivation. It is helpful for readers to understand its connection if without background in optimization based motion estimation."
            },
            "weaknesses": {
                "value": "* The flow evaluation in the paper is very weak. The method only compares to NDR on 2D project error using RAFT as pesudo ground truth. This should not be hard to achieve if using a synthetic dataset, if using the rigid moving toy example being shown in the paper. \n* The flow estimation also lacks details. Solving the least-square optimization requires sample multiple points, which the author say \"we select more than 6 points\" and sometimes it still be ill-conditioned depending on the property of the sampled points, but I don't see any discussion related to this and how the results are generated. \n* The main contribution of this paper is the SDFlow and a claim that it is beating alternative representation (SDF with warping field for example). This can be much clearer if the authors can provide more ablations studies on the contrast of the two representations. Two small experiments they author can simply do is to 1) have the network predict the integral of flow in eq. (7), or 2) predict the time dependent SDF s(x, t), all with fixed hyper-parameters. The contrasts in this ablation can reflect the performance difference in predicting SDF flow. Though the authors provide comparisons to alternative papers that the other papers compound too many other terms, and I don't think I can get the insights of why SDF flow works better here."
            },
            "questions": {
                "value": "1. Though the method is outperforming previous methods by big margin in the qualitative comparisons, the difference in qualitative comparison is less clear to me from all figures in the paper. In particular compared to Tensor4D, I can see each method wins in different level of details being captured. The paper currently summarizes it as \"all of its results share similar artifacts which we conjecture are due to the tensor decomposition. Our method performs comparably to the baselines, with fewer artifacts.\" I am not sure I can capture this from the current presentation. Will love to know a concrete summary of the areas where the model increase performance best. \n2. I did not see any discussions about the limitations of existing methods or representations. As all current multi-view dynamic reconstruction work, I assume this work will face the same limitations in  large motion (which will also break the linear motion relation between scene flow and sdf flow) and number of views. But I'd like to some more technical insights of potential downside using SDF flow in some scenarios compared to other representation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6442/Reviewer_jfGg"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698704093223,
        "cdate": 1698704093223,
        "tmdate": 1700586412746,
        "mdate": 1700586412746,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "eeJODEwEZS",
        "forum": "rzF0R6GOd4",
        "replyto": "rzF0R6GOd4",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_nuCZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6442/Reviewer_nuCZ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes an algorithm for implicit surface reconstruction in dynamic scenes. The key is to bridge the SDF flow and sceneflow by assuming the two hypothesis. Also, typically this paper requires RGBD data which is to sample points nearby surface and to make this algorithm feasible. Accordingly, this paper could be understood as 4D surface tracker."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper brilliantly bridge the concept of sceneflow and the proposed SDF flow. Based on the two assumptions, one for rigidity and the other for the linearlization (?), the proposed SDF flow is differentiably convertible into the sceneflow, which enables the network to encode sceneflow from RGBD frames. As far as my understanding, this is the pioneering work. For experiment parts, comparison with NDR (Neurips 2022) is reasonable."
            },
            "weaknesses": {
                "value": "W-1. Low fidelity results\n\nDespite the novel idea, the reconstruction quality is far below my expectation. Even though the results from NDR (Neurips 2022) are also not that good enough, there are not that much dramatic change after the authors applied the proposed SDF flow.\n\nOf course, I can see the sceneflow visualization in Fig8 of the manuscript, the reconstruction quality is not really good enough. Moreover, __we cannot judge whether the quality of the sceneflow is correct or not.__ \n\nW-2. Validity on the assumption 2.\n\nCan the authors further elaborate the validity of the proposed 2nd assumption? What geometric insight reside within this hypothesis?\n\n_\"[Assumption 2] As the time period \u0394t approaches zero, the absolute SDF change |\u0394s| of a surface point x equals the distance from x to the tangent plane to the evolved surface at the corresponding point x\u2032 and the sign of \u0394s is determined by the angle between that tangent plane\u2019s normal and the scene flow (as shown in Figure 3).\"_\n\nW-3. Necessity of Sec 3.3\n\nWhile training the network, does the understanding of sec 3.3 is needed? While this paper proposes an algorithm for bridging the SDF flow and sceneflow, there are not much material or experiments that clearly demonstrate the accuracy of the sceneflow. Moreover, the proposed assumptions are not used when training the methods. Accordingly, it is quite confusing me to understand the precise pipeline of this paper."
            },
            "questions": {
                "value": "Please refer to the question above. Especially for W-3, if there are some things that I misunderstood, please let me know.\n\nOverall, I am quite positive to this paper. Depending on the rebuttal, let me change my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper does not require ethics review."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6442/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698763261799,
        "cdate": 1698763261799,
        "tmdate": 1699636718883,
        "mdate": 1699636718883,
        "license": "CC BY 4.0",
        "version": 2
    }
]