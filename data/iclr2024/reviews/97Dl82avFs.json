[
    {
        "id": "EhjzECJw4c",
        "forum": "97Dl82avFs",
        "replyto": "97Dl82avFs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_2NmJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_2NmJ"
        ],
        "content": {
            "summary": {
                "value": "The paper presents a method to generate accessibility captions for images shared on Twitter. The proposed approach combines CLIP embeddings for the images, as well as additional context that is included in the tweet\u2019s text to create an embedding that is then fed to GPT-2 to generate the accessibility description. The paper\u2019s evaluation demonstrates that the proposed approach can outperform naive and neural-based approaches like ClipCap and BLIP-2."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "First of all, I would like to applaud the authors for working on this important and timely problem. I believe that this research is very important and can have the potential to improve the lives and online experience of many people with visual impairments. Overall, I believe that this research focuses on an important problem, and there is potential for a big impact. Second, the paper collects a large-scale dataset of images and user-generated accessibility captions from Twitter; this dataset is far bigger than previous research efforts focusing on similar research problems. Third, I believe that the paper\u2019s approach is a simple, creative, and effective method to combine CLIP embeddings, tweet text, and LLMs to generate accessibility captions for images. The paper\u2019s approach is easy to understand and combines important features for generating contextual and useful accessibility descriptions for images shared on Twitter. Finally, I like that the paper evaluates the performance of the proposed approach both with quantitative metrics as well as via a user study that aims to assess how users perceive and compare accessibility descriptions generated from the proposed method and baseline approaches."
            },
            "weaknesses": {
                "value": "I have several concerns with the paper, mainly related to the lack of gold standards for accessibility captions, the lack of important and adequate methodological details, the paper\u2019s evaluation, the paper\u2019s approach to releasing data, and the paper\u2019s ethical considerations.\n\nFirst, there is a disconnection between the paper\u2019s motivation and how the paper evaluates the performance of the proposed method. I agree with the paper\u2019s motivation that the user-generated accessibility captions are of questionable quality, given that most users are unaware of best practices for generating accessibility captions. On the other hand, however, the paper collects user-generated accessibility captions and treats them as gold standards (i.e., ground truth captions). This is problematic as in the evaluation, the paper compares the generated captions from their approach and compares them with captions that are of questionable quality. Therefore, it is not clear what is the actual performance of the proposed methods. A way to potentially mitigate this issue is to apply the proposed approach to other datasets released by previous research that include gold-standard captions (i.e., captions that adhere to the best practices for generating accessibility descriptions for images).\n\nSecond, I am puzzled about how the BLEU@4 score is calculated in the evaluation. To the best of my knowledge, the BLEU score ranges from 0 to 1 and aims to assess the precision of the n-grams included in the generated text compared to the ground truth. In the paper\u2019s evaluation, the paper mentions that the proposed approach has a BLEU@4 score of around 1.8. I suggest to the authors to clarify how they calculated the BLEU scores (e.g., if they used a modified version) and better describe how we can interpret these BLEU@4 values.\n\nThird, the paper lacks important and adequate details on the paper\u2019s methodology. Particularly, the paper refers to several appendices so that the reader can get more information, however, there are no appendices in the manuscript. This hurts the readability of the paper and does not allow us to assess the quality and robustness of the presented results in the paper. I suggest including the appendices so that we can understand how the paper conducted various steps of the research. In particular, I would have liked to read more on how the paper conducted the user study, how they recruited users, what is their background and expertise with regards to the best practices for generating accessibility descriptions, etc. All these details are paramount for understanding the quality of the presented research.\n\nFourth, I have some concerns about the paper\u2019s approach to releasing the dataset. Given the recent changes to Twitter\u2019s API, it became extremely hard to rehydrate tweets based on their IDs. So by simply releasing the Twitter IDs and the media URLs, interested researchers will not be able to reproduce the paper\u2019s results and further use this dataset for further studying this problem. I suggest to the authors to consider releasing more attributes from the dataset (specifically the tweet\u2019s text) so that interested researchers can reproduce the paper\u2019s results without relying on the closed and expensive new Twitter APIs.\n\nFifth, the paper does not properly explain how the qualitative assessment is done (in Section 6.4), which does not allow the reader to understand if it\u2019s done in a systematic way or how representative/generalizable the insights are. I suggest to the authors to include more details on how the samples for the quantitative analysis are selected and, more importantly, how the qualitative assessment is undertaken (e.g., are the people experts in the domain of accessibility description generation, are they aware of the best practices, etc.)\n\nFinally, the paper does not discuss the ethical considerations when conducting this research. This is important as the paper conducts a user study and shows participants\u2019 images shared on Twitter. For instance, did the paper ensure that there are no harmful images in the dataset and that no participants were exposed to harmful information?"
            },
            "questions": {
                "value": "1. What is the rationale for using user-generated captions as gold standards and do you have an idea how this affects the presented results?\n2. How is the BLEU@4 score calculated and did you use a modified version of the metric? \n3. How is the user-study conducted and what are the background/expertise of the recruited participants? Also, have you obtained an IRB approval before conducting the user study? How did you ensure that participants were not exposed to harmful information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper lacks a discussion of their ethical considerations. It's unclear whether the research participants were exposed to harmful images shared on Twitter."
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Reviewer_2NmJ",
                    "ICLR.cc/2024/Conference/Submission6459/Senior_Area_Chairs"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697618316928,
        "cdate": 1697618316928,
        "tmdate": 1700642647716,
        "mdate": 1700642647716,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RcBq3Piuzn",
        "forum": "97Dl82avFs",
        "replyto": "97Dl82avFs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_1eBv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_1eBv"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a method for generating \"alt-text\" for social media images. In this paper, \"alt-text\" is explained as a more detailed description and context-specific than a generic image caption.\nThe proposed method takes an image and the social media text that accompanies the image and outputs the alt-text of the image. The input image is encoded by CLIP and combined with the encoded text. The combined information is then input to GPT-2 to generate the alt-text.\nThe author has collected tweets and their corresponding images with alt-text to create a dataset for alt-text generation research. The evaluation shows the results of using this dataset to compare the proposed method with several baseline methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The author points out that in social media, images are often posted in addition to textual information, but there is little information describing the images, and in such situations, information about the images is not conveyed by text-to-speech software for the visually impaired, for example. I agree with this point and understand its importance as a study.\n\nAs for the proposed method, its basic structure consists of encoding images using CLIP and generating text using GPT-2. This structure itself is not unique, as it is a concept that has been used in existing research. I believe that the originality lies in the extended part of the basic configuration, where not only the image but also the text to which the image is attached is input.\n\nThe fact that an original data set was constructed for this study is commendable. It is also commendable that in the evaluation using this data set, comparisons with various methods were made to show the characteristics of this task."
            },
            "weaknesses": {
                "value": "As mentioned in the \"Strengths\" section, the focus on \"alt-text\" is highly evaluated, but there is room for improvement in that \"alt-text\" is not clearly defined in the paper.\n\nIn the evaluation dataset, the \"alt-text\" entered by twitter users is used as the correct answer, but it is written as \"alt-text captions on Twitter are written by untrained users, they can be noisy, inconsistent in form and specificity, and occasionally do not even describe the image contents\" in the paper, and it seems that the authors are discussing the generation method without knowing what \"alt-text\" is.\n\nAlthough the difficulty is understandable, I think that the discussion should start with clarifying what \"alt-text\" is, and then discussing the generation method."
            },
            "questions": {
                "value": "I wondered if the \"alt-text\" would be written differently for different people, even for the same image. Is it possible for the proposed method to learn well in such a case?\n\nIn addition, although the paper uses crowdsourcing for evaluation, I think it is possible that the evaluation by the viewer of the \"akt-text\" may also differ from person to person. How did the evaluation go this time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Reviewer_1eBv"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698599183423,
        "cdate": 1698599183423,
        "tmdate": 1699636722416,
        "mdate": 1699636722416,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "qMKs5OzNQs",
        "forum": "97Dl82avFs",
        "replyto": "97Dl82avFs",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_xHBk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission6459/Reviewer_xHBk"
        ],
        "content": {
            "summary": {
                "value": "This paper studies on an important problem: how to generate Alt-text to help improve the accessibility of social media images. More specifically, the authors collected and cleaned a large dataset of images incorporated with the alt-text labeled by users. With this large dataset, they are able to evaluate the proposed method and some baselines. Overall the proposed solution is not novel for the Machine Learning Community. But the research problem is interesting and meaningful. Also, the collected dataset is important for the community of HCI and Social Media Analysis."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. The collected dataset is important and useful. The data preprocess ensure its usability.\n\n2. The research problem raised in this paper is important."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed method is really limited excpet the tweet-text-based reranking.\n\n2. The experiment is somewhat not extensive. For example, from Table 1, it seems that the tweet-based reranking is the most important component. But the authors did not tried to incorporate the reranking with the baselines, which is not fair."
            },
            "questions": {
                "value": "1. Will the tweet-based reranking improve the baselines like BLIP-2 and ClipCap?\n\n2. Is there some other reranking strategies that you tried? Such as comparing the Clip-based similarity in the representation space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6459/Reviewer_xHBk"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission6459/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698609747939,
        "cdate": 1698609747939,
        "tmdate": 1700546571906,
        "mdate": 1700546571906,
        "license": "CC BY 4.0",
        "version": 2
    }
]