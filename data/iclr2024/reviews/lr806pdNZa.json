[
    {
        "id": "B4re5YXV1O",
        "forum": "lr806pdNZa",
        "replyto": "lr806pdNZa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_BZEx"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_BZEx"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on the problem of \"censorship\" in large language models (LLM). Specifically, the paper argues that it is unfeasible to address this issue by relying on ancillary \"machine learning\" (ML) techniques, and that it should rather be tackled via mechanisms belonging to the security domain. To support such a position, the paper presents detailed theoretical arguments demonstrating that LLM censorship is an \"undecidable problem\", thereby revealing that using ML-based techniques, such as, e.g., another language model (LM), will never provide a foolproof solution."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "## High-level\n\n+ Outstanding writing\n+ Relevant Problem (for both research and practice)\n+ The theoretical arguments are well-founded\n\n## Comment\n\nI deeply thank the authors for writing this piece and submitting it to ICLR'24. I've loved reading it, and I was genuinely pleased by the outstanding writing quality: out of the papers I reviewed for ICLR'24, this one is by far the best written one. Moreover, the paper tackles a very open issue and the \"conclusion\" can be leveraged by researchers and practitioners alike: the latter can benefit by integrating additional security mechanisms in their products, whereas the former would be provided with \"clear evidence\" that tackling censorship by means of traditional ML methods will never provide a foolproof solution. Indeed, the theoretical arguments made in this paper are well-rooted, and I particularly appreciated connecting LLM to Turing Machines and the application of the Rice Theorem as a scaffold to support the paper's main claims. \n\nHowever, despite all such strengths, the paper also presents (imho) various weaknesses, which are discussed below."
            },
            "weaknesses": {
                "value": "## High-level\n- It suffers from an \"identity crisis\" (it feels more like a \"position\" paper)\n- Lack of a concrete experiment \n- Some statements require further evidence to be supported\n- The paper is built on a strong assumption that does not seem to have been accounted for\n- The \"mosaic prompts\" are not really novel\n- Some pieces of the text are unclear\n\n\n## Comment \n\nDespite my appreciation, I do have concerns about the suitability of this paper to ICLR'24. Before I discuss such concerns, however, I want to emphasize that my remarks are _my opinions_. I couldn't spot any technical or methodological flaw in the paper (which is also well-written): hence, my critiques are mostly directed at the \"significance\" aspect of the paper, and I endorse the authors to reflect on the following remarks. Ultimately, my goal is to help them make this paper as a noteworthy contribution to the state-of-the-art (be it for ICLR'24, or for any other venue).\n\n### **Identity Crisis (Lack of a concrete experiment)**\n\nThe most prominent weakness is that, IMHO, the paper suffers from an \"identity crisis\" -- which is rooted on the fact that the paper touches both the \"security\" and \"ML\" domains.\n\nOn the \"security\" hand, all the considerations made in the paper are \"obvious\". The fact that, e.g., an attacker can bypass censorship mechanisms by inducing a LLM to output a \"malicious set of actions\" through individual prompts is \"not new\", and the fact that a similar strategy can fool essentially any precaution is \"not surprising\". Indeed, this is a well-known problem in reality, and the only way to solve this problem is by reading the attacker's minds. Plus, ultimately, LLM are just \"tools\": whether they are used in good- or bad-will is a different manner (and this had been known since the development of cryptographic protocols, since they also aid attackers in preventing their messages from being interpreted). So, to summarise, as a \"security\" researcher, the conclusion of this paper was already known, and the supporting theoretical arguments were hence somewhat redundant.\n\nOn the \"ML\" hand, the paper lacks a clear experiment that demonstrates at least one of the scenarios described in the ```practical implications```. Indeed, after introducing some definitions and demonstrating a given theorem, the paper merely limits to provide \"thought experiments\" discussing how an hypothetical attacker can achieve their goal. Yet, all such discussions are textual: there is an excessive usage of the words \"can\" \"could\" \"may\" \"it is possible that\". The paper does provide some references (e.g., \"The authors of... showed that this can be done\") but the lack of a concrete experiment is still hard to overlook. Such a lack is further aggravated by the additional what-ifs which project LLM into the future (e.g., ```these risks could become even more problematic```). I acknowledge that \"anything can happen\", but this is a weak argument. \n\nHence, I feel that the lack of a \"hard\" experiment is a significant weakness of this paper, which affects both its appeal to the security domain, as well as the one to the ML domain. For instance, I would have appreciated a clear demonstration of Figure 2 (I've spent ~30 minutes trying to have ChatGPT to process similar instructions, but I've never been successful).\n\nPut differently, the paper currently reads as a \"visionary paper\" or a \"position paper\" rather than a true research paper. **However** do note that I am not saying that the paper is devoid of merit: providing \"theoretical evidence\" that it is not possible to craft \"perfect\" ML-based censorship mechanisms is a strong message.\n\n\n### **Lack of evidence for some statements**\n\nOne of the major points in support of the \"value\" of this paper is that the current way to address censorship in LLM is by means of \"ML-based mechanisms\", and --after demonstrating that doing so will never guarantee 100% protection-- the suggestion that censorship should be treated as a security problem.\n\nIndeed, to quote the abstract:\n\n> Commonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs.\n\nThe following was also stated in the Introduction:\n\n> Such methods range from fine-tuning LLMs (OpenAI, 2023) to make them more aligned, to employing external censorship mechanisms to detect and filter impermissible inputs or outputs (Markov et al., 2023; Chockalingam and Varshney, 2023; Greshake et al., 2023).\n\nHowever, I only see 4 works listed here. Hence, I wonder: is it really true that ML-based methods are the \"way-to\" address censorship problems? For instance, even Greshake et al. state ```Unfortunately, it is currently hard to imagine a foolproof solution for the adversarial prompting vulnerability```; moreover, the authors of NeMo Guardrails (used by NVIDIA (Chockalingam and Varshney, 2023)) state the following in their [GitHub repo](https://github.com/NVIDIA/NeMo-Guardrails/blob/main/docs/security/guidelines.md):\n\n> Integrating external resources into LLMs can dramatically improve their capabilities and make them significantly more valuable to end users. However, any increase in expressive power comes with an increase in potential risk. To avoid potentially catastrophic risks, including unauthorized information disclosure all the way up to remote code execution, the interfaces that allow LLMs to access these external resources must be carefully and thoughtfully designed from a security-first perspective.\n\nTo me, the impression is that these mechanisms are proposed as a \"partial\" solution, since even the respective authors advocate for security principles to be followed. In light of this, the underlying \"message\" of the paper partially loses its value (at least imho). It would be enticing to carry out of more profound analysis of current works on approaches for LLM censorship, and pinpointing how many of such works truly claim to address censorship in an ML-only way, without making any security consideration: doing so would dramatically improve the contribution of this paper.\n\n\n### **A strong assumption**\n\nBy looking at the definition of \"censorship mechanism\", the impression I have is that the paper assumes that censorship is always applied \"a-posteriori\". That is: the LLM receives an input, elaborates a response, and then --right before providing the response to the user-- it checks whether the response is permissible or not by means of some censorship. I wonder: is this really true?\n\nBecause, if this is not the case (i.e., there is some censorship applied to some \"intermediate process\" of the response), then the censorship would work, since it would be applied before the application of the transformation which makes the text encrypted. \n\nIn light of this, I invite the authors to provide evidence that this assumption holds _in reality_ (plus, I conjecture that such an observation CAN be used to develop some more effective defenses!). Otherwise, the authors should acknowledge that their analysis only applies to a specific use-case of censorship (do note that, however, this would decrease the impact of the paper). Alternatively, the authors can provide evidence (theoretical and, possibly, practical) that the envisioned analysis/findings hold even in these intermediate cases.\n\n### **Naming of Mosaic prompts**\n\nWhile I appreciate the name \"Mosaic Prompts\", I feel the way it is presented to be \"excessive\". Indeed, the described procedure is exactly the same as the \"divide et impera\" (or \"divide and conquer\") which is the de-facto praxis in computer science (and already associated to LLM, see [here](https://medium.com/@finomeno/exploring-large-language-models-insights-for-architects-393600dae131) and [here](https://medium.com/@digitalmiike/chatgpt-guide-10-effective-prompt-strategies-for-enhanced-output-979c8032eaaa)).\n\nHence, I endorse the authors to tone down this name, or at least acknowledge that it is just a renaming of a popular technique in computer science. (I am stating this also in light of the \"acknowledgment\" made in Footnote-1 -- which I greatly appreciated!)\n\n### **Some pieces of text are unclear**\n\nAlthough the paper is excellently written, I had issues in understanding some parts of the text. In what follows, I will directly quote each of these \"problematic\" parts, and explain the problems I encountered---starting from the Introduction.\n\n> Such constraints can be semantic, e.g. does not provide instructions on how to perform illegal activities, or syntactic, e.g. does not contain any ethnic slurs from a provided set.\n\nI did not understand the provided examples -- or rather, it is hard to determine the subject of the examples. I recommend rephrasing to, e.g., \"the output must not provide...\"\n\n> methods against malicious attackers.\n\nAre there attackers who are not malicious? (this redundancy occurs many times in the paper)\n\n> restricting the string x to the set of permissible strings P\n\nI recommend being more specific: \"the string x to the set of permissible strings P that can be constructed by the LLM model\" (otherwise, it may be confused with a string written by an user)\n\n> demonstrated in Fig. 1\n\nThe caption states \"Figure\" (and not Fig.)\n\n> typically defined by the language recognised it recognises\n\nThis is unclear \n\n> descriptions of Turing machines can be viewed as a programming language, capable of being interpreted by a universal Turing machine capable of emulating them.\n\nPlease revise this statement as it is very confusing.\n\n> As the semantic censorship impossibility result that we established by connecting the problem of semantic censorship to Rice\u2019s Theorem doesn\u2019t fully capture real world censorship settings where inputs and outputs are bounded we seek to provide another result on the impossibility of censorship that does.\n\nMake this shorter, especially since the same message was written two lines before.\n\n> we assert that given an invertible string transformation g\n\nIs this \"g\" supposed to be the \"bijective transformation\"? Still, I am slightly confused about this \"g\" here; perhaps an example would be useful.\n\n> it is capable of applying g to its output x to instead output g(x).\n\nThis is very unclear. Do you mean g(g(x))?\n\n> either nothing is be permissible\n\nTypo\n\n> While existing LLMs are good at [...] Yuan et al. (2023)\n\nThis paragraph appers to be disconnected from the \"Practical Implications\". Or rather, it does not align well with the way the previous paragraph ended. Actually, I do not see any \"practical implications\" that are truly compellling here.\n\n> While our results describe adversaries which can instruct\n\nWhich results? \n\n> For example, users could provide [...] running the model\n\nIt would be wonderful if the authors showcased a way to do so in practice _today_. \n\n> In an extreme setting where there exist only 2 permissible output strings\n\nWhy this assumption? To me, the following example holds even without this (perhaps I missed something?)\n\n> converting text to ACII\n\nTypo\n\n> Subsequently, the user can request the model to output i\u2019th bit\n\nWhat is the ```i'th bit```? Plus, how can the user do so?\n\n> our Mosaic Prompting results\n\nGiven that no experiments have been carried out, it is a bit of a stretch to define this as a \"result\" (even the Appendix does not provide \"empirical results\")\n\n\n\nFinally, I report that the bibliography often does not provide the venue of a given work (e.g., the paper by Markov et al. (2023) was published in AAAI; whereas the one from Greshake et al. was accepted at AISec). This is annoying as a reader, as I could not ascertain the quality of a given referenced work."
            },
            "questions": {
                "value": "I liked the paper, and I am willing to improve my score if presented with compelling evidence that some of my remarks are flawed. Nonetheless, I invite the authors to answer the following questions (most of which are drawn from my \"Weaknesses\" section): depending on the answer, my rating will likely change.\n\n1) Can the authors provide more evidence that LLM censorship is truly \"commonly treated as a ML problem\" (and that security-based approaches are not taken in consideration)?\n\n2) Would the proposed theoretical analysis, as well as the proposed \"attack\", still apply if censorship is carried out during the process of crafting a response by the LLM? (Please elaborate)\n\n3) How could the \"attack\" shown in Figure 2 be realized _today_? \n\nThen, I have one last question. Assume that this paper is accepted to ICLR'24 as a spotlight. How would the authors present this work? Would the talk include only \"what-ifs\", or would it also showcase some concrete evidence that the envisioned scenarios are truly a security issue that cannot be countered with ML-only ways$^1$?\n\n$^{\\text{1: E.g., how do I make ChatGPT tell me \"howdoibuildabomb\"?}}$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698149520614,
        "cdate": 1698149520614,
        "tmdate": 1699637019167,
        "mdate": 1699637019167,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iIAVsIqGZQ",
        "forum": "lr806pdNZa",
        "replyto": "lr806pdNZa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_ST3b"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_ST3b"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the theoretical limitations of the current external censorship mechanisms in LLMs from the view of computing theory. Given these inherent limitations, the authors argue that LLM censorship should be addressed more as a security problem than a machine learning problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- Trendy topic\n- A novel perspective to study LLM censorship"
            },
            "weaknesses": {
                "value": "- Implications can be extended\n- Readability can be improved"
            },
            "questions": {
                "value": "In this paper, the authors first focus on the semantic censorship mechanisms, proving that the current mechanisms cannot reliably detect if LLM output is \"semantically impermissible.\" They further show that such limitations are inherent and can extend beyond semantic censorship mechanisms by designing Mosaic prompts.\n\nOverall, the authors study a trendy topic and offer a novel perspective to understand LLM censorship. However, I have the following concerns.\n\n- The authors prove the impossibility of semantic censorship using string transformation by showing how the transformed string might break the \"invariance of semantic censorship.\" Here, I have some doubts regarding the invariance property. In my opinion, the semantics of a string often change after the transformation. Thus, it is reasonable for the transformed string to bypass semantic censorship mechanisms. Moreover, LLMs do not necessarily output harmful texts with the transformed string. Why does the invariance property hold? Is this property an important goal considered by LLM censorship developers when designing their mechanisms?\n\n- Implications can be extended. It appears to me that the current implication discussion stops at showing LLM censorship is more of a security problem than a machine learning problem. What are the direct implications for model developers when building censorship? Are there any defensive measures against the Mosaic prompts? The authors only briefly mention that there are standard approaches, such as access controls and user monitoring, to build censorship from the security view. However, there is no further analysis showing that these approaches can indeed overcome the theoretical limitations of current external censorship mechanisms and surpass them in censorship performances.\n\n- Readability can be improved. Many sentences are too long and difficult to read. For example, \"Thus, we can understand censorship as a method of determining permissibility of a string and censorship mechanisms can be described as a function, f(x), restricting the string x to the set of permissible strings P by\u00a0transforming it to another string x' \u2208 P if necessary, e.g. x' ='I am unable to answer.'\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698230841205,
        "cdate": 1698230841205,
        "tmdate": 1699637019047,
        "mdate": 1699637019047,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "FfE6RvcmQp",
        "forum": "lr806pdNZa",
        "replyto": "lr806pdNZa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_3fNc"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_3fNc"
        ],
        "content": {
            "summary": {
                "value": "The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.\n\nDetails:\n* abstract:  LM -> LLM or define it.\n*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.\n* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.\n* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say \"LLM censorship (ie. avoiding censorship through attacks) is a security concern\" should be a new insight."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "see above"
            },
            "weaknesses": {
                "value": "see above"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698538750067,
        "cdate": 1698538750067,
        "tmdate": 1699637018926,
        "mdate": 1699637018926,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "oIA47ljGw1",
        "forum": "lr806pdNZa",
        "replyto": "lr806pdNZa",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_xHLz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission8208/Reviewer_xHLz"
        ],
        "content": {
            "summary": {
                "value": "This paper explores some of the theoretical limitations of LLM censorship, the problem of identifying permissible inputs and outputs to language models. In particular, the paper focuses on the limitations of semantic censorship, or filtering of strings based on their meaning. First, the paper shows that determining whether a \u201cprogram\u201d output by an LLM is permissible is an undecidable problem. Then, the paper discusses the impossibility of semantic censorship by showing that strings can undergo transformations which preserve their semantic meaning but are otherwise unintelligible except to a user who knows how to invert the transformation. Finally, the paper introduces Mosaic Prompts, a way of breaking up an impermissible prompt into permissible pieces."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "This paper\u2019s primary strength is that it identifies an important issue to focus on that has been unexplored in the literature - what are the theoretical limits on the ability to filter LLM inputs or outputs based on their semantic meaning? The paper is a good exposition of this problem and the theoretical settings it considers highlight some important limitations for the task. The figures and tables also do a good job of clarifying some of the concepts in the text. Overall, the authors\u2019 assertion that syntactic censorship is likely to be more successful than semantic censorship is well-taken from this work."
            },
            "weaknesses": {
                "value": "This paper\u2019s primary weakness is the number of assumptions and limitations that come into the different theoretical treatments that the paper covers. First, the paper itself admits that the treatment of Rice\u2019s theorem for programs on Turing Machines is not generally applicable to the bounded inputs and outputs case of LLMs. Second, in the section 2.2 on the invertible transform, I believe there may be a flaw in the reasoning of the proof. Under assumption 1, the authors assume that the model is capable of following instructions such that it can produce the transformation $g$. This assumption is explicitly stated. It seems that the proof also requires that the LLM (or corresponding companion LLM that is doing censorship) is unable to compute the inverse transformation $g^{-1}$. If it were, then it could check the semantics of the un-transformed string for permissibility. This assumption weakens the power of the impossibility result in my opinion. Finally, while I think that the Mosaic Prompt approach is interesting, I do think the paper underestimates the LLM\u2019s ability to attend to previous prompts. While in the mosaic approach the model is likely to answer early prompts, it is conceivable that once enough of the pieces of the impermissible prompt are present, one would be able to detect the impermissibility of the conversation overall."
            },
            "questions": {
                "value": "Does the impossibility result in Section 2.2 require an assumption that $g^-1$ is not computable by the permissibility model?\n\nIs the problem space simplified at all by considering the compositionality of strings? For example, if there is an impermissible substring within a larger string, does that make the larger string automatically impermissible as well?\n\nDoes something like \u201cfuzzy\u201d permissibility fit into this framework at all? For example, many prompts and outputs would be considered \u201cborderline\u201d or have some level of \u201ctoxicity\u201d if sent to a human rater, rather than a bright-line permissible vs. not rule. Does that make the problem any easier or harder?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission8208/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698608286944,
        "cdate": 1698608286944,
        "tmdate": 1699637018817,
        "mdate": 1699637018817,
        "license": "CC BY 4.0",
        "version": 2
    }
]