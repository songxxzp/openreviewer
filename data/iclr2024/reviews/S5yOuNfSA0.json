[
    {
        "id": "vZm0tdcxd2",
        "forum": "S5yOuNfSA0",
        "replyto": "S5yOuNfSA0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_nFDE"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_nFDE"
        ],
        "content": {
            "summary": {
                "value": "This paper theoretically examines transferable representation learning of CLIP. The analysis reveals that with a near-optimal network trained on the data, features from different modalities align, allowing for zero-shot learning when appropriate prompts are used. The paper also demonstrates that contrastive learning with sparse features can lead to unexpected positive pairs, emphasizing the need for careful consideration. Building on these general theoretical findings, the authors provide deeper insights into specific cases, illustrating how multi-modal learning aligns different features and how CLIP's learned features outperform those obtained through naive square loss. To validate their theoretical predictions, the authors conduct experiments on real data. Additionally, inspired by their theoretical findings, they propose a novel regularization technique for CLIP, effectively improving zero-shot performance across various tasks, as confirmed by empirical results."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The paper is well-written and flows smoothly, making it relatively easy for readers to understand. \n2. This article focuses on what's behind the explosive effectiveness of CLIP, and the paper attempts to delve into the principles underlying CLIP, demonstrating a certain level of originality and innovation."
            },
            "weaknesses": {
                "value": "1. More relevant experimental results are expected, such as results from a wider range of downstream tasks.\n2. The article exclusively analyzes and experiments with CLIP, without thoroughly exploring the applicability of this new methods to other relevant contrastive learning approaches."
            },
            "questions": {
                "value": "Can one simply replace the CLIP component in all CLIP-related work with CLIP+Reg to achieve a better performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698723724058,
        "cdate": 1698723724058,
        "tmdate": 1699636638150,
        "mdate": 1699636638150,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "pwVO1xjP6U",
        "forum": "S5yOuNfSA0",
        "replyto": "S5yOuNfSA0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_CWhP"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_CWhP"
        ],
        "content": {
            "summary": {
                "value": "This paper offers theoretical analysis of the underlying principles of CLIP, shedding light on why CLIP exhibits robust transferability. Additionally, the paper introduces a novel regularization technique designed to enhance the performance of CLIP."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-written and novel. This paper offers a robust analysis, including mathematical proofs, of CLIP. These contributions greatly contribute to our understanding of CLIP."
            },
            "weaknesses": {
                "value": "1. The paper's primary focus appears to be on CLIP's zero-shot transferability. While this is undoubtedly a significant aspect, it's worth considering that CLIP's robust zero-shot performance results from a strong semantic space based on extensive vision-semantic data. Therefore, an exploration of the visual-semantic alignment aspect could be an intriguing avenue for further investigation.\n\n2. In introduction section, the author cites \"blue sky\" and \"white cloud\" as examples of unique features. However, these instances might be seen as special cases.  As CLIP is based on a large amount of vision-semantic data, it's possible that the missing elements could appear in various other captions. Therefore, I question the significance of this problem. To address this concern, the author may need to conduct overall statistics on the data. Furthermore, the term 'unique features' could benefit from a more precise definition or explanation.\n\n3.  Some notations and definitions in the paper can be challenging to follow. For instance, the terms 'one-to-one mapping' or 'one-to-one matching' could benefit from clearer explanations for readers.\n\n4. Expanding the range of experiments to include various downstream tasks, rather than solely focusing on zero-shot and Linear probing, would provide a more comprehensive assessment of the paper's proposed methods and their practical applications."
            },
            "questions": {
                "value": "See `Weakness' above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698761045763,
        "cdate": 1698761045763,
        "tmdate": 1699636638043,
        "mdate": 1699636638043,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "iegnthFBIe",
        "forum": "S5yOuNfSA0",
        "replyto": "S5yOuNfSA0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on providing theoretical support for the CLIP training and its zero-shot transferability. The main claim is that the contrastive learning objective in CLIP may not cover all the positive pairs, e.g., some features in an image may not be present in its corresponding captions. In section 3, the authors show that the empirical loss converges to the true loss when the number of batches is large enough. In section 4, they show that the learned similarity score f_hat between negative pairs is smaller than the score between positive pairs given that there exists a score function f* such that this relation holds. Based on such assumption, in section 5, they conclude that a trained CLIP model can achieve small top-r error and this generalizes to different distributions as the distribution shift is bounded. Based on the prior assumption and derivations, they have three claims: 1) Margin depends on the temperature tau, 2) We should only regularize positive pairs instead of both positive and negative pairs, 3) With sufficiently small tau, we can find a f_hat with large margin. They test these claims with experiments on CC3M."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper provides theoretical bounds on CLIP training and its zero-shot transferability."
            },
            "weaknesses": {
                "value": "1. Contrastive learning has been widely studied in the community with several variants of NCE loss, with different ways to regularize positive and negative pairs to improve the margins (e.g., [a]). The behavior of temperature in contrastive learning was also studied (e.g., [b]), and so was regularization (e.g., [c]). It is not surprising that adding the regularization of the distance between positive pairs can improve the performance. Also, how does the proposed solution compare to those methods?  \n\n2. The authors propose only to regularize the distance between positive pairs, but there is no ablation comparison to the variants that regularize both or negative pairs only.\n\n3. The introduction states that the contrastive learning objective in CLIP may not cover all the positive pairs, which makes sense. However, \n it is unclear how the proposed solution addresses this issue.\n\n4. The experiments are conducted on a relatively small dataset compared to CLIP. The training behavior and the generalization ability of the representation may be different.  \n\n5. The derivations make sense but they are under several assumptions.\n\n[a] Unified Contrastive Learning in Image-Text-Label Space, CVPR'22  \n[b] Understanding the Behaviour of Contrastive Loss, CVPR'21  \n[c] Large-Margin Contrastive Learning with Distance Polarization Regularizer, ICML\u201921"
            },
            "questions": {
                "value": "My questions are listed in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5970/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5970/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790579405,
        "cdate": 1698790579405,
        "tmdate": 1699636637932,
        "mdate": 1699636637932,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "rJpoz4lihX",
        "forum": "S5yOuNfSA0",
        "replyto": "S5yOuNfSA0",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_P1Cv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission5970/Reviewer_P1Cv"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates transferable representation learning underlying CLIP and demonstrates how features from different modalities can be aligned. Then a new CLIP-type method is proposed, the effectiveness of the proposed method is proved through experiments on multiple benchmark datasets."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This paper is well-written and easy to follow. \n- This paper theoretically examines the transferable representation learning in CLIP. The theory seems sound.\n- This paper proposes an easy regularization technique for CLIP that can effectively improve its zero-shot performance."
            },
            "weaknesses": {
                "value": "My major concerns lie in the empirical studies.\n- The current pre-training experiments are all based on the CC3M, which is much smaller than the full 400M dataset used by the CLIP. It is unclear whether the proposed regularization technique holds when extended to a larger dataset. It is recommended to conduct experiments on datasets with different sizes.\n- In Table 1, why incorporating the regularization term into the contrastive objective is harmful to DTD? \n- It seems that the results of CyCLIP in Table 1 and Table 2 are inconsistent with the original CyCLIP paper.\n- Can the proposed regularization technology work in CyCLIP?\n- Is the method equally effective for other downstream tasks such as retrieval?"
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698797845669,
        "cdate": 1698797845669,
        "tmdate": 1699636637831,
        "mdate": 1699636637831,
        "license": "CC BY 4.0",
        "version": 2
    }
]