[
    {
        "id": "ELqdith5vk",
        "forum": "9nXgWT12tb",
        "replyto": "9nXgWT12tb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_rWhs"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_rWhs"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a novel \"correlated attention mechanism\" specifically designed to address the challenges presented by cross-correlations in Multivariate Time Series (MTS). Recognizing a gap in existing Transformer-based models which do not adequately capture these cross-correlations, this research seeks to bridge this gap by offering an advanced mechanism that not only grasps instantaneous cross-correlations but also encompasses lagged cross-correlations and auto-correlation.\n\nThe correlated attention mechanism is adeptly crafted to compute cross-covariance matrices between different lag values for queries and keys. A significant feature of this mechanism is its ability to be seamlessly integrated into popular Transformer models, thereby enhancing their efficiency.\n\nIn practical applications, such as production planning, the mechanism demonstrates its utility by effectively addressing the lagged interval between variations like demand and production rates. The research further strengthens its case by adapting the original multi-head attention to accommodate both temporal attentions from existing models and the newly proposed correlated attentions. This design ensures that the base Transformer's embedded layer is directly enhanced with cross-correlation information during representation learning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "**Strengths**\n\nThe authors meticulously focus on harnessing transformer-based architectures for addressing the forecasting problems associated with multivariate time series (MTS). After a thorough investigation of the prevailing methods in the industry, they present a pivotal question:\n\n*How can we seamlessly elevate the broad class of existing and future Transformer-based architectures to also capture feature-wise dependencies? Can modelling feature-wise dependencies improve Transformers\u2019 performance on non-predictive tasks?*\n\nTo address this, the authors:\n\n1. Delve deep into the mechanisms of Self-attention and De-stationary Attention. They argue that the Transformer models, as currently conceived, cannot explicitly utilize information at the feature dimension. While there have been efforts to tackle these concerns, the extant methodologies are either too specialized or do not adequately account for the intricacies inherent to MTS data.\n\n2. Introduce the Correlated Attention Block (CAB) as a remedy to the aforementioned challenges. They employ normalization to stabilize the time series and leverage lagged cross-correlation filtering to manage lag-related issues. Furthermore, score aggregation is utilized to consolidate scores from different lagged time points, culminating in the final output.\n\n3. Propose rapid computation techniques for CAB, alongside strategies for its integration into multi-head attention mechanisms.\n\n4. The paper excels in its mathematical exposition \u2013 the formulas are presented in a standardized manner, making them easy to follow. Additionally, the experiments are comprehensive and well-executed.\n\nIn terms of originality, quality, clarity, and significance, this work shines by offering both a novel perspective and tangible solutions to the MTS forecasting problems using transformer-based architectures. Combining existing ideas with innovative approaches, the paper removes limitations observed in previous results, making it a notable contribution to the domain."
            },
            "weaknesses": {
                "value": "While the paper has several strengths, there are also areas where it could be improved:\n\n1. Lack of evaluation on prediction tasks: For prediction tasks, such as the MLTSF dataset, the paper does not provide an evaluation of the impact of the Correlated Attention Block (CAB) or compare it with other models that utilize inter-variable correlations. Including such evaluations and comparisons would provide a more comprehensive understanding of the effectiveness of CAB in prediction tasks.\n\n2. Insufficient description of hyperparameter settings: The paper lacks detailed explanations of the hyperparameter settings. For example, in Equation (5), how the initial value of lambda (\\lambda) is chosen and how the values of k and c are determined are not clearly stated. Providing more guidance on these hyperparameters would help readers understand the choices made and improve reproducibility.\n\n3. Non-compliance with ICLR submission requirements: The paper does not follow the submission requirements of ICLR by placing the appendix together with the main text. It would be better to separate the appendix from the main text, following the formatting guidelines specified by the conference.\n\nAddressing these areas of improvement would enhance the clarity, reproducibility, and comprehensiveness of the paper, providing readers with a better understanding of the proposed method and its performance in prediction tasks."
            },
            "questions": {
                "value": "1. How were the hyperparameters determined in the experiments? Specifically, can you provide more details on the selection of hyperparameters such as the initial value of lambda (\\lambda) and the values of k and c? Understanding the rationale behind these choices would help in reproducing the results and provide insights into the sensitivity of the proposed method to hyperparameter settings.\n\n2. Can you provide a more detailed evaluation of the Correlated Attention Block (CAB) on prediction tasks, such as the MLTSF dataset? It would be interesting to see how CAB performs compared to other models that utilize inter-variable correlations in prediction tasks. This analysis would shed light on the effectiveness of CAB in different scenarios and provide a better understanding of its potential advantages.\n\n3. In Table 2 and Table 3, it is observed that in some cases, the performance of CAB+Transformer is not as good as Nonstationary, and in some cases, Nonstationary+CAB even leads to worse results. Can you provide an explanation for these observations? What factors contribute to the varying performance of the proposed method in different settings? Understanding the limitations and potential trade-offs of the proposed method would provide valuable insights for future improvements."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697959306047,
        "cdate": 1697959306047,
        "tmdate": 1699636270856,
        "mdate": 1699636270856,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "tlNhAzCYyZ",
        "forum": "9nXgWT12tb",
        "replyto": "9nXgWT12tb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_KQRi"
        ],
        "content": {
            "summary": {
                "value": "The paper focuses on how to learn the feature-wise correlation when applying a transformer in the multivariate time series for various tasks.  The proposed correlated attention operates across feature dimensions to compute a cross-variance matrix between keys and queries. They introduce a lag value in the process so that it can learn not only instantaneous but also aged cross-correlations. The proposed method shows improved performance on tasks such as classification and anomoly detection."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The paper's main focus is to address the learning of feature-wise correlation in the transformer attention setup. They explore if the learning feature-wise correlation actually helps in tasks other than forecasting such as anomaly detection, imputation, and classification.\n\nThe proposed correlated attention can capture not only conventional cross-correlation but also capture auto-correlation, and lagged cross-correlation.  The idea that allows one to learn lagged correlation and be able to integrate the most relevant multiple lagged correlation sounds interesting."
            },
            "weaknesses": {
                "value": "1, Some parts of the paper presentation could be improved,  such as the explanation of the methods, for more details check the question sections. \n2. The experiment section does not look very convincing due to the comparison setup (if it is fair or not, please refer to the question section) and results. Given the huge computational cost of integrating the cross-correlation, the experiment results do not look that significant."
            },
            "questions": {
                "value": "1. Some parts are a bit confusing, for instance,  \n\u201cCrossFormer deploys a convoluted architecture, which is isolated from other prevalent Transformers with their own established merits in temporal modeling and specifically designed for only MTS forecasting, thereby lacking flexibility\u201d I am a bit confused, could you explain more in detail this?\n\nThe section to explain equation 5 needs to be improved. Especially the explanation for operator argTopK()  reads a bit confusing.\n\n3. It is confusing how the value of k in equation 6 is defined, do you get the value of c first and then calculate k with the topK operation?  It is not very clear to me why not directly take top k,  for instance, top 5,  lagging value, and use it instead of getting a value k by using the topK operator?  Any motivation behind?\n\n4. I am not sure how to go from equation 7 to the result they got in the section below, maye some proof?\n5. It seems even with FFT, the computational complexity is still quite high for a time series with a large feature dimension.\n\n6. When evaluating a specific task, it is crucial to compare its performance with a model that has been explicitly designed and optimized for that particular task. For instance, both non-stationary transformers, dlinear and FEDformer are designed for forecasting tasks. The reviewer are not 100% sure that whether it is a fair comparison when applying those to classification, and anomaly detection tasks.\n\n7. I think the most fair comparison is transformer vs transformer +CBA where the transformer has the same number of heads as the transformer +CBA (when we count both temporal attention and correlated attention heads). Does the transformer in Table 2 has exact same head as the transformer +CBA? The results of the transformer in that table do not show much improvement when compared to transformer + CBA. \n\n8. In anomaly detection and classification, it shows that transformer +CBA has significant improvement compared to transformer, this was not observed in the imputation task, any insights into that?\n\n9. It would be interesting to see the performance on the forecasting task as well I think since there is nothing in the design that specifically restricts it to the non-predictive task.\n\n10 . I think the paper main motivation was addressing the feature-wise correlation specifically for non-predictive tasks, but I am missing the discussion what is the difference when you learning the feature-wise correlation for forecasting or for non-predictive task and what is in this model that makes it more fit for the non-predictive task"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698772227433,
        "cdate": 1698772227433,
        "tmdate": 1699636270776,
        "mdate": 1699636270776,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "cEp14DIJ15",
        "forum": "9nXgWT12tb",
        "replyto": "9nXgWT12tb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
        ],
        "content": {
            "summary": {
                "value": "The author extend autoformer to cross-correlation and propose a correlated attention mechanism to capture feature-wise dependencies."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The authors propose a correlated attention mechanism to capture lagged cross-covariance between variates, which can combined with existing encoder-only transformer structure.\n2. The experiments show correlated attention mechanism enhances base Transformer models."
            },
            "weaknesses": {
                "value": "1. The novelty of the paper is limited.  The proposed correlated attention is basically a extension for Autoformer, which only captures auto-correlation. However, this method neither proposes a good method to reduce the computational complexity caused by calculating cross-correlation, which is almost unacceptable in actual scenarios, nor does the author conduct a comparative experiment with Autoformer to prove that the introduction of corss-correlation can bring to achieve practical improvements. \n2. The integration of correlated attention to existing transformer structure is conducted with a mixture-of-head attention structure, which is a concatenation of CAB and transformer outputs. CAB acts as a rather independent component and does not truly integrated into existing transformer structures."
            },
            "questions": {
                "value": "1. Judging from the design of CAB, it can predict independently without requiring additional transformer deconstruction. Why didn't you test the independent CAB? \n2. The design of CAB is based on autoformer. Why is there no comparison between the effects of CAB and autoformer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_mjBo"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698777236875,
        "cdate": 1698777236875,
        "tmdate": 1700664114862,
        "mdate": 1700664114862,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "x1xtFQCnob",
        "forum": "9nXgWT12tb",
        "replyto": "9nXgWT12tb",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces a novel concept called the Correlated Attention Block (CAB), designed to efficiently capture cross-correlations between multivariate time series (MTS) data within Transformer-based models. The CAB is a versatile component that seamlessly integrates into existing models. Its key innovation lies in the correlated attention mechanism, which operates across feature channels, enabling the computation of cross-covariance matrices between queries and keys at various lag values. This selective aggregation of representations at the sub-series level opens the door to automated discovery and representation learning of both instantaneous and lagged cross-correlations while inherently encompassing time series auto-correlation.\n\nThe authors conducted an extensive series of experiments, focusing on Imputation, Anomaly Detection and Classification tasks. Their results demonstrate remarkable performance, underscoring the potential of the CAB to enhance the analysis and modeling of MTS data."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "This paper is well-structured, presenting a thorough background introduction and a step-by-step introduction of the novel concept, the Correlated Attention Block (CAB). A notable feature of this work is the seamless integration of CAB into encoder-only architectures of Transformers, making it a potentially good-to-have addition to the field.\n\nFurthermore, the authors conducted an extensive set of experiments across three different tasks, utilizing a variety of common datasets. The results consistently show impressive performance, often outperforming previous state-of-the-art methods. This robust evaluation underscores the potential of the proposed design in improving representation learning for multivariate time series, making it a valuable contribution to the field."
            },
            "weaknesses": {
                "value": "1. Page 9, Line 1 of **Conclusion And Future Work**: There's a minor typo that needs correction - \"bloc\" should be changed to \"block.\"\n2. Citation Style: The reference list shows some inconsistency in the citation style. To enhance clarity and uniformity, consider standardizing the format across all references. For example, you could list all NeurIPS papers with consistent formatting, and for papers from other conferences or sources, ensure that their respective publication details are included appropriately. For instance:\n    -   NeurIPS papers should consistently include the conference name and URL. For example, \"Vaswani et al. (2017, NeurIPS, URL) and Shen et al. (2020, NeurIPS, URL).\"\n    -   Papers from other conferences or sources should similarly follow a consistent format, such as including the conference name and URL as needed. For example, \"Cao et al. (2020, Conference Name, URL)\" and \"Li et al. (2019, Conference Name, URL).\"\n3.  Motivation for Correlated Attention Block: While the paper mentions that CAB efficiently learns feature-wise dependencies, it would be beneficial to provide more clarity on the specific aspects of CAB's design that contribute to this efficiency. Clearly articulating which components within the CAB block are the key drivers of this efficiency could help readers better understand the innovation.\n4.  Analysis of FFT Efficiency: While Section 3.2.2 discusses the time efficiency of FFT, it would be valuable to include a clear analysis that quantifies how much the use of FFT improves the performance of CAB compared to vanilla CAB or previous baselines. Providing concrete numbers or performance metrics would strengthen the paper's findings in this regard. \n5. Limitation of Encoder-Only Models: It's important to acknowledge that the design of CAB is limited to encoder-only models and does not support time series forecasting. While this limitation is briefly mentioned, expanding on the reasons behind this constraint and discussing potential avenues for future work or extensions to address this limitation would add depth to the paper."
            },
            "questions": {
                "value": "1.  In Table 2, it is evident that for the first three datasets, the TimesNet baseline consistently outperforms CAB when the mask ratio exceeds 25%. Could you provide insights into this performance discrepancy?\n2.  Could you elaborate on the primary challenges or obstacles preventing the integration of CAB into encoder-decoder models for conducting multivariate time series forecasting?\n3.  Could you provide a detailed breakdown of the distinct contributions of each component within CAB, both in terms of performance enhancement and efficiency gains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3225/Reviewer_M5Eq"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698810158305,
        "cdate": 1698810158305,
        "tmdate": 1700680799422,
        "mdate": 1700680799422,
        "license": "CC BY 4.0",
        "version": 2
    }
]