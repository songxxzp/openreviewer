[
    {
        "id": "tiCV4QnXcv",
        "forum": "G0vdDSt9XM",
        "replyto": "G0vdDSt9XM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
        ],
        "content": {
            "summary": {
                "value": "The paper presents CRAFT, a framework for tool creation and retrieval to customize large language models (LLMs) for various tasks and domains. CRAFT creates a specialized toolset by prompting LLMs to generate and abstract code solutions for problems, and validates and deduplicates the tools. CRAFT retrieves relevant tools from the toolset by multi-view matching, and adds them to the prompt of LLMs for code generation. CRAFT improves performance on vision-language, tabular processing, and mathematical reasoning tasks."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper proposes a tool generation and tool-using framework for LLMs which is a good attempt to enhance LLMs' capability to solve reasoning tasks by generating programs.\n\n2. Personally I think the idea of a \"pseudocode library\" proposed in future work is cool and meaningful.\n\n3. The experiments on baselines and different LLMs are comprehensive and the result is promising. Basically, I agree with the authors that tool generation puts a high demand on the LLMs' coding ability.\n\n4. The created toolsets are a particularly important contribution to the LLM community."
            },
            "weaknesses": {
                "value": "1. The authors mentioned that alternative backbone models like CodeLlama demonstrate near-random performance. Can the authors provide such results (the performance of different LLMs in creating and using tools) in the experiment?\n\n2. I suggest that the author should make the distinction between more specific methods more prominently in the main text (though the difference has been discussed in the experimental setting), such as by creating a table to compare various tool-augmented language model methods, and so on. The current Figure 1 appears to be similar to previous work like LATM, making it difficult to showcase the uniqueness of this article."
            },
            "questions": {
                "value": "1. What does \"bug-free\" mean and how do the authors ensure that the generated tools are \"bug-free\"?\n\n2. What is the result of tool generation with GPT-3.5-turbo and other LLMs?\n\n3. Can CRAFT be adapted to programming tasks like HumanEval since it generates \"APIs\"?\n\n4. Can the authors discuss more on the \"pseudocode library\" like can we use a natural language library and how is it different from in-context learning?\n\n5. Can the authors analyze more on the created toolsets like where they might it can be applied/generalized?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "8: accept, good paper"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Reviewer_3JvW"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698668438607,
        "cdate": 1698668438607,
        "tmdate": 1699636271660,
        "mdate": 1699636271660,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "efa9X7HXdB",
        "forum": "G0vdDSt9XM",
        "replyto": "G0vdDSt9XM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_nWn7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_nWn7"
        ],
        "content": {
            "summary": {
                "value": "This paper introduces CRAFT, a novel framework designed to enhance large language models (LLMs) by creating and retrieving task-specific tools. CRAFT creates toolsets first and equips LLMs with a component that retrieves these tools to solve complex tasks. Experiments on vision-language, tabular processing, and mathematical reasoning tasks demonstrate the superiority of this approach over strong baselines. The analysis reveals that performance improvement is consistent when scaling the number of tools and the capability of backbone models, and that the created tools exhibit low complexity and atomicity."
            },
            "soundness": {
                "value": "4 excellent"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1.\tTraditional approaches to augment LLMs with tools lack flexibility, as they rely on general-purpose APIs. CRAFT addresses this problem by reusing task-related tools, which is more flexible.\n2.\tThis method can adapt off-the-shelf LLMs to new domains and modalities without finetuning.\n3.\tExperiments show that the proposed framework can improve a lot compared to previous approaches."
            },
            "weaknesses": {
                "value": "1.\tThe setting of the experiments is a little bit limited. There are many agent benchmarks like MINT, AgentBench, and so on, which focus on the problem-solving capacity of LLMs as agents. The reviewer thinks the work needs to be further verified on broader benchmarks for agents.\n2.\tThe comparison with LATM is a little bit unfair. The toolset created by CRAFT is the output of GPT-4, while the tool used by LATM is created by an inferior model if there is no misunderstood.\n3.\tThe transferability of the toolset should be discussed as I noticed that the toolset for the VQA task and the toolset for the reasoning task are not the same. Maybe the authors can experiment to create a general tool set for all tasks and see what will happen."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698717545417,
        "cdate": 1698717545417,
        "tmdate": 1699636271583,
        "mdate": 1699636271583,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "e08cIfMo0P",
        "forum": "G0vdDSt9XM",
        "replyto": "G0vdDSt9XM",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces CRAFT, a novel framework for augmenting Large Language Models (LLMs) with specialized tools to tackle complex tasks. CRAFT generates task-specific toolsets and provides a retrieval component, enabling LLMs to offload functions to external modules through code snippets and APIs. This approach overcomes the limitations of general-purpose APIs, offering tailored solutions and improved flexibility. The framework ensures the quality and reusability of tools through validation, abstraction, and deduplication processes. Experiments across various domains, including vision-language, tabular processing, and mathematical reasoning, demonstrate substantial improvements over strong baselines. The paper's in-depth analysis confirms the scalability of CRAFT, the significance of each component, and the reliability and simplicity of the created tools. Ultimately, CRAFT presents a plug-and-play solution, enhancing the adaptability and problem-solving capabilities of off-the-shelf LLMs without requiring finetuning."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. CRAFT showcases originality by combining tool learning, code generation, and retrieval to enhance LLMs' capabilities, applying this novel approach across various tasks and domains.\n\n2. The framework is rigorously validated across different tasks, demonstrating substantial improvements and ensuring tool correctness and reusability, reflecting the high quality of the work.\n\n3. The paper is well-structured and clearly written, providing a comprehensive presentation of the CRAFT framework, its applications, and experimental results.\n\n4. CRAFT addresses crucial challenges in augmenting LLMs, offering a significant advancement in the field and demonstrating practical applicability and effectiveness across diverse domains."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a more detailed exploration of scenarios where CRAFT might not perform as expected. Understanding the limitations and potential failure cases of the framework would provide a more balanced view and help guide future improvements.\n\n2. While the paper compares CRAFT to several strong baselines, expanding this comparison to include a wider range of existing tools and frameworks (e.g., SOTA methods in VQA) would strengthen the validity of the claimed improvements. This would also help in positioning CRAFT more clearly in the landscape of existing solutions.\n\n3. The paper could provide a more in-depth analysis of the tool creation and retrieval components of CRAFT. Understanding how different types of tools contribute to performance improvements and how the retrieval mechanism interacts with various tasks would offer valuable insights.\n\n4. While the paper mentions the scalability of CRAFT, providing empirical evidence and a more thorough discussion on how the framework scales with the number of tools and the complexity of tasks would be beneficial.\n\n5. The paper could explore and address potential biases in the tool creation process, especially considering the reliance on GPT-4 for generating code solutions. Ensuring fairness and mitigating biases is crucial for the applicability of CRAFT across diverse scenarios.\n\n6. Including a user study or examples of real-world applications of CRAFT could provide additional validation of the framework's practicality and effectiveness, offering a more comprehensive evaluation."
            },
            "questions": {
                "value": "1. Failure Cases: Can the authors provide specific examples or scenarios where CRAFT may not perform optimally? Insight into challenges or limitations faced by the framework would be valuable for a comprehensive understanding.\n\n2. Baseline Comparison: Could the authors expand on the choice of baselines used for comparison? Including a broader range of existing tools and frameworks (existing SOTA methods) might help in better positioning CRAFT\u2019s contributions.\n\n3. Tool Creation and Retrieval Analysis: How do different types of tools contribute to the performance improvements observed with CRAFT? Additionally, how does the tool retrieval mechanism interact with various tasks?\n\n4. Real-World Application: Are there examples of real-world applications where CRAFT has been applied? Including such examples or results from a user study could provide additional validation for the framework.\n\n5. Tool Abstraction and Deduplication: Could the authors elaborate on the process of abstracting code solutions into reusable snippets and the criteria used for deduplication? Understanding this process in detail would provide clarity on the quality assurance of tools.\n\n6. Handling of Descriptive Responses: The paper addresses potential issues with underestimated performance due to descriptive responses from LLMs. Could the authors provide more details on how this issue is handled or mitigated in CRAFT?\n\n7. Scalability: The paper mentions the scalability of CRAFT. Could the authors provide empirical evidence or a more detailed discussion on how the framework scales with the number of tools and the complexity of tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission3233/Reviewer_A2gB"
                ]
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission3233/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698817655729,
        "cdate": 1698817655729,
        "tmdate": 1699636271485,
        "mdate": 1699636271485,
        "license": "CC BY 4.0",
        "version": 2
    }
]