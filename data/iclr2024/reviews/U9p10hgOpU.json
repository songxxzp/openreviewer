[
    {
        "id": "Mg89JxRzjo",
        "forum": "U9p10hgOpU",
        "replyto": "U9p10hgOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_an9h"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_an9h"
        ],
        "content": {
            "summary": {
                "value": "This paper focuses on lifelong learning and its challenges in handling tasks that introduce false correlations between target labels and demographic attributes, resulting in bias. The authors claim that existing solutions often overlook the diversity of task distributions, particularly in lifelong learning, and struggle to maintain fairness. The paper introduces \"Sustaining Fair Representations in Unsupervised Lifelong Learning\" (FaRULi), inspired by human learning behavior, which prioritizes simpler tasks and adjusts task schedules based on fair representations."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "The paper address a the problem of lifelong learning of fair DNN where new tasks come in resulting in an incremental improvement of the decision function. This is an interesting topic and needs to be studied from the fairness perspective."
            },
            "weaknesses": {
                "value": "Abstract: \u201eLike human who tends to prioritize simpler tasks over more challenging ones that significantly outstrip one\u2019s current knowledge scope, FaRULi reschedules a buffer of tasks based on the proximity of their fair representations.\u201c: Is there any reference which claims that human tend to prioritize simpler tasks? Looks kind of obvious but there needs to be a scientific study which shows this.\n\nThe datasets used for evaluation are quite simple once\u2019s - tabular data only. It\u2019s shown that AdaBoost is why more accurate at tabular [1] data so in general why should one use DNN here?\n\nAre there any kind of loopbacks studied? Like the algorithm does a decision which is used for new data which is used to further train the algorithm etc.?\n\n[1] https://arxiv.org/abs/2106.03253"
            },
            "questions": {
                "value": "- \u201ein-domain bias and fail to generalize well to new tasks with data distributional shifts (Barrett et al.).\u201c: Missing year in the reference\n- 4.1. in the table you write \u201etasks\u201c what exactly are the tasks in the dataset? This is not really well explained in the text.\n- There is a resent paper which uses stochastic quantized representations [2], this can ensures fair representation also after a distribution shift. However, I agree that the method proposed here is a more general way to address the problem of new incoming data.\n\n[2] https://ojs.aaai.org/index.php/AAAI/article/view/25851"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper proposes a new approach to have a life-long learning of fair algorithms which is needed when deploying such systems. A detailed legal analysis of the method is however still needed when deployed since it can directly effect discrimination."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Reviewer_an9h"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697713160476,
        "cdate": 1697713160476,
        "tmdate": 1699636238372,
        "mdate": 1699636238372,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "OfeSThSiII",
        "forum": "U9p10hgOpU",
        "replyto": "U9p10hgOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_kvtG"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_kvtG"
        ],
        "content": {
            "summary": {
                "value": "The paper tackles the problem of sustaining fairness in decisions made in an incremental learning setting. To that end, the paper introduces an approach that relies on task labels of only the first subtask, and other tasks can be unlabeled (for the target) attribute. The paper relies on an adversarial objective to learn fair representations that tries to maximize the utility of the learned representations while suppressing the protected attribute and task identity. The paper introduces a method involving elastic networks and applies the adversarial objective at every layer of the network. The proposed approach also reorders the incoming task to ensure similar tasks are trained together. The paper also provides a theoretical analysis of their approach and showcases results on tabular datasets."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The proposed approach, FaRULi, achieves significant improvement over baseline approaches on several tabular datasets. \n2. The paper provides a theoretical study of their approach."
            },
            "weaknesses": {
                "value": "1. The primary weakness of the paper is that it is not well-written. Several parts of the paper are very hard to follow (see comments below). Much of the confusion stems from the fact that the paper does not introduce elastic networks and their functioning before the method section.\n2. The technical contribution is limited given the fact FaRULi is a simple application of an adversarial objective in every layer of the network. This adversarial objective is well studied in the literature except for the task identification term, which is not well motivated in the paper. \n3. The paper needs more justification to support/clarify why their approach works so well. The theoretical section provides some insights into it but would be interesting to see how these theoretical results compare with a simple baseline, e.g. an adversarial network. \n4. The experimental section is limited as the paper provides results on tabular datasets only. More experiments on complex data like vision and NLP datasets can be performed to evaluate the efficacy of FaRULi (the FaIRL paper has a similar set of experiments).\n5. The claim of being unsupervised is not well supported as the method uses task labels for the first task. Moreover, since the focus is on learning fair representations the protected label should ideally be considered as the primary supervision. On a similar note, comparison with some of the baselines may be slightly unfair because they do not rely on task labels. \n\n\nComments:\n\nIntroduction, para - 3: \"This strategy ...\" - this sentence is quite hard to understand.\n\nSpecific contributions: (i) \"fairness\" -> fair\n\nProblem statement: may use a different variable than P_i for the protected feature as P is also used for probability\n\nSentence before Eq. 2: what does \"norm\"  mean here?\n\nNext para: \"elastic network that ...\" -> \"elastic network such that ...\"\n\nSection 2.1: 2nd line: \"eliminate\" -> eliminating\n\nSection 2.1: 13th line: what does \"both tasks\" refer to?\n\nEq. 3: it is not clear to me why similarity between representations of R_0 and T_i is desired.\n\nSection 2.2 1st sentence: terms like \"propagating fair knowledge\" are unclear to me. Please make this sentence a bit smoother.\n\nSame paragraph: it is very difficult to understand the functioning of EFRL. The full form of this term was provided in the introduction only without any citations. It would help to have a section on it or describe it using citations of previous works.\n\nThere are several terms in the same paragraph like \"negative model use\", \"independent classifier groups\" which are unclear to me. They need to be defined earlier on in the paper. \n\nSentence after Eq. 4: \"i-th layer\" -> \"l-th layer\"?\n\nThe equation of a^{(l)} is very hard to understand. It took me some time to get that \\sum L_EFRL is a power of \\beta.\n\nSection 3, unclear what \"detriment learning performance\" means in this context?"
            },
            "questions": {
                "value": "1. The setup is unclear to me. If FaRULi performs a reordering of the learning tasks after it receives different tasks when exactly are the predictions of the task recorded? Is this consistent with the baseline approaches?\n2. Section 2.3, it is unclear to me how Q measures the distance between different tasks?\n3. Theorem 1, where is the loss L_FaRULi defined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Reviewer_kvtG"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697934659760,
        "cdate": 1697934659760,
        "tmdate": 1699636238258,
        "mdate": 1699636238258,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "aKZju87uXw",
        "forum": "U9p10hgOpU",
        "replyto": "U9p10hgOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_8gZZ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_8gZZ"
        ],
        "content": {
            "summary": {
                "value": "This paper discusses the challenge of lifelong learning to improve decision-making while ensuring fairness. The authors introduce a new approach called FaRULi, which prioritizes simpler tasks over challenging ones and reschedules tasks based on their fair representations, starting with similar ones to accumulate de-biasing knowledge. It shows promise in making fair and accurate decisions in a sequence of tasks without supervision labels, backed by theory and empirical evaluation."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- This paper addresses the existing limitations in fairness representation learning by adopting a lifelong perspective, wherein it establishes a common representation space across a series of unlabeled tasks.\n\n- The suggested approach appears innovative, and the theoretical framework appears well-founded.\n\n- The method's overview, as illustrated in Figure 1, is presented in a clear and comprehensible manner to showcase its functionality.\n\n- The theoretical analysis section is robust in substantiating the approach."
            },
            "weaknesses": {
                "value": "- The information in Table 1 is presented in a manner that makes it challenging to interpret. To facilitate comparisons, it is advisable to emphasize the highest accuracy and the lowest fairness violation. In light of the results, it's apparent that FaRULi only manages to achieve a somewhat modest level of performance concerning accuracy and fairness. This raises the question of how this outcome supports the claim that \"On average, our model surpasses three leading competitors by 12.7% in prediction accuracy and by a substantial 42.8% in terms of ensuring statistical parity.\"\n\n- The experiment exclusively utilizes tabular data. However, it raises the question of how this proposed approach can effectively address more intricate tasks, such as those in the domains of computer vision or natural language processing."
            },
            "questions": {
                "value": "This paper effectively addresses a significant practical concern related to lifelong learning. However, the experimental section may not be entirely persuasive. Despite reading section Q1 on how FaRULi works, I remain somewhat perplexed. The process for making comparisons doesn't appear straightforward. As I noted in the weaknesses, the results appear to hover around the threshold, without demonstrating clear superiority over other baseline methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698215926906,
        "cdate": 1698215926906,
        "tmdate": 1699636238180,
        "mdate": 1699636238180,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "i0KlhXPWvJ",
        "forum": "U9p10hgOpU",
        "replyto": "U9p10hgOpU",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_E5F7"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2945/Reviewer_E5F7"
        ],
        "content": {
            "summary": {
                "value": "The authors propose a new unsupervised lifelong learning scheme incorporating a statistical parity fairness constraint. The method broadly works as follows:\n1) Train a model on the initial task, for which target labels are assumed to be available.\n2) Consider two new candidate tasks for which labels are unavailable. For each of them, perform unsupervised domain adaptation to adapt the model's latent representation (that is shared across tasks) such that the observed distributions in both tasks are aligned. The task for which this can be achieved by a shallower model is then selected as the next task, the rationale being that this should be a more similar task that induces a lower risk of catastrophic forgetting.\n3) For the selected task, generate pseudolabels for high-confidence predictions made by the model, and incorporate these into the retained training set for the following tasks to come.\n4) Repeat this process for all new incoming (unlabeled) tasks.\n\nIn each of these steps, a statistical parity loss component is taken into account, which penalizes the model for making positive predictions more often for one group of input data compared to another. (The protected attribute is assumed to be observed.) This corresponds to lifelong unsupervised fair representation learning, or to the notion of marginal invariance in the domain adaptation literature.\n\nIn a series of experiments on standard tabular datasets, the proposed method compares favorably against a range of different baselines and competitor methods, both in terms of overall prediction accuracy and statistical parity.\n\nFinally, the authors also provide theoretical bounds on the performance of their method."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The authors address an important, urgent, and essentially unsolved problem: maintaining the fairness of a model while adapting to new data in an unsupervised setting. I have not seen much prior work in this area, and any progress on this topic is highly welcome.\n\nThe framework proposed by the authors is relatively general and should be easy to adapt to other settings and, in particular, other fairness constraints. Empirically, it appears to perform well.\n\nMethodologically, the selection of the next task to adapt to based on the depth of the required changes to the model's representation seems novel to me, although I am not an expert in lifelong learning and hence cannot be certain about this."
            },
            "weaknesses": {
                "value": "I see three main weaknesses of the manuscript in its current form, all of which I believe can be addressed with reasonable effort.\n\n### **1. The choice, enforcement, and evaluation of the fairness constraint**\nThe authors choose demographic/statistical parity as their fairness criterion of choice, which asks that subjects from group A receive positive predictions with equal likelihood as those from group B *regardless of the two groups' baseline likelihoods of receiving a positive outcome*. For example, in the diabetes task, this would require that even if subjects from group A have a much higher prevalence of diabetes, the model would still have to predict diabetes at equal rates for both groups, essentially forcing the model to make false predictions. There are scenarios where this may be considered a desirable outcome, but they seem rather rare to me. For this reason, equal opportunity or equalized odds are much more widely used criteria in the algorithmic fairness literature. (In their discussion of fairness definitions, the authors completely gloss over these two absolute standard criteria.) \n\nLike discussed above, enforcing statistical parity in the face of baseline/prevalence differences forces model misclassifications. This is likely to have strongly affected the results obtained by the authors in the different tasks - they may simply be varying in their level of baseline differences. Moreover, the degree to which this will affect model performance/accuracy will differ based on how strongly statistical parity is enforced / the choice of the regularization constant, which seems likely to differ between the compared methods. These two factors may have strongly influenced the results obtained by the authors, maybe even more so than any intrinsic differences between the datasets/tasks and modeling/training approaches. (This might explain, for example, why FaDL obtains worse accuracy but better statistical parity on the Dutch dataset, as noted by the authors: it simply was more \"successful\" at enforcing statistical parity, which is fundamentally incompatible with high accuracy in datasets with strong baserate differences.)\n\nIn this regard, I would suggest:\n- Implementing at least one other kind of fairness constraint, preferably equal opportunity or equalized odds. I believe that due to the generality of the approach presented by the authors, this should be feasible with reasonable effort? I would expect much less of a fairness-performance trade-off with these constraints, especially equal opportunity.\n- Reporting the baseline incidences $P(Y \\mid P)$ for all protected groups in all datasets.\n- Adding single-task supervised unconstrained and fairness-constrained baselines for each of the datasets, in order to get a better sense of upper performance bounds.\n- Assessing the impact of the fairness regularization constants, or tuning these such that all approaches achieve comparable levels of fairness/unfairness.\n\nSome essential references on these issues:\n- Hardt et al., Equality of Opportunity in Supervised Learning, https://arxiv.org/pdf/1610.02413.pdf\n- https://fairmlbook.org/classification.html\n- Zhao and Gordon, Inherent Tradeoffs in Learning Fair Representations, https://jmlr.org/papers/v23/21-1427.html\n- Mehrabi et al., A Survey on Bias and Fairness in Machine Learning, https://dl.acm.org/doi/10.1145/3457607\n- Zhang et al., Improving the fairness of chest x-ray classifiers, https://proceedings.mlr.press/v174/zhang22a, is one of many prior works that study the influence of the regularization constant in fairness-constrained learning approaches.\n\n\n### **2. Language issues**\nWhile the manuscript is organized well overall and was mostly understandable, there are quite a few places where I stumbled upon sentences or formulations that did not seem to make any sense to me. I will remark on a some specific instances below, but I suggest that the authors carefully revise their language for precision and clarity throughout the manuscript.\n\n\n### **3. Unproven claims concerning the task selection mechanism**\nIn the last paragraph on page 4, and a few other places, the authors present a long list of conjectures on what their method will probably do: \"deep layers will outperform... weight parameters of deep layers [will] start to exceed that of shallow layers ... the parameters of shallow layers [will] initially increase sharply ... weights of the deepest layers [will] increase last and remain relatively small ...\"\n\nAll of these seem to me to just be conjectures, however: none of these claims are proven experimentally or theoretically. I would suggest either making clear that these are pure hypotheses, or demonstrating these effects experimentally.\n\nMoreover, this is just one (new) method to assess task similarity / distributional differences between successive tasks. Could the authors either compare to one or two other standard distributional similarity metrics (for choosing which task to incorporate next), or provide some kind of argument for why existing similarity metrics are not applicable here?"
            },
            "questions": {
                "value": "I will use this space for minor remarks.\n- In the introduction on p.2, what does \"more divergent demographic distributions\" mean, and what does \"incrementing a fair representation\" mean? This whole paragraph was not very clear to me; at the end of the introduction I had not really understood what the authors set out to do.\n- On p.3, I suggest revising \"our FaRULi approach takes a norm as follows\" and the sentence \"Disparate distribution between incoming and learned tasks invokes a deep model, indicating the occurrence of negative model reuse.\" (More generally, \"disparate distribution\" is not a usual term; I suggest rephrasing this to more precisely described what the authors want to say.)\n- On p.3 towards the bottom, the authors write that \"..., a comparable performance on Ti can be achieved\" which seems like an overly optimistic claim.\n- Also on the bottom of p.3, the authors write that \"... disparate distributions [will] likely lead to overfitting in R(0) and yield unpredictable, even negative performance on Ti.\" This is, again, just an unproven hypothesis, I believe? I also suggest revising the sentence \"We draw inspirations from that approximating two disparate distributions necessitates a more complex model.\"\n- In the caption of Fig. 1 on p.4, I suggest revising the sentence \"R(0) is initialed and engaged in fair representation learning [...].\"\n- On p.5, I suggest revising the sentences \"As disparate distributions invoking the deep network, we introduce a similarity-measurement metric [...].\" and \"Therefore, with the help of re-ordering, instances from T2 but being misclassified into R(0) via the red circle and their high confidence pseudo labels [...] are retained and incorporated into the retained dataset [...].\" (I have no idea what this sentence is supposed to mean.)\n- On p.8, I do not understand the sentence \"To ensure fairness, both competitors utilize pseudo-labels for replay to ensure the lifelong learning.\" - what do the replayed pseudo-labels have to do with ensuring fairness?\n- On p.9, the authors write \"Bias in ML models can be attributed to the information carried by protected features.\" This is a highly simplistic characterization of bias. There are many different forms of bias that have many different causes; see, e.g., the Mehrabi survey I linked to above.\n- A little further below (but in the same paragraph), the authors write that \"While these frameworks demonstrate promising results in a specific domain, they fall short in maintaining fairness when applied to o.o.d. data (Barrett et al.).\" The sentence before this one is about counterfactual fairness, suggesting that Barrett et al. show that counterfactual fairness \"falls short in maintaining fairness when applied to o.o.d. data\". They do not, however; in fact, Barrett et al. do not consider counterfactual fairness at all."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2945/Reviewer_E5F7"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2945/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698344810226,
        "cdate": 1698344810226,
        "tmdate": 1699636238080,
        "mdate": 1699636238080,
        "license": "CC BY 4.0",
        "version": 2
    }
]