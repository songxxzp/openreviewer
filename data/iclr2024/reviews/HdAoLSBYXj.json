[
    {
        "id": "O85BU9IVf5",
        "forum": "HdAoLSBYXj",
        "replyto": "HdAoLSBYXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_QHKf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_QHKf"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a set-wise contrastive learning algorithm for neural topic models, where the input document batch is divided into sets and positives and negatives are constructed by augmenting and pooling instances in each set. It casts the proposed method as a multi-objective optimization problem to balance the trade-off between the ELBO and the contrastive objectives. The ChaptGPT API is used for data augmentation to generate negative samples. Experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The set-wise contrastive learning is new and effective for resolving low-level mutual information of neural topic models. \n- Formulating the contrastive learning as a multi-task learning problem and solving it by a multi-objective optimization algorithm is an interesting idea. \n- Experimental comparisons with state-of-the-art neural topic models that include recent ones such as WeTe demonstrate the effectiveness of the proposed moethod."
            },
            "weaknesses": {
                "value": "- The effectiveness of the ChatGPT-based data augmentation is unclear.\n- The formal definition of the contrastive loss is  missing in the main text, while incomplete definition can be found in Algorithm 1. \n- The justification of the use of MaxPooling is unclear. \n- The authors seem to assume as if it is possible to find \"the optimal\" Pareto solution (a Pareto optimal solution with optimal balance), while there is no superiority or inferiority between Pareto optimal solutions."
            },
            "questions": {
                "value": "In Section 4.1, the authors adopt ChatGPT API to perform the augmentation with the specific prompt.\nHowever, at the end of Section 5.1, they use Word2Vec as pretrained embedding for their embedding-based augmentation.\nThis is confusing. Do they perform both ChatGPT-based and embedding-based augmentations for comparisons? \nAnyway, they should show the effectiveness of the ChatGPT-based augmentation. \n\nIn Section 5.5 on page 9, they justify the use of MaxPooling, for extracting set representation,  as MaxPooling directly retrieving strong features. In Table 6, they compare different pooling functions including max and min poolings. \nHowever, in Algorithm 1, both MaxPool and MinPool are used. Here, MinPool is used for positive pair. \nThey should clarify the reason why.\n\nThe purpose of multi-objective optimization is to find a  (possibly diversified) set of Pareto optimal solutions so that it represents the Pareto frontier. Therefore, I would like to know the average and standard deviation of the $\\alpha$  obtained at the end of each run to see how it is diversified. If it is not diversified, then I would like to know the reason.  \n\nJudging from the definition of $\\mathcal{L}_{set}$ in Algorithm 1, both the positive and the negative pairs are generated within the same set and the loss is then accumulated for all the sets. If this is the case, then the first sentence of the Hard Negative Sampling paragraph sounds confusing. Differenciating among sets may be more difficult, but the contrastive pair for differenciating is generated within a set not among sets.  In Algorithm 1,  $\\exp(s_i^{min}, s_i^{+})$ should be $\\exp(f(s_i^{min}, s_i^{+}))$. \n\nHow and what value for the contrastive weight $\\beta$ is determined?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no concerns."
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Reviewer_QHKf"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698639739184,
        "cdate": 1698639739184,
        "tmdate": 1699636861894,
        "mdate": 1699636861894,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DMzhAWKGmz",
        "forum": "HdAoLSBYXj",
        "replyto": "HdAoLSBYXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_3szg"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_3szg"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a setwise contrastive learning algorithm for neural topic model to address the problem of learning low-level mutual information of neural topic models. This work explicitly casts contrastive topic modeling as a gradient-based multi-objective optimization problem. Extensive experiments are performed to demonstrate the effectiveness of this method."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. This paper is well-organized and equations are clearly written.\n2. Extensive experimental are performed and results show this method consistently presents high performance.\n3. Codes are provided in supplementary materials to ensure reproducibility."
            },
            "weaknesses": {
                "value": "1. Since [1] also uses contrastive learning to capture useful semantics of topic vectors which is similar to the proposed method, this paper does not clearly compare with [1] and explain its novelty. \n2. This paper omits important baselines. For example, [1] also presents great performance in this task but this paper does not compare with it in the experiments. Which contrastive learning method performs better? \n\n[1] Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. Mitigating data sparsity for short text topic modeling by topic-semantic contrastive learning. arXiv preprint arXiv:2211.12878, 2022."
            },
            "questions": {
                "value": "1. I have a question about the baselines. Table 5 shows topic diversity (TD) score of WeTe is 0.878\u00b10.012 when T = 50 in AG News dataset. However, in [1], this score is 0.966 without data augmentation and 0.991 with data augmentation. There is a large gap between these scores. I wonder how authors implement these baselines. Could you please provide more details or any codes?\n\n[1] Xiaobao Wu, Anh Tuan Luu, and Xinshuai Dong. Mitigating data sparsity for short text topic modeling by topic-semantic contrastive learning. arXiv preprint arXiv:2211.12878, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7239/Reviewer_3szg"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698744772421,
        "cdate": 1698744772421,
        "tmdate": 1700731818307,
        "mdate": 1700731818307,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "sv7HjXnCkQ",
        "forum": "HdAoLSBYXj",
        "replyto": "HdAoLSBYXj",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_ioHf"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission7239/Reviewer_ioHf"
        ],
        "content": {
            "summary": {
                "value": "This paper aims to address challenges in neural topic models (NTMs) caused by recent approaches that optimize the combination of ELBO and contrastive learning. These challenges include capturing low-level mutual information and a conflict between ELBO's focus on input details for reconstruction quality and contrastive learning's goal of generalizing topic representations. To overcome these issues, the authors propose a novel setwise contrastive learning method for sets of documents, aiming to capture shared semantics among documents. Additionally, they formulate contrastive topic modeling as a multi-objective optimization problem to achieve a balanced solution. Experimental results on 4 benchmark datasets demonstrate that their approach consistently produces higher-performing NTMs in terms of topic coherence, diversity, and document classification performance compared to existing methods."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "4 excellent"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "+ The key idea of this paper (i.e., learning low-level mutual information of neural topic models that optimize ELBO and contrastive learning together) is very well-motivated.\n\n+ The usage of setwise contrastive topic modeling is reasonable. Casting it as a multi-task learning problem and adopting multi-objective optimization to find a Pareto solution are technically novel.\n\n+ A comprehensive set of benchmark datasets and baselines are considered. The authors also perform detailed ablation studies, hyperparameter analyses, and case studies."
            },
            "weaknesses": {
                "value": "- Statistical significance tests are missing. It is unclear whether the gaps between the proposed model and baselines/ablation versions are statistically significant or not. In particular, some gaps in Tables 3 and 6 are quite subtle, and the variances of classification scores in Table 2 are unknown, therefore p-values should be reported.\n\n- Only automatic metrics (e.g., NPMI and TD) are used to evaluate topic quality. Although the authors also examine document classification as a downstream task, the classification performance is just an indirect measurement of topic quality. Recent work [1] has shown that automatic metrics may deviate from humans' judgment. Therefore, I feel human evaluation (e.g., the intrusion test [2]) is still needed.\n\n[1] Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence. NeurIPS'21.\n\n[2] Topic Intrusion for Automatic Topic Model Evaluation. EMNLP'18."
            },
            "questions": {
                "value": "- Could you conduct statistical significance tests to compare your method with the baselines in the experiment tables?\n\n- Could you perform human evaluation (e.g., the intrusion test) to directly examine the quality of extracted topics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission7239/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699047341316,
        "cdate": 1699047341316,
        "tmdate": 1699636861677,
        "mdate": 1699636861677,
        "license": "CC BY 4.0",
        "version": 2
    }
]