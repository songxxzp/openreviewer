[
    {
        "id": "fLVUiVhOmH",
        "forum": "QKqWnNkwPL",
        "replyto": "QKqWnNkwPL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_Robz"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_Robz"
        ],
        "content": {
            "summary": {
                "value": "This work proposes direct self distillation  (DSD) which is a method to distill diffusion models into itself. The core idea is to perform two consecutive sampling steps to get the latents $z_t, z_{t-1}$ and $z_{t-2}$, and use $x_\\theta (z_{t-1})$ as prediction and  $x_\\theta (z_{t-2})$ as target and minimize the squared distance between them.\n\nThe primary distinction from prior works on online diffusion like Progressive distillation (PD) is that PD needs 3 network evaluations per parameter update of student model while DSD needs 2 network evaluations per parameter update. It is worth noting that offline distillation methods also use fewer network evaluations (usually 1 or 2 evaluations) than online distillation methods.\n\nOverall this work does not aim to distill down diffusion models to as few sampling steps as possible but rather to compare DSD to PD while reducing the required model updates for similar image quality."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "1. The proposed method reduces GPU compute cost as it reduces the number of network evaluation per parameter update from three to two. Self distillation eliminates the need to maintain 2 copies of the model in memory (for student and teacher), in turn reducing overall compute requirements. \n2. DSD also uses lower number of parameter updates (4K-5K) compared to prior online distillation techniques which need thousands of parameter updates. For instance, results in Table 2 for CelebA HQ and LSUN Bedroom with 8 steps uses 4K parameter updates performs better than teacher-student distillation at similar number of parameter updates."
            },
            "weaknesses": {
                "value": "1. Advantages of self-distillation while training diffusion models from scratch is unclear and additional results are needed to demonstrate its benefits while training from scratch. Figure 3 compares FID scores for distilled and the regular diffusion model while training for scratch only for the first 30k-60K steps. These FID scores are quite high for DDIM and longer training is needed. For instance, on CelebA, the original DDIM (with 10 sampling steps) achieves FID of 17.33 while in Figure 3, the lowest FID is around 100. Thus longer training will help improve FID scores, however It is unclear whether the gains seen in these plots due to self-distillation will continue for the entire duration of training, and whether the final FID scores of model that is trained from scratch with self-distillation is strictly better than the model that is trained without any self-distillation.\u2028\n\n2. I understand that the primary motivation of DSD is to use fewer parameter updates while achieving comparable image quality with progressive distillation, however the current FID scores in Table 2 are high across the datasets. Further, DSD does not result in high quality images, in terms of FID, in few step sampling (2-4 steps). In order for someone to strictly prefer DSD over PD, one would have to show at least one of the two:  1) DSD can achieve comparable or better FID than PD while using fewer parameter updates, incase we use fine-tuning DSD. 2) Using DSD while training from scratch simultaneously improves FID and reduces sampling steps than training a model from scratch and later doing PD. Currently, the paper does not include sufficient results to show either 1) or 2). DSD seems to outperform TSD at fewer parameter updates (From Table 2) but it is unclear if DSD can match FID of PD with more parameter updates. Currently, the models in the paper use 4K-5K parameter updates. Further, DSD quickly deteriorates in terms of FID while using 2 and 4 step sampling. The results would be much more convincing if FID scores can be matched with PD with slightly more parameter updates. A table could be added that shows FID, sampling steps, and NFEs for DSD and PD. Ideally, we should have comparable FID and sampling steps with much fewer NFEs than PD. It is also worth noting that DSD is orthogonal to PD. Thus it should be possible to use DSD, with say 64 sampling steps which might have better FID, and then do PD to get distilled model with better FID scores at 2-4 sampling steps."
            },
            "questions": {
                "value": "1. Does DSD maintain an exponentially moving average (EMA) copy of the model weights during training/fine-tuning? \n2. Can you help me understand Figure 2? Specifically, I don\u2019t understand X axis. My understanding is that this is $N_i$. Why are the number of denoising steps along X axis small? Shouldn\u2019t we have large step sizes $N_i$ in self distillation initially?\n3. I am not sure that this statement in introduction (second paragraph) about the current distillation techniques is true \u2014 \u201cThe student is trained to mimic the teacher, but in fewer iterations, so that, eventually the teacher and the student diverge during training.\u201d In my experience with distillation techniques, I haven\u2019t seen that student and teacher diverge in the cases when distillation is successful. Could the authors elaborate on what they mean by \u201cdiverge\u201d here?\u2028\n\nMinor Suggestions:\n1. The last line in abstract has a minor grammatical error. It currently reads \u201chelp the model to convergence produce high-quality samples more quickly.\u201d This sentence could be revised to \u201chelp the model converge quickly, ultimately producing higher-quality samples.\"\n2. The readability of paper would greatly improve if the notation is made more consistent.\n    - The loss in Eq. 1 uses the notation of $\\alpha \\cdot \\||\\hat{z}\\_{t-1} - z_{t-1}\\||^2 + \\beta \\cdot \\|| \\hat{z}\\_{t-1} - \\hat{z}\\_{t-2}||^2$ but previous sections use $\\||\\hat{x}\\_{t-1} - \\hat{x}\\_{t-2}\\||^2$ as loss for distillation. Further, the usual choice of loss objective for training diffusion models is $\\||\\hat{x}\\_{t-1} - x_{gt}\\||^2$ ($x_{gt}$ is ground truth image) or $\\||\\hat{\\epsilon}\\_{t-1} - \\epsilon\\||^2$. Why was  $\\||\\hat{z}\\_{t-1} - z\\_{t-1}\\||^2$ chosen instead for training (I'm not asking about the second loss term that corresponds to self distillation loss here)?\n    - Minor: The results section uses S (See header for Table 1) without defining it in previous sections. I presume this denotes number of sampling steps in distillation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2569/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2569/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2569/Reviewer_Robz"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698790115224,
        "cdate": 1698790115224,
        "tmdate": 1700637026761,
        "mdate": 1700637026761,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "yAki1jfEoX",
        "forum": "QKqWnNkwPL",
        "replyto": "QKqWnNkwPL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_EnYo"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_EnYo"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a self-distillation methodology that improves Progressive Distillation (PD). This approach is applied to distill diffusion models to generate high-quality samples more quickly. It diverges from previous methods like PD that utilize a static teacher model by integrating both teacher and student roles within the diffusion models. Furthermore, it avoids accessing the training dataset, initiating instead from a noise distribution. Experiments demonstrate that this distillation method reduces the number of model evaluations required for image synthesis."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The self-distillation approach removes the static teacher from distillation and simplifies the distillation process from PD.\n- The proposed method circumvents accessing the training data. This allows for data-free distillation.\n- This approach converges faster than PD and achieves comparable image quality using a restricted number of function evaluations (NFEs)."
            },
            "weaknesses": {
                "value": "The manuscript is not prepared for acceptance due to several substantive concerns.\n\n1. Clarity of writing. The manuscript suffers from significant issues with clarity, which impedes the reader's understanding of the methodological details. For example, on page 2, the latent variables  $\\mathbf{z}_t$ are introduced without adequate context, leading to confusion. It is only in Section 3 that the reader learns these notations are adopted from Progressive Distillation (PD). Furthermore, the manuscript lacks a necessary introduction to PD and fails to clearly articulate the distinctions between Teacher-Student Distillation (TSD) and PD. The writing quality would benefit from thorough proofreading and reorganization.\n  \n2. The complexity of the method. The proposed method entails a three-level nested loop, as presented in Algorithm 2. Although it is not technically unacceptable, without a clear introduction of the notations, the writing issue complicates the understanding of the algorithm and scheduling. The recycling of previous latent variables in the loop, coupled with the scheduling, together add to the complexity. Despite eliminating the static teacher, the method does not appear to be a conceptual simplification of PD.\n\n3. Inadequate experiment comparisons. The popular benchmarks for this task, for example, CIFAR-10 and ImageNet 64$\\times$64, are missing from the current manuscript. The experiment's design makes it unable to compare with recent progress. Furthermore, the algorithm seems not to demonstrate clear superiority over TSD or the original models in some cases.\n\n4. Missing literature. Distillation methods developed subsequent to PD are missing in the manuscript. There is a notable absence of current data-free distillation methods and those that have transitioned from using a static teacher to an online teacher. Progress on diffusion samplers without retraining should be introduced in the literature."
            },
            "questions": {
                "value": "Please see the comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699181391634,
        "cdate": 1699181391634,
        "tmdate": 1699636193787,
        "mdate": 1699636193787,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "JIcaOHPlML",
        "forum": "QKqWnNkwPL",
        "replyto": "QKqWnNkwPL",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_tQwJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2569/Reviewer_tQwJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a Direct Self-Distillation (DSD) method for diffusion models. DSD allows the model to distill itself during training, resulting in outputs in fewer iterations. Based on the results presented in the paper, DSD appears to improve the convergence rate of training. However, in the context of distillation models, the goal is typically to train a student model that achieves similar performance to the teacher model but with fewer resources. Unfortunately, the experiments of this paper show that the distilled model has significantly worse performance than the teacher model, which makes the reader highly suspect the validity of the method. \n\nAdditionally, it is important to compare the number of parameters in the distilled model to the original model. This information would demonstrate the potential benefits of the distilled model in terms of model size and resource requirements."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "1. The problem of distilling diffusion models to other efficient models is important for efficient sampling of diffusion models."
            },
            "weaknesses": {
                "value": "1. The most severe weakness of this paper is the weak performance. It is well-known that the goal of distillation is to obtain more efficient student models with comparable performance to teacher models. However, in this paper, the proposed distilled model is not even close to the teacher model (LSUN results in Table 2). I strongly recommend the author to check the results and validity before submitting.\n\n2. I recommend the author compare the number of parameters in the distilled model to the original model. This information would demonstrate the potential benefits of the distilled model in terms of model size and resource requirements.\n\n3. Some minor points such as writing and presentation. \n(1) The 4th line of related work sections should use citations with brackets;\n(2) I recommend the author combine Figure 3 and Figure 4 into one line such that a lot of space can be saved."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "3: reject, not good enough"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2569/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699238233888,
        "cdate": 1699238233888,
        "tmdate": 1699636193710,
        "mdate": 1699636193710,
        "license": "CC BY 4.0",
        "version": 2
    }
]