[
    {
        "id": "0Vr0aeNF6l",
        "forum": "EGjvMcKrrl",
        "replyto": "EGjvMcKrrl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
        ],
        "content": {
            "summary": {
                "value": "This paper proves a generalization error bound for SSMs, where the input data are assumed to be sampled from a Gaussian process, which incorporates temporal dependency. The error bound motivates both a new initialization scaling strategy and a regularized loss function in training. The effect of the new initialization and regularization are evaluated using both a synthetic dataset and the Long-Range Arena benchmark collection."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "* The error bound in Theorem 1 incorporates the temporal dependency of the input data. It is nicely justified in the `Comparison` section why this is important and how it is missing from the previous work.\n* As far as I know, the scaling and the regularization strategies are new to the SSM society. The paper demonstrates their potential promises using experiments.\n* Overall, the paper is clearly written, and the mathematical statements are mostly properly made. (See `Questions` below for a couple of clarification questions.)"
            },
            "weaknesses": {
                "value": "* My biggest concern is the assumption that the inputs in `Theorem 1` are sampled from a Gaussian process. Perhaps the Gaussian assumption is more reasonable in a non-temporal setting, such as linear regression. However, most time series inputs that we encounter in practice cannot be ''sampled'' from a Gaussian process. For example, you cannot find a single GP that accounts for the flattened MNIST images, because images representing different numbers may have their own unique features, and such distinct features cannot be captured solely by the randomness in your GP. If you fix a GP and randomly sample your sequential pixels, then most figures you obtain won't represent any number. I understand that this Gaussian assumption is crucial in proving your generalization error bound and there is perhaps no way out, but this indeed results in a gap between your theory and the methodologies you proposed.\n\n* The proposed regularization method (9) combines a normalized $\\ell^2$ loss and a regularization term. In an SSM, one usually chains multiple LTI systems; however, only the target output of the entire SSM is known (e.g., whether the maze is solvable, what the number in the MNIST figure is). In that case, it is unclear how the ''target outputs'' $y_i$ of the intermediate LTI systems are defined.\n\n* The evaluation of the model does not show clear evidence of why the scaling of the initialization makes the model more robust. For example, in `Table 1`, comparing the cases `w/o (8), (9)` to `w/ (8)`, it seems that adding the scaling improves the training accuracy but makes the generalization accuracy even worse. This actually contradicts the claim that the model is made more robust by scaling the initialization.\n\n* Since the regularization involves a hyperparameter $\\lambda$, it is a good practice to perform an ablation study to demonstrate the effect of changing $\\lambda$."
            },
            "questions": {
                "value": "* The setup of this paper does not consider the matrix $\\mathbf{D}$ in an LTI system. How easy is it to incorporate that matrix and do you need to scale $\\mathbf{D}$ in initialization?\n\n* In `Theorem 1`, can you show the explicit dependency of $C_T$ on $T$? This is important because in training an SSM, the discretization size $\\Delta$ is usually trainable, making the final time $T$ in the continuous world change from time to time. Hence, in order to apply your theory, it is better if we can understand the role of $T$.\n\n* In `Theorem 1`, when you say $\\tilde{\\mathcal{O}}(\\cdot)$ hides the logarithmic factor, which variables are considered? For example, it clearly does not hide $\\log(1/\\delta)$.\n\n* The presence of `Proposition 1` seems a bit abrupt. How does that relate to your ''robustness'' discussion? In addition, what kind of ''stability'' are you referring to? This is a fairly ambiguous term, which can represent the stability of a numerical algorithm, the asymptotic stability of your LTI system (i.e., if your eigenvalues of $\\mathbf{A}$ are all in the left half-plane), or something else.\n\n* In your experiments, it is shown that the regularizer improves the training accuracy, which is a bit counter-intuitive. Do you have a justification for that?\n\n* Not a question but a side note: in order to comply with the ICLR formatting guide, all matrices and vectors should be made boldface."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission1640/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1640/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission1640/Reviewer_SPHH"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1697590332143,
        "cdate": 1697590332143,
        "tmdate": 1700747249073,
        "mdate": 1700747249073,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "RnX7s6Ub1p",
        "forum": "EGjvMcKrrl",
        "replyto": "EGjvMcKrrl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_nrdv"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_nrdv"
        ],
        "content": {
            "summary": {
                "value": "This paper investigates the generalization performance of state space model, in which the data-dependent generalization bound are established. Motivated by the the theoretical findings, the authors design a scaling rule for model initialization and introduce a new regularization mechanism to improve both the robustness and generalization performance of SSM."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "1. Each section of the paper is clear presented and motivates the paper well.\n2. The generalization results appear to be interesting, and the experimental results support the theoretical claims."
            },
            "weaknesses": {
                "value": "The current theoretical results could be more plentiful, e.g. replenish the generalization analysis on  regularized model (9), which may help to answer the question raised below."
            },
            "questions": {
                "value": "1. In Theorem 1 , the authors claim that the SSM generalization is characterized by the temporal dependencies of the sequential data. More details on how does the dependency of the sequential data affect the generalization error should be included. Moreover, in order to achieve small generalization error, the mean and variance of the GP should remain a small level. While these two key parameters rely on the GP assumption, independent  of data. This seems inconsistent with data-dependent generalization error bounds, as claimed in the paper.\n\n2. In speak of enhancing the robustness of SSMs on different temporal dependencies, the authors take   $1/\\sqrt{\\tau(\\theta)}$  as a rescaling factor for initialization. Any theoretical guarantees (e.g. variance analysis)  on the robustness comparing with the HiPPO framework?\n\n3. The main techniques adopted in the proof are sub-exponential property of r.v. and Borell-TIS inequality, how did they yield to  temporal dependency generalization bounds since both of them are temporal independent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698557009921,
        "cdate": 1698557009921,
        "tmdate": 1699636092459,
        "mdate": 1699636092459,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "1ETb81MJZa",
        "forum": "EGjvMcKrrl",
        "replyto": "EGjvMcKrrl",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_2971"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission1640/Reviewer_2971"
        ],
        "content": {
            "summary": {
                "value": "This paper presents an analysis of a generalization bound for linear SSMs.  These SSMs are the building blocks of a new class of deep sequence models.  The authors posit that understanding this bound promises to inform the design of initialization and regularization schemes.  Networks trained using these techniques are shown to have better performance or favorable training characteristics on two simple examples."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "Studying the scaling properties of different initialization schemes and regularizers is clearly important.  Other papers have started to study this also in a bid to understand if HiPPO is simply an example of a wider family of options.  Furthermore, regularizers arising naturally from generalization (which is what a regularizer is trying to target!) is intuitively appealing.  The detail the authors go through their derivations is (for better or for worse) incredible.  I have not gone through the derivations line by line, but I think I understand the general gist of them.  The intuitions given are enough to allow most readers to grasp the core concepts.  The experimental results show promise.  Overall, the paper is fairly well written and prepared."
            },
            "weaknesses": {
                "value": "I am very on the fence on this work.  I think the work is sound, and the authors are to be commended for the detail they go into, but I am not quite convinced that it is at the requisite threshold for acceptance.  Ironically, I am actually left wanting slightly more.  As someone who uses SSM models, I am not yet convinced to integrate this into our workflow, and would need to see more evidence that it is worth incorporating.  Furthermore, I think there are some disconnects between the theory and the practice.\n\nMy main comment is that the experimental evaluation isn\u2019t quite complete enough to convince me:\n- There is quite a lot of work here to just get better generalization as shown on a near-pathological synthetic example, and marginal improvement on LRA (see Q.1. as well). \n- I would have also liked to have seen more evaluation of the initialization across different sizes of models, sensitivity to hyperparameters etc.  Experimental repeats are also important to ensure that the results are reliable.  \n- The additional time complexity is also theoretically commensurate with the original S4 model, but I would like to see a concrete comparison of the runtimes to confirm this.  \n- I would also like to see a more thorough comparison to, e.g., the initialization and metrics suggested by Orvieto et al. [2023], or evaluation of whether this initialization/regularization scheme can be applied to methods adjacent to S4 (e.g. S4D, S5, Liquid-S4, MEGA).  \n- How reasonable are the assumptions, and how tight are the bounds in practice?  I do not have a great understanding of whether the GP assumption is sensible in practice, and there doesn\u2019t appear to be any validation of this.  How does the fidelity of the GP approximation impact the performance of the regularizer?  \n- It would be interesting to try and establish exactly how the initialization and regularization terms affect the learned model.  I understand that L2 regularization reduces the magnitude of the parameters (controlling a notion of complexity), but what does the regularizer in (9) actually encourage in the learned models?  How are the regularized models different from regular S4 models?  This analysis might enable the design of even better SSM structures.  \n- It is also a shame that Path-X wasn\u2019t included in the paper.  My understanding is that Path-X is the only really challenging LRA benchmark.  While I am willing to overlook this in this evaluation, I encourage the authors to complete the benchmarks.  \n\nThere are additional results in the supplement that are basically not commented on, and seem important (e.g. Figure 3 and Figure 4).  These should be explained more thoroughly, and brought up to the main if they are truly important.  I think these extra experiments that probe the method are super important to verifying that the method is working as expected.\n\n**Summary**:  Right now I am just about convinced that the method just about works, but I think some arguments and opportunities aren\u2019t fully explored.  There is clearly an opportunity for this line of work to become very impactful, but I think it would benefit from a round of revisions, and expanding the breadth and depth of the experimental evaluation.  That said, I am very open to revising my review score should the authors remedy some of my concerns.  \n\n## Minor comments\n- Figures, tables etc should be floated to the top or bottom of pages, as opposed to inserted in the middle of the text. \n- Table 1 should be prepared using the same (and correct) style as used by Table 2.\n- Only proper nouns should be capitalized."
            },
            "questions": {
                "value": "**Q.1.**:  Can the authors clarify whether, in Table 2, w/o (8, 9) corresponds to the original S4 model?  The numbers are slightly lower than in the original paper, and I am trying to clarify whether these numbers are like-for-like within the table, and how comparable to S4 they are.  \n\n**Q.2.**:  The theories and algorithms presented are for one-layer networks, but then in Section 4.4 you use multilayer networks.  Can the authors comment how the theories translate to multi-layer networks, where, presumably, the statistics of the input to each layer are not constant.  \n\n**Q.3.**:  Can you clarify how the rescaling in Line 7 of Algorithm 1 works: (a) across epochs and (b) extends to multiple layers.  R.e. (a): is the value of $\\tilde{C}$ rescaled at the beginning of every epoch?  I.e. it is being \u201creinitialized\u201d by rescaling its previous values.  R.e. (b): does rescaling $\\tilde{C}$ at each layer disrupt the action of other layers?  Or is there a different method for rescaling between layers?  \n\n**Q.4.**:  The experiment in Figure 2, is it really Gaussian white noise?  Or is it more like Brownian motion?\n\n**Q.5.**:  Does the training loss in Figure 2 (right) include the regularization term?  I believe it should actually be labeled as \u201cTraining set MSE\u201d.  \n\n**Q.6.**:  An appealing benefit of S4 is the zero-shot transfer to other sampling frequencies.  However, this might change the scale of the time-dependencies.  Can the authors clarify whether there are drawbacks to this method with respect to zero-shot transfer? \n\n**Q.7.**:  The theory is presented for linear SSMs, but in practice, multi-layer S4 models are interleaved with position-wise nonlinearities.  I cannot see any discussion of how these nonlinearities (and the parameters in these nonlinearities, e.g. GLU) interact with the regularization of the parameters in the SSM, or, how the warping effect of the nonlinearity affects/interacts with the theoretical results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission1640/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698751354224,
        "cdate": 1698751354224,
        "tmdate": 1699636092387,
        "mdate": 1699636092387,
        "license": "CC BY 4.0",
        "version": 2
    }
]