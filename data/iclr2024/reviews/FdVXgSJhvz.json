[
    {
        "id": "E7TSaO3McS",
        "forum": "FdVXgSJhvz",
        "replyto": "FdVXgSJhvz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_egVm"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_egVm"
        ],
        "content": {
            "summary": {
                "value": "This paper presents a data filtering method based on ChatGPT to filter out the low-quality instruction-finetuning data. With the proposed mechanism, the authors demonstrate using only 9k high-quality data from ALPACA can achieve good performance. The proposed method is evaluated by ChatGPT and human on several language models."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "* The paper is generally well-written and clear.\n* This paper again demonstrates that the data quality in instruction tuning is more important than quantity. \n* Many ablation experiments are provided in Appendix."
            },
            "weaknesses": {
                "value": "* As the authors stated in the limitation part, only 7B and 13B models are verified. I understand that this is mainly due to the computational constraint, but one concern is that similar observations may not scale on larger models.\n* Knowing that the LLM models is sensitive to the prompt,  more prompt templates on data filtering are expected to be studied. \n* The evaluation is mostly based on ChatGPT as the judge and the \"Win-Tie-Lose\" metric. More evaluation results are expected, especially on benchmarks related to adversarial and out-of-domain robustness. Evaluating solely on the curated instruction set is not enough to demonstrate the effectiveness of the data filtering method on other dimensions."
            },
            "questions": {
                "value": "* Regarding the second point of weakness. For example, does the granularity of the score affect the performance? What's the effect of system prompts on final performance?\n* Regarding the third point of weakness. Does the reduced quantity of data have a detrimental effect on the robustness of tuned models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission2715/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2715/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission2715/Reviewer_egVm"
                ]
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2715/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698268912019,
        "cdate": 1698268912019,
        "tmdate": 1700673159190,
        "mdate": 1700673159190,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "DhDvThsKgV",
        "forum": "FdVXgSJhvz",
        "replyto": "FdVXgSJhvz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_BizM"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_BizM"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes a data selection method to remove noisy or low-quality data from the 52k Alpaca data for instruction-finetuning (IFT). Specifically, the authors use ChatGPT to evaluate the quality of the Alpaca 52K instruction tuning dataset and filter out the data samples of low quality. The experimental results show that LLaMA finetuned on the selected dataset outperforms the LLaMA finetuned on the whole Alpaca 52K dataset, as evaluated by GPT-4."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "- The paper is generally clear to read and easy to follow.\n- The demonstrated method for data selection is simple and easy to understand. \n- The experimental results show some improvement over the baseline finetuned on the whole dataset. \n- The overall experiments are quite extensive."
            },
            "weaknesses": {
                "value": "- The authors claim that it costs less to finetune with the filtered dataset by comparing the computation costs in the finetuning stage. However, the authors didn't consider the costs in the data filtering stage where they used the ChatGPT to get the scores for the data samples, which also cost money, computation, and time.\n- Alpagasus is partially weaker than Alpaca (as in Table 2), especially on the large MMLU dataset. The datasets where Alpagasus outperforms Alpaca seem to be small datasets, mostly consisting of a few hundred samples. Moreover, the improvement of Alpagasus over Alpaca is marginal. These issues make the method of Alpagasus not quite convincing. \n- Since the data quality scores are evaluated by ChatGPT (using the OpenAI GPT-4 API?) and the final model output evaluation on test sets is performed by GPT-4, it may lead to a bias towards the GPT-4's preference in the finetuning and testing stages. In other words, GPT-4 selects the data samples it prefers, which can naturally be used to finetuned a model with the same output characteristics that is preferred by GPT-4.\n- It is unclear how many runs are performed for the random splits. The standard deviations of the results are also needed in the paper. \n- In some test sets, the results of the model finetuned on a random split are comparable to, sometimes even better than, those finetuned on the whole dataset. This observation may suggest that the instruction tuning actually suffers from overfitting to the whole training set so the results obtained by training on random splits are even better."
            },
            "questions": {
                "value": "- What are the standard deviations of the results obtained from the random data splits? And how many runs?\n- Are the results in Figure 14 also evaluated by GPT-4?\n- What is the agreement between the scores obtained using Claude2 as the LLM filter and those using ChatGPT?\n- How reliable is the LLM filter?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2715/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698625170574,
        "cdate": 1698625170574,
        "tmdate": 1699636213665,
        "mdate": 1699636213665,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "58mPXwhF3x",
        "forum": "FdVXgSJhvz",
        "replyto": "FdVXgSJhvz",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_qmSJ"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission2715/Reviewer_qmSJ"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes filtering low-quality instruction data by an LLM judger (GPT-4 specifically). The authors show that fine-tuning an LLM on the filtered data offers performance better than on the entire raw dataset."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "2 fair"
            },
            "strengths": {
                "value": "The idea of filtering low-quality data is reasonable. The experiments are extensive: The authors compare the the model fine-tuned on the filtered datasets on several types of evaluation suites."
            },
            "weaknesses": {
                "value": "- The filtering model ablation study: The performance of Alpagasus seems to highly depend on the quality of the filtering model. How often does GPT-4 as the filtering model makes a mistake? What is the effect of using different filtering model (GPT-3.5, Claude-2, Llama, etc)? As of now, the paper does not offer any study or motivation of using GPT-4 as the filtering model, and simply assumes that is the best choice."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission2715/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698729743489,
        "cdate": 1698729743489,
        "tmdate": 1699636213571,
        "mdate": 1699636213571,
        "license": "CC BY 4.0",
        "version": 2
    }
]