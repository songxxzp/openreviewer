[
    {
        "id": "BaFf982TCF",
        "forum": "bnPALC6S4l",
        "replyto": "bnPALC6S4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_PJmS"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_PJmS"
        ],
        "content": {
            "summary": {
                "value": "Neural networks can be used to learn the solution operators for PDEs. The more complex the PDE problem (in terms of initial data etc.) the harder it is to learn solution operators applicable to a wide class of problem instances. The authors propose to combine the ideas of domain decomposition methods with that of Fourier neural operators. They demonstrate how to successfully learn the solution operator for smaller sub-domains for problems that can be solved by iterating on a domain decomposition by updating the boundary condition based on neighboring solutions. The authors demonstrate that their learned neural operators can solve example problems in electromagnetics (magnetic field wave equation) and fluid mechanics (incompressible Navier-Stokes equation) via domain decomposed iteration. To successfully learn the solution operator the authors propose to add residual connections and a self-modulation mechanism to the Fourier neural operator architecture from the literature and demonstrate their usefulness in ablation experiments. Due to the iterated nature of the author\u2019s proposed solution approach the solution is not one-shot and the computational cost increases in a non-trivial way as a function of problem size. Overall, it is not clear that using the proposed SNAP-DDM approach outperforms a conventional finite difference frequency domain solver."
            },
            "soundness": {
                "value": "2 fair"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "1 poor"
            },
            "strengths": {
                "value": "- The authors present an original idea of applying the neural operator approach to domain decompositions which should be easier to learn systematically due to keeping them to a smaller sub-domain which should hopefully contain less complexity.\n- The authors contribute architectural improvements to FNOs and demonstrate the usefulness to the example problems that they consider. These ideas seem likely to be useful to other applications of the FNO architecture as well."
            },
            "weaknesses": {
                "value": "- The evaluation setup is not very clear to me in terms of what was done on training data and what was done on evaluation data. For example, the 99% accuracy made in the introduction seems to be on training data (Table 1)? It makes sense to me that we want to understand how well in general the architecture can fit a neural operator if the training data for that specific problem instance is provided, but that does not tell me anything about how well the approach generalizes. My understanding is that Section 3.2 uses the previously trained neural operators on a held out problem instance (Figure 4). I think this distinction and implications about generalization need to be discussed more clearly. Especially, I'm wondering how dissimilar the instances can become and still converge or how does out-of-domain-ness impact convergence speed.\n- Related work and background of how the contribution ties into the field is explained only on a surface level (in the introduction). Literature cited is mostly from numerical simulation publication venues and not machine learning. Lots of terminology that is used without definition will not be familiar to the broader ICLR audience, e.g. TE and TM polarization not necessarily familiar to non-physics audience, Bloch phase, finite difference frequence domain solvers. Furthermore, the insights gained in the paper are mostly useful to the numerical simulation community in a specific application area. Not sure how generally useful the contribution is to machine learning field because no new machine learning concept are introduced. So I am not sure the broader ICLR audience is the right one for this paper."
            },
            "questions": {
                "value": "- Since neural operators are approximations whose approximation error is hard to bound in advance, how is it possible how useful they will be for a given problem instance? Is there some way of combining them or using them to warm-start or speed up a traditional numerical solver with theoretical convergence guarantees (more like a pre-conditioner)?\n- When can DDM methods be applied in principle / how general are they? Do they only lend themselves to the type of PDEs considered in the paper or are they much broader?\n- Is the setup used in the paper (2D 960x960 grid) relevant to praciticioners? Can useful real-world problems be solved at that resolution?\n- c1 and c2 be set to 1 for \"simplicity\" -> is this based on any sort of hyper param experiment? Technically, you also only need one of these hyper params, since the other one is implicit in alpha.\n- For the time complexity measurements: Mean absolute accuracy of 15% seems like far from convergend (less than first 16 iterations in Figure 4), why was this chosen?\n- Nit: I was a bit confused by the use of \"pixels\" terminology in a paper about PDE solutions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 1,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698659817600,
        "cdate": 1698659817600,
        "tmdate": 1699636389577,
        "mdate": 1699636389577,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "NIXHsSRWf3",
        "forum": "bnPALC6S4l",
        "replyto": "bnPALC6S4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_scWL"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_scWL"
        ],
        "content": {
            "summary": {
                "value": "This paper proposes SNAP-DDM, a method that utilizes deep learning in the context of domain decomposition methods. Each subdomain inside a given DDM framework is separately solved with a neural network-based solver and stitched together using appropriate boundary conditions. Depending on the contents of each subdomain, specialized neural operators are used. The main contribution of this work are modifications to the FNO architecture, in the form of residual connections inspired by the ResNet architecture and self-modulating connections inspired by transformers. The authors evaluate the proposed method on an electromagnetic and a fluid flow problem."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "2 fair"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "I see two main strengths of this paper, mainly in terms of originality and significance:\n\nFirst, to me the application of established PDE deep learning methods to DDM appears to be an original and promising direction, even though I am not an expert in the domain that this paper targets. Nevertheless, I am still unsure if this paper would not be a better fit in a physics journal, as especially Section 2 heavy relies on physical details in the current version of this work.\n\nSecond, if the modifications to FNO hold in a more general setting, the proposed residual and self-modulating connections would be a valuable addition to the neural operator toolbox. Especially so, considering the success of ResNets and transformers compared to previous network architectures in the vision and language modeling domains.\n\nFurthermore, source code is provided alongside this submission, which should help to improve the reproducibility of the shown results. However, I did not run or investigate the source code."
            },
            "weaknesses": {
                "value": "This paper appears to be unfinished in various aspects, and several presentation issues and lacking details make it difficult to clearly understand the methodology and judge the presented empirical results.\n\n### Presentation\n\n**P1:**\nEven though there are a range of references in the introduction, in my opinion this paper would clearly benefit from a dedicated related work section and a more generic introduction. Otherwise connections relative to prior work are difficult to draw, especially for non-experts on the overlap area of DDMs, neural operators, and machine learning like myself. \n\n**P2:**\nThere are statements throughout the paper that are vague, need further explanation, or require citations. Some examples are the following, but this is not an exhaustive list:\n- *\"but the are largely limited to systems featuring predetermined problem sites or fixed PDE parameters\u201d* (abstract)\n- the spectral bias issues of PINNs (end of second paragraph in section 1)\n- the problems of current neural operator methods *\u201cWhile much progress have been made\u2026 resource consuming and undesirable.\u201d* (at the bottom of page 1 / the top of page 2)\n- *\u201cFurthermore, the training of separate networks\u2026 without loss of generality.\u201d* (at the top of page 4)\n\n**P3:**\nThe overall structure of the paper and especially the presentation of the methodology should be improved. It is not clear how exactly the network is used or what its outputs are, even at the end of the methodology section. Furthermore, the methodology section is not written in generic terms of the method, but simply as an example description on the electromagnetic experiment. For instance, it is not discussed how to choose networks in the generic case, or how SNAP-DDM works on the fluid flow problem.\n\n**P4:**\nThe paper generally lacks polishing and contains a range of smaller issues and typos, for example:\n- Number of abbreviations is relatively large and can easily become confusing\n- Fig. 3/4/6: In my opinion, it is not ideal to show ground truth field and error in same colormap\n- Fig. 4: Colorbars are missing\n- Fig. 4/6/7: Subfigures are plotted inconsistently\n- Fig. 7: What are the differently colored lines here? To me that was not clear from the description. Furthermore, the fontsize is too small\n- Section 1, paragraph 2: \u201cansartz\u201d\n- Abstract: \u201cnear unity accuracy\u201d (unclear and unusual formulation)\n- Top of page 3: \u201cYee formalism (cite)\u201d\n- Begin of Section 4: \u201cwith L the being number of layers\u201d\n\n\n### Evaluations\n**E1:**\nThe proposed changes in this work are first and foremost improvements to FNOs and not DDM. It is not clear why they are only tested on the DDM case, which makes things unnecessarily complicated. Instead, a direct comparison against FNOs and other baselines on full-sized PDE problems would be much more logical as a first step. With the current scope it is unclear if the observed improvements hold more generally.\n\n**E2:**\nIt would be nice to have a comparison to a non-DDM model as a baseline, to get an idea of the achieved performance level. For example, a direct prediction via a fully convolutional neural network trained on different domain sizes comes to mind.\n\n**E3:**\nThe physical problems are relatively limited, especially the fluid flow problem (where it is even unclear how SNAP-DDM works, as mentioned in P3 above). An interesting case would be a more complex fluid problem, like an unsteady flow. This would requires multiple time iterations (in addition to the solver iterations), but show that DDM is robust to temporal rollouts, which is a highly desirable property.\n\n**E4:**\nIt is not mentioned how the data sets are split. This is especially relevant for Tab. 1 and 2, as it is unclear what is actually shown here, training loss, validation, or test performance? Furthermore, it would be interesting to see how SNAP-DDM performs for more complex evaluation tasks (slightly) outside of the training domain, for example different Reynolds numbers in the flow experiment. Or boundaries created by a fundamentally different generation method.\n\n**E5:**\nTab 1 shows that more training data substantially improves model performance. Why are the other baselines not evaluated with the same amount of data for a fair comparison at the computational limit that would be used in practice as well?\n\n**E6:**\nTraining details of the baselines are missing, and the appendix only contains a very rough overview of some parameters of each baseline. Furthermore, how are the most important hyperparameters for the baselines chosen?\n\n**E7:**\nWhile the proposed changes are promising compared to FNOs, it seems that other simpler architectures like Unet can achieve similar performance. Especially with recent Unet modernizations commonly used for diffusion models, even better results might be possible (see *\u201cDenoising diffusion probabilistic models\u201d* by Ho et al., NeurIPS 2020, or *\u201cDiffusion models beat GANs on image synthesis\u201d* by Dhariwal and Nichol, NeurIPS 2021).\n\n### Summary\nOverall, this work feels unfinished and in my opinion needs a larger revision in terms of presentation and more rigorous evaluations. Due to the chosen problem setup inside a DDM solver, it is also difficult to tell if the improvements to the FNO architecture actually hold in a more general setting. This leads to my overall recommendation of reject for the current state of this paper.\n\n### Update after Author Response\nWith the improved presentation and additional results, the insights from combining DDM with FNOs are certainly an interesting direction. Nevertheless, I think this paper would clearly benefit from further improving the presentation and investigating the strengths of the other baseline variants, especially U-Net in more detail. Finally, showing the benefits of the proposed improvements to FNOs in a more general setting would be a useful addition. As a result, I reconsidered my original evaluation of this work and updated my original review with the following changes:\n- Soundness Score: increased from *2 fair* to *3 good*\n- Presentation Score: increased from *1 poor* to *2 fair*\n- Overall Score: increased from *3 reject* to *6 marginally above the acceptance threshold*"
            },
            "questions": {
                "value": "**Q1:**\nHow can the data error computed via an L1 loss in Fig. 3/6 be negative? According to equation 4 it is just a sum over the absolute difference which should always be positive?\n\n**Q2:**\nWhat are the results when evaluating the best baseline (i.e. Unet) in the same way as shown for SNAP-DDM in Fig. 4/6b?\n\n**Q3:**\nI did not fully understand the choice for the ablation study models in Tab. 3, especially w.r.t. changing the architecture at the same time as the model sizes $L$ and $C$. The current presentation makes it unclear if the differences are due to architectural changes or the model size.\n\n**Q4:**\nOn a rather abstract level, the iterative refinement approach of the solver has some interesting connections to diffusion models, that also iteratively refine an initial prediction. What are your thoughts on the usage of diffusion models within DDMs: Do you see some potential to map the solver iterations to an iterative model training schedule, achieving a physical diffusion-style DDM framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4225/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4225/Reviewer_scWL",
                    "ICLR.cc/2024/Conference/Submission4225/Senior_Area_Chairs"
                ]
            }
        },
        "number": 2,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698756561245,
        "cdate": 1698756561245,
        "tmdate": 1700755351829,
        "mdate": 1700755351829,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "vrwv6wcqr1",
        "forum": "bnPALC6S4l",
        "replyto": "bnPALC6S4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_mta4"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_mta4"
        ],
        "content": {
            "summary": {
                "value": "The paper introduces a method that employs domain decomposition techniques to help solve steady-state partial differential equations PDEs, for which known physical prior information is available. The model operates by mapping the parameter functions of the equation to its steady-state solution. This is achieved through the division of the entire domain into multiple subdomains. Notably, the surrogate models are exclusively trained using data from the subdomains' results and boundary conditions. The PDE is then solved iteratively, incorporating a physical loss, and updating the boundary conditions for each subdomain. The results of the proposed model are promising."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "1 poor"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- The paper investigates the utilization of an existing physical model to streamline the solving of partial differential equations (PDEs) by spatially decomposing the domain. This approach reduces the complexity of the required model for each subdomain. This offers insights into incorporating additional physical priors to enhance the scalability of current neural PDE surrogate models.\n- The method is evaluated on various electromagnetic systems with diverse domain shapes. The modular design of the model makes it applicable for solving the same PDE in different settings."
            },
            "weaknesses": {
                "value": "- In terms of presentation quality, I recommend enhancing the overall structure of the paper by providing a more visible and distinct description of the model, including a concise definition of the input and the output. Additionally, it would be beneficial to separate and clarify the sections for the model, training, and evaluation processes. Currently, understanding the entire training pipeline is challenging as it is intertwined with the model description, the existing algorithm, and the experimental results. Furthermore, I suggest reorganizing the presentation of experimental results with a clearer structure, dedicating specific sections to data preparation, technical details, and result analysis.\n- Baselines comparison: While the model is currently benchmarked against baseline architectures for subdomain training, it is essential to consider a broader comparison with physics-informed baselines such as PINNs and PINOs/PiDeepONet on the entire domain. This extended evaluation would provide a more comprehensive understanding of how the proposed method compares to established DL-based physics-informed techniques.\n- Regarding terminology: I was somewhat surprised when I noticed that the authors introduced an additional residual connection alongside the one that already existed. (Please note that point-wise convolution is equivalent to applying a linear layer to each pixel and is a conventional implementation of residual connections when the input and output channel counts do not match.) Have the authors considered replacing the point-wise convolution with a simple identity addition?"
            },
            "questions": {
                "value": "- It's quite unexpected that the original FNO struggles to predict the outputs of not-so-complex subdomains accurately. It would be valuable if the authors could provide additional insights into the specific challenges or limitations the original FNO faces when applied to these cases.\n- In Figure 7, it's intriguing to observe the solver's convergence with iterations, especially considering that the subdomain models are trained solely with the ground truth subdomain boundary conditions. To gain a deeper understanding of this convergence behavior, it would be beneficial to include a more detailed graph that provides further insights into the solver's performance throughout the entire testing process. The current figures show data from only five selected points, but a more precise graph could shed light on the solver's convergence across the entire testing period."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "5: marginally below the acceptance threshold"
            },
            "confidence": {
                "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        "number": 3,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1698841520119,
        "cdate": 1698841520119,
        "tmdate": 1699636389410,
        "mdate": 1699636389410,
        "license": "CC BY 4.0",
        "version": 2
    },
    {
        "id": "5wDCvsRbSk",
        "forum": "bnPALC6S4l",
        "replyto": "bnPALC6S4l",
        "signatures": [
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_RuAk"
        ],
        "nonreaders": [],
        "readers": [
            "everyone"
        ],
        "writers": [
            "ICLR.cc/2024/Conference",
            "ICLR.cc/2024/Conference/Submission4225/Reviewer_RuAk"
        ],
        "content": {
            "summary": {
                "value": "# Initial comment\nIn this paper, the authors try to extend the current learning-based PDE surrogate solvers to handle problems with larger scale, more complicated boundaries, and varying parameters, by integrating domain decomposition methods. In each subdomain, the problem is solved by neural operators with extra residual connection and modulation encoders. Experiments on electromagnetic and fluidic flow problems are performed to demonstrate the effectiveness of the proposed model, compared with FNO, UNet, and Swin Transformer.\n\n# After author-reviewer rebuttal\nOverall, thanks very much for your detailed response, enhanced experiments, and modification of the manuscript.\n\n- Thanks for confirming some of the points I provided in the previous review.\n- Thanks for the clarification on how the boundary information is handled in the proposed method.\n\nWith all the factors considered, as well as the fact that a positive score is already given, I decide to remain my original score for now."
            },
            "soundness": {
                "value": "3 good"
            },
            "presentation": {
                "value": "3 good"
            },
            "contribution": {
                "value": "3 good"
            },
            "strengths": {
                "value": "- This is one of the earlist works to combine Neural Operators and DDM.\n- The motivation is clear and reasonable, as described at the end of the third paragraph in the Introduction Section."
            },
            "weaknesses": {
                "value": "- The proposed method will perform self-consistent iterations, introducing extra complexity and convergence issues.\n- The scale and shape of the PDE problems considered in this paper are not complicated enough to be fully convincing to me.\n- The design is relatively straightforward and inflexible, resulting in limitations such as fixed subdomain size and shape."
            },
            "questions": {
                "value": "- One contribution the paper claimed is that extra residual connection is added to the FNO module. But the original FNO block already contains a parameterized residual connection. From the description in the paper, I understand that it is validated by experiments. But what is the benefit of such design in principle? Specifically speaking, if the W matrix in FNO is initialized as an identity matrix, will there be any difference?\n- I am still not very clear about how the boundary information is fed into SNAP-DDM. A bit more explanation will be appreciated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": "6: marginally above the acceptance threshold"
            },
            "confidence": {
                "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
            },
            "code_of_conduct": {
                "value": "Yes"
            },
            "first_time_reviewer": {
                "value": "Yes",
                "readers": [
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission4225/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission4225/Reviewer_RuAk",
                    "ICLR.cc/2024/Conference/Submission4225/Senior_Area_Chairs"
                ]
            }
        },
        "number": 4,
        "invitations": [
            "ICLR.cc/2024/Conference/Submission4225/-/Official_Review",
            "ICLR.cc/2024/Conference/-/Edit"
        ],
        "domain": "ICLR.cc/2024/Conference",
        "tcdate": 1699519447375,
        "cdate": 1699519447375,
        "tmdate": 1700747907620,
        "mdate": 1700747907620,
        "license": "CC BY 4.0",
        "version": 2
    }
]